{"image_index": 0, "image_name": "000000000139.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000139.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1655.287, "latencies_ms": [1655.287], "images_per_second": 0.604, "prompt_tokens": 9, "response_tokens_est": 32, "n_tiles": 6, "output_text": "The image depicts a brightly lit, modern living room with a yellow wall, a wooden floor, and a dining area with a table and chairs.", "error": null, "sys_before": {"cpu_percent": 5.6, "ram_used_mb": 26809.2, "ram_available_mb": 98963.0, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 26793.9, "ram_available_mb": 98978.3, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 7740.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 7740.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.77, "peak": 14.2, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 14.05, "peak": 14.57, "min": 13.0}, "VDD_GPU": {"avg": 29.73, "peak": 37.83, "min": 22.86}, "VIN": {"avg": 67.9, "peak": 92.45, "min": 59.87}}, "power_watts_avg": 29.73, "energy_joules_est": 49.23, "sample_count": 12, "duration_seconds": 1.656}, "timestamp": "2026-01-17T14:19:15.391008"}
{"image_index": 0, "image_name": "000000000139.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000139.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 4717.404, "latencies_ms": [4717.404], "images_per_second": 0.212, "prompt_tokens": 23, "response_tokens_est": 129, "n_tiles": 6, "output_text": "- table: 1\n- vase: 1\n- chair: 3\n- table: 1\n- vase: 1\n- table: 1\n- vase: 1\n- table: 1\n- vase: 1\n- table: 1\n- vase: 1\n- table: 1\n- vase: 1\n- table: 1\n- vase: 1\n- table: 1\n- vase: 1\n- table: 1\n- vase: 1\n- table: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26793.9, "ram_available_mb": 98978.3, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 26766.1, "ram_available_mb": 99006.1, "ram_percent": 21.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 7740.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 7740.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.11, "peak": 14.63, "min": 13.31}, "VDD_CPU_SOC_MSS": {"avg": 14.18, "peak": 14.97, "min": 13.39}, "VDD_GPU": {"avg": 24.21, "peak": 37.04, "min": 20.89}, "VIN": {"avg": 62.68, "peak": 92.58, "min": 55.69}}, "power_watts_avg": 24.21, "energy_joules_est": 114.22, "sample_count": 37, "duration_seconds": 4.718}, "timestamp": "2026-01-17T14:19:20.115371"}
{"image_index": 0, "image_name": "000000000139.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000139.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 4265.911, "latencies_ms": [4265.911], "images_per_second": 0.234, "prompt_tokens": 27, "response_tokens_est": 114, "n_tiles": 6, "output_text": "The main objects in the image are arranged in a way that showcases a harmonious spatial relationship. The television is positioned on the left side of the room, with a small vase and a plant placed near it. The dining table is centrally located, with chairs arranged around it. The kitchen area is on the right side, with a refrigerator and cabinets visible. The fireplace is situated in the background, adding warmth to the room. The floor is covered with polished wood, and a large rug is placed in the foreground, providing a cozy and inviting atmosphere.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 26766.1, "ram_available_mb": 99006.1, "ram_percent": 21.3}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 26726.0, "ram_available_mb": 99046.2, "ram_percent": 21.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 7740.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 7740.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.14, "peak": 14.63, "min": 13.31}, "VDD_CPU_SOC_MSS": {"avg": 14.17, "peak": 14.97, "min": 13.39}, "VDD_GPU": {"avg": 24.51, "peak": 37.04, "min": 20.89}, "VIN": {"avg": 63.1, "peak": 94.5, "min": 47.42}}, "power_watts_avg": 24.51, "energy_joules_est": 104.57, "sample_count": 33, "duration_seconds": 4.266}, "timestamp": "2026-01-17T14:19:24.391654"}
{"image_index": 0, "image_name": "000000000139.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000139.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3233.0, "latencies_ms": [3233.0], "images_per_second": 0.309, "prompt_tokens": 21, "response_tokens_est": 81, "n_tiles": 6, "output_text": "The image depicts a brightly lit, modern living room with a warm yellow and orange color scheme. The room features a wooden floor, a large dining table with chairs, a television on a stand, a vase with a pink flower on a table, and a green vase on a table. There is a person visible in the background, seemingly engaged in an activity near the kitchen area.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 26726.0, "ram_available_mb": 99046.2, "ram_percent": 21.2}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 26701.6, "ram_available_mb": 99070.6, "ram_percent": 21.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 7740.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 7740.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.11, "peak": 14.63, "min": 13.31}, "VDD_CPU_SOC_MSS": {"avg": 14.23, "peak": 14.97, "min": 13.39}, "VDD_GPU": {"avg": 25.67, "peak": 36.65, "min": 20.89}, "VIN": {"avg": 63.35, "peak": 100.42, "min": 51.07}}, "power_watts_avg": 25.67, "energy_joules_est": 83.0, "sample_count": 25, "duration_seconds": 3.233}, "timestamp": "2026-01-17T14:19:27.631208"}
{"image_index": 0, "image_name": "000000000139.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000139.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2733.461, "latencies_ms": [2733.461], "images_per_second": 0.366, "prompt_tokens": 19, "response_tokens_est": 65, "n_tiles": 6, "output_text": "The room is brightly lit with natural light streaming in through large windows, creating a warm and inviting atmosphere. The walls are painted in a soft yellow, complemented by a green accent wall. The wooden floor adds warmth to the space, and the furniture is made of a combination of materials, including wood and metal.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26701.6, "ram_available_mb": 99070.6, "ram_percent": 21.2}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 26678.6, "ram_available_mb": 99093.6, "ram_percent": 21.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 7740.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 7740.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.11, "peak": 14.63, "min": 13.31}, "VDD_CPU_SOC_MSS": {"avg": 14.31, "peak": 14.97, "min": 13.39}, "VDD_GPU": {"avg": 26.56, "peak": 37.04, "min": 20.89}, "VIN": {"avg": 66.45, "peak": 105.26, "min": 50.59}}, "power_watts_avg": 26.56, "energy_joules_est": 72.61, "sample_count": 21, "duration_seconds": 2.734}, "timestamp": "2026-01-17T14:19:30.371209"}
{"image_index": 1, "image_name": "000000000285.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000285.jpg", "image_width": 586, "image_height": 640, "image_resolution": "586x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 997.131, "latencies_ms": [997.131], "images_per_second": 1.003, "prompt_tokens": 9, "response_tokens_est": 34, "n_tiles": 1, "output_text": "The image features a close-up of a brown bear with a soft, fluffy fur coat, sitting on a grassy surface, displaying a calm and curious expression.", "error": null, "sys_before": {"cpu_percent": 23.5, "ram_used_mb": 26678.6, "ram_available_mb": 99093.6, "ram_percent": 21.2}, "sys_after": {"cpu_percent": 6.3, "ram_used_mb": 26681.0, "ram_available_mb": 99091.2, "ram_percent": 21.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 7838.0, "gpu_max_mem_alloc_mb": 4845.7, "gpu_max_mem_reserved_mb": 7838.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.93, "peak": 14.3, "min": 13.39}, "VDD_CPU_SOC_MSS": {"avg": 14.24, "peak": 14.58, "min": 13.79}, "VDD_GPU": {"avg": 20.27, "peak": 23.26, "min": 18.53}, "VIN": {"avg": 54.78, "peak": 59.4, "min": 40.96}}, "power_watts_avg": 20.27, "energy_joules_est": 20.22, "sample_count": 7, "duration_seconds": 0.998}, "timestamp": "2026-01-17T14:19:31.396253"}
{"image_index": 1, "image_name": "000000000285.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000285.jpg", "image_width": 586, "image_height": 640, "image_resolution": "586x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 369.225, "latencies_ms": [369.225], "images_per_second": 2.708, "prompt_tokens": 23, "response_tokens_est": 11, "n_tiles": 1, "output_text": "bear: 1\ngrass: 1", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 26672.3, "ram_available_mb": 99099.9, "ram_percent": 21.2}, "sys_after": {"cpu_percent": 6.4, "ram_used_mb": 26672.2, "ram_available_mb": 99100.0, "ram_percent": 21.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 7838.0, "gpu_max_mem_alloc_mb": 4856.2, "gpu_max_mem_reserved_mb": 7838.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.0, "peak": 14.1, "min": 13.9}, "VDD_CPU_SOC_MSS": {"avg": 14.38, "peak": 14.58, "min": 14.19}, "VDD_GPU": {"avg": 21.49, "peak": 22.08, "min": 20.89}, "VIN": {"avg": 56.91, "peak": 58.81, "min": 55.0}}, "power_watts_avg": 21.49, "energy_joules_est": 7.94, "sample_count": 2, "duration_seconds": 0.37}, "timestamp": "2026-01-17T14:19:31.773099"}
{"image_index": 1, "image_name": "000000000285.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000285.jpg", "image_width": 586, "image_height": 640, "image_resolution": "586x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1328.543, "latencies_ms": [1328.543], "images_per_second": 0.753, "prompt_tokens": 27, "response_tokens_est": 48, "n_tiles": 1, "output_text": "The main object in the image is a brown bear. The bear is positioned in the foreground, with its head and upper body visible. The background consists of a grassy area, indicating that the bear is likely in a natural habitat.", "error": null, "sys_before": {"cpu_percent": 16.7, "ram_used_mb": 26672.2, "ram_available_mb": 99100.0, "ram_percent": 21.2}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 26664.3, "ram_available_mb": 99107.8, "ram_percent": 21.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 7838.0, "gpu_max_mem_alloc_mb": 4859.2, "gpu_max_mem_reserved_mb": 7838.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.26, "peak": 14.51, "min": 13.79}, "VDD_CPU_SOC_MSS": {"avg": 14.5, "peak": 14.57, "min": 14.18}, "VDD_GPU": {"avg": 19.83, "peak": 23.65, "min": 18.14}, "VIN": {"avg": 58.38, "peak": 63.55, "min": 55.52}}, "power_watts_avg": 19.83, "energy_joules_est": 26.35, "sample_count": 10, "duration_seconds": 1.329}, "timestamp": "2026-01-17T14:19:33.107815"}
{"image_index": 1, "image_name": "000000000285.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000285.jpg", "image_width": 586, "image_height": 640, "image_resolution": "586x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1375.177, "latencies_ms": [1375.177], "images_per_second": 0.727, "prompt_tokens": 21, "response_tokens_est": 50, "n_tiles": 1, "output_text": "The image depicts a brown bear sitting on a grassy surface, likely in a natural habitat. The bear appears to be calm and relaxed, with its eyes slightly open and its mouth slightly open, possibly indicating a moment of rest or curiosity.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26664.3, "ram_available_mb": 99107.8, "ram_percent": 21.2}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 26656.2, "ram_available_mb": 99115.9, "ram_percent": 21.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 7838.0, "gpu_max_mem_alloc_mb": 4854.7, "gpu_max_mem_reserved_mb": 7838.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.38, "peak": 14.51, "min": 14.0}, "VDD_CPU_SOC_MSS": {"avg": 14.58, "peak": 14.97, "min": 14.18}, "VDD_GPU": {"avg": 19.24, "peak": 22.08, "min": 17.74}, "VIN": {"avg": 57.6, "peak": 61.61, "min": 55.24}}, "power_watts_avg": 19.24, "energy_joules_est": 26.46, "sample_count": 10, "duration_seconds": 1.375}, "timestamp": "2026-01-17T14:19:34.489709"}
{"image_index": 1, "image_name": "000000000285.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000285.jpg", "image_width": 586, "image_height": 640, "image_resolution": "586x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1249.417, "latencies_ms": [1249.417], "images_per_second": 0.8, "prompt_tokens": 19, "response_tokens_est": 45, "n_tiles": 1, "output_text": "The image features a brown bear with a textured fur coat, standing on a grassy surface. The lighting is natural, casting soft shadows on the bear's face, and the weather appears to be clear and sunny.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26656.2, "ram_available_mb": 99115.9, "ram_percent": 21.2}, "sys_after": {"cpu_percent": 6.7, "ram_used_mb": 26646.2, "ram_available_mb": 99126.0, "ram_percent": 21.2}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 7838.0, "gpu_max_mem_alloc_mb": 4853.2, "gpu_max_mem_reserved_mb": 7838.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.38, "peak": 14.51, "min": 14.1}, "VDD_CPU_SOC_MSS": {"avg": 14.71, "peak": 14.97, "min": 14.19}, "VDD_GPU": {"avg": 19.23, "peak": 21.68, "min": 17.74}, "VIN": {"avg": 58.64, "peak": 68.15, "min": 52.2}}, "power_watts_avg": 19.23, "energy_joules_est": 24.04, "sample_count": 9, "duration_seconds": 1.25}, "timestamp": "2026-01-17T14:19:35.745286"}
{"image_index": 2, "image_name": "000000000632.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000632.jpg", "image_width": 640, "image_height": 483, "image_resolution": "640x483", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 3052.466, "latencies_ms": [3052.466], "images_per_second": 0.328, "prompt_tokens": 9, "response_tokens_est": 33, "n_tiles": 12, "output_text": "The image depicts a cozy bedroom with a large window that allows natural light to flood the room, illuminating the space and creating a warm and inviting atmosphere.", "error": null, "sys_before": {"cpu_percent": 47.0, "ram_used_mb": 26617.5, "ram_available_mb": 99154.7, "ram_percent": 21.2}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 26042.8, "ram_available_mb": 99729.3, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 10140.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 10140.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.23, "peak": 15.75, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 14.57, "peak": 16.15, "min": 13.39}, "VDD_GPU": {"avg": 31.7, "peak": 39.8, "min": 24.05}, "VIN": {"avg": 70.98, "peak": 99.42, "min": 52.49}}, "power_watts_avg": 31.7, "energy_joules_est": 96.78, "sample_count": 23, "duration_seconds": 3.053}, "timestamp": "2026-01-17T14:19:38.899545"}
{"image_index": 2, "image_name": "000000000632.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000632.jpg", "image_width": 640, "image_height": 483, "image_resolution": "640x483", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 3732.252, "latencies_ms": [3732.252], "images_per_second": 0.268, "prompt_tokens": 23, "response_tokens_est": 50, "n_tiles": 12, "output_text": "- bed: 1\n- dresser: 1\n- mirror: 1\n- bookshelf: 1\n- window: 1\n- chair: 1\n- plant: 1\n- basket: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26042.8, "ram_available_mb": 99729.3, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 26045.6, "ram_available_mb": 99726.6, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11854.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11854.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.31, "peak": 15.75, "min": 13.31}, "VDD_CPU_SOC_MSS": {"avg": 14.48, "peak": 16.15, "min": 13.39}, "VDD_GPU": {"avg": 31.29, "peak": 40.98, "min": 23.65}, "VIN": {"avg": 70.11, "peak": 112.95, "min": 50.74}}, "power_watts_avg": 31.29, "energy_joules_est": 116.79, "sample_count": 29, "duration_seconds": 3.733}, "timestamp": "2026-01-17T14:19:42.638300"}
{"image_index": 2, "image_name": "000000000632.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000632.jpg", "image_width": 640, "image_height": 483, "image_resolution": "640x483", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 4970.086, "latencies_ms": [4970.086], "images_per_second": 0.201, "prompt_tokens": 27, "response_tokens_est": 82, "n_tiles": 12, "output_text": "The main objects in the image are a bed, a dresser, a bookshelf, and a window. The bed is positioned in the foreground, with its headboard and footboard visible. The dresser is situated to the left of the bed, and the bookshelf is positioned to the right of the bed. The window is located in the background, allowing natural light to illuminate the room.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26045.6, "ram_available_mb": 99726.6, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26043.3, "ram_available_mb": 99728.9, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.27, "peak": 15.65, "min": 13.19}, "VDD_CPU_SOC_MSS": {"avg": 14.39, "peak": 16.15, "min": 13.39}, "VDD_GPU": {"avg": 29.22, "peak": 40.98, "min": 23.26}, "VIN": {"avg": 70.62, "peak": 114.9, "min": 60.34}}, "power_watts_avg": 29.22, "energy_joules_est": 145.24, "sample_count": 39, "duration_seconds": 4.97}, "timestamp": "2026-01-17T14:19:47.615331"}
{"image_index": 2, "image_name": "000000000632.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000632.jpg", "image_width": 640, "image_height": 483, "image_resolution": "640x483", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 4811.802, "latencies_ms": [4811.802], "images_per_second": 0.208, "prompt_tokens": 21, "response_tokens_est": 78, "n_tiles": 12, "output_text": "The image depicts a cozy, well-organized bedroom with a large window that allows natural light to flood the room. The room is filled with various items, including a wooden dresser, a bookshelf filled with books, a small potted plant, a cushioned chair, and a basket. The overall atmosphere is warm and inviting, suggesting a comfortable and lived-in space.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 26043.3, "ram_available_mb": 99728.9, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26004.6, "ram_available_mb": 99767.5, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.27, "peak": 15.75, "min": 13.19}, "VDD_CPU_SOC_MSS": {"avg": 14.42, "peak": 16.15, "min": 13.39}, "VDD_GPU": {"avg": 29.52, "peak": 40.98, "min": 23.26}, "VIN": {"avg": 69.39, "peak": 101.89, "min": 58.13}}, "power_watts_avg": 29.52, "energy_joules_est": 142.06, "sample_count": 38, "duration_seconds": 4.812}, "timestamp": "2026-01-17T14:19:52.434087"}
{"image_index": 2, "image_name": "000000000632.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000632.jpg", "image_width": 640, "image_height": 483, "image_resolution": "640x483", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3275.413, "latencies_ms": [3275.413], "images_per_second": 0.305, "prompt_tokens": 19, "response_tokens_est": 38, "n_tiles": 12, "output_text": "The room is bathed in natural light, creating a warm and inviting atmosphere. The walls are adorned with a floral wallpaper pattern, adding a touch of elegance to the space.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26004.6, "ram_available_mb": 99767.5, "ram_percent": 20.7}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 25982.0, "ram_available_mb": 99790.2, "ram_percent": 20.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.35, "peak": 15.75, "min": 13.31}, "VDD_CPU_SOC_MSS": {"avg": 14.46, "peak": 16.15, "min": 13.39}, "VDD_GPU": {"avg": 32.43, "peak": 40.98, "min": 24.05}, "VIN": {"avg": 72.05, "peak": 122.76, "min": 50.99}}, "power_watts_avg": 32.43, "energy_joules_est": 106.24, "sample_count": 25, "duration_seconds": 3.276}, "timestamp": "2026-01-17T14:19:55.716165"}
{"image_index": 3, "image_name": "000000000724.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000724.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2783.684, "latencies_ms": [2783.684], "images_per_second": 0.359, "prompt_tokens": 9, "response_tokens_est": 26, "n_tiles": 12, "output_text": "A red stop sign is mounted on a pole, indicating a stop traffic signal, with a clear blue sky in the background.", "error": null, "sys_before": {"cpu_percent": 41.4, "ram_used_mb": 25970.9, "ram_available_mb": 99801.3, "ram_percent": 20.6}, "sys_after": {"cpu_percent": 2.6, "ram_used_mb": 25953.2, "ram_available_mb": 99818.9, "ram_percent": 20.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.24, "peak": 15.75, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 14.44, "peak": 16.15, "min": 13.0}, "VDD_GPU": {"avg": 33.01, "peak": 40.98, "min": 24.84}, "VIN": {"avg": 76.89, "peak": 123.66, "min": 53.29}}, "power_watts_avg": 33.01, "energy_joules_est": 91.9, "sample_count": 21, "duration_seconds": 2.784}, "timestamp": "2026-01-17T14:19:58.600968"}
{"image_index": 3, "image_name": "000000000724.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000724.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 3158.491, "latencies_ms": [3158.491], "images_per_second": 0.317, "prompt_tokens": 23, "response_tokens_est": 34, "n_tiles": 12, "output_text": "stop sign: 1\ntree: 1\ntrash can: 1\nbuilding: 1\nroad: 1\npole: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25953.2, "ram_available_mb": 99818.9, "ram_percent": 20.6}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 25945.5, "ram_available_mb": 99826.7, "ram_percent": 20.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.21, "peak": 15.54, "min": 13.29}, "VDD_CPU_SOC_MSS": {"avg": 14.44, "peak": 16.15, "min": 13.39}, "VDD_GPU": {"avg": 32.86, "peak": 41.77, "min": 23.65}, "VIN": {"avg": 69.47, "peak": 100.6, "min": 53.73}}, "power_watts_avg": 32.86, "energy_joules_est": 103.8, "sample_count": 24, "duration_seconds": 3.159}, "timestamp": "2026-01-17T14:20:01.766059"}
{"image_index": 3, "image_name": "000000000724.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000724.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 4506.88, "latencies_ms": [4506.88], "images_per_second": 0.222, "prompt_tokens": 27, "response_tokens_est": 69, "n_tiles": 12, "output_text": "The main objects in the image are a stop sign and a street lamp. The stop sign is positioned in the foreground, near the sidewalk, while the street lamp is in the background, slightly to the left. The stop sign is closer to the viewer, while the street lamp is further away, creating a sense of depth in the image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25945.5, "ram_available_mb": 99826.7, "ram_percent": 20.6}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25915.0, "ram_available_mb": 99857.2, "ram_percent": 20.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.02, "peak": 15.44, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 14.17, "peak": 16.15, "min": 13.39}, "VDD_GPU": {"avg": 29.76, "peak": 40.98, "min": 23.26}, "VIN": {"avg": 71.97, "peak": 128.05, "min": 58.54}}, "power_watts_avg": 29.76, "energy_joules_est": 134.14, "sample_count": 35, "duration_seconds": 4.508}, "timestamp": "2026-01-17T14:20:06.279686"}
{"image_index": 3, "image_name": "000000000724.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000724.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 4658.259, "latencies_ms": [4658.259], "images_per_second": 0.215, "prompt_tokens": 21, "response_tokens_est": 73, "n_tiles": 12, "output_text": "The image depicts a street scene with a stop sign prominently displayed in the foreground. The sign is red with white lettering, and it is mounted on a metal pole. In the background, there is a tree-lined street with a white building and a parked truck. The setting appears to be a suburban area with clear skies and a calm atmosphere.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25914.7, "ram_available_mb": 99857.5, "ram_percent": 20.6}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25907.0, "ram_available_mb": 99865.2, "ram_percent": 20.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.0, "peak": 15.44, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 14.11, "peak": 16.14, "min": 13.0}, "VDD_GPU": {"avg": 29.54, "peak": 40.59, "min": 22.86}, "VIN": {"avg": 71.2, "peak": 106.45, "min": 55.17}}, "power_watts_avg": 29.54, "energy_joules_est": 137.62, "sample_count": 37, "duration_seconds": 4.659}, "timestamp": "2026-01-17T14:20:10.944758"}
{"image_index": 3, "image_name": "000000000724.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000724.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 4107.439, "latencies_ms": [4107.439], "images_per_second": 0.243, "prompt_tokens": 19, "response_tokens_est": 59, "n_tiles": 12, "output_text": "The image features a red stop sign with white lettering, mounted on a metal pole. The sign is set against a backdrop of a clear blue sky with scattered clouds, indicating a sunny day. The surrounding area includes green trees, a sidewalk, and a road with a white line marking.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25907.0, "ram_available_mb": 99865.2, "ram_percent": 20.6}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 25883.2, "ram_available_mb": 99889.0, "ram_percent": 20.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.03, "peak": 15.44, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 14.14, "peak": 15.75, "min": 12.99}, "VDD_GPU": {"avg": 30.32, "peak": 40.98, "min": 23.26}, "VIN": {"avg": 69.6, "peak": 119.35, "min": 58.6}}, "power_watts_avg": 30.32, "energy_joules_est": 124.55, "sample_count": 32, "duration_seconds": 4.108}, "timestamp": "2026-01-17T14:20:15.058862"}
{"image_index": 4, "image_name": "000000000776.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000776.jpg", "image_width": 428, "image_height": 640, "image_resolution": "428x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1806.713, "latencies_ms": [1806.713], "images_per_second": 0.553, "prompt_tokens": 9, "response_tokens_est": 37, "n_tiles": 6, "output_text": "The image shows a close-up of two teddy bears, one in a light brown color and the other in a darker shade, both with soft, fuzzy textures and round ears.", "error": null, "sys_before": {"cpu_percent": 34.2, "ram_used_mb": 25894.8, "ram_available_mb": 99877.3, "ram_percent": 20.6}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 25888.4, "ram_available_mb": 99883.7, "ram_percent": 20.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.83, "peak": 14.3, "min": 12.68}, "VDD_CPU_SOC_MSS": {"avg": 13.99, "peak": 14.57, "min": 13.0}, "VDD_GPU": {"avg": 29.36, "peak": 38.23, "min": 22.08}, "VIN": {"avg": 66.84, "peak": 95.78, "min": 53.94}}, "power_watts_avg": 29.36, "energy_joules_est": 53.07, "sample_count": 14, "duration_seconds": 1.807}, "timestamp": "2026-01-17T14:20:16.927937"}
{"image_index": 4, "image_name": "000000000776.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000776.jpg", "image_width": 428, "image_height": 640, "image_resolution": "428x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2451.216, "latencies_ms": [2451.216], "images_per_second": 0.408, "prompt_tokens": 23, "response_tokens_est": 56, "n_tiles": 6, "output_text": "1. Brown teddy bear\n2. Brown teddy bear\n3. Brown teddy bear\n4. Brown teddy bear\n5. Brown teddy bear\n6. Brown teddy bear\n7. Brown teddy bear\n8. Brown teddy bear", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 25888.4, "ram_available_mb": 99883.7, "ram_percent": 20.6}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 25881.0, "ram_available_mb": 99891.1, "ram_percent": 20.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.18, "peak": 14.73, "min": 13.41}, "VDD_CPU_SOC_MSS": {"avg": 14.41, "peak": 14.96, "min": 13.39}, "VDD_GPU": {"avg": 27.36, "peak": 37.04, "min": 21.29}, "VIN": {"avg": 67.59, "peak": 108.76, "min": 57.3}}, "power_watts_avg": 27.36, "energy_joules_est": 67.08, "sample_count": 19, "duration_seconds": 2.452}, "timestamp": "2026-01-17T14:20:19.385425"}
{"image_index": 4, "image_name": "000000000776.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000776.jpg", "image_width": 428, "image_height": 640, "image_resolution": "428x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2791.38, "latencies_ms": [2791.38], "images_per_second": 0.358, "prompt_tokens": 27, "response_tokens_est": 67, "n_tiles": 6, "output_text": "The main objects in the image are two teddy bears. The teddy bear on the left is in the foreground, while the teddy bear on the right is slightly out of focus in the background. The teddy bear on the left is closer to the viewer, while the teddy bear on the right is further away.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25881.0, "ram_available_mb": 99891.1, "ram_percent": 20.6}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 25865.8, "ram_available_mb": 99906.4, "ram_percent": 20.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.19, "peak": 14.73, "min": 13.41}, "VDD_CPU_SOC_MSS": {"avg": 14.47, "peak": 14.97, "min": 13.39}, "VDD_GPU": {"avg": 26.6, "peak": 37.04, "min": 21.28}, "VIN": {"avg": 65.62, "peak": 107.14, "min": 51.23}}, "power_watts_avg": 26.6, "energy_joules_est": 74.26, "sample_count": 22, "duration_seconds": 2.792}, "timestamp": "2026-01-17T14:20:22.183134"}
{"image_index": 4, "image_name": "000000000776.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000776.jpg", "image_width": 428, "image_height": 640, "image_resolution": "428x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2605.567, "latencies_ms": [2605.567], "images_per_second": 0.384, "prompt_tokens": 21, "response_tokens_est": 61, "n_tiles": 6, "output_text": "The image depicts a cozy and intimate scene featuring two teddy bears. One bear is lying down and the other is standing close to it, both appearing to be in a comfortable and familiar environment. The setting suggests a home or a personal space where the bears are being cared for and cherished.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25865.8, "ram_available_mb": 99906.4, "ram_percent": 20.6}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 25857.7, "ram_available_mb": 99914.5, "ram_percent": 20.6}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.16, "peak": 14.73, "min": 13.41}, "VDD_CPU_SOC_MSS": {"avg": 14.44, "peak": 14.97, "min": 13.39}, "VDD_GPU": {"avg": 26.94, "peak": 37.04, "min": 21.29}, "VIN": {"avg": 68.26, "peak": 107.85, "min": 46.71}}, "power_watts_avg": 26.94, "energy_joules_est": 70.2, "sample_count": 20, "duration_seconds": 2.606}, "timestamp": "2026-01-17T14:20:24.795494"}
{"image_index": 4, "image_name": "000000000776.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000776.jpg", "image_width": 428, "image_height": 640, "image_resolution": "428x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2606.95, "latencies_ms": [2606.95], "images_per_second": 0.384, "prompt_tokens": 19, "response_tokens_est": 61, "n_tiles": 6, "output_text": "The image features two teddy bears with a warm, earthy brown color palette. The lighting is soft and diffused, casting gentle shadows and highlighting the plush texture of the bears. The weather appears to be overcast, as the overall lighting is subdued and the shadows are soft.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25857.7, "ram_available_mb": 99914.5, "ram_percent": 20.6}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 25842.2, "ram_available_mb": 99930.0, "ram_percent": 20.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.17, "peak": 14.63, "min": 13.41}, "VDD_CPU_SOC_MSS": {"avg": 14.46, "peak": 14.97, "min": 13.39}, "VDD_GPU": {"avg": 26.9, "peak": 37.44, "min": 21.29}, "VIN": {"avg": 64.49, "peak": 91.17, "min": 55.65}}, "power_watts_avg": 26.9, "energy_joules_est": 70.14, "sample_count": 20, "duration_seconds": 2.607}, "timestamp": "2026-01-17T14:20:27.408727"}
{"image_index": 5, "image_name": "000000000785.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000785.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1775.408, "latencies_ms": [1775.408], "images_per_second": 0.563, "prompt_tokens": 9, "response_tokens_est": 36, "n_tiles": 6, "output_text": "A person is skiing down a snowy slope, wearing a red jacket, black pants, and a black beanie, with blue and red skis and ski poles.", "error": null, "sys_before": {"cpu_percent": 41.8, "ram_used_mb": 25841.9, "ram_available_mb": 99930.3, "ram_percent": 20.5}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 25833.9, "ram_available_mb": 99938.3, "ram_percent": 20.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.88, "peak": 14.2, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 14.18, "peak": 14.58, "min": 13.39}, "VDD_GPU": {"avg": 29.59, "peak": 38.23, "min": 22.46}, "VIN": {"avg": 68.35, "peak": 100.58, "min": 56.63}}, "power_watts_avg": 29.59, "energy_joules_est": 52.54, "sample_count": 13, "duration_seconds": 1.776}, "timestamp": "2026-01-17T14:20:29.246082"}
{"image_index": 5, "image_name": "000000000785.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000785.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1946.167, "latencies_ms": [1946.167], "images_per_second": 0.514, "prompt_tokens": 23, "response_tokens_est": 40, "n_tiles": 6, "output_text": "object: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25833.9, "ram_available_mb": 99938.3, "ram_percent": 20.5}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 25834.1, "ram_available_mb": 99938.1, "ram_percent": 20.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.13, "peak": 14.73, "min": 13.41}, "VDD_CPU_SOC_MSS": {"avg": 14.42, "peak": 14.97, "min": 13.39}, "VDD_GPU": {"avg": 29.19, "peak": 37.44, "min": 22.08}, "VIN": {"avg": 65.33, "peak": 96.18, "min": 48.94}}, "power_watts_avg": 29.19, "energy_joules_est": 56.82, "sample_count": 15, "duration_seconds": 1.946}, "timestamp": "2026-01-17T14:20:31.198311"}
{"image_index": 5, "image_name": "000000000785.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000785.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3331.559, "latencies_ms": [3331.559], "images_per_second": 0.3, "prompt_tokens": 27, "response_tokens_est": 84, "n_tiles": 6, "output_text": "The main subject, a person, is positioned in the foreground of the image. They are wearing a red jacket, black pants, and blue ski boots. The person is holding ski poles and appears to be skiing on a snowy slope. In the background, there is a snow-covered hill with a fence and orange poles. The person is near the fence, which is in the middle of the image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25834.1, "ram_available_mb": 99938.1, "ram_percent": 20.5}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 25802.9, "ram_available_mb": 99969.3, "ram_percent": 20.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.21, "peak": 14.73, "min": 13.41}, "VDD_CPU_SOC_MSS": {"avg": 14.48, "peak": 14.96, "min": 13.39}, "VDD_GPU": {"avg": 25.83, "peak": 37.44, "min": 20.89}, "VIN": {"avg": 64.11, "peak": 89.65, "min": 52.9}}, "power_watts_avg": 25.83, "energy_joules_est": 86.07, "sample_count": 26, "duration_seconds": 3.332}, "timestamp": "2026-01-17T14:20:34.536326"}
{"image_index": 5, "image_name": "000000000785.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000785.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3232.592, "latencies_ms": [3232.592], "images_per_second": 0.309, "prompt_tokens": 21, "response_tokens_est": 81, "n_tiles": 6, "output_text": "The image depicts a person skiing down a snowy slope. The individual is dressed in a red jacket, black pants, and a black beanie, with sunglasses and gloves. They are holding ski poles and appear to be in motion, creating a trail of snow behind them. The setting is a snowy mountain slope, with a clear sky and a few orange poles visible in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25802.9, "ram_available_mb": 99969.3, "ram_percent": 20.5}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 25788.8, "ram_available_mb": 99983.4, "ram_percent": 20.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.21, "peak": 14.73, "min": 13.31}, "VDD_CPU_SOC_MSS": {"avg": 14.48, "peak": 14.96, "min": 13.39}, "VDD_GPU": {"avg": 25.82, "peak": 37.44, "min": 20.89}, "VIN": {"avg": 64.5, "peak": 94.64, "min": 57.35}}, "power_watts_avg": 25.82, "energy_joules_est": 83.48, "sample_count": 25, "duration_seconds": 3.233}, "timestamp": "2026-01-17T14:20:37.775572"}
{"image_index": 5, "image_name": "000000000785.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000785.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2572.041, "latencies_ms": [2572.041], "images_per_second": 0.389, "prompt_tokens": 19, "response_tokens_est": 60, "n_tiles": 6, "output_text": "The notable visual attributes of the image include a person dressed in a red jacket and black pants, wearing a black beanie and sunglasses, and blue and red ski boots. The scene is set on a snowy slope with a clear sky, and the lighting is bright, indicating it is daytime.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25788.8, "ram_available_mb": 99983.4, "ram_percent": 20.5}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 25783.0, "ram_available_mb": 99989.2, "ram_percent": 20.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.19, "peak": 14.73, "min": 13.41}, "VDD_CPU_SOC_MSS": {"avg": 14.48, "peak": 15.36, "min": 13.39}, "VDD_GPU": {"avg": 27.04, "peak": 37.44, "min": 21.29}, "VIN": {"avg": 68.3, "peak": 109.73, "min": 59.62}}, "power_watts_avg": 27.04, "energy_joules_est": 69.56, "sample_count": 20, "duration_seconds": 2.572}, "timestamp": "2026-01-17T14:20:40.353798"}
{"image_index": 6, "image_name": "000000000802.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000802.jpg", "image_width": 424, "image_height": 640, "image_resolution": "424x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1498.787, "latencies_ms": [1498.787], "images_per_second": 0.667, "prompt_tokens": 9, "response_tokens_est": 27, "n_tiles": 6, "output_text": "The image shows a kitchen with a white refrigerator, wooden cabinets, and a white stove, all set against a beige wall.", "error": null, "sys_before": {"cpu_percent": 42.7, "ram_used_mb": 25778.0, "ram_available_mb": 99994.2, "ram_percent": 20.5}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25774.4, "ram_available_mb": 99997.8, "ram_percent": 20.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.8, "peak": 14.2, "min": 12.88}, "VDD_CPU_SOC_MSS": {"avg": 14.29, "peak": 14.57, "min": 13.39}, "VDD_GPU": {"avg": 30.77, "peak": 38.23, "min": 24.04}, "VIN": {"avg": 73.89, "peak": 113.77, "min": 60.62}}, "power_watts_avg": 30.77, "energy_joules_est": 46.13, "sample_count": 11, "duration_seconds": 1.499}, "timestamp": "2026-01-17T14:20:41.913073"}
{"image_index": 6, "image_name": "000000000802.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000802.jpg", "image_width": 424, "image_height": 640, "image_resolution": "424x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1951.49, "latencies_ms": [1951.49], "images_per_second": 0.512, "prompt_tokens": 23, "response_tokens_est": 40, "n_tiles": 6, "output_text": "object: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25774.4, "ram_available_mb": 99997.8, "ram_percent": 20.5}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25767.0, "ram_available_mb": 100005.1, "ram_percent": 20.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.16, "peak": 14.73, "min": 13.41}, "VDD_CPU_SOC_MSS": {"avg": 14.42, "peak": 14.96, "min": 13.39}, "VDD_GPU": {"avg": 29.37, "peak": 37.83, "min": 22.07}, "VIN": {"avg": 70.54, "peak": 92.91, "min": 56.63}}, "power_watts_avg": 29.37, "energy_joules_est": 57.32, "sample_count": 15, "duration_seconds": 1.952}, "timestamp": "2026-01-17T14:20:43.869950"}
{"image_index": 6, "image_name": "000000000802.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000802.jpg", "image_width": 424, "image_height": 640, "image_resolution": "424x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2403.629, "latencies_ms": [2403.629], "images_per_second": 0.416, "prompt_tokens": 27, "response_tokens_est": 52, "n_tiles": 6, "output_text": "The main objects in the image are a white refrigerator and a white stove. The refrigerator is located to the right of the stove, and the stove is positioned in the foreground. The refrigerator is closer to the foreground, while the stove is in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25767.0, "ram_available_mb": 100005.1, "ram_percent": 20.5}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25759.1, "ram_available_mb": 100013.1, "ram_percent": 20.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.97, "peak": 14.63, "min": 13.41}, "VDD_CPU_SOC_MSS": {"avg": 14.2, "peak": 14.97, "min": 13.79}, "VDD_GPU": {"avg": 27.57, "peak": 37.83, "min": 20.89}, "VIN": {"avg": 66.59, "peak": 108.59, "min": 53.08}}, "power_watts_avg": 27.57, "energy_joules_est": 66.29, "sample_count": 18, "duration_seconds": 2.404}, "timestamp": "2026-01-17T14:20:46.285143"}
{"image_index": 6, "image_name": "000000000802.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000802.jpg", "image_width": 424, "image_height": 640, "image_resolution": "424x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2079.786, "latencies_ms": [2079.786], "images_per_second": 0.481, "prompt_tokens": 21, "response_tokens_est": 42, "n_tiles": 6, "output_text": "The image depicts a small kitchen with a white refrigerator and wooden cabinets. The room is well-lit, and the tiled floor is visible. There are no people or animals present in the scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25759.1, "ram_available_mb": 100013.1, "ram_percent": 20.5}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25755.1, "ram_available_mb": 100017.1, "ram_percent": 20.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.92, "peak": 14.53, "min": 13.21}, "VDD_CPU_SOC_MSS": {"avg": 14.11, "peak": 14.96, "min": 13.39}, "VDD_GPU": {"avg": 28.13, "peak": 37.44, "min": 21.29}, "VIN": {"avg": 65.92, "peak": 101.19, "min": 52.16}}, "power_watts_avg": 28.13, "energy_joules_est": 58.51, "sample_count": 16, "duration_seconds": 2.08}, "timestamp": "2026-01-17T14:20:48.371241"}
{"image_index": 6, "image_name": "000000000802.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000802.jpg", "image_width": 424, "image_height": 640, "image_resolution": "424x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1979.9, "latencies_ms": [1979.9], "images_per_second": 0.505, "prompt_tokens": 19, "response_tokens_est": 39, "n_tiles": 6, "output_text": "The kitchen features a warm, beige color scheme with wooden cabinets and a white refrigerator. The lighting is soft and natural, likely from a nearby window, creating a cozy and inviting atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25755.1, "ram_available_mb": 100017.1, "ram_percent": 20.5}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 25754.7, "ram_available_mb": 100017.4, "ram_percent": 20.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.93, "peak": 14.53, "min": 13.31}, "VDD_CPU_SOC_MSS": {"avg": 14.07, "peak": 14.96, "min": 13.39}, "VDD_GPU": {"avg": 28.88, "peak": 37.83, "min": 21.67}, "VIN": {"avg": 65.6, "peak": 83.36, "min": 57.16}}, "power_watts_avg": 28.88, "energy_joules_est": 57.2, "sample_count": 15, "duration_seconds": 1.981}, "timestamp": "2026-01-17T14:20:50.358358"}
{"image_index": 7, "image_name": "000000000872.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000872.jpg", "image_width": 621, "image_height": 640, "image_resolution": "621x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1122.405, "latencies_ms": [1122.405], "images_per_second": 0.891, "prompt_tokens": 9, "response_tokens_est": 40, "n_tiles": 1, "output_text": "Two baseball players are in the midst of a game, with one player in a white uniform and the other in a green uniform, both wearing caps and gloves, as they sprint towards the base.", "error": null, "sys_before": {"cpu_percent": 17.1, "ram_used_mb": 25754.7, "ram_available_mb": 100017.4, "ram_percent": 20.5}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25754.7, "ram_available_mb": 100017.4, "ram_percent": 20.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 4845.7, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.19, "peak": 14.61, "min": 13.59}, "VDD_CPU_SOC_MSS": {"avg": 14.48, "peak": 14.97, "min": 13.79}, "VDD_GPU": {"avg": 20.4, "peak": 23.65, "min": 18.53}, "VIN": {"avg": 57.74, "peak": 62.81, "min": 55.2}}, "power_watts_avg": 20.4, "energy_joules_est": 22.91, "sample_count": 8, "duration_seconds": 1.123}, "timestamp": "2026-01-17T14:20:51.510661"}
{"image_index": 7, "image_name": "000000000872.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000872.jpg", "image_width": 621, "image_height": 640, "image_resolution": "621x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 3417.482, "latencies_ms": [3417.482], "images_per_second": 0.293, "prompt_tokens": 23, "response_tokens_est": 129, "n_tiles": 1, "output_text": "- baseball player: 2\n- baseball: 1\n- baseball glove: 1\n- baseball uniform: 1\n- baseball cap: 1\n- baseball bat: 1\n- baseball field: 1\n- baseball glove: 1\n- baseball: 1\n- baseball player: 1\n- baseball player: 1\n- baseball player: 1\n- baseball player: 1\n- baseball player: 1\n- baseball player: 1\n- baseball player: 1\n- baseball player: 1\n- baseball player: 1\n- baseball player:", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25754.7, "ram_available_mb": 100017.4, "ram_percent": 20.5}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25755.0, "ram_available_mb": 100017.2, "ram_percent": 20.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 4856.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.56, "peak": 14.61, "min": 14.2}, "VDD_CPU_SOC_MSS": {"avg": 14.91, "peak": 14.97, "min": 14.57}, "VDD_GPU": {"avg": 18.46, "peak": 22.08, "min": 17.74}, "VIN": {"avg": 56.69, "peak": 60.4, "min": 47.0}}, "power_watts_avg": 18.46, "energy_joules_est": 63.09, "sample_count": 27, "duration_seconds": 3.418}, "timestamp": "2026-01-17T14:20:54.934153"}
{"image_index": 7, "image_name": "000000000872.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000872.jpg", "image_width": 621, "image_height": 640, "image_resolution": "621x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2523.074, "latencies_ms": [2523.074], "images_per_second": 0.396, "prompt_tokens": 27, "response_tokens_est": 94, "n_tiles": 1, "output_text": "In the image, the main object, a baseball player in the foreground, is positioned near the center of the frame. The player is wearing a white shirt, gray pants, and a blue helmet. In the background, another player is seen running towards the left side of the frame. The second player is wearing a green shirt and a green cap. The player in the background is slightly farther away from the main subject, creating a sense of depth in the scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25755.0, "ram_available_mb": 100017.2, "ram_percent": 20.5}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25754.5, "ram_available_mb": 100017.7, "ram_percent": 20.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 4859.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.52, "peak": 14.61, "min": 14.1}, "VDD_CPU_SOC_MSS": {"avg": 14.87, "peak": 14.97, "min": 14.57}, "VDD_GPU": {"avg": 18.63, "peak": 22.08, "min": 17.74}, "VIN": {"avg": 57.5, "peak": 69.19, "min": 48.87}}, "power_watts_avg": 18.63, "energy_joules_est": 47.02, "sample_count": 19, "duration_seconds": 2.524}, "timestamp": "2026-01-17T14:20:57.467643"}
{"image_index": 7, "image_name": "000000000872.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000872.jpg", "image_width": 621, "image_height": 640, "image_resolution": "621x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2331.648, "latencies_ms": [2331.648], "images_per_second": 0.429, "prompt_tokens": 21, "response_tokens_est": 87, "n_tiles": 1, "output_text": "The image captures a dynamic moment during a baseball game, with two players in mid-action on the field. One player, wearing a white shirt and blue helmet, is running towards first base while the other, dressed in a green shirt and blue cap, is running towards second base. The background features lush green trees and a well-maintained grassy area, indicating that the game is taking place in a park or recreational field.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25754.5, "ram_available_mb": 100017.7, "ram_percent": 20.5}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25754.3, "ram_available_mb": 100017.9, "ram_percent": 20.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 4854.7, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.53, "peak": 14.71, "min": 14.1}, "VDD_CPU_SOC_MSS": {"avg": 14.88, "peak": 14.97, "min": 14.57}, "VDD_GPU": {"avg": 18.64, "peak": 22.08, "min": 17.74}, "VIN": {"avg": 58.16, "peak": 65.01, "min": 52.78}}, "power_watts_avg": 18.64, "energy_joules_est": 43.47, "sample_count": 18, "duration_seconds": 2.332}, "timestamp": "2026-01-17T14:20:59.805526"}
{"image_index": 7, "image_name": "000000000872.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000872.jpg", "image_width": 621, "image_height": 640, "image_resolution": "621x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1069.364, "latencies_ms": [1069.364], "images_per_second": 0.935, "prompt_tokens": 19, "response_tokens_est": 38, "n_tiles": 1, "output_text": "The image depicts a baseball game with players in white and gray uniforms, running on a dirt field. The scene is brightly lit by natural sunlight, casting shadows on the ground.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25754.3, "ram_available_mb": 100017.9, "ram_percent": 20.5}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 25751.7, "ram_available_mb": 100020.5, "ram_percent": 20.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 4853.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.43, "peak": 14.61, "min": 14.1}, "VDD_CPU_SOC_MSS": {"avg": 14.82, "peak": 14.97, "min": 14.57}, "VDD_GPU": {"avg": 19.66, "peak": 22.08, "min": 18.14}, "VIN": {"avg": 59.66, "peak": 68.72, "min": 52.19}}, "power_watts_avg": 19.66, "energy_joules_est": 21.03, "sample_count": 8, "duration_seconds": 1.07}, "timestamp": "2026-01-17T14:21:00.881067"}
{"image_index": 8, "image_name": "000000000885.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000885.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1586.51, "latencies_ms": [1586.51], "images_per_second": 0.63, "prompt_tokens": 9, "response_tokens_est": 30, "n_tiles": 6, "output_text": "A man in a white shirt and blue shorts is in the midst of a tennis match, bending over to hit the ball with his racket.", "error": null, "sys_before": {"cpu_percent": 43.3, "ram_used_mb": 25751.9, "ram_available_mb": 100020.2, "ram_percent": 20.5}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25751.8, "ram_available_mb": 100020.4, "ram_percent": 20.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.99, "peak": 14.3, "min": 13.29}, "VDD_CPU_SOC_MSS": {"avg": 14.44, "peak": 14.96, "min": 13.79}, "VDD_GPU": {"avg": 29.59, "peak": 37.83, "min": 23.26}, "VIN": {"avg": 68.71, "peak": 96.98, "min": 59.48}}, "power_watts_avg": 29.59, "energy_joules_est": 46.95, "sample_count": 12, "duration_seconds": 1.587}, "timestamp": "2026-01-17T14:21:02.512269"}
{"image_index": 8, "image_name": "000000000885.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000885.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2144.745, "latencies_ms": [2144.745], "images_per_second": 0.466, "prompt_tokens": 23, "response_tokens_est": 44, "n_tiles": 6, "output_text": "1. Tennis player\n2. Tennis racket\n3. Tennis ball\n4. Tennis court\n5. Spectator\n6. Advertisement board\n7. Advertisement board\n8. Advertisement board", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25751.8, "ram_available_mb": 100020.4, "ram_percent": 20.5}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25751.8, "ram_available_mb": 100020.4, "ram_percent": 20.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.98, "peak": 14.53, "min": 13.41}, "VDD_CPU_SOC_MSS": {"avg": 14.2, "peak": 14.96, "min": 13.39}, "VDD_GPU": {"avg": 28.28, "peak": 37.83, "min": 21.29}, "VIN": {"avg": 67.22, "peak": 98.36, "min": 57.38}}, "power_watts_avg": 28.28, "energy_joules_est": 60.66, "sample_count": 17, "duration_seconds": 2.145}, "timestamp": "2026-01-17T14:21:04.664627"}
{"image_index": 8, "image_name": "000000000885.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000885.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3762.985, "latencies_ms": [3762.985], "images_per_second": 0.266, "prompt_tokens": 27, "response_tokens_est": 93, "n_tiles": 6, "output_text": "The main object in the foreground is a tennis player in a white and blue outfit, who is in the process of hitting a tennis ball. The player is positioned on the left side of the image, with the tennis ball in the middle. The background features a blue barrier with the word \"POLO\" written on it, and a person standing near the barrier. The player's shadow is cast on the tennis court, indicating the direction of the light source.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25751.8, "ram_available_mb": 100020.4, "ram_percent": 20.5}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 25751.5, "ram_available_mb": 100020.7, "ram_percent": 20.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.98, "peak": 14.53, "min": 13.31}, "VDD_CPU_SOC_MSS": {"avg": 14.14, "peak": 14.96, "min": 13.39}, "VDD_GPU": {"avg": 24.85, "peak": 37.44, "min": 20.49}, "VIN": {"avg": 64.29, "peak": 92.92, "min": 49.3}}, "power_watts_avg": 24.85, "energy_joules_est": 93.52, "sample_count": 29, "duration_seconds": 3.763}, "timestamp": "2026-01-17T14:21:08.434322"}
{"image_index": 8, "image_name": "000000000885.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000885.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3593.101, "latencies_ms": [3593.101], "images_per_second": 0.278, "prompt_tokens": 21, "response_tokens_est": 88, "n_tiles": 6, "output_text": "The image captures a moment during a tennis match on a well-maintained court. A player, dressed in a white shirt and blue shorts, is in the midst of a powerful serve, with his body leaning forward and his racket extended towards the ball. The background features a blue barrier with the \"J.P. Morgan\" logo, indicating sponsorship, and a few spectators are seated on the sidelines, observing the match.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25751.5, "ram_available_mb": 100020.7, "ram_percent": 20.5}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 25751.5, "ram_available_mb": 100020.7, "ram_percent": 20.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.98, "peak": 14.53, "min": 13.29}, "VDD_CPU_SOC_MSS": {"avg": 14.13, "peak": 14.97, "min": 13.39}, "VDD_GPU": {"avg": 24.88, "peak": 37.04, "min": 20.49}, "VIN": {"avg": 63.98, "peak": 100.63, "min": 52.52}}, "power_watts_avg": 24.88, "energy_joules_est": 89.42, "sample_count": 28, "duration_seconds": 3.594}, "timestamp": "2026-01-17T14:21:12.034741"}
{"image_index": 8, "image_name": "000000000885.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000885.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2767.801, "latencies_ms": [2767.801], "images_per_second": 0.361, "prompt_tokens": 19, "response_tokens_est": 63, "n_tiles": 6, "output_text": "The image captures a scene on a tennis court with a blue and green surface. The lighting is bright, indicating a sunny day, and the shadows are clearly visible, suggesting the sun is high in the sky. The court is marked with white lines, and there are advertisements on the blue wall behind the players.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25751.5, "ram_available_mb": 100020.7, "ram_percent": 20.5}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 25751.5, "ram_available_mb": 100020.7, "ram_percent": 20.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.94, "peak": 14.53, "min": 13.29}, "VDD_CPU_SOC_MSS": {"avg": 14.11, "peak": 14.96, "min": 13.39}, "VDD_GPU": {"avg": 26.42, "peak": 37.44, "min": 20.89}, "VIN": {"avg": 66.44, "peak": 105.4, "min": 52.89}}, "power_watts_avg": 26.42, "energy_joules_est": 73.14, "sample_count": 21, "duration_seconds": 2.768}, "timestamp": "2026-01-17T14:21:14.809985"}
{"image_index": 9, "image_name": "000000001000.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001000.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2632.503, "latencies_ms": [2632.503], "images_per_second": 0.38, "prompt_tokens": 9, "response_tokens_est": 22, "n_tiles": 12, "output_text": "The image shows a group of young tennis players and their coach posing for a photo on a tennis court.", "error": null, "sys_before": {"cpu_percent": 42.1, "ram_used_mb": 25708.0, "ram_available_mb": 100064.2, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 2.3, "ram_used_mb": 25708.7, "ram_available_mb": 100063.4, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.27, "peak": 15.85, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 14.73, "peak": 16.54, "min": 13.0}, "VDD_GPU": {"avg": 33.38, "peak": 40.2, "min": 25.62}, "VIN": {"avg": 70.39, "peak": 99.78, "min": 50.03}}, "power_watts_avg": 33.38, "energy_joules_est": 87.89, "sample_count": 20, "duration_seconds": 2.633}, "timestamp": "2026-01-17T14:21:17.543211"}
{"image_index": 9, "image_name": "000000001000.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001000.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 3512.742, "latencies_ms": [3512.742], "images_per_second": 0.285, "prompt_tokens": 23, "response_tokens_est": 44, "n_tiles": 12, "output_text": "1. Tennis players\n2. Tennis rackets\n3. Tennis balls\n4. Tennis net\n5. Tennis court\n6. Tennis court net\n7. Tennis court net\n8. Tennis court net", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25708.7, "ram_available_mb": 100063.4, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 25709.0, "ram_available_mb": 100063.2, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.41, "peak": 15.75, "min": 13.29}, "VDD_CPU_SOC_MSS": {"avg": 14.75, "peak": 16.14, "min": 13.39}, "VDD_GPU": {"avg": 32.3, "peak": 41.38, "min": 24.04}, "VIN": {"avg": 71.71, "peak": 98.91, "min": 59.56}}, "power_watts_avg": 32.3, "energy_joules_est": 113.49, "sample_count": 27, "duration_seconds": 3.514}, "timestamp": "2026-01-17T14:21:21.063199"}
{"image_index": 9, "image_name": "000000001000.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001000.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 4010.666, "latencies_ms": [4010.666], "images_per_second": 0.249, "prompt_tokens": 27, "response_tokens_est": 57, "n_tiles": 12, "output_text": "The main objects in the image are a group of young boys and girls standing on a tennis court. The boys are positioned in the foreground, with the girls slightly behind them. The tennis court is surrounded by a green fence, and the background features trees and a clear blue sky.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 25709.0, "ram_available_mb": 100063.2, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 25709.0, "ram_available_mb": 100063.2, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.38, "peak": 15.75, "min": 13.29}, "VDD_CPU_SOC_MSS": {"avg": 14.61, "peak": 16.14, "min": 13.39}, "VDD_GPU": {"avg": 31.03, "peak": 40.98, "min": 23.64}, "VIN": {"avg": 70.67, "peak": 110.78, "min": 50.31}}, "power_watts_avg": 31.03, "energy_joules_est": 124.47, "sample_count": 31, "duration_seconds": 4.011}, "timestamp": "2026-01-17T14:21:25.080649"}
{"image_index": 9, "image_name": "000000001000.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001000.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 4423.298, "latencies_ms": [4423.298], "images_per_second": 0.226, "prompt_tokens": 21, "response_tokens_est": 68, "n_tiles": 12, "output_text": "The scene depicts a group of young tennis players and their coach standing on a tennis court, likely after a match or practice session. The players are dressed in athletic attire, with some holding tennis rackets, and the coach is holding a trophy. The setting is outdoors, with trees and a green fence visible in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25709.0, "ram_available_mb": 100063.2, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 25709.0, "ram_available_mb": 100063.2, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.4, "peak": 15.85, "min": 13.29}, "VDD_CPU_SOC_MSS": {"avg": 14.54, "peak": 16.14, "min": 13.39}, "VDD_GPU": {"avg": 30.27, "peak": 40.97, "min": 23.64}, "VIN": {"avg": 70.77, "peak": 107.94, "min": 55.43}}, "power_watts_avg": 30.27, "energy_joules_est": 133.91, "sample_count": 35, "duration_seconds": 4.424}, "timestamp": "2026-01-17T14:21:29.511368"}
{"image_index": 9, "image_name": "000000001000.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001000.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 4774.266, "latencies_ms": [4774.266], "images_per_second": 0.209, "prompt_tokens": 19, "response_tokens_est": 77, "n_tiles": 12, "output_text": "The image depicts a group of young tennis players and a woman standing on a tennis court. The court is surrounded by a green fence, and the players are dressed in athletic attire, including white shirts, red caps, and various sports shoes. The lighting is bright, indicating a sunny day, and the shadows cast by the trees on the court add depth to the scene.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25709.0, "ram_available_mb": 100063.2, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25709.5, "ram_available_mb": 100062.7, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.36, "peak": 15.75, "min": 13.29}, "VDD_CPU_SOC_MSS": {"avg": 14.55, "peak": 16.14, "min": 13.39}, "VDD_GPU": {"avg": 29.84, "peak": 40.98, "min": 23.64}, "VIN": {"avg": 69.18, "peak": 93.8, "min": 54.62}}, "power_watts_avg": 29.84, "energy_joules_est": 142.48, "sample_count": 37, "duration_seconds": 4.775}, "timestamp": "2026-01-17T14:21:34.292768"}
{"image_index": 10, "image_name": "000000001268.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001268.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1255.735, "latencies_ms": [1255.735], "images_per_second": 0.796, "prompt_tokens": 9, "response_tokens_est": 19, "n_tiles": 6, "output_text": "A woman is taking a photo of a white bird standing in the water near a bridge.", "error": null, "sys_before": {"cpu_percent": 15.8, "ram_used_mb": 25740.3, "ram_available_mb": 100031.9, "ram_percent": 20.5}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 25740.3, "ram_available_mb": 100031.9, "ram_percent": 20.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.79, "peak": 14.2, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 14.13, "peak": 14.57, "min": 13.0}, "VDD_GPU": {"avg": 32.58, "peak": 38.62, "min": 26.41}, "VIN": {"avg": 71.11, "peak": 83.5, "min": 59.17}}, "power_watts_avg": 32.58, "energy_joules_est": 40.93, "sample_count": 9, "duration_seconds": 1.256}, "timestamp": "2026-01-17T14:21:35.596581"}
{"image_index": 10, "image_name": "000000001268.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001268.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2537.141, "latencies_ms": [2537.141], "images_per_second": 0.394, "prompt_tokens": 23, "response_tokens_est": 56, "n_tiles": 6, "output_text": "1. Woman: 1\n2. Woman: 1\n3. Woman: 1\n4. Woman: 1\n5. Woman: 1\n6. Woman: 1\n7. Woman: 1\n8. Woman: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25740.3, "ram_available_mb": 100031.9, "ram_percent": 20.5}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 25740.3, "ram_available_mb": 100031.9, "ram_percent": 20.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.04, "peak": 14.63, "min": 13.41}, "VDD_CPU_SOC_MSS": {"avg": 14.18, "peak": 14.96, "min": 13.39}, "VDD_GPU": {"avg": 27.59, "peak": 38.23, "min": 20.89}, "VIN": {"avg": 66.1, "peak": 99.61, "min": 55.81}}, "power_watts_avg": 27.59, "energy_joules_est": 70.01, "sample_count": 20, "duration_seconds": 2.538}, "timestamp": "2026-01-17T14:21:38.142344"}
{"image_index": 10, "image_name": "000000001268.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001268.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2149.229, "latencies_ms": [2149.229], "images_per_second": 0.465, "prompt_tokens": 27, "response_tokens_est": 44, "n_tiles": 6, "output_text": "The main objects in the image are a group of people sitting near a body of water, with a person taking a photo in the foreground. The water is in the background, and the people are in the foreground.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25740.3, "ram_available_mb": 100031.9, "ram_percent": 20.5}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 25741.3, "ram_available_mb": 100030.9, "ram_percent": 20.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.98, "peak": 14.53, "min": 13.29}, "VDD_CPU_SOC_MSS": {"avg": 14.08, "peak": 14.96, "min": 13.39}, "VDD_GPU": {"avg": 28.38, "peak": 37.44, "min": 21.29}, "VIN": {"avg": 68.42, "peak": 97.85, "min": 56.27}}, "power_watts_avg": 28.38, "energy_joules_est": 61.0, "sample_count": 16, "duration_seconds": 2.15}, "timestamp": "2026-01-17T14:21:40.302089"}
{"image_index": 10, "image_name": "000000001268.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001268.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3457.079, "latencies_ms": [3457.079], "images_per_second": 0.289, "prompt_tokens": 21, "response_tokens_est": 84, "n_tiles": 6, "output_text": "The image depicts a serene riverside scene during twilight, with a group of people sitting on a stone bench by the water's edge. A woman in a red shirt is taking a photo of the swan, while others are engaged in conversation or simply enjoying the view. The setting is a picturesque riverside promenade, with a bridge overhead and a calm body of water in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25741.3, "ram_available_mb": 100030.9, "ram_percent": 20.5}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 25741.3, "ram_available_mb": 100030.9, "ram_percent": 20.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.03, "peak": 14.53, "min": 13.29}, "VDD_CPU_SOC_MSS": {"avg": 14.14, "peak": 14.96, "min": 13.39}, "VDD_GPU": {"avg": 25.29, "peak": 37.83, "min": 20.5}, "VIN": {"avg": 63.64, "peak": 81.3, "min": 52.44}}, "power_watts_avg": 25.29, "energy_joules_est": 87.44, "sample_count": 27, "duration_seconds": 3.458}, "timestamp": "2026-01-17T14:21:43.767466"}
{"image_index": 10, "image_name": "000000001268.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001268.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2997.57, "latencies_ms": [2997.57], "images_per_second": 0.334, "prompt_tokens": 19, "response_tokens_est": 70, "n_tiles": 6, "output_text": "The image features a serene scene with a woman taking a photograph of a white bird standing in a calm body of water. The lighting is soft and warm, with the sun casting a gentle glow on the scene. The woman is wearing a green dress and a brown shoulder bag, while the bird is white and stands on the water's edge.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25741.3, "ram_available_mb": 100030.9, "ram_percent": 20.5}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 25741.2, "ram_available_mb": 100030.9, "ram_percent": 20.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.02, "peak": 14.53, "min": 13.29}, "VDD_CPU_SOC_MSS": {"avg": 14.14, "peak": 14.96, "min": 13.39}, "VDD_GPU": {"avg": 25.88, "peak": 37.44, "min": 20.89}, "VIN": {"avg": 65.45, "peak": 91.21, "min": 56.97}}, "power_watts_avg": 25.88, "energy_joules_est": 77.59, "sample_count": 24, "duration_seconds": 2.998}, "timestamp": "2026-01-17T14:21:46.771808"}
{"image_index": 11, "image_name": "000000001296.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001296.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2212.518, "latencies_ms": [2212.518], "images_per_second": 0.452, "prompt_tokens": 9, "response_tokens_est": 50, "n_tiles": 6, "output_text": "A woman is taking a selfie with a Hello Kitty phone case, wearing a white top with a black and white graphic design, and accessorized with a green and yellow bracelet, a silver watch, and a white paper in her hand.", "error": null, "sys_before": {"cpu_percent": 28.8, "ram_used_mb": 25741.2, "ram_available_mb": 100030.9, "ram_percent": 20.5}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 25741.2, "ram_available_mb": 100030.9, "ram_percent": 20.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.04, "peak": 14.3, "min": 13.09}, "VDD_CPU_SOC_MSS": {"avg": 14.36, "peak": 14.57, "min": 12.99}, "VDD_GPU": {"avg": 27.8, "peak": 38.23, "min": 21.68}, "VIN": {"avg": 68.7, "peak": 115.63, "min": 58.57}}, "power_watts_avg": 27.8, "energy_joules_est": 61.52, "sample_count": 17, "duration_seconds": 2.213}, "timestamp": "2026-01-17T14:21:49.022690"}
{"image_index": 11, "image_name": "000000001296.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001296.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2235.807, "latencies_ms": [2235.807], "images_per_second": 0.447, "prompt_tokens": 23, "response_tokens_est": 49, "n_tiles": 6, "output_text": "1. Phone (1)\n2. Phone (1)\n3. Phone (1)\n4. Phone (1)\n5. Phone (1)\n6. Phone (1)\n7. Phone (1)\n8. Phone (1)", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25741.2, "ram_available_mb": 100030.9, "ram_percent": 20.5}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25741.2, "ram_available_mb": 100030.9, "ram_percent": 20.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.21, "peak": 14.83, "min": 13.51}, "VDD_CPU_SOC_MSS": {"avg": 14.46, "peak": 15.36, "min": 13.39}, "VDD_GPU": {"avg": 28.26, "peak": 37.04, "min": 21.68}, "VIN": {"avg": 70.32, "peak": 101.47, "min": 59.6}}, "power_watts_avg": 28.26, "energy_joules_est": 63.2, "sample_count": 17, "duration_seconds": 2.236}, "timestamp": "2026-01-17T14:21:51.264407"}
{"image_index": 11, "image_name": "000000001296.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001296.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2445.41, "latencies_ms": [2445.41], "images_per_second": 0.409, "prompt_tokens": 27, "response_tokens_est": 56, "n_tiles": 6, "output_text": "The main object in the foreground is a person holding a smartphone with a Hello Kitty sticker. The person's left hand is visible, and they are wearing a white sleeveless top. The background is blurred, but there are hints of other people and possibly a crowd.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25741.2, "ram_available_mb": 100030.9, "ram_percent": 20.5}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 25741.7, "ram_available_mb": 100030.4, "ram_percent": 20.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.25, "peak": 14.63, "min": 13.51}, "VDD_CPU_SOC_MSS": {"avg": 14.49, "peak": 14.96, "min": 13.39}, "VDD_GPU": {"avg": 27.51, "peak": 37.83, "min": 21.28}, "VIN": {"avg": 68.29, "peak": 107.62, "min": 59.03}}, "power_watts_avg": 27.51, "energy_joules_est": 67.29, "sample_count": 19, "duration_seconds": 2.446}, "timestamp": "2026-01-17T14:21:53.716464"}
{"image_index": 11, "image_name": "000000001296.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001296.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2566.644, "latencies_ms": [2566.644], "images_per_second": 0.39, "prompt_tokens": 21, "response_tokens_est": 60, "n_tiles": 6, "output_text": "The image depicts a young woman holding a smartphone, taking a selfie. She is wearing a white top with a graphic design and has a green and yellow bracelet on her wrist. The background is blurred, but it appears to be an outdoor setting with other people and possibly a crowd.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25741.7, "ram_available_mb": 100030.4, "ram_percent": 20.5}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 25742.3, "ram_available_mb": 100029.9, "ram_percent": 20.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.25, "peak": 14.73, "min": 13.51}, "VDD_CPU_SOC_MSS": {"avg": 14.47, "peak": 14.96, "min": 13.39}, "VDD_GPU": {"avg": 27.52, "peak": 37.83, "min": 21.29}, "VIN": {"avg": 67.07, "peak": 100.58, "min": 59.86}}, "power_watts_avg": 27.52, "energy_joules_est": 70.65, "sample_count": 19, "duration_seconds": 2.567}, "timestamp": "2026-01-17T14:21:56.289476"}
{"image_index": 11, "image_name": "000000001296.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001296.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2469.708, "latencies_ms": [2469.708], "images_per_second": 0.405, "prompt_tokens": 19, "response_tokens_est": 57, "n_tiles": 6, "output_text": "The notable visual attributes of the image include the woman's black hair, the white and pink Hello Kitty phone case, her green and yellow chunky bracelet, and her white sleeveless top. The lighting is bright, and the background is blurred, suggesting a sunny day.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25742.3, "ram_available_mb": 100029.9, "ram_percent": 20.5}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 25741.8, "ram_available_mb": 100030.4, "ram_percent": 20.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.26, "peak": 14.83, "min": 13.51}, "VDD_CPU_SOC_MSS": {"avg": 14.47, "peak": 15.36, "min": 13.39}, "VDD_GPU": {"avg": 27.52, "peak": 37.44, "min": 21.28}, "VIN": {"avg": 69.97, "peak": 106.06, "min": 53.7}}, "power_watts_avg": 27.52, "energy_joules_est": 67.97, "sample_count": 19, "duration_seconds": 2.47}, "timestamp": "2026-01-17T14:21:58.765534"}
{"image_index": 12, "image_name": "000000001353.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001353.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2625.183, "latencies_ms": [2625.183], "images_per_second": 0.381, "prompt_tokens": 9, "response_tokens_est": 22, "n_tiles": 12, "output_text": "A group of children are sitting in a red and white train carriage, enjoying a ride on a train.", "error": null, "sys_before": {"cpu_percent": 46.6, "ram_used_mb": 25712.7, "ram_available_mb": 100059.5, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 2.3, "ram_used_mb": 25712.8, "ram_available_mb": 100059.3, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.27, "peak": 15.65, "min": 12.78}, "VDD_CPU_SOC_MSS": {"avg": 14.69, "peak": 16.14, "min": 13.39}, "VDD_GPU": {"avg": 33.56, "peak": 41.38, "min": 25.23}, "VIN": {"avg": 71.86, "peak": 101.09, "min": 56.36}}, "power_watts_avg": 33.56, "energy_joules_est": 88.11, "sample_count": 20, "duration_seconds": 2.626}, "timestamp": "2026-01-17T14:22:01.478908"}
{"image_index": 12, "image_name": "000000001353.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001353.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 3369.806, "latencies_ms": [3369.806], "images_per_second": 0.297, "prompt_tokens": 23, "response_tokens_est": 40, "n_tiles": 12, "output_text": "object: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25712.8, "ram_available_mb": 100059.3, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 25713.1, "ram_available_mb": 100059.1, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.17, "peak": 15.57, "min": 13.09}, "VDD_CPU_SOC_MSS": {"avg": 14.41, "peak": 16.14, "min": 13.39}, "VDD_GPU": {"avg": 32.48, "peak": 41.77, "min": 23.65}, "VIN": {"avg": 67.81, "peak": 87.83, "min": 59.64}}, "power_watts_avg": 32.48, "energy_joules_est": 109.47, "sample_count": 26, "duration_seconds": 3.37}, "timestamp": "2026-01-17T14:22:04.856199"}
{"image_index": 12, "image_name": "000000001353.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001353.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 4586.33, "latencies_ms": [4586.33], "images_per_second": 0.218, "prompt_tokens": 27, "response_tokens_est": 71, "n_tiles": 12, "output_text": "The main object in the foreground is a red and white toy train. The train is positioned on a track, with a black base and a small wheel visible. In the background, there is a group of children sitting on a bench, with one child standing and another sitting on a chair. The children are in a room with wooden floors and walls.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25713.1, "ram_available_mb": 100059.1, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 25713.0, "ram_available_mb": 100059.1, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.1, "peak": 15.54, "min": 13.09}, "VDD_CPU_SOC_MSS": {"avg": 14.31, "peak": 16.14, "min": 13.39}, "VDD_GPU": {"avg": 29.92, "peak": 41.36, "min": 23.25}, "VIN": {"avg": 66.52, "peak": 85.12, "min": 59.35}}, "power_watts_avg": 29.92, "energy_joules_est": 137.23, "sample_count": 35, "duration_seconds": 4.587}, "timestamp": "2026-01-17T14:22:09.454334"}
{"image_index": 12, "image_name": "000000001353.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001353.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 4582.516, "latencies_ms": [4582.516], "images_per_second": 0.218, "prompt_tokens": 21, "response_tokens_est": 71, "n_tiles": 12, "output_text": "The image depicts a group of children sitting in a small, red and white train carriage, which is situated on a wooden floor. The children appear to be engaged in a lively conversation, with some looking towards the camera and others looking away. The setting suggests a playful and interactive environment, possibly at a children's play area or a themed attraction.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25713.0, "ram_available_mb": 100059.1, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25713.0, "ram_available_mb": 100059.2, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.11, "peak": 15.54, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 14.23, "peak": 16.14, "min": 13.0}, "VDD_GPU": {"avg": 29.89, "peak": 40.98, "min": 23.25}, "VIN": {"avg": 68.67, "peak": 116.65, "min": 57.82}}, "power_watts_avg": 29.89, "energy_joules_est": 136.99, "sample_count": 36, "duration_seconds": 4.583}, "timestamp": "2026-01-17T14:22:14.043792"}
{"image_index": 12, "image_name": "000000001353.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001353.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3407.644, "latencies_ms": [3407.644], "images_per_second": 0.293, "prompt_tokens": 19, "response_tokens_est": 41, "n_tiles": 12, "output_text": "The image features a vibrant red and white carousel with a shiny, glossy finish. The carousel is situated indoors, illuminated by warm, ambient lighting that casts a soft glow on the wooden floor.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25713.0, "ram_available_mb": 100059.2, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 25713.8, "ram_available_mb": 100058.4, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.17, "peak": 15.54, "min": 13.09}, "VDD_CPU_SOC_MSS": {"avg": 14.47, "peak": 16.14, "min": 13.39}, "VDD_GPU": {"avg": 32.42, "peak": 40.98, "min": 23.64}, "VIN": {"avg": 67.46, "peak": 86.11, "min": 52.6}}, "power_watts_avg": 32.42, "energy_joules_est": 110.49, "sample_count": 26, "duration_seconds": 3.408}, "timestamp": "2026-01-17T14:22:17.462251"}
{"image_index": 13, "image_name": "000000001425.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001425.jpg", "image_width": 640, "image_height": 512, "image_resolution": "640x512", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 3227.294, "latencies_ms": [3227.294], "images_per_second": 0.31, "prompt_tokens": 9, "response_tokens_est": 38, "n_tiles": 12, "output_text": "The image shows a close-up of a slice of bread with a dark filling, possibly a dessert or a savory dish, placed on a white plate, with a blurred background.", "error": null, "sys_before": {"cpu_percent": 42.6, "ram_used_mb": 25696.8, "ram_available_mb": 100075.4, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 25697.5, "ram_available_mb": 100074.7, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.35, "peak": 15.95, "min": 12.38}, "VDD_CPU_SOC_MSS": {"avg": 14.73, "peak": 16.54, "min": 13.0}, "VDD_GPU": {"avg": 32.01, "peak": 40.98, "min": 24.04}, "VIN": {"avg": 75.19, "peak": 110.54, "min": 57.3}}, "power_watts_avg": 32.01, "energy_joules_est": 103.32, "sample_count": 25, "duration_seconds": 3.228}, "timestamp": "2026-01-17T14:22:20.790307"}
{"image_index": 13, "image_name": "000000001425.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001425.jpg", "image_width": 640, "image_height": 512, "image_resolution": "640x512", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 3364.744, "latencies_ms": [3364.744], "images_per_second": 0.297, "prompt_tokens": 23, "response_tokens_est": 40, "n_tiles": 12, "output_text": "object: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25697.5, "ram_available_mb": 100074.7, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 25697.9, "ram_available_mb": 100074.3, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.45, "peak": 15.75, "min": 13.29}, "VDD_CPU_SOC_MSS": {"avg": 14.71, "peak": 16.14, "min": 13.39}, "VDD_GPU": {"avg": 32.42, "peak": 40.98, "min": 24.04}, "VIN": {"avg": 71.85, "peak": 91.16, "min": 55.8}}, "power_watts_avg": 32.42, "energy_joules_est": 109.1, "sample_count": 26, "duration_seconds": 3.365}, "timestamp": "2026-01-17T14:22:24.161854"}
{"image_index": 13, "image_name": "000000001425.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001425.jpg", "image_width": 640, "image_height": 512, "image_resolution": "640x512", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 4241.684, "latencies_ms": [4241.684], "images_per_second": 0.236, "prompt_tokens": 27, "response_tokens_est": 63, "n_tiles": 12, "output_text": "The main object in the foreground is a sandwich, which is placed on a white plate. The background features a blurred object, possibly a cup or container, which is out of focus. The sandwich is positioned near the center of the image, with the blurred object slightly to the right and behind it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25697.9, "ram_available_mb": 100074.3, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 25696.8, "ram_available_mb": 100075.3, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.43, "peak": 15.85, "min": 13.29}, "VDD_CPU_SOC_MSS": {"avg": 14.7, "peak": 16.14, "min": 13.39}, "VDD_GPU": {"avg": 30.69, "peak": 40.98, "min": 23.64}, "VIN": {"avg": 71.6, "peak": 124.97, "min": 62.14}}, "power_watts_avg": 30.69, "energy_joules_est": 130.19, "sample_count": 33, "duration_seconds": 4.242}, "timestamp": "2026-01-17T14:22:28.410245"}
{"image_index": 13, "image_name": "000000001425.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001425.jpg", "image_width": 640, "image_height": 512, "image_resolution": "640x512", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3883.426, "latencies_ms": [3883.426], "images_per_second": 0.258, "prompt_tokens": 21, "response_tokens_est": 54, "n_tiles": 12, "output_text": "The image depicts a close-up of a slice of bread with a dark filling, possibly a dessert or a savory dish, placed on a white plate. The background is blurred, with indistinct shapes and colors, suggesting a shallow depth of field.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25696.8, "ram_available_mb": 100075.3, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 25697.3, "ram_available_mb": 100074.8, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.45, "peak": 15.85, "min": 13.39}, "VDD_CPU_SOC_MSS": {"avg": 14.74, "peak": 16.54, "min": 13.39}, "VDD_GPU": {"avg": 31.21, "peak": 41.36, "min": 24.04}, "VIN": {"avg": 69.25, "peak": 95.77, "min": 62.44}}, "power_watts_avg": 31.21, "energy_joules_est": 121.22, "sample_count": 30, "duration_seconds": 3.884}, "timestamp": "2026-01-17T14:22:32.300531"}
{"image_index": 13, "image_name": "000000001425.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001425.jpg", "image_width": 640, "image_height": 512, "image_resolution": "640x512", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3914.109, "latencies_ms": [3914.109], "images_per_second": 0.255, "prompt_tokens": 19, "response_tokens_est": 55, "n_tiles": 12, "output_text": "The image is a black and white photograph featuring a close-up of a sandwich with a white, fluffy bread base. The lighting is soft and diffused, creating a warm and inviting atmosphere. The background is blurred, drawing attention to the sandwich in the foreground.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25697.3, "ram_available_mb": 100074.8, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 25697.6, "ram_available_mb": 100074.6, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.47, "peak": 15.85, "min": 13.51}, "VDD_CPU_SOC_MSS": {"avg": 14.76, "peak": 16.54, "min": 13.39}, "VDD_GPU": {"avg": 31.32, "peak": 41.76, "min": 24.04}, "VIN": {"avg": 75.26, "peak": 117.25, "min": 61.91}}, "power_watts_avg": 31.32, "energy_joules_est": 122.6, "sample_count": 30, "duration_seconds": 3.915}, "timestamp": "2026-01-17T14:22:36.221278"}
{"image_index": 14, "image_name": "000000001490.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001490.jpg", "image_width": 640, "image_height": 315, "image_resolution": "640x315", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 699.526, "latencies_ms": [699.526], "images_per_second": 1.43, "prompt_tokens": 9, "response_tokens_est": 19, "n_tiles": 2, "output_text": "A person is standing on a paddleboard in the water, paddling with a paddle.", "error": null, "sys_before": {"cpu_percent": 35.7, "ram_used_mb": 25699.0, "ram_available_mb": 100073.1, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 25700.5, "ram_available_mb": 100071.7, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5055.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.51, "peak": 14.0, "min": 13.09}, "VDD_CPU_SOC_MSS": {"avg": 13.87, "peak": 14.18, "min": 13.39}, "VDD_GPU": {"avg": 26.33, "peak": 29.16, "min": 22.86}, "VIN": {"avg": 65.37, "peak": 100.19, "min": 54.46}}, "power_watts_avg": 26.33, "energy_joules_est": 18.43, "sample_count": 5, "duration_seconds": 0.7}, "timestamp": "2026-01-17T14:22:36.954051"}
{"image_index": 14, "image_name": "000000001490.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001490.jpg", "image_width": 640, "image_height": 315, "image_resolution": "640x315", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1188.905, "latencies_ms": [1188.905], "images_per_second": 0.841, "prompt_tokens": 23, "response_tokens_est": 37, "n_tiles": 2, "output_text": "1. Paddleboard\n2. Person\n3. Water\n4. Surfboard\n5. Water\n6. Paddle\n7. Person\n8. Water", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25700.5, "ram_available_mb": 100071.7, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 25701.7, "ram_available_mb": 100070.4, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5064.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.03, "peak": 14.4, "min": 13.59}, "VDD_CPU_SOC_MSS": {"avg": 14.35, "peak": 14.57, "min": 13.78}, "VDD_GPU": {"avg": 23.95, "peak": 29.56, "min": 20.1}, "VIN": {"avg": 61.23, "peak": 93.99, "min": 50.24}}, "power_watts_avg": 23.95, "energy_joules_est": 28.49, "sample_count": 9, "duration_seconds": 1.189}, "timestamp": "2026-01-17T14:22:38.149655"}
{"image_index": 14, "image_name": "000000001490.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001490.jpg", "image_width": 640, "image_height": 315, "image_resolution": "640x315", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2427.271, "latencies_ms": [2427.271], "images_per_second": 0.412, "prompt_tokens": 27, "response_tokens_est": 83, "n_tiles": 2, "output_text": "The main object in the foreground is a person standing on a paddleboard, holding a paddle. The person is wearing a wetsuit and is positioned near the center of the image. In the background, there is a body of water extending to the horizon, with a line of trees and a distant shoreline visible. The paddleboard is positioned in the middle ground, slightly to the right of the person.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25701.7, "ram_available_mb": 100070.4, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25702.7, "ram_available_mb": 100069.5, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5067.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.29, "peak": 14.51, "min": 13.69}, "VDD_CPU_SOC_MSS": {"avg": 14.49, "peak": 14.57, "min": 14.18}, "VDD_GPU": {"avg": 21.06, "peak": 28.37, "min": 18.92}, "VIN": {"avg": 60.04, "peak": 85.66, "min": 55.25}}, "power_watts_avg": 21.06, "energy_joules_est": 51.13, "sample_count": 19, "duration_seconds": 2.428}, "timestamp": "2026-01-17T14:22:40.583150"}
{"image_index": 14, "image_name": "000000001490.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001490.jpg", "image_width": 640, "image_height": 315, "image_resolution": "640x315", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1750.497, "latencies_ms": [1750.497], "images_per_second": 0.571, "prompt_tokens": 21, "response_tokens_est": 58, "n_tiles": 2, "output_text": "The image depicts a person standing on a paddleboard in a body of water, likely a lake or sea, with a calm and expansive horizon. The individual is dressed in a wetsuit and is holding a paddle, suggesting they are engaged in water sports or a leisure activity.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25702.7, "ram_available_mb": 100069.5, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 25702.9, "ram_available_mb": 100069.2, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5063.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.22, "peak": 14.51, "min": 13.69}, "VDD_CPU_SOC_MSS": {"avg": 14.45, "peak": 14.57, "min": 14.18}, "VDD_GPU": {"avg": 21.83, "peak": 27.59, "min": 18.92}, "VIN": {"avg": 60.57, "peak": 83.91, "min": 55.66}}, "power_watts_avg": 21.83, "energy_joules_est": 38.23, "sample_count": 13, "duration_seconds": 1.751}, "timestamp": "2026-01-17T14:22:42.344714"}
{"image_index": 14, "image_name": "000000001490.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001490.jpg", "image_width": 640, "image_height": 315, "image_resolution": "640x315", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1878.43, "latencies_ms": [1878.43], "images_per_second": 0.532, "prompt_tokens": 19, "response_tokens_est": 63, "n_tiles": 2, "output_text": "The image is a black and white photograph featuring a person standing on a paddleboard in a body of water. The individual is wearing a wetsuit and holding a paddle, and the water appears calm with minimal ripples. The lighting is natural, suggesting daytime, and the overall weather seems clear and mild.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25702.9, "ram_available_mb": 100069.2, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25703.4, "ram_available_mb": 100068.7, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5062.6, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.27, "peak": 14.51, "min": 13.69}, "VDD_CPU_SOC_MSS": {"avg": 14.66, "peak": 14.97, "min": 14.18}, "VDD_GPU": {"avg": 21.51, "peak": 27.59, "min": 18.92}, "VIN": {"avg": 61.42, "peak": 94.02, "min": 54.91}}, "power_watts_avg": 21.51, "energy_joules_est": 40.41, "sample_count": 14, "duration_seconds": 1.879}, "timestamp": "2026-01-17T14:22:44.229071"}
{"image_index": 15, "image_name": "000000001503.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001503.jpg", "image_width": 320, "image_height": 240, "image_resolution": "320x240", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 3189.822, "latencies_ms": [3189.822], "images_per_second": 0.313, "prompt_tokens": 9, "response_tokens_est": 37, "n_tiles": 12, "output_text": "The image shows a neatly arranged desk with a laptop, a white mouse, a keyboard, and a small figurine on the right side, all set against a plain background.", "error": null, "sys_before": {"cpu_percent": 49.1, "ram_used_mb": 25702.2, "ram_available_mb": 100070.0, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 25703.9, "ram_available_mb": 100068.2, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.43, "peak": 15.95, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 14.88, "peak": 16.54, "min": 13.78}, "VDD_GPU": {"avg": 31.8, "peak": 40.57, "min": 24.43}, "VIN": {"avg": 71.39, "peak": 109.33, "min": 51.82}}, "power_watts_avg": 31.8, "energy_joules_est": 101.45, "sample_count": 24, "duration_seconds": 3.19}, "timestamp": "2026-01-17T14:22:47.502338"}
{"image_index": 15, "image_name": "000000001503.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001503.jpg", "image_width": 320, "image_height": 240, "image_resolution": "320x240", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2767.377, "latencies_ms": [2767.377], "images_per_second": 0.361, "prompt_tokens": 23, "response_tokens_est": 27, "n_tiles": 12, "output_text": "laptop: 1\nkeyboard: 1\nmouse: 1\nprinter: 1\ndesk: 1", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25703.9, "ram_available_mb": 100068.2, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 2.4, "ram_used_mb": 25705.4, "ram_available_mb": 100066.8, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.52, "peak": 15.85, "min": 13.29}, "VDD_CPU_SOC_MSS": {"avg": 15.08, "peak": 16.54, "min": 13.39}, "VDD_GPU": {"avg": 34.65, "peak": 40.97, "min": 26.8}, "VIN": {"avg": 76.98, "peak": 118.36, "min": 63.19}}, "power_watts_avg": 34.65, "energy_joules_est": 95.9, "sample_count": 21, "duration_seconds": 2.768}, "timestamp": "2026-01-17T14:22:50.276308"}
{"image_index": 15, "image_name": "000000001503.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001503.jpg", "image_width": 320, "image_height": 240, "image_resolution": "320x240", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 4240.891, "latencies_ms": [4240.891], "images_per_second": 0.236, "prompt_tokens": 27, "response_tokens_est": 63, "n_tiles": 12, "output_text": "The main objects in the image are a laptop, a mouse, and a keyboard. The laptop is positioned on the left side of the image, with the mouse and keyboard placed in front of it. The laptop is the closest object to the viewer, while the mouse and keyboard are further back in the scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25705.4, "ram_available_mb": 100066.8, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 25706.4, "ram_available_mb": 100065.8, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.46, "peak": 15.85, "min": 13.49}, "VDD_CPU_SOC_MSS": {"avg": 14.82, "peak": 16.54, "min": 13.78}, "VDD_GPU": {"avg": 30.92, "peak": 41.76, "min": 23.65}, "VIN": {"avg": 72.3, "peak": 120.54, "min": 61.14}}, "power_watts_avg": 30.92, "energy_joules_est": 131.15, "sample_count": 33, "duration_seconds": 4.242}, "timestamp": "2026-01-17T14:22:54.524565"}
{"image_index": 15, "image_name": "000000001503.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001503.jpg", "image_width": 320, "image_height": 240, "image_resolution": "320x240", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3512.013, "latencies_ms": [3512.013], "images_per_second": 0.285, "prompt_tokens": 21, "response_tokens_est": 47, "n_tiles": 12, "output_text": "The image shows a cluttered desk with a laptop, a white mouse, a keyboard, and a small figurine on top of the desk. The setting appears to be a personal workspace, possibly in a home or office environment.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25706.4, "ram_available_mb": 100065.8, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 25707.4, "ram_available_mb": 100064.8, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.54, "peak": 15.95, "min": 13.39}, "VDD_CPU_SOC_MSS": {"avg": 15.04, "peak": 16.54, "min": 13.78}, "VDD_GPU": {"avg": 32.46, "peak": 41.36, "min": 24.04}, "VIN": {"avg": 69.73, "peak": 100.66, "min": 62.27}}, "power_watts_avg": 32.46, "energy_joules_est": 114.02, "sample_count": 27, "duration_seconds": 3.513}, "timestamp": "2026-01-17T14:22:58.043886"}
{"image_index": 15, "image_name": "000000001503.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001503.jpg", "image_width": 320, "image_height": 240, "image_resolution": "320x240", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3524.881, "latencies_ms": [3524.881], "images_per_second": 0.284, "prompt_tokens": 19, "response_tokens_est": 46, "n_tiles": 12, "output_text": "The image shows a desk with a white computer monitor, a white keyboard, a white mouse, and a white lamp. The desk is well-lit, and the overall color scheme is neutral with white and light gray tones.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25707.4, "ram_available_mb": 100064.8, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 24983.6, "ram_available_mb": 100788.6, "ram_percent": 19.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.41, "peak": 16.05, "min": 13.51}, "VDD_CPU_SOC_MSS": {"avg": 14.85, "peak": 16.93, "min": 13.78}, "VDD_GPU": {"avg": 32.25, "peak": 41.76, "min": 23.64}, "VIN": {"avg": 71.47, "peak": 105.99, "min": 55.5}}, "power_watts_avg": 32.25, "energy_joules_est": 113.69, "sample_count": 28, "duration_seconds": 3.525}, "timestamp": "2026-01-17T14:23:01.575390"}
{"image_index": 16, "image_name": "000000001532.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001532.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 3880.12, "latencies_ms": [3880.12], "images_per_second": 0.258, "prompt_tokens": 9, "response_tokens_est": 58, "n_tiles": 12, "output_text": "The image depicts a busy highway scene with multiple vehicles, including a taxi and a black SUV, traveling under a bridge. The bridge has green directional signs indicating directions to \"North Ventura\" and \"Sunset Blvd,\" with a \"20\" sign indicating a speed limit.", "error": null, "sys_before": {"cpu_percent": 40.7, "ram_used_mb": 25007.4, "ram_available_mb": 100764.7, "ram_percent": 19.9}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 25000.5, "ram_available_mb": 100771.7, "ram_percent": 19.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.41, "peak": 16.05, "min": 12.48}, "VDD_CPU_SOC_MSS": {"avg": 14.88, "peak": 16.93, "min": 13.39}, "VDD_GPU": {"avg": 31.18, "peak": 40.97, "min": 24.04}, "VIN": {"avg": 77.36, "peak": 114.7, "min": 51.8}}, "power_watts_avg": 31.18, "energy_joules_est": 120.99, "sample_count": 30, "duration_seconds": 3.88}, "timestamp": "2026-01-17T14:23:05.560043"}
{"image_index": 16, "image_name": "000000001532.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001532.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2944.271, "latencies_ms": [2944.271], "images_per_second": 0.34, "prompt_tokens": 23, "response_tokens_est": 33, "n_tiles": 12, "output_text": "1. Taxi\n2. SUV\n3. Van\n4. Car\n5. Car\n6. Car\n7. Car\n8. Car", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25000.5, "ram_available_mb": 100771.7, "ram_percent": 19.9}, "sys_after": {"cpu_percent": 2.6, "ram_used_mb": 24998.1, "ram_available_mb": 100774.1, "ram_percent": 19.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.54, "peak": 15.85, "min": 13.29}, "VDD_CPU_SOC_MSS": {"avg": 15.07, "peak": 16.54, "min": 13.39}, "VDD_GPU": {"avg": 34.26, "peak": 41.36, "min": 26.41}, "VIN": {"avg": 73.22, "peak": 102.14, "min": 56.22}}, "power_watts_avg": 34.26, "energy_joules_est": 100.89, "sample_count": 23, "duration_seconds": 2.945}, "timestamp": "2026-01-17T14:23:08.510605"}
{"image_index": 16, "image_name": "000000001532.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001532.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 4375.309, "latencies_ms": [4375.309], "images_per_second": 0.229, "prompt_tokens": 27, "response_tokens_est": 77, "n_tiles": 12, "output_text": "The main objects in the image are a highway sign, a car, and a van. The car is positioned in the foreground, slightly to the right, while the van is in the background, closer to the left. The highway sign is mounted on a bridge, indicating the direction to the left. The van is parked on the right side of the road, near the sign.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24998.1, "ram_available_mb": 100774.1, "ram_percent": 19.9}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 25000.1, "ram_available_mb": 100772.1, "ram_percent": 19.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.67, "peak": 15.85, "min": 13.59}, "VDD_CPU_SOC_MSS": {"avg": 15.35, "peak": 16.54, "min": 14.18}, "VDD_GPU": {"avg": 31.78, "peak": 44.91, "min": 25.22}, "VIN": {"avg": 71.2, "peak": 97.91, "min": 58.48}}, "power_watts_avg": 31.78, "energy_joules_est": 139.07, "sample_count": 35, "duration_seconds": 4.376}, "timestamp": "2026-01-17T14:23:12.893937"}
{"image_index": 16, "image_name": "000000001532.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001532.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 4520.357, "latencies_ms": [4520.357], "images_per_second": 0.221, "prompt_tokens": 21, "response_tokens_est": 81, "n_tiles": 12, "output_text": "The image depicts a busy highway scene under a bridge with multiple green directional signs indicating directions to various locations such as \"North Ventura\" and \"Sunset Blvd.\" The setting appears to be in a city, with a clear blue sky and some trees visible in the background. The scene is bustling with traffic, including a taxi and several cars, suggesting a typical day on the road.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25000.1, "ram_available_mb": 100772.1, "ram_percent": 19.9}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25001.3, "ram_available_mb": 100770.9, "ram_percent": 19.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.62, "peak": 16.05, "min": 13.61}, "VDD_CPU_SOC_MSS": {"avg": 15.34, "peak": 16.93, "min": 13.78}, "VDD_GPU": {"avg": 31.41, "peak": 42.54, "min": 25.22}, "VIN": {"avg": 72.56, "peak": 133.96, "min": 56.5}}, "power_watts_avg": 31.41, "energy_joules_est": 142.0, "sample_count": 35, "duration_seconds": 4.521}, "timestamp": "2026-01-17T14:23:17.421104"}
{"image_index": 16, "image_name": "000000001532.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001532.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3942.976, "latencies_ms": [3942.976], "images_per_second": 0.254, "prompt_tokens": 19, "response_tokens_est": 64, "n_tiles": 12, "output_text": "The image depicts a sunny day on a highway with a clear blue sky. The road is covered with a green overhead sign indicating directions to \"North Ventura\" and \"Sunset Blvd.\" The vehicles on the road include a taxi and a black SUV, and there are trees and buildings visible in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25001.3, "ram_available_mb": 100770.9, "ram_percent": 19.9}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25001.3, "ram_available_mb": 100770.9, "ram_percent": 19.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.65, "peak": 16.15, "min": 13.79}, "VDD_CPU_SOC_MSS": {"avg": 15.29, "peak": 17.32, "min": 14.18}, "VDD_GPU": {"avg": 32.29, "peak": 44.51, "min": 25.22}, "VIN": {"avg": 75.1, "peak": 123.33, "min": 54.75}}, "power_watts_avg": 32.29, "energy_joules_est": 127.33, "sample_count": 31, "duration_seconds": 3.943}, "timestamp": "2026-01-17T14:23:21.371043"}
{"image_index": 17, "image_name": "000000001584.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001584.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1105.838, "latencies_ms": [1105.838], "images_per_second": 0.904, "prompt_tokens": 9, "response_tokens_est": 47, "n_tiles": 1, "output_text": "A red double-decker bus, marked with the number 15 and the name \"Aldwych,\" is in motion on a city street, with a clear sky above and a few pedestrians and other vehicles in the background.", "error": null, "sys_before": {"cpu_percent": 32.4, "ram_used_mb": 25001.3, "ram_available_mb": 100770.9, "ram_percent": 19.9}, "sys_after": {"cpu_percent": 6.8, "ram_used_mb": 25001.5, "ram_available_mb": 100770.7, "ram_percent": 19.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 4845.7, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.7, "peak": 15.11, "min": 14.0}, "VDD_CPU_SOC_MSS": {"avg": 15.9, "peak": 16.54, "min": 14.97}, "VDD_GPU": {"avg": 22.41, "peak": 26.4, "min": 20.1}, "VIN": {"avg": 62.05, "peak": 64.84, "min": 55.91}}, "power_watts_avg": 22.41, "energy_joules_est": 24.79, "sample_count": 8, "duration_seconds": 1.106}, "timestamp": "2026-01-17T14:23:22.502723"}
{"image_index": 17, "image_name": "000000001584.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001584.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1083.914, "latencies_ms": [1083.914], "images_per_second": 0.923, "prompt_tokens": 23, "response_tokens_est": 47, "n_tiles": 1, "output_text": "- Red double-decker bus: 1\n- Bus: 1\n- People: 1\n- Cars: 1\n- Trees: 1\n- Buildings: 1\n- Sky: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25001.5, "ram_available_mb": 100770.7, "ram_percent": 19.9}, "sys_after": {"cpu_percent": 7.1, "ram_used_mb": 25003.0, "ram_available_mb": 100769.2, "ram_percent": 19.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 4856.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.06, "peak": 15.22, "min": 14.81}, "VDD_CPU_SOC_MSS": {"avg": 16.43, "peak": 16.54, "min": 16.15}, "VDD_GPU": {"avg": 21.39, "peak": 23.24, "min": 20.11}, "VIN": {"avg": 62.59, "peak": 69.74, "min": 56.15}}, "power_watts_avg": 21.39, "energy_joules_est": 23.19, "sample_count": 7, "duration_seconds": 1.084}, "timestamp": "2026-01-17T14:23:23.592707"}
{"image_index": 17, "image_name": "000000001584.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001584.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1278.258, "latencies_ms": [1278.258], "images_per_second": 0.782, "prompt_tokens": 27, "response_tokens_est": 56, "n_tiles": 1, "output_text": "The main object in the image is a red double-decker bus, which is in the foreground. The bus is moving on a road, and there are other vehicles in the background. The bus is near the center of the image, and the background features buildings and trees.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25003.0, "ram_available_mb": 100769.2, "ram_percent": 19.9}, "sys_after": {"cpu_percent": 8.0, "ram_used_mb": 25002.7, "ram_available_mb": 100769.4, "ram_percent": 19.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 4859.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.07, "peak": 15.22, "min": 14.71}, "VDD_CPU_SOC_MSS": {"avg": 16.54, "peak": 16.94, "min": 16.15}, "VDD_GPU": {"avg": 21.17, "peak": 24.04, "min": 19.71}, "VIN": {"avg": 62.19, "peak": 65.34, "min": 55.24}}, "power_watts_avg": 21.17, "energy_joules_est": 27.07, "sample_count": 10, "duration_seconds": 1.279}, "timestamp": "2026-01-17T14:23:24.877709"}
{"image_index": 17, "image_name": "000000001584.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001584.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1534.85, "latencies_ms": [1534.85], "images_per_second": 0.652, "prompt_tokens": 21, "response_tokens_est": 68, "n_tiles": 1, "output_text": "The image depicts a vibrant red double-decker bus in motion on a city street, with a clear blue sky overhead. The bus is adorned with various advertisements and has the number \"15\" displayed on its front. In the background, there are buildings, trees, and a few pedestrians, suggesting a bustling urban environment.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25002.7, "ram_available_mb": 100769.4, "ram_percent": 19.9}, "sys_after": {"cpu_percent": 8.0, "ram_used_mb": 25003.2, "ram_available_mb": 100769.0, "ram_percent": 19.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 4854.7, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.15, "peak": 15.22, "min": 14.91}, "VDD_CPU_SOC_MSS": {"avg": 16.61, "peak": 16.94, "min": 16.14}, "VDD_GPU": {"avg": 20.75, "peak": 23.26, "min": 19.7}, "VIN": {"avg": 63.3, "peak": 69.83, "min": 58.93}}, "power_watts_avg": 20.75, "energy_joules_est": 31.86, "sample_count": 11, "duration_seconds": 1.535}, "timestamp": "2026-01-17T14:23:26.418450"}
{"image_index": 17, "image_name": "000000001584.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001584.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1230.529, "latencies_ms": [1230.529], "images_per_second": 0.813, "prompt_tokens": 19, "response_tokens_est": 54, "n_tiles": 1, "output_text": "The red double-decker bus in the image has a striking color contrast against the overcast sky, with its bright red hue standing out. The lighting is diffused, likely due to the cloudy weather, casting a soft, even illumination on the bus.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25003.2, "ram_available_mb": 100769.0, "ram_percent": 19.9}, "sys_after": {"cpu_percent": 7.7, "ram_used_mb": 25004.0, "ram_available_mb": 100768.2, "ram_percent": 19.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 4853.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.16, "peak": 15.32, "min": 14.81}, "VDD_CPU_SOC_MSS": {"avg": 16.63, "peak": 16.95, "min": 16.15}, "VDD_GPU": {"avg": 21.06, "peak": 23.24, "min": 19.7}, "VIN": {"avg": 62.73, "peak": 71.17, "min": 60.12}}, "power_watts_avg": 21.06, "energy_joules_est": 25.92, "sample_count": 9, "duration_seconds": 1.231}, "timestamp": "2026-01-17T14:23:27.655494"}
{"image_index": 18, "image_name": "000000001675.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001675.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2592.459, "latencies_ms": [2592.459], "images_per_second": 0.386, "prompt_tokens": 9, "response_tokens_est": 29, "n_tiles": 12, "output_text": "The image shows a black and white cat with yellow eyes, resting on top of a laptop keyboard, which is partially visible in the foreground.", "error": null, "sys_before": {"cpu_percent": 46.2, "ram_used_mb": 25011.9, "ram_available_mb": 100760.2, "ram_percent": 19.9}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25007.4, "ram_available_mb": 100764.8, "ram_percent": 19.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.87, "peak": 16.26, "min": 13.59}, "VDD_CPU_SOC_MSS": {"avg": 15.92, "peak": 17.32, "min": 14.96}, "VDD_GPU": {"avg": 35.55, "peak": 42.94, "min": 27.19}, "VIN": {"avg": 83.91, "peak": 128.08, "min": 58.2}}, "power_watts_avg": 35.55, "energy_joules_est": 92.17, "sample_count": 18, "duration_seconds": 2.593}, "timestamp": "2026-01-17T14:23:30.333855"}
{"image_index": 18, "image_name": "000000001675.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001675.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2787.248, "latencies_ms": [2787.248], "images_per_second": 0.359, "prompt_tokens": 23, "response_tokens_est": 33, "n_tiles": 12, "output_text": "1. Laptop\n2. Keyboard\n3. Mouse\n4. Laptop screen\n5. Laptop\n6. Laptop\n7. Laptop\n8. Laptop", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25007.4, "ram_available_mb": 100764.8, "ram_percent": 19.9}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 25004.2, "ram_available_mb": 100768.0, "ram_percent": 19.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.84, "peak": 16.15, "min": 13.79}, "VDD_CPU_SOC_MSS": {"avg": 15.6, "peak": 17.32, "min": 14.18}, "VDD_GPU": {"avg": 36.08, "peak": 44.91, "min": 26.8}, "VIN": {"avg": 79.52, "peak": 122.66, "min": 61.37}}, "power_watts_avg": 36.08, "energy_joules_est": 100.58, "sample_count": 21, "duration_seconds": 2.788}, "timestamp": "2026-01-17T14:23:33.128059"}
{"image_index": 18, "image_name": "000000001675.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001675.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 4116.163, "latencies_ms": [4116.163], "images_per_second": 0.243, "prompt_tokens": 27, "response_tokens_est": 72, "n_tiles": 12, "output_text": "The main object in the image is a laptop, which is positioned in the foreground. The keyboard of the laptop is visible, with the keys arranged in a standard QWERTY layout. The laptop is situated near the bottom of the image, with its screen partially visible. The background is out of focus, emphasizing the laptop as the central subject.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25004.2, "ram_available_mb": 100768.0, "ram_percent": 19.9}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25004.4, "ram_available_mb": 100767.7, "ram_percent": 19.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.75, "peak": 16.15, "min": 13.69}, "VDD_CPU_SOC_MSS": {"avg": 15.54, "peak": 17.32, "min": 14.18}, "VDD_GPU": {"avg": 32.52, "peak": 44.12, "min": 25.61}, "VIN": {"avg": 73.58, "peak": 113.15, "min": 61.15}}, "power_watts_avg": 32.52, "energy_joules_est": 133.87, "sample_count": 32, "duration_seconds": 4.117}, "timestamp": "2026-01-17T14:23:37.251052"}
{"image_index": 18, "image_name": "000000001675.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001675.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 5347.682, "latencies_ms": [5347.682], "images_per_second": 0.187, "prompt_tokens": 21, "response_tokens_est": 108, "n_tiles": 12, "output_text": "The image depicts a close-up view of a black and white cat sitting in front of a laptop keyboard. The cat's fur is predominantly black with white markings, and it appears to be looking directly at the camera with a somewhat curious or alert expression. The laptop keyboard is visible in the foreground, with the keys in focus, and the laptop itself is partially visible behind the cat. The setting seems to be indoors, possibly in a home or office environment, with a plain background that does not distract from the main subject.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25004.4, "ram_available_mb": 100767.7, "ram_percent": 19.9}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25005.4, "ram_available_mb": 100766.8, "ram_percent": 19.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.73, "peak": 16.15, "min": 13.79}, "VDD_CPU_SOC_MSS": {"avg": 15.47, "peak": 17.32, "min": 14.18}, "VDD_GPU": {"avg": 30.72, "peak": 44.91, "min": 25.22}, "VIN": {"avg": 71.65, "peak": 114.91, "min": 56.38}}, "power_watts_avg": 30.72, "energy_joules_est": 164.3, "sample_count": 42, "duration_seconds": 5.348}, "timestamp": "2026-01-17T14:23:42.606105"}
{"image_index": 18, "image_name": "000000001675.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001675.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3379.052, "latencies_ms": [3379.052], "images_per_second": 0.296, "prompt_tokens": 19, "response_tokens_est": 51, "n_tiles": 12, "output_text": "The image features a black and white cat with striking yellow eyes, sitting in front of a laptop keyboard. The cat's fur appears soft and fluffy, and the lighting is bright, highlighting the details of the cat's fur and the laptop keyboard.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25005.4, "ram_available_mb": 100766.8, "ram_percent": 19.9}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 25006.4, "ram_available_mb": 100765.8, "ram_percent": 19.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.85, "peak": 16.26, "min": 13.69}, "VDD_CPU_SOC_MSS": {"avg": 15.57, "peak": 17.32, "min": 14.57}, "VDD_GPU": {"avg": 34.1, "peak": 44.12, "min": 25.62}, "VIN": {"avg": 76.84, "peak": 106.71, "min": 64.12}}, "power_watts_avg": 34.1, "energy_joules_est": 115.24, "sample_count": 26, "duration_seconds": 3.379}, "timestamp": "2026-01-17T14:23:45.991418"}
{"image_index": 19, "image_name": "000000001761.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001761.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1666.284, "latencies_ms": [1666.284], "images_per_second": 0.6, "prompt_tokens": 9, "response_tokens_est": 39, "n_tiles": 6, "output_text": "The image depicts a large, dark-colored steel arch bridge spanning across a body of water, with a city skyline in the background featuring a distinctive building with a unique, curved design.", "error": null, "sys_before": {"cpu_percent": 40.8, "ram_used_mb": 25021.3, "ram_available_mb": 100750.8, "ram_percent": 19.9}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 25022.3, "ram_available_mb": 100749.9, "ram_percent": 19.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.33, "peak": 14.73, "min": 13.49}, "VDD_CPU_SOC_MSS": {"avg": 15.18, "peak": 15.76, "min": 14.18}, "VDD_GPU": {"avg": 31.67, "peak": 40.57, "min": 24.43}, "VIN": {"avg": 72.48, "peak": 112.03, "min": 57.98}}, "power_watts_avg": 31.67, "energy_joules_est": 52.79, "sample_count": 13, "duration_seconds": 1.667}, "timestamp": "2026-01-17T14:23:47.714761"}
{"image_index": 19, "image_name": "000000001761.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001761.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1775.159, "latencies_ms": [1775.159], "images_per_second": 0.563, "prompt_tokens": 23, "response_tokens_est": 40, "n_tiles": 6, "output_text": "object: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25022.3, "ram_available_mb": 100749.9, "ram_percent": 19.9}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 25023.6, "ram_available_mb": 100748.6, "ram_percent": 19.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.42, "peak": 14.83, "min": 13.92}, "VDD_CPU_SOC_MSS": {"avg": 15.24, "peak": 15.75, "min": 14.57}, "VDD_GPU": {"avg": 31.67, "peak": 40.59, "min": 24.04}, "VIN": {"avg": 71.39, "peak": 110.2, "min": 55.12}}, "power_watts_avg": 31.67, "energy_joules_est": 56.23, "sample_count": 13, "duration_seconds": 1.776}, "timestamp": "2026-01-17T14:23:49.496112"}
{"image_index": 19, "image_name": "000000001761.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001761.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2258.129, "latencies_ms": [2258.129], "images_per_second": 0.443, "prompt_tokens": 27, "response_tokens_est": 57, "n_tiles": 6, "output_text": "The main objects in the image are the Sydney Harbour Bridge and the Sydney Opera House. The Sydney Harbour Bridge is in the foreground, while the Sydney Opera House is in the background. The Sydney Opera House is located near the water, and the bridge spans over the water.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25023.6, "ram_available_mb": 100748.6, "ram_percent": 19.9}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 25025.5, "ram_available_mb": 100746.7, "ram_percent": 19.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.43, "peak": 14.83, "min": 13.82}, "VDD_CPU_SOC_MSS": {"avg": 15.22, "peak": 15.75, "min": 14.18}, "VDD_GPU": {"avg": 29.67, "peak": 40.18, "min": 22.86}, "VIN": {"avg": 70.8, "peak": 108.21, "min": 57.72}}, "power_watts_avg": 29.67, "energy_joules_est": 67.01, "sample_count": 17, "duration_seconds": 2.259}, "timestamp": "2026-01-17T14:23:51.760620"}
{"image_index": 19, "image_name": "000000001761.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001761.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2365.521, "latencies_ms": [2365.521], "images_per_second": 0.423, "prompt_tokens": 21, "response_tokens_est": 61, "n_tiles": 6, "output_text": "The image depicts a large, modern steel arch bridge spanning across a body of water, likely a river or bay, with a cloudy sky overhead. The bridge is a prominent feature in the foreground, while the city skyline in the background includes a distinctive building with a unique, curved design.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25025.5, "ram_available_mb": 100746.7, "ram_percent": 19.9}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 25026.5, "ram_available_mb": 100745.7, "ram_percent": 19.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.44, "peak": 14.73, "min": 13.82}, "VDD_CPU_SOC_MSS": {"avg": 15.21, "peak": 15.36, "min": 14.18}, "VDD_GPU": {"avg": 29.2, "peak": 40.57, "min": 22.86}, "VIN": {"avg": 66.81, "peak": 92.61, "min": 53.47}}, "power_watts_avg": 29.2, "energy_joules_est": 69.09, "sample_count": 18, "duration_seconds": 2.366}, "timestamp": "2026-01-17T14:23:54.133212"}
{"image_index": 19, "image_name": "000000001761.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001761.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1766.706, "latencies_ms": [1766.706], "images_per_second": 0.566, "prompt_tokens": 19, "response_tokens_est": 40, "n_tiles": 6, "output_text": "The image features a large, dark-colored steel arch bridge with a cloudy sky in the background. The lighting is soft and diffused, with no harsh shadows, indicating an overcast day.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25026.5, "ram_available_mb": 100745.7, "ram_percent": 19.9}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25027.2, "ram_available_mb": 100745.0, "ram_percent": 19.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.44, "peak": 14.94, "min": 13.9}, "VDD_CPU_SOC_MSS": {"avg": 15.24, "peak": 15.75, "min": 14.57}, "VDD_GPU": {"avg": 31.43, "peak": 39.78, "min": 24.04}, "VIN": {"avg": 75.6, "peak": 112.71, "min": 61.42}}, "power_watts_avg": 31.43, "energy_joules_est": 55.54, "sample_count": 13, "duration_seconds": 1.767}, "timestamp": "2026-01-17T14:23:55.906156"}
{"image_index": 20, "image_name": "000000001818.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001818.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1601.538, "latencies_ms": [1601.538], "images_per_second": 0.624, "prompt_tokens": 9, "response_tokens_est": 37, "n_tiles": 6, "output_text": "The image captures a zebra with its distinctive black and white striped pattern, standing in a natural grassy environment, showcasing the unique and striking markings of its species.", "error": null, "sys_before": {"cpu_percent": 42.7, "ram_used_mb": 25031.2, "ram_available_mb": 100741.0, "ram_percent": 19.9}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 25033.2, "ram_available_mb": 100739.0, "ram_percent": 19.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.34, "peak": 14.81, "min": 13.39}, "VDD_CPU_SOC_MSS": {"avg": 15.23, "peak": 15.76, "min": 14.18}, "VDD_GPU": {"avg": 31.78, "peak": 40.18, "min": 24.84}, "VIN": {"avg": 72.12, "peak": 100.71, "min": 57.66}}, "power_watts_avg": 31.78, "energy_joules_est": 50.91, "sample_count": 12, "duration_seconds": 1.602}, "timestamp": "2026-01-17T14:23:57.573134"}
{"image_index": 20, "image_name": "000000001818.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001818.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 946.314, "latencies_ms": [946.314], "images_per_second": 1.057, "prompt_tokens": 23, "response_tokens_est": 11, "n_tiles": 6, "output_text": "zebra: 1\ngrass: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25033.2, "ram_available_mb": 100739.0, "ram_percent": 19.9}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 25033.4, "ram_available_mb": 100738.8, "ram_percent": 19.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.51, "peak": 15.14, "min": 13.92}, "VDD_CPU_SOC_MSS": {"avg": 15.19, "peak": 15.75, "min": 14.57}, "VDD_GPU": {"avg": 36.58, "peak": 39.78, "min": 31.92}, "VIN": {"avg": 78.81, "peak": 101.05, "min": 62.19}}, "power_watts_avg": 36.58, "energy_joules_est": 34.63, "sample_count": 7, "duration_seconds": 0.947}, "timestamp": "2026-01-17T14:23:58.525995"}
{"image_index": 20, "image_name": "000000001818.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001818.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2763.591, "latencies_ms": [2763.591], "images_per_second": 0.362, "prompt_tokens": 27, "response_tokens_est": 75, "n_tiles": 6, "output_text": "The main object in the foreground is a zebra, with its distinctive black and white stripes clearly visible. The zebra is positioned slightly to the left of the center of the image, with its head turned to the right. In the background, there is a blurred natural setting, likely a grassy field, which provides a context for the zebra's habitat.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25033.4, "ram_available_mb": 100738.8, "ram_percent": 19.9}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 25033.9, "ram_available_mb": 100738.3, "ram_percent": 19.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.49, "peak": 14.83, "min": 13.92}, "VDD_CPU_SOC_MSS": {"avg": 15.28, "peak": 15.75, "min": 14.57}, "VDD_GPU": {"avg": 29.29, "peak": 42.15, "min": 22.46}, "VIN": {"avg": 68.04, "peak": 104.33, "min": 59.8}}, "power_watts_avg": 29.29, "energy_joules_est": 80.95, "sample_count": 21, "duration_seconds": 2.764}, "timestamp": "2026-01-17T14:24:01.295840"}
{"image_index": 20, "image_name": "000000001818.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001818.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2362.185, "latencies_ms": [2362.185], "images_per_second": 0.423, "prompt_tokens": 21, "response_tokens_est": 61, "n_tiles": 6, "output_text": "The image captures a zebra in a natural setting, likely a savanna or grassland. The zebra is seen in profile, with its distinctive black and white stripes clearly visible. The background consists of tall grasses, indicating a wild, open environment typical of a zebra's natural habitat.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25033.7, "ram_available_mb": 100738.5, "ram_percent": 19.9}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 25034.6, "ram_available_mb": 100737.5, "ram_percent": 19.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.48, "peak": 14.83, "min": 13.92}, "VDD_CPU_SOC_MSS": {"avg": 15.18, "peak": 15.75, "min": 14.18}, "VDD_GPU": {"avg": 29.22, "peak": 39.78, "min": 22.86}, "VIN": {"avg": 66.71, "peak": 89.78, "min": 56.04}}, "power_watts_avg": 29.22, "energy_joules_est": 69.03, "sample_count": 18, "duration_seconds": 2.362}, "timestamp": "2026-01-17T14:24:03.664993"}
{"image_index": 20, "image_name": "000000001818.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001818.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2588.734, "latencies_ms": [2588.734], "images_per_second": 0.386, "prompt_tokens": 19, "response_tokens_est": 69, "n_tiles": 6, "output_text": "The image is a black and white photograph of a zebra, showcasing its distinctive black and white striped pattern. The lighting is natural, likely from the sun, casting shadows that enhance the texture of the stripes. The zebra's coat appears to be dry and well-maintained, with no visible signs of wear or damage.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25034.6, "ram_available_mb": 100737.5, "ram_percent": 19.9}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 25034.6, "ram_available_mb": 100737.5, "ram_percent": 19.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.47, "peak": 14.83, "min": 13.9}, "VDD_CPU_SOC_MSS": {"avg": 15.26, "peak": 15.75, "min": 14.57}, "VDD_GPU": {"avg": 28.49, "peak": 39.78, "min": 22.46}, "VIN": {"avg": 70.08, "peak": 99.44, "min": 58.69}}, "power_watts_avg": 28.49, "energy_joules_est": 73.77, "sample_count": 20, "duration_seconds": 2.589}, "timestamp": "2026-01-17T14:24:06.260277"}
{"image_index": 21, "image_name": "000000001993.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001993.jpg", "image_width": 640, "image_height": 419, "image_resolution": "640x419", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1658.178, "latencies_ms": [1658.178], "images_per_second": 0.603, "prompt_tokens": 9, "response_tokens_est": 39, "n_tiles": 6, "output_text": "The image shows a small, cozy room with a bed, a table, and a chair, all set against a backdrop of a brick wall and a window with a view of a balcony.", "error": null, "sys_before": {"cpu_percent": 42.7, "ram_used_mb": 25034.6, "ram_available_mb": 100737.5, "ram_percent": 19.9}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 25034.6, "ram_available_mb": 100737.5, "ram_percent": 19.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.34, "peak": 14.81, "min": 13.39}, "VDD_CPU_SOC_MSS": {"avg": 15.16, "peak": 15.75, "min": 14.18}, "VDD_GPU": {"avg": 31.82, "peak": 40.18, "min": 24.83}, "VIN": {"avg": 75.07, "peak": 115.57, "min": 58.25}}, "power_watts_avg": 31.82, "energy_joules_est": 52.77, "sample_count": 12, "duration_seconds": 1.658}, "timestamp": "2026-01-17T14:24:07.977010"}
{"image_index": 21, "image_name": "000000001993.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001993.jpg", "image_width": 640, "image_height": 419, "image_resolution": "640x419", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1647.446, "latencies_ms": [1647.446], "images_per_second": 0.607, "prompt_tokens": 23, "response_tokens_est": 37, "n_tiles": 6, "output_text": "1. Bed\n2. Bedding\n3. Bed frame\n4. Bedspread\n5. Floor\n6. Flooring\n7. Rug\n8. Rug pad", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25034.6, "ram_available_mb": 100737.5, "ram_percent": 19.9}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 25034.6, "ram_available_mb": 100737.5, "ram_percent": 19.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.64, "peak": 15.14, "min": 14.02}, "VDD_CPU_SOC_MSS": {"avg": 15.32, "peak": 15.75, "min": 14.57}, "VDD_GPU": {"avg": 32.54, "peak": 39.78, "min": 24.83}, "VIN": {"avg": 71.1, "peak": 110.65, "min": 61.69}}, "power_watts_avg": 32.54, "energy_joules_est": 53.62, "sample_count": 12, "duration_seconds": 1.648}, "timestamp": "2026-01-17T14:24:09.630047"}
{"image_index": 21, "image_name": "000000001993.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001993.jpg", "image_width": 640, "image_height": 419, "image_resolution": "640x419", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2561.651, "latencies_ms": [2561.651], "images_per_second": 0.39, "prompt_tokens": 27, "response_tokens_est": 68, "n_tiles": 6, "output_text": "The main objects in the image are a bed and a table. The bed is positioned in the foreground, with a colorful patterned bedspread. The table is located to the left of the bed, with a small black chair next to it. The background features a window with a view of a building and a lamp on the wall.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25034.6, "ram_available_mb": 100737.5, "ram_percent": 19.9}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 25034.9, "ram_available_mb": 100737.3, "ram_percent": 19.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.56, "peak": 14.94, "min": 14.02}, "VDD_CPU_SOC_MSS": {"avg": 15.27, "peak": 15.75, "min": 14.57}, "VDD_GPU": {"avg": 28.97, "peak": 40.57, "min": 22.46}, "VIN": {"avg": 67.04, "peak": 89.44, "min": 57.87}}, "power_watts_avg": 28.97, "energy_joules_est": 74.22, "sample_count": 19, "duration_seconds": 2.562}, "timestamp": "2026-01-17T14:24:12.198231"}
{"image_index": 21, "image_name": "000000001993.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001993.jpg", "image_width": 640, "image_height": 419, "image_resolution": "640x419", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2843.389, "latencies_ms": [2843.389], "images_per_second": 0.352, "prompt_tokens": 21, "response_tokens_est": 78, "n_tiles": 6, "output_text": "The image depicts a cozy, well-lit room with a bed and a small table. The bed is adorned with a colorful patterned comforter, and the room has a purple carpet. The table is situated near the window, and there is a chair visible next to it. The room appears to be a bedroom or a guest room, with a cozy and inviting atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25034.9, "ram_available_mb": 100737.3, "ram_percent": 19.9}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 25035.9, "ram_available_mb": 100736.3, "ram_percent": 19.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.49, "peak": 14.83, "min": 13.9}, "VDD_CPU_SOC_MSS": {"avg": 15.23, "peak": 15.75, "min": 14.18}, "VDD_GPU": {"avg": 27.94, "peak": 40.18, "min": 22.46}, "VIN": {"avg": 67.14, "peak": 92.47, "min": 57.73}}, "power_watts_avg": 27.94, "energy_joules_est": 79.45, "sample_count": 22, "duration_seconds": 2.844}, "timestamp": "2026-01-17T14:24:15.047292"}
{"image_index": 21, "image_name": "000000001993.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001993.jpg", "image_width": 640, "image_height": 419, "image_resolution": "640x419", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1933.444, "latencies_ms": [1933.444], "images_per_second": 0.517, "prompt_tokens": 19, "response_tokens_est": 46, "n_tiles": 6, "output_text": "The room features a purple carpet, a bed with a colorful patterned duvet, a small round table with a black chair, and a window with white frames. The lighting is soft and natural, suggesting it is daytime.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25035.9, "ram_available_mb": 100736.3, "ram_percent": 19.9}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 25036.6, "ram_available_mb": 100735.6, "ram_percent": 19.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.47, "peak": 14.83, "min": 13.9}, "VDD_CPU_SOC_MSS": {"avg": 15.2, "peak": 15.75, "min": 14.57}, "VDD_GPU": {"avg": 30.47, "peak": 39.78, "min": 23.25}, "VIN": {"avg": 75.59, "peak": 115.67, "min": 56.15}}, "power_watts_avg": 30.47, "energy_joules_est": 58.92, "sample_count": 15, "duration_seconds": 1.934}, "timestamp": "2026-01-17T14:24:16.987000"}
{"image_index": 22, "image_name": "000000002006.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002006.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2661.66, "latencies_ms": [2661.66], "images_per_second": 0.376, "prompt_tokens": 9, "response_tokens_est": 31, "n_tiles": 12, "output_text": "A purple bus with the number 96 is driving on a street, with a person in the driver's seat and another person walking on the sidewalk.", "error": null, "sys_before": {"cpu_percent": 40.8, "ram_used_mb": 25002.7, "ram_available_mb": 100769.5, "ram_percent": 19.9}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 24999.8, "ram_available_mb": 100772.3, "ram_percent": 19.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.78, "peak": 16.26, "min": 13.09}, "VDD_CPU_SOC_MSS": {"avg": 15.61, "peak": 17.32, "min": 14.18}, "VDD_GPU": {"avg": 35.18, "peak": 43.33, "min": 26.8}, "VIN": {"avg": 73.26, "peak": 101.32, "min": 64.78}}, "power_watts_avg": 35.18, "energy_joules_est": 93.65, "sample_count": 20, "duration_seconds": 2.662}, "timestamp": "2026-01-17T14:24:19.739155"}
{"image_index": 22, "image_name": "000000002006.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002006.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 5980.543, "latencies_ms": [5980.543], "images_per_second": 0.167, "prompt_tokens": 23, "response_tokens_est": 129, "n_tiles": 12, "output_text": "- bus: 1\n- driver: 1\n- bus stop: 1\n- bus stop sign: 1\n- bus stop pole: 1\n- bus stop light: 1\n- bus stop signboard: 1\n- bus stop signboard pole: 1\n- bus stop signboard pole pole: 1\n- bus stop signboard pole pole pole: 1\n- bus stop signboard pole pole pole pole: 1\n- bus stop signboard pole pole pole pole pole pole pole pole pole pole pole pole pole pole pole pole pole pole pole pole pole pole pole pole pole", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 24999.8, "ram_available_mb": 100772.3, "ram_percent": 19.9}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 24994.6, "ram_available_mb": 100777.5, "ram_percent": 19.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.86, "peak": 16.26, "min": 13.79}, "VDD_CPU_SOC_MSS": {"avg": 15.47, "peak": 17.32, "min": 14.57}, "VDD_GPU": {"avg": 30.49, "peak": 44.51, "min": 25.61}, "VIN": {"avg": 71.55, "peak": 114.08, "min": 61.62}}, "power_watts_avg": 30.49, "energy_joules_est": 182.36, "sample_count": 47, "duration_seconds": 5.981}, "timestamp": "2026-01-17T14:24:25.726247"}
{"image_index": 22, "image_name": "000000002006.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002006.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3567.015, "latencies_ms": [3567.015], "images_per_second": 0.28, "prompt_tokens": 27, "response_tokens_est": 56, "n_tiles": 12, "output_text": "The main object in the foreground is a purple bus with the number 96 on its front. The bus is parked on the side of a street, with a person walking in front of it. The background features a tree-lined street and a building with a chimney.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24994.6, "ram_available_mb": 100777.5, "ram_percent": 19.9}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 24995.6, "ram_available_mb": 100776.6, "ram_percent": 19.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.91, "peak": 16.26, "min": 14.02}, "VDD_CPU_SOC_MSS": {"avg": 15.58, "peak": 17.32, "min": 14.57}, "VDD_GPU": {"avg": 33.79, "peak": 44.51, "min": 25.61}, "VIN": {"avg": 78.37, "peak": 120.88, "min": 61.84}}, "power_watts_avg": 33.79, "energy_joules_est": 120.55, "sample_count": 27, "duration_seconds": 3.567}, "timestamp": "2026-01-17T14:24:29.300090"}
{"image_index": 22, "image_name": "000000002006.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002006.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3264.759, "latencies_ms": [3264.759], "images_per_second": 0.306, "prompt_tokens": 21, "response_tokens_est": 47, "n_tiles": 12, "output_text": "The image depicts a purple bus with the number 96 on its destination sign, traveling on a road. A man in a white shirt is driving the bus, and there are other people walking on the sidewalk in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24995.6, "ram_available_mb": 100776.6, "ram_percent": 19.9}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 24996.4, "ram_available_mb": 100775.8, "ram_percent": 19.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.87, "peak": 16.26, "min": 13.79}, "VDD_CPU_SOC_MSS": {"avg": 15.53, "peak": 17.32, "min": 14.18}, "VDD_GPU": {"avg": 34.54, "peak": 44.12, "min": 25.61}, "VIN": {"avg": 76.26, "peak": 108.28, "min": 53.77}}, "power_watts_avg": 34.54, "energy_joules_est": 112.77, "sample_count": 25, "duration_seconds": 3.265}, "timestamp": "2026-01-17T14:24:32.571446"}
{"image_index": 22, "image_name": "000000002006.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002006.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3436.754, "latencies_ms": [3436.754], "images_per_second": 0.291, "prompt_tokens": 19, "response_tokens_est": 52, "n_tiles": 12, "output_text": "The notable visual attributes of the bus include its purple color, which stands out against the background. The lighting is bright, indicating it is daytime. The materials used for the bus are likely metal and plastic, and the weather appears to be clear and sunny.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24996.4, "ram_available_mb": 100775.8, "ram_percent": 19.9}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 24994.8, "ram_available_mb": 100777.4, "ram_percent": 19.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.88, "peak": 16.26, "min": 13.9}, "VDD_CPU_SOC_MSS": {"avg": 15.54, "peak": 17.32, "min": 14.17}, "VDD_GPU": {"avg": 33.84, "peak": 44.91, "min": 25.61}, "VIN": {"avg": 76.14, "peak": 111.74, "min": 62.88}}, "power_watts_avg": 33.84, "energy_joules_est": 116.31, "sample_count": 27, "duration_seconds": 3.437}, "timestamp": "2026-01-17T14:24:36.015304"}
{"image_index": 23, "image_name": "000000002149.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002149.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1461.925, "latencies_ms": [1461.925], "images_per_second": 0.684, "prompt_tokens": 9, "response_tokens_est": 32, "n_tiles": 6, "output_text": "The image shows a close-up view of several green apples arranged in a bowl, with a focus on their smooth, glossy surfaces and uniform green color.", "error": null, "sys_before": {"cpu_percent": 43.9, "ram_used_mb": 25008.7, "ram_available_mb": 100763.5, "ram_percent": 19.9}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25009.4, "ram_available_mb": 100762.7, "ram_percent": 19.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.38, "peak": 14.91, "min": 13.39}, "VDD_CPU_SOC_MSS": {"avg": 15.14, "peak": 15.75, "min": 14.18}, "VDD_GPU": {"avg": 32.77, "peak": 40.18, "min": 25.61}, "VIN": {"avg": 69.86, "peak": 89.42, "min": 57.41}}, "power_watts_avg": 32.77, "energy_joules_est": 47.92, "sample_count": 11, "duration_seconds": 1.462}, "timestamp": "2026-01-17T14:24:37.540201"}
{"image_index": 23, "image_name": "000000002149.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002149.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1714.309, "latencies_ms": [1714.309], "images_per_second": 0.583, "prompt_tokens": 23, "response_tokens_est": 40, "n_tiles": 6, "output_text": "1. Green apples\n2. Green apples\n3. Green apples\n4. Green apples\n5. Green apples\n6. Green apples\n7. Green apples\n8. Green apples", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25009.4, "ram_available_mb": 100762.7, "ram_percent": 19.9}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 25010.7, "ram_available_mb": 100761.5, "ram_percent": 19.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.78, "peak": 15.24, "min": 14.02}, "VDD_CPU_SOC_MSS": {"avg": 15.45, "peak": 16.14, "min": 14.57}, "VDD_GPU": {"avg": 32.4, "peak": 40.57, "min": 24.83}, "VIN": {"avg": 74.7, "peak": 112.44, "min": 58.88}}, "power_watts_avg": 32.4, "energy_joules_est": 55.56, "sample_count": 13, "duration_seconds": 1.715}, "timestamp": "2026-01-17T14:24:39.260936"}
{"image_index": 23, "image_name": "000000002149.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002149.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2090.183, "latencies_ms": [2090.183], "images_per_second": 0.478, "prompt_tokens": 27, "response_tokens_est": 54, "n_tiles": 6, "output_text": "The main objects in the image are green apples. The apples are arranged in a way that the foreground has a few apples, while the background has more apples. The apples in the foreground are closer to the viewer, while the apples in the background are further away.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25010.7, "ram_available_mb": 100761.5, "ram_percent": 19.9}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 25011.4, "ram_available_mb": 100760.8, "ram_percent": 19.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.86, "peak": 15.24, "min": 14.12}, "VDD_CPU_SOC_MSS": {"avg": 15.53, "peak": 16.14, "min": 14.57}, "VDD_GPU": {"avg": 30.44, "peak": 39.78, "min": 23.64}, "VIN": {"avg": 70.26, "peak": 95.13, "min": 58.39}}, "power_watts_avg": 30.44, "energy_joules_est": 63.64, "sample_count": 16, "duration_seconds": 2.091}, "timestamp": "2026-01-17T14:24:41.358269"}
{"image_index": 23, "image_name": "000000002149.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002149.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2248.593, "latencies_ms": [2248.593], "images_per_second": 0.445, "prompt_tokens": 21, "response_tokens_est": 59, "n_tiles": 6, "output_text": "The image depicts a close-up view of a white bowl filled with fresh green apples. The apples are arranged in a neat pile, with some partially overlapping each other. The setting appears to be a kitchen or a dining area, as the bowl is placed on a table or countertop.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25011.4, "ram_available_mb": 100760.8, "ram_percent": 19.9}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 25011.4, "ram_available_mb": 100760.8, "ram_percent": 19.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.73, "peak": 15.14, "min": 14.12}, "VDD_CPU_SOC_MSS": {"avg": 15.56, "peak": 16.14, "min": 14.57}, "VDD_GPU": {"avg": 29.99, "peak": 40.57, "min": 23.25}, "VIN": {"avg": 69.09, "peak": 87.21, "min": 59.91}}, "power_watts_avg": 29.99, "energy_joules_est": 67.45, "sample_count": 17, "duration_seconds": 2.249}, "timestamp": "2026-01-17T14:24:43.612855"}
{"image_index": 23, "image_name": "000000002149.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002149.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1807.001, "latencies_ms": [1807.001], "images_per_second": 0.553, "prompt_tokens": 19, "response_tokens_est": 43, "n_tiles": 6, "output_text": "The image features a bowl filled with fresh green apples, which are bright and vibrant in color. The lighting is soft and diffused, casting gentle shadows and highlighting the smooth, glossy surface of the apples.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25011.4, "ram_available_mb": 100760.8, "ram_percent": 19.9}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25011.7, "ram_available_mb": 100760.5, "ram_percent": 19.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.72, "peak": 15.14, "min": 14.12}, "VDD_CPU_SOC_MSS": {"avg": 15.5, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 31.41, "peak": 40.18, "min": 24.04}, "VIN": {"avg": 73.52, "peak": 124.05, "min": 55.68}}, "power_watts_avg": 31.41, "energy_joules_est": 56.77, "sample_count": 14, "duration_seconds": 1.807}, "timestamp": "2026-01-17T14:24:45.426030"}
{"image_index": 24, "image_name": "000000002153.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002153.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2726.657, "latencies_ms": [2726.657], "images_per_second": 0.367, "prompt_tokens": 9, "response_tokens_est": 33, "n_tiles": 12, "output_text": "A baseball player is standing on the pitcher's mound, preparing to swing at the pitch, while a umpire and catcher are positioned behind him.", "error": null, "sys_before": {"cpu_percent": 41.8, "ram_used_mb": 24981.5, "ram_available_mb": 100790.7, "ram_percent": 19.9}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 24982.4, "ram_available_mb": 100789.7, "ram_percent": 19.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.92, "peak": 16.36, "min": 13.49}, "VDD_CPU_SOC_MSS": {"avg": 15.73, "peak": 17.32, "min": 14.57}, "VDD_GPU": {"avg": 34.99, "peak": 43.73, "min": 26.4}, "VIN": {"avg": 79.1, "peak": 118.87, "min": 62.07}}, "power_watts_avg": 34.99, "energy_joules_est": 95.42, "sample_count": 21, "duration_seconds": 2.727}, "timestamp": "2026-01-17T14:24:48.228005"}
{"image_index": 24, "image_name": "000000002153.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002153.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 3125.909, "latencies_ms": [3125.909], "images_per_second": 0.32, "prompt_tokens": 23, "response_tokens_est": 43, "n_tiles": 12, "output_text": "1. Batter\n2. Catcher\n3. umpire\n4. Fielder\n5. Pitcher\n6. Infielder\n7. Outfielder\n8. Home plate", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24982.4, "ram_available_mb": 100789.7, "ram_percent": 19.9}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 24983.4, "ram_available_mb": 100788.8, "ram_percent": 19.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.92, "peak": 16.26, "min": 14.0}, "VDD_CPU_SOC_MSS": {"avg": 15.62, "peak": 17.32, "min": 14.57}, "VDD_GPU": {"avg": 34.9, "peak": 44.91, "min": 26.01}, "VIN": {"avg": 79.28, "peak": 137.7, "min": 65.56}}, "power_watts_avg": 34.9, "energy_joules_est": 109.11, "sample_count": 24, "duration_seconds": 3.126}, "timestamp": "2026-01-17T14:24:51.361427"}
{"image_index": 24, "image_name": "000000002153.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002153.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 4392.603, "latencies_ms": [4392.603], "images_per_second": 0.228, "prompt_tokens": 27, "response_tokens_est": 80, "n_tiles": 12, "output_text": "The main object in the foreground is the batter, who is positioned on the pitcher's mound. The umpire is standing near the batter, ready to make a call. The catcher is crouched behind the umpire, ready to catch the ball. The background features the outfield and the infield, with the outfield fence and the infield dirt clearly visible.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24983.4, "ram_available_mb": 100788.8, "ram_percent": 19.9}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 24986.0, "ram_available_mb": 100786.1, "ram_percent": 19.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.86, "peak": 16.26, "min": 13.92}, "VDD_CPU_SOC_MSS": {"avg": 15.55, "peak": 17.32, "min": 14.57}, "VDD_GPU": {"avg": 32.08, "peak": 44.91, "min": 25.6}, "VIN": {"avg": 74.87, "peak": 123.91, "min": 55.06}}, "power_watts_avg": 32.08, "energy_joules_est": 140.93, "sample_count": 35, "duration_seconds": 4.393}, "timestamp": "2026-01-17T14:24:55.760899"}
{"image_index": 24, "image_name": "000000002153.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002153.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 4462.465, "latencies_ms": [4462.465], "images_per_second": 0.224, "prompt_tokens": 21, "response_tokens_est": 82, "n_tiles": 12, "output_text": "The image captures a baseball game in progress, with a batter at the plate preparing to swing at a pitch. The batter is wearing a red helmet and a white uniform with red accents, while the catcher and umpire are positioned behind him, ready to catch the ball. The scene takes place on a baseball field with a green outfield and a dirt infield, surrounded by a mesh fence.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24986.0, "ram_available_mb": 100786.1, "ram_percent": 19.9}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 24987.3, "ram_available_mb": 100784.9, "ram_percent": 19.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.85, "peak": 16.36, "min": 13.92}, "VDD_CPU_SOC_MSS": {"avg": 15.57, "peak": 17.32, "min": 14.57}, "VDD_GPU": {"avg": 31.96, "peak": 44.91, "min": 25.22}, "VIN": {"avg": 76.19, "peak": 113.36, "min": 63.02}}, "power_watts_avg": 31.96, "energy_joules_est": 142.64, "sample_count": 35, "duration_seconds": 4.463}, "timestamp": "2026-01-17T14:25:00.229913"}
{"image_index": 24, "image_name": "000000002153.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002153.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3411.011, "latencies_ms": [3411.011], "images_per_second": 0.293, "prompt_tokens": 19, "response_tokens_est": 51, "n_tiles": 12, "output_text": "The image captures a baseball game in progress, with a player in a white uniform and red helmet preparing to swing at a pitch. The field is well-maintained with green grass and a dirt infield, and the lighting suggests it is daytime.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24987.3, "ram_available_mb": 100784.9, "ram_percent": 19.9}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 24988.2, "ram_available_mb": 100783.9, "ram_percent": 19.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.65, "peak": 16.05, "min": 13.69}, "VDD_CPU_SOC_MSS": {"avg": 15.22, "peak": 16.93, "min": 14.17}, "VDD_GPU": {"avg": 33.91, "peak": 45.32, "min": 25.22}, "VIN": {"avg": 76.09, "peak": 115.79, "min": 55.25}}, "power_watts_avg": 33.91, "energy_joules_est": 115.69, "sample_count": 27, "duration_seconds": 3.412}, "timestamp": "2026-01-17T14:25:03.647911"}
{"image_index": 25, "image_name": "000000002157.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002157.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2007.492, "latencies_ms": [2007.492], "images_per_second": 0.498, "prompt_tokens": 9, "response_tokens_est": 52, "n_tiles": 6, "output_text": "The image shows a beautifully arranged table with a variety of food items, including a large white cake with fresh berries, a selection of cheeses, a plate of grapes, and a glass of water, all set on a red tablecloth.", "error": null, "sys_before": {"cpu_percent": 40.0, "ram_used_mb": 25008.2, "ram_available_mb": 100764.0, "ram_percent": 19.9}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 25009.6, "ram_available_mb": 100762.5, "ram_percent": 19.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.49, "peak": 14.91, "min": 13.29}, "VDD_CPU_SOC_MSS": {"avg": 15.25, "peak": 15.76, "min": 13.78}, "VDD_GPU": {"avg": 30.36, "peak": 40.18, "min": 23.64}, "VIN": {"avg": 70.51, "peak": 89.01, "min": 59.33}}, "power_watts_avg": 30.36, "energy_joules_est": 60.96, "sample_count": 15, "duration_seconds": 2.008}, "timestamp": "2026-01-17T14:25:05.717628"}
{"image_index": 25, "image_name": "000000002157.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002157.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 4127.079, "latencies_ms": [4127.079], "images_per_second": 0.242, "prompt_tokens": 23, "response_tokens_est": 128, "n_tiles": 6, "output_text": "object: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject:", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25009.6, "ram_available_mb": 100762.5, "ram_percent": 19.9}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 25011.4, "ram_available_mb": 100760.8, "ram_percent": 19.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.84, "peak": 15.24, "min": 14.12}, "VDD_CPU_SOC_MSS": {"avg": 15.64, "peak": 16.14, "min": 14.57}, "VDD_GPU": {"avg": 26.68, "peak": 39.78, "min": 22.86}, "VIN": {"avg": 67.02, "peak": 99.28, "min": 56.82}}, "power_watts_avg": 26.68, "energy_joules_est": 110.12, "sample_count": 32, "duration_seconds": 4.128}, "timestamp": "2026-01-17T14:25:09.851065"}
{"image_index": 25, "image_name": "000000002157.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002157.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2819.365, "latencies_ms": [2819.365], "images_per_second": 0.355, "prompt_tokens": 27, "response_tokens_est": 80, "n_tiles": 6, "output_text": "The main object in the foreground is a white cake with a blueberry and raspberry topping. To the right of the cake, there is a plate with various cheeses, grapes, and bread. In the background, there are multiple glasses and plates, suggesting a dining setup. The cake is placed on a red tablecloth, and the grapes and cheese are arranged around it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25011.4, "ram_available_mb": 100760.8, "ram_percent": 19.9}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 25012.1, "ram_available_mb": 100760.1, "ram_percent": 19.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.8, "peak": 15.14, "min": 14.02}, "VDD_CPU_SOC_MSS": {"avg": 15.57, "peak": 15.76, "min": 14.57}, "VDD_GPU": {"avg": 28.32, "peak": 39.78, "min": 23.25}, "VIN": {"avg": 68.97, "peak": 109.56, "min": 61.64}}, "power_watts_avg": 28.32, "energy_joules_est": 79.86, "sample_count": 22, "duration_seconds": 2.82}, "timestamp": "2026-01-17T14:25:12.676788"}
{"image_index": 25, "image_name": "000000002157.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002157.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3082.834, "latencies_ms": [3082.834], "images_per_second": 0.324, "prompt_tokens": 21, "response_tokens_est": 90, "n_tiles": 6, "output_text": "The image depicts a well-arranged outdoor dining setup with a red tablecloth. The table is set with a variety of food items, including a large white cake with fresh berries, a plate of cheese, a bowl of grapes, and a selection of bread and crackers. The scene suggests a casual, yet elegant gathering, possibly a wedding or a celebratory event, with the focus on the delicious spread of food.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25012.1, "ram_available_mb": 100760.1, "ram_percent": 19.9}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 25012.8, "ram_available_mb": 100759.4, "ram_percent": 19.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.82, "peak": 15.24, "min": 14.12}, "VDD_CPU_SOC_MSS": {"avg": 15.58, "peak": 16.14, "min": 14.57}, "VDD_GPU": {"avg": 28.08, "peak": 39.78, "min": 23.25}, "VIN": {"avg": 69.74, "peak": 113.22, "min": 55.47}}, "power_watts_avg": 28.08, "energy_joules_est": 86.58, "sample_count": 23, "duration_seconds": 3.083}, "timestamp": "2026-01-17T14:25:15.769973"}
{"image_index": 25, "image_name": "000000002157.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002157.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2053.259, "latencies_ms": [2053.259], "images_per_second": 0.487, "prompt_tokens": 19, "response_tokens_est": 52, "n_tiles": 6, "output_text": "The image showcases a vibrant and colorful spread of food and drinks on a red tablecloth. The lighting is bright and natural, suggesting a sunny day, and the colors are vivid, with reds, blues, and greens dominating the scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25012.8, "ram_available_mb": 100759.4, "ram_percent": 19.9}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 25013.3, "ram_available_mb": 100758.9, "ram_percent": 19.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.73, "peak": 15.04, "min": 14.02}, "VDD_CPU_SOC_MSS": {"avg": 15.45, "peak": 15.75, "min": 14.57}, "VDD_GPU": {"avg": 30.34, "peak": 40.18, "min": 23.64}, "VIN": {"avg": 69.62, "peak": 91.95, "min": 54.49}}, "power_watts_avg": 30.34, "energy_joules_est": 62.31, "sample_count": 16, "duration_seconds": 2.054}, "timestamp": "2026-01-17T14:25:17.830033"}
{"image_index": 26, "image_name": "000000002261.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002261.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 953.684, "latencies_ms": [953.684], "images_per_second": 1.049, "prompt_tokens": 9, "response_tokens_est": 13, "n_tiles": 6, "output_text": "A man is surfing in the ocean, riding a wave.", "error": null, "sys_before": {"cpu_percent": 40.8, "ram_used_mb": 25013.3, "ram_available_mb": 100758.9, "ram_percent": 19.9}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 25013.5, "ram_available_mb": 100758.7, "ram_percent": 19.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.3, "peak": 14.83, "min": 13.69}, "VDD_CPU_SOC_MSS": {"avg": 15.19, "peak": 16.14, "min": 14.18}, "VDD_GPU": {"avg": 35.62, "peak": 39.78, "min": 31.13}, "VIN": {"avg": 76.75, "peak": 113.98, "min": 58.78}}, "power_watts_avg": 35.62, "energy_joules_est": 33.99, "sample_count": 7, "duration_seconds": 0.954}, "timestamp": "2026-01-17T14:25:18.843252"}
{"image_index": 26, "image_name": "000000002261.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002261.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1764.659, "latencies_ms": [1764.659], "images_per_second": 0.567, "prompt_tokens": 23, "response_tokens_est": 40, "n_tiles": 6, "output_text": "object: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25013.5, "ram_available_mb": 100758.7, "ram_percent": 19.9}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 25013.5, "ram_available_mb": 100758.7, "ram_percent": 19.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.53, "peak": 14.94, "min": 14.02}, "VDD_CPU_SOC_MSS": {"avg": 15.24, "peak": 15.75, "min": 14.57}, "VDD_GPU": {"avg": 32.67, "peak": 42.15, "min": 24.04}, "VIN": {"avg": 72.87, "peak": 115.22, "min": 50.41}}, "power_watts_avg": 32.67, "energy_joules_est": 57.71, "sample_count": 14, "duration_seconds": 1.767}, "timestamp": "2026-01-17T14:25:20.614833"}
{"image_index": 26, "image_name": "000000002261.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002261.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3300.87, "latencies_ms": [3300.87], "images_per_second": 0.303, "prompt_tokens": 27, "response_tokens_est": 94, "n_tiles": 6, "output_text": "The main object in the image is a person riding a surfboard in the water. The person is positioned in the foreground, slightly to the left, and is the closest to the viewer. The surfboard is also in the foreground, closer to the person. The background consists of the ocean, which is slightly blurred, indicating the depth of the water. The person is near the water's surface, and the surfboard is closer to the water's bottom.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25013.5, "ram_available_mb": 100758.7, "ram_percent": 19.9}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 25013.7, "ram_available_mb": 100758.4, "ram_percent": 19.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.62, "peak": 14.94, "min": 14.02}, "VDD_CPU_SOC_MSS": {"avg": 15.3, "peak": 15.75, "min": 14.57}, "VDD_GPU": {"avg": 27.43, "peak": 40.97, "min": 22.45}, "VIN": {"avg": 66.38, "peak": 89.74, "min": 52.9}}, "power_watts_avg": 27.43, "energy_joules_est": 90.56, "sample_count": 26, "duration_seconds": 3.302}, "timestamp": "2026-01-17T14:25:23.922731"}
{"image_index": 26, "image_name": "000000002261.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002261.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2248.001, "latencies_ms": [2248.001], "images_per_second": 0.445, "prompt_tokens": 21, "response_tokens_est": 57, "n_tiles": 6, "output_text": "The image depicts a man in a blue surfboard riding a wave in the ocean. The water is a vibrant green, indicating a shallow area near the shore. The man appears to be enjoying the ride, with his body partially submerged and his arms raised above his head.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25013.7, "ram_available_mb": 100758.4, "ram_percent": 19.9}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 25014.2, "ram_available_mb": 100757.9, "ram_percent": 19.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.57, "peak": 15.04, "min": 13.92}, "VDD_CPU_SOC_MSS": {"avg": 15.2, "peak": 15.75, "min": 14.18}, "VDD_GPU": {"avg": 29.36, "peak": 39.78, "min": 22.86}, "VIN": {"avg": 66.84, "peak": 89.73, "min": 54.24}}, "power_watts_avg": 29.36, "energy_joules_est": 66.01, "sample_count": 17, "duration_seconds": 2.248}, "timestamp": "2026-01-17T14:25:26.177083"}
{"image_index": 26, "image_name": "000000002261.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002261.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1738.263, "latencies_ms": [1738.263], "images_per_second": 0.575, "prompt_tokens": 19, "response_tokens_est": 39, "n_tiles": 6, "output_text": "The image depicts a person riding a surfboard in the ocean. The water is a vibrant green, and the surfboard is blue. The lighting is bright, indicating it is daytime.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25014.2, "ram_available_mb": 100757.9, "ram_percent": 19.9}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25014.1, "ram_available_mb": 100758.0, "ram_percent": 19.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.51, "peak": 14.94, "min": 14.0}, "VDD_CPU_SOC_MSS": {"avg": 15.21, "peak": 15.75, "min": 14.57}, "VDD_GPU": {"avg": 31.73, "peak": 40.57, "min": 24.04}, "VIN": {"avg": 75.29, "peak": 117.55, "min": 61.54}}, "power_watts_avg": 31.73, "energy_joules_est": 55.17, "sample_count": 13, "duration_seconds": 1.739}, "timestamp": "2026-01-17T14:25:27.921556"}
{"image_index": 27, "image_name": "000000002299.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002299.jpg", "image_width": 500, "image_height": 302, "image_resolution": "500x302", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1494.194, "latencies_ms": [1494.194], "images_per_second": 0.669, "prompt_tokens": 9, "response_tokens_est": 33, "n_tiles": 6, "output_text": "The image is a black and white photograph of a group of children, likely from a school, posing for a photo in front of a building with stone walls.", "error": null, "sys_before": {"cpu_percent": 41.6, "ram_used_mb": 25014.1, "ram_available_mb": 100758.0, "ram_percent": 19.9}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 25014.1, "ram_available_mb": 100758.0, "ram_percent": 19.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.4, "peak": 14.83, "min": 13.49}, "VDD_CPU_SOC_MSS": {"avg": 15.25, "peak": 16.14, "min": 14.18}, "VDD_GPU": {"avg": 32.52, "peak": 39.78, "min": 25.61}, "VIN": {"avg": 74.31, "peak": 108.28, "min": 58.02}}, "power_watts_avg": 32.52, "energy_joules_est": 48.61, "sample_count": 11, "duration_seconds": 1.495}, "timestamp": "2026-01-17T14:25:29.475583"}
{"image_index": 27, "image_name": "000000002299.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002299.jpg", "image_width": 500, "image_height": 302, "image_resolution": "500x302", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 4123.729, "latencies_ms": [4123.729], "images_per_second": 0.242, "prompt_tokens": 23, "response_tokens_est": 128, "n_tiles": 6, "output_text": "- 1 boy: 2\n- 1 girl: 2\n- 1 boy: 1\n- 1 girl: 1\n- 1 boy: 1\n- 1 girl: 1\n- 1 boy: 1\n- 1 girl: 1\n- 1 boy: 1\n- 1 girl: 1\n- 1 boy: 1\n- 1 girl: 1\n- 1 boy: 1\n- 1 girl: 1\n- 1 boy: 1\n- 1 girl: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25014.1, "ram_available_mb": 100758.0, "ram_percent": 19.9}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 25014.4, "ram_available_mb": 100757.8, "ram_percent": 19.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.87, "peak": 15.14, "min": 14.12}, "VDD_CPU_SOC_MSS": {"avg": 15.65, "peak": 16.14, "min": 14.57}, "VDD_GPU": {"avg": 26.97, "peak": 40.57, "min": 23.25}, "VIN": {"avg": 68.75, "peak": 113.15, "min": 58.77}}, "power_watts_avg": 26.97, "energy_joules_est": 111.23, "sample_count": 32, "duration_seconds": 4.124}, "timestamp": "2026-01-17T14:25:33.606426"}
{"image_index": 27, "image_name": "000000002299.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002299.jpg", "image_width": 500, "image_height": 302, "image_resolution": "500x302", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2168.622, "latencies_ms": [2168.622], "images_per_second": 0.461, "prompt_tokens": 27, "response_tokens_est": 56, "n_tiles": 6, "output_text": "The main objects in the image are the children, who are positioned in the foreground. The children are arranged in a semi-circular formation, with some sitting on the ground and others standing. The background features a stone wall, providing a solid and stable backdrop for the children.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25014.4, "ram_available_mb": 100757.8, "ram_percent": 19.9}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 25016.2, "ram_available_mb": 100756.0, "ram_percent": 19.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.77, "peak": 15.14, "min": 14.02}, "VDD_CPU_SOC_MSS": {"avg": 15.52, "peak": 15.76, "min": 14.57}, "VDD_GPU": {"avg": 29.99, "peak": 40.18, "min": 23.25}, "VIN": {"avg": 70.57, "peak": 112.36, "min": 49.26}}, "power_watts_avg": 29.99, "energy_joules_est": 65.05, "sample_count": 17, "duration_seconds": 2.169}, "timestamp": "2026-01-17T14:25:35.781009"}
{"image_index": 27, "image_name": "000000002299.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002299.jpg", "image_width": 500, "image_height": 302, "image_resolution": "500x302", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2701.758, "latencies_ms": [2701.758], "images_per_second": 0.37, "prompt_tokens": 21, "response_tokens_est": 76, "n_tiles": 6, "output_text": "The image depicts a group of children, likely in a school setting, posing for a group photo. The children are dressed in various school uniforms, and they are arranged in a semi-circular formation, with some sitting on the ground and others standing. The setting appears to be outdoors, possibly in a schoolyard or a park, with a stone wall in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25016.2, "ram_available_mb": 100756.0, "ram_percent": 19.9}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 25016.8, "ram_available_mb": 100755.4, "ram_percent": 19.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.81, "peak": 15.14, "min": 14.12}, "VDD_CPU_SOC_MSS": {"avg": 15.62, "peak": 16.14, "min": 14.57}, "VDD_GPU": {"avg": 28.71, "peak": 39.78, "min": 23.25}, "VIN": {"avg": 69.05, "peak": 113.51, "min": 58.52}}, "power_watts_avg": 28.71, "energy_joules_est": 77.58, "sample_count": 21, "duration_seconds": 2.702}, "timestamp": "2026-01-17T14:25:38.488987"}
{"image_index": 27, "image_name": "000000002299.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002299.jpg", "image_width": 500, "image_height": 302, "image_resolution": "500x302", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2377.134, "latencies_ms": [2377.134], "images_per_second": 0.421, "prompt_tokens": 19, "response_tokens_est": 64, "n_tiles": 6, "output_text": "The black and white photograph features a group of children in a school setting, with notable visual attributes such as a monochromatic color scheme, soft lighting, and a slightly grainy texture. The children are dressed in various school uniforms, and the background includes a stone wall, indicating a historical or educational context.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25016.8, "ram_available_mb": 100755.4, "ram_percent": 19.9}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 25017.3, "ram_available_mb": 100754.9, "ram_percent": 19.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.8, "peak": 15.24, "min": 14.12}, "VDD_CPU_SOC_MSS": {"avg": 15.55, "peak": 16.14, "min": 14.57}, "VDD_GPU": {"avg": 29.55, "peak": 39.78, "min": 23.25}, "VIN": {"avg": 71.51, "peak": 103.02, "min": 60.72}}, "power_watts_avg": 29.55, "energy_joules_est": 70.25, "sample_count": 18, "duration_seconds": 2.377}, "timestamp": "2026-01-17T14:25:40.872056"}
{"image_index": 28, "image_name": "000000002431.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002431.jpg", "image_width": 457, "image_height": 640, "image_resolution": "457x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2726.303, "latencies_ms": [2726.303], "images_per_second": 0.367, "prompt_tokens": 9, "response_tokens_est": 33, "n_tiles": 12, "output_text": "The image shows a plate of bread with a slice of bread on it, accompanied by a small bowl of sauce, a wine glass, and a napkin.", "error": null, "sys_before": {"cpu_percent": 41.7, "ram_used_mb": 24993.3, "ram_available_mb": 100778.9, "ram_percent": 19.9}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 24994.5, "ram_available_mb": 100777.7, "ram_percent": 19.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.92, "peak": 16.36, "min": 13.29}, "VDD_CPU_SOC_MSS": {"avg": 15.71, "peak": 17.72, "min": 14.18}, "VDD_GPU": {"avg": 34.91, "peak": 43.33, "min": 26.79}, "VIN": {"avg": 79.24, "peak": 124.9, "min": 62.97}}, "power_watts_avg": 34.91, "energy_joules_est": 95.19, "sample_count": 21, "duration_seconds": 2.727}, "timestamp": "2026-01-17T14:25:43.696657"}
{"image_index": 28, "image_name": "000000002431.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002431.jpg", "image_width": 457, "image_height": 640, "image_resolution": "457x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2654.554, "latencies_ms": [2654.554], "images_per_second": 0.377, "prompt_tokens": 23, "response_tokens_est": 29, "n_tiles": 12, "output_text": "bread: 4\nbowl: 1\nglass: 1\nspoon: 1\ntowel: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24994.5, "ram_available_mb": 100777.7, "ram_percent": 19.9}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 24996.2, "ram_available_mb": 100776.0, "ram_percent": 19.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.01, "peak": 16.36, "min": 13.9}, "VDD_CPU_SOC_MSS": {"avg": 15.69, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 36.64, "peak": 44.91, "min": 27.59}, "VIN": {"avg": 81.58, "peak": 124.25, "min": 61.69}}, "power_watts_avg": 36.64, "energy_joules_est": 97.28, "sample_count": 20, "duration_seconds": 2.655}, "timestamp": "2026-01-17T14:25:46.358774"}
{"image_index": 28, "image_name": "000000002431.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002431.jpg", "image_width": 457, "image_height": 640, "image_resolution": "457x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3984.378, "latencies_ms": [3984.378], "images_per_second": 0.251, "prompt_tokens": 27, "response_tokens_est": 68, "n_tiles": 12, "output_text": "The main objects in the image are a plate of bread, a glass of red wine, and a white bowl. The plate of bread is in the foreground, with the glass of red wine and the white bowl placed behind it. The glass of red wine is slightly out of focus, indicating it is farther away from the main objects.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24996.2, "ram_available_mb": 100776.0, "ram_percent": 19.9}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 24996.2, "ram_available_mb": 100776.0, "ram_percent": 19.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.9, "peak": 16.36, "min": 13.9}, "VDD_CPU_SOC_MSS": {"avg": 15.6, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 32.98, "peak": 45.3, "min": 25.6}, "VIN": {"avg": 75.07, "peak": 123.67, "min": 56.9}}, "power_watts_avg": 32.98, "energy_joules_est": 131.42, "sample_count": 31, "duration_seconds": 3.985}, "timestamp": "2026-01-17T14:25:50.349183"}
{"image_index": 28, "image_name": "000000002431.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002431.jpg", "image_width": 457, "image_height": 640, "image_resolution": "457x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 4078.553, "latencies_ms": [4078.553], "images_per_second": 0.245, "prompt_tokens": 21, "response_tokens_est": 71, "n_tiles": 12, "output_text": "The image depicts a dining setting with a focus on a plate of bread and a glass of red wine. The bread is placed on a white plate, and a white napkin is tied around the stem of a wine glass. The setting appears to be a restaurant or a dining area, with a blurred background suggesting other patrons and a table.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24996.2, "ram_available_mb": 100776.0, "ram_percent": 19.9}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 24996.7, "ram_available_mb": 100775.5, "ram_percent": 19.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.92, "peak": 16.36, "min": 13.9}, "VDD_CPU_SOC_MSS": {"avg": 15.58, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 32.75, "peak": 44.91, "min": 25.61}, "VIN": {"avg": 77.02, "peak": 137.37, "min": 57.02}}, "power_watts_avg": 32.75, "energy_joules_est": 133.59, "sample_count": 32, "duration_seconds": 4.079}, "timestamp": "2026-01-17T14:25:54.434170"}
{"image_index": 28, "image_name": "000000002431.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002431.jpg", "image_width": 457, "image_height": 640, "image_resolution": "457x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3191.986, "latencies_ms": [3191.986], "images_per_second": 0.313, "prompt_tokens": 19, "response_tokens_est": 45, "n_tiles": 12, "output_text": "The image features a rustic wooden table with a clear glass wine glass and a white plate with a slice of bread and a spoon resting on it. The lighting is warm and natural, casting a soft glow on the scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24996.7, "ram_available_mb": 100775.5, "ram_percent": 19.9}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 24997.3, "ram_available_mb": 100774.9, "ram_percent": 19.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.94, "peak": 16.36, "min": 13.9}, "VDD_CPU_SOC_MSS": {"avg": 15.64, "peak": 17.32, "min": 14.57}, "VDD_GPU": {"avg": 34.77, "peak": 45.3, "min": 26.01}, "VIN": {"avg": 74.5, "peak": 110.44, "min": 57.04}}, "power_watts_avg": 34.77, "energy_joules_est": 111.0, "sample_count": 25, "duration_seconds": 3.193}, "timestamp": "2026-01-17T14:25:57.633514"}
{"image_index": 29, "image_name": "000000002473.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002473.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1277.889, "latencies_ms": [1277.889], "images_per_second": 0.783, "prompt_tokens": 9, "response_tokens_est": 25, "n_tiles": 6, "output_text": "A snowboarder is performing a jump over a snowy slope, with trees and clear blue skies in the background.", "error": null, "sys_before": {"cpu_percent": 42.7, "ram_used_mb": 25013.9, "ram_available_mb": 100758.2, "ram_percent": 19.9}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25014.9, "ram_available_mb": 100757.3, "ram_percent": 19.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.33, "peak": 14.83, "min": 13.49}, "VDD_CPU_SOC_MSS": {"avg": 15.09, "peak": 15.75, "min": 14.18}, "VDD_GPU": {"avg": 34.54, "peak": 40.18, "min": 27.98}, "VIN": {"avg": 79.68, "peak": 117.15, "min": 61.46}}, "power_watts_avg": 34.54, "energy_joules_est": 44.16, "sample_count": 9, "duration_seconds": 1.278}, "timestamp": "2026-01-17T14:25:58.977910"}
{"image_index": 29, "image_name": "000000002473.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002473.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1790.419, "latencies_ms": [1790.419], "images_per_second": 0.559, "prompt_tokens": 23, "response_tokens_est": 41, "n_tiles": 6, "output_text": "1. Person skiing\n2. Person standing\n3. Person standing\n4. Person standing\n5. Person standing\n6. Person standing\n7. Person standing\n8. Person standing", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25014.9, "ram_available_mb": 100757.3, "ram_percent": 19.9}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25014.7, "ram_available_mb": 100757.5, "ram_percent": 19.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.69, "peak": 15.24, "min": 14.12}, "VDD_CPU_SOC_MSS": {"avg": 15.3, "peak": 15.74, "min": 14.57}, "VDD_GPU": {"avg": 31.94, "peak": 40.97, "min": 23.64}, "VIN": {"avg": 72.7, "peak": 108.0, "min": 57.41}}, "power_watts_avg": 31.94, "energy_joules_est": 57.2, "sample_count": 14, "duration_seconds": 1.791}, "timestamp": "2026-01-17T14:26:00.774066"}
{"image_index": 29, "image_name": "000000002473.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002473.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2510.769, "latencies_ms": [2510.769], "images_per_second": 0.398, "prompt_tokens": 27, "response_tokens_est": 66, "n_tiles": 6, "output_text": "The main objects in the image are a skier in mid-air, a snowboarder standing on the snow, and a group of people standing on the snowy slope. The skier is in the foreground, the snowboarder is in the background, and the group of people is near the bottom of the slope.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25014.7, "ram_available_mb": 100757.5, "ram_percent": 19.9}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 25016.1, "ram_available_mb": 100756.0, "ram_percent": 19.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.61, "peak": 14.94, "min": 14.02}, "VDD_CPU_SOC_MSS": {"avg": 15.28, "peak": 15.75, "min": 14.57}, "VDD_GPU": {"avg": 28.9, "peak": 40.97, "min": 22.86}, "VIN": {"avg": 67.09, "peak": 87.63, "min": 55.8}}, "power_watts_avg": 28.9, "energy_joules_est": 72.58, "sample_count": 20, "duration_seconds": 2.511}, "timestamp": "2026-01-17T14:26:03.291260"}
{"image_index": 29, "image_name": "000000002473.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002473.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1852.449, "latencies_ms": [1852.449], "images_per_second": 0.54, "prompt_tokens": 21, "response_tokens_est": 43, "n_tiles": 6, "output_text": "The image depicts a snowy mountainous landscape under a clear blue sky. A person is skiing downhill, performing a jump, while another person stands on the snowy slope, observing the action.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25016.1, "ram_available_mb": 100756.0, "ram_percent": 19.9}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25016.1, "ram_available_mb": 100756.0, "ram_percent": 19.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.56, "peak": 14.94, "min": 14.02}, "VDD_CPU_SOC_MSS": {"avg": 15.24, "peak": 15.75, "min": 14.57}, "VDD_GPU": {"avg": 30.9, "peak": 39.78, "min": 23.64}, "VIN": {"avg": 69.42, "peak": 109.01, "min": 61.31}}, "power_watts_avg": 30.9, "energy_joules_est": 57.26, "sample_count": 14, "duration_seconds": 1.853}, "timestamp": "2026-01-17T14:26:05.150123"}
{"image_index": 29, "image_name": "000000002473.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002473.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2083.345, "latencies_ms": [2083.345], "images_per_second": 0.48, "prompt_tokens": 19, "response_tokens_est": 51, "n_tiles": 6, "output_text": "The image depicts a snowy landscape with a clear blue sky. The snow-covered ground is bright white, and there are several snow-covered trees in the background. The lighting is bright and sunny, casting a clear reflection on the snow.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25016.1, "ram_available_mb": 100756.0, "ram_percent": 19.9}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 25015.8, "ram_available_mb": 100756.3, "ram_percent": 19.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.56, "peak": 14.94, "min": 14.02}, "VDD_CPU_SOC_MSS": {"avg": 15.26, "peak": 15.74, "min": 14.57}, "VDD_GPU": {"avg": 30.16, "peak": 40.57, "min": 23.25}, "VIN": {"avg": 69.15, "peak": 116.11, "min": 60.97}}, "power_watts_avg": 30.16, "energy_joules_est": 62.85, "sample_count": 16, "duration_seconds": 2.084}, "timestamp": "2026-01-17T14:26:07.239821"}
{"image_index": 30, "image_name": "000000002532.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002532.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2556.525, "latencies_ms": [2556.525], "images_per_second": 0.391, "prompt_tokens": 9, "response_tokens_est": 28, "n_tiles": 12, "output_text": "A person is skiing down a snowy mountain slope, with their skis cutting through the snow and their poles guiding their movement.", "error": null, "sys_before": {"cpu_percent": 42.3, "ram_used_mb": 24984.9, "ram_available_mb": 100787.3, "ram_percent": 19.9}, "sys_after": {"cpu_percent": 2.6, "ram_used_mb": 24986.1, "ram_available_mb": 100786.1, "ram_percent": 19.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.92, "peak": 16.46, "min": 13.19}, "VDD_CPU_SOC_MSS": {"avg": 15.73, "peak": 17.32, "min": 14.18}, "VDD_GPU": {"avg": 35.75, "peak": 43.33, "min": 27.19}, "VIN": {"avg": 80.01, "peak": 120.05, "min": 57.6}}, "power_watts_avg": 35.75, "energy_joules_est": 91.41, "sample_count": 19, "duration_seconds": 2.557}, "timestamp": "2026-01-17T14:26:09.894488"}
{"image_index": 30, "image_name": "000000002532.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002532.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 3026.028, "latencies_ms": [3026.028], "images_per_second": 0.33, "prompt_tokens": 23, "response_tokens_est": 40, "n_tiles": 12, "output_text": "1. Skier\n2. Snow\n3. Ski poles\n4. Snowboard\n5. Snow\n6. Snowboard tracks\n7. Snowboard\n8. Snowboard tracks", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24986.1, "ram_available_mb": 100786.1, "ram_percent": 19.9}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 24987.6, "ram_available_mb": 100784.6, "ram_percent": 19.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.01, "peak": 16.36, "min": 14.02}, "VDD_CPU_SOC_MSS": {"avg": 15.66, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 35.25, "peak": 44.51, "min": 26.4}, "VIN": {"avg": 79.88, "peak": 110.22, "min": 65.26}}, "power_watts_avg": 35.25, "energy_joules_est": 106.68, "sample_count": 23, "duration_seconds": 3.026}, "timestamp": "2026-01-17T14:26:12.926711"}
{"image_index": 30, "image_name": "000000002532.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002532.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3778.993, "latencies_ms": [3778.993], "images_per_second": 0.265, "prompt_tokens": 27, "response_tokens_est": 62, "n_tiles": 12, "output_text": "The main objects in the image are a person skiing and a snowy landscape. The person is positioned in the foreground, with their back to the camera, while the snowy landscape stretches out in the background. The person is near the center of the image, with the snowy terrain surrounding them.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24987.6, "ram_available_mb": 100784.6, "ram_percent": 19.9}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 24988.7, "ram_available_mb": 100783.5, "ram_percent": 19.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.95, "peak": 16.36, "min": 13.9}, "VDD_CPU_SOC_MSS": {"avg": 15.61, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 33.14, "peak": 45.3, "min": 25.61}, "VIN": {"avg": 73.47, "peak": 106.87, "min": 55.33}}, "power_watts_avg": 33.14, "energy_joules_est": 125.26, "sample_count": 30, "duration_seconds": 3.78}, "timestamp": "2026-01-17T14:26:16.712150"}
{"image_index": 30, "image_name": "000000002532.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002532.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 4342.474, "latencies_ms": [4342.474], "images_per_second": 0.23, "prompt_tokens": 21, "response_tokens_est": 79, "n_tiles": 12, "output_text": "The image depicts a snowy landscape with a person standing on skis, holding ski poles. The person is dressed in a green long-sleeved top and black pants, and is surrounded by snow-covered terrain with a few trees and rocks visible. The sky is partly cloudy, and the overall scene suggests a winter day with a person engaging in skiing or snowboarding.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24988.7, "ram_available_mb": 100783.5, "ram_percent": 19.9}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 24988.6, "ram_available_mb": 100783.6, "ram_percent": 19.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.97, "peak": 16.36, "min": 14.02}, "VDD_CPU_SOC_MSS": {"avg": 15.56, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 32.31, "peak": 44.91, "min": 25.6}, "VIN": {"avg": 73.91, "peak": 121.69, "min": 57.64}}, "power_watts_avg": 32.31, "energy_joules_est": 140.32, "sample_count": 34, "duration_seconds": 4.343}, "timestamp": "2026-01-17T14:26:21.061694"}
{"image_index": 30, "image_name": "000000002532.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002532.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2920.496, "latencies_ms": [2920.496], "images_per_second": 0.342, "prompt_tokens": 19, "response_tokens_est": 37, "n_tiles": 12, "output_text": "The image depicts a snowy landscape under a clear blue sky with scattered white clouds. The scene is illuminated by natural sunlight, casting shadows on the snow-covered ground.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24988.6, "ram_available_mb": 100783.6, "ram_percent": 19.9}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 24988.6, "ram_available_mb": 100783.6, "ram_percent": 19.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.01, "peak": 16.36, "min": 14.02}, "VDD_CPU_SOC_MSS": {"avg": 15.68, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 35.65, "peak": 44.91, "min": 26.39}, "VIN": {"avg": 76.0, "peak": 107.98, "min": 54.17}}, "power_watts_avg": 35.65, "energy_joules_est": 104.13, "sample_count": 22, "duration_seconds": 2.921}, "timestamp": "2026-01-17T14:26:23.988528"}
{"image_index": 31, "image_name": "000000002587.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002587.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2720.687, "latencies_ms": [2720.687], "images_per_second": 0.368, "prompt_tokens": 9, "response_tokens_est": 33, "n_tiles": 12, "output_text": "The image shows a close-up of a brown, round object with a smooth surface, which appears to be a chocolate-covered donut or a similar treat.", "error": null, "sys_before": {"cpu_percent": 41.2, "ram_used_mb": 24976.3, "ram_available_mb": 100795.9, "ram_percent": 19.9}, "sys_after": {"cpu_percent": 2.6, "ram_used_mb": 24973.2, "ram_available_mb": 100798.9, "ram_percent": 19.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.98, "peak": 16.46, "min": 13.39}, "VDD_CPU_SOC_MSS": {"avg": 15.75, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 35.48, "peak": 44.12, "min": 26.8}, "VIN": {"avg": 80.9, "peak": 115.32, "min": 57.87}}, "power_watts_avg": 35.48, "energy_joules_est": 96.54, "sample_count": 21, "duration_seconds": 2.721}, "timestamp": "2026-01-17T14:26:26.798178"}
{"image_index": 31, "image_name": "000000002587.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002587.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2042.727, "latencies_ms": [2042.727], "images_per_second": 0.49, "prompt_tokens": 23, "response_tokens_est": 11, "n_tiles": 12, "output_text": "apple: 1\nbanana: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24973.2, "ram_available_mb": 100798.9, "ram_percent": 19.9}, "sys_after": {"cpu_percent": 1.9, "ram_used_mb": 24975.9, "ram_available_mb": 100796.2, "ram_percent": 19.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.12, "peak": 16.46, "min": 14.12}, "VDD_CPU_SOC_MSS": {"avg": 15.8, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 39.26, "peak": 45.3, "min": 31.92}, "VIN": {"avg": 85.6, "peak": 124.24, "min": 66.83}}, "power_watts_avg": 39.26, "energy_joules_est": 80.22, "sample_count": 15, "duration_seconds": 2.043}, "timestamp": "2026-01-17T14:26:28.847010"}
{"image_index": 31, "image_name": "000000002587.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002587.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3572.829, "latencies_ms": [3572.829], "images_per_second": 0.28, "prompt_tokens": 27, "response_tokens_est": 56, "n_tiles": 12, "output_text": "The main object in the foreground is a banana, which is yellow and slightly curved. The banana is placed on a dark surface, likely a table or countertop. In the background, there is a blurred object that appears to be a bag or container, partially visible.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24975.9, "ram_available_mb": 100796.2, "ram_percent": 19.9}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 24976.9, "ram_available_mb": 100795.3, "ram_percent": 19.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.02, "peak": 16.36, "min": 14.12}, "VDD_CPU_SOC_MSS": {"avg": 15.64, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 34.26, "peak": 46.48, "min": 25.6}, "VIN": {"avg": 73.93, "peak": 108.67, "min": 60.65}}, "power_watts_avg": 34.26, "energy_joules_est": 122.43, "sample_count": 28, "duration_seconds": 3.574}, "timestamp": "2026-01-17T14:26:32.426917"}
{"image_index": 31, "image_name": "000000002587.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002587.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3567.692, "latencies_ms": [3567.692], "images_per_second": 0.28, "prompt_tokens": 21, "response_tokens_est": 56, "n_tiles": 12, "output_text": "The image shows a close-up of a brown, round object with a glossy surface, which appears to be a chocolate bar or a similar treat. The background is dark, and the focus is on the object, making it difficult to discern any additional details about the setting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24976.9, "ram_available_mb": 100795.3, "ram_percent": 19.9}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 24978.1, "ram_available_mb": 100794.0, "ram_percent": 19.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.99, "peak": 16.36, "min": 13.92}, "VDD_CPU_SOC_MSS": {"avg": 15.62, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 33.8, "peak": 45.3, "min": 25.61}, "VIN": {"avg": 72.32, "peak": 115.21, "min": 56.9}}, "power_watts_avg": 33.8, "energy_joules_est": 120.62, "sample_count": 28, "duration_seconds": 3.569}, "timestamp": "2026-01-17T14:26:36.002496"}
{"image_index": 31, "image_name": "000000002587.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002587.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3325.131, "latencies_ms": [3325.131], "images_per_second": 0.301, "prompt_tokens": 19, "response_tokens_est": 49, "n_tiles": 12, "output_text": "The image features a close-up of a chocolate-covered banana, which is a common treat. The lighting is dim, casting a warm glow on the banana, and the background is blurred, emphasizing the smooth texture of the banana.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24978.1, "ram_available_mb": 100794.0, "ram_percent": 19.9}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 24978.8, "ram_available_mb": 100793.3, "ram_percent": 19.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.01, "peak": 16.46, "min": 13.9}, "VDD_CPU_SOC_MSS": {"avg": 15.66, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 34.5, "peak": 45.3, "min": 26.01}, "VIN": {"avg": 77.69, "peak": 120.58, "min": 57.87}}, "power_watts_avg": 34.5, "energy_joules_est": 114.74, "sample_count": 26, "duration_seconds": 3.326}, "timestamp": "2026-01-17T14:26:39.334402"}
{"image_index": 32, "image_name": "000000002592.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002592.jpg", "image_width": 640, "image_height": 366, "image_resolution": "640x366", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1466.444, "latencies_ms": [1466.444], "images_per_second": 0.682, "prompt_tokens": 9, "response_tokens_est": 32, "n_tiles": 6, "output_text": "The image shows a white mug with a black skull and crossbones design, and a black knife with a silver blade lying on a textured surface.", "error": null, "sys_before": {"cpu_percent": 43.8, "ram_used_mb": 25006.6, "ram_available_mb": 100765.6, "ram_percent": 19.9}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 25006.2, "ram_available_mb": 100766.0, "ram_percent": 19.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.49, "peak": 14.94, "min": 13.49}, "VDD_CPU_SOC_MSS": {"avg": 15.21, "peak": 15.75, "min": 14.18}, "VDD_GPU": {"avg": 32.99, "peak": 40.18, "min": 25.61}, "VIN": {"avg": 78.09, "peak": 117.31, "min": 59.71}}, "power_watts_avg": 32.99, "energy_joules_est": 48.39, "sample_count": 11, "duration_seconds": 1.467}, "timestamp": "2026-01-17T14:26:40.867726"}
{"image_index": 32, "image_name": "000000002592.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002592.jpg", "image_width": 640, "image_height": 366, "image_resolution": "640x366", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1766.37, "latencies_ms": [1766.37], "images_per_second": 0.566, "prompt_tokens": 23, "response_tokens_est": 40, "n_tiles": 6, "output_text": "1. Skull and crossed bones on mug\n2. Knife\n3. Mug\n4. Knife\n5. Mug\n6. Knife\n7. Mug\n8. Knife", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25006.2, "ram_available_mb": 100766.0, "ram_percent": 19.9}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25006.9, "ram_available_mb": 100765.3, "ram_percent": 19.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.64, "peak": 15.04, "min": 14.12}, "VDD_CPU_SOC_MSS": {"avg": 15.26, "peak": 15.74, "min": 14.57}, "VDD_GPU": {"avg": 32.34, "peak": 40.97, "min": 24.43}, "VIN": {"avg": 69.71, "peak": 96.87, "min": 57.66}}, "power_watts_avg": 32.34, "energy_joules_est": 57.14, "sample_count": 13, "duration_seconds": 1.767}, "timestamp": "2026-01-17T14:26:42.640132"}
{"image_index": 32, "image_name": "000000002592.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002592.jpg", "image_width": 640, "image_height": 366, "image_resolution": "640x366", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2678.084, "latencies_ms": [2678.084], "images_per_second": 0.373, "prompt_tokens": 27, "response_tokens_est": 72, "n_tiles": 6, "output_text": "The main objects in the image are a white mug with a skull and crossed bones design, and a black knife with a serrated edge. The white mug is positioned in the foreground, while the black knife is located in the background. The mug is placed on a surface with a striped pattern, and the knife is lying parallel to the mug.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25007.1, "ram_available_mb": 100765.1, "ram_percent": 19.9}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 25007.6, "ram_available_mb": 100764.6, "ram_percent": 19.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.71, "peak": 15.04, "min": 14.1}, "VDD_CPU_SOC_MSS": {"avg": 15.3, "peak": 15.74, "min": 14.57}, "VDD_GPU": {"avg": 28.57, "peak": 40.18, "min": 22.86}, "VIN": {"avg": 67.98, "peak": 111.36, "min": 59.16}}, "power_watts_avg": 28.57, "energy_joules_est": 76.53, "sample_count": 21, "duration_seconds": 2.679}, "timestamp": "2026-01-17T14:26:45.324234"}
{"image_index": 32, "image_name": "000000002592.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002592.jpg", "image_width": 640, "image_height": 366, "image_resolution": "640x366", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2446.718, "latencies_ms": [2446.718], "images_per_second": 0.409, "prompt_tokens": 21, "response_tokens_est": 64, "n_tiles": 6, "output_text": "The image shows a white mug with a black skull and crossbones design on it, placed on a surface with a striped pattern. To the right of the mug, there is a black knife with a silver blade lying on the same surface. The mug and knife are the only objects visible in the image.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25007.6, "ram_available_mb": 100764.6, "ram_percent": 19.9}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 25007.8, "ram_available_mb": 100764.3, "ram_percent": 19.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.68, "peak": 15.04, "min": 14.1}, "VDD_CPU_SOC_MSS": {"avg": 15.29, "peak": 15.75, "min": 14.57}, "VDD_GPU": {"avg": 29.05, "peak": 40.57, "min": 22.86}, "VIN": {"avg": 70.82, "peak": 112.51, "min": 55.22}}, "power_watts_avg": 29.05, "energy_joules_est": 71.09, "sample_count": 19, "duration_seconds": 2.447}, "timestamp": "2026-01-17T14:26:47.776975"}
{"image_index": 32, "image_name": "000000002592.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002592.jpg", "image_width": 640, "image_height": 366, "image_resolution": "640x366", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2163.442, "latencies_ms": [2163.442], "images_per_second": 0.462, "prompt_tokens": 19, "response_tokens_est": 54, "n_tiles": 6, "output_text": "The image features a white mug with a black skull and crossbones design, and a black knife with a silver blade. The mug and knife are placed on a textured surface, and the lighting is soft and diffused, creating a calm and neutral atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25007.8, "ram_available_mb": 100764.3, "ram_percent": 19.9}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 25009.1, "ram_available_mb": 100763.1, "ram_percent": 19.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.64, "peak": 14.94, "min": 14.12}, "VDD_CPU_SOC_MSS": {"avg": 15.29, "peak": 15.75, "min": 14.57}, "VDD_GPU": {"avg": 29.83, "peak": 40.97, "min": 23.25}, "VIN": {"avg": 68.69, "peak": 96.71, "min": 58.89}}, "power_watts_avg": 29.83, "energy_joules_est": 64.55, "sample_count": 17, "duration_seconds": 2.164}, "timestamp": "2026-01-17T14:26:49.946652"}
{"image_index": 33, "image_name": "000000002685.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002685.jpg", "image_width": 640, "image_height": 555, "image_resolution": "640x555", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 829.317, "latencies_ms": [829.317], "images_per_second": 1.206, "prompt_tokens": 9, "response_tokens_est": 34, "n_tiles": 1, "output_text": "A group of people is gathered around a bar counter, with one person in a gray suit and glasses standing at the counter, seemingly engaged in conversation with the others.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25008.8, "ram_available_mb": 100763.4, "ram_percent": 19.9}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 25009.3, "ram_available_mb": 100762.9, "ram_percent": 19.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 4845.7, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.86, "peak": 15.22, "min": 14.3}, "VDD_CPU_SOC_MSS": {"avg": 15.88, "peak": 16.54, "min": 15.36}, "VDD_GPU": {"avg": 22.52, "peak": 25.21, "min": 20.89}, "VIN": {"avg": 61.48, "peak": 64.55, "min": 59.58}}, "power_watts_avg": 22.52, "energy_joules_est": 18.69, "sample_count": 6, "duration_seconds": 0.83}, "timestamp": "2026-01-17T14:26:50.799853"}
{"image_index": 33, "image_name": "000000002685.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002685.jpg", "image_width": 640, "image_height": 555, "image_resolution": "640x555", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2877.609, "latencies_ms": [2877.609], "images_per_second": 0.348, "prompt_tokens": 23, "response_tokens_est": 129, "n_tiles": 1, "output_text": "- man: 3\n- woman: 2\n- man: 2\n- woman: 2\n- man: 1\n- woman: 1\n- man: 1\n- woman: 1\n- man: 1\n- woman: 1\n- man: 1\n- woman: 1\n- man: 1\n- woman: 1\n- man: 1\n- woman: 1\n- man: 1\n- woman: 1\n- man: 1\n- woman: 1\n- man: 1\n- woman", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25009.3, "ram_available_mb": 100762.9, "ram_percent": 19.9}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25010.1, "ram_available_mb": 100762.1, "ram_percent": 19.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 4856.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.42, "peak": 15.72, "min": 14.91}, "VDD_CPU_SOC_MSS": {"avg": 16.57, "peak": 16.94, "min": 16.14}, "VDD_GPU": {"avg": 20.46, "peak": 24.43, "min": 19.7}, "VIN": {"avg": 62.86, "peak": 68.61, "min": 55.47}}, "power_watts_avg": 20.46, "energy_joules_est": 58.89, "sample_count": 22, "duration_seconds": 2.878}, "timestamp": "2026-01-17T14:26:53.683754"}
{"image_index": 33, "image_name": "000000002685.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002685.jpg", "image_width": 640, "image_height": 555, "image_resolution": "640x555", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2151.304, "latencies_ms": [2151.304], "images_per_second": 0.465, "prompt_tokens": 27, "response_tokens_est": 95, "n_tiles": 1, "output_text": "In the image, the main objects are the people and the wine display. The people are positioned in the foreground, with the man in the light blue shirt and the woman in the black top standing near the wine display. The wine bottles are arranged on the counter, and the display case is located to the left of the counter. The background features a wooden counter and shelves with various items, while the far right side shows a person standing near a counter with a laptop.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25010.1, "ram_available_mb": 100762.1, "ram_percent": 19.9}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25010.5, "ram_available_mb": 100761.6, "ram_percent": 19.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 4859.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.5, "peak": 15.62, "min": 15.22}, "VDD_CPU_SOC_MSS": {"avg": 16.52, "peak": 16.94, "min": 16.14}, "VDD_GPU": {"avg": 20.54, "peak": 23.64, "min": 19.7}, "VIN": {"avg": 62.66, "peak": 68.23, "min": 55.74}}, "power_watts_avg": 20.54, "energy_joules_est": 44.2, "sample_count": 17, "duration_seconds": 2.152}, "timestamp": "2026-01-17T14:26:55.840820"}
{"image_index": 33, "image_name": "000000002685.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002685.jpg", "image_width": 640, "image_height": 555, "image_resolution": "640x555", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1512.514, "latencies_ms": [1512.514], "images_per_second": 0.661, "prompt_tokens": 21, "response_tokens_est": 66, "n_tiles": 1, "output_text": "The scene depicts a bustling wine tasting event taking place in a cozy, well-lit room with wooden floors and walls. A group of people, including a man in a gray suit and glasses, are engaged in a lively discussion around a wooden bar counter, surrounded by various wine bottles and a display of wine glasses.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25010.5, "ram_available_mb": 100761.6, "ram_percent": 19.9}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25010.3, "ram_available_mb": 100761.9, "ram_percent": 19.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 4854.7, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.41, "peak": 15.52, "min": 15.01}, "VDD_CPU_SOC_MSS": {"avg": 16.61, "peak": 16.94, "min": 16.15}, "VDD_GPU": {"avg": 20.89, "peak": 23.64, "min": 19.7}, "VIN": {"avg": 64.06, "peak": 73.71, "min": 60.53}}, "power_watts_avg": 20.89, "energy_joules_est": 31.61, "sample_count": 11, "duration_seconds": 1.513}, "timestamp": "2026-01-17T14:26:57.363362"}
{"image_index": 33, "image_name": "000000002685.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002685.jpg", "image_width": 640, "image_height": 555, "image_resolution": "640x555", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1052.741, "latencies_ms": [1052.741], "images_per_second": 0.95, "prompt_tokens": 19, "response_tokens_est": 45, "n_tiles": 1, "output_text": "The room is painted in a soft teal color, with a warm, ambient lighting that creates a cozy atmosphere. The wooden counter and bar stools are made of light-colored wood, complementing the overall color scheme.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25010.3, "ram_available_mb": 100761.9, "ram_percent": 19.9}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25010.8, "ram_available_mb": 100761.4, "ram_percent": 19.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 4853.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.35, "peak": 15.62, "min": 14.81}, "VDD_CPU_SOC_MSS": {"avg": 16.44, "peak": 16.54, "min": 16.15}, "VDD_GPU": {"avg": 21.33, "peak": 23.64, "min": 20.1}, "VIN": {"avg": 61.64, "peak": 63.0, "min": 59.52}}, "power_watts_avg": 21.33, "energy_joules_est": 22.46, "sample_count": 8, "duration_seconds": 1.053}, "timestamp": "2026-01-17T14:26:58.421832"}
{"image_index": 34, "image_name": "000000002923.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002923.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2546.509, "latencies_ms": [2546.509], "images_per_second": 0.393, "prompt_tokens": 9, "response_tokens_est": 27, "n_tiles": 12, "output_text": "A large white bird is flying in the sky above a grassy field with a few boats and ships docked in the background.", "error": null, "sys_before": {"cpu_percent": 48.8, "ram_used_mb": 24975.8, "ram_available_mb": 100796.4, "ram_percent": 19.9}, "sys_after": {"cpu_percent": 2.5, "ram_used_mb": 24974.7, "ram_available_mb": 100797.5, "ram_percent": 19.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.09, "peak": 16.46, "min": 14.1}, "VDD_CPU_SOC_MSS": {"avg": 15.89, "peak": 17.71, "min": 14.96}, "VDD_GPU": {"avg": 35.58, "peak": 42.94, "min": 27.19}, "VIN": {"avg": 82.52, "peak": 112.56, "min": 62.61}}, "power_watts_avg": 35.58, "energy_joules_est": 90.62, "sample_count": 19, "duration_seconds": 2.547}, "timestamp": "2026-01-17T14:27:01.032974"}
{"image_index": 34, "image_name": "000000002923.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002923.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2579.991, "latencies_ms": [2579.991], "images_per_second": 0.388, "prompt_tokens": 23, "response_tokens_est": 27, "n_tiles": 12, "output_text": "bird: 1\nboat: 1\nbuilding: 1\ntower: 1\ntube: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24974.7, "ram_available_mb": 100797.5, "ram_percent": 19.9}, "sys_after": {"cpu_percent": 2.5, "ram_used_mb": 24975.5, "ram_available_mb": 100796.7, "ram_percent": 19.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.82, "peak": 16.15, "min": 13.72}, "VDD_CPU_SOC_MSS": {"avg": 15.39, "peak": 16.93, "min": 14.17}, "VDD_GPU": {"avg": 36.67, "peak": 45.3, "min": 27.19}, "VIN": {"avg": 80.47, "peak": 124.68, "min": 64.88}}, "power_watts_avg": 36.67, "energy_joules_est": 94.63, "sample_count": 20, "duration_seconds": 2.581}, "timestamp": "2026-01-17T14:27:03.620126"}
{"image_index": 34, "image_name": "000000002923.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002923.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3275.437, "latencies_ms": [3275.437], "images_per_second": 0.305, "prompt_tokens": 27, "response_tokens_est": 47, "n_tiles": 12, "output_text": "The main objects in the image are a large field of tall grass, a white bird in flight, and a docked boat in the background. The bird is in the foreground, while the docked boat is in the background.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 24975.5, "ram_available_mb": 100796.7, "ram_percent": 19.9}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 24976.0, "ram_available_mb": 100796.2, "ram_percent": 19.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.75, "peak": 16.15, "min": 13.82}, "VDD_CPU_SOC_MSS": {"avg": 15.29, "peak": 16.92, "min": 14.17}, "VDD_GPU": {"avg": 34.83, "peak": 45.69, "min": 25.61}, "VIN": {"avg": 73.67, "peak": 116.87, "min": 62.16}}, "power_watts_avg": 34.83, "energy_joules_est": 114.1, "sample_count": 25, "duration_seconds": 3.276}, "timestamp": "2026-01-17T14:27:06.905954"}
{"image_index": 34, "image_name": "000000002923.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002923.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 4177.393, "latencies_ms": [4177.393], "images_per_second": 0.239, "prompt_tokens": 21, "response_tokens_est": 73, "n_tiles": 12, "output_text": "The image depicts a serene coastal scene with a large expanse of green grass in the foreground, likely a marsh or wetland area. In the background, there is a dock with various boats and fishing equipment, and a cloudy sky above. A white bird is seen flying in the distance, adding a dynamic element to the tranquil setting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24976.0, "ram_available_mb": 100796.2, "ram_percent": 19.9}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 24976.5, "ram_available_mb": 100795.7, "ram_percent": 19.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.7, "peak": 16.15, "min": 13.69}, "VDD_CPU_SOC_MSS": {"avg": 15.21, "peak": 16.93, "min": 14.17}, "VDD_GPU": {"avg": 32.62, "peak": 45.32, "min": 25.21}, "VIN": {"avg": 74.76, "peak": 123.8, "min": 54.41}}, "power_watts_avg": 32.62, "energy_joules_est": 136.28, "sample_count": 32, "duration_seconds": 4.178}, "timestamp": "2026-01-17T14:27:11.089812"}
{"image_index": 34, "image_name": "000000002923.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002923.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3363.527, "latencies_ms": [3363.527], "images_per_second": 0.297, "prompt_tokens": 19, "response_tokens_est": 50, "n_tiles": 12, "output_text": "The image depicts a serene coastal scene with a mix of green and brown hues, indicating a mix of vegetation and sandy terrain. The sky is overcast with a mix of gray and white clouds, suggesting a cloudy day.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24976.5, "ram_available_mb": 100795.7, "ram_percent": 19.9}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 24976.5, "ram_available_mb": 100795.7, "ram_percent": 19.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.77, "peak": 16.15, "min": 13.69}, "VDD_CPU_SOC_MSS": {"avg": 15.31, "peak": 16.92, "min": 14.17}, "VDD_GPU": {"avg": 34.14, "peak": 45.3, "min": 25.6}, "VIN": {"avg": 72.8, "peak": 115.92, "min": 58.0}}, "power_watts_avg": 34.14, "energy_joules_est": 114.84, "sample_count": 26, "duration_seconds": 3.364}, "timestamp": "2026-01-17T14:27:14.459666"}
{"image_index": 35, "image_name": "000000003156.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000003156.jpg", "image_width": 443, "image_height": 640, "image_resolution": "443x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1248.908, "latencies_ms": [1248.908], "images_per_second": 0.801, "prompt_tokens": 9, "response_tokens_est": 24, "n_tiles": 6, "output_text": "A young man is sitting on the toilet, wearing a dark shirt and jeans, with his hands on the seat.", "error": null, "sys_before": {"cpu_percent": 44.3, "ram_used_mb": 25007.8, "ram_available_mb": 100764.3, "ram_percent": 19.9}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 25009.1, "ram_available_mb": 100763.1, "ram_percent": 19.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.41, "peak": 14.94, "min": 13.51}, "VDD_CPU_SOC_MSS": {"avg": 15.05, "peak": 15.75, "min": 14.18}, "VDD_GPU": {"avg": 34.4, "peak": 40.18, "min": 27.57}, "VIN": {"avg": 74.66, "peak": 119.48, "min": 56.57}}, "power_watts_avg": 34.4, "energy_joules_est": 42.97, "sample_count": 9, "duration_seconds": 1.249}, "timestamp": "2026-01-17T14:27:15.766375"}
{"image_index": 35, "image_name": "000000003156.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000003156.jpg", "image_width": 443, "image_height": 640, "image_resolution": "443x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1621.058, "latencies_ms": [1621.058], "images_per_second": 0.617, "prompt_tokens": 23, "response_tokens_est": 35, "n_tiles": 6, "output_text": "1. Man\n2. Toilet\n3. Clothes\n4. Shoes\n5. Floor\n6. Wall\n7. Sink\n8. Ceiling", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25009.1, "ram_available_mb": 100763.1, "ram_percent": 19.9}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 25010.1, "ram_available_mb": 100762.1, "ram_percent": 19.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.67, "peak": 15.14, "min": 14.12}, "VDD_CPU_SOC_MSS": {"avg": 15.29, "peak": 15.75, "min": 14.57}, "VDD_GPU": {"avg": 33.09, "peak": 40.97, "min": 24.83}, "VIN": {"avg": 73.04, "peak": 117.85, "min": 62.15}}, "power_watts_avg": 33.09, "energy_joules_est": 53.65, "sample_count": 12, "duration_seconds": 1.621}, "timestamp": "2026-01-17T14:27:17.394173"}
{"image_index": 35, "image_name": "000000003156.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000003156.jpg", "image_width": 443, "image_height": 640, "image_resolution": "443x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2358.683, "latencies_ms": [2358.683], "images_per_second": 0.424, "prompt_tokens": 27, "response_tokens_est": 61, "n_tiles": 6, "output_text": "The main object in the foreground is a man sitting on the toilet. He is wearing a dark-colored shirt and jeans, with his legs crossed. The toilet is positioned to the right of him. In the background, there is a wall with a checkered pattern and a sink visible to the left.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25010.1, "ram_available_mb": 100762.1, "ram_percent": 19.9}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 25011.3, "ram_available_mb": 100760.9, "ram_percent": 19.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.71, "peak": 15.04, "min": 14.12}, "VDD_CPU_SOC_MSS": {"avg": 15.27, "peak": 15.74, "min": 14.57}, "VDD_GPU": {"avg": 29.68, "peak": 41.36, "min": 22.86}, "VIN": {"avg": 73.41, "peak": 110.14, "min": 61.27}}, "power_watts_avg": 29.68, "energy_joules_est": 70.03, "sample_count": 18, "duration_seconds": 2.359}, "timestamp": "2026-01-17T14:27:19.759478"}
{"image_index": 35, "image_name": "000000003156.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000003156.jpg", "image_width": 443, "image_height": 640, "image_resolution": "443x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2477.236, "latencies_ms": [2477.236], "images_per_second": 0.404, "prompt_tokens": 21, "response_tokens_est": 65, "n_tiles": 6, "output_text": "The image depicts a young man sitting on the toilet in a bathroom. He is wearing a dark-colored shirt and jeans, and his legs are crossed. The setting is a bathroom with a black and white checkered tile wall, a white toilet, and a black and white checkered towel or mat on the floor.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25011.3, "ram_available_mb": 100760.9, "ram_percent": 19.9}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 25011.8, "ram_available_mb": 100760.4, "ram_percent": 19.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.7, "peak": 15.04, "min": 14.12}, "VDD_CPU_SOC_MSS": {"avg": 15.27, "peak": 15.75, "min": 14.57}, "VDD_GPU": {"avg": 28.99, "peak": 40.57, "min": 22.86}, "VIN": {"avg": 67.74, "peak": 98.07, "min": 54.0}}, "power_watts_avg": 28.99, "energy_joules_est": 71.83, "sample_count": 19, "duration_seconds": 2.478}, "timestamp": "2026-01-17T14:27:22.243679"}
{"image_index": 35, "image_name": "000000003156.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000003156.jpg", "image_width": 443, "image_height": 640, "image_resolution": "443x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1996.058, "latencies_ms": [1996.058], "images_per_second": 0.501, "prompt_tokens": 19, "response_tokens_est": 48, "n_tiles": 6, "output_text": "The image is a black and white photograph featuring a young man sitting on the toilet. The man is wearing a dark-colored shirt, jeans, and sneakers. The lighting is soft and even, creating a monochromatic effect.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25011.8, "ram_available_mb": 100760.4, "ram_percent": 19.9}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 25011.8, "ram_available_mb": 100760.4, "ram_percent": 19.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.7, "peak": 15.14, "min": 14.12}, "VDD_CPU_SOC_MSS": {"avg": 15.25, "peak": 15.74, "min": 14.57}, "VDD_GPU": {"avg": 30.55, "peak": 40.18, "min": 23.25}, "VIN": {"avg": 72.24, "peak": 115.27, "min": 57.84}}, "power_watts_avg": 30.55, "energy_joules_est": 60.99, "sample_count": 15, "duration_seconds": 1.996}, "timestamp": "2026-01-17T14:27:24.245556"}
{"image_index": 36, "image_name": "000000003255.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000003255.jpg", "image_width": 640, "image_height": 363, "image_resolution": "640x363", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 840.022, "latencies_ms": [840.022], "images_per_second": 1.19, "prompt_tokens": 9, "response_tokens_est": 29, "n_tiles": 2, "output_text": "A group of people is standing on a snowy mountain slope, with tracks in the snow leading up to them, indicating recent human activity.", "error": null, "sys_before": {"cpu_percent": 20.8, "ram_used_mb": 25011.8, "ram_available_mb": 100760.4, "ram_percent": 19.9}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25012.5, "ram_available_mb": 100759.7, "ram_percent": 19.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5055.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.51, "peak": 14.91, "min": 14.0}, "VDD_CPU_SOC_MSS": {"avg": 15.12, "peak": 15.76, "min": 14.57}, "VDD_GPU": {"avg": 26.55, "peak": 31.12, "min": 23.23}, "VIN": {"avg": 59.65, "peak": 63.2, "min": 47.48}}, "power_watts_avg": 26.55, "energy_joules_est": 22.32, "sample_count": 5, "duration_seconds": 0.841}, "timestamp": "2026-01-17T14:27:25.106373"}
{"image_index": 36, "image_name": "000000003255.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000003255.jpg", "image_width": 640, "image_height": 363, "image_resolution": "640x363", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 3138.159, "latencies_ms": [3138.159], "images_per_second": 0.319, "prompt_tokens": 23, "response_tokens_est": 129, "n_tiles": 2, "output_text": "- Snow\n- Mountain\n- People\n- Tracks\n- Hills\n- Slopes\n- Snow\n- Path\n- Ridge\n- Ridge\n- Slope\n- Hill\n- Mountain\n- Snow\n- Path\n- Ridge\n- Ridge\n- Slope\n- Hill\n- Mountain\n- Snow\n- Path\n- Ridge\n- Ridge\n- Slope\n- Hill\n- Mountain\n- Snow\n- Path\n- Ridge\n- Ridge\n- Slope\n- Hill\n- Mountain\n- Snow\n- Path\n- Ridge\n- Ridge\n- Slope\n- Hill\n-", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25012.5, "ram_available_mb": 100759.7, "ram_percent": 19.9}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 25014.6, "ram_available_mb": 100757.5, "ram_percent": 19.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5064.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.14, "peak": 15.42, "min": 14.51}, "VDD_CPU_SOC_MSS": {"avg": 15.99, "peak": 16.15, "min": 14.96}, "VDD_GPU": {"avg": 22.25, "peak": 32.29, "min": 20.49}, "VIN": {"avg": 64.21, "peak": 77.97, "min": 58.38}}, "power_watts_avg": 22.25, "energy_joules_est": 69.84, "sample_count": 23, "duration_seconds": 3.139}, "timestamp": "2026-01-17T14:27:28.251485"}
{"image_index": 36, "image_name": "000000003255.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000003255.jpg", "image_width": 640, "image_height": 363, "image_resolution": "640x363", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1768.045, "latencies_ms": [1768.045], "images_per_second": 0.566, "prompt_tokens": 27, "response_tokens_est": 69, "n_tiles": 2, "output_text": "The main objects in the image are a group of people and a snowy mountain landscape. The people are located in the foreground, near the center of the image, while the snowy mountain and its rocky peaks are in the background. The tracks in the snow indicate that the people are near the mountain, possibly hiking or exploring the area.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25014.6, "ram_available_mb": 100757.5, "ram_percent": 19.9}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 25014.9, "ram_available_mb": 100757.3, "ram_percent": 19.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5067.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.23, "peak": 15.52, "min": 14.61}, "VDD_CPU_SOC_MSS": {"avg": 15.93, "peak": 16.15, "min": 15.36}, "VDD_GPU": {"avg": 23.12, "peak": 30.33, "min": 20.49}, "VIN": {"avg": 63.95, "peak": 71.71, "min": 61.19}}, "power_watts_avg": 23.12, "energy_joules_est": 40.89, "sample_count": 13, "duration_seconds": 1.768}, "timestamp": "2026-01-17T14:27:30.025385"}
{"image_index": 36, "image_name": "000000003255.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000003255.jpg", "image_width": 640, "image_height": 363, "image_resolution": "640x363", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1329.001, "latencies_ms": [1329.001], "images_per_second": 0.752, "prompt_tokens": 21, "response_tokens_est": 50, "n_tiles": 2, "output_text": "The image depicts a snowy mountainous landscape with a group of people gathered on the slopes. The clear blue sky and the tracks in the snow suggest recent human activity, possibly indicating a hiking or trekking expedition in a mountainous region.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25014.9, "ram_available_mb": 100757.3, "ram_percent": 19.9}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 25014.6, "ram_available_mb": 100757.6, "ram_percent": 19.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5063.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.11, "peak": 15.42, "min": 14.51}, "VDD_CPU_SOC_MSS": {"avg": 15.66, "peak": 16.15, "min": 15.35}, "VDD_GPU": {"avg": 24.34, "peak": 30.33, "min": 21.28}, "VIN": {"avg": 66.17, "peak": 79.89, "min": 61.42}}, "power_watts_avg": 24.34, "energy_joules_est": 32.36, "sample_count": 9, "duration_seconds": 1.329}, "timestamp": "2026-01-17T14:27:31.359967"}
{"image_index": 36, "image_name": "000000003255.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000003255.jpg", "image_width": 640, "image_height": 363, "image_resolution": "640x363", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1152.342, "latencies_ms": [1152.342], "images_per_second": 0.868, "prompt_tokens": 19, "response_tokens_est": 42, "n_tiles": 2, "output_text": "The image showcases a snowy mountain landscape with a clear blue sky. The snow-covered slopes are illuminated by the bright sunlight, creating a striking contrast between the white snow and the blue sky.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25014.6, "ram_available_mb": 100757.6, "ram_percent": 19.9}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25014.6, "ram_available_mb": 100757.6, "ram_percent": 19.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5062.6, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.09, "peak": 15.52, "min": 14.51}, "VDD_CPU_SOC_MSS": {"avg": 15.66, "peak": 16.15, "min": 14.96}, "VDD_GPU": {"avg": 24.87, "peak": 30.34, "min": 21.66}, "VIN": {"avg": 67.45, "peak": 104.17, "min": 58.33}}, "power_watts_avg": 24.87, "energy_joules_est": 28.66, "sample_count": 8, "duration_seconds": 1.153}, "timestamp": "2026-01-17T14:27:32.517973"}
{"image_index": 37, "image_name": "000000003501.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000003501.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 842.337, "latencies_ms": [842.337], "images_per_second": 1.187, "prompt_tokens": 9, "response_tokens_est": 35, "n_tiles": 1, "output_text": "The image shows a bowl of food consisting of white rice, a portion of broccoli, and a portion of red-orange sauce, all placed on a wooden surface.", "error": null, "sys_before": {"cpu_percent": 6.2, "ram_used_mb": 25014.6, "ram_available_mb": 100757.6, "ram_percent": 19.9}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25014.9, "ram_available_mb": 100757.3, "ram_percent": 19.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 4845.7, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.25, "peak": 15.42, "min": 15.01}, "VDD_CPU_SOC_MSS": {"avg": 16.08, "peak": 16.54, "min": 15.76}, "VDD_GPU": {"avg": 22.12, "peak": 24.82, "min": 20.48}, "VIN": {"avg": 63.84, "peak": 71.25, "min": 61.05}}, "power_watts_avg": 22.12, "energy_joules_est": 18.65, "sample_count": 6, "duration_seconds": 0.843}, "timestamp": "2026-01-17T14:27:33.377140"}
{"image_index": 37, "image_name": "000000003501.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000003501.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2889.426, "latencies_ms": [2889.426], "images_per_second": 0.346, "prompt_tokens": 23, "response_tokens_est": 128, "n_tiles": 1, "output_text": "object: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject:", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25014.9, "ram_available_mb": 100757.3, "ram_percent": 19.9}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25014.8, "ram_available_mb": 100757.3, "ram_percent": 19.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 4856.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.5, "peak": 15.82, "min": 15.11}, "VDD_CPU_SOC_MSS": {"avg": 16.47, "peak": 16.54, "min": 16.14}, "VDD_GPU": {"avg": 20.37, "peak": 24.04, "min": 19.7}, "VIN": {"avg": 62.66, "peak": 72.55, "min": 52.84}}, "power_watts_avg": 20.37, "energy_joules_est": 58.87, "sample_count": 22, "duration_seconds": 2.89}, "timestamp": "2026-01-17T14:27:36.272336"}
{"image_index": 37, "image_name": "000000003501.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000003501.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1429.189, "latencies_ms": [1429.189], "images_per_second": 0.7, "prompt_tokens": 27, "response_tokens_est": 62, "n_tiles": 1, "output_text": "The main objects in the image are a bowl of food and a bowl of broccoli. The bowl of food is placed in the foreground, while the bowl of broccoli is positioned in the background. The food is placed on a wooden surface, and the bowl of broccoli is slightly out of focus.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25014.8, "ram_available_mb": 100757.3, "ram_percent": 19.9}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25015.8, "ram_available_mb": 100756.4, "ram_percent": 19.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 4859.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.48, "peak": 15.72, "min": 15.01}, "VDD_CPU_SOC_MSS": {"avg": 16.36, "peak": 16.54, "min": 15.75}, "VDD_GPU": {"avg": 20.99, "peak": 23.64, "min": 19.7}, "VIN": {"avg": 64.04, "peak": 71.68, "min": 60.46}}, "power_watts_avg": 20.99, "energy_joules_est": 30.01, "sample_count": 11, "duration_seconds": 1.43}, "timestamp": "2026-01-17T14:27:37.707470"}
{"image_index": 37, "image_name": "000000003501.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000003501.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1295.028, "latencies_ms": [1295.028], "images_per_second": 0.772, "prompt_tokens": 21, "response_tokens_est": 56, "n_tiles": 1, "output_text": "The image shows a simple yet appetizing meal consisting of a bowl of white rice, a portion of broccoli, and a serving of red-orange stew. The meal is presented on a white plate, and the setting appears to be a dining table with a wooden surface.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25015.8, "ram_available_mb": 100756.4, "ram_percent": 19.9}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25015.8, "ram_available_mb": 100756.4, "ram_percent": 19.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 4854.7, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.4, "peak": 15.52, "min": 15.11}, "VDD_CPU_SOC_MSS": {"avg": 16.4, "peak": 16.54, "min": 16.14}, "VDD_GPU": {"avg": 21.15, "peak": 23.64, "min": 19.71}, "VIN": {"avg": 63.52, "peak": 67.7, "min": 59.43}}, "power_watts_avg": 21.15, "energy_joules_est": 27.4, "sample_count": 9, "duration_seconds": 1.295}, "timestamp": "2026-01-17T14:27:39.008827"}
{"image_index": 37, "image_name": "000000003501.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000003501.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1076.661, "latencies_ms": [1076.661], "images_per_second": 0.929, "prompt_tokens": 19, "response_tokens_est": 46, "n_tiles": 1, "output_text": "The image features a white bowl containing a meal consisting of broccoli, white rice, and a red-orange sauce. The bowl is placed on a wooden surface, and the lighting is soft and natural, casting gentle shadows.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25015.8, "ram_available_mb": 100756.4, "ram_percent": 19.9}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25016.1, "ram_available_mb": 100756.1, "ram_percent": 19.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 4853.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.34, "peak": 15.52, "min": 15.11}, "VDD_CPU_SOC_MSS": {"avg": 16.39, "peak": 16.54, "min": 16.14}, "VDD_GPU": {"avg": 21.33, "peak": 23.64, "min": 20.1}, "VIN": {"avg": 62.86, "peak": 67.17, "min": 58.95}}, "power_watts_avg": 21.33, "energy_joules_est": 22.97, "sample_count": 8, "duration_seconds": 1.077}, "timestamp": "2026-01-17T14:27:40.091145"}
{"image_index": 38, "image_name": "000000003553.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000003553.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1896.169, "latencies_ms": [1896.169], "images_per_second": 0.527, "prompt_tokens": 9, "response_tokens_est": 48, "n_tiles": 6, "output_text": "The image shows a person's lower legs and feet, wearing black shoes with white soles, standing on a skateboard with a checkered pattern on the deck, positioned on a wooden surface with grass and a bench in the background.", "error": null, "sys_before": {"cpu_percent": 23.3, "ram_used_mb": 25016.1, "ram_available_mb": 100756.1, "ram_percent": 19.9}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 25015.8, "ram_available_mb": 100756.4, "ram_percent": 19.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.86, "peak": 15.24, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.66, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 30.45, "peak": 39.0, "min": 24.04}, "VIN": {"avg": 72.4, "peak": 119.92, "min": 58.86}}, "power_watts_avg": 30.45, "energy_joules_est": 57.75, "sample_count": 14, "duration_seconds": 1.896}, "timestamp": "2026-01-17T14:27:42.015812"}
{"image_index": 38, "image_name": "000000003553.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000003553.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2130.498, "latencies_ms": [2130.498], "images_per_second": 0.469, "prompt_tokens": 23, "response_tokens_est": 55, "n_tiles": 6, "output_text": "skateboard: 1\nskateboard wheels: 1\nskateboard deck: 1\nskateboard handle: 1\nskateboard footwear: 1\nskateboard strap: 1\nskateboard: 1", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 25015.8, "ram_available_mb": 100756.4, "ram_percent": 19.9}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 25015.8, "ram_available_mb": 100756.4, "ram_percent": 19.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.89, "peak": 15.34, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.58, "peak": 16.14, "min": 14.57}, "VDD_GPU": {"avg": 30.51, "peak": 40.18, "min": 23.64}, "VIN": {"avg": 73.37, "peak": 111.77, "min": 61.49}}, "power_watts_avg": 30.51, "energy_joules_est": 65.01, "sample_count": 16, "duration_seconds": 2.131}, "timestamp": "2026-01-17T14:27:44.152493"}
{"image_index": 38, "image_name": "000000003553.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000003553.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2268.86, "latencies_ms": [2268.86], "images_per_second": 0.441, "prompt_tokens": 27, "response_tokens_est": 60, "n_tiles": 6, "output_text": "The main object in the foreground is a skateboard with a checkered pattern on the deck. The skateboard is positioned on a wooden surface, likely a bench or a deck. In the background, there is a wooden plank and a grassy area, indicating that the scene is outdoors.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25015.8, "ram_available_mb": 100756.4, "ram_percent": 19.9}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 25015.8, "ram_available_mb": 100756.4, "ram_percent": 19.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.93, "peak": 15.34, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.56, "peak": 16.14, "min": 14.57}, "VDD_GPU": {"avg": 30.18, "peak": 40.57, "min": 23.64}, "VIN": {"avg": 72.05, "peak": 116.22, "min": 58.3}}, "power_watts_avg": 30.18, "energy_joules_est": 68.48, "sample_count": 17, "duration_seconds": 2.269}, "timestamp": "2026-01-17T14:27:46.427810"}
{"image_index": 38, "image_name": "000000003553.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000003553.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2072.841, "latencies_ms": [2072.841], "images_per_second": 0.482, "prompt_tokens": 21, "response_tokens_est": 53, "n_tiles": 6, "output_text": "The image depicts a skateboard resting on a wooden surface, likely a deck or a bench, with a grassy background. The skateboard has a checkered pattern on its wheels, and the person wearing the skateboard is not visible in the frame.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25015.8, "ram_available_mb": 100756.4, "ram_percent": 19.9}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 25016.1, "ram_available_mb": 100756.1, "ram_percent": 19.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.91, "peak": 15.34, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.55, "peak": 16.14, "min": 14.57}, "VDD_GPU": {"avg": 30.41, "peak": 40.18, "min": 23.64}, "VIN": {"avg": 72.27, "peak": 113.9, "min": 62.44}}, "power_watts_avg": 30.41, "energy_joules_est": 63.05, "sample_count": 16, "duration_seconds": 2.073}, "timestamp": "2026-01-17T14:27:48.506694"}
{"image_index": 38, "image_name": "000000003553.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000003553.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1887.826, "latencies_ms": [1887.826], "images_per_second": 0.53, "prompt_tokens": 19, "response_tokens_est": 46, "n_tiles": 6, "output_text": "The notable visual attributes of the image include a skateboard with a checkered pattern on the front, resting on a wooden surface. The lighting is bright, casting shadows on the ground, and the weather appears to be sunny.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25016.1, "ram_available_mb": 100756.1, "ram_percent": 19.9}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 25016.0, "ram_available_mb": 100756.1, "ram_percent": 19.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.91, "peak": 15.34, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.55, "peak": 16.14, "min": 14.57}, "VDD_GPU": {"avg": 31.55, "peak": 40.57, "min": 24.04}, "VIN": {"avg": 73.38, "peak": 120.48, "min": 56.53}}, "power_watts_avg": 31.55, "energy_joules_est": 59.58, "sample_count": 14, "duration_seconds": 1.888}, "timestamp": "2026-01-17T14:27:50.404770"}
{"image_index": 39, "image_name": "000000003661.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000003661.jpg", "image_width": 640, "image_height": 384, "image_resolution": "640x384", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1574.655, "latencies_ms": [1574.655], "images_per_second": 0.635, "prompt_tokens": 9, "response_tokens_est": 36, "n_tiles": 6, "output_text": "The image shows a bunch of bananas with one of them slightly bent, resting on a wooden surface, with a blurred background featuring a computer monitor and a blue cup.", "error": null, "sys_before": {"cpu_percent": 44.4, "ram_used_mb": 25015.8, "ram_available_mb": 100756.4, "ram_percent": 19.9}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25016.0, "ram_available_mb": 100756.1, "ram_percent": 19.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.63, "peak": 15.04, "min": 13.79}, "VDD_CPU_SOC_MSS": {"avg": 15.32, "peak": 15.75, "min": 14.18}, "VDD_GPU": {"avg": 32.07, "peak": 40.18, "min": 24.82}, "VIN": {"avg": 71.56, "peak": 89.94, "min": 60.56}}, "power_watts_avg": 32.07, "energy_joules_est": 50.51, "sample_count": 12, "duration_seconds": 1.575}, "timestamp": "2026-01-17T14:27:52.028852"}
{"image_index": 39, "image_name": "000000003661.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000003661.jpg", "image_width": 640, "image_height": 384, "image_resolution": "640x384", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1507.411, "latencies_ms": [1507.411], "images_per_second": 0.663, "prompt_tokens": 23, "response_tokens_est": 32, "n_tiles": 6, "output_text": "- Banana: 3\n- Mango: 1\n- Cup: 1\n- Table: 1\n- Computer: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25016.0, "ram_available_mb": 100756.1, "ram_percent": 19.9}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 25016.0, "ram_available_mb": 100756.1, "ram_percent": 19.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.86, "peak": 15.34, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.53, "peak": 16.14, "min": 14.57}, "VDD_GPU": {"avg": 33.38, "peak": 40.18, "min": 25.61}, "VIN": {"avg": 75.45, "peak": 101.18, "min": 62.89}}, "power_watts_avg": 33.38, "energy_joules_est": 50.33, "sample_count": 11, "duration_seconds": 1.508}, "timestamp": "2026-01-17T14:27:53.542247"}
{"image_index": 39, "image_name": "000000003661.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000003661.jpg", "image_width": 640, "image_height": 384, "image_resolution": "640x384", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2344.666, "latencies_ms": [2344.666], "images_per_second": 0.426, "prompt_tokens": 27, "response_tokens_est": 63, "n_tiles": 6, "output_text": "The main object in the foreground is a bunch of bananas. The bananas are positioned on a wooden surface, with the bunch being the closest to the camera. In the background, there is a blurred object that appears to be a computer monitor, and a blue cup is partially visible to the right.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25016.0, "ram_available_mb": 100756.1, "ram_percent": 19.9}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 25016.0, "ram_available_mb": 100756.1, "ram_percent": 19.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.96, "peak": 15.34, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.62, "peak": 16.14, "min": 14.57}, "VDD_GPU": {"avg": 30.07, "peak": 40.57, "min": 23.25}, "VIN": {"avg": 71.66, "peak": 114.64, "min": 60.19}}, "power_watts_avg": 30.07, "energy_joules_est": 70.52, "sample_count": 18, "duration_seconds": 2.345}, "timestamp": "2026-01-17T14:27:55.892730"}
{"image_index": 39, "image_name": "000000003661.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000003661.jpg", "image_width": 640, "image_height": 384, "image_resolution": "640x384", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2613.866, "latencies_ms": [2613.866], "images_per_second": 0.383, "prompt_tokens": 21, "response_tokens_est": 73, "n_tiles": 6, "output_text": "The image depicts a bunch of bananas placed on a wooden surface, possibly a table or countertop. The bananas are in focus, with one partially cut, revealing the inside of the fruit. In the background, there is a blurred image of a computer monitor and a blue cup, suggesting an indoor setting, likely a home or office environment.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25016.0, "ram_available_mb": 100756.1, "ram_percent": 19.9}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 25016.0, "ram_available_mb": 100756.2, "ram_percent": 19.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.96, "peak": 15.34, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.61, "peak": 16.14, "min": 14.57}, "VDD_GPU": {"avg": 29.02, "peak": 39.78, "min": 23.25}, "VIN": {"avg": 69.79, "peak": 103.0, "min": 58.26}}, "power_watts_avg": 29.02, "energy_joules_est": 75.86, "sample_count": 20, "duration_seconds": 2.614}, "timestamp": "2026-01-17T14:27:58.512671"}
{"image_index": 39, "image_name": "000000003661.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000003661.jpg", "image_width": 640, "image_height": 384, "image_resolution": "640x384", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2444.0, "latencies_ms": [2444.0], "images_per_second": 0.409, "prompt_tokens": 19, "response_tokens_est": 65, "n_tiles": 6, "output_text": "The image features a bunch of ripe bananas with a yellowish-green hue, indicating ripeness. The lighting is soft and natural, casting gentle shadows on the wooden surface. The bananas are placed on a table, and there is a blurred background with a blue object and a cup, suggesting an indoor setting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25016.0, "ram_available_mb": 100756.2, "ram_percent": 19.9}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 25016.0, "ram_available_mb": 100756.2, "ram_percent": 19.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.9, "peak": 15.34, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.5, "peak": 16.14, "min": 14.57}, "VDD_GPU": {"avg": 29.14, "peak": 40.18, "min": 22.86}, "VIN": {"avg": 68.94, "peak": 95.65, "min": 56.69}}, "power_watts_avg": 29.14, "energy_joules_est": 71.23, "sample_count": 19, "duration_seconds": 2.444}, "timestamp": "2026-01-17T14:28:00.963327"}
{"image_index": 40, "image_name": "000000003845.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000003845.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2837.128, "latencies_ms": [2837.128], "images_per_second": 0.352, "prompt_tokens": 9, "response_tokens_est": 36, "n_tiles": 12, "output_text": "The image shows a plate of food consisting of stir-fried chicken, broccoli, carrots, and rice, all served on a colorful plate with a red and white pattern.", "error": null, "sys_before": {"cpu_percent": 46.2, "ram_used_mb": 24977.6, "ram_available_mb": 100794.6, "ram_percent": 19.9}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 24816.9, "ram_available_mb": 100955.3, "ram_percent": 19.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.02, "peak": 16.56, "min": 13.49}, "VDD_CPU_SOC_MSS": {"avg": 15.75, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 34.62, "peak": 43.33, "min": 26.4}, "VIN": {"avg": 75.28, "peak": 125.33, "min": 60.43}}, "power_watts_avg": 34.62, "energy_joules_est": 98.23, "sample_count": 22, "duration_seconds": 2.837}, "timestamp": "2026-01-17T14:28:03.878794"}
{"image_index": 40, "image_name": "000000003845.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000003845.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2856.394, "latencies_ms": [2856.394], "images_per_second": 0.35, "prompt_tokens": 23, "response_tokens_est": 35, "n_tiles": 12, "output_text": "chicken: 2\nrice: 1\ncarrot: 1\nbroccoli: 1\npepper: 1\ngarlic: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24816.9, "ram_available_mb": 100955.3, "ram_percent": 19.7}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 24817.3, "ram_available_mb": 100954.8, "ram_percent": 19.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.12, "peak": 16.46, "min": 14.12}, "VDD_CPU_SOC_MSS": {"avg": 15.82, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 36.13, "peak": 44.91, "min": 26.79}, "VIN": {"avg": 78.86, "peak": 114.25, "min": 65.7}}, "power_watts_avg": 36.13, "energy_joules_est": 103.22, "sample_count": 21, "duration_seconds": 2.857}, "timestamp": "2026-01-17T14:28:06.741755"}
{"image_index": 40, "image_name": "000000003845.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000003845.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 4052.537, "latencies_ms": [4052.537], "images_per_second": 0.247, "prompt_tokens": 27, "response_tokens_est": 70, "n_tiles": 12, "output_text": "The main objects in the image are a plate of food and a glass of water. The plate of food is placed in the foreground, with the glass of water positioned slightly behind it. The food is the central focus of the image, while the glass of water is positioned in the background, providing a sense of depth and context to the scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24817.3, "ram_available_mb": 100954.8, "ram_percent": 19.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 24818.3, "ram_available_mb": 100953.9, "ram_percent": 19.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.07, "peak": 16.46, "min": 14.12}, "VDD_CPU_SOC_MSS": {"avg": 15.58, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 32.73, "peak": 44.91, "min": 25.6}, "VIN": {"avg": 78.66, "peak": 127.82, "min": 64.39}}, "power_watts_avg": 32.73, "energy_joules_est": 132.65, "sample_count": 32, "duration_seconds": 4.053}, "timestamp": "2026-01-17T14:28:10.800660"}
{"image_index": 40, "image_name": "000000003845.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000003845.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3743.955, "latencies_ms": [3743.955], "images_per_second": 0.267, "prompt_tokens": 21, "response_tokens_est": 62, "n_tiles": 12, "output_text": "The image depicts a plate of food on a wooden table, likely in a dining setting. The plate contains a variety of dishes, including rice, broccoli, carrots, and chicken, all arranged in a visually appealing manner. The setting suggests a casual dining experience, possibly at home or a restaurant.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24818.3, "ram_available_mb": 100953.9, "ram_percent": 19.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 24818.8, "ram_available_mb": 100953.4, "ram_percent": 19.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.16, "peak": 16.46, "min": 14.02}, "VDD_CPU_SOC_MSS": {"avg": 15.68, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 33.57, "peak": 44.51, "min": 26.0}, "VIN": {"avg": 76.29, "peak": 115.98, "min": 60.27}}, "power_watts_avg": 33.57, "energy_joules_est": 125.7, "sample_count": 28, "duration_seconds": 3.744}, "timestamp": "2026-01-17T14:28:14.551279"}
{"image_index": 40, "image_name": "000000003845.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000003845.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3673.901, "latencies_ms": [3673.901], "images_per_second": 0.272, "prompt_tokens": 19, "response_tokens_est": 59, "n_tiles": 12, "output_text": "The image showcases a plate of food with a vibrant and colorful design. The plate is adorned with a mix of red, orange, and green hues, creating a visually appealing contrast. The lighting is warm and natural, casting a soft glow on the food and enhancing the colors.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 24818.8, "ram_available_mb": 100953.4, "ram_percent": 19.7}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 24818.8, "ram_available_mb": 100953.4, "ram_percent": 19.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.1, "peak": 16.46, "min": 14.12}, "VDD_CPU_SOC_MSS": {"avg": 15.62, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 33.49, "peak": 44.51, "min": 25.61}, "VIN": {"avg": 73.91, "peak": 118.09, "min": 65.4}}, "power_watts_avg": 33.49, "energy_joules_est": 123.05, "sample_count": 28, "duration_seconds": 3.674}, "timestamp": "2026-01-17T14:28:18.231839"}
{"image_index": 41, "image_name": "000000003934.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000003934.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2419.931, "latencies_ms": [2419.931], "images_per_second": 0.413, "prompt_tokens": 9, "response_tokens_est": 24, "n_tiles": 12, "output_text": "A young girl is playing a video game on a couch in a living room, surrounded by people and various items.", "error": null, "sys_before": {"cpu_percent": 43.2, "ram_used_mb": 24824.0, "ram_available_mb": 100948.2, "ram_percent": 19.7}, "sys_after": {"cpu_percent": 2.4, "ram_used_mb": 24825.9, "ram_available_mb": 100946.3, "ram_percent": 19.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.12, "peak": 16.56, "min": 13.39}, "VDD_CPU_SOC_MSS": {"avg": 15.86, "peak": 17.71, "min": 14.18}, "VDD_GPU": {"avg": 36.77, "peak": 44.12, "min": 28.36}, "VIN": {"avg": 80.79, "peak": 125.36, "min": 57.39}}, "power_watts_avg": 36.77, "energy_joules_est": 89.0, "sample_count": 18, "duration_seconds": 2.42}, "timestamp": "2026-01-17T14:28:20.743490"}
{"image_index": 41, "image_name": "000000003934.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000003934.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 6027.14, "latencies_ms": [6027.14], "images_per_second": 0.166, "prompt_tokens": 23, "response_tokens_est": 129, "n_tiles": 12, "output_text": "- girl: 1\n- woman: 2\n- man: 1\n- woman: 2\n- woman: 2\n- woman: 2\n- woman: 2\n- woman: 2\n- woman: 2\n- woman: 2\n- woman: 2\n- woman: 2\n- woman: 2\n- woman: 2\n- woman: 2\n- woman: 2\n- woman: 2\n- woman: 2\n- woman: 2\n- woman: 2\n- woman: 2\n- woman", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24825.9, "ram_available_mb": 100946.3, "ram_percent": 19.7}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 24825.9, "ram_available_mb": 100946.3, "ram_percent": 19.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.04, "peak": 16.56, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.51, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 30.47, "peak": 45.3, "min": 25.6}, "VIN": {"avg": 73.29, "peak": 131.92, "min": 57.84}}, "power_watts_avg": 30.47, "energy_joules_est": 183.66, "sample_count": 47, "duration_seconds": 6.028}, "timestamp": "2026-01-17T14:28:26.777253"}
{"image_index": 41, "image_name": "000000003934.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000003934.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3849.438, "latencies_ms": [3849.438], "images_per_second": 0.26, "prompt_tokens": 27, "response_tokens_est": 64, "n_tiles": 12, "output_text": "The main objects in the image are a young girl, a man, and a woman. The girl is in the foreground, standing on a grey rug, while the man and woman are in the background, near a couch. The man is standing near a bar, and the woman is standing near a staircase.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24825.9, "ram_available_mb": 100946.3, "ram_percent": 19.7}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 24827.1, "ram_available_mb": 100945.0, "ram_percent": 19.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.08, "peak": 16.56, "min": 14.12}, "VDD_CPU_SOC_MSS": {"avg": 15.62, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 33.13, "peak": 45.3, "min": 25.61}, "VIN": {"avg": 73.85, "peak": 121.3, "min": 55.92}}, "power_watts_avg": 33.13, "energy_joules_est": 127.55, "sample_count": 30, "duration_seconds": 3.85}, "timestamp": "2026-01-17T14:28:30.634322"}
{"image_index": 41, "image_name": "000000003934.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000003934.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3769.585, "latencies_ms": [3769.585], "images_per_second": 0.265, "prompt_tokens": 21, "response_tokens_est": 62, "n_tiles": 12, "output_text": "The scene is set in a cozy living room with a gray sofa, a red patterned cushion, and a gray rug. A group of people, including a young girl in a colorful dress, are gathered around a television, with some standing and others sitting. The atmosphere suggests a casual, social gathering.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24827.1, "ram_available_mb": 100945.0, "ram_percent": 19.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 24828.4, "ram_available_mb": 100943.8, "ram_percent": 19.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.11, "peak": 16.56, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.69, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 33.54, "peak": 45.3, "min": 25.6}, "VIN": {"avg": 76.18, "peak": 124.44, "min": 59.36}}, "power_watts_avg": 33.54, "energy_joules_est": 126.45, "sample_count": 29, "duration_seconds": 3.77}, "timestamp": "2026-01-17T14:28:34.410666"}
{"image_index": 41, "image_name": "000000003934.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000003934.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2991.877, "latencies_ms": [2991.877], "images_per_second": 0.334, "prompt_tokens": 19, "response_tokens_est": 39, "n_tiles": 12, "output_text": "The room is well-lit with warm, ambient lighting, creating a cozy atmosphere. The walls are painted in a light, neutral color, complementing the wooden floor and the gray sofa.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24828.4, "ram_available_mb": 100943.8, "ram_percent": 19.7}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 24828.4, "ram_available_mb": 100943.8, "ram_percent": 19.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.12, "peak": 16.46, "min": 14.02}, "VDD_CPU_SOC_MSS": {"avg": 15.66, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 35.52, "peak": 44.91, "min": 26.4}, "VIN": {"avg": 76.64, "peak": 115.52, "min": 55.79}}, "power_watts_avg": 35.52, "energy_joules_est": 106.29, "sample_count": 23, "duration_seconds": 2.992}, "timestamp": "2026-01-17T14:28:37.409267"}
{"image_index": 42, "image_name": "000000004134.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000004134.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1685.815, "latencies_ms": [1685.815], "images_per_second": 0.593, "prompt_tokens": 9, "response_tokens_est": 40, "n_tiles": 6, "output_text": "A man in a dark suit and striped tie is shaking hands with another man in a light-colored shirt and tie, while a group of people are gathered around tables in a formal event setting.", "error": null, "sys_before": {"cpu_percent": 41.2, "ram_used_mb": 24859.1, "ram_available_mb": 100913.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 24861.3, "ram_available_mb": 100910.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.62, "peak": 15.01, "min": 13.61}, "VDD_CPU_SOC_MSS": {"avg": 15.32, "peak": 15.75, "min": 14.18}, "VDD_GPU": {"avg": 31.94, "peak": 40.97, "min": 24.42}, "VIN": {"avg": 74.68, "peak": 119.18, "min": 58.77}}, "power_watts_avg": 31.94, "energy_joules_est": 53.86, "sample_count": 13, "duration_seconds": 1.686}, "timestamp": "2026-01-17T14:28:39.162696"}
{"image_index": 42, "image_name": "000000004134.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000004134.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1947.845, "latencies_ms": [1947.845], "images_per_second": 0.513, "prompt_tokens": 23, "response_tokens_est": 48, "n_tiles": 6, "output_text": "1. Man in suit\n2. Man in suit\n3. Man in suit\n4. Man in suit\n5. Man in suit\n6. Man in suit\n7. Man in suit\n8. Man in suit", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24861.3, "ram_available_mb": 100910.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 24862.6, "ram_available_mb": 100909.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.92, "peak": 15.44, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.61, "peak": 16.14, "min": 14.57}, "VDD_GPU": {"avg": 31.43, "peak": 40.18, "min": 24.04}, "VIN": {"avg": 74.68, "peak": 113.94, "min": 59.51}}, "power_watts_avg": 31.43, "energy_joules_est": 61.23, "sample_count": 14, "duration_seconds": 1.948}, "timestamp": "2026-01-17T14:28:41.116612"}
{"image_index": 42, "image_name": "000000004134.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000004134.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2113.258, "latencies_ms": [2113.258], "images_per_second": 0.473, "prompt_tokens": 27, "response_tokens_est": 54, "n_tiles": 6, "output_text": "The main object in the foreground is a man in a dark suit and yellow tie, shaking hands with another man in a light blue shirt and dark jacket. The background features a crowded room with tables set for a formal event, where other attendees are engaged in conversations.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 24862.6, "ram_available_mb": 100909.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 24864.0, "ram_available_mb": 100908.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.97, "peak": 15.44, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.6, "peak": 16.14, "min": 14.57}, "VDD_GPU": {"avg": 30.51, "peak": 40.18, "min": 23.64}, "VIN": {"avg": 73.57, "peak": 116.12, "min": 58.8}}, "power_watts_avg": 30.51, "energy_joules_est": 64.49, "sample_count": 16, "duration_seconds": 2.114}, "timestamp": "2026-01-17T14:28:43.236862"}
{"image_index": 42, "image_name": "000000004134.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000004134.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3004.226, "latencies_ms": [3004.226], "images_per_second": 0.333, "prompt_tokens": 21, "response_tokens_est": 87, "n_tiles": 6, "output_text": "The image depicts a formal event, likely a conference or banquet, taking place in a spacious, well-lit room with elegant decor. Several attendees are engaged in conversations, while others are seated at tables covered with white tablecloths, some with plates and glasses. The central focus is on two men shaking hands, one in a dark suit and the other in a striped suit, indicating a formal interaction or greeting.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24864.0, "ram_available_mb": 100908.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 24863.8, "ram_available_mb": 100908.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.0, "peak": 15.44, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.63, "peak": 16.14, "min": 14.57}, "VDD_GPU": {"avg": 28.21, "peak": 39.78, "min": 23.25}, "VIN": {"avg": 68.84, "peak": 101.37, "min": 59.55}}, "power_watts_avg": 28.21, "energy_joules_est": 84.76, "sample_count": 23, "duration_seconds": 3.005}, "timestamp": "2026-01-17T14:28:46.247366"}
{"image_index": 42, "image_name": "000000004134.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000004134.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1920.206, "latencies_ms": [1920.206], "images_per_second": 0.521, "prompt_tokens": 19, "response_tokens_est": 47, "n_tiles": 6, "output_text": "The image depicts a man in a dark suit and striped tie shaking hands with another person. The background shows a well-lit indoor event with tables covered in white tablecloths, and the lighting is warm and ambient.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24863.8, "ram_available_mb": 100908.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 24865.3, "ram_available_mb": 100906.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.94, "peak": 15.44, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.55, "peak": 16.14, "min": 14.57}, "VDD_GPU": {"avg": 31.21, "peak": 39.78, "min": 24.04}, "VIN": {"avg": 68.56, "peak": 99.19, "min": 55.67}}, "power_watts_avg": 31.21, "energy_joules_est": 59.94, "sample_count": 14, "duration_seconds": 1.92}, "timestamp": "2026-01-17T14:28:48.177773"}
{"image_index": 43, "image_name": "000000004395.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000004395.jpg", "image_width": 425, "image_height": 640, "image_resolution": "425x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1286.994, "latencies_ms": [1286.994], "images_per_second": 0.777, "prompt_tokens": 9, "response_tokens_est": 25, "n_tiles": 6, "output_text": "The image shows a man wearing a white shirt and a striped tie, standing indoors with a slightly blurred background.", "error": null, "sys_before": {"cpu_percent": 45.3, "ram_used_mb": 24865.3, "ram_available_mb": 100906.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 24866.0, "ram_available_mb": 100906.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.5, "peak": 15.04, "min": 13.69}, "VDD_CPU_SOC_MSS": {"avg": 15.18, "peak": 15.75, "min": 14.18}, "VDD_GPU": {"avg": 33.87, "peak": 39.38, "min": 27.57}, "VIN": {"avg": 81.88, "peak": 111.56, "min": 59.96}}, "power_watts_avg": 33.87, "energy_joules_est": 43.61, "sample_count": 9, "duration_seconds": 1.288}, "timestamp": "2026-01-17T14:28:49.527768"}
{"image_index": 43, "image_name": "000000004395.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000004395.jpg", "image_width": 425, "image_height": 640, "image_resolution": "425x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1567.847, "latencies_ms": [1567.847], "images_per_second": 0.638, "prompt_tokens": 23, "response_tokens_est": 34, "n_tiles": 6, "output_text": "1. Man\n2. Shirt\n3. Tie\n4. Background\n5. Light\n6. Person\n7. Hand\n8. Cigarette", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24866.0, "ram_available_mb": 100906.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 24866.0, "ram_available_mb": 100906.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.89, "peak": 15.34, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.52, "peak": 16.14, "min": 14.57}, "VDD_GPU": {"avg": 33.22, "peak": 40.57, "min": 25.21}, "VIN": {"avg": 72.99, "peak": 100.87, "min": 57.17}}, "power_watts_avg": 33.22, "energy_joules_est": 52.09, "sample_count": 12, "duration_seconds": 1.568}, "timestamp": "2026-01-17T14:28:51.101527"}
{"image_index": 43, "image_name": "000000004395.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000004395.jpg", "image_width": 425, "image_height": 640, "image_resolution": "425x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2295.902, "latencies_ms": [2295.902], "images_per_second": 0.436, "prompt_tokens": 27, "response_tokens_est": 61, "n_tiles": 6, "output_text": "The main object in the foreground is a person wearing a white shirt and a striped tie. The person's face is slightly turned to the right, and their hand is holding a cigarette. The background is blurred, but it appears to be an indoor setting with indistinct shapes and lights.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24866.0, "ram_available_mb": 100906.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 24867.0, "ram_available_mb": 100905.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.96, "peak": 15.34, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.61, "peak": 16.14, "min": 14.57}, "VDD_GPU": {"avg": 30.31, "peak": 40.18, "min": 23.64}, "VIN": {"avg": 70.63, "peak": 111.23, "min": 60.54}}, "power_watts_avg": 30.31, "energy_joules_est": 69.6, "sample_count": 17, "duration_seconds": 2.296}, "timestamp": "2026-01-17T14:28:53.404592"}
{"image_index": 43, "image_name": "000000004395.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000004395.jpg", "image_width": 425, "image_height": 640, "image_resolution": "425x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2537.493, "latencies_ms": [2537.493], "images_per_second": 0.394, "prompt_tokens": 21, "response_tokens_est": 70, "n_tiles": 6, "output_text": "The image depicts a man in a professional setting, likely an office or a similar environment. He is dressed in a white shirt and a striped tie, and appears to be in a moment of contemplation or deep thought. The background is dimly lit, with indistinct shapes and colors, suggesting an indoor setting with artificial lighting.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 24867.0, "ram_available_mb": 100905.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 24867.0, "ram_available_mb": 100905.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.0, "peak": 15.34, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.6, "peak": 16.14, "min": 14.57}, "VDD_GPU": {"avg": 29.32, "peak": 39.78, "min": 23.25}, "VIN": {"avg": 73.3, "peak": 116.14, "min": 62.11}}, "power_watts_avg": 29.32, "energy_joules_est": 74.41, "sample_count": 19, "duration_seconds": 2.538}, "timestamp": "2026-01-17T14:28:55.947891"}
{"image_index": 43, "image_name": "000000004395.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000004395.jpg", "image_width": 425, "image_height": 640, "image_resolution": "425x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2047.027, "latencies_ms": [2047.027], "images_per_second": 0.489, "prompt_tokens": 19, "response_tokens_est": 52, "n_tiles": 6, "output_text": "The notable visual attributes in the image include a person wearing a white shirt and a striped tie, with a dark background. The lighting is dim, and the person's face is illuminated by a soft light source, creating a warm and intimate atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24867.0, "ram_available_mb": 100905.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 24867.2, "ram_available_mb": 100905.0, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.97, "peak": 15.34, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.57, "peak": 16.14, "min": 14.57}, "VDD_GPU": {"avg": 30.94, "peak": 40.18, "min": 24.04}, "VIN": {"avg": 74.23, "peak": 114.55, "min": 46.24}}, "power_watts_avg": 30.94, "energy_joules_est": 63.34, "sample_count": 15, "duration_seconds": 2.047}, "timestamp": "2026-01-17T14:28:58.000611"}
{"image_index": 44, "image_name": "000000004495.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000004495.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2611.025, "latencies_ms": [2611.025], "images_per_second": 0.383, "prompt_tokens": 9, "response_tokens_est": 29, "n_tiles": 12, "output_text": "The image shows a cozy living room with a television on a stand, a plaid-patterned couch, and a small wooden chair.", "error": null, "sys_before": {"cpu_percent": 50.0, "ram_used_mb": 24827.6, "ram_available_mb": 100944.6, "ram_percent": 19.7}, "sys_after": {"cpu_percent": 2.6, "ram_used_mb": 24817.8, "ram_available_mb": 100954.3, "ram_percent": 19.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.03, "peak": 16.66, "min": 13.39}, "VDD_CPU_SOC_MSS": {"avg": 15.71, "peak": 17.72, "min": 14.18}, "VDD_GPU": {"avg": 35.57, "peak": 44.12, "min": 26.79}, "VIN": {"avg": 80.15, "peak": 118.5, "min": 54.41}}, "power_watts_avg": 35.57, "energy_joules_est": 92.88, "sample_count": 20, "duration_seconds": 2.611}, "timestamp": "2026-01-17T14:29:00.701131"}
{"image_index": 44, "image_name": "000000004495.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000004495.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2824.999, "latencies_ms": [2824.999], "images_per_second": 0.354, "prompt_tokens": 23, "response_tokens_est": 34, "n_tiles": 12, "output_text": "chair: 1\ntv stand: 1\ntv: 1\nwallpaper: 1\ncouch: 2\nblanket: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24817.8, "ram_available_mb": 100954.3, "ram_percent": 19.7}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 24818.1, "ram_available_mb": 100954.1, "ram_percent": 19.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.9, "peak": 16.26, "min": 13.92}, "VDD_CPU_SOC_MSS": {"avg": 15.35, "peak": 17.32, "min": 14.17}, "VDD_GPU": {"avg": 35.92, "peak": 45.3, "min": 26.39}, "VIN": {"avg": 79.47, "peak": 117.33, "min": 59.58}}, "power_watts_avg": 35.92, "energy_joules_est": 101.49, "sample_count": 22, "duration_seconds": 2.825}, "timestamp": "2026-01-17T14:29:03.532346"}
{"image_index": 44, "image_name": "000000004495.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000004495.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3934.058, "latencies_ms": [3934.058], "images_per_second": 0.254, "prompt_tokens": 27, "response_tokens_est": 66, "n_tiles": 12, "output_text": "The main objects in the image are a television, a couch, and a chair. The television is positioned in the background, slightly to the right, while the couch and chair are in the foreground, closer to the viewer. The couch is placed near the television, and the chair is positioned to the left of the couch.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24818.1, "ram_available_mb": 100954.1, "ram_percent": 19.7}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 24818.3, "ram_available_mb": 100953.9, "ram_percent": 19.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.81, "peak": 16.26, "min": 13.92}, "VDD_CPU_SOC_MSS": {"avg": 15.24, "peak": 16.92, "min": 14.17}, "VDD_GPU": {"avg": 33.12, "peak": 45.69, "min": 25.21}, "VIN": {"avg": 74.6, "peak": 119.97, "min": 63.36}}, "power_watts_avg": 33.12, "energy_joules_est": 130.31, "sample_count": 30, "duration_seconds": 3.935}, "timestamp": "2026-01-17T14:29:07.472601"}
{"image_index": 44, "image_name": "000000004495.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000004495.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 4137.208, "latencies_ms": [4137.208], "images_per_second": 0.242, "prompt_tokens": 21, "response_tokens_est": 72, "n_tiles": 12, "output_text": "The image depicts a cozy living room with a television set on a wooden stand against a yellow wall. The room is furnished with a plaid-patterned couch, a red chair, and a blue and red striped blanket draped over a chair. The setting appears to be a home, possibly in a living room or a casual sitting area.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24818.3, "ram_available_mb": 100953.9, "ram_percent": 19.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 24818.3, "ram_available_mb": 100953.9, "ram_percent": 19.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.8, "peak": 16.26, "min": 13.92}, "VDD_CPU_SOC_MSS": {"avg": 15.29, "peak": 17.32, "min": 14.17}, "VDD_GPU": {"avg": 32.59, "peak": 44.91, "min": 25.22}, "VIN": {"avg": 73.57, "peak": 116.83, "min": 57.1}}, "power_watts_avg": 32.59, "energy_joules_est": 134.84, "sample_count": 32, "duration_seconds": 4.138}, "timestamp": "2026-01-17T14:29:11.617006"}
{"image_index": 44, "image_name": "000000004495.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000004495.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3273.448, "latencies_ms": [3273.448], "images_per_second": 0.305, "prompt_tokens": 19, "response_tokens_est": 47, "n_tiles": 12, "output_text": "The room is dimly lit, with a warm and cozy ambiance. The walls are painted in a light yellow color, and the furniture includes a plaid-patterned armchair and a blue and red checkered sofa.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24818.3, "ram_available_mb": 100953.9, "ram_percent": 19.7}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 24818.3, "ram_available_mb": 100953.9, "ram_percent": 19.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.84, "peak": 16.15, "min": 13.82}, "VDD_CPU_SOC_MSS": {"avg": 15.32, "peak": 16.92, "min": 14.17}, "VDD_GPU": {"avg": 34.87, "peak": 45.69, "min": 25.61}, "VIN": {"avg": 73.78, "peak": 110.15, "min": 60.93}}, "power_watts_avg": 34.87, "energy_joules_est": 114.16, "sample_count": 25, "duration_seconds": 3.274}, "timestamp": "2026-01-17T14:29:14.896806"}
{"image_index": 45, "image_name": "000000004765.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000004765.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 526.528, "latencies_ms": [526.528], "images_per_second": 1.899, "prompt_tokens": 9, "response_tokens_est": 20, "n_tiles": 1, "output_text": "A surfer is riding a wave in the ocean, wearing a yellow shirt and red shorts.", "error": null, "sys_before": {"cpu_percent": 18.7, "ram_used_mb": 24822.0, "ram_available_mb": 100950.2, "ram_percent": 19.7}, "sys_after": {"cpu_percent": 6.2, "ram_used_mb": 24823.0, "ram_available_mb": 100949.2, "ram_percent": 19.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 4845.7, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.57, "peak": 14.91, "min": 14.2}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.36, "min": 14.57}, "VDD_GPU": {"avg": 25.08, "peak": 26.79, "min": 23.64}, "VIN": {"avg": 59.23, "peak": 61.74, "min": 55.13}}, "power_watts_avg": 25.08, "energy_joules_est": 13.22, "sample_count": 3, "duration_seconds": 0.527}, "timestamp": "2026-01-17T14:29:15.450717"}
{"image_index": 45, "image_name": "000000004765.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000004765.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 944.006, "latencies_ms": [944.006], "images_per_second": 1.059, "prompt_tokens": 23, "response_tokens_est": 40, "n_tiles": 1, "output_text": "object: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 24823.0, "ram_available_mb": 100949.2, "ram_percent": 19.7}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24823.2, "ram_available_mb": 100949.0, "ram_percent": 19.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 4856.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.24, "peak": 15.52, "min": 14.81}, "VDD_CPU_SOC_MSS": {"avg": 16.2, "peak": 16.54, "min": 15.75}, "VDD_GPU": {"avg": 22.45, "peak": 25.6, "min": 20.48}, "VIN": {"avg": 63.48, "peak": 71.01, "min": 54.76}}, "power_watts_avg": 22.45, "energy_joules_est": 21.21, "sample_count": 7, "duration_seconds": 0.945}, "timestamp": "2026-01-17T14:29:16.400897"}
{"image_index": 45, "image_name": "000000004765.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000004765.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1407.83, "latencies_ms": [1407.83], "images_per_second": 0.71, "prompt_tokens": 27, "response_tokens_est": 61, "n_tiles": 1, "output_text": "The main object, the surfer, is positioned in the foreground, riding a wave. The wave is in the background, creating a dynamic and energetic scene. The surfboard is near the surfer, with the surfer's feet firmly on the board, indicating their active engagement in the sport.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24823.2, "ram_available_mb": 100949.0, "ram_percent": 19.7}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24823.2, "ram_available_mb": 100949.0, "ram_percent": 19.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 4859.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.46, "peak": 15.62, "min": 15.11}, "VDD_CPU_SOC_MSS": {"avg": 16.5, "peak": 16.94, "min": 16.14}, "VDD_GPU": {"avg": 21.35, "peak": 24.43, "min": 20.1}, "VIN": {"avg": 62.72, "peak": 67.02, "min": 54.02}}, "power_watts_avg": 21.35, "energy_joules_est": 30.06, "sample_count": 11, "duration_seconds": 1.408}, "timestamp": "2026-01-17T14:29:17.814114"}
{"image_index": 45, "image_name": "000000004765.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000004765.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1731.007, "latencies_ms": [1731.007], "images_per_second": 0.578, "prompt_tokens": 21, "response_tokens_est": 76, "n_tiles": 1, "output_text": "The scene captures a surfer riding a wave in the ocean. The surfer, dressed in a yellow shirt and red shorts, is skillfully maneuvering a surfboard amidst the frothy white waves. The setting is a dynamic and energetic environment typical of a surfing session, with the surfer's focused posture and the swirling water indicating the intensity of the ride.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24823.2, "ram_available_mb": 100949.0, "ram_percent": 19.7}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24823.2, "ram_available_mb": 100949.0, "ram_percent": 19.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 4854.7, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.47, "peak": 15.72, "min": 15.11}, "VDD_CPU_SOC_MSS": {"avg": 16.66, "peak": 16.94, "min": 16.15}, "VDD_GPU": {"avg": 20.89, "peak": 24.03, "min": 19.7}, "VIN": {"avg": 62.9, "peak": 68.3, "min": 56.59}}, "power_watts_avg": 20.89, "energy_joules_est": 36.17, "sample_count": 13, "duration_seconds": 1.731}, "timestamp": "2026-01-17T14:29:19.550743"}
{"image_index": 45, "image_name": "000000004765.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000004765.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 766.51, "latencies_ms": [766.51], "images_per_second": 1.305, "prompt_tokens": 19, "response_tokens_est": 32, "n_tiles": 1, "output_text": "The surfer is wearing a yellow shirt and red shorts, which stand out against the blue-green water. The lighting is bright, indicating it is daytime.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24823.2, "ram_available_mb": 100949.0, "ram_percent": 19.7}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24823.9, "ram_available_mb": 100948.2, "ram_percent": 19.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 4853.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.42, "peak": 15.62, "min": 15.22}, "VDD_CPU_SOC_MSS": {"avg": 16.46, "peak": 16.54, "min": 16.15}, "VDD_GPU": {"avg": 22.22, "peak": 24.03, "min": 20.89}, "VIN": {"avg": 62.26, "peak": 63.77, "min": 60.87}}, "power_watts_avg": 22.22, "energy_joules_est": 17.05, "sample_count": 5, "duration_seconds": 0.767}, "timestamp": "2026-01-17T14:29:20.327222"}
{"image_index": 46, "image_name": "000000004795.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000004795.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2423.972, "latencies_ms": [2423.972], "images_per_second": 0.413, "prompt_tokens": 9, "response_tokens_est": 24, "n_tiles": 12, "output_text": "The image shows a cat sitting on a desk with a laptop, a phone, and a keyboard in the background.", "error": null, "sys_before": {"cpu_percent": 43.6, "ram_used_mb": 24833.7, "ram_available_mb": 100938.5, "ram_percent": 19.7}, "sys_after": {"cpu_percent": 2.5, "ram_used_mb": 24829.6, "ram_available_mb": 100942.5, "ram_percent": 19.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.25, "peak": 16.66, "min": 14.0}, "VDD_CPU_SOC_MSS": {"avg": 16.07, "peak": 17.71, "min": 14.96}, "VDD_GPU": {"avg": 36.18, "peak": 42.94, "min": 27.98}, "VIN": {"avg": 79.48, "peak": 118.57, "min": 63.69}}, "power_watts_avg": 36.18, "energy_joules_est": 87.72, "sample_count": 18, "duration_seconds": 2.424}, "timestamp": "2026-01-17T14:29:22.823656"}
{"image_index": 46, "image_name": "000000004795.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000004795.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 3402.523, "latencies_ms": [3402.523], "images_per_second": 0.294, "prompt_tokens": 23, "response_tokens_est": 51, "n_tiles": 12, "output_text": "- Laptop: 1\n- Keyboard: 1\n- Phone: 1\n- Computer mouse: 1\n- Computer monitor: 1\n- Computer: 1\n- Mouse: 1\n- Mouse pad: 1", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24829.6, "ram_available_mb": 100942.5, "ram_percent": 19.7}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 24822.6, "ram_available_mb": 100949.6, "ram_percent": 19.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.12, "peak": 16.56, "min": 14.12}, "VDD_CPU_SOC_MSS": {"avg": 15.66, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 34.49, "peak": 45.69, "min": 26.0}, "VIN": {"avg": 72.42, "peak": 100.79, "min": 64.64}}, "power_watts_avg": 34.49, "energy_joules_est": 117.37, "sample_count": 26, "duration_seconds": 3.403}, "timestamp": "2026-01-17T14:29:26.233245"}
{"image_index": 46, "image_name": "000000004795.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000004795.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3943.748, "latencies_ms": [3943.748], "images_per_second": 0.254, "prompt_tokens": 27, "response_tokens_est": 67, "n_tiles": 12, "output_text": "The main objects in the image are a laptop, a cat, and a phone. The laptop is positioned in the foreground, with the cat resting on it. The phone is placed to the right of the laptop, and the cat is near the phone. The background is out of focus, emphasizing the laptop and the cat.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 24822.6, "ram_available_mb": 100949.6, "ram_percent": 19.7}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 24824.1, "ram_available_mb": 100948.1, "ram_percent": 19.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.12, "peak": 16.56, "min": 14.12}, "VDD_CPU_SOC_MSS": {"avg": 15.63, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 33.07, "peak": 44.51, "min": 25.61}, "VIN": {"avg": 76.19, "peak": 117.15, "min": 64.53}}, "power_watts_avg": 33.07, "energy_joules_est": 130.43, "sample_count": 31, "duration_seconds": 3.944}, "timestamp": "2026-01-17T14:29:30.183367"}
{"image_index": 46, "image_name": "000000004795.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000004795.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3292.683, "latencies_ms": [3292.683], "images_per_second": 0.304, "prompt_tokens": 21, "response_tokens_est": 48, "n_tiles": 12, "output_text": "The image depicts a cluttered office workspace with a laptop, a phone, and a cat. The cat is resting on the laptop, and the office environment includes various items such as a keyboard, a mouse, and a phone.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24824.1, "ram_available_mb": 100948.1, "ram_percent": 19.7}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 24824.1, "ram_available_mb": 100948.1, "ram_percent": 19.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.17, "peak": 16.56, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.67, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 34.36, "peak": 45.28, "min": 26.0}, "VIN": {"avg": 76.89, "peak": 114.22, "min": 60.47}}, "power_watts_avg": 34.36, "energy_joules_est": 113.15, "sample_count": 26, "duration_seconds": 3.293}, "timestamp": "2026-01-17T14:29:33.482340"}
{"image_index": 46, "image_name": "000000004795.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000004795.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3331.748, "latencies_ms": [3331.748], "images_per_second": 0.3, "prompt_tokens": 19, "response_tokens_est": 49, "n_tiles": 12, "output_text": "The image features a laptop with a white keyboard and a black mouse. The laptop is placed on a wooden desk, and the background includes a computer monitor displaying a webpage. The lighting is bright, and the overall setting appears to be indoors.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24825.1, "ram_available_mb": 100947.1, "ram_percent": 19.7}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 24825.1, "ram_available_mb": 100947.1, "ram_percent": 19.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.13, "peak": 16.46, "min": 14.12}, "VDD_CPU_SOC_MSS": {"avg": 15.64, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 34.51, "peak": 45.69, "min": 26.01}, "VIN": {"avg": 76.73, "peak": 108.28, "min": 61.5}}, "power_watts_avg": 34.51, "energy_joules_est": 115.0, "sample_count": 26, "duration_seconds": 3.332}, "timestamp": "2026-01-17T14:29:36.820718"}
{"image_index": 47, "image_name": "000000005001.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005001.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2861.13, "latencies_ms": [2861.13], "images_per_second": 0.35, "prompt_tokens": 9, "response_tokens_est": 37, "n_tiles": 12, "output_text": "A group of people is gathered outside a building, with a man holding a pair of scissors and cutting a red ribbon, while a young girl in a pink jacket looks on.", "error": null, "sys_before": {"cpu_percent": 37.8, "ram_used_mb": 24836.0, "ram_available_mb": 100936.2, "ram_percent": 19.7}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 24829.9, "ram_available_mb": 100942.2, "ram_percent": 19.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.1, "peak": 16.56, "min": 13.39}, "VDD_CPU_SOC_MSS": {"avg": 15.8, "peak": 17.71, "min": 14.18}, "VDD_GPU": {"avg": 34.94, "peak": 44.12, "min": 26.39}, "VIN": {"avg": 74.99, "peak": 111.33, "min": 60.56}}, "power_watts_avg": 34.94, "energy_joules_est": 99.99, "sample_count": 22, "duration_seconds": 2.862}, "timestamp": "2026-01-17T14:29:39.773629"}
{"image_index": 47, "image_name": "000000005001.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005001.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 3028.148, "latencies_ms": [3028.148], "images_per_second": 0.33, "prompt_tokens": 23, "response_tokens_est": 40, "n_tiles": 12, "output_text": "object: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24829.9, "ram_available_mb": 100942.2, "ram_percent": 19.7}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 24826.8, "ram_available_mb": 100945.4, "ram_percent": 19.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.17, "peak": 16.56, "min": 14.12}, "VDD_CPU_SOC_MSS": {"avg": 15.7, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 35.32, "peak": 45.69, "min": 26.4}, "VIN": {"avg": 78.05, "peak": 119.48, "min": 66.05}}, "power_watts_avg": 35.32, "energy_joules_est": 106.97, "sample_count": 24, "duration_seconds": 3.029}, "timestamp": "2026-01-17T14:29:42.808173"}
{"image_index": 47, "image_name": "000000005001.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005001.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3699.039, "latencies_ms": [3699.039], "images_per_second": 0.27, "prompt_tokens": 27, "response_tokens_est": 60, "n_tiles": 12, "output_text": "In the image, the main objects are a group of people gathered around a table, with a child holding a ribbon. The child is positioned near the table, while the other individuals are standing in the background. The ribbon is held by the child, and the table is situated in the foreground.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24826.8, "ram_available_mb": 100945.4, "ram_percent": 19.7}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 24827.8, "ram_available_mb": 100944.4, "ram_percent": 19.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.17, "peak": 16.56, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.65, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 33.65, "peak": 44.91, "min": 26.0}, "VIN": {"avg": 76.61, "peak": 116.04, "min": 64.48}}, "power_watts_avg": 33.65, "energy_joules_est": 124.48, "sample_count": 29, "duration_seconds": 3.699}, "timestamp": "2026-01-17T14:29:46.513453"}
{"image_index": 47, "image_name": "000000005001.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005001.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 4181.917, "latencies_ms": [4181.917], "images_per_second": 0.239, "prompt_tokens": 21, "response_tokens_est": 74, "n_tiles": 12, "output_text": "The scene depicts a group of people gathered outside a building, likely at a ribbon-cutting ceremony. The setting appears to be a public space, possibly a street or a plaza, with a blue balloon and a red ribbon being the focal points of the event. The individuals are dressed in casual attire, and some are holding cameras, capturing the moment.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24827.8, "ram_available_mb": 100944.4, "ram_percent": 19.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 24829.0, "ram_available_mb": 100943.1, "ram_percent": 19.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.14, "peak": 16.56, "min": 14.12}, "VDD_CPU_SOC_MSS": {"avg": 15.61, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 32.64, "peak": 44.51, "min": 25.61}, "VIN": {"avg": 75.17, "peak": 115.44, "min": 55.94}}, "power_watts_avg": 32.64, "energy_joules_est": 136.51, "sample_count": 32, "duration_seconds": 4.182}, "timestamp": "2026-01-17T14:29:50.701657"}
{"image_index": 47, "image_name": "000000005001.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005001.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3397.405, "latencies_ms": [3397.405], "images_per_second": 0.294, "prompt_tokens": 19, "response_tokens_est": 51, "n_tiles": 12, "output_text": "The image depicts a group of people gathered around a table, with a blue balloon and a red ribbon being held by a child. The scene is well-lit, with a mix of natural and artificial light, creating a vibrant and lively atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24829.0, "ram_available_mb": 100943.1, "ram_percent": 19.7}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 24830.0, "ram_available_mb": 100942.2, "ram_percent": 19.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.16, "peak": 16.56, "min": 14.12}, "VDD_CPU_SOC_MSS": {"avg": 15.66, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 34.33, "peak": 44.51, "min": 26.01}, "VIN": {"avg": 79.55, "peak": 116.98, "min": 55.27}}, "power_watts_avg": 34.33, "energy_joules_est": 116.64, "sample_count": 26, "duration_seconds": 3.398}, "timestamp": "2026-01-17T14:29:54.105693"}
{"image_index": 48, "image_name": "000000005037.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005037.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1386.488, "latencies_ms": [1386.488], "images_per_second": 0.721, "prompt_tokens": 9, "response_tokens_est": 29, "n_tiles": 6, "output_text": "A bus with the destination \"First Group\" is parked on a street, with a sign indicating it is free to use Wi-Fi onboard.", "error": null, "sys_before": {"cpu_percent": 34.9, "ram_used_mb": 24853.4, "ram_available_mb": 100918.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 24854.4, "ram_available_mb": 100917.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.56, "peak": 15.04, "min": 13.61}, "VDD_CPU_SOC_MSS": {"avg": 15.24, "peak": 15.75, "min": 14.18}, "VDD_GPU": {"avg": 33.88, "peak": 40.57, "min": 26.39}, "VIN": {"avg": 74.77, "peak": 116.9, "min": 55.85}}, "power_watts_avg": 33.88, "energy_joules_est": 46.99, "sample_count": 10, "duration_seconds": 1.387}, "timestamp": "2026-01-17T14:29:55.560877"}
{"image_index": 48, "image_name": "000000005037.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005037.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1776.954, "latencies_ms": [1776.954], "images_per_second": 0.563, "prompt_tokens": 23, "response_tokens_est": 40, "n_tiles": 6, "output_text": "object: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24854.4, "ram_available_mb": 100917.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 24855.1, "ram_available_mb": 100917.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.86, "peak": 15.44, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.38, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 32.18, "peak": 40.57, "min": 24.04}, "VIN": {"avg": 68.37, "peak": 85.72, "min": 56.82}}, "power_watts_avg": 32.18, "energy_joules_est": 57.2, "sample_count": 13, "duration_seconds": 1.777}, "timestamp": "2026-01-17T14:29:57.344023"}
{"image_index": 48, "image_name": "000000005037.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005037.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2932.363, "latencies_ms": [2932.363], "images_per_second": 0.341, "prompt_tokens": 27, "response_tokens_est": 81, "n_tiles": 6, "output_text": "The main object in the foreground is a bus, which is positioned on the right side of the image. The bus is facing the left side of the frame, indicating that it is moving towards the viewer. In the background, there are buildings and a street, which are further away from the bus. The bus is parked on the street, and there are other vehicles and people visible in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24855.1, "ram_available_mb": 100917.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 24856.6, "ram_available_mb": 100915.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.83, "peak": 15.14, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.41, "peak": 15.76, "min": 14.56}, "VDD_GPU": {"avg": 28.16, "peak": 40.97, "min": 22.86}, "VIN": {"avg": 68.7, "peak": 113.36, "min": 56.38}}, "power_watts_avg": 28.16, "energy_joules_est": 82.58, "sample_count": 23, "duration_seconds": 2.933}, "timestamp": "2026-01-17T14:30:00.282429"}
{"image_index": 48, "image_name": "000000005037.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005037.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2476.894, "latencies_ms": [2476.894], "images_per_second": 0.404, "prompt_tokens": 21, "response_tokens_est": 65, "n_tiles": 6, "output_text": "The image depicts a modern city bus on a city street, with a clear sky overhead. The bus is adorned with a white and purple color scheme, and the destination sign reads \"First Group.\" The bus is parked on a street with a pedestrian crossing, and there are buildings and other vehicles in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24856.6, "ram_available_mb": 100915.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 24857.8, "ram_available_mb": 100914.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.81, "peak": 15.14, "min": 14.2}, "VDD_CPU_SOC_MSS": {"avg": 15.27, "peak": 15.74, "min": 14.57}, "VDD_GPU": {"avg": 29.15, "peak": 40.57, "min": 22.86}, "VIN": {"avg": 69.75, "peak": 114.39, "min": 48.05}}, "power_watts_avg": 29.15, "energy_joules_est": 72.21, "sample_count": 19, "duration_seconds": 2.477}, "timestamp": "2026-01-17T14:30:02.765616"}
{"image_index": 48, "image_name": "000000005037.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005037.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1513.551, "latencies_ms": [1513.551], "images_per_second": 0.661, "prompt_tokens": 19, "response_tokens_est": 31, "n_tiles": 6, "output_text": "The bus in the image is predominantly white with purple and pink accents. The lighting is bright, indicating daytime, and the weather appears to be clear.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24857.8, "ram_available_mb": 100914.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 24858.5, "ram_available_mb": 100913.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.72, "peak": 15.14, "min": 14.2}, "VDD_CPU_SOC_MSS": {"avg": 15.25, "peak": 15.74, "min": 14.56}, "VDD_GPU": {"avg": 33.13, "peak": 40.18, "min": 25.61}, "VIN": {"avg": 72.11, "peak": 116.68, "min": 61.92}}, "power_watts_avg": 33.13, "energy_joules_est": 50.16, "sample_count": 11, "duration_seconds": 1.514}, "timestamp": "2026-01-17T14:30:04.285430"}
{"image_index": 49, "image_name": "000000005060.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005060.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2289.188, "latencies_ms": [2289.188], "images_per_second": 0.437, "prompt_tokens": 9, "response_tokens_est": 20, "n_tiles": 12, "output_text": "A man is sitting on the floor in a room, taking a selfie with his smartphone.", "error": null, "sys_before": {"cpu_percent": 42.4, "ram_used_mb": 24831.8, "ram_available_mb": 100940.3, "ram_percent": 19.7}, "sys_after": {"cpu_percent": 2.4, "ram_used_mb": 24832.0, "ram_available_mb": 100940.1, "ram_percent": 19.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.12, "peak": 16.56, "min": 13.29}, "VDD_CPU_SOC_MSS": {"avg": 15.83, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 36.66, "peak": 43.73, "min": 28.37}, "VIN": {"avg": 80.83, "peak": 112.92, "min": 65.57}}, "power_watts_avg": 36.66, "energy_joules_est": 83.93, "sample_count": 18, "duration_seconds": 2.289}, "timestamp": "2026-01-17T14:30:06.676906"}
{"image_index": 49, "image_name": "000000005060.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005060.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 3470.649, "latencies_ms": [3470.649], "images_per_second": 0.288, "prompt_tokens": 23, "response_tokens_est": 53, "n_tiles": 12, "output_text": "- man: 1\n- green shirt: 1\n- black flip-flops: 2\n- phone: 1\n- mirror: 1\n- wooden floor: 1\n- wall: 1\n- door: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24832.0, "ram_available_mb": 100940.1, "ram_percent": 19.7}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 24832.5, "ram_available_mb": 100939.6, "ram_percent": 19.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.18, "peak": 16.56, "min": 14.12}, "VDD_CPU_SOC_MSS": {"avg": 15.66, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 34.4, "peak": 45.69, "min": 26.0}, "VIN": {"avg": 76.84, "peak": 132.48, "min": 57.39}}, "power_watts_avg": 34.4, "energy_joules_est": 119.41, "sample_count": 27, "duration_seconds": 3.471}, "timestamp": "2026-01-17T14:30:10.154112"}
{"image_index": 49, "image_name": "000000005060.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005060.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3539.647, "latencies_ms": [3539.647], "images_per_second": 0.283, "prompt_tokens": 27, "response_tokens_est": 55, "n_tiles": 12, "output_text": "The main object in the foreground is a man taking a selfie in a mirror. The mirror is placed on a wooden floor, and the man is sitting on the floor. The background features a room with a wooden floor, a red wall, and a white door.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24832.5, "ram_available_mb": 100939.6, "ram_percent": 19.7}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 24832.8, "ram_available_mb": 100939.4, "ram_percent": 19.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.15, "peak": 16.56, "min": 14.12}, "VDD_CPU_SOC_MSS": {"avg": 15.63, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 34.01, "peak": 45.69, "min": 26.0}, "VIN": {"avg": 76.94, "peak": 115.3, "min": 56.55}}, "power_watts_avg": 34.01, "energy_joules_est": 120.4, "sample_count": 28, "duration_seconds": 3.54}, "timestamp": "2026-01-17T14:30:13.700365"}
{"image_index": 49, "image_name": "000000005060.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005060.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3672.638, "latencies_ms": [3672.638], "images_per_second": 0.272, "prompt_tokens": 21, "response_tokens_est": 59, "n_tiles": 12, "output_text": "The image depicts a man sitting on a wooden floor in a room with a wooden door frame and a mirror. He is taking a selfie with his smartphone, capturing a moment in his home. The room has a warm, homely feel with wooden flooring and a light-colored wall.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24832.8, "ram_available_mb": 100939.4, "ram_percent": 19.7}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 24832.8, "ram_available_mb": 100939.4, "ram_percent": 19.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.17, "peak": 16.56, "min": 14.12}, "VDD_CPU_SOC_MSS": {"avg": 15.65, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 33.61, "peak": 44.91, "min": 26.0}, "VIN": {"avg": 79.3, "peak": 134.81, "min": 59.12}}, "power_watts_avg": 33.61, "energy_joules_est": 123.45, "sample_count": 28, "duration_seconds": 3.673}, "timestamp": "2026-01-17T14:30:17.379198"}
{"image_index": 49, "image_name": "000000005060.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005060.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3226.089, "latencies_ms": [3226.089], "images_per_second": 0.31, "prompt_tokens": 19, "response_tokens_est": 46, "n_tiles": 12, "output_text": "The image features a wooden-framed mirror with a warm, natural finish, placed on a light wooden floor. The room is well-lit with natural light streaming in through a window, creating a bright and airy atmosphere.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24832.8, "ram_available_mb": 100939.4, "ram_percent": 19.7}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 24833.7, "ram_available_mb": 100938.4, "ram_percent": 19.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.17, "peak": 16.56, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.65, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 34.98, "peak": 45.69, "min": 26.01}, "VIN": {"avg": 76.49, "peak": 105.31, "min": 65.65}}, "power_watts_avg": 34.98, "energy_joules_est": 112.86, "sample_count": 25, "duration_seconds": 3.227}, "timestamp": "2026-01-17T14:30:20.615696"}
{"image_index": 50, "image_name": "000000005193.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005193.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1489.869, "latencies_ms": [1489.869], "images_per_second": 0.671, "prompt_tokens": 9, "response_tokens_est": 33, "n_tiles": 6, "output_text": "The image depicts a group of young men and a woman, all holding surfboards, standing in a room with various surfboards and equipment in the background.", "error": null, "sys_before": {"cpu_percent": 46.8, "ram_used_mb": 24850.8, "ram_available_mb": 100921.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 24852.1, "ram_available_mb": 100920.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.69, "peak": 15.14, "min": 13.72}, "VDD_CPU_SOC_MSS": {"avg": 15.36, "peak": 16.14, "min": 14.18}, "VDD_GPU": {"avg": 33.2, "peak": 40.97, "min": 25.61}, "VIN": {"avg": 73.09, "peak": 108.26, "min": 50.13}}, "power_watts_avg": 33.2, "energy_joules_est": 49.47, "sample_count": 11, "duration_seconds": 1.49}, "timestamp": "2026-01-17T14:30:22.165365"}
{"image_index": 50, "image_name": "000000005193.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005193.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2518.017, "latencies_ms": [2518.017], "images_per_second": 0.397, "prompt_tokens": 23, "response_tokens_est": 69, "n_tiles": 6, "output_text": "1. Surfboards: 3\n2. People: 4\n3. Water bottles: 1\n4. Bottles: 1\n5. Air conditioner: 1\n6. Air duct: 1\n7. Wall-mounted light: 1\n8. Wall-mounted fan: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24852.1, "ram_available_mb": 100920.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 24853.3, "ram_available_mb": 100918.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.03, "peak": 15.44, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.64, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 29.49, "peak": 40.57, "min": 23.25}, "VIN": {"avg": 72.63, "peak": 109.44, "min": 57.65}}, "power_watts_avg": 29.49, "energy_joules_est": 74.27, "sample_count": 19, "duration_seconds": 2.518}, "timestamp": "2026-01-17T14:30:24.689331"}
{"image_index": 50, "image_name": "000000005193.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005193.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2302.807, "latencies_ms": [2302.807], "images_per_second": 0.434, "prompt_tokens": 27, "response_tokens_est": 61, "n_tiles": 6, "output_text": "The main objects in the image are a group of young men holding surfboards. The surfboards are positioned in the foreground, with the men standing in the background. The surfboards are near the men, and the background includes a wall with various items, such as a water bottle and a fan.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24853.3, "ram_available_mb": 100918.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 24853.3, "ram_available_mb": 100918.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.04, "peak": 15.44, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.62, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 29.79, "peak": 40.18, "min": 23.64}, "VIN": {"avg": 68.11, "peak": 93.31, "min": 56.75}}, "power_watts_avg": 29.79, "energy_joules_est": 68.61, "sample_count": 18, "duration_seconds": 2.303}, "timestamp": "2026-01-17T14:30:26.998159"}
{"image_index": 50, "image_name": "000000005193.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005193.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2513.749, "latencies_ms": [2513.749], "images_per_second": 0.398, "prompt_tokens": 21, "response_tokens_est": 69, "n_tiles": 6, "output_text": "The scene depicts a group of young men gathered indoors, likely in a room with a white wall and a window. They are holding surfboards, with one of them holding a surfboard with a yellow and red design. The setting appears to be a casual, possibly recreational space, with personal items and surfboards visible in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24853.3, "ram_available_mb": 100918.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 24853.5, "ram_available_mb": 100918.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.03, "peak": 15.34, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.65, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 29.23, "peak": 40.57, "min": 23.25}, "VIN": {"avg": 69.88, "peak": 88.2, "min": 63.35}}, "power_watts_avg": 29.23, "energy_joules_est": 73.49, "sample_count": 20, "duration_seconds": 2.514}, "timestamp": "2026-01-17T14:30:29.517914"}
{"image_index": 50, "image_name": "000000005193.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005193.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2508.885, "latencies_ms": [2508.885], "images_per_second": 0.399, "prompt_tokens": 19, "response_tokens_est": 69, "n_tiles": 6, "output_text": "The image depicts a group of young men holding surfboards, with one man in the foreground holding a surfboard with a vibrant yellow and red design. The lighting is dim, suggesting an indoor setting, possibly during an evening or nighttime event. The surfboards are made of wood, and the men are dressed in casual attire.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24853.5, "ram_available_mb": 100918.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 24854.3, "ram_available_mb": 100917.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.04, "peak": 15.44, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.64, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 29.55, "peak": 40.57, "min": 23.64}, "VIN": {"avg": 69.86, "peak": 108.17, "min": 58.48}}, "power_watts_avg": 29.55, "energy_joules_est": 74.15, "sample_count": 19, "duration_seconds": 2.509}, "timestamp": "2026-01-17T14:30:32.032967"}
{"image_index": 51, "image_name": "000000005477.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005477.jpg", "image_width": 640, "image_height": 349, "image_resolution": "640x349", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1047.819, "latencies_ms": [1047.819], "images_per_second": 0.954, "prompt_tokens": 9, "response_tokens_est": 38, "n_tiles": 2, "output_text": "The image features a large, yellow airplane with the word \"LOT\" written on its fuselage, parked on a tarmac with a cloudy sky in the background.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 24854.3, "ram_available_mb": 100917.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 24855.3, "ram_available_mb": 100916.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5055.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.73, "peak": 15.32, "min": 14.1}, "VDD_CPU_SOC_MSS": {"avg": 15.55, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 25.46, "peak": 31.51, "min": 21.66}, "VIN": {"avg": 68.18, "peak": 100.4, "min": 59.68}}, "power_watts_avg": 25.46, "energy_joules_est": 26.69, "sample_count": 8, "duration_seconds": 1.048}, "timestamp": "2026-01-17T14:30:33.110393"}
{"image_index": 51, "image_name": "000000005477.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005477.jpg", "image_width": 640, "image_height": 349, "image_resolution": "640x349", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1291.012, "latencies_ms": [1291.012], "images_per_second": 0.775, "prompt_tokens": 23, "response_tokens_est": 48, "n_tiles": 2, "output_text": "- airplane: 1\n- plane: 1\n- aircraft: 1\n- aircraft: 1\n- airplane: 1\n- plane: 1\n- airplane: 1\n- plane: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24855.3, "ram_available_mb": 100916.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 24855.5, "ram_available_mb": 100916.7, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5064.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.91, "peak": 15.11, "min": 14.51}, "VDD_CPU_SOC_MSS": {"avg": 15.87, "peak": 16.55, "min": 15.35}, "VDD_GPU": {"avg": 24.47, "peak": 31.12, "min": 21.28}, "VIN": {"avg": 67.11, "peak": 93.9, "min": 60.53}}, "power_watts_avg": 24.47, "energy_joules_est": 31.6, "sample_count": 10, "duration_seconds": 1.291}, "timestamp": "2026-01-17T14:30:34.407236"}
{"image_index": 51, "image_name": "000000005477.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005477.jpg", "image_width": 640, "image_height": 349, "image_resolution": "640x349", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1886.589, "latencies_ms": [1886.589], "images_per_second": 0.53, "prompt_tokens": 27, "response_tokens_est": 74, "n_tiles": 2, "output_text": "The main object in the image is a yellow airplane with the word \"LOT\" written on its fuselage. The airplane is positioned in the foreground, slightly to the right. In the background, there are other airplanes, including a red and white airplane, which are further away. The sky is partly cloudy, providing a backdrop to the scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24855.5, "ram_available_mb": 100916.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 24856.5, "ram_available_mb": 100915.7, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5067.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.11, "peak": 15.52, "min": 14.4}, "VDD_CPU_SOC_MSS": {"avg": 15.98, "peak": 16.54, "min": 15.36}, "VDD_GPU": {"avg": 23.45, "peak": 30.34, "min": 20.48}, "VIN": {"avg": 65.77, "peak": 98.88, "min": 56.44}}, "power_watts_avg": 23.45, "energy_joules_est": 44.25, "sample_count": 14, "duration_seconds": 1.887}, "timestamp": "2026-01-17T14:30:36.303635"}
{"image_index": 51, "image_name": "000000005477.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005477.jpg", "image_width": 640, "image_height": 349, "image_resolution": "640x349", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2045.37, "latencies_ms": [2045.37], "images_per_second": 0.489, "prompt_tokens": 21, "response_tokens_est": 81, "n_tiles": 2, "output_text": "The image depicts a large, yellow commercial airplane parked on a tarmac, with a cloudy sky in the background. The airplane is adorned with the logo and name \"LOT\" on its fuselage, and the word \"POLSKIE LINE\" is visible on the tail. The scene suggests that the airplane is either preparing for departure or has just arrived at the airport.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24856.5, "ram_available_mb": 100915.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24856.5, "ram_available_mb": 100915.7, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5063.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.17, "peak": 15.52, "min": 14.61}, "VDD_CPU_SOC_MSS": {"avg": 16.0, "peak": 16.16, "min": 15.36}, "VDD_GPU": {"avg": 22.9, "peak": 29.94, "min": 20.49}, "VIN": {"avg": 65.89, "peak": 109.35, "min": 59.47}}, "power_watts_avg": 22.9, "energy_joules_est": 46.85, "sample_count": 16, "duration_seconds": 2.046}, "timestamp": "2026-01-17T14:30:38.355419"}
{"image_index": 51, "image_name": "000000005477.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005477.jpg", "image_width": 640, "image_height": 349, "image_resolution": "640x349", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1486.299, "latencies_ms": [1486.299], "images_per_second": 0.673, "prompt_tokens": 19, "response_tokens_est": 57, "n_tiles": 2, "output_text": "The notable visual attributes of the airplane in the image include its golden color, which stands out against the cloudy sky. The lighting is bright, casting shadows on the runway, indicating it is daytime. The weather appears to be partly cloudy, with patches of blue sky visible.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24856.5, "ram_available_mb": 100915.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 24856.5, "ram_available_mb": 100915.7, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5062.6, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.09, "peak": 15.42, "min": 14.61}, "VDD_CPU_SOC_MSS": {"avg": 15.89, "peak": 16.54, "min": 14.96}, "VDD_GPU": {"avg": 23.93, "peak": 29.94, "min": 20.89}, "VIN": {"avg": 64.62, "peak": 87.59, "min": 58.22}}, "power_watts_avg": 23.93, "energy_joules_est": 35.58, "sample_count": 11, "duration_seconds": 1.487}, "timestamp": "2026-01-17T14:30:39.847531"}
{"image_index": 52, "image_name": "000000005503.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005503.jpg", "image_width": 333, "image_height": 500, "image_resolution": "333x500", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1011.0, "latencies_ms": [1011.0], "images_per_second": 0.989, "prompt_tokens": 9, "response_tokens_est": 15, "n_tiles": 6, "output_text": "A person is holding a toilet seat in a black and white photo.", "error": null, "sys_before": {"cpu_percent": 40.0, "ram_used_mb": 24856.5, "ram_available_mb": 100915.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 24857.5, "ram_available_mb": 100914.7, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.61, "peak": 15.14, "min": 14.1}, "VDD_CPU_SOC_MSS": {"avg": 15.3, "peak": 16.14, "min": 14.57}, "VDD_GPU": {"avg": 35.12, "peak": 39.39, "min": 29.56}, "VIN": {"avg": 81.9, "peak": 105.67, "min": 57.74}}, "power_watts_avg": 35.12, "energy_joules_est": 35.52, "sample_count": 7, "duration_seconds": 1.011}, "timestamp": "2026-01-17T14:30:40.911248"}
{"image_index": 52, "image_name": "000000005503.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005503.jpg", "image_width": 333, "image_height": 500, "image_resolution": "333x500", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2322.32, "latencies_ms": [2322.32], "images_per_second": 0.431, "prompt_tokens": 23, "response_tokens_est": 62, "n_tiles": 6, "output_text": "toilet paper roll: 1\ntoilet brush: 1\ntoilet seat cover: 1\ntoilet paper holder: 1\ntoilet paper: 1\ntoilet paper roll holder: 1\ntoilet brush holder: 1\ntoilet brush: 1", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 24857.5, "ram_available_mb": 100914.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 24857.7, "ram_available_mb": 100914.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.0, "peak": 15.34, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.6, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 30.73, "peak": 41.76, "min": 23.64}, "VIN": {"avg": 72.42, "peak": 112.84, "min": 61.47}}, "power_watts_avg": 30.73, "energy_joules_est": 71.38, "sample_count": 18, "duration_seconds": 2.323}, "timestamp": "2026-01-17T14:30:43.239581"}
{"image_index": 52, "image_name": "000000005503.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005503.jpg", "image_width": 333, "image_height": 500, "image_resolution": "333x500", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2003.898, "latencies_ms": [2003.898], "images_per_second": 0.499, "prompt_tokens": 27, "response_tokens_est": 50, "n_tiles": 6, "output_text": "The main object in the foreground is a toilet with a seat and lid. The person's legs are visible, suggesting the person is standing near the toilet. The background is not clearly visible, but the person's hand is holding the toilet seat.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24857.7, "ram_available_mb": 100914.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 24857.7, "ram_available_mb": 100914.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.02, "peak": 15.54, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.62, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 30.81, "peak": 40.18, "min": 24.04}, "VIN": {"avg": 73.44, "peak": 112.67, "min": 58.56}}, "power_watts_avg": 30.81, "energy_joules_est": 61.75, "sample_count": 15, "duration_seconds": 2.004}, "timestamp": "2026-01-17T14:30:45.249300"}
{"image_index": 52, "image_name": "000000005503.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005503.jpg", "image_width": 333, "image_height": 500, "image_resolution": "333x500", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1837.379, "latencies_ms": [1837.379], "images_per_second": 0.544, "prompt_tokens": 21, "response_tokens_est": 44, "n_tiles": 6, "output_text": "The image depicts a black and white scene of a toilet with a person's legs visible, holding the toilet seat. The setting appears to be a bathroom, and the person is likely preparing to use the toilet.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24857.7, "ram_available_mb": 100914.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 24857.7, "ram_available_mb": 100914.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.97, "peak": 15.34, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.55, "peak": 16.14, "min": 14.57}, "VDD_GPU": {"avg": 31.66, "peak": 40.57, "min": 24.43}, "VIN": {"avg": 69.04, "peak": 95.83, "min": 48.57}}, "power_watts_avg": 31.66, "energy_joules_est": 58.19, "sample_count": 14, "duration_seconds": 1.838}, "timestamp": "2026-01-17T14:30:47.093291"}
{"image_index": 52, "image_name": "000000005503.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005503.jpg", "image_width": 333, "image_height": 500, "image_resolution": "333x500", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1675.991, "latencies_ms": [1675.991], "images_per_second": 0.597, "prompt_tokens": 19, "response_tokens_est": 38, "n_tiles": 6, "output_text": "The image is a black and white photograph of a toilet seat with a towel draped over it. The lighting is dim, and the overall atmosphere appears to be cold or chilly.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24857.7, "ram_available_mb": 100914.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 24857.9, "ram_available_mb": 100914.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.99, "peak": 15.44, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.52, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 32.77, "peak": 40.18, "min": 25.22}, "VIN": {"avg": 75.46, "peak": 107.66, "min": 61.0}}, "power_watts_avg": 32.77, "energy_joules_est": 54.93, "sample_count": 12, "duration_seconds": 1.676}, "timestamp": "2026-01-17T14:30:48.775950"}
{"image_index": 53, "image_name": "000000005529.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005529.jpg", "image_width": 444, "image_height": 640, "image_resolution": "444x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1387.177, "latencies_ms": [1387.177], "images_per_second": 0.721, "prompt_tokens": 9, "response_tokens_est": 28, "n_tiles": 6, "output_text": "A skier is navigating through a snowy landscape, wearing a blue jacket and a helmet, with snowflakes falling around them.", "error": null, "sys_before": {"cpu_percent": 38.8, "ram_used_mb": 24857.9, "ram_available_mb": 100914.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 24857.7, "ram_available_mb": 100914.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.55, "peak": 15.04, "min": 13.9}, "VDD_CPU_SOC_MSS": {"avg": 15.2, "peak": 15.74, "min": 14.17}, "VDD_GPU": {"avg": 33.48, "peak": 40.57, "min": 26.0}, "VIN": {"avg": 72.88, "peak": 109.39, "min": 61.03}}, "power_watts_avg": 33.48, "energy_joules_est": 46.45, "sample_count": 10, "duration_seconds": 1.387}, "timestamp": "2026-01-17T14:30:50.224418"}
{"image_index": 53, "image_name": "000000005529.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005529.jpg", "image_width": 444, "image_height": 640, "image_resolution": "444x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1737.223, "latencies_ms": [1737.223], "images_per_second": 0.576, "prompt_tokens": 23, "response_tokens_est": 39, "n_tiles": 6, "output_text": "1. Person\n2. Ski poles\n3. Ski\n4. Snow\n5. Snowboard\n6. Snowboarder\n7. Snowboard\n8. Snowboarder", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24857.7, "ram_available_mb": 100914.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 24857.7, "ram_available_mb": 100914.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.78, "peak": 15.14, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.23, "peak": 15.74, "min": 14.57}, "VDD_GPU": {"avg": 32.51, "peak": 41.34, "min": 24.42}, "VIN": {"avg": 72.43, "peak": 96.51, "min": 57.28}}, "power_watts_avg": 32.51, "energy_joules_est": 56.49, "sample_count": 13, "duration_seconds": 1.738}, "timestamp": "2026-01-17T14:30:51.967776"}
{"image_index": 53, "image_name": "000000005529.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005529.jpg", "image_width": 444, "image_height": 640, "image_resolution": "444x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3672.676, "latencies_ms": [3672.676], "images_per_second": 0.272, "prompt_tokens": 27, "response_tokens_est": 107, "n_tiles": 6, "output_text": "The main objects in the image are two skiers. The skier on the left is in the foreground, wearing a blue jacket and a white helmet, and is skiing on a snowy slope. The skier on the right is further back, also in a blue jacket and a white helmet, and is skiing on a snowy slope. The skiers are positioned near the center of the image, with the skier on the left slightly closer to the foreground and the skier on the right slightly further back.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24857.7, "ram_available_mb": 100914.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 24858.7, "ram_available_mb": 100913.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.84, "peak": 15.24, "min": 14.2}, "VDD_CPU_SOC_MSS": {"avg": 15.44, "peak": 15.75, "min": 14.56}, "VDD_GPU": {"avg": 27.1, "peak": 40.18, "min": 22.85}, "VIN": {"avg": 66.98, "peak": 107.0, "min": 50.84}}, "power_watts_avg": 27.1, "energy_joules_est": 99.54, "sample_count": 28, "duration_seconds": 3.673}, "timestamp": "2026-01-17T14:30:55.646736"}
{"image_index": 53, "image_name": "000000005529.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005529.jpg", "image_width": 444, "image_height": 640, "image_resolution": "444x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2873.757, "latencies_ms": [2873.757], "images_per_second": 0.348, "prompt_tokens": 21, "response_tokens_est": 79, "n_tiles": 6, "output_text": "The image depicts a snowy mountainous landscape with a person skiing down a snow-covered slope. The individual is wearing a blue jacket, black pants, and a white helmet, and is using ski poles to navigate the snowy terrain. The scene is set in a snowy environment, likely a ski resort or a mountainous area, with trees heavily laden with snow.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24858.7, "ram_available_mb": 100913.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 24859.9, "ram_available_mb": 100912.3, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.82, "peak": 15.14, "min": 14.2}, "VDD_CPU_SOC_MSS": {"avg": 15.34, "peak": 15.76, "min": 14.57}, "VDD_GPU": {"avg": 28.22, "peak": 40.18, "min": 22.86}, "VIN": {"avg": 72.43, "peak": 115.17, "min": 55.55}}, "power_watts_avg": 28.22, "energy_joules_est": 81.11, "sample_count": 22, "duration_seconds": 2.874}, "timestamp": "2026-01-17T14:30:58.526695"}
{"image_index": 53, "image_name": "000000005529.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005529.jpg", "image_width": 444, "image_height": 640, "image_resolution": "444x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2220.51, "latencies_ms": [2220.51], "images_per_second": 0.45, "prompt_tokens": 19, "response_tokens_est": 56, "n_tiles": 6, "output_text": "The image depicts a snowy landscape with a person skiing. The individual is wearing a blue jacket and a white helmet, and is using yellow ski poles. The snow is white and powdery, and the trees are covered in snow, indicating a winter setting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24859.9, "ram_available_mb": 100912.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 24860.1, "ram_available_mb": 100912.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.79, "peak": 15.14, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.38, "peak": 15.75, "min": 14.57}, "VDD_GPU": {"avg": 29.62, "peak": 39.78, "min": 23.25}, "VIN": {"avg": 71.75, "peak": 115.01, "min": 61.98}}, "power_watts_avg": 29.62, "energy_joules_est": 65.79, "sample_count": 17, "duration_seconds": 2.221}, "timestamp": "2026-01-17T14:31:00.753685"}
{"image_index": 54, "image_name": "000000005586.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005586.jpg", "image_width": 320, "image_height": 240, "image_resolution": "320x240", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2388.629, "latencies_ms": [2388.629], "images_per_second": 0.419, "prompt_tokens": 9, "response_tokens_est": 23, "n_tiles": 12, "output_text": "A tennis player is preparing to serve on a blue court, with a crowd of spectators in the background.", "error": null, "sys_before": {"cpu_percent": 42.7, "ram_used_mb": 24840.0, "ram_available_mb": 100932.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.3, "ram_used_mb": 24841.5, "ram_available_mb": 100930.7, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.16, "peak": 16.66, "min": 13.39}, "VDD_CPU_SOC_MSS": {"avg": 15.94, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 36.59, "peak": 43.33, "min": 28.37}, "VIN": {"avg": 76.96, "peak": 96.55, "min": 57.62}}, "power_watts_avg": 36.59, "energy_joules_est": 87.41, "sample_count": 18, "duration_seconds": 2.389}, "timestamp": "2026-01-17T14:31:03.233044"}
{"image_index": 54, "image_name": "000000005586.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005586.jpg", "image_width": 320, "image_height": 240, "image_resolution": "320x240", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 3023.963, "latencies_ms": [3023.963], "images_per_second": 0.331, "prompt_tokens": 23, "response_tokens_est": 40, "n_tiles": 12, "output_text": "1. Tennis player\n2. Tennis racket\n3. Tennis court\n4. Spectator\n5. Advertisements\n6. Stadium\n7. Ball\n8. Ball court", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24841.5, "ram_available_mb": 100930.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 24841.7, "ram_available_mb": 100930.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.18, "peak": 16.56, "min": 14.12}, "VDD_CPU_SOC_MSS": {"avg": 15.71, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 35.59, "peak": 44.91, "min": 26.4}, "VIN": {"avg": 75.55, "peak": 108.76, "min": 65.85}}, "power_watts_avg": 35.59, "energy_joules_est": 107.64, "sample_count": 23, "duration_seconds": 3.024}, "timestamp": "2026-01-17T14:31:06.262988"}
{"image_index": 54, "image_name": "000000005586.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005586.jpg", "image_width": 320, "image_height": 240, "image_resolution": "320x240", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3911.719, "latencies_ms": [3911.719], "images_per_second": 0.256, "prompt_tokens": 27, "response_tokens_est": 66, "n_tiles": 12, "output_text": "The main object in the foreground is a tennis player wearing a yellow shirt and black shorts. The player is standing on a blue tennis court with a white boundary line. In the background, there is a scoreboard displaying various advertisements and a person in an orange shirt. The scoreboard is located near the edge of the court.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24841.7, "ram_available_mb": 100930.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 24842.7, "ram_available_mb": 100929.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.16, "peak": 16.56, "min": 14.12}, "VDD_CPU_SOC_MSS": {"avg": 15.63, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 33.1, "peak": 44.91, "min": 25.6}, "VIN": {"avg": 76.84, "peak": 113.99, "min": 57.86}}, "power_watts_avg": 33.1, "energy_joules_est": 129.51, "sample_count": 31, "duration_seconds": 3.913}, "timestamp": "2026-01-17T14:31:10.182897"}
{"image_index": 54, "image_name": "000000005586.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005586.jpg", "image_width": 320, "image_height": 240, "image_resolution": "320x240", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3055.488, "latencies_ms": [3055.488], "images_per_second": 0.327, "prompt_tokens": 21, "response_tokens_est": 41, "n_tiles": 12, "output_text": "The image depicts a scene from a tennis match taking place on a blue court. The player is in the process of serving the ball, while a referee and another player are visible in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24842.7, "ram_available_mb": 100929.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 24843.4, "ram_available_mb": 100928.7, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.19, "peak": 16.56, "min": 14.12}, "VDD_CPU_SOC_MSS": {"avg": 15.71, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 35.03, "peak": 44.51, "min": 26.39}, "VIN": {"avg": 78.69, "peak": 113.8, "min": 59.39}}, "power_watts_avg": 35.03, "energy_joules_est": 107.06, "sample_count": 24, "duration_seconds": 3.056}, "timestamp": "2026-01-17T14:31:13.245298"}
{"image_index": 54, "image_name": "000000005586.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005586.jpg", "image_width": 320, "image_height": 240, "image_resolution": "320x240", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2684.441, "latencies_ms": [2684.441], "images_per_second": 0.373, "prompt_tokens": 19, "response_tokens_est": 30, "n_tiles": 12, "output_text": "The image shows a blue tennis court with white boundary lines and a blue surface. The lighting is bright, indicating an indoor setting with artificial lighting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24843.4, "ram_available_mb": 100928.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.6, "ram_used_mb": 24843.7, "ram_available_mb": 100928.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.23, "peak": 16.56, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.73, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 36.5, "peak": 45.3, "min": 27.18}, "VIN": {"avg": 80.21, "peak": 134.41, "min": 56.95}}, "power_watts_avg": 36.5, "energy_joules_est": 98.0, "sample_count": 21, "duration_seconds": 2.685}, "timestamp": "2026-01-17T14:31:15.936184"}
{"image_index": 55, "image_name": "000000005600.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005600.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1470.592, "latencies_ms": [1470.592], "images_per_second": 0.68, "prompt_tokens": 9, "response_tokens_est": 57, "n_tiles": 2, "output_text": "The image shows a plate with a bowl of sweet, round, orange fruits, likely apricots, placed on a napkin, accompanied by a small bowl of a dark, chunky stew or curry, likely containing meat or vegetables, on a table with a dark cloth.", "error": null, "sys_before": {"cpu_percent": 27.0, "ram_used_mb": 24843.9, "ram_available_mb": 100928.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24844.9, "ram_available_mb": 100927.3, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5055.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.77, "peak": 15.42, "min": 13.9}, "VDD_CPU_SOC_MSS": {"avg": 15.61, "peak": 16.15, "min": 14.57}, "VDD_GPU": {"avg": 25.18, "peak": 34.27, "min": 21.27}, "VIN": {"avg": 65.62, "peak": 78.86, "min": 58.65}}, "power_watts_avg": 25.18, "energy_joules_est": 37.04, "sample_count": 11, "duration_seconds": 1.471}, "timestamp": "2026-01-17T14:31:17.438964"}
{"image_index": 55, "image_name": "000000005600.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005600.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1407.416, "latencies_ms": [1407.416], "images_per_second": 0.711, "prompt_tokens": 23, "response_tokens_est": 54, "n_tiles": 2, "output_text": "- plate: 1\n- bowl: 2\n- plate: 1\n- bowl: 1\n- plate: 1\n- bowl: 1\n- plate: 1\n- bowl: 1\n- plate: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24844.9, "ram_available_mb": 100927.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24845.9, "ram_available_mb": 100926.3, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5064.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.1, "peak": 15.62, "min": 14.61}, "VDD_CPU_SOC_MSS": {"avg": 16.03, "peak": 16.54, "min": 15.35}, "VDD_GPU": {"avg": 24.62, "peak": 30.33, "min": 21.27}, "VIN": {"avg": 65.41, "peak": 91.31, "min": 54.06}}, "power_watts_avg": 24.62, "energy_joules_est": 34.67, "sample_count": 10, "duration_seconds": 1.408}, "timestamp": "2026-01-17T14:31:18.852688"}
{"image_index": 55, "image_name": "000000005600.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005600.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1637.667, "latencies_ms": [1637.667], "images_per_second": 0.611, "prompt_tokens": 27, "response_tokens_est": 64, "n_tiles": 2, "output_text": "The main objects in the image are a plate with a bowl of meat and a bowl of peaches. The plate is placed on a tablecloth, and the bowl of meat is positioned to the left of the plate. The bowl of peaches is in the foreground, slightly to the right of the plate.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24845.9, "ram_available_mb": 100926.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 24846.1, "ram_available_mb": 100926.0, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5067.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.13, "peak": 15.52, "min": 14.61}, "VDD_CPU_SOC_MSS": {"avg": 16.28, "peak": 16.95, "min": 15.35}, "VDD_GPU": {"avg": 23.93, "peak": 30.33, "min": 20.89}, "VIN": {"avg": 64.09, "peak": 89.35, "min": 53.74}}, "power_watts_avg": 23.93, "energy_joules_est": 39.2, "sample_count": 12, "duration_seconds": 1.638}, "timestamp": "2026-01-17T14:31:20.496058"}
{"image_index": 55, "image_name": "000000005600.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005600.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1898.305, "latencies_ms": [1898.305], "images_per_second": 0.527, "prompt_tokens": 21, "response_tokens_est": 76, "n_tiles": 2, "output_text": "The image depicts a dining setting with a focus on a plate of food. The plate contains a variety of items, including a bowl of meat, a bowl of peaches, and a bowl of what appears to be a sweet dish. The setting is likely a restaurant or a dining area, and the food items are being served on a table covered with a dark cloth.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24846.1, "ram_available_mb": 100926.0, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24846.1, "ram_available_mb": 100926.0, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5063.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.12, "peak": 15.52, "min": 14.61}, "VDD_CPU_SOC_MSS": {"avg": 16.26, "peak": 16.55, "min": 15.36}, "VDD_GPU": {"avg": 23.5, "peak": 29.94, "min": 20.87}, "VIN": {"avg": 64.55, "peak": 92.92, "min": 56.42}}, "power_watts_avg": 23.5, "energy_joules_est": 44.62, "sample_count": 14, "duration_seconds": 1.899}, "timestamp": "2026-01-17T14:31:22.400151"}
{"image_index": 55, "image_name": "000000005600.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005600.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1387.834, "latencies_ms": [1387.834], "images_per_second": 0.721, "prompt_tokens": 19, "response_tokens_est": 53, "n_tiles": 2, "output_text": "The image features a plate with a dark blue rim and a white center, containing a serving of meat and a bowl of peaches. The lighting is warm, casting a soft glow on the food, and the peaches have a glossy, ripe appearance.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24846.1, "ram_available_mb": 100926.0, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 24846.4, "ram_available_mb": 100925.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5062.6, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.99, "peak": 15.32, "min": 14.71}, "VDD_CPU_SOC_MSS": {"avg": 15.95, "peak": 16.15, "min": 15.36}, "VDD_GPU": {"avg": 24.27, "peak": 29.94, "min": 21.28}, "VIN": {"avg": 67.69, "peak": 107.92, "min": 60.22}}, "power_watts_avg": 24.27, "energy_joules_est": 33.69, "sample_count": 10, "duration_seconds": 1.388}, "timestamp": "2026-01-17T14:31:23.793845"}
{"image_index": 56, "image_name": "000000005992.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005992.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1229.661, "latencies_ms": [1229.661], "images_per_second": 0.813, "prompt_tokens": 9, "response_tokens_est": 23, "n_tiles": 6, "output_text": "A group of sheep are standing in a field, with one sheep in the foreground looking directly at the camera.", "error": null, "sys_before": {"cpu_percent": 38.8, "ram_used_mb": 24859.3, "ram_available_mb": 100912.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 24859.5, "ram_available_mb": 100912.7, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.71, "peak": 15.24, "min": 14.1}, "VDD_CPU_SOC_MSS": {"avg": 15.44, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 33.4, "peak": 39.78, "min": 27.19}, "VIN": {"avg": 70.12, "peak": 86.78, "min": 60.4}}, "power_watts_avg": 33.4, "energy_joules_est": 41.08, "sample_count": 9, "duration_seconds": 1.23}, "timestamp": "2026-01-17T14:31:25.080237"}
{"image_index": 56, "image_name": "000000005992.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005992.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1733.496, "latencies_ms": [1733.496], "images_per_second": 0.577, "prompt_tokens": 23, "response_tokens_est": 40, "n_tiles": 6, "output_text": "1. Sheep\n2. Sheep\n3. Sheep\n4. Sheep\n5. Sheep\n6. Sheep\n7. Sheep\n8. Sheep", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24859.5, "ram_available_mb": 100912.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 24861.0, "ram_available_mb": 100911.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.94, "peak": 15.44, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.6, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 32.64, "peak": 40.97, "min": 24.83}, "VIN": {"avg": 72.64, "peak": 105.01, "min": 59.69}}, "power_watts_avg": 32.64, "energy_joules_est": 56.59, "sample_count": 13, "duration_seconds": 1.734}, "timestamp": "2026-01-17T14:31:26.821856"}
{"image_index": 56, "image_name": "000000005992.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005992.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2167.083, "latencies_ms": [2167.083], "images_per_second": 0.461, "prompt_tokens": 27, "response_tokens_est": 54, "n_tiles": 6, "output_text": "The main objects in the image are sheep, with the closest sheep in the foreground and the others in the background. The sheep in the foreground are standing on a grassy area, while the sheep in the background are further away, possibly in a pen or enclosure.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24861.0, "ram_available_mb": 100911.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 24861.2, "ram_available_mb": 100910.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.82, "peak": 15.24, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.38, "peak": 15.75, "min": 14.57}, "VDD_GPU": {"avg": 30.2, "peak": 40.97, "min": 23.25}, "VIN": {"avg": 69.43, "peak": 112.94, "min": 50.84}}, "power_watts_avg": 30.2, "energy_joules_est": 65.46, "sample_count": 17, "duration_seconds": 2.168}, "timestamp": "2026-01-17T14:31:28.996043"}
{"image_index": 56, "image_name": "000000005992.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005992.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2589.499, "latencies_ms": [2589.499], "images_per_second": 0.386, "prompt_tokens": 21, "response_tokens_est": 69, "n_tiles": 6, "output_text": "The image depicts a group of sheep in a pastoral setting, likely a farm or a rural area. The sheep are standing on a grassy field with a brick wall in the background, and there is a playground visible in the distance. The sheep appear to be calm and relaxed, with some of them looking directly at the camera.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24861.2, "ram_available_mb": 100910.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 24861.5, "ram_available_mb": 100910.7, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.84, "peak": 15.24, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.58, "peak": 16.14, "min": 14.57}, "VDD_GPU": {"avg": 28.88, "peak": 40.57, "min": 22.86}, "VIN": {"avg": 69.43, "peak": 112.71, "min": 53.54}}, "power_watts_avg": 28.88, "energy_joules_est": 74.8, "sample_count": 19, "duration_seconds": 2.59}, "timestamp": "2026-01-17T14:31:31.595655"}
{"image_index": 56, "image_name": "000000005992.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005992.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2390.848, "latencies_ms": [2390.848], "images_per_second": 0.418, "prompt_tokens": 19, "response_tokens_est": 62, "n_tiles": 6, "output_text": "The sheep in the image have a thick, woolly coat that appears to be a mix of brown and gray hues. The lighting is bright and natural, suggesting it is daytime with ample sunlight. The weather seems to be clear and sunny, as the sheep are comfortably standing in a grassy area.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24861.5, "ram_available_mb": 100910.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 24861.5, "ram_available_mb": 100910.7, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.8, "peak": 15.14, "min": 14.2}, "VDD_CPU_SOC_MSS": {"avg": 15.53, "peak": 16.14, "min": 14.57}, "VDD_GPU": {"avg": 29.53, "peak": 40.57, "min": 22.86}, "VIN": {"avg": 71.92, "peak": 112.78, "min": 60.46}}, "power_watts_avg": 29.53, "energy_joules_est": 70.62, "sample_count": 18, "duration_seconds": 2.391}, "timestamp": "2026-01-17T14:31:33.992818"}
{"image_index": 57, "image_name": "000000006012.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006012.jpg", "image_width": 640, "image_height": 531, "image_resolution": "640x531", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2528.772, "latencies_ms": [2528.772], "images_per_second": 0.395, "prompt_tokens": 9, "response_tokens_est": 27, "n_tiles": 12, "output_text": "The image features a bunch of ripe bananas and a red apple, both resting on a fabric surface with a blue floral pattern.", "error": null, "sys_before": {"cpu_percent": 41.8, "ram_used_mb": 24827.4, "ram_available_mb": 100944.8, "ram_percent": 19.7}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 24828.4, "ram_available_mb": 100943.8, "ram_percent": 19.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.09, "peak": 16.66, "min": 13.29}, "VDD_CPU_SOC_MSS": {"avg": 15.94, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 35.65, "peak": 43.33, "min": 27.19}, "VIN": {"avg": 78.52, "peak": 114.86, "min": 62.63}}, "power_watts_avg": 35.65, "energy_joules_est": 90.16, "sample_count": 20, "duration_seconds": 2.529}, "timestamp": "2026-01-17T14:31:36.621154"}
{"image_index": 57, "image_name": "000000006012.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006012.jpg", "image_width": 640, "image_height": 531, "image_resolution": "640x531", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2993.846, "latencies_ms": [2993.846], "images_per_second": 0.334, "prompt_tokens": 23, "response_tokens_est": 39, "n_tiles": 12, "output_text": "1. Banana\n2. Apple\n3. Banana\n4. Banana\n5. Banana\n6. Banana\n7. Banana\n8. Banana", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24828.4, "ram_available_mb": 100943.8, "ram_percent": 19.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 24829.9, "ram_available_mb": 100942.3, "ram_percent": 19.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.17, "peak": 16.56, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.78, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 35.57, "peak": 45.69, "min": 26.4}, "VIN": {"avg": 76.94, "peak": 139.02, "min": 57.57}}, "power_watts_avg": 35.57, "energy_joules_est": 106.51, "sample_count": 23, "duration_seconds": 2.994}, "timestamp": "2026-01-17T14:31:39.621350"}
{"image_index": 57, "image_name": "000000006012.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006012.jpg", "image_width": 640, "image_height": 531, "image_resolution": "640x531", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 4256.04, "latencies_ms": [4256.04], "images_per_second": 0.235, "prompt_tokens": 27, "response_tokens_est": 76, "n_tiles": 12, "output_text": "The main objects in the image are a bunch of bananas and a red apple. The bananas are positioned in the foreground, with one partially cut and lying on the surface. The apple is in the background, slightly to the right. The bananas are near the bottom of the image, while the apple is further back, creating a clear spatial relationship between the two.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24829.9, "ram_available_mb": 100942.3, "ram_percent": 19.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 24830.4, "ram_available_mb": 100941.8, "ram_percent": 19.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.14, "peak": 16.56, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.59, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 32.62, "peak": 45.3, "min": 25.6}, "VIN": {"avg": 76.67, "peak": 137.86, "min": 66.69}}, "power_watts_avg": 32.62, "energy_joules_est": 138.85, "sample_count": 33, "duration_seconds": 4.257}, "timestamp": "2026-01-17T14:31:43.883738"}
{"image_index": 57, "image_name": "000000006012.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006012.jpg", "image_width": 640, "image_height": 531, "image_resolution": "640x531", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 4113.363, "latencies_ms": [4113.363], "images_per_second": 0.243, "prompt_tokens": 21, "response_tokens_est": 72, "n_tiles": 12, "output_text": "The image features a bunch of ripe bananas and a red apple resting on a patterned blue fabric surface. The bananas are green with some brown spots, while the apple has a vibrant red and yellow color. The setting appears to be indoors, possibly in a kitchen or dining area, as the fabric is likely a tablecloth or placemat.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24830.4, "ram_available_mb": 100941.8, "ram_percent": 19.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 24830.1, "ram_available_mb": 100942.1, "ram_percent": 19.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.15, "peak": 16.56, "min": 14.12}, "VDD_CPU_SOC_MSS": {"avg": 15.71, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 32.77, "peak": 44.91, "min": 25.61}, "VIN": {"avg": 72.72, "peak": 110.71, "min": 59.07}}, "power_watts_avg": 32.77, "energy_joules_est": 134.81, "sample_count": 32, "duration_seconds": 4.114}, "timestamp": "2026-01-17T14:31:48.004514"}
{"image_index": 57, "image_name": "000000006012.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006012.jpg", "image_width": 640, "image_height": 531, "image_resolution": "640x531", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3195.664, "latencies_ms": [3195.664], "images_per_second": 0.313, "prompt_tokens": 19, "response_tokens_est": 45, "n_tiles": 12, "output_text": "The image features a bunch of bananas and a red apple, both resting on a fabric surface with a blue floral pattern. The lighting is soft and natural, casting gentle shadows and highlighting the vibrant colors of the fruits.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24830.1, "ram_available_mb": 100942.1, "ram_percent": 19.7}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 24831.3, "ram_available_mb": 100940.8, "ram_percent": 19.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.16, "peak": 16.56, "min": 14.12}, "VDD_CPU_SOC_MSS": {"avg": 15.78, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 34.73, "peak": 45.3, "min": 26.0}, "VIN": {"avg": 73.98, "peak": 114.78, "min": 62.39}}, "power_watts_avg": 34.73, "energy_joules_est": 111.0, "sample_count": 25, "duration_seconds": 3.196}, "timestamp": "2026-01-17T14:31:51.207068"}
{"image_index": 58, "image_name": "000000006040.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006040.jpg", "image_width": 640, "image_height": 351, "image_resolution": "640x351", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 956.433, "latencies_ms": [956.433], "images_per_second": 1.046, "prompt_tokens": 9, "response_tokens_est": 34, "n_tiles": 2, "output_text": "A modern tram, painted in blue and white, is moving along a track in a city, with a clear blue sky above and a few trees in the background.", "error": null, "sys_before": {"cpu_percent": 31.6, "ram_used_mb": 24831.1, "ram_available_mb": 100941.1, "ram_percent": 19.7}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24832.3, "ram_available_mb": 100939.9, "ram_percent": 19.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5055.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.62, "peak": 15.22, "min": 13.9}, "VDD_CPU_SOC_MSS": {"avg": 15.24, "peak": 15.74, "min": 14.57}, "VDD_GPU": {"avg": 26.56, "peak": 33.09, "min": 22.46}, "VIN": {"avg": 65.1, "peak": 79.04, "min": 60.14}}, "power_watts_avg": 26.56, "energy_joules_est": 25.41, "sample_count": 7, "duration_seconds": 0.957}, "timestamp": "2026-01-17T14:31:52.194641"}
{"image_index": 58, "image_name": "000000006040.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006040.jpg", "image_width": 640, "image_height": 351, "image_resolution": "640x351", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1092.265, "latencies_ms": [1092.265], "images_per_second": 0.916, "prompt_tokens": 23, "response_tokens_est": 40, "n_tiles": 2, "output_text": "object: 1\nobject: 2\nobject: 3\nobject: 4\nobject: 5\nobject: 6\nobject: 7\nobject: 8", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24832.3, "ram_available_mb": 100939.9, "ram_percent": 19.7}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 24832.5, "ram_available_mb": 100939.6, "ram_percent": 19.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5064.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.87, "peak": 15.22, "min": 14.61}, "VDD_CPU_SOC_MSS": {"avg": 15.69, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 25.57, "peak": 30.73, "min": 22.08}, "VIN": {"avg": 69.68, "peak": 99.2, "min": 62.44}}, "power_watts_avg": 25.57, "energy_joules_est": 27.97, "sample_count": 8, "duration_seconds": 1.094}, "timestamp": "2026-01-17T14:31:53.294329"}
{"image_index": 58, "image_name": "000000006040.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006040.jpg", "image_width": 640, "image_height": 351, "image_resolution": "640x351", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2491.321, "latencies_ms": [2491.321], "images_per_second": 0.401, "prompt_tokens": 27, "response_tokens_est": 102, "n_tiles": 2, "output_text": "The main object in the image is a blue and white tram, which is positioned in the foreground on the right side of the frame. The tram is moving along the tracks, indicating its role as a mode of public transportation. In the background, there is a parked vehicle, possibly a van, and some trees, suggesting the tram is in a suburban or urban setting. The clear blue sky and the absence of other significant objects in the immediate vicinity further emphasize the tram as the central focus of the image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24832.5, "ram_available_mb": 100939.6, "ram_percent": 19.7}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24832.8, "ram_available_mb": 100939.4, "ram_percent": 19.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5067.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.31, "peak": 15.82, "min": 14.61}, "VDD_CPU_SOC_MSS": {"avg": 16.04, "peak": 16.54, "min": 15.34}, "VDD_GPU": {"avg": 22.92, "peak": 31.13, "min": 20.88}, "VIN": {"avg": 64.46, "peak": 86.47, "min": 59.55}}, "power_watts_avg": 22.92, "energy_joules_est": 57.11, "sample_count": 19, "duration_seconds": 2.492}, "timestamp": "2026-01-17T14:31:55.791525"}
{"image_index": 58, "image_name": "000000006040.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006040.jpg", "image_width": 640, "image_height": 351, "image_resolution": "640x351", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1885.014, "latencies_ms": [1885.014], "images_per_second": 0.531, "prompt_tokens": 21, "response_tokens_est": 75, "n_tiles": 2, "output_text": "The image depicts a modern tram in motion on a city street, with clear blue skies overhead. The tram is painted in a combination of white and blue, and it has the number \"2\" displayed on its front. The tram is moving along a set of tracks, and there are other vehicles and trees visible in the background, suggesting a bustling urban environment.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24832.8, "ram_available_mb": 100939.4, "ram_percent": 19.7}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 24834.0, "ram_available_mb": 100938.2, "ram_percent": 19.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5063.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.03, "peak": 15.22, "min": 14.51}, "VDD_CPU_SOC_MSS": {"avg": 16.03, "peak": 16.54, "min": 15.36}, "VDD_GPU": {"avg": 23.31, "peak": 29.95, "min": 20.89}, "VIN": {"avg": 64.95, "peak": 99.13, "min": 56.26}}, "power_watts_avg": 23.31, "energy_joules_est": 43.95, "sample_count": 14, "duration_seconds": 1.885}, "timestamp": "2026-01-17T14:31:57.682252"}
{"image_index": 58, "image_name": "000000006040.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006040.jpg", "image_width": 640, "image_height": 351, "image_resolution": "640x351", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1329.703, "latencies_ms": [1329.703], "images_per_second": 0.752, "prompt_tokens": 19, "response_tokens_est": 51, "n_tiles": 2, "output_text": "The notable visual attributes of the tram include its predominantly blue and white color scheme, sleek and modern design, and the use of glass for the windows. The lighting is bright and natural, indicating daytime, and the weather appears to be clear and sunny.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24834.0, "ram_available_mb": 100938.2, "ram_percent": 19.7}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 24834.0, "ram_available_mb": 100938.2, "ram_percent": 19.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5062.6, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.18, "peak": 15.42, "min": 14.61}, "VDD_CPU_SOC_MSS": {"avg": 15.91, "peak": 16.54, "min": 15.35}, "VDD_GPU": {"avg": 24.55, "peak": 29.94, "min": 21.27}, "VIN": {"avg": 67.14, "peak": 101.29, "min": 58.88}}, "power_watts_avg": 24.55, "energy_joules_est": 32.65, "sample_count": 10, "duration_seconds": 1.33}, "timestamp": "2026-01-17T14:31:59.017661"}
{"image_index": 59, "image_name": "000000006213.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006213.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1549.064, "latencies_ms": [1549.064], "images_per_second": 0.646, "prompt_tokens": 9, "response_tokens_est": 34, "n_tiles": 6, "output_text": "The image shows a well-lit bathroom with a wooden vanity, a white bathtub, a red patterned curtain, and a large mirror with gold fixtures.", "error": null, "sys_before": {"cpu_percent": 44.2, "ram_used_mb": 24851.4, "ram_available_mb": 100920.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 24853.1, "ram_available_mb": 100919.0, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.76, "peak": 15.24, "min": 14.2}, "VDD_CPU_SOC_MSS": {"avg": 15.68, "peak": 16.53, "min": 14.96}, "VDD_GPU": {"avg": 31.58, "peak": 39.39, "min": 24.43}, "VIN": {"avg": 73.49, "peak": 117.94, "min": 60.97}}, "power_watts_avg": 31.58, "energy_joules_est": 48.93, "sample_count": 12, "duration_seconds": 1.549}, "timestamp": "2026-01-17T14:32:00.609184"}
{"image_index": 59, "image_name": "000000006213.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006213.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1775.168, "latencies_ms": [1775.168], "images_per_second": 0.563, "prompt_tokens": 23, "response_tokens_est": 40, "n_tiles": 6, "output_text": "object: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24853.1, "ram_available_mb": 100919.0, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 24853.9, "ram_available_mb": 100918.3, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.76, "peak": 15.24, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.32, "peak": 16.14, "min": 14.57}, "VDD_GPU": {"avg": 31.82, "peak": 40.57, "min": 24.04}, "VIN": {"avg": 71.07, "peak": 103.55, "min": 55.35}}, "power_watts_avg": 31.82, "energy_joules_est": 56.5, "sample_count": 13, "duration_seconds": 1.776}, "timestamp": "2026-01-17T14:32:02.390175"}
{"image_index": 59, "image_name": "000000006213.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006213.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3484.21, "latencies_ms": [3484.21], "images_per_second": 0.287, "prompt_tokens": 27, "response_tokens_est": 100, "n_tiles": 6, "output_text": "The main objects in the image are a wooden vanity with a sink, a white bathtub, and a red patterned curtain. The vanity is positioned in the foreground, with the sink and faucet visible. The bathtub is located to the left of the vanity, and the red curtain is partially covering the bathtub. The mirror above the vanity is positioned to the right, reflecting the bathroom's interior. The overall layout suggests a well-organized and functional bathroom space.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24853.9, "ram_available_mb": 100918.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 24854.9, "ram_available_mb": 100917.3, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.8, "peak": 15.14, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.54, "peak": 15.75, "min": 14.56}, "VDD_GPU": {"avg": 27.34, "peak": 40.56, "min": 22.85}, "VIN": {"avg": 66.61, "peak": 87.51, "min": 59.02}}, "power_watts_avg": 27.34, "energy_joules_est": 95.27, "sample_count": 27, "duration_seconds": 3.485}, "timestamp": "2026-01-17T14:32:05.880634"}
{"image_index": 59, "image_name": "000000006213.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006213.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3190.666, "latencies_ms": [3190.666], "images_per_second": 0.313, "prompt_tokens": 21, "response_tokens_est": 90, "n_tiles": 6, "output_text": "The image depicts a well-lit bathroom with a modern and elegant design. The room features a wooden vanity with a white countertop, two sinks with gold faucets, and a red patterned shower curtain. The walls are painted in a light beige color, and there is a white bathtub with a towel draped over it. The bathroom appears to be clean and tidy, with a focus on a classic and luxurious aesthetic.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24854.9, "ram_available_mb": 100917.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 24855.6, "ram_available_mb": 100916.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.82, "peak": 15.14, "min": 14.2}, "VDD_CPU_SOC_MSS": {"avg": 15.47, "peak": 15.75, "min": 14.56}, "VDD_GPU": {"avg": 27.58, "peak": 40.18, "min": 22.86}, "VIN": {"avg": 69.48, "peak": 101.25, "min": 62.64}}, "power_watts_avg": 27.58, "energy_joules_est": 88.01, "sample_count": 25, "duration_seconds": 3.191}, "timestamp": "2026-01-17T14:32:09.077409"}
{"image_index": 59, "image_name": "000000006213.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006213.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2192.694, "latencies_ms": [2192.694], "images_per_second": 0.456, "prompt_tokens": 19, "response_tokens_est": 55, "n_tiles": 6, "output_text": "The bathroom features a warm, neutral color palette with beige walls and a white bathtub. The lighting is soft and ambient, with recessed ceiling lights providing a gentle glow. The wooden vanity and red patterned curtain add a touch of contrast and visual interest.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24855.6, "ram_available_mb": 100916.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 24856.6, "ram_available_mb": 100915.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.79, "peak": 15.14, "min": 14.2}, "VDD_CPU_SOC_MSS": {"avg": 15.45, "peak": 15.75, "min": 14.57}, "VDD_GPU": {"avg": 29.71, "peak": 39.78, "min": 23.25}, "VIN": {"avg": 67.89, "peak": 86.93, "min": 62.19}}, "power_watts_avg": 29.71, "energy_joules_est": 65.15, "sample_count": 17, "duration_seconds": 2.193}, "timestamp": "2026-01-17T14:32:11.276012"}
{"image_index": 60, "image_name": "000000006460.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006460.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1007.894, "latencies_ms": [1007.894], "images_per_second": 0.992, "prompt_tokens": 9, "response_tokens_est": 15, "n_tiles": 6, "output_text": "A surfer is performing a trick on a wave in the ocean.", "error": null, "sys_before": {"cpu_percent": 47.8, "ram_used_mb": 24856.6, "ram_available_mb": 100915.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 24857.1, "ram_available_mb": 100915.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.51, "peak": 15.14, "min": 13.79}, "VDD_CPU_SOC_MSS": {"avg": 15.24, "peak": 16.14, "min": 14.57}, "VDD_GPU": {"avg": 35.62, "peak": 40.18, "min": 31.12}, "VIN": {"avg": 89.46, "peak": 115.53, "min": 64.1}}, "power_watts_avg": 35.62, "energy_joules_est": 35.91, "sample_count": 7, "duration_seconds": 1.008}, "timestamp": "2026-01-17T14:32:12.334683"}
{"image_index": 60, "image_name": "000000006460.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006460.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1730.605, "latencies_ms": [1730.605], "images_per_second": 0.578, "prompt_tokens": 23, "response_tokens_est": 40, "n_tiles": 6, "output_text": "object: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24857.1, "ram_available_mb": 100915.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 24857.1, "ram_available_mb": 100915.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.93, "peak": 15.34, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.54, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 33.64, "peak": 42.54, "min": 24.83}, "VIN": {"avg": 70.27, "peak": 88.91, "min": 59.19}}, "power_watts_avg": 33.64, "energy_joules_est": 58.23, "sample_count": 13, "duration_seconds": 1.731}, "timestamp": "2026-01-17T14:32:14.071671"}
{"image_index": 60, "image_name": "000000006460.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006460.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2628.929, "latencies_ms": [2628.929], "images_per_second": 0.38, "prompt_tokens": 27, "response_tokens_est": 73, "n_tiles": 6, "output_text": "The main object in the foreground is a surfer performing a trick on a wave. The surfer is positioned near the center of the image, slightly to the left. The background features the ocean, with waves crashing towards the right side of the image. The surfer is in the middle of the wave, which is the most prominent feature in the scene.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 24857.1, "ram_available_mb": 100915.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 24858.0, "ram_available_mb": 100914.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.04, "peak": 15.44, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.63, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 29.25, "peak": 40.18, "min": 23.25}, "VIN": {"avg": 70.29, "peak": 108.68, "min": 60.46}}, "power_watts_avg": 29.25, "energy_joules_est": 76.91, "sample_count": 20, "duration_seconds": 2.629}, "timestamp": "2026-01-17T14:32:16.706545"}
{"image_index": 60, "image_name": "000000006460.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006460.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2488.151, "latencies_ms": [2488.151], "images_per_second": 0.402, "prompt_tokens": 21, "response_tokens_est": 68, "n_tiles": 6, "output_text": "The image captures a dynamic scene of a surfer riding a large wave in the ocean. The surfer is in mid-air, performing a trick, with the wave's crest reaching up to their waist. The ocean is rough, with white foam and splashes, indicating the surfer's skill and the power of the wave.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24858.0, "ram_available_mb": 100914.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 24858.0, "ram_available_mb": 100914.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.04, "peak": 15.44, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.6, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 29.3, "peak": 39.78, "min": 23.64}, "VIN": {"avg": 69.32, "peak": 89.05, "min": 53.95}}, "power_watts_avg": 29.3, "energy_joules_est": 72.91, "sample_count": 19, "duration_seconds": 2.488}, "timestamp": "2026-01-17T14:32:19.201431"}
{"image_index": 60, "image_name": "000000006460.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006460.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3002.56, "latencies_ms": [3002.56], "images_per_second": 0.333, "prompt_tokens": 19, "response_tokens_est": 87, "n_tiles": 6, "output_text": "The image is a black and white photograph capturing a dynamic scene of a surfer riding a large wave. The surfer is in mid-air, with the wave's crest reaching up to their waist, and the water droplets are frozen in motion, creating a sense of movement and energy. The lighting is natural, with the sun illuminating the scene, casting shadows and highlighting the textures of the water and surfboard.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24858.0, "ram_available_mb": 100914.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 24858.0, "ram_available_mb": 100914.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.07, "peak": 15.44, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.66, "peak": 16.14, "min": 14.57}, "VDD_GPU": {"avg": 28.35, "peak": 39.78, "min": 23.25}, "VIN": {"avg": 69.11, "peak": 113.82, "min": 57.18}}, "power_watts_avg": 28.35, "energy_joules_est": 85.14, "sample_count": 23, "duration_seconds": 3.003}, "timestamp": "2026-01-17T14:32:22.210130"}
{"image_index": 61, "image_name": "000000006471.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006471.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1604.386, "latencies_ms": [1604.386], "images_per_second": 0.623, "prompt_tokens": 9, "response_tokens_est": 37, "n_tiles": 6, "output_text": "A baseball player in a white uniform with the number 10 is preparing to swing at a pitch, while a catcher and umpire are positioned behind him on the field.", "error": null, "sys_before": {"cpu_percent": 42.6, "ram_used_mb": 24858.0, "ram_available_mb": 100914.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 24859.3, "ram_available_mb": 100912.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.76, "peak": 15.14, "min": 14.0}, "VDD_CPU_SOC_MSS": {"avg": 15.42, "peak": 16.14, "min": 14.57}, "VDD_GPU": {"avg": 31.91, "peak": 39.78, "min": 24.83}, "VIN": {"avg": 72.63, "peak": 100.51, "min": 46.11}}, "power_watts_avg": 31.91, "energy_joules_est": 51.21, "sample_count": 12, "duration_seconds": 1.605}, "timestamp": "2026-01-17T14:32:23.865810"}
{"image_index": 61, "image_name": "000000006471.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006471.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1823.529, "latencies_ms": [1823.529], "images_per_second": 0.548, "prompt_tokens": 23, "response_tokens_est": 42, "n_tiles": 6, "output_text": "baseball player: 1\ncatcher: 1\numpire: 1\nhome plate: 1\npitcher: 1\nbatter: 1\nball: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24859.3, "ram_available_mb": 100912.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 24859.2, "ram_available_mb": 100912.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.82, "peak": 15.34, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.32, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 32.0, "peak": 40.95, "min": 24.04}, "VIN": {"avg": 72.71, "peak": 105.55, "min": 56.15}}, "power_watts_avg": 32.0, "energy_joules_est": 58.37, "sample_count": 13, "duration_seconds": 1.824}, "timestamp": "2026-01-17T14:32:25.695415"}
{"image_index": 61, "image_name": "000000006471.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006471.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3423.727, "latencies_ms": [3423.727], "images_per_second": 0.292, "prompt_tokens": 27, "response_tokens_est": 98, "n_tiles": 6, "output_text": "The main objects in the image are a baseball player, a catcher, and a umpire. The baseball player is positioned in the foreground, wearing a white uniform with the number 10. The catcher is crouched near the base, wearing a black uniform with red and white accents. The umpire is standing to the left of the catcher, wearing a blue shirt and gray pants. The background features a baseball field with a dugout and spectators.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24859.2, "ram_available_mb": 100912.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 24859.2, "ram_available_mb": 100912.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.85, "peak": 15.14, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.34, "peak": 15.75, "min": 14.56}, "VDD_GPU": {"avg": 27.53, "peak": 40.97, "min": 22.85}, "VIN": {"avg": 68.43, "peak": 108.94, "min": 53.48}}, "power_watts_avg": 27.53, "energy_joules_est": 94.26, "sample_count": 26, "duration_seconds": 3.424}, "timestamp": "2026-01-17T14:32:29.129069"}
{"image_index": 61, "image_name": "000000006471.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006471.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2255.527, "latencies_ms": [2255.527], "images_per_second": 0.443, "prompt_tokens": 21, "response_tokens_est": 57, "n_tiles": 6, "output_text": "The image captures a moment during a baseball game, with a batter in a white uniform preparing to swing at a pitch. The scene takes place on a well-maintained baseball field, with a catcher and umpire positioned behind home plate, ready to catch the ball.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24859.2, "ram_available_mb": 100912.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 24859.2, "ram_available_mb": 100912.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.76, "peak": 15.14, "min": 14.1}, "VDD_CPU_SOC_MSS": {"avg": 15.26, "peak": 15.74, "min": 14.57}, "VDD_GPU": {"avg": 29.78, "peak": 40.57, "min": 22.86}, "VIN": {"avg": 68.18, "peak": 101.15, "min": 58.24}}, "power_watts_avg": 29.78, "energy_joules_est": 67.18, "sample_count": 17, "duration_seconds": 2.256}, "timestamp": "2026-01-17T14:32:31.390612"}
{"image_index": 61, "image_name": "000000006471.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006471.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2306.026, "latencies_ms": [2306.026], "images_per_second": 0.434, "prompt_tokens": 19, "response_tokens_est": 59, "n_tiles": 6, "output_text": "The baseball player is wearing a white uniform with black and orange accents, and he is holding a bat. The catcher is wearing a black uniform with red and white accents, and he is crouched behind home plate. The lighting is bright, and the weather appears to be clear.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24859.2, "ram_available_mb": 100912.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 24860.0, "ram_available_mb": 100912.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.79, "peak": 15.14, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.38, "peak": 15.75, "min": 14.57}, "VDD_GPU": {"avg": 29.41, "peak": 40.18, "min": 22.86}, "VIN": {"avg": 67.78, "peak": 88.36, "min": 58.52}}, "power_watts_avg": 29.41, "energy_joules_est": 67.83, "sample_count": 18, "duration_seconds": 2.306}, "timestamp": "2026-01-17T14:32:33.702775"}
{"image_index": 62, "image_name": "000000006614.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006614.jpg", "image_width": 500, "image_height": 396, "image_resolution": "500x396", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2767.857, "latencies_ms": [2767.857], "images_per_second": 0.361, "prompt_tokens": 9, "response_tokens_est": 34, "n_tiles": 12, "output_text": "The image depicts a collection of various fruits, including apples, pears, and possibly peaches, arranged in a somewhat haphazard manner on a surface.", "error": null, "sys_before": {"cpu_percent": 47.1, "ram_used_mb": 24819.4, "ram_available_mb": 100952.8, "ram_percent": 19.7}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 24819.9, "ram_available_mb": 100952.3, "ram_percent": 19.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.16, "peak": 16.76, "min": 13.49}, "VDD_CPU_SOC_MSS": {"avg": 15.93, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 35.34, "peak": 43.73, "min": 26.79}, "VIN": {"avg": 79.13, "peak": 130.62, "min": 65.71}}, "power_watts_avg": 35.34, "energy_joules_est": 97.83, "sample_count": 21, "duration_seconds": 2.768}, "timestamp": "2026-01-17T14:32:36.549523"}
{"image_index": 62, "image_name": "000000006614.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006614.jpg", "image_width": 500, "image_height": 396, "image_resolution": "500x396", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 6034.395, "latencies_ms": [6034.395], "images_per_second": 0.166, "prompt_tokens": 23, "response_tokens_est": 128, "n_tiles": 12, "output_text": "apple: 1\npear: 1\npeach: 1\nkiwi: 1\nkiwi: 1\nkiwi: 1\nkiwi: 1\nkiwi: 1\nkiwi: 1\nkiwi: 1\nkiwi: 1\nkiwi: 1\nkiwi: 1\nkiwi: 1\nkiwi: 1\nkiwi: 1\nkiwi: 1\nkiwi: 1\nkiwi: 1\nkiwi: 1\nkiwi: 1\nkiwi:", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24819.9, "ram_available_mb": 100952.3, "ram_percent": 19.7}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 24820.1, "ram_available_mb": 100952.1, "ram_percent": 19.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.11, "peak": 16.56, "min": 14.12}, "VDD_CPU_SOC_MSS": {"avg": 15.57, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 30.47, "peak": 45.3, "min": 25.6}, "VIN": {"avg": 72.27, "peak": 116.98, "min": 53.5}}, "power_watts_avg": 30.47, "energy_joules_est": 183.88, "sample_count": 48, "duration_seconds": 6.035}, "timestamp": "2026-01-17T14:32:42.590364"}
{"image_index": 62, "image_name": "000000006614.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006614.jpg", "image_width": 500, "image_height": 396, "image_resolution": "500x396", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3099.307, "latencies_ms": [3099.307], "images_per_second": 0.323, "prompt_tokens": 27, "response_tokens_est": 42, "n_tiles": 12, "output_text": "The main objects in the image are a cluster of peanuts and a single apple. The peanuts are scattered in the foreground, while the apple is positioned slightly to the right in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24820.1, "ram_available_mb": 100952.1, "ram_percent": 19.7}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 24820.6, "ram_available_mb": 100951.6, "ram_percent": 19.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.19, "peak": 16.56, "min": 14.12}, "VDD_CPU_SOC_MSS": {"avg": 15.71, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 35.01, "peak": 44.51, "min": 26.0}, "VIN": {"avg": 75.83, "peak": 103.97, "min": 55.36}}, "power_watts_avg": 35.01, "energy_joules_est": 108.52, "sample_count": 24, "duration_seconds": 3.1}, "timestamp": "2026-01-17T14:32:45.695599"}
{"image_index": 62, "image_name": "000000006614.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006614.jpg", "image_width": 500, "image_height": 396, "image_resolution": "500x396", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3360.16, "latencies_ms": [3360.16], "images_per_second": 0.298, "prompt_tokens": 21, "response_tokens_est": 50, "n_tiles": 12, "output_text": "The image depicts a collection of peanuts scattered on a surface, with a few peanuts partially buried among them. The setting appears to be a plain, light-colored background, providing a neutral backdrop that highlights the peanuts.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24820.6, "ram_available_mb": 100951.6, "ram_percent": 19.7}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 24821.1, "ram_available_mb": 100951.1, "ram_percent": 19.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.19, "peak": 16.56, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.69, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 34.42, "peak": 44.91, "min": 26.0}, "VIN": {"avg": 81.48, "peak": 131.72, "min": 56.13}}, "power_watts_avg": 34.42, "energy_joules_est": 115.67, "sample_count": 26, "duration_seconds": 3.361}, "timestamp": "2026-01-17T14:32:49.062237"}
{"image_index": 62, "image_name": "000000006614.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006614.jpg", "image_width": 500, "image_height": 396, "image_resolution": "500x396", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3602.231, "latencies_ms": [3602.231], "images_per_second": 0.278, "prompt_tokens": 19, "response_tokens_est": 57, "n_tiles": 12, "output_text": "The image depicts a black and white photograph of a cluster of apples and peaches. The apples and peaches are depicted with a high level of detail, showcasing their textures and shapes. The lighting is soft and even, highlighting the natural colors and textures of the fruits.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24821.1, "ram_available_mb": 100951.1, "ram_percent": 19.7}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 24821.8, "ram_available_mb": 100950.3, "ram_percent": 19.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.15, "peak": 16.56, "min": 14.12}, "VDD_CPU_SOC_MSS": {"avg": 15.63, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 33.81, "peak": 45.3, "min": 25.61}, "VIN": {"avg": 74.82, "peak": 115.6, "min": 62.07}}, "power_watts_avg": 33.81, "energy_joules_est": 121.82, "sample_count": 28, "duration_seconds": 3.603}, "timestamp": "2026-01-17T14:32:52.671273"}
{"image_index": 63, "image_name": "000000006723.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006723.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1195.984, "latencies_ms": [1195.984], "images_per_second": 0.836, "prompt_tokens": 9, "response_tokens_est": 45, "n_tiles": 2, "output_text": "The image depicts a modern urban street scene with a curved road, a sidewalk, and a row of multi-story buildings on one side, while the other side features a green area with some trees and a bus stop.", "error": null, "sys_before": {"cpu_percent": 34.8, "ram_used_mb": 24830.2, "ram_available_mb": 100942.0, "ram_percent": 19.7}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24831.2, "ram_available_mb": 100941.0, "ram_percent": 19.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5055.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.58, "peak": 15.11, "min": 13.9}, "VDD_CPU_SOC_MSS": {"avg": 15.49, "peak": 16.54, "min": 14.57}, "VDD_GPU": {"avg": 25.7, "peak": 33.09, "min": 21.67}, "VIN": {"avg": 67.11, "peak": 98.03, "min": 59.1}}, "power_watts_avg": 25.7, "energy_joules_est": 30.75, "sample_count": 9, "duration_seconds": 1.196}, "timestamp": "2026-01-17T14:32:53.903403"}
{"image_index": 63, "image_name": "000000006723.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006723.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 871.166, "latencies_ms": [871.166], "images_per_second": 1.148, "prompt_tokens": 23, "response_tokens_est": 30, "n_tiles": 2, "output_text": "- Street\n- Car\n- Bus\n- Pedestrian\n- Building\n- Street light\n- Greenery\n- Sidewalk", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24831.2, "ram_available_mb": 100941.0, "ram_percent": 19.7}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 24832.4, "ram_available_mb": 100939.8, "ram_percent": 19.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5064.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.73, "peak": 15.11, "min": 14.3}, "VDD_CPU_SOC_MSS": {"avg": 15.69, "peak": 16.15, "min": 15.36}, "VDD_GPU": {"avg": 26.53, "peak": 30.74, "min": 23.25}, "VIN": {"avg": 71.03, "peak": 107.21, "min": 60.15}}, "power_watts_avg": 26.53, "energy_joules_est": 23.12, "sample_count": 6, "duration_seconds": 0.872}, "timestamp": "2026-01-17T14:32:54.780374"}
{"image_index": 63, "image_name": "000000006723.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006723.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1820.725, "latencies_ms": [1820.725], "images_per_second": 0.549, "prompt_tokens": 27, "response_tokens_est": 72, "n_tiles": 2, "output_text": "The main objects in the image are a road, a sidewalk, and a building. The road is on the left side of the image, with a sidewalk running parallel to it on the right. The building is located on the right side of the image, near the sidewalk. The road curves gently to the right, and the sidewalk curves to the left.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24832.4, "ram_available_mb": 100939.8, "ram_percent": 19.7}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24832.4, "ram_available_mb": 100939.8, "ram_percent": 19.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5067.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.98, "peak": 15.42, "min": 14.4}, "VDD_CPU_SOC_MSS": {"avg": 16.03, "peak": 16.54, "min": 15.36}, "VDD_GPU": {"avg": 23.76, "peak": 31.12, "min": 20.89}, "VIN": {"avg": 64.73, "peak": 103.52, "min": 55.27}}, "power_watts_avg": 23.76, "energy_joules_est": 43.31, "sample_count": 14, "duration_seconds": 1.823}, "timestamp": "2026-01-17T14:32:56.608874"}
{"image_index": 63, "image_name": "000000006723.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006723.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1773.06, "latencies_ms": [1773.06], "images_per_second": 0.564, "prompt_tokens": 21, "response_tokens_est": 70, "n_tiles": 2, "output_text": "The image depicts a modern urban street scene with a mix of residential and commercial buildings lining the street. The road is divided by a central median strip, and there are several parked cars and a bus visible. The sky is partly cloudy, and the overall atmosphere suggests a quiet, suburban area with a focus on pedestrian and vehicular traffic.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24832.4, "ram_available_mb": 100939.8, "ram_percent": 19.7}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 24832.7, "ram_available_mb": 100939.5, "ram_percent": 19.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5063.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.94, "peak": 15.11, "min": 14.51}, "VDD_CPU_SOC_MSS": {"avg": 15.99, "peak": 16.54, "min": 15.35}, "VDD_GPU": {"avg": 23.53, "peak": 30.34, "min": 20.89}, "VIN": {"avg": 67.28, "peak": 111.2, "min": 61.21}}, "power_watts_avg": 23.53, "energy_joules_est": 41.73, "sample_count": 13, "duration_seconds": 1.774}, "timestamp": "2026-01-17T14:32:58.392149"}
{"image_index": 63, "image_name": "000000006723.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006723.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1323.29, "latencies_ms": [1323.29], "images_per_second": 0.756, "prompt_tokens": 19, "response_tokens_est": 50, "n_tiles": 2, "output_text": "The image depicts a city street scene with a mix of modern and older buildings. The street is paved with asphalt, and there are white lines marking the lanes. The sky is partly cloudy, providing a mix of sunlight and shade.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24832.7, "ram_available_mb": 100939.5, "ram_percent": 19.7}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 24832.4, "ram_available_mb": 100939.8, "ram_percent": 19.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5062.6, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.98, "peak": 15.32, "min": 14.4}, "VDD_CPU_SOC_MSS": {"avg": 15.87, "peak": 16.54, "min": 15.36}, "VDD_GPU": {"avg": 24.31, "peak": 30.71, "min": 21.28}, "VIN": {"avg": 65.68, "peak": 88.32, "min": 59.98}}, "power_watts_avg": 24.31, "energy_joules_est": 32.18, "sample_count": 10, "duration_seconds": 1.324}, "timestamp": "2026-01-17T14:32:59.721290"}
{"image_index": 64, "image_name": "000000006763.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006763.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2830.214, "latencies_ms": [2830.214], "images_per_second": 0.353, "prompt_tokens": 9, "response_tokens_est": 36, "n_tiles": 12, "output_text": "The image depicts a group of three individuals seated at a table in what appears to be a restaurant, with one person holding a smartphone and the others smiling at the camera.", "error": null, "sys_before": {"cpu_percent": 44.3, "ram_used_mb": 24817.4, "ram_available_mb": 100954.8, "ram_percent": 19.7}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 24818.9, "ram_available_mb": 100953.3, "ram_percent": 19.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.94, "peak": 16.36, "min": 13.9}, "VDD_CPU_SOC_MSS": {"avg": 15.57, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 34.65, "peak": 43.33, "min": 26.0}, "VIN": {"avg": 79.78, "peak": 134.24, "min": 65.2}}, "power_watts_avg": 34.65, "energy_joules_est": 98.08, "sample_count": 22, "duration_seconds": 2.831}, "timestamp": "2026-01-17T14:33:02.625067"}
{"image_index": 64, "image_name": "000000006763.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006763.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 6095.195, "latencies_ms": [6095.195], "images_per_second": 0.164, "prompt_tokens": 23, "response_tokens_est": 129, "n_tiles": 12, "output_text": "- television: 1\n- television set: 1\n- man: 1\n- woman: 1\n- camera: 1\n- camera: 1\n- table: 1\n- table: 1\n- wall: 1\n- wall: 1\n- wall: 1\n- wall: 1\n- wall: 1\n- wall: 1\n- wall: 1\n- wall: 1\n- wall: 1\n- wall: 1\n- wall: 1\n- wall: 1\n- wall: 1\n-", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24818.9, "ram_available_mb": 100953.3, "ram_percent": 19.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 24818.7, "ram_available_mb": 100953.5, "ram_percent": 19.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.78, "peak": 16.26, "min": 13.92}, "VDD_CPU_SOC_MSS": {"avg": 15.18, "peak": 17.32, "min": 14.17}, "VDD_GPU": {"avg": 30.42, "peak": 45.69, "min": 25.21}, "VIN": {"avg": 71.18, "peak": 109.02, "min": 58.67}}, "power_watts_avg": 30.42, "energy_joules_est": 185.43, "sample_count": 47, "duration_seconds": 6.096}, "timestamp": "2026-01-17T14:33:08.726549"}
{"image_index": 64, "image_name": "000000006763.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006763.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3345.328, "latencies_ms": [3345.328], "images_per_second": 0.299, "prompt_tokens": 27, "response_tokens_est": 49, "n_tiles": 12, "output_text": "The main objects in the image are a man and a woman. The man is in the foreground, while the woman is slightly behind him. The man is holding a phone in his right hand, and the woman is standing close to him.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24818.7, "ram_available_mb": 100953.5, "ram_percent": 19.7}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 24818.7, "ram_available_mb": 100953.5, "ram_percent": 19.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.88, "peak": 16.26, "min": 13.82}, "VDD_CPU_SOC_MSS": {"avg": 15.38, "peak": 17.32, "min": 14.17}, "VDD_GPU": {"avg": 34.43, "peak": 44.91, "min": 25.61}, "VIN": {"avg": 75.21, "peak": 116.19, "min": 53.86}}, "power_watts_avg": 34.43, "energy_joules_est": 115.19, "sample_count": 26, "duration_seconds": 3.346}, "timestamp": "2026-01-17T14:33:12.078491"}
{"image_index": 64, "image_name": "000000006763.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006763.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 4105.005, "latencies_ms": [4105.005], "images_per_second": 0.244, "prompt_tokens": 21, "response_tokens_est": 71, "n_tiles": 12, "output_text": "The image depicts a cozy indoor setting, likely a restaurant or bar, with a group of people gathered around a table. The individuals are smiling and appear to be enjoying each other's company, with one person holding a smartphone. The room is warmly lit, and there are framed pictures and posters on the walls, adding to the ambiance.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24818.7, "ram_available_mb": 100953.5, "ram_percent": 19.7}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 24819.8, "ram_available_mb": 100952.4, "ram_percent": 19.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.84, "peak": 16.26, "min": 13.92}, "VDD_CPU_SOC_MSS": {"avg": 15.24, "peak": 16.92, "min": 14.17}, "VDD_GPU": {"avg": 32.52, "peak": 45.3, "min": 25.22}, "VIN": {"avg": 74.95, "peak": 129.66, "min": 54.9}}, "power_watts_avg": 32.52, "energy_joules_est": 133.51, "sample_count": 32, "duration_seconds": 4.105}, "timestamp": "2026-01-17T14:33:16.190083"}
{"image_index": 64, "image_name": "000000006763.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006763.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3648.38, "latencies_ms": [3648.38], "images_per_second": 0.274, "prompt_tokens": 19, "response_tokens_est": 58, "n_tiles": 12, "output_text": "The image features a man and a woman seated at a table, both smiling and posing for the camera. The man is wearing glasses and a blue shirt, while the woman is in a white top. The lighting is warm and ambient, with a soft glow on the table and walls.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24819.8, "ram_available_mb": 100952.4, "ram_percent": 19.7}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 24821.1, "ram_available_mb": 100951.1, "ram_percent": 19.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.87, "peak": 16.26, "min": 13.92}, "VDD_CPU_SOC_MSS": {"avg": 15.34, "peak": 16.92, "min": 14.17}, "VDD_GPU": {"avg": 33.58, "peak": 44.91, "min": 25.6}, "VIN": {"avg": 78.67, "peak": 117.98, "min": 58.17}}, "power_watts_avg": 33.58, "energy_joules_est": 122.52, "sample_count": 28, "duration_seconds": 3.649}, "timestamp": "2026-01-17T14:33:19.844773"}
{"image_index": 65, "image_name": "000000006771.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006771.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1145.159, "latencies_ms": [1145.159], "images_per_second": 0.873, "prompt_tokens": 9, "response_tokens_est": 20, "n_tiles": 6, "output_text": "A woman in a costume is talking on her phone while surrounded by other people at an event.", "error": null, "sys_before": {"cpu_percent": 24.5, "ram_used_mb": 24851.1, "ram_available_mb": 100921.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 24852.8, "ram_available_mb": 100919.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.52, "peak": 15.04, "min": 13.72}, "VDD_CPU_SOC_MSS": {"avg": 15.06, "peak": 15.75, "min": 14.18}, "VDD_GPU": {"avg": 35.45, "peak": 40.57, "min": 29.54}, "VIN": {"avg": 81.55, "peak": 123.88, "min": 61.75}}, "power_watts_avg": 35.45, "energy_joules_est": 40.61, "sample_count": 8, "duration_seconds": 1.146}, "timestamp": "2026-01-17T14:33:21.033458"}
{"image_index": 65, "image_name": "000000006771.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006771.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2155.564, "latencies_ms": [2155.564], "images_per_second": 0.464, "prompt_tokens": 23, "response_tokens_est": 56, "n_tiles": 6, "output_text": "1. Woman: 1\n2. Man: 2\n3. Woman: 1\n4. Man: 1\n5. Woman: 1\n6. Man: 1\n7. Woman: 1\n8. Man: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24852.8, "ram_available_mb": 100919.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 24853.8, "ram_available_mb": 100918.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.02, "peak": 15.44, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.6, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 31.02, "peak": 41.34, "min": 23.64}, "VIN": {"avg": 72.18, "peak": 103.47, "min": 63.12}}, "power_watts_avg": 31.02, "energy_joules_est": 66.88, "sample_count": 16, "duration_seconds": 2.156}, "timestamp": "2026-01-17T14:33:23.195103"}
{"image_index": 65, "image_name": "000000006771.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006771.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2959.049, "latencies_ms": [2959.049], "images_per_second": 0.338, "prompt_tokens": 27, "response_tokens_est": 85, "n_tiles": 6, "output_text": "The main object in the foreground is a woman wearing a costume with gold accents and a helmet. She is holding a phone to her ear and appears to be engaged in a conversation. In the background, there are several other people, including a man wearing a white cap and another man with a beard. The woman is positioned near the center of the image, while the other people are slightly out of focus in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24853.8, "ram_available_mb": 100918.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 24855.0, "ram_available_mb": 100917.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.03, "peak": 15.44, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.64, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 28.47, "peak": 39.78, "min": 23.24}, "VIN": {"avg": 66.5, "peak": 78.4, "min": 55.23}}, "power_watts_avg": 28.47, "energy_joules_est": 84.26, "sample_count": 22, "duration_seconds": 2.959}, "timestamp": "2026-01-17T14:33:26.160419"}
{"image_index": 65, "image_name": "000000006771.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006771.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2838.914, "latencies_ms": [2838.914], "images_per_second": 0.352, "prompt_tokens": 21, "response_tokens_est": 81, "n_tiles": 6, "output_text": "The image depicts a lively outdoor gathering, possibly a festival or a convention, with a diverse crowd of people. The setting appears to be in a public area, possibly a park or a plaza, with buildings and structures in the background. The focus is on a woman in the foreground, who is smiling and talking on a mobile phone, while others in the background are engaged in various activities.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24855.0, "ram_available_mb": 100917.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 24855.0, "ram_available_mb": 100917.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.03, "peak": 15.44, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.65, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 28.72, "peak": 39.78, "min": 23.25}, "VIN": {"avg": 71.94, "peak": 109.76, "min": 57.71}}, "power_watts_avg": 28.72, "energy_joules_est": 81.54, "sample_count": 21, "duration_seconds": 2.839}, "timestamp": "2026-01-17T14:33:29.005591"}
{"image_index": 65, "image_name": "000000006771.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006771.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1590.36, "latencies_ms": [1590.36], "images_per_second": 0.629, "prompt_tokens": 19, "response_tokens_est": 35, "n_tiles": 6, "output_text": "The woman in the image is wearing a black and gold costume with intricate designs and accessories. The lighting is bright, and the scene appears to be outdoors during the daytime.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24855.0, "ram_available_mb": 100917.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 24855.2, "ram_available_mb": 100916.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.98, "peak": 15.54, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.58, "peak": 16.14, "min": 14.57}, "VDD_GPU": {"avg": 32.27, "peak": 40.18, "min": 24.82}, "VIN": {"avg": 73.24, "peak": 112.62, "min": 61.71}}, "power_watts_avg": 32.27, "energy_joules_est": 51.33, "sample_count": 12, "duration_seconds": 1.591}, "timestamp": "2026-01-17T14:33:30.602137"}
{"image_index": 66, "image_name": "000000006818.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006818.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1840.255, "latencies_ms": [1840.255], "images_per_second": 0.543, "prompt_tokens": 9, "response_tokens_est": 44, "n_tiles": 6, "output_text": "The image shows a small, white tiled bathroom with a toilet, a green bucket, and a red bucket on the floor, along with a white toilet paper holder and a white shower head mounted on the wall.", "error": null, "sys_before": {"cpu_percent": 21.1, "ram_used_mb": 24855.2, "ram_available_mb": 100916.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 24856.7, "ram_available_mb": 100915.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.73, "peak": 15.14, "min": 14.12}, "VDD_CPU_SOC_MSS": {"avg": 15.27, "peak": 15.74, "min": 14.57}, "VDD_GPU": {"avg": 31.26, "peak": 40.57, "min": 23.64}, "VIN": {"avg": 73.35, "peak": 106.34, "min": 60.01}}, "power_watts_avg": 31.26, "energy_joules_est": 57.54, "sample_count": 14, "duration_seconds": 1.841}, "timestamp": "2026-01-17T14:33:32.473948"}
{"image_index": 66, "image_name": "000000006818.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006818.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1823.066, "latencies_ms": [1823.066], "images_per_second": 0.549, "prompt_tokens": 23, "response_tokens_est": 42, "n_tiles": 6, "output_text": "1. Toilet\n2. Water heater\n3. Showerhead\n4. Tiled floor\n5. Tiled wall\n6. Green bucket\n7. Red bucket\n8. White pipe", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24856.7, "ram_available_mb": 100915.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 24857.9, "ram_available_mb": 100914.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.8, "peak": 15.24, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.26, "peak": 15.74, "min": 14.56}, "VDD_GPU": {"avg": 31.91, "peak": 40.57, "min": 24.04}, "VIN": {"avg": 70.83, "peak": 101.82, "min": 59.7}}, "power_watts_avg": 31.91, "energy_joules_est": 58.18, "sample_count": 13, "duration_seconds": 1.823}, "timestamp": "2026-01-17T14:33:34.307207"}
{"image_index": 66, "image_name": "000000006818.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006818.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2939.23, "latencies_ms": [2939.23], "images_per_second": 0.34, "prompt_tokens": 27, "response_tokens_est": 81, "n_tiles": 6, "output_text": "The main objects in the image are a toilet, a green bucket, and a red bucket. The toilet is located in the foreground, with the green bucket and red bucket positioned near it. The toilet is situated on a tiled floor, with the green bucket and red bucket placed on the floor in front of it. The white pipes and the white tiled walls are also visible in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24857.9, "ram_available_mb": 100914.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 24857.9, "ram_available_mb": 100914.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.83, "peak": 15.24, "min": 14.12}, "VDD_CPU_SOC_MSS": {"avg": 15.35, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 28.22, "peak": 40.56, "min": 22.86}, "VIN": {"avg": 70.21, "peak": 110.41, "min": 58.55}}, "power_watts_avg": 28.22, "energy_joules_est": 82.96, "sample_count": 22, "duration_seconds": 2.94}, "timestamp": "2026-01-17T14:33:37.252799"}
{"image_index": 66, "image_name": "000000006818.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006818.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2874.915, "latencies_ms": [2874.915], "images_per_second": 0.348, "prompt_tokens": 21, "response_tokens_est": 79, "n_tiles": 6, "output_text": "The image depicts a small, utilitarian bathroom with white tiled walls and floor. The room features a white toilet with a black circular opening, a green bucket, and a red bucket on the floor. There is a white shower head mounted on the wall, and a white toilet paper holder is visible. The setting appears to be a public restroom, possibly in a residential area.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24857.9, "ram_available_mb": 100914.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 24857.9, "ram_available_mb": 100914.3, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.83, "peak": 15.24, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.41, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 28.25, "peak": 39.78, "min": 22.86}, "VIN": {"avg": 69.08, "peak": 99.84, "min": 60.79}}, "power_watts_avg": 28.25, "energy_joules_est": 81.22, "sample_count": 21, "duration_seconds": 2.875}, "timestamp": "2026-01-17T14:33:40.134120"}
{"image_index": 66, "image_name": "000000006818.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006818.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2248.872, "latencies_ms": [2248.872], "images_per_second": 0.445, "prompt_tokens": 19, "response_tokens_est": 57, "n_tiles": 6, "output_text": "The image depicts a small, white tiled bathroom with a single white toilet. The walls and floor are tiled in white, and there is a green bucket and a red bucket on the floor. The lighting is dim, and the overall atmosphere is clean and minimalistic.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24857.9, "ram_available_mb": 100914.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 24859.3, "ram_available_mb": 100912.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.84, "peak": 15.24, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.33, "peak": 16.14, "min": 14.57}, "VDD_GPU": {"avg": 29.57, "peak": 40.18, "min": 22.86}, "VIN": {"avg": 70.24, "peak": 99.69, "min": 61.89}}, "power_watts_avg": 29.57, "energy_joules_est": 66.51, "sample_count": 17, "duration_seconds": 2.249}, "timestamp": "2026-01-17T14:33:42.388759"}
{"image_index": 67, "image_name": "000000006894.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006894.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2055.942, "latencies_ms": [2055.942], "images_per_second": 0.486, "prompt_tokens": 9, "response_tokens_est": 13, "n_tiles": 12, "output_text": "A man is laughing while holding the ear of an elephant.", "error": null, "sys_before": {"cpu_percent": 40.4, "ram_used_mb": 24831.1, "ram_available_mb": 100941.1, "ram_percent": 19.7}, "sys_after": {"cpu_percent": 2.1, "ram_used_mb": 24833.0, "ram_available_mb": 100939.2, "ram_percent": 19.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.21, "peak": 16.66, "min": 13.59}, "VDD_CPU_SOC_MSS": {"avg": 15.93, "peak": 17.71, "min": 14.17}, "VDD_GPU": {"avg": 37.79, "peak": 43.73, "min": 29.56}, "VIN": {"avg": 78.17, "peak": 104.74, "min": 57.16}}, "power_watts_avg": 37.79, "energy_joules_est": 77.71, "sample_count": 15, "duration_seconds": 2.056}, "timestamp": "2026-01-17T14:33:44.518806"}
{"image_index": 67, "image_name": "000000006894.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006894.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2849.684, "latencies_ms": [2849.684], "images_per_second": 0.351, "prompt_tokens": 23, "response_tokens_est": 35, "n_tiles": 12, "output_text": "1. Elephant\n2. Man\n3. Glasses\n4. T-shirt\n5. Ear\n6. Neck\n7. Face\n8. Hair", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24833.0, "ram_available_mb": 100939.2, "ram_percent": 19.7}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 24833.0, "ram_available_mb": 100939.2, "ram_percent": 19.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.24, "peak": 16.56, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.76, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 36.61, "peak": 45.68, "min": 26.79}, "VIN": {"avg": 79.76, "peak": 133.39, "min": 58.47}}, "power_watts_avg": 36.61, "energy_joules_est": 104.34, "sample_count": 22, "duration_seconds": 2.85}, "timestamp": "2026-01-17T14:33:47.375083"}
{"image_index": 67, "image_name": "000000006894.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006894.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3360.656, "latencies_ms": [3360.656], "images_per_second": 0.298, "prompt_tokens": 27, "response_tokens_est": 50, "n_tiles": 12, "output_text": "The main object in the foreground is a man wearing a gray t-shirt. He is interacting with an elephant, which is partially visible in the background. The elephant is standing close to the man, suggesting a close relationship or familiarity between them.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24833.0, "ram_available_mb": 100939.2, "ram_percent": 19.7}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 24833.5, "ram_available_mb": 100938.7, "ram_percent": 19.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.22, "peak": 16.56, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.69, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 34.6, "peak": 44.89, "min": 26.0}, "VIN": {"avg": 75.15, "peak": 110.71, "min": 60.55}}, "power_watts_avg": 34.6, "energy_joules_est": 116.29, "sample_count": 26, "duration_seconds": 3.361}, "timestamp": "2026-01-17T14:33:50.741828"}
{"image_index": 67, "image_name": "000000006894.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006894.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3805.214, "latencies_ms": [3805.214], "images_per_second": 0.263, "prompt_tokens": 21, "response_tokens_est": 63, "n_tiles": 12, "output_text": "The image depicts a man in a natural setting, likely a park or wildlife reserve, with a large elephant in the background. The man is smiling broadly and appears to be enjoying the moment, possibly interacting with the elephant. The background features lush greenery and hills, suggesting a serene and peaceful environment.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24833.5, "ram_available_mb": 100938.7, "ram_percent": 19.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 24833.7, "ram_available_mb": 100938.4, "ram_percent": 19.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.17, "peak": 16.56, "min": 14.12}, "VDD_CPU_SOC_MSS": {"avg": 15.68, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 33.54, "peak": 44.91, "min": 26.0}, "VIN": {"avg": 77.33, "peak": 115.74, "min": 61.85}}, "power_watts_avg": 33.54, "energy_joules_est": 127.64, "sample_count": 29, "duration_seconds": 3.806}, "timestamp": "2026-01-17T14:33:54.554393"}
{"image_index": 67, "image_name": "000000006894.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006894.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3023.328, "latencies_ms": [3023.328], "images_per_second": 0.331, "prompt_tokens": 19, "response_tokens_est": 40, "n_tiles": 12, "output_text": "The image features a man wearing glasses and a light-colored t-shirt. The man is smiling and appears to be in a sunny outdoor setting, with the background showing lush greenery and a hill.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24833.7, "ram_available_mb": 100938.4, "ram_percent": 19.7}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 24834.5, "ram_available_mb": 100937.7, "ram_percent": 19.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.19, "peak": 16.56, "min": 14.12}, "VDD_CPU_SOC_MSS": {"avg": 15.7, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 35.31, "peak": 45.3, "min": 26.4}, "VIN": {"avg": 74.61, "peak": 110.55, "min": 60.6}}, "power_watts_avg": 35.31, "energy_joules_est": 106.78, "sample_count": 24, "duration_seconds": 3.024}, "timestamp": "2026-01-17T14:33:57.584148"}
{"image_index": 68, "image_name": "000000006954.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006954.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2981.183, "latencies_ms": [2981.183], "images_per_second": 0.335, "prompt_tokens": 9, "response_tokens_est": 40, "n_tiles": 12, "output_text": "The image depicts a group of four children sitting on the grass, each holding a white frisbee, with one child in the foreground holding a frisbee with a logo on it.", "error": null, "sys_before": {"cpu_percent": 39.4, "ram_used_mb": 24835.3, "ram_available_mb": 100936.8, "ram_percent": 19.7}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 24836.6, "ram_available_mb": 100935.6, "ram_percent": 19.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.07, "peak": 16.66, "min": 13.29}, "VDD_CPU_SOC_MSS": {"avg": 15.68, "peak": 17.71, "min": 14.17}, "VDD_GPU": {"avg": 34.67, "peak": 44.12, "min": 26.0}, "VIN": {"avg": 79.02, "peak": 112.82, "min": 64.7}}, "power_watts_avg": 34.67, "energy_joules_est": 103.37, "sample_count": 23, "duration_seconds": 2.981}, "timestamp": "2026-01-17T14:34:00.673753"}
{"image_index": 68, "image_name": "000000006954.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006954.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2917.941, "latencies_ms": [2917.941], "images_per_second": 0.343, "prompt_tokens": 23, "response_tokens_est": 37, "n_tiles": 12, "output_text": "1. Frisbee: 2\n2. Children: 3\n3. Grass: 1\n4. Tree: 1\n5. Background: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24836.6, "ram_available_mb": 100935.6, "ram_percent": 19.7}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 24838.0, "ram_available_mb": 100934.1, "ram_percent": 19.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.95, "peak": 16.36, "min": 13.92}, "VDD_CPU_SOC_MSS": {"avg": 15.45, "peak": 17.32, "min": 14.17}, "VDD_GPU": {"avg": 35.88, "peak": 45.69, "min": 26.39}, "VIN": {"avg": 73.11, "peak": 109.02, "min": 58.16}}, "power_watts_avg": 35.88, "energy_joules_est": 104.71, "sample_count": 23, "duration_seconds": 2.918}, "timestamp": "2026-01-17T14:34:03.598960"}
{"image_index": 68, "image_name": "000000006954.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006954.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 4149.003, "latencies_ms": [4149.003], "images_per_second": 0.241, "prompt_tokens": 27, "response_tokens_est": 72, "n_tiles": 12, "output_text": "The main objects in the image are three children sitting on the grass. The child on the left is holding a frisbee, the child in the middle is also holding a frisbee, and the child on the right is holding a frisbee as well. The background consists of trees and foliage, indicating that the children are outdoors.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24838.0, "ram_available_mb": 100934.1, "ram_percent": 19.7}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 24838.5, "ram_available_mb": 100933.6, "ram_percent": 19.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.83, "peak": 16.26, "min": 14.02}, "VDD_CPU_SOC_MSS": {"avg": 15.31, "peak": 17.32, "min": 14.17}, "VDD_GPU": {"avg": 32.72, "peak": 46.09, "min": 25.21}, "VIN": {"avg": 73.07, "peak": 104.7, "min": 54.17}}, "power_watts_avg": 32.72, "energy_joules_est": 135.77, "sample_count": 33, "duration_seconds": 4.15}, "timestamp": "2026-01-17T14:34:07.754787"}
{"image_index": 68, "image_name": "000000006954.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006954.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2926.826, "latencies_ms": [2926.826], "images_per_second": 0.342, "prompt_tokens": 21, "response_tokens_est": 37, "n_tiles": 12, "output_text": "The scene is set outdoors in a grassy area with a group of children gathered around. They are holding white frisbees and appear to be engaged in a playful activity.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24838.5, "ram_available_mb": 100933.6, "ram_percent": 19.7}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 24838.8, "ram_available_mb": 100933.4, "ram_percent": 19.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.91, "peak": 16.26, "min": 13.82}, "VDD_CPU_SOC_MSS": {"avg": 15.35, "peak": 16.92, "min": 14.17}, "VDD_GPU": {"avg": 35.5, "peak": 45.69, "min": 26.0}, "VIN": {"avg": 73.63, "peak": 100.25, "min": 50.01}}, "power_watts_avg": 35.5, "energy_joules_est": 103.92, "sample_count": 23, "duration_seconds": 2.927}, "timestamp": "2026-01-17T14:34:10.689636"}
{"image_index": 68, "image_name": "000000006954.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006954.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3242.17, "latencies_ms": [3242.17], "images_per_second": 0.308, "prompt_tokens": 19, "response_tokens_est": 46, "n_tiles": 12, "output_text": "The image depicts a group of children playing outdoors on a sunny day. The children are wearing casual clothing, and the grass is green and lush. The lighting is bright and natural, indicating a clear day with ample sunlight.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 24838.8, "ram_available_mb": 100933.4, "ram_percent": 19.7}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 24839.7, "ram_available_mb": 100932.4, "ram_percent": 19.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.9, "peak": 16.36, "min": 13.92}, "VDD_CPU_SOC_MSS": {"avg": 15.43, "peak": 17.32, "min": 14.17}, "VDD_GPU": {"avg": 34.67, "peak": 45.3, "min": 25.61}, "VIN": {"avg": 74.99, "peak": 120.63, "min": 53.91}}, "power_watts_avg": 34.67, "energy_joules_est": 112.42, "sample_count": 25, "duration_seconds": 3.243}, "timestamp": "2026-01-17T14:34:13.938426"}
{"image_index": 69, "image_name": "000000007088.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007088.jpg", "image_width": 478, "image_height": 640, "image_resolution": "478x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2392.096, "latencies_ms": [2392.096], "images_per_second": 0.418, "prompt_tokens": 9, "response_tokens_est": 23, "n_tiles": 12, "output_text": "A young child is holding a black umbrella with a pattern, standing on a wet sidewalk next to a hedge.", "error": null, "sys_before": {"cpu_percent": 41.5, "ram_used_mb": 24853.1, "ram_available_mb": 100919.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.4, "ram_used_mb": 24846.1, "ram_available_mb": 100926.0, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.1, "peak": 16.66, "min": 13.09}, "VDD_CPU_SOC_MSS": {"avg": 15.9, "peak": 17.71, "min": 14.17}, "VDD_GPU": {"avg": 36.88, "peak": 44.12, "min": 28.36}, "VIN": {"avg": 82.57, "peak": 118.6, "min": 58.8}}, "power_watts_avg": 36.88, "energy_joules_est": 88.23, "sample_count": 18, "duration_seconds": 2.392}, "timestamp": "2026-01-17T14:34:16.438687"}
{"image_index": 69, "image_name": "000000007088.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007088.jpg", "image_width": 478, "image_height": 640, "image_resolution": "478x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 3086.926, "latencies_ms": [3086.926], "images_per_second": 0.324, "prompt_tokens": 23, "response_tokens_est": 42, "n_tiles": 12, "output_text": "umbrella: 1\nchild: 1\ncoat: 1\nsweater: 1\nsocks: 1\nshoe: 1\nglove: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24846.1, "ram_available_mb": 100926.0, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 24842.4, "ram_available_mb": 100929.7, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.24, "peak": 16.56, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.91, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 35.47, "peak": 45.68, "min": 26.4}, "VIN": {"avg": 79.88, "peak": 117.83, "min": 59.22}}, "power_watts_avg": 35.47, "energy_joules_est": 109.51, "sample_count": 24, "duration_seconds": 3.087}, "timestamp": "2026-01-17T14:34:19.531700"}
{"image_index": 69, "image_name": "000000007088.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007088.jpg", "image_width": 478, "image_height": 640, "image_resolution": "478x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3520.22, "latencies_ms": [3520.22], "images_per_second": 0.284, "prompt_tokens": 27, "response_tokens_est": 55, "n_tiles": 12, "output_text": "The main object in the foreground is a young child holding a black umbrella with a pattern. The child is standing on a wet sidewalk, with a bush and a parked car visible in the background. The child is near the bush, and the car is parked further back.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24842.4, "ram_available_mb": 100929.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 24836.6, "ram_available_mb": 100935.6, "ram_percent": 19.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.22, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.89, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 34.32, "peak": 44.49, "min": 26.01}, "VIN": {"avg": 76.4, "peak": 112.61, "min": 65.32}}, "power_watts_avg": 34.32, "energy_joules_est": 120.83, "sample_count": 27, "duration_seconds": 3.521}, "timestamp": "2026-01-17T14:34:23.058417"}
{"image_index": 69, "image_name": "000000007088.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007088.jpg", "image_width": 478, "image_height": 640, "image_resolution": "478x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3902.681, "latencies_ms": [3902.681], "images_per_second": 0.256, "prompt_tokens": 21, "response_tokens_est": 66, "n_tiles": 12, "output_text": "The image depicts a young child standing under a black umbrella in a residential area during a rainy day. The child is dressed in a red coat and blue jeans, holding the umbrella with both hands. The background shows a wet sidewalk, a parked car, and a neatly trimmed hedge, indicating a typical suburban setting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24836.6, "ram_available_mb": 100935.6, "ram_percent": 19.7}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 24837.6, "ram_available_mb": 100934.6, "ram_percent": 19.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.19, "peak": 16.56, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.73, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 33.21, "peak": 44.91, "min": 25.6}, "VIN": {"avg": 75.81, "peak": 116.72, "min": 59.16}}, "power_watts_avg": 33.21, "energy_joules_est": 129.62, "sample_count": 30, "duration_seconds": 3.903}, "timestamp": "2026-01-17T14:34:26.967796"}
{"image_index": 69, "image_name": "000000007088.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007088.jpg", "image_width": 478, "image_height": 640, "image_resolution": "478x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3910.485, "latencies_ms": [3910.485], "images_per_second": 0.256, "prompt_tokens": 19, "response_tokens_est": 66, "n_tiles": 12, "output_text": "The image depicts a young child holding a black umbrella with a pink pattern, standing on a wet sidewalk. The child is wearing a red coat and dark pants, and the umbrella provides some protection from the rain. The lighting is soft and diffused, suggesting an overcast day, and the wet ground reflects the surroundings.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24837.6, "ram_available_mb": 100934.6, "ram_percent": 19.7}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 24838.1, "ram_available_mb": 100934.1, "ram_percent": 19.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.16, "peak": 16.56, "min": 14.12}, "VDD_CPU_SOC_MSS": {"avg": 15.68, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 33.19, "peak": 44.91, "min": 26.0}, "VIN": {"avg": 76.26, "peak": 116.93, "min": 54.47}}, "power_watts_avg": 33.19, "energy_joules_est": 129.8, "sample_count": 30, "duration_seconds": 3.911}, "timestamp": "2026-01-17T14:34:30.884919"}
{"image_index": 70, "image_name": "000000007108.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007108.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1682.012, "latencies_ms": [1682.012], "images_per_second": 0.595, "prompt_tokens": 9, "response_tokens_est": 40, "n_tiles": 6, "output_text": "The image captures a group of elephants in a dusty, outdoor environment, with one elephant prominently in the foreground, its trunk raised as if it is either greeting or communicating with the others.", "error": null, "sys_before": {"cpu_percent": 39.8, "ram_used_mb": 24861.9, "ram_available_mb": 100910.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 24862.4, "ram_available_mb": 100909.7, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.72, "peak": 15.11, "min": 13.72}, "VDD_CPU_SOC_MSS": {"avg": 15.38, "peak": 16.14, "min": 14.18}, "VDD_GPU": {"avg": 31.91, "peak": 40.57, "min": 24.82}, "VIN": {"avg": 73.18, "peak": 108.36, "min": 59.53}}, "power_watts_avg": 31.91, "energy_joules_est": 53.68, "sample_count": 13, "duration_seconds": 1.682}, "timestamp": "2026-01-17T14:34:32.637238"}
{"image_index": 70, "image_name": "000000007108.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007108.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1251.196, "latencies_ms": [1251.196], "images_per_second": 0.799, "prompt_tokens": 23, "response_tokens_est": 22, "n_tiles": 6, "output_text": "elephant: 2\nwater: 1\ntrees: 1\nsky: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24862.4, "ram_available_mb": 100909.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 24864.2, "ram_available_mb": 100908.0, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.82, "peak": 15.24, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.35, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 35.19, "peak": 40.56, "min": 28.36}, "VIN": {"avg": 79.81, "peak": 108.76, "min": 62.21}}, "power_watts_avg": 35.19, "energy_joules_est": 44.04, "sample_count": 9, "duration_seconds": 1.252}, "timestamp": "2026-01-17T14:34:33.894828"}
{"image_index": 70, "image_name": "000000007108.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007108.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2406.187, "latencies_ms": [2406.187], "images_per_second": 0.416, "prompt_tokens": 27, "response_tokens_est": 63, "n_tiles": 6, "output_text": "The main object in the foreground is an elephant, which is positioned near the center of the image. The background features another elephant, slightly out of focus, indicating that the primary focus is on the elephant in the foreground. The elephant in the background is further away, creating a sense of depth in the image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24864.2, "ram_available_mb": 100908.0, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 24865.1, "ram_available_mb": 100907.0, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.9, "peak": 15.24, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.55, "peak": 15.75, "min": 14.56}, "VDD_GPU": {"avg": 30.25, "peak": 41.36, "min": 23.25}, "VIN": {"avg": 67.58, "peak": 101.87, "min": 57.05}}, "power_watts_avg": 30.25, "energy_joules_est": 72.8, "sample_count": 18, "duration_seconds": 2.407}, "timestamp": "2026-01-17T14:34:36.307627"}
{"image_index": 70, "image_name": "000000007108.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007108.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2392.743, "latencies_ms": [2392.743], "images_per_second": 0.418, "prompt_tokens": 21, "response_tokens_est": 62, "n_tiles": 6, "output_text": "The image captures a group of elephants in a natural, outdoor setting. The elephants are standing on a dirt path, with one elephant in the foreground appearing to be in motion, possibly walking or running. The background shows more elephants, some of which are also in motion, creating a dynamic and lively scene.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24865.1, "ram_available_mb": 100907.0, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 24866.4, "ram_available_mb": 100905.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.83, "peak": 15.24, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.42, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 29.28, "peak": 40.18, "min": 22.86}, "VIN": {"avg": 68.08, "peak": 112.46, "min": 57.56}}, "power_watts_avg": 29.28, "energy_joules_est": 70.07, "sample_count": 18, "duration_seconds": 2.393}, "timestamp": "2026-01-17T14:34:38.706327"}
{"image_index": 70, "image_name": "000000007108.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007108.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2306.231, "latencies_ms": [2306.231], "images_per_second": 0.434, "prompt_tokens": 19, "response_tokens_est": 59, "n_tiles": 6, "output_text": "The image features a group of elephants in a dusty, reddish-brown environment. The elephants have rough, wrinkled skin, and their tusks are visible. The lighting is natural, with a soft, diffused quality, suggesting it might be early morning or late afternoon.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24866.4, "ram_available_mb": 100905.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 24866.4, "ram_available_mb": 100905.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.83, "peak": 15.24, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.33, "peak": 16.14, "min": 14.57}, "VDD_GPU": {"avg": 29.37, "peak": 40.18, "min": 23.25}, "VIN": {"avg": 67.91, "peak": 94.55, "min": 55.15}}, "power_watts_avg": 29.37, "energy_joules_est": 67.76, "sample_count": 18, "duration_seconds": 2.307}, "timestamp": "2026-01-17T14:34:41.019321"}
{"image_index": 71, "image_name": "000000007278.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007278.jpg", "image_width": 640, "image_height": 482, "image_resolution": "640x482", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2289.081, "latencies_ms": [2289.081], "images_per_second": 0.437, "prompt_tokens": 9, "response_tokens_est": 20, "n_tiles": 12, "output_text": "A surfer is riding a wave in the ocean, wearing a red shirt and black shorts.", "error": null, "sys_before": {"cpu_percent": 40.3, "ram_used_mb": 24837.1, "ram_available_mb": 100935.1, "ram_percent": 19.7}, "sys_after": {"cpu_percent": 2.4, "ram_used_mb": 24837.3, "ram_available_mb": 100934.9, "ram_percent": 19.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.12, "peak": 16.66, "min": 13.39}, "VDD_CPU_SOC_MSS": {"avg": 15.95, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 36.82, "peak": 43.73, "min": 28.76}, "VIN": {"avg": 84.13, "peak": 123.82, "min": 64.67}}, "power_watts_avg": 36.82, "energy_joules_est": 84.29, "sample_count": 17, "duration_seconds": 2.289}, "timestamp": "2026-01-17T14:34:43.408804"}
{"image_index": 71, "image_name": "000000007278.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007278.jpg", "image_width": 640, "image_height": 482, "image_resolution": "640x482", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 3025.462, "latencies_ms": [3025.462], "images_per_second": 0.331, "prompt_tokens": 23, "response_tokens_est": 40, "n_tiles": 12, "output_text": "object: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24837.3, "ram_available_mb": 100934.9, "ram_percent": 19.7}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 24838.3, "ram_available_mb": 100933.9, "ram_percent": 19.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.23, "peak": 16.56, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.76, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 35.78, "peak": 45.3, "min": 26.39}, "VIN": {"avg": 76.08, "peak": 116.32, "min": 66.22}}, "power_watts_avg": 35.78, "energy_joules_est": 108.26, "sample_count": 23, "duration_seconds": 3.026}, "timestamp": "2026-01-17T14:34:46.440533"}
{"image_index": 71, "image_name": "000000007278.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007278.jpg", "image_width": 640, "image_height": 482, "image_resolution": "640x482", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2787.021, "latencies_ms": [2787.021], "images_per_second": 0.359, "prompt_tokens": 27, "response_tokens_est": 33, "n_tiles": 12, "output_text": "The main object in the foreground is a surfer riding a wave. The wave is in the background, and the surfer is near the water's surface.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24838.3, "ram_available_mb": 100933.9, "ram_percent": 19.7}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 24838.8, "ram_available_mb": 100933.4, "ram_percent": 19.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.24, "peak": 16.56, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.76, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 36.29, "peak": 45.68, "min": 26.79}, "VIN": {"avg": 80.54, "peak": 135.54, "min": 66.17}}, "power_watts_avg": 36.29, "energy_joules_est": 101.15, "sample_count": 22, "duration_seconds": 2.787}, "timestamp": "2026-01-17T14:34:49.236800"}
{"image_index": 71, "image_name": "000000007278.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007278.jpg", "image_width": 640, "image_height": 482, "image_resolution": "640x482", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 4042.654, "latencies_ms": [4042.654], "images_per_second": 0.247, "prompt_tokens": 21, "response_tokens_est": 70, "n_tiles": 12, "output_text": "The image captures a surfer skillfully riding a wave in the ocean. The surfer is wearing a red and black wetsuit, and the wave is breaking to the right, creating a spray of water around the surfer. The setting is a dynamic and powerful ocean environment, with the surfer navigating the wave with precision and skill.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24838.8, "ram_available_mb": 100933.4, "ram_percent": 19.7}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 24839.0, "ram_available_mb": 100933.2, "ram_percent": 19.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.18, "peak": 16.56, "min": 14.12}, "VDD_CPU_SOC_MSS": {"avg": 15.77, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 33.18, "peak": 44.89, "min": 26.0}, "VIN": {"avg": 74.6, "peak": 111.58, "min": 55.15}}, "power_watts_avg": 33.18, "energy_joules_est": 134.15, "sample_count": 31, "duration_seconds": 4.043}, "timestamp": "2026-01-17T14:34:53.290288"}
{"image_index": 71, "image_name": "000000007278.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007278.jpg", "image_width": 640, "image_height": 482, "image_resolution": "640x482", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3668.178, "latencies_ms": [3668.178], "images_per_second": 0.273, "prompt_tokens": 19, "response_tokens_est": 59, "n_tiles": 12, "output_text": "The image captures a surfer skillfully riding a wave, with the surfer dressed in a red and black wetsuit. The wave is a vibrant green, indicating a sunny day with clear skies. The lighting is bright and natural, casting a clear reflection on the water's surface.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24839.0, "ram_available_mb": 100933.2, "ram_percent": 19.7}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 24839.8, "ram_available_mb": 100932.4, "ram_percent": 19.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.17, "peak": 16.66, "min": 14.12}, "VDD_CPU_SOC_MSS": {"avg": 15.68, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 33.65, "peak": 44.91, "min": 26.0}, "VIN": {"avg": 77.31, "peak": 116.45, "min": 55.7}}, "power_watts_avg": 33.65, "energy_joules_est": 123.45, "sample_count": 29, "duration_seconds": 3.669}, "timestamp": "2026-01-17T14:34:56.965215"}
{"image_index": 72, "image_name": "000000007281.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007281.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 793.304, "latencies_ms": [793.304], "images_per_second": 1.261, "prompt_tokens": 9, "response_tokens_est": 27, "n_tiles": 2, "output_text": "Two people are riding horses on a sandy beach, with one person holding a stick, while the other is holding a flag.", "error": null, "sys_before": {"cpu_percent": 34.9, "ram_used_mb": 24839.8, "ram_available_mb": 100932.4, "ram_percent": 19.7}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 24840.0, "ram_available_mb": 100932.2, "ram_percent": 19.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5055.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.51, "peak": 15.11, "min": 14.0}, "VDD_CPU_SOC_MSS": {"avg": 15.23, "peak": 15.75, "min": 14.57}, "VDD_GPU": {"avg": 27.51, "peak": 33.48, "min": 23.25}, "VIN": {"avg": 65.98, "peak": 78.49, "min": 63.13}}, "power_watts_avg": 27.51, "energy_joules_est": 21.84, "sample_count": 6, "duration_seconds": 0.794}, "timestamp": "2026-01-17T14:34:57.790804"}
{"image_index": 72, "image_name": "000000007281.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007281.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1539.888, "latencies_ms": [1539.888], "images_per_second": 0.649, "prompt_tokens": 23, "response_tokens_est": 60, "n_tiles": 2, "output_text": "1. Horse: 1\n2. Rider: 2\n3. Horseback rider: 1\n4. Horse: 1\n5. Horseback rider: 1\n6. Horse: 1\n7. Horse: 1\n8. Horse: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24840.0, "ram_available_mb": 100932.2, "ram_percent": 19.7}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 24839.7, "ram_available_mb": 100932.4, "ram_percent": 19.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5064.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.23, "peak": 15.72, "min": 14.61}, "VDD_CPU_SOC_MSS": {"avg": 16.05, "peak": 16.54, "min": 15.36}, "VDD_GPU": {"avg": 24.49, "peak": 32.3, "min": 20.89}, "VIN": {"avg": 67.43, "peak": 101.69, "min": 58.27}}, "power_watts_avg": 24.49, "energy_joules_est": 37.72, "sample_count": 12, "duration_seconds": 1.54}, "timestamp": "2026-01-17T14:34:59.337401"}
{"image_index": 72, "image_name": "000000007281.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007281.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2856.978, "latencies_ms": [2856.978], "images_per_second": 0.35, "prompt_tokens": 27, "response_tokens_est": 114, "n_tiles": 2, "output_text": "The main objects in the image are two people riding horses on a sandy beach. The person on the left is wearing a white shirt and a hat, while the person on the right is wearing a white shirt and a hat as well. The horses are in the foreground, with the person on the left riding a white horse and the person on the right riding a brown horse. In the background, there are more people on the beach, some near the water and others further away. The sky is partly cloudy, and the ocean is visible in the distance.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24839.7, "ram_available_mb": 100932.4, "ram_percent": 19.7}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 24841.0, "ram_available_mb": 100931.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5067.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.29, "peak": 15.42, "min": 14.71}, "VDD_CPU_SOC_MSS": {"avg": 16.25, "peak": 16.54, "min": 15.35}, "VDD_GPU": {"avg": 22.3, "peak": 30.33, "min": 20.48}, "VIN": {"avg": 64.63, "peak": 108.35, "min": 52.72}}, "power_watts_avg": 22.3, "energy_joules_est": 63.72, "sample_count": 23, "duration_seconds": 2.857}, "timestamp": "2026-01-17T14:35:02.200286"}
{"image_index": 72, "image_name": "000000007281.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007281.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1971.795, "latencies_ms": [1971.795], "images_per_second": 0.507, "prompt_tokens": 21, "response_tokens_est": 76, "n_tiles": 2, "output_text": "The image depicts a lively beach scene with two individuals riding horses. One person is on a white horse, while the other is on a brown horse. They are both wearing traditional attire and appear to be enjoying a moment of leisure on the sandy beach. The background shows a calm ocean and a partly cloudy sky, with other beachgoers in the distance.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24841.0, "ram_available_mb": 100931.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 24841.0, "ram_available_mb": 100931.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5063.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.18, "peak": 15.42, "min": 14.61}, "VDD_CPU_SOC_MSS": {"avg": 16.12, "peak": 16.54, "min": 15.36}, "VDD_GPU": {"avg": 23.01, "peak": 29.94, "min": 20.49}, "VIN": {"avg": 65.81, "peak": 104.22, "min": 57.64}}, "power_watts_avg": 23.01, "energy_joules_est": 45.38, "sample_count": 15, "duration_seconds": 1.972}, "timestamp": "2026-01-17T14:35:04.178016"}
{"image_index": 72, "image_name": "000000007281.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007281.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1123.552, "latencies_ms": [1123.552], "images_per_second": 0.89, "prompt_tokens": 19, "response_tokens_est": 40, "n_tiles": 2, "output_text": "The image depicts a sunny beach scene with clear blue skies and scattered clouds. The sandy beach is populated with people, a horse, and a rider, all bathed in bright sunlight.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24841.0, "ram_available_mb": 100931.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24841.7, "ram_available_mb": 100930.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5062.6, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.01, "peak": 15.42, "min": 14.71}, "VDD_CPU_SOC_MSS": {"avg": 15.9, "peak": 16.14, "min": 15.36}, "VDD_GPU": {"avg": 24.77, "peak": 29.54, "min": 21.66}, "VIN": {"avg": 67.2, "peak": 101.39, "min": 58.53}}, "power_watts_avg": 24.77, "energy_joules_est": 27.84, "sample_count": 8, "duration_seconds": 1.124}, "timestamp": "2026-01-17T14:35:05.307603"}
{"image_index": 73, "image_name": "000000007386.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007386.jpg", "image_width": 600, "image_height": 400, "image_resolution": "600x400", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1541.641, "latencies_ms": [1541.641], "images_per_second": 0.649, "prompt_tokens": 9, "response_tokens_est": 35, "n_tiles": 6, "output_text": "A small dog is standing on the ground next to a red tractor with a black seat, which is parked in a yard with a bicycle and a blue tarp nearby.", "error": null, "sys_before": {"cpu_percent": 39.5, "ram_used_mb": 24854.6, "ram_available_mb": 100917.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 24855.3, "ram_available_mb": 100916.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.78, "peak": 15.24, "min": 14.0}, "VDD_CPU_SOC_MSS": {"avg": 15.5, "peak": 16.14, "min": 14.57}, "VDD_GPU": {"avg": 31.95, "peak": 39.39, "min": 25.61}, "VIN": {"avg": 74.83, "peak": 106.43, "min": 63.42}}, "power_watts_avg": 31.95, "energy_joules_est": 49.26, "sample_count": 11, "duration_seconds": 1.542}, "timestamp": "2026-01-17T14:35:06.908175"}
{"image_index": 73, "image_name": "000000007386.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007386.jpg", "image_width": 600, "image_height": 400, "image_resolution": "600x400", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1755.842, "latencies_ms": [1755.842], "images_per_second": 0.57, "prompt_tokens": 23, "response_tokens_est": 41, "n_tiles": 6, "output_text": "1. Motorcycle\n2. Lawn mower\n3. Dog\n4. Bicycle\n5. Car\n6. Tire\n7. Wheel\n8. Lawn mower", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24855.3, "ram_available_mb": 100916.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 24855.6, "ram_available_mb": 100916.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.98, "peak": 15.44, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.54, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 32.49, "peak": 40.57, "min": 24.83}, "VIN": {"avg": 77.38, "peak": 104.52, "min": 62.39}}, "power_watts_avg": 32.49, "energy_joules_est": 57.06, "sample_count": 13, "duration_seconds": 1.756}, "timestamp": "2026-01-17T14:35:08.672158"}
{"image_index": 73, "image_name": "000000007386.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007386.jpg", "image_width": 600, "image_height": 400, "image_resolution": "600x400", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2617.754, "latencies_ms": [2617.754], "images_per_second": 0.382, "prompt_tokens": 27, "response_tokens_est": 70, "n_tiles": 6, "output_text": "The main object in the foreground is a red tractor with a black seat. The tractor is positioned on a gravel surface, with a small dog standing near it. In the background, there is a parked silver car and a blue tarp covering part of the car. The tractor is situated near the car, and the dog is near the tractor.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24855.6, "ram_available_mb": 100916.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 24856.6, "ram_available_mb": 100915.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.88, "peak": 15.34, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.43, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 29.06, "peak": 40.56, "min": 22.86}, "VIN": {"avg": 70.59, "peak": 96.65, "min": 62.02}}, "power_watts_avg": 29.06, "energy_joules_est": 76.08, "sample_count": 20, "duration_seconds": 2.618}, "timestamp": "2026-01-17T14:35:11.297022"}
{"image_index": 73, "image_name": "000000007386.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007386.jpg", "image_width": 600, "image_height": 400, "image_resolution": "600x400", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1769.521, "latencies_ms": [1769.521], "images_per_second": 0.565, "prompt_tokens": 21, "response_tokens_est": 40, "n_tiles": 6, "output_text": "The image depicts a rustic outdoor scene with a red tractor and a small dog standing beside it. The setting appears to be a rural area with greenery and a clear sky in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24856.6, "ram_available_mb": 100915.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 24857.6, "ram_available_mb": 100914.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.8, "peak": 15.24, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.29, "peak": 15.74, "min": 14.56}, "VDD_GPU": {"avg": 31.85, "peak": 40.57, "min": 24.43}, "VIN": {"avg": 71.81, "peak": 100.55, "min": 57.22}}, "power_watts_avg": 31.85, "energy_joules_est": 56.37, "sample_count": 13, "duration_seconds": 1.77}, "timestamp": "2026-01-17T14:35:13.072683"}
{"image_index": 73, "image_name": "000000007386.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007386.jpg", "image_width": 600, "image_height": 400, "image_resolution": "600x400", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1908.895, "latencies_ms": [1908.895], "images_per_second": 0.524, "prompt_tokens": 19, "response_tokens_est": 45, "n_tiles": 6, "output_text": "The image features a red tractor with a black seat, situated on a gravel surface. The tractor is equipped with a metal ladder, and the surrounding area is illuminated by natural sunlight, creating a warm and inviting atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24857.5, "ram_available_mb": 100914.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 24858.8, "ram_available_mb": 100913.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.8, "peak": 15.24, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.3, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 31.32, "peak": 40.57, "min": 24.03}, "VIN": {"avg": 69.87, "peak": 92.89, "min": 57.63}}, "power_watts_avg": 31.32, "energy_joules_est": 59.8, "sample_count": 14, "duration_seconds": 1.909}, "timestamp": "2026-01-17T14:35:14.991835"}
{"image_index": 74, "image_name": "000000007511.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007511.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2662.775, "latencies_ms": [2662.775], "images_per_second": 0.376, "prompt_tokens": 9, "response_tokens_est": 31, "n_tiles": 12, "output_text": "A man is standing on a sandy beach, holding a kite in his hands, with a few people around him, enjoying the sunny day.", "error": null, "sys_before": {"cpu_percent": 44.7, "ram_used_mb": 24833.8, "ram_available_mb": 100938.4, "ram_percent": 19.7}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 24835.7, "ram_available_mb": 100936.4, "ram_percent": 19.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.11, "peak": 16.66, "min": 13.29}, "VDD_CPU_SOC_MSS": {"avg": 15.82, "peak": 17.71, "min": 14.17}, "VDD_GPU": {"avg": 35.53, "peak": 43.33, "min": 27.19}, "VIN": {"avg": 77.36, "peak": 114.62, "min": 59.62}}, "power_watts_avg": 35.53, "energy_joules_est": 94.62, "sample_count": 20, "duration_seconds": 2.663}, "timestamp": "2026-01-17T14:35:17.758780"}
{"image_index": 74, "image_name": "000000007511.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007511.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 3028.25, "latencies_ms": [3028.25], "images_per_second": 0.33, "prompt_tokens": 23, "response_tokens_est": 40, "n_tiles": 12, "output_text": "object: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24835.7, "ram_available_mb": 100936.4, "ram_percent": 19.7}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 24836.7, "ram_available_mb": 100935.5, "ram_percent": 19.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.2, "peak": 16.56, "min": 14.12}, "VDD_CPU_SOC_MSS": {"avg": 15.76, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 35.2, "peak": 45.28, "min": 26.39}, "VIN": {"avg": 76.39, "peak": 105.83, "min": 58.61}}, "power_watts_avg": 35.2, "energy_joules_est": 106.62, "sample_count": 24, "duration_seconds": 3.029}, "timestamp": "2026-01-17T14:35:20.794606"}
{"image_index": 74, "image_name": "000000007511.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007511.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 4219.316, "latencies_ms": [4219.316], "images_per_second": 0.237, "prompt_tokens": 27, "response_tokens_est": 75, "n_tiles": 12, "output_text": "The main object in the foreground is a person standing on the sandy beach, with their back to the camera. The person is wearing a black shirt and blue jeans. In the background, there is a large body of water, a few trees, and a few people scattered around. The person in the foreground is near the water, while the background is more distant.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24836.7, "ram_available_mb": 100935.5, "ram_percent": 19.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 24837.4, "ram_available_mb": 100934.7, "ram_percent": 19.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.16, "peak": 16.56, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.63, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 32.77, "peak": 45.69, "min": 25.6}, "VIN": {"avg": 75.73, "peak": 131.51, "min": 62.09}}, "power_watts_avg": 32.77, "energy_joules_est": 138.28, "sample_count": 33, "duration_seconds": 4.22}, "timestamp": "2026-01-17T14:35:25.020240"}
{"image_index": 74, "image_name": "000000007511.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007511.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3980.514, "latencies_ms": [3980.514], "images_per_second": 0.251, "prompt_tokens": 21, "response_tokens_est": 68, "n_tiles": 12, "output_text": "The image depicts a sunny beach scene with a man standing on the sand, holding a kite in his hands. The beach is populated with people enjoying the day, with some walking along the shore and others relaxing on the sand. The sky is clear with a few scattered clouds, and the overall atmosphere is relaxed and leisurely.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 24837.4, "ram_available_mb": 100934.7, "ram_percent": 19.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 24837.9, "ram_available_mb": 100934.3, "ram_percent": 19.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.17, "peak": 16.56, "min": 14.12}, "VDD_CPU_SOC_MSS": {"avg": 15.61, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 33.22, "peak": 45.68, "min": 25.6}, "VIN": {"avg": 73.86, "peak": 109.6, "min": 60.79}}, "power_watts_avg": 33.22, "energy_joules_est": 132.24, "sample_count": 31, "duration_seconds": 3.981}, "timestamp": "2026-01-17T14:35:29.007522"}
{"image_index": 74, "image_name": "000000007511.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007511.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2958.144, "latencies_ms": [2958.144], "images_per_second": 0.338, "prompt_tokens": 19, "response_tokens_est": 38, "n_tiles": 12, "output_text": "The image depicts a sunny beach scene with a clear blue sky and scattered clouds. The sandy beach is populated with people, some walking and others sitting, enjoying the pleasant weather.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24837.9, "ram_available_mb": 100934.3, "ram_percent": 19.7}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 24838.2, "ram_available_mb": 100934.0, "ram_percent": 19.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.2, "peak": 16.56, "min": 14.12}, "VDD_CPU_SOC_MSS": {"avg": 15.73, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 35.61, "peak": 45.3, "min": 26.4}, "VIN": {"avg": 77.3, "peak": 115.01, "min": 59.42}}, "power_watts_avg": 35.61, "energy_joules_est": 105.36, "sample_count": 23, "duration_seconds": 2.959}, "timestamp": "2026-01-17T14:35:31.972366"}
{"image_index": 75, "image_name": "000000007574.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007574.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2904.569, "latencies_ms": [2904.569], "images_per_second": 0.344, "prompt_tokens": 9, "response_tokens_est": 38, "n_tiles": 12, "output_text": "The image depicts a cluttered kitchen counter with various items scattered around, including a green bottle, a red gift, a blue bottle, a yellow notebook, and a silver knife.", "error": null, "sys_before": {"cpu_percent": 40.7, "ram_used_mb": 24832.6, "ram_available_mb": 100939.6, "ram_percent": 19.7}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 24833.5, "ram_available_mb": 100938.6, "ram_percent": 19.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.1, "peak": 16.66, "min": 13.29}, "VDD_CPU_SOC_MSS": {"avg": 15.8, "peak": 17.71, "min": 14.17}, "VDD_GPU": {"avg": 34.97, "peak": 44.12, "min": 26.4}, "VIN": {"avg": 81.17, "peak": 121.2, "min": 59.15}}, "power_watts_avg": 34.97, "energy_joules_est": 101.58, "sample_count": 22, "duration_seconds": 2.905}, "timestamp": "2026-01-17T14:35:34.989860"}
{"image_index": 75, "image_name": "000000007574.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007574.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 6033.973, "latencies_ms": [6033.973], "images_per_second": 0.166, "prompt_tokens": 23, "response_tokens_est": 129, "n_tiles": 12, "output_text": "- sink: 1\n- cup: 1\n- plate: 1\n- knife: 1\n- cutting board: 1\n- knife block: 1\n- cutting board: 1\n- cutting board: 1\n- cutting board: 1\n- cutting board: 1\n- cutting board: 1\n- cutting board: 1\n- cutting board: 1\n- cutting board: 1\n- cutting board: 1\n- cutting board: 1\n- cutting board: 1\n- cutting board: 1\n- cutting board: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24833.5, "ram_available_mb": 100938.6, "ram_percent": 19.7}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 24834.5, "ram_available_mb": 100937.7, "ram_percent": 19.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.12, "peak": 16.56, "min": 14.12}, "VDD_CPU_SOC_MSS": {"avg": 15.6, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 30.62, "peak": 45.3, "min": 25.6}, "VIN": {"avg": 74.19, "peak": 129.32, "min": 59.99}}, "power_watts_avg": 30.62, "energy_joules_est": 184.77, "sample_count": 47, "duration_seconds": 6.034}, "timestamp": "2026-01-17T14:35:41.031026"}
{"image_index": 75, "image_name": "000000007574.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007574.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 5113.451, "latencies_ms": [5113.451], "images_per_second": 0.196, "prompt_tokens": 27, "response_tokens_est": 101, "n_tiles": 12, "output_text": "The main objects in the image are located in the kitchen, with the sink and countertop in the foreground, and various kitchen items and appliances in the background. The sink is positioned near the countertop, with a green bottle and a red gift placed on the countertop. The kitchen cabinets are on the left side of the image, and the appliances, such as the refrigerator and microwave, are on the right side. The tiled backsplash and the wall tiles are also visible in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24834.5, "ram_available_mb": 100937.7, "ram_percent": 19.7}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 24834.5, "ram_available_mb": 100937.7, "ram_percent": 19.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.14, "peak": 16.56, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.69, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 31.41, "peak": 44.49, "min": 25.6}, "VIN": {"avg": 75.03, "peak": 138.11, "min": 64.59}}, "power_watts_avg": 31.41, "energy_joules_est": 160.62, "sample_count": 40, "duration_seconds": 5.114}, "timestamp": "2026-01-17T14:35:46.154633"}
{"image_index": 75, "image_name": "000000007574.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007574.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3807.304, "latencies_ms": [3807.304], "images_per_second": 0.263, "prompt_tokens": 21, "response_tokens_est": 63, "n_tiles": 12, "output_text": "The image depicts a cluttered kitchen counter with various items scattered around. The counter is cluttered with kitchen utensils, containers, and a green bottle, while the surrounding cabinets and countertops are visible. The setting appears to be a kitchen with a mix of modern and somewhat dated appliances and decor.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24834.5, "ram_available_mb": 100937.7, "ram_percent": 19.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 24834.8, "ram_available_mb": 100937.4, "ram_percent": 19.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.15, "peak": 16.56, "min": 14.12}, "VDD_CPU_SOC_MSS": {"avg": 15.68, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 33.3, "peak": 45.28, "min": 25.6}, "VIN": {"avg": 74.03, "peak": 112.96, "min": 61.03}}, "power_watts_avg": 33.3, "energy_joules_est": 126.81, "sample_count": 30, "duration_seconds": 3.808}, "timestamp": "2026-01-17T14:35:49.968547"}
{"image_index": 75, "image_name": "000000007574.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007574.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2720.619, "latencies_ms": [2720.619], "images_per_second": 0.368, "prompt_tokens": 19, "response_tokens_est": 31, "n_tiles": 12, "output_text": "The kitchen is well-lit with natural light streaming in from a window. The cabinets are made of light wood, and the countertops are black.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24835.0, "ram_available_mb": 100937.2, "ram_percent": 19.7}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 24836.2, "ram_available_mb": 100935.9, "ram_percent": 19.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.23, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.78, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 36.35, "peak": 44.91, "min": 27.19}, "VIN": {"avg": 82.02, "peak": 128.85, "min": 61.98}}, "power_watts_avg": 36.35, "energy_joules_est": 98.91, "sample_count": 21, "duration_seconds": 2.721}, "timestamp": "2026-01-17T14:35:52.695479"}
{"image_index": 76, "image_name": "000000007784.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007784.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2288.978, "latencies_ms": [2288.978], "images_per_second": 0.437, "prompt_tokens": 9, "response_tokens_est": 20, "n_tiles": 12, "output_text": "A kite with a red and white color scheme is flying high in the clear blue sky.", "error": null, "sys_before": {"cpu_percent": 40.6, "ram_used_mb": 24831.8, "ram_available_mb": 100940.4, "ram_percent": 19.7}, "sys_after": {"cpu_percent": 2.4, "ram_used_mb": 24832.5, "ram_available_mb": 100939.6, "ram_percent": 19.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.17, "peak": 16.66, "min": 13.41}, "VDD_CPU_SOC_MSS": {"avg": 15.98, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 37.26, "peak": 44.12, "min": 29.16}, "VIN": {"avg": 80.82, "peak": 116.97, "min": 59.35}}, "power_watts_avg": 37.26, "energy_joules_est": 85.3, "sample_count": 17, "duration_seconds": 2.289}, "timestamp": "2026-01-17T14:35:55.089877"}
{"image_index": 76, "image_name": "000000007784.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007784.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 3029.37, "latencies_ms": [3029.37], "images_per_second": 0.33, "prompt_tokens": 23, "response_tokens_est": 40, "n_tiles": 12, "output_text": "object: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24832.5, "ram_available_mb": 100939.6, "ram_percent": 19.7}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 24833.5, "ram_available_mb": 100938.6, "ram_percent": 19.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.22, "peak": 16.56, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.85, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 35.57, "peak": 44.91, "min": 26.39}, "VIN": {"avg": 75.2, "peak": 100.13, "min": 62.36}}, "power_watts_avg": 35.57, "energy_joules_est": 107.77, "sample_count": 23, "duration_seconds": 3.03}, "timestamp": "2026-01-17T14:35:58.125962"}
{"image_index": 76, "image_name": "000000007784.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007784.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3916.474, "latencies_ms": [3916.474], "images_per_second": 0.255, "prompt_tokens": 27, "response_tokens_est": 65, "n_tiles": 12, "output_text": "The main object in the image is a kite, which is prominently positioned in the foreground. The kite is flying high in the sky, with its tail extending towards the right side of the image. The background features a clear blue sky, providing a contrasting backdrop to the vibrant colors of the kite.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24833.5, "ram_available_mb": 100938.6, "ram_percent": 19.7}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 24831.3, "ram_available_mb": 100940.9, "ram_percent": 19.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.04, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.54, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 33.15, "peak": 44.89, "min": 25.61}, "VIN": {"avg": 78.17, "peak": 135.78, "min": 56.26}}, "power_watts_avg": 33.15, "energy_joules_est": 129.85, "sample_count": 30, "duration_seconds": 3.917}, "timestamp": "2026-01-17T14:36:02.048616"}
{"image_index": 76, "image_name": "000000007784.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007784.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 4383.599, "latencies_ms": [4383.599], "images_per_second": 0.228, "prompt_tokens": 21, "response_tokens_est": 79, "n_tiles": 12, "output_text": "The image depicts a vibrant scene of a kite flying in the clear blue sky. The kite, with its striking red and white color scheme, is captured in mid-flight, with its tail and tail fin visible against the backdrop of the sky. The scene suggests a sunny day, with the kite's design and the clear blue sky indicating a pleasant and enjoyable outdoor activity.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24831.3, "ram_available_mb": 100940.9, "ram_percent": 19.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 24831.5, "ram_available_mb": 100940.7, "ram_percent": 19.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.85, "peak": 16.36, "min": 13.92}, "VDD_CPU_SOC_MSS": {"avg": 15.28, "peak": 17.32, "min": 14.17}, "VDD_GPU": {"avg": 32.1, "peak": 44.91, "min": 25.22}, "VIN": {"avg": 72.95, "peak": 116.81, "min": 63.27}}, "power_watts_avg": 32.1, "energy_joules_est": 140.72, "sample_count": 34, "duration_seconds": 4.384}, "timestamp": "2026-01-17T14:36:06.438667"}
{"image_index": 76, "image_name": "000000007784.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007784.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2926.83, "latencies_ms": [2926.83], "images_per_second": 0.342, "prompt_tokens": 19, "response_tokens_est": 37, "n_tiles": 12, "output_text": "The kite in the image is predominantly white with a striking red and black design. It is flying high against a clear blue sky, indicating a sunny day with good weather conditions.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24831.5, "ram_available_mb": 100940.7, "ram_percent": 19.7}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 24832.5, "ram_available_mb": 100939.7, "ram_percent": 19.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.94, "peak": 16.26, "min": 13.92}, "VDD_CPU_SOC_MSS": {"avg": 15.6, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 35.99, "peak": 44.91, "min": 26.39}, "VIN": {"avg": 77.4, "peak": 117.79, "min": 58.07}}, "power_watts_avg": 35.99, "energy_joules_est": 105.36, "sample_count": 22, "duration_seconds": 2.927}, "timestamp": "2026-01-17T14:36:09.372506"}
{"image_index": 77, "image_name": "000000007795.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007795.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1903.281, "latencies_ms": [1903.281], "images_per_second": 0.525, "prompt_tokens": 9, "response_tokens_est": 48, "n_tiles": 6, "output_text": "The image shows a well-lit, modern bedroom with a large window that offers a view of trees outside, a bed with white and black pillows, a nightstand with a lamp, and a door that leads to another room.", "error": null, "sys_before": {"cpu_percent": 41.5, "ram_used_mb": 24862.3, "ram_available_mb": 100909.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 24862.8, "ram_available_mb": 100909.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.69, "peak": 15.11, "min": 13.51}, "VDD_CPU_SOC_MSS": {"avg": 15.35, "peak": 15.76, "min": 14.18}, "VDD_GPU": {"avg": 31.26, "peak": 40.57, "min": 24.03}, "VIN": {"avg": 73.46, "peak": 112.78, "min": 57.03}}, "power_watts_avg": 31.26, "energy_joules_est": 59.51, "sample_count": 14, "duration_seconds": 1.904}, "timestamp": "2026-01-17T14:36:11.344751"}
{"image_index": 77, "image_name": "000000007795.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007795.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1893.542, "latencies_ms": [1893.542], "images_per_second": 0.528, "prompt_tokens": 23, "response_tokens_est": 46, "n_tiles": 6, "output_text": "bed: 2\npillows: 4\npillow covers: 2\nnightstand: 1\ntable lamp: 1\nwindow: 1\ndoor: 1\nwall art: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24862.8, "ram_available_mb": 100909.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 24863.6, "ram_available_mb": 100908.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.0, "peak": 15.44, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.61, "peak": 16.14, "min": 14.57}, "VDD_GPU": {"avg": 31.6, "peak": 40.18, "min": 24.43}, "VIN": {"avg": 75.31, "peak": 107.31, "min": 62.94}}, "power_watts_avg": 31.6, "energy_joules_est": 59.85, "sample_count": 14, "duration_seconds": 1.894}, "timestamp": "2026-01-17T14:36:13.244466"}
{"image_index": 77, "image_name": "000000007795.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007795.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2595.633, "latencies_ms": [2595.633], "images_per_second": 0.385, "prompt_tokens": 27, "response_tokens_est": 72, "n_tiles": 6, "output_text": "The main objects in the image are a bed and a nightstand. The bed is positioned in the foreground, with a dark-colored headboard and a white bedspread. The nightstand is located to the left of the bed, holding a lamp with a white shade. The background features a window with blinds, allowing natural light to enter the room.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24863.6, "ram_available_mb": 100908.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 24863.6, "ram_available_mb": 100908.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.07, "peak": 15.44, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.77, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 29.29, "peak": 40.18, "min": 23.64}, "VIN": {"avg": 67.98, "peak": 101.83, "min": 53.12}}, "power_watts_avg": 29.29, "energy_joules_est": 76.04, "sample_count": 20, "duration_seconds": 2.596}, "timestamp": "2026-01-17T14:36:15.846197"}
{"image_index": 77, "image_name": "000000007795.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007795.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1915.689, "latencies_ms": [1915.689], "images_per_second": 0.522, "prompt_tokens": 21, "response_tokens_est": 47, "n_tiles": 6, "output_text": "The image depicts a well-lit, modern bedroom with a large window that offers a view of trees outside. The room is furnished with a bed, a nightstand, and a lamp, creating a cozy and inviting atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24863.6, "ram_available_mb": 100908.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 24879.6, "ram_available_mb": 100892.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.03, "peak": 15.44, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.7, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 31.01, "peak": 40.16, "min": 24.04}, "VIN": {"avg": 69.88, "peak": 88.07, "min": 62.99}}, "power_watts_avg": 31.01, "energy_joules_est": 59.43, "sample_count": 15, "duration_seconds": 1.916}, "timestamp": "2026-01-17T14:36:17.768314"}
{"image_index": 77, "image_name": "000000007795.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007795.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1860.946, "latencies_ms": [1860.946], "images_per_second": 0.537, "prompt_tokens": 19, "response_tokens_est": 45, "n_tiles": 6, "output_text": "The room features a warm, yellowish-green wall color, complemented by a wooden ceiling with a metal grid. The lighting is soft and ambient, with a warm glow from a bedside lamp casting a cozy atmosphere.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24879.6, "ram_available_mb": 100892.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 24879.3, "ram_available_mb": 100892.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.02, "peak": 15.44, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.64, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 31.68, "peak": 40.16, "min": 24.42}, "VIN": {"avg": 76.13, "peak": 112.19, "min": 55.49}}, "power_watts_avg": 31.68, "energy_joules_est": 58.96, "sample_count": 14, "duration_seconds": 1.861}, "timestamp": "2026-01-17T14:36:19.635408"}
{"image_index": 78, "image_name": "000000007816.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007816.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1710.315, "latencies_ms": [1710.315], "images_per_second": 0.585, "prompt_tokens": 9, "response_tokens_est": 41, "n_tiles": 6, "output_text": "A motorcyclist wearing a white helmet and a white and green jacket is riding a white motorcycle with black accents, while a group of people stands behind a fence on the side of a road.", "error": null, "sys_before": {"cpu_percent": 37.7, "ram_used_mb": 24879.3, "ram_available_mb": 100892.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 24879.3, "ram_available_mb": 100892.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.76, "peak": 15.11, "min": 13.9}, "VDD_CPU_SOC_MSS": {"avg": 15.54, "peak": 16.14, "min": 14.57}, "VDD_GPU": {"avg": 31.73, "peak": 40.57, "min": 24.42}, "VIN": {"avg": 78.42, "peak": 117.68, "min": 56.78}}, "power_watts_avg": 31.73, "energy_joules_est": 54.29, "sample_count": 13, "duration_seconds": 1.711}, "timestamp": "2026-01-17T14:36:21.402466"}
{"image_index": 78, "image_name": "000000007816.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007816.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1674.371, "latencies_ms": [1674.371], "images_per_second": 0.597, "prompt_tokens": 23, "response_tokens_est": 38, "n_tiles": 6, "output_text": "1. Motorcycle\n2. Rider\n3. Helmet\n4. Gloves\n5. Motorcycle\n6. Rider\n7. Helmet\n8. Gloves", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24879.3, "ram_available_mb": 100892.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 24880.3, "ram_available_mb": 100891.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.99, "peak": 15.44, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.58, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 32.99, "peak": 40.18, "min": 25.21}, "VIN": {"avg": 73.54, "peak": 108.13, "min": 62.66}}, "power_watts_avg": 32.99, "energy_joules_est": 55.25, "sample_count": 12, "duration_seconds": 1.675}, "timestamp": "2026-01-17T14:36:23.083115"}
{"image_index": 78, "image_name": "000000007816.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007816.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2377.196, "latencies_ms": [2377.196], "images_per_second": 0.421, "prompt_tokens": 27, "response_tokens_est": 64, "n_tiles": 6, "output_text": "The main object in the foreground is a white motorcycle with black accents, positioned on the right side of the road. The background features a group of people standing behind a wooden fence, with a person lying on the grass to the left. The road curves to the left, and the fence runs parallel to the road.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24880.3, "ram_available_mb": 100891.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 24880.0, "ram_available_mb": 100892.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.07, "peak": 15.44, "min": 14.43}, "VDD_CPU_SOC_MSS": {"avg": 15.64, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 30.16, "peak": 41.34, "min": 23.64}, "VIN": {"avg": 70.43, "peak": 107.04, "min": 56.85}}, "power_watts_avg": 30.16, "energy_joules_est": 71.71, "sample_count": 18, "duration_seconds": 2.378}, "timestamp": "2026-01-17T14:36:25.466552"}
{"image_index": 78, "image_name": "000000007816.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007816.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1997.594, "latencies_ms": [1997.594], "images_per_second": 0.501, "prompt_tokens": 21, "response_tokens_est": 50, "n_tiles": 6, "output_text": "The image captures a dynamic scene of a motorcyclist in full gear, including a helmet, riding a white and green motorcycle on a road. The background features a grassy area with a fence and a few people standing or sitting nearby.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24880.0, "ram_available_mb": 100892.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 24880.0, "ram_available_mb": 100892.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.02, "peak": 15.34, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.64, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 31.1, "peak": 40.57, "min": 24.04}, "VIN": {"avg": 72.22, "peak": 104.69, "min": 59.8}}, "power_watts_avg": 31.1, "energy_joules_est": 62.14, "sample_count": 15, "duration_seconds": 1.998}, "timestamp": "2026-01-17T14:36:27.470647"}
{"image_index": 78, "image_name": "000000007816.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007816.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1807.08, "latencies_ms": [1807.08], "images_per_second": 0.553, "prompt_tokens": 19, "response_tokens_est": 43, "n_tiles": 6, "output_text": "The motorcycle in the image is predominantly white with black accents, featuring a sleek design and a visible logo on the side. The lighting is natural, suggesting daytime, and the weather appears to be clear and sunny.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24880.0, "ram_available_mb": 100892.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 24879.9, "ram_available_mb": 100892.3, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.02, "peak": 15.44, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.58, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 31.68, "peak": 40.57, "min": 24.43}, "VIN": {"avg": 72.39, "peak": 104.2, "min": 61.77}}, "power_watts_avg": 31.68, "energy_joules_est": 57.26, "sample_count": 14, "duration_seconds": 1.807}, "timestamp": "2026-01-17T14:36:29.284432"}
{"image_index": 79, "image_name": "000000007818.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007818.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1660.11, "latencies_ms": [1660.11], "images_per_second": 0.602, "prompt_tokens": 9, "response_tokens_est": 39, "n_tiles": 6, "output_text": "The image shows a well-lit dining table with a white tablecloth, elegantly arranged with a clear glass vase containing white flowers, and a pair of silver cutlery.", "error": null, "sys_before": {"cpu_percent": 44.3, "ram_used_mb": 24880.1, "ram_available_mb": 100892.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 24880.6, "ram_available_mb": 100891.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.69, "peak": 15.11, "min": 13.79}, "VDD_CPU_SOC_MSS": {"avg": 15.45, "peak": 16.14, "min": 14.57}, "VDD_GPU": {"avg": 32.3, "peak": 40.18, "min": 25.22}, "VIN": {"avg": 75.38, "peak": 112.96, "min": 60.26}}, "power_watts_avg": 32.3, "energy_joules_est": 53.64, "sample_count": 12, "duration_seconds": 1.661}, "timestamp": "2026-01-17T14:36:31.003574"}
{"image_index": 79, "image_name": "000000007818.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007818.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1762.374, "latencies_ms": [1762.374], "images_per_second": 0.567, "prompt_tokens": 23, "response_tokens_est": 41, "n_tiles": 6, "output_text": "table: 1\ntablecloth: 1\nglass: 1\nvase: 1\nflowers: 1\ntableware: 1\nnapkin: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24880.6, "ram_available_mb": 100891.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 24880.3, "ram_available_mb": 100891.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.98, "peak": 15.44, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.56, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 32.24, "peak": 40.56, "min": 24.83}, "VIN": {"avg": 73.69, "peak": 109.63, "min": 52.86}}, "power_watts_avg": 32.24, "energy_joules_est": 56.83, "sample_count": 13, "duration_seconds": 1.763}, "timestamp": "2026-01-17T14:36:32.772231"}
{"image_index": 79, "image_name": "000000007818.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007818.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3168.155, "latencies_ms": [3168.155], "images_per_second": 0.316, "prompt_tokens": 27, "response_tokens_est": 89, "n_tiles": 6, "output_text": "The main objects in the image are a table and a vase with flowers. The table is in the foreground, with a white tablecloth covering it. The vase with the flowers is placed near the center of the table, slightly to the left. In the background, there are other tables and chairs, suggesting a dining area. The vase is the most prominent object in the foreground, while the background is slightly blurred.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24880.3, "ram_available_mb": 100891.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 24881.1, "ram_available_mb": 100891.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.89, "peak": 15.34, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.49, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 27.96, "peak": 40.95, "min": 22.86}, "VIN": {"avg": 68.82, "peak": 105.64, "min": 50.61}}, "power_watts_avg": 27.96, "energy_joules_est": 88.59, "sample_count": 24, "duration_seconds": 3.169}, "timestamp": "2026-01-17T14:36:35.946827"}
{"image_index": 79, "image_name": "000000007818.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007818.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2675.482, "latencies_ms": [2675.482], "images_per_second": 0.374, "prompt_tokens": 21, "response_tokens_est": 72, "n_tiles": 6, "output_text": "The image depicts a dimly lit dining area with a table set for a meal. The table is adorned with white tablecloths, and there are several clear glassware pieces, including wine glasses and a vase with white flowers. The setting suggests a formal or semi-formal dining experience, possibly in a restaurant or a private event space.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24881.1, "ram_available_mb": 100891.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 24881.5, "ram_available_mb": 100890.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.84, "peak": 15.24, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.43, "peak": 15.75, "min": 14.56}, "VDD_GPU": {"avg": 28.66, "peak": 40.18, "min": 22.86}, "VIN": {"avg": 72.0, "peak": 113.8, "min": 60.74}}, "power_watts_avg": 28.66, "energy_joules_est": 76.69, "sample_count": 20, "duration_seconds": 2.676}, "timestamp": "2026-01-17T14:36:38.628360"}
{"image_index": 79, "image_name": "000000007818.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007818.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2053.963, "latencies_ms": [2053.963], "images_per_second": 0.487, "prompt_tokens": 19, "response_tokens_est": 50, "n_tiles": 6, "output_text": "The image depicts a dimly lit dining table with a white tablecloth, illuminated by soft, warm lighting. The table is adorned with elegant glassware, including a clear vase with white flowers and a smaller glass bowl.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24881.5, "ram_available_mb": 100890.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 24881.8, "ram_available_mb": 100890.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.81, "peak": 15.24, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.35, "peak": 15.75, "min": 14.56}, "VDD_GPU": {"avg": 30.7, "peak": 40.18, "min": 23.64}, "VIN": {"avg": 67.49, "peak": 94.31, "min": 57.84}}, "power_watts_avg": 30.7, "energy_joules_est": 63.07, "sample_count": 15, "duration_seconds": 2.054}, "timestamp": "2026-01-17T14:36:40.688698"}
{"image_index": 80, "image_name": "000000007888.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007888.jpg", "image_width": 638, "image_height": 640, "image_resolution": "638x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1673.14, "latencies_ms": [1673.14], "images_per_second": 0.598, "prompt_tokens": 9, "response_tokens_est": 52, "n_tiles": 4, "output_text": "The image features a black and white photograph of a classic street clock mounted on a pole, with its face displaying the time at approximately 10:10. The clock is set against a blurred background of a grassy field, suggesting an outdoor setting.", "error": null, "sys_before": {"cpu_percent": 40.3, "ram_used_mb": 24881.5, "ram_available_mb": 100890.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 24882.8, "ram_available_mb": 100889.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4606.7, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5444.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.65, "peak": 15.11, "min": 13.9}, "VDD_CPU_SOC_MSS": {"avg": 15.48, "peak": 16.14, "min": 14.57}, "VDD_GPU": {"avg": 28.04, "peak": 36.24, "min": 22.86}, "VIN": {"avg": 67.15, "peak": 99.84, "min": 57.61}}, "power_watts_avg": 28.04, "energy_joules_est": 46.93, "sample_count": 12, "duration_seconds": 1.674}, "timestamp": "2026-01-17T14:36:42.408023"}
{"image_index": 80, "image_name": "000000007888.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007888.jpg", "image_width": 638, "image_height": 640, "image_resolution": "638x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1226.906, "latencies_ms": [1226.906], "images_per_second": 0.815, "prompt_tokens": 23, "response_tokens_est": 34, "n_tiles": 4, "output_text": "1. Clock\n2. Street light\n3. Sky\n4. Grass\n5. Person\n6. Bird\n7. Tree\n8. Street sign", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24882.8, "ram_available_mb": 100889.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 24882.5, "ram_available_mb": 100889.7, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4606.7, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5455.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.81, "peak": 15.11, "min": 14.51}, "VDD_CPU_SOC_MSS": {"avg": 15.57, "peak": 15.75, "min": 15.35}, "VDD_GPU": {"avg": 30.29, "peak": 37.42, "min": 24.43}, "VIN": {"avg": 71.06, "peak": 110.51, "min": 55.54}}, "power_watts_avg": 30.29, "energy_joules_est": 37.17, "sample_count": 9, "duration_seconds": 1.227}, "timestamp": "2026-01-17T14:36:43.640914"}
{"image_index": 80, "image_name": "000000007888.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007888.jpg", "image_width": 638, "image_height": 640, "image_resolution": "638x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1768.605, "latencies_ms": [1768.605], "images_per_second": 0.565, "prompt_tokens": 27, "response_tokens_est": 56, "n_tiles": 4, "output_text": "The main object in the image is a clock, which is positioned in the foreground. The clock is mounted on a pole, and its face is visible, showing the time. The background features a blurred natural landscape, suggesting that the clock is placed in an outdoor setting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24882.5, "ram_available_mb": 100889.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 24883.5, "ram_available_mb": 100888.7, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4606.7, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5458.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.96, "peak": 15.22, "min": 14.53}, "VDD_CPU_SOC_MSS": {"avg": 15.75, "peak": 16.14, "min": 15.35}, "VDD_GPU": {"avg": 28.55, "peak": 38.19, "min": 22.86}, "VIN": {"avg": 66.68, "peak": 85.1, "min": 59.34}}, "power_watts_avg": 28.55, "energy_joules_est": 50.51, "sample_count": 13, "duration_seconds": 1.769}, "timestamp": "2026-01-17T14:36:45.415570"}
{"image_index": 80, "image_name": "000000007888.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007888.jpg", "image_width": 638, "image_height": 640, "image_resolution": "638x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1871.72, "latencies_ms": [1871.72], "images_per_second": 0.534, "prompt_tokens": 21, "response_tokens_est": 60, "n_tiles": 4, "output_text": "The image depicts a black and white photograph of a classic street clock mounted on a pole. The clock face is visible, showing the time as approximately 10:10. The background is blurred, but it appears to be an outdoor setting with a grassy field and a clear sky.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24883.5, "ram_available_mb": 100888.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 24883.2, "ram_available_mb": 100888.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4606.7, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5453.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.97, "peak": 15.32, "min": 14.53}, "VDD_CPU_SOC_MSS": {"avg": 15.83, "peak": 16.14, "min": 15.35}, "VDD_GPU": {"avg": 27.57, "peak": 37.03, "min": 22.45}, "VIN": {"avg": 69.84, "peak": 105.65, "min": 56.0}}, "power_watts_avg": 27.57, "energy_joules_est": 51.62, "sample_count": 14, "duration_seconds": 1.872}, "timestamp": "2026-01-17T14:36:47.294467"}
{"image_index": 80, "image_name": "000000007888.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007888.jpg", "image_width": 638, "image_height": 640, "image_resolution": "638x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2189.401, "latencies_ms": [2189.401], "images_per_second": 0.457, "prompt_tokens": 19, "response_tokens_est": 73, "n_tiles": 4, "output_text": "The image features a black and white photograph of a classic street clock, which is prominently displayed against a blurred background of a grassy field. The clock has a traditional design with Roman numerals and a decorative border, and it is mounted on a pole. The lighting in the photograph is soft and diffused, creating a nostalgic and timeless atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24883.2, "ram_available_mb": 100888.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 24883.2, "ram_available_mb": 100888.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4606.7, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5451.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.06, "peak": 15.32, "min": 14.61}, "VDD_CPU_SOC_MSS": {"avg": 15.84, "peak": 16.14, "min": 15.35}, "VDD_GPU": {"avg": 26.81, "peak": 37.42, "min": 22.46}, "VIN": {"avg": 67.7, "peak": 88.73, "min": 59.05}}, "power_watts_avg": 26.81, "energy_joules_est": 58.71, "sample_count": 17, "duration_seconds": 2.19}, "timestamp": "2026-01-17T14:36:49.489784"}
{"image_index": 81, "image_name": "000000007977.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007977.jpg", "image_width": 429, "image_height": 640, "image_resolution": "429x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1304.367, "latencies_ms": [1304.367], "images_per_second": 0.767, "prompt_tokens": 9, "response_tokens_est": 26, "n_tiles": 6, "output_text": "A young man is performing a trick on a skateboard in an outdoor setting with trees and a clear sky in the background.", "error": null, "sys_before": {"cpu_percent": 43.6, "ram_used_mb": 24883.2, "ram_available_mb": 100888.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 24883.2, "ram_available_mb": 100888.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.67, "peak": 15.14, "min": 13.9}, "VDD_CPU_SOC_MSS": {"avg": 15.43, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 33.21, "peak": 39.38, "min": 26.79}, "VIN": {"avg": 77.86, "peak": 113.16, "min": 56.15}}, "power_watts_avg": 33.21, "energy_joules_est": 43.33, "sample_count": 10, "duration_seconds": 1.305}, "timestamp": "2026-01-17T14:36:50.853602"}
{"image_index": 81, "image_name": "000000007977.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007977.jpg", "image_width": 429, "image_height": 640, "image_resolution": "429x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1962.179, "latencies_ms": [1962.179], "images_per_second": 0.51, "prompt_tokens": 23, "response_tokens_est": 47, "n_tiles": 6, "output_text": "1. Skateboard\n2. Person\n3. Skateboarder\n4. Skateboard\n5. Skateboard\n6. Skateboard\n7. Skateboard\n8. Skateboard", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24883.2, "ram_available_mb": 100888.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 24883.0, "ram_available_mb": 100889.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.83, "peak": 15.24, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.48, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 31.41, "peak": 40.97, "min": 23.64}, "VIN": {"avg": 73.22, "peak": 111.15, "min": 61.65}}, "power_watts_avg": 31.41, "energy_joules_est": 61.65, "sample_count": 15, "duration_seconds": 1.963}, "timestamp": "2026-01-17T14:36:52.822075"}
{"image_index": 81, "image_name": "000000007977.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007977.jpg", "image_width": 429, "image_height": 640, "image_resolution": "429x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2789.323, "latencies_ms": [2789.323], "images_per_second": 0.359, "prompt_tokens": 27, "response_tokens_est": 76, "n_tiles": 6, "output_text": "The main object in the foreground is a young man skateboarding. He is positioned slightly to the left and in the foreground of the image. The background features a tree and a few other people, indicating that the skateboarder is in an outdoor setting. The skateboard itself is located near the bottom center of the image, slightly to the left of the skateboarder.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24883.0, "ram_available_mb": 100889.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 24883.2, "ram_available_mb": 100889.0, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.86, "peak": 15.24, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.32, "peak": 15.74, "min": 14.57}, "VDD_GPU": {"avg": 28.63, "peak": 40.56, "min": 22.86}, "VIN": {"avg": 70.12, "peak": 113.89, "min": 61.53}}, "power_watts_avg": 28.63, "energy_joules_est": 79.87, "sample_count": 21, "duration_seconds": 2.79}, "timestamp": "2026-01-17T14:36:55.617545"}
{"image_index": 81, "image_name": "000000007977.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007977.jpg", "image_width": 429, "image_height": 640, "image_resolution": "429x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2614.89, "latencies_ms": [2614.89], "images_per_second": 0.382, "prompt_tokens": 21, "response_tokens_est": 70, "n_tiles": 6, "output_text": "The image depicts a young man skateboarding in an outdoor urban setting. He is captured mid-action, performing a trick on a skateboard. The skateboarder is wearing a dark-colored t-shirt, light-colored pants, and sneakers, and is positioned on a concrete surface with a tree and a few other people visible in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24883.2, "ram_available_mb": 100889.0, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 24884.7, "ram_available_mb": 100887.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.84, "peak": 15.24, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.4, "peak": 15.75, "min": 14.57}, "VDD_GPU": {"avg": 28.99, "peak": 40.18, "min": 22.86}, "VIN": {"avg": 73.68, "peak": 113.31, "min": 62.52}}, "power_watts_avg": 28.99, "energy_joules_est": 75.82, "sample_count": 19, "duration_seconds": 2.615}, "timestamp": "2026-01-17T14:36:58.239154"}
{"image_index": 81, "image_name": "000000007977.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007977.jpg", "image_width": 429, "image_height": 640, "image_resolution": "429x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2668.272, "latencies_ms": [2668.272], "images_per_second": 0.375, "prompt_tokens": 19, "response_tokens_est": 72, "n_tiles": 6, "output_text": "The image depicts a young man skateboarding in an outdoor setting. The skateboarder is wearing a dark-colored t-shirt, light-colored pants, and a baseball cap. The scene is well-lit, with natural daylight illuminating the surroundings. The ground appears to be made of concrete, and there are trees and a clear sky in the background.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 24884.7, "ram_available_mb": 100887.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 24884.9, "ram_available_mb": 100887.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.85, "peak": 15.24, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.45, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 28.74, "peak": 40.16, "min": 22.86}, "VIN": {"avg": 69.5, "peak": 92.97, "min": 58.18}}, "power_watts_avg": 28.74, "energy_joules_est": 76.69, "sample_count": 20, "duration_seconds": 2.669}, "timestamp": "2026-01-17T14:37:00.913818"}
{"image_index": 82, "image_name": "000000007991.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007991.jpg", "image_width": 640, "image_height": 359, "image_resolution": "640x359", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 802.039, "latencies_ms": [802.039], "images_per_second": 1.247, "prompt_tokens": 9, "response_tokens_est": 28, "n_tiles": 2, "output_text": "The image shows a white plate filled with orange carrots, green peas, and purple beets, all arranged on a kitchen countertop.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 24884.9, "ram_available_mb": 100887.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 24884.9, "ram_available_mb": 100887.3, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5055.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.86, "peak": 15.42, "min": 14.1}, "VDD_CPU_SOC_MSS": {"avg": 15.22, "peak": 15.74, "min": 14.96}, "VDD_GPU": {"avg": 26.66, "peak": 31.91, "min": 22.85}, "VIN": {"avg": 65.78, "peak": 78.82, "min": 60.26}}, "power_watts_avg": 26.66, "energy_joules_est": 21.39, "sample_count": 6, "duration_seconds": 0.802}, "timestamp": "2026-01-17T14:37:01.742635"}
{"image_index": 82, "image_name": "000000007991.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007991.jpg", "image_width": 640, "image_height": 359, "image_resolution": "640x359", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 858.669, "latencies_ms": [858.669], "images_per_second": 1.165, "prompt_tokens": 23, "response_tokens_est": 30, "n_tiles": 2, "output_text": "carrot: 8\npea: 1\npotato: 1\nknife: 1\ncutting board: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24884.9, "ram_available_mb": 100887.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 24884.9, "ram_available_mb": 100887.3, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5064.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.0, "peak": 15.11, "min": 14.81}, "VDD_CPU_SOC_MSS": {"avg": 15.75, "peak": 16.15, "min": 15.35}, "VDD_GPU": {"avg": 26.99, "peak": 31.91, "min": 23.24}, "VIN": {"avg": 66.75, "peak": 79.14, "min": 61.68}}, "power_watts_avg": 26.99, "energy_joules_est": 23.19, "sample_count": 6, "duration_seconds": 0.859}, "timestamp": "2026-01-17T14:37:02.607286"}
{"image_index": 82, "image_name": "000000007991.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007991.jpg", "image_width": 640, "image_height": 359, "image_resolution": "640x359", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2127.438, "latencies_ms": [2127.438], "images_per_second": 0.47, "prompt_tokens": 27, "response_tokens_est": 85, "n_tiles": 2, "output_text": "The main objects in the image are a white plate filled with orange carrots, a blue knife, and a bunch of green beans. The plate is placed on a kitchen countertop, with the green beans and the bunch of carrots in the foreground. The knife is positioned to the right of the plate, near the bunch of green beans. The background includes a white container and a white cup, which are out of focus.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 24884.9, "ram_available_mb": 100887.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 24885.2, "ram_available_mb": 100887.0, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5067.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.22, "peak": 15.62, "min": 14.61}, "VDD_CPU_SOC_MSS": {"avg": 16.32, "peak": 16.54, "min": 15.75}, "VDD_GPU": {"avg": 23.42, "peak": 31.91, "min": 20.88}, "VIN": {"avg": 64.39, "peak": 80.28, "min": 53.86}}, "power_watts_avg": 23.42, "energy_joules_est": 49.83, "sample_count": 16, "duration_seconds": 2.128}, "timestamp": "2026-01-17T14:37:04.741141"}
{"image_index": 82, "image_name": "000000007991.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007991.jpg", "image_width": 640, "image_height": 359, "image_resolution": "640x359", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1508.079, "latencies_ms": [1508.079], "images_per_second": 0.663, "prompt_tokens": 21, "response_tokens_est": 58, "n_tiles": 2, "output_text": "The image depicts a kitchen countertop with a white plate containing a mix of cooked carrots and green beans, alongside a bunch of fresh carrots and a bunch of fresh beets. The setting appears to be a kitchen, with a stainless steel sink and a white container in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24885.2, "ram_available_mb": 100887.0, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 24885.2, "ram_available_mb": 100887.0, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5063.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.37, "peak": 15.82, "min": 14.91}, "VDD_CPU_SOC_MSS": {"avg": 15.93, "peak": 16.14, "min": 15.35}, "VDD_GPU": {"avg": 23.96, "peak": 30.33, "min": 20.87}, "VIN": {"avg": 65.72, "peak": 104.35, "min": 56.65}}, "power_watts_avg": 23.96, "energy_joules_est": 36.14, "sample_count": 11, "duration_seconds": 1.508}, "timestamp": "2026-01-17T14:37:06.255253"}
{"image_index": 82, "image_name": "000000007991.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007991.jpg", "image_width": 640, "image_height": 359, "image_resolution": "640x359", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1554.016, "latencies_ms": [1554.016], "images_per_second": 0.643, "prompt_tokens": 19, "response_tokens_est": 60, "n_tiles": 2, "output_text": "The image features a white plate filled with orange carrots, green peas, and purple beets. The carrots are brightly colored, contrasting with the muted tones of the other vegetables and the countertop. The lighting is soft and natural, casting gentle shadows and highlighting the textures of the vegetables.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24885.2, "ram_available_mb": 100887.0, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 24885.4, "ram_available_mb": 100886.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5062.6, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.33, "peak": 15.52, "min": 14.91}, "VDD_CPU_SOC_MSS": {"avg": 15.74, "peak": 16.14, "min": 15.35}, "VDD_GPU": {"avg": 23.96, "peak": 30.33, "min": 20.89}, "VIN": {"avg": 66.47, "peak": 108.08, "min": 55.3}}, "power_watts_avg": 23.96, "energy_joules_est": 37.25, "sample_count": 11, "duration_seconds": 1.555}, "timestamp": "2026-01-17T14:37:07.815509"}
{"image_index": 83, "image_name": "000000008021.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008021.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2531.068, "latencies_ms": [2531.068], "images_per_second": 0.395, "prompt_tokens": 9, "response_tokens_est": 27, "n_tiles": 12, "output_text": "A man in a suit is standing on a stage, speaking to an audience, with a large screen behind him displaying his image.", "error": null, "sys_before": {"cpu_percent": 48.8, "ram_used_mb": 24857.3, "ram_available_mb": 100914.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 24858.0, "ram_available_mb": 100914.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.21, "peak": 16.66, "min": 13.69}, "VDD_CPU_SOC_MSS": {"avg": 15.93, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 35.68, "peak": 43.31, "min": 27.57}, "VIN": {"avg": 83.02, "peak": 131.44, "min": 59.18}}, "power_watts_avg": 35.68, "energy_joules_est": 90.32, "sample_count": 19, "duration_seconds": 2.531}, "timestamp": "2026-01-17T14:37:10.437835"}
{"image_index": 83, "image_name": "000000008021.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008021.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2823.9, "latencies_ms": [2823.9], "images_per_second": 0.354, "prompt_tokens": 23, "response_tokens_est": 34, "n_tiles": 12, "output_text": "1. Man\n2. Suits\n3. Background\n4. Speaker\n5. Table\n6. Water bottles\n7. Table\n8. Speaker", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24858.0, "ram_available_mb": 100914.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 24858.0, "ram_available_mb": 100914.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.23, "peak": 16.56, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.8, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 36.1, "peak": 45.28, "min": 26.79}, "VIN": {"avg": 78.32, "peak": 115.44, "min": 65.27}}, "power_watts_avg": 36.1, "energy_joules_est": 101.96, "sample_count": 22, "duration_seconds": 2.824}, "timestamp": "2026-01-17T14:37:13.268371"}
{"image_index": 83, "image_name": "000000008021.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008021.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3951.424, "latencies_ms": [3951.424], "images_per_second": 0.253, "prompt_tokens": 27, "response_tokens_est": 67, "n_tiles": 12, "output_text": "The main object in the foreground is a person with red hair, who is facing away from the camera. The person is standing in front of a large screen displaying a man in a suit. The screen is positioned behind the person, creating a clear spatial relationship where the person is in the foreground and the screen is in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24858.0, "ram_available_mb": 100914.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 24858.2, "ram_available_mb": 100913.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.17, "peak": 16.56, "min": 14.12}, "VDD_CPU_SOC_MSS": {"avg": 15.79, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 33.33, "peak": 44.91, "min": 25.61}, "VIN": {"avg": 77.25, "peak": 128.2, "min": 54.31}}, "power_watts_avg": 33.33, "energy_joules_est": 131.72, "sample_count": 30, "duration_seconds": 3.952}, "timestamp": "2026-01-17T14:37:17.227630"}
{"image_index": 83, "image_name": "000000008021.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008021.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3227.746, "latencies_ms": [3227.746], "images_per_second": 0.31, "prompt_tokens": 21, "response_tokens_est": 46, "n_tiles": 12, "output_text": "The image depicts a man standing on a stage, delivering a speech or presentation. The background features a large screen displaying a projected image of the same man, suggesting that the presentation is being delivered in front of an audience.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24858.2, "ram_available_mb": 100913.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 24859.0, "ram_available_mb": 100913.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.21, "peak": 16.66, "min": 14.12}, "VDD_CPU_SOC_MSS": {"avg": 15.79, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 34.71, "peak": 44.91, "min": 26.01}, "VIN": {"avg": 74.97, "peak": 105.18, "min": 55.21}}, "power_watts_avg": 34.71, "energy_joules_est": 112.05, "sample_count": 25, "duration_seconds": 3.228}, "timestamp": "2026-01-17T14:37:20.462187"}
{"image_index": 83, "image_name": "000000008021.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008021.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3533.138, "latencies_ms": [3533.138], "images_per_second": 0.283, "prompt_tokens": 19, "response_tokens_est": 55, "n_tiles": 12, "output_text": "The image features a man standing on a stage with a large screen behind him displaying a colorful geometric pattern. The lighting is warm, casting a soft glow on the scene, and the colors are vibrant, with a mix of blues, oranges, and purples.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24859.0, "ram_available_mb": 100913.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 24859.0, "ram_available_mb": 100913.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.17, "peak": 16.56, "min": 14.12}, "VDD_CPU_SOC_MSS": {"avg": 15.69, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 34.07, "peak": 45.68, "min": 26.0}, "VIN": {"avg": 75.9, "peak": 110.5, "min": 56.29}}, "power_watts_avg": 34.07, "energy_joules_est": 120.4, "sample_count": 28, "duration_seconds": 3.534}, "timestamp": "2026-01-17T14:37:24.002112"}
{"image_index": 84, "image_name": "000000008211.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008211.jpg", "image_width": 640, "image_height": 459, "image_resolution": "640x459", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2217.825, "latencies_ms": [2217.825], "images_per_second": 0.451, "prompt_tokens": 9, "response_tokens_est": 18, "n_tiles": 12, "output_text": "Two workers in blue uniforms are conversing near a scooter parked on the sidewalk.", "error": null, "sys_before": {"cpu_percent": 42.9, "ram_used_mb": 24849.5, "ram_available_mb": 100922.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.2, "ram_used_mb": 24850.7, "ram_available_mb": 100921.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.14, "peak": 16.66, "min": 13.39}, "VDD_CPU_SOC_MSS": {"avg": 15.98, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 37.23, "peak": 44.12, "min": 29.15}, "VIN": {"avg": 81.76, "peak": 115.96, "min": 68.04}}, "power_watts_avg": 37.23, "energy_joules_est": 82.58, "sample_count": 17, "duration_seconds": 2.218}, "timestamp": "2026-01-17T14:37:26.328976"}
{"image_index": 84, "image_name": "000000008211.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008211.jpg", "image_width": 640, "image_height": 459, "image_resolution": "640x459", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 3019.58, "latencies_ms": [3019.58], "images_per_second": 0.331, "prompt_tokens": 23, "response_tokens_est": 40, "n_tiles": 12, "output_text": "object: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24850.7, "ram_available_mb": 100921.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 24851.7, "ram_available_mb": 100920.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.26, "peak": 16.66, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.88, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 36.0, "peak": 45.3, "min": 26.4}, "VIN": {"avg": 77.83, "peak": 125.32, "min": 60.04}}, "power_watts_avg": 36.0, "energy_joules_est": 108.72, "sample_count": 23, "duration_seconds": 3.02}, "timestamp": "2026-01-17T14:37:29.354599"}
{"image_index": 84, "image_name": "000000008211.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008211.jpg", "image_width": 640, "image_height": 459, "image_resolution": "640x459", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 4702.849, "latencies_ms": [4702.849], "images_per_second": 0.213, "prompt_tokens": 27, "response_tokens_est": 89, "n_tiles": 12, "output_text": "In the image, there is a scooter parked on the left side of the frame, near the curb. The scooter is facing the right side of the frame. In the background, there is a building with a sign that reads \"\u4e0a\u6d77\u5efa\u5de5\u96c6\u56e2\" (Shanghai Construction Group). The sign is mounted on a pole and is positioned near the building. The ground is paved, and there is a small trash can nearby.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24851.7, "ram_available_mb": 100920.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 24851.9, "ram_available_mb": 100920.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.16, "peak": 16.56, "min": 14.12}, "VDD_CPU_SOC_MSS": {"avg": 15.65, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 32.11, "peak": 44.49, "min": 25.6}, "VIN": {"avg": 74.05, "peak": 112.05, "min": 60.15}}, "power_watts_avg": 32.11, "energy_joules_est": 151.02, "sample_count": 36, "duration_seconds": 4.703}, "timestamp": "2026-01-17T14:37:34.064451"}
{"image_index": 84, "image_name": "000000008211.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008211.jpg", "image_width": 640, "image_height": 459, "image_resolution": "640x459", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 4146.07, "latencies_ms": [4146.07], "images_per_second": 0.241, "prompt_tokens": 21, "response_tokens_est": 73, "n_tiles": 12, "output_text": "The image depicts a scene in an urban setting, likely a street or a parking area, with a concrete sidewalk and a paved road. Two individuals, one in a blue uniform and the other in casual attire, are standing near a scooter. The background features a building with a sign, a blue and white advertisement, and a street lamp.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24851.9, "ram_available_mb": 100920.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 24852.7, "ram_available_mb": 100919.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.17, "peak": 16.56, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.65, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 32.95, "peak": 45.3, "min": 25.6}, "VIN": {"avg": 75.54, "peak": 130.71, "min": 60.41}}, "power_watts_avg": 32.95, "energy_joules_est": 136.63, "sample_count": 32, "duration_seconds": 4.147}, "timestamp": "2026-01-17T14:37:38.221335"}
{"image_index": 84, "image_name": "000000008211.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008211.jpg", "image_width": 640, "image_height": 459, "image_resolution": "640x459", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3366.531, "latencies_ms": [3366.531], "images_per_second": 0.297, "prompt_tokens": 19, "response_tokens_est": 50, "n_tiles": 12, "output_text": "The image depicts a scene with a blue and white sign on a building, featuring Chinese characters. The sign is mounted on a gray pillar. The pavement is wet, indicating recent rain. The lighting is natural, suggesting it is daytime.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24852.7, "ram_available_mb": 100919.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 24852.9, "ram_available_mb": 100919.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.18, "peak": 16.56, "min": 14.12}, "VDD_CPU_SOC_MSS": {"avg": 15.7, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 34.48, "peak": 45.3, "min": 26.0}, "VIN": {"avg": 76.3, "peak": 125.52, "min": 64.46}}, "power_watts_avg": 34.48, "energy_joules_est": 116.09, "sample_count": 26, "duration_seconds": 3.367}, "timestamp": "2026-01-17T14:37:41.594553"}
{"image_index": 85, "image_name": "000000008277.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008277.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 953.433, "latencies_ms": [953.433], "images_per_second": 1.049, "prompt_tokens": 9, "response_tokens_est": 40, "n_tiles": 1, "output_text": "The image shows a plate of food, which includes a black, square-shaped dish containing a green, creamy soup and a portion of shredded chicken, all placed on a white, round plate.", "error": null, "sys_before": {"cpu_percent": 21.2, "ram_used_mb": 24852.9, "ram_available_mb": 100919.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 24853.2, "ram_available_mb": 100919.0, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 4845.7, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.19, "peak": 15.62, "min": 14.51}, "VDD_CPU_SOC_MSS": {"avg": 15.86, "peak": 16.54, "min": 14.96}, "VDD_GPU": {"avg": 23.25, "peak": 26.79, "min": 20.89}, "VIN": {"avg": 63.6, "peak": 72.31, "min": 56.35}}, "power_watts_avg": 23.25, "energy_joules_est": 22.17, "sample_count": 7, "duration_seconds": 0.954}, "timestamp": "2026-01-17T14:37:42.574905"}
{"image_index": 85, "image_name": "000000008277.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008277.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2840.032, "latencies_ms": [2840.032], "images_per_second": 0.352, "prompt_tokens": 23, "response_tokens_est": 128, "n_tiles": 1, "output_text": "object: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject:", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 24853.2, "ram_available_mb": 100919.0, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24853.2, "ram_available_mb": 100919.0, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 4856.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.64, "peak": 15.82, "min": 15.22}, "VDD_CPU_SOC_MSS": {"avg": 16.64, "peak": 16.93, "min": 16.14}, "VDD_GPU": {"avg": 20.76, "peak": 24.43, "min": 20.09}, "VIN": {"avg": 63.85, "peak": 70.72, "min": 59.22}}, "power_watts_avg": 20.76, "energy_joules_est": 58.97, "sample_count": 22, "duration_seconds": 2.84}, "timestamp": "2026-01-17T14:37:45.425115"}
{"image_index": 85, "image_name": "000000008277.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008277.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1607.986, "latencies_ms": [1607.986], "images_per_second": 0.622, "prompt_tokens": 27, "response_tokens_est": 71, "n_tiles": 1, "output_text": "The main objects in the image are a black casserole dish filled with a green and yellow vegetable mixture, and a white plate holding the dish. The casserole dish is positioned in the foreground, with the plate placed behind it. The green and yellow mixture is in the casserole dish, while the plate is placed in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24853.2, "ram_available_mb": 100919.0, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24853.2, "ram_available_mb": 100919.0, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 4859.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.54, "peak": 15.72, "min": 15.22}, "VDD_CPU_SOC_MSS": {"avg": 16.67, "peak": 16.94, "min": 16.14}, "VDD_GPU": {"avg": 21.04, "peak": 24.03, "min": 20.09}, "VIN": {"avg": 62.36, "peak": 65.05, "min": 59.16}}, "power_watts_avg": 21.04, "energy_joules_est": 33.84, "sample_count": 12, "duration_seconds": 1.608}, "timestamp": "2026-01-17T14:37:47.040109"}
{"image_index": 85, "image_name": "000000008277.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008277.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1114.215, "latencies_ms": [1114.215], "images_per_second": 0.897, "prompt_tokens": 21, "response_tokens_est": 48, "n_tiles": 1, "output_text": "The image shows a white plate with a black, square-shaped dish containing a green, creamy soup and chunks of chicken. The dish is placed on a beige carpeted floor, and there is a silver fork resting on the plate.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24853.2, "ram_available_mb": 100919.0, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24853.9, "ram_available_mb": 100918.3, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 4854.7, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.47, "peak": 15.62, "min": 15.22}, "VDD_CPU_SOC_MSS": {"avg": 16.44, "peak": 16.54, "min": 16.14}, "VDD_GPU": {"avg": 21.53, "peak": 24.03, "min": 20.1}, "VIN": {"avg": 63.77, "peak": 67.36, "min": 61.8}}, "power_watts_avg": 21.53, "energy_joules_est": 23.99, "sample_count": 8, "duration_seconds": 1.114}, "timestamp": "2026-01-17T14:37:48.160299"}
{"image_index": 85, "image_name": "000000008277.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008277.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1199.63, "latencies_ms": [1199.63], "images_per_second": 0.834, "prompt_tokens": 19, "response_tokens_est": 52, "n_tiles": 1, "output_text": "The image features a white plate with a black rectangular baking dish containing a green and yellow vegetable dish, likely a creamy soup or stew. The dish is placed on a beige carpeted floor, and the lighting is soft and natural, casting gentle shadows.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24853.9, "ram_available_mb": 100918.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24854.1, "ram_available_mb": 100918.0, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 4853.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.5, "peak": 15.62, "min": 15.22}, "VDD_CPU_SOC_MSS": {"avg": 16.45, "peak": 16.54, "min": 16.14}, "VDD_GPU": {"avg": 21.37, "peak": 24.04, "min": 20.1}, "VIN": {"avg": 62.55, "peak": 64.94, "min": 57.03}}, "power_watts_avg": 21.37, "energy_joules_est": 25.65, "sample_count": 9, "duration_seconds": 1.2}, "timestamp": "2026-01-17T14:37:49.365741"}
{"image_index": 86, "image_name": "000000008532.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008532.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1548.818, "latencies_ms": [1548.818], "images_per_second": 0.646, "prompt_tokens": 9, "response_tokens_est": 35, "n_tiles": 6, "output_text": "The image features a man wearing a blue shirt, a red tie, and a plaid flat cap, standing outdoors with a building and a clear sky in the background.", "error": null, "sys_before": {"cpu_percent": 48.1, "ram_used_mb": 24872.4, "ram_available_mb": 100899.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 24873.3, "ram_available_mb": 100898.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.82, "peak": 15.24, "min": 14.2}, "VDD_CPU_SOC_MSS": {"avg": 15.71, "peak": 16.53, "min": 14.96}, "VDD_GPU": {"avg": 32.01, "peak": 38.98, "min": 25.6}, "VIN": {"avg": 78.37, "peak": 113.12, "min": 60.79}}, "power_watts_avg": 32.01, "energy_joules_est": 49.59, "sample_count": 11, "duration_seconds": 1.549}, "timestamp": "2026-01-17T14:37:50.972169"}
{"image_index": 86, "image_name": "000000008532.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008532.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1733.508, "latencies_ms": [1733.508], "images_per_second": 0.577, "prompt_tokens": 23, "response_tokens_est": 40, "n_tiles": 6, "output_text": "object: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24873.3, "ram_available_mb": 100898.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 24873.1, "ram_available_mb": 100899.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.98, "peak": 15.44, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.59, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 32.09, "peak": 40.16, "min": 24.43}, "VIN": {"avg": 79.55, "peak": 114.2, "min": 64.67}}, "power_watts_avg": 32.09, "energy_joules_est": 55.64, "sample_count": 13, "duration_seconds": 1.734}, "timestamp": "2026-01-17T14:37:52.712364"}
{"image_index": 86, "image_name": "000000008532.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008532.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2251.116, "latencies_ms": [2251.116], "images_per_second": 0.444, "prompt_tokens": 27, "response_tokens_est": 57, "n_tiles": 6, "output_text": "The main object in the foreground is a man wearing a blue shirt and a red tie. He is positioned slightly off-center to the right. The background features a building with a green roof and a window. The man is in the foreground, and the building is in the background.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 24873.1, "ram_available_mb": 100899.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 24873.1, "ram_available_mb": 100899.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.86, "peak": 15.24, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.35, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 30.24, "peak": 40.57, "min": 23.25}, "VIN": {"avg": 72.13, "peak": 113.56, "min": 60.0}}, "power_watts_avg": 30.24, "energy_joules_est": 68.09, "sample_count": 17, "duration_seconds": 2.252}, "timestamp": "2026-01-17T14:37:54.973317"}
{"image_index": 86, "image_name": "000000008532.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008532.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2340.325, "latencies_ms": [2340.325], "images_per_second": 0.427, "prompt_tokens": 21, "response_tokens_est": 60, "n_tiles": 6, "output_text": "The image shows a man wearing a light blue shirt, a red tie, and a plaid flat cap. He is outdoors, likely in a park or a public area, with a building and a window visible in the background. The man appears to be smiling and looking directly at the camera.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24873.1, "ram_available_mb": 100899.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 24873.1, "ram_available_mb": 100899.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.8, "peak": 15.24, "min": 14.2}, "VDD_CPU_SOC_MSS": {"avg": 15.27, "peak": 15.74, "min": 14.57}, "VDD_GPU": {"avg": 29.46, "peak": 40.18, "min": 22.86}, "VIN": {"avg": 70.12, "peak": 109.04, "min": 55.69}}, "power_watts_avg": 29.46, "energy_joules_est": 68.96, "sample_count": 18, "duration_seconds": 2.341}, "timestamp": "2026-01-17T14:37:57.319770"}
{"image_index": 86, "image_name": "000000008532.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008532.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1683.53, "latencies_ms": [1683.53], "images_per_second": 0.594, "prompt_tokens": 19, "response_tokens_est": 37, "n_tiles": 6, "output_text": "The man in the image is wearing a light blue shirt and a red tie. The background is blurred, but it appears to be a bright, sunny day with clear lighting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24873.1, "ram_available_mb": 100899.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 24873.1, "ram_available_mb": 100899.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.79, "peak": 15.24, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.35, "peak": 15.75, "min": 14.56}, "VDD_GPU": {"avg": 31.82, "peak": 40.18, "min": 24.03}, "VIN": {"avg": 73.82, "peak": 113.62, "min": 56.38}}, "power_watts_avg": 31.82, "energy_joules_est": 53.58, "sample_count": 13, "duration_seconds": 1.684}, "timestamp": "2026-01-17T14:37:59.009433"}
{"image_index": 87, "image_name": "000000008629.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008629.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1119.456, "latencies_ms": [1119.456], "images_per_second": 0.893, "prompt_tokens": 9, "response_tokens_est": 30, "n_tiles": 4, "output_text": "The image is a collage of four photographs showing various slices of pizza, each with different toppings and appearances, all placed on white plates.", "error": null, "sys_before": {"cpu_percent": 38.5, "ram_used_mb": 24873.1, "ram_available_mb": 100899.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 24873.3, "ram_available_mb": 100898.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4606.7, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5444.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.54, "peak": 15.01, "min": 14.0}, "VDD_CPU_SOC_MSS": {"avg": 15.35, "peak": 16.14, "min": 14.57}, "VDD_GPU": {"avg": 30.92, "peak": 36.63, "min": 25.21}, "VIN": {"avg": 67.93, "peak": 85.09, "min": 51.77}}, "power_watts_avg": 30.92, "energy_joules_est": 34.62, "sample_count": 8, "duration_seconds": 1.12}, "timestamp": "2026-01-17T14:38:00.169516"}
{"image_index": 87, "image_name": "000000008629.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008629.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1215.048, "latencies_ms": [1215.048], "images_per_second": 0.823, "prompt_tokens": 23, "response_tokens_est": 32, "n_tiles": 4, "output_text": "1. Pizza\n2. Pizza\n3. Pizza\n4. Pizza\n5. Pizza\n6. Pizza\n7. Pizza\n8. Pizza", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 24873.3, "ram_available_mb": 100898.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 24874.5, "ram_available_mb": 100897.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4606.7, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5455.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.7, "peak": 15.01, "min": 14.4}, "VDD_CPU_SOC_MSS": {"avg": 15.49, "peak": 15.75, "min": 15.35}, "VDD_GPU": {"avg": 30.5, "peak": 37.8, "min": 24.42}, "VIN": {"avg": 71.2, "peak": 99.25, "min": 61.57}}, "power_watts_avg": 30.5, "energy_joules_est": 37.07, "sample_count": 9, "duration_seconds": 1.215}, "timestamp": "2026-01-17T14:38:01.390474"}
{"image_index": 87, "image_name": "000000008629.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008629.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3380.285, "latencies_ms": [3380.285], "images_per_second": 0.296, "prompt_tokens": 27, "response_tokens_est": 114, "n_tiles": 4, "output_text": "The main objects in the image are pizzas, and they are arranged in a way that showcases their spatial relationships. The pizzas are positioned in the center of the image, with the leftmost pizza being the closest to the viewer, while the rightmost pizza is the furthest away. The foreground pizza is the most prominent, with its toppings and crust clearly visible. The background pizza is slightly out of focus, indicating that it is further away. The pizzas are placed on white plates, which help to highlight their colors and textures.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24874.5, "ram_available_mb": 100897.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 24867.2, "ram_available_mb": 100905.0, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4606.7, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5458.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.92, "peak": 15.11, "min": 14.4}, "VDD_CPU_SOC_MSS": {"avg": 15.66, "peak": 15.75, "min": 15.35}, "VDD_GPU": {"avg": 24.93, "peak": 37.82, "min": 21.66}, "VIN": {"avg": 65.59, "peak": 101.94, "min": 57.45}}, "power_watts_avg": 24.93, "energy_joules_est": 84.28, "sample_count": 26, "duration_seconds": 3.381}, "timestamp": "2026-01-17T14:38:04.777115"}
{"image_index": 87, "image_name": "000000008629.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008629.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1783.328, "latencies_ms": [1783.328], "images_per_second": 0.561, "prompt_tokens": 21, "response_tokens_est": 54, "n_tiles": 4, "output_text": "The image is a collage of four photographs depicting various slices of pizza. The pizza appears to be on a white plate, and the slices are being cut and eaten by someone. The overall setting suggests a casual dining environment, possibly a restaurant or a home kitchen.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24867.2, "ram_available_mb": 100905.0, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 24868.6, "ram_available_mb": 100903.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4606.7, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5453.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.81, "peak": 15.11, "min": 14.4}, "VDD_CPU_SOC_MSS": {"avg": 15.48, "peak": 15.75, "min": 14.96}, "VDD_GPU": {"avg": 27.58, "peak": 36.63, "min": 22.46}, "VIN": {"avg": 69.25, "peak": 106.96, "min": 58.35}}, "power_watts_avg": 27.58, "energy_joules_est": 49.2, "sample_count": 13, "duration_seconds": 1.784}, "timestamp": "2026-01-17T14:38:06.566583"}
{"image_index": 87, "image_name": "000000008629.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008629.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1940.233, "latencies_ms": [1940.233], "images_per_second": 0.515, "prompt_tokens": 19, "response_tokens_est": 60, "n_tiles": 4, "output_text": "The image showcases a collage of four photographs featuring a pizza with various toppings. The pizza crust appears golden-brown, and the toppings are visible, including melted cheese, red tomato pieces, and green herbs. The lighting is bright, highlighting the textures and colors of the food.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24868.6, "ram_available_mb": 100903.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 24869.3, "ram_available_mb": 100902.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4606.7, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5451.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.88, "peak": 15.11, "min": 14.51}, "VDD_CPU_SOC_MSS": {"avg": 15.61, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 27.27, "peak": 36.63, "min": 22.07}, "VIN": {"avg": 68.03, "peak": 102.7, "min": 44.26}}, "power_watts_avg": 27.27, "energy_joules_est": 52.92, "sample_count": 14, "duration_seconds": 1.941}, "timestamp": "2026-01-17T14:38:08.512666"}
{"image_index": 88, "image_name": "000000008690.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008690.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2730.387, "latencies_ms": [2730.387], "images_per_second": 0.366, "prompt_tokens": 9, "response_tokens_est": 33, "n_tiles": 12, "output_text": "Two young girls are standing in front of a fence, with one of them holding a black goat's head, while the other girl is holding a pink object.", "error": null, "sys_before": {"cpu_percent": 45.2, "ram_used_mb": 24845.0, "ram_available_mb": 100927.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 24846.2, "ram_available_mb": 100925.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.18, "peak": 16.66, "min": 13.59}, "VDD_CPU_SOC_MSS": {"avg": 15.9, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 34.95, "peak": 43.33, "min": 26.79}, "VIN": {"avg": 77.93, "peak": 118.86, "min": 57.7}}, "power_watts_avg": 34.95, "energy_joules_est": 95.44, "sample_count": 21, "duration_seconds": 2.731}, "timestamp": "2026-01-17T14:38:11.333141"}
{"image_index": 88, "image_name": "000000008690.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008690.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 3096.832, "latencies_ms": [3096.832], "images_per_second": 0.323, "prompt_tokens": 23, "response_tokens_est": 42, "n_tiles": 12, "output_text": "- goat: 2\n- girl: 2\n- woman: 1\n- dog: 1\n- fence: 1\n- child: 1\n- pet: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24846.2, "ram_available_mb": 100925.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 24846.0, "ram_available_mb": 100926.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.2, "peak": 16.56, "min": 14.12}, "VDD_CPU_SOC_MSS": {"avg": 15.75, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 35.1, "peak": 44.91, "min": 26.39}, "VIN": {"avg": 80.15, "peak": 119.15, "min": 66.26}}, "power_watts_avg": 35.1, "energy_joules_est": 108.72, "sample_count": 24, "duration_seconds": 3.097}, "timestamp": "2026-01-17T14:38:14.436464"}
{"image_index": 88, "image_name": "000000008690.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008690.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 4395.42, "latencies_ms": [4395.42], "images_per_second": 0.228, "prompt_tokens": 27, "response_tokens_est": 80, "n_tiles": 12, "output_text": "In the image, the main objects are a black goat and a young girl. The goat is in the foreground, while the girl is positioned slightly behind it. The girl is holding the goat's head, indicating a close interaction between them. The goat is also in the foreground, while the girl is in the background. The background features a fence and a house, providing context to the setting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24846.0, "ram_available_mb": 100926.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 24846.5, "ram_available_mb": 100925.7, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.15, "peak": 16.56, "min": 14.12}, "VDD_CPU_SOC_MSS": {"avg": 15.6, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 32.41, "peak": 44.51, "min": 25.6}, "VIN": {"avg": 75.3, "peak": 114.17, "min": 55.9}}, "power_watts_avg": 32.41, "energy_joules_est": 142.47, "sample_count": 34, "duration_seconds": 4.396}, "timestamp": "2026-01-17T14:38:18.838445"}
{"image_index": 88, "image_name": "000000008690.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008690.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3839.711, "latencies_ms": [3839.711], "images_per_second": 0.26, "prompt_tokens": 21, "response_tokens_est": 64, "n_tiles": 12, "output_text": "The image depicts a scene in a petting zoo or a similar animal enclosure. A young girl and another child are interacting with a black goat, which is being petted by the girl. The setting is outdoors, with a fence and greenery visible in the background, indicating a controlled environment for the animals.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24846.5, "ram_available_mb": 100925.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 24847.7, "ram_available_mb": 100924.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.17, "peak": 16.56, "min": 14.12}, "VDD_CPU_SOC_MSS": {"avg": 15.72, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 33.14, "peak": 44.91, "min": 26.0}, "VIN": {"avg": 76.63, "peak": 130.72, "min": 61.49}}, "power_watts_avg": 33.14, "energy_joules_est": 127.26, "sample_count": 30, "duration_seconds": 3.84}, "timestamp": "2026-01-17T14:38:22.687686"}
{"image_index": 88, "image_name": "000000008690.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008690.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3392.433, "latencies_ms": [3392.433], "images_per_second": 0.295, "prompt_tokens": 19, "response_tokens_est": 51, "n_tiles": 12, "output_text": "The image features a bright, sunny day with clear blue skies. The lighting is natural, casting soft shadows on the ground. The scene includes a white goat with black patches, a black and white dog, and a girl in a colorful floral dress.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24847.7, "ram_available_mb": 100924.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 24848.7, "ram_available_mb": 100923.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.21, "peak": 16.56, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.78, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 34.48, "peak": 44.91, "min": 26.01}, "VIN": {"avg": 76.35, "peak": 104.06, "min": 66.27}}, "power_watts_avg": 34.48, "energy_joules_est": 116.99, "sample_count": 26, "duration_seconds": 3.393}, "timestamp": "2026-01-17T14:38:26.087542"}
{"image_index": 89, "image_name": "000000008762.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008762.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1604.25, "latencies_ms": [1604.25], "images_per_second": 0.623, "prompt_tokens": 9, "response_tokens_est": 37, "n_tiles": 6, "output_text": "The image depicts a nighttime scene of a roadway with traffic lights, illuminated signs, and a dark sky, with a hint of a mountain range in the background.", "error": null, "sys_before": {"cpu_percent": 47.7, "ram_used_mb": 24865.6, "ram_available_mb": 100906.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 24867.8, "ram_available_mb": 100904.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.68, "peak": 15.11, "min": 13.72}, "VDD_CPU_SOC_MSS": {"avg": 15.29, "peak": 15.75, "min": 14.17}, "VDD_GPU": {"avg": 32.69, "peak": 40.57, "min": 25.21}, "VIN": {"avg": 74.82, "peak": 110.11, "min": 61.36}}, "power_watts_avg": 32.69, "energy_joules_est": 52.45, "sample_count": 12, "duration_seconds": 1.605}, "timestamp": "2026-01-17T14:38:27.754643"}
{"image_index": 89, "image_name": "000000008762.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008762.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1797.298, "latencies_ms": [1797.298], "images_per_second": 0.556, "prompt_tokens": 23, "response_tokens_est": 41, "n_tiles": 6, "output_text": "- Traffic light: 2\n- Street light: 1\n- Road: 1\n- Buildings: 3\n- Power lines: 2\n- Road sign: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24867.8, "ram_available_mb": 100904.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 24868.8, "ram_available_mb": 100903.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.83, "peak": 15.34, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.47, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 32.03, "peak": 40.97, "min": 24.04}, "VIN": {"avg": 74.77, "peak": 113.34, "min": 63.0}}, "power_watts_avg": 32.03, "energy_joules_est": 57.58, "sample_count": 13, "duration_seconds": 1.798}, "timestamp": "2026-01-17T14:38:29.558113"}
{"image_index": 89, "image_name": "000000008762.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008762.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2621.838, "latencies_ms": [2621.838], "images_per_second": 0.381, "prompt_tokens": 27, "response_tokens_est": 70, "n_tiles": 6, "output_text": "The main objects in the image are the traffic lights and the road. The traffic lights are positioned in the foreground, with the green light illuminated, indicating that vehicles can proceed. The road is in the background, extending into the distance with other traffic lights and signs visible. The distant mountains and buildings provide a sense of depth to the scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24868.8, "ram_available_mb": 100903.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 24868.8, "ram_available_mb": 100903.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.84, "peak": 15.24, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.43, "peak": 15.75, "min": 14.56}, "VDD_GPU": {"avg": 28.97, "peak": 40.57, "min": 22.86}, "VIN": {"avg": 68.45, "peak": 114.4, "min": 54.78}}, "power_watts_avg": 28.97, "energy_joules_est": 75.96, "sample_count": 20, "duration_seconds": 2.622}, "timestamp": "2026-01-17T14:38:32.185810"}
{"image_index": 89, "image_name": "000000008762.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008762.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2335.381, "latencies_ms": [2335.381], "images_per_second": 0.428, "prompt_tokens": 21, "response_tokens_est": 60, "n_tiles": 6, "output_text": "The image depicts a nighttime scene of a highway intersection during twilight. The sky is dark, and the streetlights are on, illuminating the scene. The traffic lights are green, indicating that vehicles can proceed. The road is empty, and there are no visible pedestrians or vehicles.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24868.8, "ram_available_mb": 100903.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 24869.0, "ram_available_mb": 100903.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.85, "peak": 15.24, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.46, "peak": 15.75, "min": 14.56}, "VDD_GPU": {"avg": 29.42, "peak": 40.18, "min": 23.25}, "VIN": {"avg": 71.43, "peak": 106.38, "min": 59.13}}, "power_watts_avg": 29.42, "energy_joules_est": 68.72, "sample_count": 18, "duration_seconds": 2.336}, "timestamp": "2026-01-17T14:38:34.527023"}
{"image_index": 89, "image_name": "000000008762.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008762.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2362.248, "latencies_ms": [2362.248], "images_per_second": 0.423, "prompt_tokens": 19, "response_tokens_est": 61, "n_tiles": 6, "output_text": "The image depicts a nighttime scene with a clear sky and a few scattered lights from vehicles and streetlights. The traffic lights are illuminated green, indicating that vehicles can proceed. The overall lighting is dim, with the main source of light being the streetlights and the illuminated traffic lights.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24869.0, "ram_available_mb": 100903.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 24869.3, "ram_available_mb": 100902.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.82, "peak": 15.14, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.51, "peak": 15.75, "min": 14.57}, "VDD_GPU": {"avg": 29.5, "peak": 40.95, "min": 23.24}, "VIN": {"avg": 68.65, "peak": 85.99, "min": 62.61}}, "power_watts_avg": 29.5, "energy_joules_est": 69.71, "sample_count": 18, "duration_seconds": 2.363}, "timestamp": "2026-01-17T14:38:36.895760"}
{"image_index": 90, "image_name": "000000008844.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008844.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1067.852, "latencies_ms": [1067.852], "images_per_second": 0.936, "prompt_tokens": 9, "response_tokens_est": 17, "n_tiles": 6, "output_text": "A smiling woman is standing in front of a table filled with ripe bananas.", "error": null, "sys_before": {"cpu_percent": 45.3, "ram_used_mb": 24869.3, "ram_available_mb": 100902.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 24870.5, "ram_available_mb": 100901.7, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.54, "peak": 15.04, "min": 13.79}, "VDD_CPU_SOC_MSS": {"avg": 15.25, "peak": 16.14, "min": 14.17}, "VDD_GPU": {"avg": 35.01, "peak": 40.18, "min": 29.54}, "VIN": {"avg": 80.76, "peak": 123.14, "min": 55.85}}, "power_watts_avg": 35.01, "energy_joules_est": 37.4, "sample_count": 8, "duration_seconds": 1.068}, "timestamp": "2026-01-17T14:38:38.013734"}
{"image_index": 90, "image_name": "000000008844.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008844.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1081.449, "latencies_ms": [1081.449], "images_per_second": 0.925, "prompt_tokens": 23, "response_tokens_est": 16, "n_tiles": 6, "output_text": "bananas: 20\nwoman: 1\nshirt: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24870.5, "ram_available_mb": 100901.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 24870.5, "ram_available_mb": 100901.7, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.85, "peak": 15.34, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.5, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 37.47, "peak": 41.36, "min": 30.73}, "VIN": {"avg": 78.44, "peak": 107.57, "min": 62.65}}, "power_watts_avg": 37.47, "energy_joules_est": 40.54, "sample_count": 8, "duration_seconds": 1.082}, "timestamp": "2026-01-17T14:38:39.101209"}
{"image_index": 90, "image_name": "000000008844.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008844.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1887.281, "latencies_ms": [1887.281], "images_per_second": 0.53, "prompt_tokens": 27, "response_tokens_est": 44, "n_tiles": 6, "output_text": "The main objects in the image are bananas and a woman. The bananas are in the foreground, with a bunch of them in the center. The woman is in the background, standing near the bananas.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24870.5, "ram_available_mb": 100901.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 24870.5, "ram_available_mb": 100901.7, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.84, "peak": 15.24, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.41, "peak": 15.75, "min": 14.56}, "VDD_GPU": {"avg": 32.69, "peak": 41.74, "min": 24.03}, "VIN": {"avg": 72.67, "peak": 114.49, "min": 62.55}}, "power_watts_avg": 32.69, "energy_joules_est": 61.71, "sample_count": 14, "duration_seconds": 1.888}, "timestamp": "2026-01-17T14:38:40.994373"}
{"image_index": 90, "image_name": "000000008844.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008844.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2449.377, "latencies_ms": [2449.377], "images_per_second": 0.408, "prompt_tokens": 21, "response_tokens_est": 64, "n_tiles": 6, "output_text": "The image depicts a woman standing in front of a market stall, surrounded by a variety of bananas. She is smiling and appears to be enjoying her time at the market. The setting is an outdoor market, with the woman standing in front of a stall that is filled with fresh produce, including bananas.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24870.5, "ram_available_mb": 100901.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 24870.7, "ram_available_mb": 100901.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.83, "peak": 15.24, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.54, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 29.2, "peak": 40.18, "min": 22.86}, "VIN": {"avg": 67.92, "peak": 99.59, "min": 57.01}}, "power_watts_avg": 29.2, "energy_joules_est": 71.53, "sample_count": 19, "duration_seconds": 2.45}, "timestamp": "2026-01-17T14:38:43.450696"}
{"image_index": 90, "image_name": "000000008844.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008844.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1794.185, "latencies_ms": [1794.185], "images_per_second": 0.557, "prompt_tokens": 19, "response_tokens_est": 41, "n_tiles": 6, "output_text": "The image features a woman with a warm smile, wearing a colorful, patterned shirt. The background includes a rustic wooden table with a bunch of ripe bananas, suggesting a warm, sunny day.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24870.7, "ram_available_mb": 100901.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 24872.0, "ram_available_mb": 100900.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.8, "peak": 15.24, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.32, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 31.57, "peak": 40.16, "min": 24.04}, "VIN": {"avg": 70.24, "peak": 103.02, "min": 61.1}}, "power_watts_avg": 31.57, "energy_joules_est": 56.66, "sample_count": 13, "duration_seconds": 1.795}, "timestamp": "2026-01-17T14:38:45.250662"}
{"image_index": 91, "image_name": "000000008899.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008899.jpg", "image_width": 640, "image_height": 539, "image_resolution": "640x539", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2909.762, "latencies_ms": [2909.762], "images_per_second": 0.344, "prompt_tokens": 9, "response_tokens_est": 39, "n_tiles": 12, "output_text": "The image depicts a street scene featuring a metal shuttered storefront with graffiti on its door and window, a rusty fire hydrant, and a bicycle leaning against a building.", "error": null, "sys_before": {"cpu_percent": 39.8, "ram_used_mb": 24838.5, "ram_available_mb": 100933.7, "ram_percent": 19.7}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 24840.9, "ram_available_mb": 100931.3, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.19, "peak": 16.66, "min": 13.59}, "VDD_CPU_SOC_MSS": {"avg": 15.92, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 34.7, "peak": 43.73, "min": 26.79}, "VIN": {"avg": 75.71, "peak": 113.38, "min": 65.34}}, "power_watts_avg": 34.7, "energy_joules_est": 100.98, "sample_count": 23, "duration_seconds": 2.91}, "timestamp": "2026-01-17T14:38:48.246495"}
{"image_index": 91, "image_name": "000000008899.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008899.jpg", "image_width": 640, "image_height": 539, "image_resolution": "640x539", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2901.636, "latencies_ms": [2901.636], "images_per_second": 0.345, "prompt_tokens": 23, "response_tokens_est": 37, "n_tiles": 12, "output_text": "fire hydrant: 1\nbicycle: 1\nbuilding: 1\nwindow: 2\ngarage door: 2\ngraffiti: 4", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 24840.9, "ram_available_mb": 100931.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 24842.1, "ram_available_mb": 100930.0, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.29, "peak": 16.56, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.82, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 36.26, "peak": 45.28, "min": 27.18}, "VIN": {"avg": 78.57, "peak": 126.17, "min": 66.12}}, "power_watts_avg": 36.26, "energy_joules_est": 105.23, "sample_count": 22, "duration_seconds": 2.902}, "timestamp": "2026-01-17T14:38:51.158890"}
{"image_index": 91, "image_name": "000000008899.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008899.jpg", "image_width": 640, "image_height": 539, "image_resolution": "640x539", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 4298.047, "latencies_ms": [4298.047], "images_per_second": 0.233, "prompt_tokens": 27, "response_tokens_est": 77, "n_tiles": 12, "output_text": "The main objects in the image are a red brick building with a green metal fire escape, a closed metal shutter door, and a green door with graffiti. The fire escape is attached to the building, and the green door is located near the fire hydrant. The graffiti on the door and shutter is in the foreground, while the fire hydrant is in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24842.1, "ram_available_mb": 100930.0, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 24843.1, "ram_available_mb": 100929.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.16, "peak": 16.66, "min": 14.12}, "VDD_CPU_SOC_MSS": {"avg": 15.67, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 32.57, "peak": 44.91, "min": 25.6}, "VIN": {"avg": 73.9, "peak": 106.41, "min": 60.72}}, "power_watts_avg": 32.57, "energy_joules_est": 140.0, "sample_count": 33, "duration_seconds": 4.299}, "timestamp": "2026-01-17T14:38:55.463491"}
{"image_index": 91, "image_name": "000000008899.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008899.jpg", "image_width": 640, "image_height": 539, "image_resolution": "640x539", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 4771.112, "latencies_ms": [4771.112], "images_per_second": 0.21, "prompt_tokens": 21, "response_tokens_est": 91, "n_tiles": 12, "output_text": "The image depicts an urban alleyway with a red brick building on the left and a green metal shuttered storefront on the right. The storefront has various graffiti tags on it, including a prominent blue and red graffiti on the door. There is a fire hydrant on the sidewalk, and a bicycle is parked near the building. The overall scene suggests a quiet, possibly neglected urban area with a mix of commercial and residential structures.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24843.4, "ram_available_mb": 100928.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 24844.3, "ram_available_mb": 100927.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.15, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.59, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 31.82, "peak": 45.28, "min": 25.6}, "VIN": {"avg": 76.14, "peak": 113.65, "min": 66.13}}, "power_watts_avg": 31.82, "energy_joules_est": 151.83, "sample_count": 37, "duration_seconds": 4.771}, "timestamp": "2026-01-17T14:39:00.241519"}
{"image_index": 91, "image_name": "000000008899.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008899.jpg", "image_width": 640, "image_height": 539, "image_resolution": "640x539", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 4039.628, "latencies_ms": [4039.628], "images_per_second": 0.248, "prompt_tokens": 19, "response_tokens_est": 69, "n_tiles": 12, "output_text": "The image depicts a weathered, urban scene with a red brick building featuring a green metal fire escape. The building has a weathered look with visible signs of wear and tear, including graffiti on the windows and doors. The lighting is dim, suggesting it might be early morning or late afternoon, with a subdued atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24844.3, "ram_available_mb": 100927.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 24845.3, "ram_available_mb": 100926.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.88, "peak": 16.36, "min": 14.02}, "VDD_CPU_SOC_MSS": {"avg": 15.34, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 33.04, "peak": 45.3, "min": 25.22}, "VIN": {"avg": 75.71, "peak": 125.13, "min": 63.17}}, "power_watts_avg": 33.04, "energy_joules_est": 133.49, "sample_count": 31, "duration_seconds": 4.04}, "timestamp": "2026-01-17T14:39:04.287940"}
{"image_index": 92, "image_name": "000000009378.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009378.jpg", "image_width": 600, "image_height": 400, "image_resolution": "600x400", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1521.597, "latencies_ms": [1521.597], "images_per_second": 0.657, "prompt_tokens": 9, "response_tokens_est": 34, "n_tiles": 6, "output_text": "A man is in the process of throwing a bright yellow frisbee, with his eyes wide open and mouth open, indicating he is fully engaged in the activity.", "error": null, "sys_before": {"cpu_percent": 47.1, "ram_used_mb": 24852.5, "ram_available_mb": 100919.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 24854.0, "ram_available_mb": 100918.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.59, "peak": 15.04, "min": 13.51}, "VDD_CPU_SOC_MSS": {"avg": 15.28, "peak": 15.75, "min": 14.17}, "VDD_GPU": {"avg": 33.24, "peak": 40.97, "min": 26.01}, "VIN": {"avg": 75.28, "peak": 117.27, "min": 61.28}}, "power_watts_avg": 33.24, "energy_joules_est": 50.59, "sample_count": 11, "duration_seconds": 1.522}, "timestamp": "2026-01-17T14:39:05.875197"}
{"image_index": 92, "image_name": "000000009378.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009378.jpg", "image_width": 600, "image_height": 400, "image_resolution": "600x400", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1626.613, "latencies_ms": [1626.613], "images_per_second": 0.615, "prompt_tokens": 23, "response_tokens_est": 35, "n_tiles": 6, "output_text": "frisbee: 1\nman: 1\nhat: 1\nshirt: 1\nclothes: 1\nsunglasses: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24854.0, "ram_available_mb": 100918.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 24854.7, "ram_available_mb": 100917.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.81, "peak": 15.24, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.55, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 32.99, "peak": 41.36, "min": 24.83}, "VIN": {"avg": 71.18, "peak": 92.23, "min": 58.83}}, "power_watts_avg": 32.99, "energy_joules_est": 53.69, "sample_count": 12, "duration_seconds": 1.627}, "timestamp": "2026-01-17T14:39:07.508531"}
{"image_index": 92, "image_name": "000000009378.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009378.jpg", "image_width": 600, "image_height": 400, "image_resolution": "600x400", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2050.626, "latencies_ms": [2050.626], "images_per_second": 0.488, "prompt_tokens": 27, "response_tokens_est": 50, "n_tiles": 6, "output_text": "The main object in the foreground is a man holding a bright green frisbee. He is positioned slightly off-center to the left. The background is dimly lit, with indistinct figures and objects, suggesting a crowded or indoor setting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24854.7, "ram_available_mb": 100917.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 24854.8, "ram_available_mb": 100917.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.84, "peak": 15.24, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.41, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 31.07, "peak": 40.57, "min": 23.64}, "VIN": {"avg": 73.99, "peak": 109.96, "min": 62.51}}, "power_watts_avg": 31.07, "energy_joules_est": 63.72, "sample_count": 15, "duration_seconds": 2.051}, "timestamp": "2026-01-17T14:39:09.564850"}
{"image_index": 92, "image_name": "000000009378.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009378.jpg", "image_width": 600, "image_height": 400, "image_resolution": "600x400", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2474.735, "latencies_ms": [2474.735], "images_per_second": 0.404, "prompt_tokens": 21, "response_tokens_est": 65, "n_tiles": 6, "output_text": "The image depicts a man in a black sleeveless top and a blue and white striped beanie, holding a bright yellow frisbee in his right hand. He appears to be in a dimly lit indoor setting, possibly a recreational area or a sports facility, with a crowd of people in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24854.8, "ram_available_mb": 100917.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 24855.7, "ram_available_mb": 100916.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.85, "peak": 15.24, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.48, "peak": 15.75, "min": 14.56}, "VDD_GPU": {"avg": 29.03, "peak": 40.18, "min": 22.85}, "VIN": {"avg": 68.61, "peak": 94.65, "min": 50.89}}, "power_watts_avg": 29.03, "energy_joules_est": 71.86, "sample_count": 19, "duration_seconds": 2.475}, "timestamp": "2026-01-17T14:39:12.046572"}
{"image_index": 92, "image_name": "000000009378.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009378.jpg", "image_width": 600, "image_height": 400, "image_resolution": "600x400", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1596.504, "latencies_ms": [1596.504], "images_per_second": 0.626, "prompt_tokens": 19, "response_tokens_est": 34, "n_tiles": 6, "output_text": "The man in the image is wearing a black sleeveless top and a blue and white striped beanie. The lighting is dim, creating a moody atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24855.7, "ram_available_mb": 100916.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 24856.0, "ram_available_mb": 100916.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.77, "peak": 15.14, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.32, "peak": 15.75, "min": 14.57}, "VDD_GPU": {"avg": 32.59, "peak": 40.95, "min": 24.82}, "VIN": {"avg": 73.74, "peak": 102.3, "min": 53.35}}, "power_watts_avg": 32.59, "energy_joules_est": 52.05, "sample_count": 12, "duration_seconds": 1.597}, "timestamp": "2026-01-17T14:39:13.649629"}
{"image_index": 93, "image_name": "000000009400.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009400.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 3033.141, "latencies_ms": [3033.141], "images_per_second": 0.33, "prompt_tokens": 9, "response_tokens_est": 42, "n_tiles": 12, "output_text": "The image depicts a group of people sitting around a table, each engrossed in their own computer screens, with a variety of laptops and computers scattered around, indicating a busy and collaborative work environment.", "error": null, "sys_before": {"cpu_percent": 43.1, "ram_used_mb": 24831.5, "ram_available_mb": 100940.6, "ram_percent": 19.7}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 24828.1, "ram_available_mb": 100944.1, "ram_percent": 19.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.92, "peak": 16.36, "min": 13.59}, "VDD_CPU_SOC_MSS": {"avg": 15.62, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 34.44, "peak": 43.71, "min": 26.0}, "VIN": {"avg": 75.55, "peak": 118.02, "min": 49.24}}, "power_watts_avg": 34.44, "energy_joules_est": 104.48, "sample_count": 24, "duration_seconds": 3.034}, "timestamp": "2026-01-17T14:39:16.765199"}
{"image_index": 93, "image_name": "000000009400.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009400.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2753.518, "latencies_ms": [2753.518], "images_per_second": 0.363, "prompt_tokens": 23, "response_tokens_est": 32, "n_tiles": 12, "output_text": "1. Laptop\n2. Laptop\n3. Laptop\n4. Laptop\n5. Laptop\n6. Laptop\n7. Laptop\n8. Laptop", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24828.1, "ram_available_mb": 100944.1, "ram_percent": 19.7}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 24829.5, "ram_available_mb": 100942.6, "ram_percent": 19.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.96, "peak": 16.26, "min": 13.92}, "VDD_CPU_SOC_MSS": {"avg": 15.43, "peak": 16.92, "min": 14.17}, "VDD_GPU": {"avg": 36.5, "peak": 45.3, "min": 26.79}, "VIN": {"avg": 75.49, "peak": 131.61, "min": 55.08}}, "power_watts_avg": 36.5, "energy_joules_est": 100.52, "sample_count": 21, "duration_seconds": 2.754}, "timestamp": "2026-01-17T14:39:19.526430"}
{"image_index": 93, "image_name": "000000009400.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009400.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3867.988, "latencies_ms": [3867.988], "images_per_second": 0.259, "prompt_tokens": 27, "response_tokens_est": 64, "n_tiles": 12, "output_text": "The main objects in the image are a laptop, a keyboard, and a mouse. The laptop is positioned in the foreground, with the keyboard and mouse placed in front of it. The laptop screen is turned on, displaying some content. The keyboard and mouse are in the foreground, with the laptop in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24829.5, "ram_available_mb": 100942.6, "ram_percent": 19.7}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 24829.8, "ram_available_mb": 100942.4, "ram_percent": 19.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.87, "peak": 16.36, "min": 13.92}, "VDD_CPU_SOC_MSS": {"avg": 15.27, "peak": 17.32, "min": 14.17}, "VDD_GPU": {"avg": 33.48, "peak": 46.09, "min": 25.6}, "VIN": {"avg": 74.32, "peak": 118.07, "min": 59.65}}, "power_watts_avg": 33.48, "energy_joules_est": 129.52, "sample_count": 30, "duration_seconds": 3.868}, "timestamp": "2026-01-17T14:39:23.401864"}
{"image_index": 93, "image_name": "000000009400.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009400.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 4106.093, "latencies_ms": [4106.093], "images_per_second": 0.244, "prompt_tokens": 21, "response_tokens_est": 71, "n_tiles": 12, "output_text": "The image depicts a dimly lit room filled with people engaged in various activities. The setting appears to be a casual gathering or meeting place, possibly a bar or a small restaurant, with a table cluttered with laptops, drinks, and other personal items. The individuals are focused on their screens, suggesting they are either working or browsing through content.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24829.8, "ram_available_mb": 100942.4, "ram_percent": 19.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 24829.8, "ram_available_mb": 100942.4, "ram_percent": 19.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.87, "peak": 16.36, "min": 13.92}, "VDD_CPU_SOC_MSS": {"avg": 15.3, "peak": 17.32, "min": 14.17}, "VDD_GPU": {"avg": 32.73, "peak": 44.91, "min": 25.6}, "VIN": {"avg": 76.73, "peak": 126.09, "min": 54.73}}, "power_watts_avg": 32.73, "energy_joules_est": 134.4, "sample_count": 32, "duration_seconds": 4.106}, "timestamp": "2026-01-17T14:39:27.514633"}
{"image_index": 93, "image_name": "000000009400.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009400.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3306.662, "latencies_ms": [3306.662], "images_per_second": 0.302, "prompt_tokens": 19, "response_tokens_est": 48, "n_tiles": 12, "output_text": "The image depicts a dimly lit room with a warm, yellowish glow from a lamp. The lighting creates a cozy atmosphere, highlighting the cluttered desk with various electronic devices, including laptops, a keyboard, and a mouse.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24829.8, "ram_available_mb": 100942.4, "ram_percent": 19.7}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 24831.0, "ram_available_mb": 100941.2, "ram_percent": 19.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.91, "peak": 16.26, "min": 13.92}, "VDD_CPU_SOC_MSS": {"avg": 15.34, "peak": 17.32, "min": 14.17}, "VDD_GPU": {"avg": 34.48, "peak": 45.69, "min": 25.6}, "VIN": {"avg": 77.33, "peak": 125.92, "min": 64.32}}, "power_watts_avg": 34.48, "energy_joules_est": 114.02, "sample_count": 26, "duration_seconds": 3.307}, "timestamp": "2026-01-17T14:39:30.827592"}
{"image_index": 94, "image_name": "000000009448.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009448.jpg", "image_width": 551, "image_height": 640, "image_resolution": "551x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2309.108, "latencies_ms": [2309.108], "images_per_second": 0.433, "prompt_tokens": 9, "response_tokens_est": 20, "n_tiles": 12, "output_text": "A young girl with blonde hair is holding a blue umbrella and a brown bottle in her hand.", "error": null, "sys_before": {"cpu_percent": 43.7, "ram_used_mb": 24835.4, "ram_available_mb": 100936.8, "ram_percent": 19.7}, "sys_after": {"cpu_percent": 2.3, "ram_used_mb": 24836.6, "ram_available_mb": 100935.6, "ram_percent": 19.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.04, "peak": 16.66, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 15.66, "peak": 17.71, "min": 13.78}, "VDD_GPU": {"avg": 36.9, "peak": 44.12, "min": 28.36}, "VIN": {"avg": 76.84, "peak": 106.12, "min": 59.56}}, "power_watts_avg": 36.9, "energy_joules_est": 85.22, "sample_count": 18, "duration_seconds": 2.309}, "timestamp": "2026-01-17T14:39:33.249477"}
{"image_index": 94, "image_name": "000000009448.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009448.jpg", "image_width": 551, "image_height": 640, "image_resolution": "551x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2987.752, "latencies_ms": [2987.752], "images_per_second": 0.335, "prompt_tokens": 23, "response_tokens_est": 39, "n_tiles": 12, "output_text": "1. Girl\n2. Blue umbrella\n3. Blue umbrella\n4. Blue umbrella\n5. Blue umbrella\n6. Blue umbrella\n7. Blue umbrella\n8. Blue umbrella", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24836.6, "ram_available_mb": 100935.6, "ram_percent": 19.7}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 24837.6, "ram_available_mb": 100934.6, "ram_percent": 19.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.99, "peak": 16.36, "min": 14.12}, "VDD_CPU_SOC_MSS": {"avg": 15.56, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 35.91, "peak": 45.3, "min": 26.0}, "VIN": {"avg": 77.9, "peak": 112.6, "min": 62.1}}, "power_watts_avg": 35.91, "energy_joules_est": 107.31, "sample_count": 23, "duration_seconds": 2.988}, "timestamp": "2026-01-17T14:39:36.243511"}
{"image_index": 94, "image_name": "000000009448.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009448.jpg", "image_width": 551, "image_height": 640, "image_resolution": "551x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2715.249, "latencies_ms": [2715.249], "images_per_second": 0.368, "prompt_tokens": 27, "response_tokens_est": 31, "n_tiles": 12, "output_text": "The main object in the foreground is a young girl holding a blue umbrella. The background features a blue umbrella, and the ground is covered with gravel.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24837.6, "ram_available_mb": 100934.6, "ram_percent": 19.7}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 24837.3, "ram_available_mb": 100934.9, "ram_percent": 19.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.01, "peak": 16.36, "min": 14.02}, "VDD_CPU_SOC_MSS": {"avg": 15.65, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 36.54, "peak": 45.3, "min": 26.79}, "VIN": {"avg": 76.12, "peak": 115.63, "min": 61.55}}, "power_watts_avg": 36.54, "energy_joules_est": 99.23, "sample_count": 21, "duration_seconds": 2.716}, "timestamp": "2026-01-17T14:39:38.965261"}
{"image_index": 94, "image_name": "000000009448.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009448.jpg", "image_width": 551, "image_height": 640, "image_resolution": "551x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3409.941, "latencies_ms": [3409.941], "images_per_second": 0.293, "prompt_tokens": 21, "response_tokens_est": 51, "n_tiles": 12, "output_text": "The image depicts a young girl standing under a large blue umbrella. She is holding a brown object in her hand, possibly a toy or a snack. The setting appears to be outdoors, with a gravel surface and a blue sky in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24837.3, "ram_available_mb": 100934.9, "ram_percent": 19.7}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 24837.8, "ram_available_mb": 100934.4, "ram_percent": 19.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.93, "peak": 16.36, "min": 14.02}, "VDD_CPU_SOC_MSS": {"avg": 15.41, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 34.62, "peak": 46.09, "min": 25.6}, "VIN": {"avg": 73.84, "peak": 117.5, "min": 56.9}}, "power_watts_avg": 34.62, "energy_joules_est": 118.07, "sample_count": 26, "duration_seconds": 3.411}, "timestamp": "2026-01-17T14:39:42.385972"}
{"image_index": 94, "image_name": "000000009448.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009448.jpg", "image_width": 551, "image_height": 640, "image_resolution": "551x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3195.296, "latencies_ms": [3195.296], "images_per_second": 0.313, "prompt_tokens": 19, "response_tokens_est": 45, "n_tiles": 12, "output_text": "The notable visual attributes of the image include a young girl with light blonde hair, wearing a pink jacket and a colorful scarf. The background features a blue umbrella, and the lighting suggests it is daytime with natural sunlight.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24837.8, "ram_available_mb": 100934.4, "ram_percent": 19.7}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 24838.0, "ram_available_mb": 100934.2, "ram_percent": 19.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.92, "peak": 16.26, "min": 13.92}, "VDD_CPU_SOC_MSS": {"avg": 15.48, "peak": 17.32, "min": 14.17}, "VDD_GPU": {"avg": 35.0, "peak": 45.69, "min": 26.01}, "VIN": {"avg": 76.74, "peak": 122.29, "min": 52.16}}, "power_watts_avg": 35.0, "energy_joules_est": 111.85, "sample_count": 25, "duration_seconds": 3.196}, "timestamp": "2026-01-17T14:39:45.589661"}
{"image_index": 95, "image_name": "000000009483.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009483.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2528.46, "latencies_ms": [2528.46], "images_per_second": 0.395, "prompt_tokens": 9, "response_tokens_est": 27, "n_tiles": 12, "output_text": "The image shows a man standing in an office with a computer setup on a desk, including a monitor, keyboard, and mouse.", "error": null, "sys_before": {"cpu_percent": 41.9, "ram_used_mb": 24847.9, "ram_available_mb": 100924.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 24840.3, "ram_available_mb": 100931.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.09, "peak": 16.66, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 15.81, "peak": 17.71, "min": 13.78}, "VDD_GPU": {"avg": 36.36, "peak": 44.12, "min": 27.98}, "VIN": {"avg": 79.62, "peak": 115.61, "min": 63.43}}, "power_watts_avg": 36.36, "energy_joules_est": 91.95, "sample_count": 19, "duration_seconds": 2.529}, "timestamp": "2026-01-17T14:39:48.231076"}
{"image_index": 95, "image_name": "000000009483.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009483.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2890.916, "latencies_ms": [2890.916], "images_per_second": 0.346, "prompt_tokens": 23, "response_tokens_est": 36, "n_tiles": 12, "output_text": "1. Computer monitor\n2. Computer mouse\n3. Keyboard\n4. Printer\n5. Desk\n6. Chair\n7. Cables\n8. Power outlet", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24840.3, "ram_available_mb": 100931.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 24836.8, "ram_available_mb": 100935.4, "ram_percent": 19.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.25, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.83, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 36.24, "peak": 44.89, "min": 26.8}, "VIN": {"avg": 81.61, "peak": 117.28, "min": 58.93}}, "power_watts_avg": 36.24, "energy_joules_est": 104.79, "sample_count": 22, "duration_seconds": 2.892}, "timestamp": "2026-01-17T14:39:51.129742"}
{"image_index": 95, "image_name": "000000009483.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009483.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3336.643, "latencies_ms": [3336.643], "images_per_second": 0.3, "prompt_tokens": 27, "response_tokens_est": 49, "n_tiles": 12, "output_text": "The main objects in the image are a man in a suit and a computer setup. The man is positioned in the foreground, standing near the computer setup. The computer setup is located in the background, slightly to the right of the man.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24836.8, "ram_available_mb": 100935.4, "ram_percent": 19.7}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 24834.1, "ram_available_mb": 100938.1, "ram_percent": 19.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.2, "peak": 16.56, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.79, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 34.7, "peak": 44.91, "min": 26.0}, "VIN": {"avg": 77.39, "peak": 110.81, "min": 55.41}}, "power_watts_avg": 34.7, "energy_joules_est": 115.8, "sample_count": 26, "duration_seconds": 3.337}, "timestamp": "2026-01-17T14:39:54.473117"}
{"image_index": 95, "image_name": "000000009483.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009483.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 4050.816, "latencies_ms": [4050.816], "images_per_second": 0.247, "prompt_tokens": 21, "response_tokens_est": 70, "n_tiles": 12, "output_text": "The image depicts a man standing in an office or workspace, with a computer setup in front of him. The room appears to be well-lit, with a window providing natural light. The man is dressed in a dark suit and tie, and he seems to be engaged in a task, possibly working on a computer or using a device.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24834.1, "ram_available_mb": 100938.1, "ram_percent": 19.7}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 24834.1, "ram_available_mb": 100938.1, "ram_percent": 19.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.19, "peak": 16.56, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.7, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 33.17, "peak": 45.3, "min": 26.01}, "VIN": {"avg": 76.93, "peak": 114.62, "min": 62.69}}, "power_watts_avg": 33.17, "energy_joules_est": 134.39, "sample_count": 31, "duration_seconds": 4.051}, "timestamp": "2026-01-17T14:39:58.534599"}
{"image_index": 95, "image_name": "000000009483.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009483.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3124.487, "latencies_ms": [3124.487], "images_per_second": 0.32, "prompt_tokens": 19, "response_tokens_est": 42, "n_tiles": 12, "output_text": "The image depicts a well-lit office environment with a white wall and a window with a partially drawn curtain. The desk is equipped with a computer monitor, keyboard, mouse, and various electronic devices.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24834.1, "ram_available_mb": 100938.1, "ram_percent": 19.7}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 24834.1, "ram_available_mb": 100938.1, "ram_percent": 19.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.06, "peak": 16.56, "min": 14.02}, "VDD_CPU_SOC_MSS": {"avg": 15.58, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 35.08, "peak": 45.69, "min": 26.01}, "VIN": {"avg": 76.04, "peak": 111.56, "min": 54.55}}, "power_watts_avg": 35.08, "energy_joules_est": 109.63, "sample_count": 24, "duration_seconds": 3.125}, "timestamp": "2026-01-17T14:40:01.667351"}
{"image_index": 96, "image_name": "000000009590.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009590.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1879.102, "latencies_ms": [1879.102], "images_per_second": 0.532, "prompt_tokens": 9, "response_tokens_est": 47, "n_tiles": 6, "output_text": "The image depicts a group of four men sitting around a wooden dining table, engaged in a casual conversation, with various items on the table, including food, drinks, and personal belongings, suggesting a relaxed and informal gathering.", "error": null, "sys_before": {"cpu_percent": 41.2, "ram_used_mb": 24860.4, "ram_available_mb": 100911.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 24860.2, "ram_available_mb": 100912.0, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.69, "peak": 15.11, "min": 13.51}, "VDD_CPU_SOC_MSS": {"avg": 15.38, "peak": 15.75, "min": 14.18}, "VDD_GPU": {"avg": 31.2, "peak": 40.57, "min": 24.03}, "VIN": {"avg": 73.69, "peak": 116.35, "min": 57.39}}, "power_watts_avg": 31.2, "energy_joules_est": 58.64, "sample_count": 14, "duration_seconds": 1.879}, "timestamp": "2026-01-17T14:40:03.611942"}
{"image_index": 96, "image_name": "000000009590.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009590.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1544.588, "latencies_ms": [1544.588], "images_per_second": 0.647, "prompt_tokens": 23, "response_tokens_est": 33, "n_tiles": 6, "output_text": "1. Table\n2. People\n3. Coffee\n4. Cups\n5. Bowl\n6. Bottle\n7. Glass\n8. Plate", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 24860.2, "ram_available_mb": 100912.0, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 24861.4, "ram_available_mb": 100910.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.98, "peak": 15.44, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.62, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 32.89, "peak": 40.16, "min": 25.21}, "VIN": {"avg": 77.22, "peak": 126.48, "min": 61.82}}, "power_watts_avg": 32.89, "energy_joules_est": 50.82, "sample_count": 12, "duration_seconds": 1.545}, "timestamp": "2026-01-17T14:40:05.163323"}
{"image_index": 96, "image_name": "000000009590.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009590.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2792.602, "latencies_ms": [2792.602], "images_per_second": 0.358, "prompt_tokens": 27, "response_tokens_est": 79, "n_tiles": 6, "output_text": "The main objects in the image are a group of people sitting around a wooden dining table. The table is the central focus, with various items placed on it, including a coffee maker, a bottle of sauce, and bowls of food. The people are positioned in the background, with the foreground showing the legs of one person and the table. The lighting is warm, creating a cozy atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24861.4, "ram_available_mb": 100910.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 24862.4, "ram_available_mb": 100909.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.05, "peak": 15.44, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.75, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 29.26, "peak": 41.34, "min": 23.25}, "VIN": {"avg": 73.21, "peak": 117.98, "min": 61.14}}, "power_watts_avg": 29.26, "energy_joules_est": 81.72, "sample_count": 21, "duration_seconds": 2.793}, "timestamp": "2026-01-17T14:40:07.961667"}
{"image_index": 96, "image_name": "000000009590.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009590.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2436.813, "latencies_ms": [2436.813], "images_per_second": 0.41, "prompt_tokens": 21, "response_tokens_est": 66, "n_tiles": 6, "output_text": "The image depicts a cozy indoor setting where four men are gathered around a wooden dining table. They appear to be enjoying a meal together, with some of them holding cups and plates, while others are engaged in conversation. The room is warmly lit by natural light coming through a window, creating a relaxed and intimate atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24862.4, "ram_available_mb": 100909.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 24862.1, "ram_available_mb": 100910.0, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.05, "peak": 15.44, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.64, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 29.5, "peak": 40.18, "min": 23.64}, "VIN": {"avg": 72.05, "peak": 113.58, "min": 59.65}}, "power_watts_avg": 29.5, "energy_joules_est": 71.9, "sample_count": 19, "duration_seconds": 2.437}, "timestamp": "2026-01-17T14:40:10.404427"}
{"image_index": 96, "image_name": "000000009590.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009590.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2380.783, "latencies_ms": [2380.783], "images_per_second": 0.42, "prompt_tokens": 19, "response_tokens_est": 64, "n_tiles": 6, "output_text": "The image depicts a cozy dining room with a wooden ceiling and walls, featuring natural light streaming in through large windows with light-colored curtains. The room is filled with various items on the wooden table, including a coffee maker, a bottle of sauce, and a bowl of fruit, creating a warm and inviting atmosphere.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 24862.1, "ram_available_mb": 100910.0, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 24862.4, "ram_available_mb": 100909.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.03, "peak": 15.44, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.64, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 29.87, "peak": 40.18, "min": 23.64}, "VIN": {"avg": 71.42, "peak": 107.15, "min": 59.61}}, "power_watts_avg": 29.87, "energy_joules_est": 71.13, "sample_count": 18, "duration_seconds": 2.381}, "timestamp": "2026-01-17T14:40:12.791097"}
{"image_index": 97, "image_name": "000000009769.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009769.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2591.718, "latencies_ms": [2591.718], "images_per_second": 0.386, "prompt_tokens": 9, "response_tokens_est": 29, "n_tiles": 12, "output_text": "A red pickup truck is parked on a snowy street, with a snow-covered roof and a snow-covered house in the background.", "error": null, "sys_before": {"cpu_percent": 41.2, "ram_used_mb": 24839.9, "ram_available_mb": 100932.3, "ram_percent": 19.7}, "sys_after": {"cpu_percent": 2.5, "ram_used_mb": 24841.1, "ram_available_mb": 100931.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.23, "peak": 16.76, "min": 13.79}, "VDD_CPU_SOC_MSS": {"avg": 15.96, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 35.87, "peak": 44.12, "min": 27.18}, "VIN": {"avg": 82.56, "peak": 137.41, "min": 61.79}}, "power_watts_avg": 35.87, "energy_joules_est": 92.97, "sample_count": 20, "duration_seconds": 2.592}, "timestamp": "2026-01-17T14:40:15.459321"}
{"image_index": 97, "image_name": "000000009769.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009769.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 3470.962, "latencies_ms": [3470.962], "images_per_second": 0.288, "prompt_tokens": 23, "response_tokens_est": 53, "n_tiles": 12, "output_text": "1. Snow\n2. Snow-covered truck\n3. Snow-covered house\n4. Snow-covered trees\n5. Snow-covered driveway\n6. Snow-covered sidewalk\n7. Snow-covered car\n8. Snow-covered vehicle", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24841.1, "ram_available_mb": 100931.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 24842.3, "ram_available_mb": 100929.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.19, "peak": 16.56, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.67, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 34.46, "peak": 45.68, "min": 26.0}, "VIN": {"avg": 73.34, "peak": 118.04, "min": 58.36}}, "power_watts_avg": 34.46, "energy_joules_est": 119.62, "sample_count": 27, "duration_seconds": 3.471}, "timestamp": "2026-01-17T14:40:18.937573"}
{"image_index": 97, "image_name": "000000009769.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009769.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 4192.801, "latencies_ms": [4192.801], "images_per_second": 0.239, "prompt_tokens": 27, "response_tokens_est": 74, "n_tiles": 12, "output_text": "The main objects in the image are a red pickup truck and a snow-covered residential area. The truck is parked on the right side of the image, with its front facing the viewer. The residential area is in the background, with houses and trees visible. The truck is positioned near the snow-covered sidewalk, and the houses are further back in the scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24842.3, "ram_available_mb": 100929.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 24842.3, "ram_available_mb": 100929.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.16, "peak": 16.56, "min": 14.12}, "VDD_CPU_SOC_MSS": {"avg": 15.6, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 32.77, "peak": 44.91, "min": 26.01}, "VIN": {"avg": 74.46, "peak": 113.76, "min": 55.38}}, "power_watts_avg": 32.77, "energy_joules_est": 137.41, "sample_count": 33, "duration_seconds": 4.193}, "timestamp": "2026-01-17T14:40:23.137043"}
{"image_index": 97, "image_name": "000000009769.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009769.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3912.851, "latencies_ms": [3912.851], "images_per_second": 0.256, "prompt_tokens": 21, "response_tokens_est": 66, "n_tiles": 12, "output_text": "The image depicts a snowy residential area with a red pickup truck parked on the side of a snow-covered street. A person is seen clearing snow from the truck's bed, indicating ongoing snow removal efforts. The scene is set in a typical winter environment with snow-covered trees, houses, and a residential street.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24842.3, "ram_available_mb": 100929.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 24842.3, "ram_available_mb": 100929.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.19, "peak": 16.66, "min": 14.12}, "VDD_CPU_SOC_MSS": {"avg": 15.66, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 33.23, "peak": 45.3, "min": 25.6}, "VIN": {"avg": 76.83, "peak": 123.35, "min": 57.31}}, "power_watts_avg": 33.23, "energy_joules_est": 130.04, "sample_count": 31, "duration_seconds": 3.913}, "timestamp": "2026-01-17T14:40:27.056505"}
{"image_index": 97, "image_name": "000000009769.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009769.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3839.115, "latencies_ms": [3839.115], "images_per_second": 0.26, "prompt_tokens": 19, "response_tokens_est": 64, "n_tiles": 12, "output_text": "The image depicts a snowy scene with a red pickup truck parked on a snow-covered street. The truck is covered in snow, and the surrounding area is blanketed in white snow, indicating a winter setting. The lighting is subdued, with a grayish tone, suggesting an overcast day.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24842.3, "ram_available_mb": 100929.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 24843.5, "ram_available_mb": 100928.7, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.19, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.79, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 33.4, "peak": 44.89, "min": 26.01}, "VIN": {"avg": 77.98, "peak": 122.86, "min": 60.91}}, "power_watts_avg": 33.4, "energy_joules_est": 128.24, "sample_count": 30, "duration_seconds": 3.84}, "timestamp": "2026-01-17T14:40:30.902290"}
{"image_index": 98, "image_name": "000000009772.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009772.jpg", "image_width": 550, "image_height": 640, "image_resolution": "550x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2525.842, "latencies_ms": [2525.842], "images_per_second": 0.396, "prompt_tokens": 9, "response_tokens_est": 27, "n_tiles": 12, "output_text": "The image shows a luxurious bathroom with a marble countertop, gold faucets, and a mirror reflecting a person taking a photo.", "error": null, "sys_before": {"cpu_percent": 44.1, "ram_used_mb": 24836.8, "ram_available_mb": 100935.4, "ram_percent": 19.7}, "sys_after": {"cpu_percent": 2.6, "ram_used_mb": 24838.5, "ram_available_mb": 100933.7, "ram_percent": 19.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.16, "peak": 16.66, "min": 13.29}, "VDD_CPU_SOC_MSS": {"avg": 15.91, "peak": 17.71, "min": 14.18}, "VDD_GPU": {"avg": 36.36, "peak": 44.1, "min": 27.97}, "VIN": {"avg": 74.69, "peak": 102.12, "min": 57.04}}, "power_watts_avg": 36.36, "energy_joules_est": 91.85, "sample_count": 19, "duration_seconds": 2.526}, "timestamp": "2026-01-17T14:40:33.527264"}
{"image_index": 98, "image_name": "000000009772.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009772.jpg", "image_width": 550, "image_height": 640, "image_resolution": "550x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 3010.201, "latencies_ms": [3010.201], "images_per_second": 0.332, "prompt_tokens": 23, "response_tokens_est": 40, "n_tiles": 12, "output_text": "object: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24838.5, "ram_available_mb": 100933.7, "ram_percent": 19.7}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 24840.2, "ram_available_mb": 100931.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.3, "peak": 16.66, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.85, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 35.83, "peak": 44.91, "min": 26.79}, "VIN": {"avg": 76.2, "peak": 113.55, "min": 64.56}}, "power_watts_avg": 35.83, "energy_joules_est": 107.87, "sample_count": 23, "duration_seconds": 3.011}, "timestamp": "2026-01-17T14:40:36.543975"}
{"image_index": 98, "image_name": "000000009772.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009772.jpg", "image_width": 550, "image_height": 640, "image_resolution": "550x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3876.271, "latencies_ms": [3876.271], "images_per_second": 0.258, "prompt_tokens": 27, "response_tokens_est": 66, "n_tiles": 12, "output_text": "The main objects in the image are located in the foreground, with the bathroom sink and mirror being the most prominent features. The sink and mirror are positioned near the center of the image, with the bathroom tiles and fixtures surrounding them. The bathroom is also partially visible in the background, with a doorway leading to another room.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24840.2, "ram_available_mb": 100931.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 24840.5, "ram_available_mb": 100931.7, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.27, "peak": 16.66, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.77, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 33.54, "peak": 44.89, "min": 26.0}, "VIN": {"avg": 73.92, "peak": 111.18, "min": 61.67}}, "power_watts_avg": 33.54, "energy_joules_est": 130.02, "sample_count": 30, "duration_seconds": 3.877}, "timestamp": "2026-01-17T14:40:40.426923"}
{"image_index": 98, "image_name": "000000009772.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009772.jpg", "image_width": 550, "image_height": 640, "image_resolution": "550x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2876.403, "latencies_ms": [2876.403], "images_per_second": 0.348, "prompt_tokens": 21, "response_tokens_est": 36, "n_tiles": 12, "output_text": "The image depicts a luxurious bathroom with a marble countertop and a large mirror. A man is taking a selfie in the mirror, capturing the bathroom's elegance.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24840.5, "ram_available_mb": 100931.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 24841.0, "ram_available_mb": 100931.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.3, "peak": 16.56, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.87, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 35.79, "peak": 44.51, "min": 26.8}, "VIN": {"avg": 80.75, "peak": 122.2, "min": 64.54}}, "power_watts_avg": 35.79, "energy_joules_est": 102.96, "sample_count": 22, "duration_seconds": 2.877}, "timestamp": "2026-01-17T14:40:43.309859"}
{"image_index": 98, "image_name": "000000009772.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009772.jpg", "image_width": 550, "image_height": 640, "image_resolution": "550x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2729.96, "latencies_ms": [2729.96], "images_per_second": 0.366, "prompt_tokens": 19, "response_tokens_est": 32, "n_tiles": 12, "output_text": "The bathroom features a warm, inviting ambiance with beige walls and a marble countertop. The lighting is soft and warm, creating a cozy atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24841.0, "ram_available_mb": 100931.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 24841.9, "ram_available_mb": 100930.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.35, "peak": 16.66, "min": 14.43}, "VDD_CPU_SOC_MSS": {"avg": 15.88, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 36.76, "peak": 44.89, "min": 27.18}, "VIN": {"avg": 85.44, "peak": 125.52, "min": 66.08}}, "power_watts_avg": 36.76, "energy_joules_est": 100.37, "sample_count": 21, "duration_seconds": 2.73}, "timestamp": "2026-01-17T14:40:46.046235"}
{"image_index": 99, "image_name": "000000009891.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009891.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2177.322, "latencies_ms": [2177.322], "images_per_second": 0.459, "prompt_tokens": 9, "response_tokens_est": 17, "n_tiles": 12, "output_text": "A group of people are seen loading luggage into a white SUV at an airport.", "error": null, "sys_before": {"cpu_percent": 41.1, "ram_used_mb": 24849.6, "ram_available_mb": 100922.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.1, "ram_used_mb": 24851.1, "ram_available_mb": 100921.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.23, "peak": 16.66, "min": 13.51}, "VDD_CPU_SOC_MSS": {"avg": 16.05, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 37.56, "peak": 44.51, "min": 29.54}, "VIN": {"avg": 84.28, "peak": 128.82, "min": 65.04}}, "power_watts_avg": 37.56, "energy_joules_est": 81.8, "sample_count": 17, "duration_seconds": 2.178}, "timestamp": "2026-01-17T14:40:48.329877"}
{"image_index": 99, "image_name": "000000009891.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009891.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2731.523, "latencies_ms": [2731.523], "images_per_second": 0.366, "prompt_tokens": 23, "response_tokens_est": 32, "n_tiles": 12, "output_text": "1. Car\n2. Car\n3. Car\n4. Car\n5. Car\n6. Car\n7. Car\n8. Car", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24851.1, "ram_available_mb": 100921.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 24851.8, "ram_available_mb": 100920.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.36, "peak": 16.66, "min": 14.43}, "VDD_CPU_SOC_MSS": {"avg": 15.97, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 37.19, "peak": 45.68, "min": 27.57}, "VIN": {"avg": 84.37, "peak": 126.02, "min": 66.96}}, "power_watts_avg": 37.19, "energy_joules_est": 101.6, "sample_count": 21, "duration_seconds": 2.732}, "timestamp": "2026-01-17T14:40:51.067704"}
{"image_index": 99, "image_name": "000000009891.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009891.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 4407.56, "latencies_ms": [4407.56], "images_per_second": 0.227, "prompt_tokens": 27, "response_tokens_est": 81, "n_tiles": 12, "output_text": "In the image, the main objects are a luggage cart, a man, and a white SUV. The luggage cart is positioned near the foreground, with a man standing beside it. The man is handling a large black suitcase. The white SUV is in the background, slightly to the right. The man is standing near the SUV, with his attention directed towards the luggage cart and the suitcase.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24851.8, "ram_available_mb": 100920.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 24852.0, "ram_available_mb": 100920.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.23, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.87, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 32.74, "peak": 45.3, "min": 26.0}, "VIN": {"avg": 76.73, "peak": 113.8, "min": 67.51}}, "power_watts_avg": 32.74, "energy_joules_est": 144.32, "sample_count": 34, "duration_seconds": 4.408}, "timestamp": "2026-01-17T14:40:55.485463"}
{"image_index": 99, "image_name": "000000009891.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009891.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 4213.349, "latencies_ms": [4213.349], "images_per_second": 0.237, "prompt_tokens": 21, "response_tokens_est": 75, "n_tiles": 12, "output_text": "The image depicts a scene inside a car dealership, specifically at the entrance where a group of people is gathered. They appear to be preparing to enter a white SUV, with one person carrying a black suitcase and another holding a black bag. The setting is indoors, with a high ceiling and artificial lighting, and there are various other vehicles and people in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24852.0, "ram_available_mb": 100920.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 24852.8, "ram_available_mb": 100919.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.17, "peak": 16.56, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.84, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 32.81, "peak": 45.3, "min": 26.0}, "VIN": {"avg": 74.39, "peak": 113.15, "min": 55.56}}, "power_watts_avg": 32.81, "energy_joules_est": 138.25, "sample_count": 33, "duration_seconds": 4.214}, "timestamp": "2026-01-17T14:40:59.705224"}
{"image_index": 99, "image_name": "000000009891.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009891.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3651.497, "latencies_ms": [3651.497], "images_per_second": 0.274, "prompt_tokens": 19, "response_tokens_est": 58, "n_tiles": 12, "output_text": "The image depicts a scene inside a car dealership, with a white SUV parked in the background. The lighting is bright, illuminating the interior and the vehicles. The materials used in the scene include metal for the car and luggage, and the flooring appears to be made of concrete.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24852.8, "ram_available_mb": 100919.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 24852.8, "ram_available_mb": 100919.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.95, "peak": 16.26, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.52, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 33.97, "peak": 45.69, "min": 25.6}, "VIN": {"avg": 74.08, "peak": 117.1, "min": 49.07}}, "power_watts_avg": 33.97, "energy_joules_est": 124.05, "sample_count": 29, "duration_seconds": 3.652}, "timestamp": "2026-01-17T14:41:03.362721"}
{"image_index": 100, "image_name": "000000009914.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009914.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2804.379, "latencies_ms": [2804.379], "images_per_second": 0.357, "prompt_tokens": 9, "response_tokens_est": 35, "n_tiles": 12, "output_text": "The image shows a plate of food consisting of a grilled chicken sandwich with sesame seeds on top, accompanied by a side of French fries and a small bowl of sauce.", "error": null, "sys_before": {"cpu_percent": 44.4, "ram_used_mb": 24859.1, "ram_available_mb": 100913.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 24856.1, "ram_available_mb": 100916.0, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.09, "peak": 16.76, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 15.69, "peak": 17.71, "min": 13.78}, "VDD_GPU": {"avg": 35.11, "peak": 44.51, "min": 26.8}, "VIN": {"avg": 77.28, "peak": 115.79, "min": 58.61}}, "power_watts_avg": 35.11, "energy_joules_est": 98.48, "sample_count": 22, "duration_seconds": 2.805}, "timestamp": "2026-01-17T14:41:06.278852"}
{"image_index": 100, "image_name": "000000009914.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009914.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 3193.685, "latencies_ms": [3193.685], "images_per_second": 0.313, "prompt_tokens": 23, "response_tokens_est": 45, "n_tiles": 12, "output_text": "1. Bun\n2. Potato chips\n3. Cheese\n4. Sausage\n5. Sausage\n6. Sausage\n7. Sausage\n8. Sausage", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24856.1, "ram_available_mb": 100916.0, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 24857.3, "ram_available_mb": 100914.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.23, "peak": 16.56, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.76, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 35.15, "peak": 46.07, "min": 26.39}, "VIN": {"avg": 80.76, "peak": 130.69, "min": 63.01}}, "power_watts_avg": 35.15, "energy_joules_est": 112.28, "sample_count": 25, "duration_seconds": 3.194}, "timestamp": "2026-01-17T14:41:09.479409"}
{"image_index": 100, "image_name": "000000009914.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009914.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3357.84, "latencies_ms": [3357.84], "images_per_second": 0.298, "prompt_tokens": 27, "response_tokens_est": 50, "n_tiles": 12, "output_text": "The main objects in the image are a hamburger and a side of fries. The hamburger is positioned in the foreground, with a sesame seed bun and a grilled patty. The fries are in the background, slightly out of focus.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24857.3, "ram_available_mb": 100914.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 24855.1, "ram_available_mb": 100917.0, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.23, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.79, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 34.74, "peak": 45.3, "min": 26.0}, "VIN": {"avg": 77.06, "peak": 110.78, "min": 65.71}}, "power_watts_avg": 34.74, "energy_joules_est": 116.67, "sample_count": 26, "duration_seconds": 3.358}, "timestamp": "2026-01-17T14:41:12.843956"}
{"image_index": 100, "image_name": "000000009914.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009914.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3398.702, "latencies_ms": [3398.702], "images_per_second": 0.294, "prompt_tokens": 21, "response_tokens_est": 51, "n_tiles": 12, "output_text": "The image depicts a plate of food, specifically a hamburger and fries, accompanied by a side of ketchup. The setting appears to be a casual dining environment, possibly a fast-food restaurant, with a focus on the food items.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24855.1, "ram_available_mb": 100917.0, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 24855.1, "ram_available_mb": 100917.0, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.21, "peak": 16.56, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.76, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 34.67, "peak": 45.3, "min": 26.0}, "VIN": {"avg": 75.1, "peak": 112.76, "min": 61.72}}, "power_watts_avg": 34.67, "energy_joules_est": 117.85, "sample_count": 26, "duration_seconds": 3.399}, "timestamp": "2026-01-17T14:41:16.249055"}
{"image_index": 100, "image_name": "000000009914.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009914.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3695.793, "latencies_ms": [3695.793], "images_per_second": 0.271, "prompt_tokens": 19, "response_tokens_est": 60, "n_tiles": 12, "output_text": "The image features a plate of food, including a grilled chicken sandwich with sesame seeds, a side of fries, and a small bowl of sauce. The lighting is bright, and the colors are vibrant, with the golden-brown fries contrasting against the white plate and the dark blue table.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24855.1, "ram_available_mb": 100917.0, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 24855.7, "ram_available_mb": 100916.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.21, "peak": 16.56, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.87, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 33.87, "peak": 44.89, "min": 26.0}, "VIN": {"avg": 75.45, "peak": 118.25, "min": 63.52}}, "power_watts_avg": 33.87, "energy_joules_est": 125.19, "sample_count": 28, "duration_seconds": 3.696}, "timestamp": "2026-01-17T14:41:19.951927"}
{"image_index": 101, "image_name": "000000010092.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000010092.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1951.167, "latencies_ms": [1951.167], "images_per_second": 0.513, "prompt_tokens": 9, "response_tokens_est": 50, "n_tiles": 6, "output_text": "The image depicts a cozy, well-lit room with a large bed covered by a green mosquito net, a small wooden table with a candle, and a wooden chair, all set against a backdrop of yellow walls and large windows with curtains.", "error": null, "sys_before": {"cpu_percent": 38.6, "ram_used_mb": 24867.4, "ram_available_mb": 100904.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 24868.1, "ram_available_mb": 100904.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.78, "peak": 15.11, "min": 13.72}, "VDD_CPU_SOC_MSS": {"avg": 15.43, "peak": 16.14, "min": 14.17}, "VDD_GPU": {"avg": 30.88, "peak": 40.57, "min": 24.03}, "VIN": {"avg": 71.62, "peak": 101.32, "min": 58.92}}, "power_watts_avg": 30.88, "energy_joules_est": 60.26, "sample_count": 15, "duration_seconds": 1.951}, "timestamp": "2026-01-17T14:41:21.970380"}
{"image_index": 101, "image_name": "000000010092.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000010092.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1834.687, "latencies_ms": [1834.687], "images_per_second": 0.545, "prompt_tokens": 23, "response_tokens_est": 44, "n_tiles": 6, "output_text": "bed: 1\ntable: 1\nchair: 1\ntable lamp: 1\nwindow: 2\ncurtain: 2\nwall art: 2\nfloor lamp: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24868.1, "ram_available_mb": 100904.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 24868.1, "ram_available_mb": 100904.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.07, "peak": 15.54, "min": 14.43}, "VDD_CPU_SOC_MSS": {"avg": 15.66, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 31.63, "peak": 40.16, "min": 24.43}, "VIN": {"avg": 75.85, "peak": 113.37, "min": 58.71}}, "power_watts_avg": 31.63, "energy_joules_est": 58.04, "sample_count": 14, "duration_seconds": 1.835}, "timestamp": "2026-01-17T14:41:23.811079"}
{"image_index": 101, "image_name": "000000010092.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000010092.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2986.688, "latencies_ms": [2986.688], "images_per_second": 0.335, "prompt_tokens": 27, "response_tokens_est": 83, "n_tiles": 6, "output_text": "The main object in the foreground is a bed with a green canopy, which is positioned near the center of the image. The bed is surrounded by a wooden frame and is partially covered by a green sheet. In the background, there is a wooden table with a lamp on it, and a window with curtains. The room has a tiled floor and wooden furniture, creating a cozy and natural ambiance.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24868.1, "ram_available_mb": 100904.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 24869.6, "ram_available_mb": 100902.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.92, "peak": 15.34, "min": 14.43}, "VDD_CPU_SOC_MSS": {"avg": 15.58, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 28.31, "peak": 40.97, "min": 22.85}, "VIN": {"avg": 69.12, "peak": 110.05, "min": 60.64}}, "power_watts_avg": 28.31, "energy_joules_est": 84.56, "sample_count": 23, "duration_seconds": 2.987}, "timestamp": "2026-01-17T14:41:26.803892"}
{"image_index": 101, "image_name": "000000010092.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000010092.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2452.669, "latencies_ms": [2452.669], "images_per_second": 0.408, "prompt_tokens": 21, "response_tokens_est": 64, "n_tiles": 6, "output_text": "The image depicts a cozy, well-lit room with a large bed covered in a green mosquito net. The room features wooden furniture, including a table and chairs, and is decorated with paintings and curtains. The setting appears to be a bedroom or a guest room, with natural light streaming in through large windows.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24869.6, "ram_available_mb": 100902.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 24870.6, "ram_available_mb": 100901.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.86, "peak": 15.24, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.48, "peak": 16.14, "min": 14.57}, "VDD_GPU": {"avg": 29.15, "peak": 40.16, "min": 22.85}, "VIN": {"avg": 72.16, "peak": 114.92, "min": 57.05}}, "power_watts_avg": 29.15, "energy_joules_est": 71.51, "sample_count": 19, "duration_seconds": 2.453}, "timestamp": "2026-01-17T14:41:29.263423"}
{"image_index": 101, "image_name": "000000010092.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000010092.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1879.571, "latencies_ms": [1879.571], "images_per_second": 0.532, "prompt_tokens": 19, "response_tokens_est": 44, "n_tiles": 6, "output_text": "The room is brightly lit with natural light streaming through large windows adorned with sheer curtains. The walls are painted in a warm, earthy tone, complemented by wooden furniture and a green canopy bed.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24870.6, "ram_available_mb": 100901.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 24871.8, "ram_available_mb": 100900.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.83, "peak": 15.24, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.35, "peak": 15.75, "min": 14.56}, "VDD_GPU": {"avg": 31.28, "peak": 40.16, "min": 24.03}, "VIN": {"avg": 71.82, "peak": 110.94, "min": 61.6}}, "power_watts_avg": 31.28, "energy_joules_est": 58.81, "sample_count": 14, "duration_seconds": 1.88}, "timestamp": "2026-01-17T14:41:31.149425"}
{"image_index": 102, "image_name": "000000010363.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000010363.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 691.683, "latencies_ms": [691.683], "images_per_second": 1.446, "prompt_tokens": 9, "response_tokens_est": 23, "n_tiles": 2, "output_text": "A gray and white cat is perched on the hood of a black car, seemingly curious about the surroundings.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 24871.8, "ram_available_mb": 100900.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 24872.0, "ram_available_mb": 100900.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5055.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.63, "peak": 15.01, "min": 14.2}, "VDD_CPU_SOC_MSS": {"avg": 15.44, "peak": 16.15, "min": 14.96}, "VDD_GPU": {"avg": 27.73, "peak": 32.3, "min": 24.03}, "VIN": {"avg": 67.41, "peak": 77.47, "min": 59.53}}, "power_watts_avg": 27.73, "energy_joules_est": 19.19, "sample_count": 5, "duration_seconds": 0.692}, "timestamp": "2026-01-17T14:41:31.866477"}
{"image_index": 102, "image_name": "000000010363.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000010363.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 952.303, "latencies_ms": [952.303], "images_per_second": 1.05, "prompt_tokens": 23, "response_tokens_est": 34, "n_tiles": 2, "output_text": "1. Cat\n2. Car\n3. Lamp\n4. Floor\n5. Floor lamp\n6. Table\n7. Table lamp\n8. Box", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24872.0, "ram_available_mb": 100900.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 24872.3, "ram_available_mb": 100899.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5064.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.93, "peak": 15.22, "min": 14.71}, "VDD_CPU_SOC_MSS": {"avg": 15.98, "peak": 16.54, "min": 15.36}, "VDD_GPU": {"avg": 26.62, "peak": 32.68, "min": 22.45}, "VIN": {"avg": 67.79, "peak": 97.18, "min": 54.81}}, "power_watts_avg": 26.62, "energy_joules_est": 25.36, "sample_count": 7, "duration_seconds": 0.953}, "timestamp": "2026-01-17T14:41:32.824635"}
{"image_index": 102, "image_name": "000000010363.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000010363.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1899.381, "latencies_ms": [1899.381], "images_per_second": 0.526, "prompt_tokens": 27, "response_tokens_est": 75, "n_tiles": 2, "output_text": "In the image, there is a black car parked in the foreground, with a cat standing on its roof. The cat is positioned near the front of the car, near the windshield. The background features a cluttered kitchen counter with various items, including a lamp, a box, and a bicycle. The cat appears to be curious and is looking towards the camera.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24872.3, "ram_available_mb": 100899.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 24872.0, "ram_available_mb": 100900.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5067.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.11, "peak": 15.42, "min": 14.71}, "VDD_CPU_SOC_MSS": {"avg": 16.34, "peak": 16.54, "min": 15.75}, "VDD_GPU": {"avg": 23.72, "peak": 31.51, "min": 20.88}, "VIN": {"avg": 66.97, "peak": 104.99, "min": 58.36}}, "power_watts_avg": 23.72, "energy_joules_est": 45.06, "sample_count": 14, "duration_seconds": 1.9}, "timestamp": "2026-01-17T14:41:34.729940"}
{"image_index": 102, "image_name": "000000010363.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000010363.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1138.885, "latencies_ms": [1138.885], "images_per_second": 0.878, "prompt_tokens": 21, "response_tokens_est": 41, "n_tiles": 2, "output_text": "The image depicts a cozy, cluttered kitchen with various items on the countertops and shelves. A tabby cat is perched on the hood of a black car, looking around with curiosity.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24872.0, "ram_available_mb": 100900.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24872.0, "ram_available_mb": 100900.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5063.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.78, "peak": 15.22, "min": 14.51}, "VDD_CPU_SOC_MSS": {"avg": 15.95, "peak": 16.54, "min": 15.36}, "VDD_GPU": {"avg": 25.02, "peak": 30.33, "min": 21.67}, "VIN": {"avg": 67.33, "peak": 103.72, "min": 58.5}}, "power_watts_avg": 25.02, "energy_joules_est": 28.51, "sample_count": 8, "duration_seconds": 1.139}, "timestamp": "2026-01-17T14:41:35.874452"}
{"image_index": 102, "image_name": "000000010363.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000010363.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1269.307, "latencies_ms": [1269.307], "images_per_second": 0.788, "prompt_tokens": 19, "response_tokens_est": 47, "n_tiles": 2, "output_text": "The image features a black car with a reflective surface, and a gray and white striped cat is perched on the roof. The lighting is bright, and the scene appears to be indoors, possibly in a garage or workshop.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24872.0, "ram_available_mb": 100900.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 24872.8, "ram_available_mb": 100899.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5062.6, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.07, "peak": 15.42, "min": 14.61}, "VDD_CPU_SOC_MSS": {"avg": 15.79, "peak": 16.14, "min": 15.35}, "VDD_GPU": {"avg": 24.91, "peak": 30.33, "min": 21.67}, "VIN": {"avg": 66.25, "peak": 90.35, "min": 59.4}}, "power_watts_avg": 24.91, "energy_joules_est": 31.63, "sample_count": 9, "duration_seconds": 1.27}, "timestamp": "2026-01-17T14:41:37.153276"}
{"image_index": 103, "image_name": "000000010583.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000010583.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 980.428, "latencies_ms": [980.428], "images_per_second": 1.02, "prompt_tokens": 9, "response_tokens_est": 41, "n_tiles": 1, "output_text": "The image shows a plate of food, which includes a hearty meat dish topped with a layer of creamy gravy, garnished with chopped herbs, and served with a side of mashed potatoes.", "error": null, "sys_before": {"cpu_percent": 35.5, "ram_used_mb": 24872.8, "ram_available_mb": 100899.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24872.5, "ram_available_mb": 100899.7, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 4845.7, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.16, "peak": 15.52, "min": 14.61}, "VDD_CPU_SOC_MSS": {"avg": 16.2, "peak": 16.54, "min": 15.75}, "VDD_GPU": {"avg": 21.79, "peak": 24.04, "min": 20.49}, "VIN": {"avg": 62.65, "peak": 65.51, "min": 54.69}}, "power_watts_avg": 21.79, "energy_joules_est": 21.37, "sample_count": 7, "duration_seconds": 0.981}, "timestamp": "2026-01-17T14:41:38.156909"}
{"image_index": 103, "image_name": "000000010583.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000010583.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2901.539, "latencies_ms": [2901.539], "images_per_second": 0.345, "prompt_tokens": 23, "response_tokens_est": 128, "n_tiles": 1, "output_text": "object: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject:", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24872.5, "ram_available_mb": 100899.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24873.2, "ram_available_mb": 100898.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 4856.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.63, "peak": 15.93, "min": 15.11}, "VDD_CPU_SOC_MSS": {"avg": 16.59, "peak": 16.93, "min": 16.14}, "VDD_GPU": {"avg": 20.43, "peak": 24.04, "min": 19.7}, "VIN": {"avg": 62.81, "peak": 64.7, "min": 57.53}}, "power_watts_avg": 20.43, "energy_joules_est": 59.28, "sample_count": 22, "duration_seconds": 2.902}, "timestamp": "2026-01-17T14:41:41.063900"}
{"image_index": 103, "image_name": "000000010583.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000010583.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1592.768, "latencies_ms": [1592.768], "images_per_second": 0.628, "prompt_tokens": 27, "response_tokens_est": 69, "n_tiles": 1, "output_text": "The main object in the image is a plate of food, which is positioned in the foreground. The plate is placed on a metal table, and there is a small portion of food on the left side of the plate. The background features a metal grate, and there is a small portion of food on the right side of the plate.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24873.2, "ram_available_mb": 100898.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24874.2, "ram_available_mb": 100898.0, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 4859.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.47, "peak": 15.62, "min": 15.11}, "VDD_CPU_SOC_MSS": {"avg": 16.5, "peak": 16.54, "min": 16.15}, "VDD_GPU": {"avg": 21.01, "peak": 24.03, "min": 19.7}, "VIN": {"avg": 63.83, "peak": 70.64, "min": 54.9}}, "power_watts_avg": 21.01, "energy_joules_est": 33.47, "sample_count": 12, "duration_seconds": 1.593}, "timestamp": "2026-01-17T14:41:42.662390"}
{"image_index": 103, "image_name": "000000010583.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000010583.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1480.881, "latencies_ms": [1480.881], "images_per_second": 0.675, "prompt_tokens": 21, "response_tokens_est": 64, "n_tiles": 1, "output_text": "The image depicts a plate of food, specifically a hearty meat dish with a creamy sauce, garnished with herbs, and topped with a layer of roasted vegetables. The setting appears to be a casual dining environment, possibly a restaurant, with a metal table and a partially visible plate of fries in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24874.2, "ram_available_mb": 100898.0, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24874.9, "ram_available_mb": 100897.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 4854.7, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.5, "peak": 15.62, "min": 15.22}, "VDD_CPU_SOC_MSS": {"avg": 16.61, "peak": 16.94, "min": 16.14}, "VDD_GPU": {"avg": 20.96, "peak": 23.64, "min": 19.7}, "VIN": {"avg": 63.17, "peak": 65.38, "min": 57.05}}, "power_watts_avg": 20.96, "energy_joules_est": 31.05, "sample_count": 11, "duration_seconds": 1.481}, "timestamp": "2026-01-17T14:41:44.148866"}
{"image_index": 103, "image_name": "000000010583.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000010583.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1836.155, "latencies_ms": [1836.155], "images_per_second": 0.545, "prompt_tokens": 19, "response_tokens_est": 80, "n_tiles": 1, "output_text": "The plate is white, with a glossy finish, and is placed on a metal table. The food consists of a rich, dark brown sauce, a layer of mashed potatoes, and a layer of meat, topped with a layer of roasted vegetables and herbs. The lighting is bright, casting a warm glow on the food, and the overall ambiance suggests a cozy, inviting dining experience.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24874.9, "ram_available_mb": 100897.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24875.2, "ram_available_mb": 100897.0, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 4853.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.53, "peak": 15.72, "min": 15.22}, "VDD_CPU_SOC_MSS": {"avg": 16.59, "peak": 16.93, "min": 16.14}, "VDD_GPU": {"avg": 20.69, "peak": 23.64, "min": 19.7}, "VIN": {"avg": 63.58, "peak": 67.6, "min": 60.34}}, "power_watts_avg": 20.69, "energy_joules_est": 38.0, "sample_count": 14, "duration_seconds": 1.837}, "timestamp": "2026-01-17T14:41:45.990711"}
{"image_index": 104, "image_name": "000000010707.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000010707.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 3066.562, "latencies_ms": [3066.562], "images_per_second": 0.326, "prompt_tokens": 9, "response_tokens_est": 43, "n_tiles": 12, "output_text": "The image depicts a group of people gathered around a table in a cozy living room, with a man in a white t-shirt holding a controller and another man in a blue t-shirt playing a video game.", "error": null, "sys_before": {"cpu_percent": 43.0, "ram_used_mb": 24855.3, "ram_available_mb": 100916.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 24854.9, "ram_available_mb": 100917.3, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.23, "peak": 16.66, "min": 14.0}, "VDD_CPU_SOC_MSS": {"avg": 15.93, "peak": 17.71, "min": 14.96}, "VDD_GPU": {"avg": 33.94, "peak": 43.33, "min": 26.01}, "VIN": {"avg": 78.69, "peak": 118.5, "min": 61.52}}, "power_watts_avg": 33.94, "energy_joules_est": 104.09, "sample_count": 24, "duration_seconds": 3.067}, "timestamp": "2026-01-17T14:41:49.127317"}
{"image_index": 104, "image_name": "000000010707.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000010707.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2847.694, "latencies_ms": [2847.694], "images_per_second": 0.351, "prompt_tokens": 23, "response_tokens_est": 35, "n_tiles": 12, "output_text": "1. Couch\n2. Table\n3. People\n4. Laptop\n5. Beverages\n6. Cans\n7. Bottle\n8. Remote control", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24854.9, "ram_available_mb": 100917.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 24856.0, "ram_available_mb": 100916.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.28, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.89, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 36.08, "peak": 44.51, "min": 26.79}, "VIN": {"avg": 76.86, "peak": 111.18, "min": 63.59}}, "power_watts_avg": 36.08, "energy_joules_est": 102.76, "sample_count": 22, "duration_seconds": 2.848}, "timestamp": "2026-01-17T14:41:51.982111"}
{"image_index": 104, "image_name": "000000010707.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000010707.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 4221.615, "latencies_ms": [4221.615], "images_per_second": 0.237, "prompt_tokens": 27, "response_tokens_est": 75, "n_tiles": 12, "output_text": "The main objects in the image are a group of people and various items on a table. The people are seated on a couch, with one person holding a bottle. The table is located in the foreground, with various items such as cans, a remote, and a bottle of beer. The background features a window with blinds, a lamp, and a red couch.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24856.0, "ram_available_mb": 100916.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 24856.5, "ram_available_mb": 100915.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.18, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.82, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 32.72, "peak": 44.91, "min": 25.61}, "VIN": {"avg": 74.56, "peak": 116.17, "min": 55.3}}, "power_watts_avg": 32.72, "energy_joules_est": 138.15, "sample_count": 33, "duration_seconds": 4.222}, "timestamp": "2026-01-17T14:41:56.211344"}
{"image_index": 104, "image_name": "000000010707.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000010707.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3942.803, "latencies_ms": [3942.803], "images_per_second": 0.254, "prompt_tokens": 21, "response_tokens_est": 67, "n_tiles": 12, "output_text": "The image depicts a cozy living room scene with a group of people gathered around a table. The room is warmly lit, and various items such as a laptop, a bottle, and a cup are scattered on the table. The people appear to be engaged in a casual, social gathering, possibly enjoying drinks and snacks together.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24856.5, "ram_available_mb": 100915.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 24857.5, "ram_available_mb": 100914.7, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.19, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.78, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 33.14, "peak": 44.91, "min": 25.6}, "VIN": {"avg": 75.86, "peak": 128.3, "min": 58.91}}, "power_watts_avg": 33.14, "energy_joules_est": 130.68, "sample_count": 31, "duration_seconds": 3.943}, "timestamp": "2026-01-17T14:42:00.164814"}
{"image_index": 104, "image_name": "000000010707.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000010707.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3231.67, "latencies_ms": [3231.67], "images_per_second": 0.309, "prompt_tokens": 19, "response_tokens_est": 46, "n_tiles": 12, "output_text": "The room is warmly lit with a yellowish hue, creating a cozy atmosphere. The furniture and objects in the room are made of various materials, including plaid blankets, a red couch, and a pink table.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24857.5, "ram_available_mb": 100914.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 24858.2, "ram_available_mb": 100913.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.96, "peak": 16.26, "min": 14.12}, "VDD_CPU_SOC_MSS": {"avg": 15.45, "peak": 16.92, "min": 14.56}, "VDD_GPU": {"avg": 34.7, "peak": 44.91, "min": 25.6}, "VIN": {"avg": 76.94, "peak": 115.65, "min": 51.15}}, "power_watts_avg": 34.7, "energy_joules_est": 112.15, "sample_count": 25, "duration_seconds": 3.232}, "timestamp": "2026-01-17T14:42:03.403148"}
{"image_index": 105, "image_name": "000000010764.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000010764.jpg", "image_width": 640, "image_height": 424, "image_resolution": "640x424", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1094.495, "latencies_ms": [1094.495], "images_per_second": 0.914, "prompt_tokens": 9, "response_tokens_est": 18, "n_tiles": 6, "output_text": "A baseball player is crouched on the field, ready to catch a ball.", "error": null, "sys_before": {"cpu_percent": 43.0, "ram_used_mb": 24868.5, "ram_available_mb": 100903.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 24869.9, "ram_available_mb": 100902.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.46, "peak": 15.04, "min": 13.61}, "VDD_CPU_SOC_MSS": {"avg": 15.11, "peak": 15.75, "min": 14.18}, "VDD_GPU": {"avg": 35.5, "peak": 40.56, "min": 29.54}, "VIN": {"avg": 73.43, "peak": 95.02, "min": 61.06}}, "power_watts_avg": 35.5, "energy_joules_est": 38.87, "sample_count": 8, "duration_seconds": 1.095}, "timestamp": "2026-01-17T14:42:04.558232"}
{"image_index": 105, "image_name": "000000010764.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000010764.jpg", "image_width": 640, "image_height": 424, "image_resolution": "640x424", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2220.445, "latencies_ms": [2220.445], "images_per_second": 0.45, "prompt_tokens": 23, "response_tokens_est": 58, "n_tiles": 6, "output_text": "baseball player: 1\ncatcher's gear: 1\nbaseball bat: 1\nbaseball glove: 1\nbaseball uniform: 1\nbaseball helmet: 1\nbaseball cleats: 1\nbaseball bat: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24869.9, "ram_available_mb": 100902.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 24870.7, "ram_available_mb": 100901.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.99, "peak": 15.44, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.63, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 30.86, "peak": 41.34, "min": 23.64}, "VIN": {"avg": 73.46, "peak": 107.6, "min": 61.41}}, "power_watts_avg": 30.86, "energy_joules_est": 68.53, "sample_count": 17, "duration_seconds": 2.221}, "timestamp": "2026-01-17T14:42:06.784535"}
{"image_index": 105, "image_name": "000000010764.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000010764.jpg", "image_width": 640, "image_height": 424, "image_resolution": "640x424", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2170.493, "latencies_ms": [2170.493], "images_per_second": 0.461, "prompt_tokens": 27, "response_tokens_est": 56, "n_tiles": 6, "output_text": "The main object in the foreground is a baseball player crouched near the home plate, wearing a black and white uniform. The player is holding a baseball glove near their left hand. The background features a well-maintained green field with white lines marking the baseball diamond.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24870.7, "ram_available_mb": 100901.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 24874.2, "ram_available_mb": 100898.0, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.0, "peak": 15.44, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.67, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 30.63, "peak": 40.18, "min": 24.04}, "VIN": {"avg": 73.48, "peak": 110.11, "min": 62.74}}, "power_watts_avg": 30.63, "energy_joules_est": 66.49, "sample_count": 16, "duration_seconds": 2.171}, "timestamp": "2026-01-17T14:42:08.961581"}
{"image_index": 105, "image_name": "000000010764.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000010764.jpg", "image_width": 640, "image_height": 424, "image_resolution": "640x424", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2929.56, "latencies_ms": [2929.56], "images_per_second": 0.341, "prompt_tokens": 21, "response_tokens_est": 84, "n_tiles": 6, "output_text": "The image captures a baseball player in a white and black uniform crouched on the dirt infield of a baseball field. The player is wearing a protective helmet, gloves, and cleats, indicating that he is either preparing to catch a ball or has just completed a play. The field is well-maintained with white lines marking the bases, and the grass is green, suggesting it is a sunny day.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24874.2, "ram_available_mb": 100898.0, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 24873.8, "ram_available_mb": 100898.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.05, "peak": 15.44, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.68, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 28.74, "peak": 40.57, "min": 23.25}, "VIN": {"avg": 68.49, "peak": 100.25, "min": 57.14}}, "power_watts_avg": 28.74, "energy_joules_est": 84.21, "sample_count": 22, "duration_seconds": 2.93}, "timestamp": "2026-01-17T14:42:11.897252"}
{"image_index": 105, "image_name": "000000010764.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000010764.jpg", "image_width": 640, "image_height": 424, "image_resolution": "640x424", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2054.647, "latencies_ms": [2054.647], "images_per_second": 0.487, "prompt_tokens": 19, "response_tokens_est": 52, "n_tiles": 6, "output_text": "The baseball player is wearing a black and white uniform, complete with a helmet, chest protector, and batting gloves. The field is well-maintained with white chalk lines marking the playing area, and the lighting suggests it is daytime with clear skies.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24873.8, "ram_available_mb": 100898.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 24873.8, "ram_available_mb": 100898.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.02, "peak": 15.44, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.65, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 30.63, "peak": 40.56, "min": 23.64}, "VIN": {"avg": 70.04, "peak": 102.95, "min": 50.53}}, "power_watts_avg": 30.63, "energy_joules_est": 62.95, "sample_count": 16, "duration_seconds": 2.055}, "timestamp": "2026-01-17T14:42:13.958134"}
{"image_index": 106, "image_name": "000000010977.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000010977.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 3032.053, "latencies_ms": [3032.053], "images_per_second": 0.33, "prompt_tokens": 9, "response_tokens_est": 42, "n_tiles": 12, "output_text": "The image shows a bathroom with a wooden vanity, a white toilet, a pink tiled wall, a white towel hanging on the wall, a glass shower door, and a green mat on the floor.", "error": null, "sys_before": {"cpu_percent": 45.7, "ram_used_mb": 24847.0, "ram_available_mb": 100925.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 24848.9, "ram_available_mb": 100923.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.2, "peak": 16.76, "min": 13.79}, "VDD_CPU_SOC_MSS": {"avg": 15.88, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 34.58, "peak": 43.71, "min": 26.39}, "VIN": {"avg": 77.54, "peak": 112.56, "min": 57.86}}, "power_watts_avg": 34.58, "energy_joules_est": 104.86, "sample_count": 23, "duration_seconds": 3.032}, "timestamp": "2026-01-17T14:42:17.067226"}
{"image_index": 106, "image_name": "000000010977.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000010977.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 3020.82, "latencies_ms": [3020.82], "images_per_second": 0.331, "prompt_tokens": 23, "response_tokens_est": 40, "n_tiles": 12, "output_text": "object: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24848.9, "ram_available_mb": 100923.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 24851.1, "ram_available_mb": 100921.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.24, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.83, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 35.5, "peak": 44.51, "min": 26.39}, "VIN": {"avg": 75.68, "peak": 124.79, "min": 55.9}}, "power_watts_avg": 35.5, "energy_joules_est": 107.25, "sample_count": 23, "duration_seconds": 3.021}, "timestamp": "2026-01-17T14:42:20.094755"}
{"image_index": 106, "image_name": "000000010977.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000010977.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3741.274, "latencies_ms": [3741.274], "images_per_second": 0.267, "prompt_tokens": 27, "response_tokens_est": 61, "n_tiles": 12, "output_text": "The main objects in the image are a toilet and a bathtub. The toilet is located in the foreground, while the bathtub is situated in the background. The bathtub is partially visible through a glass door, and the toilet is positioned to the right of the bathtub.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24851.1, "ram_available_mb": 100921.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 24851.4, "ram_available_mb": 100920.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.19, "peak": 16.66, "min": 14.12}, "VDD_CPU_SOC_MSS": {"avg": 15.73, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 33.74, "peak": 44.89, "min": 26.0}, "VIN": {"avg": 75.74, "peak": 117.21, "min": 57.35}}, "power_watts_avg": 33.74, "energy_joules_est": 126.25, "sample_count": 29, "duration_seconds": 3.742}, "timestamp": "2026-01-17T14:42:23.843175"}
{"image_index": 106, "image_name": "000000010977.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000010977.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2820.064, "latencies_ms": [2820.064], "images_per_second": 0.355, "prompt_tokens": 21, "response_tokens_est": 34, "n_tiles": 12, "output_text": "The image shows a bathroom with a wooden vanity and a white toilet. The room is well-lit, and there is a window with a white curtain partially drawn.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24851.4, "ram_available_mb": 100920.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 24852.6, "ram_available_mb": 100919.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.26, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.85, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 36.13, "peak": 44.91, "min": 26.79}, "VIN": {"avg": 76.77, "peak": 109.43, "min": 64.98}}, "power_watts_avg": 36.13, "energy_joules_est": 101.91, "sample_count": 22, "duration_seconds": 2.821}, "timestamp": "2026-01-17T14:42:26.669429"}
{"image_index": 106, "image_name": "000000010977.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000010977.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3536.519, "latencies_ms": [3536.519], "images_per_second": 0.283, "prompt_tokens": 19, "response_tokens_est": 55, "n_tiles": 12, "output_text": "The bathroom features a pinkish-purple tiled wall, a white bathtub, a wooden vanity with a white sink, a white toilet, and a white towel hanging on the wall. The lighting is bright, and the overall color scheme is warm and inviting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24852.6, "ram_available_mb": 100919.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 24853.6, "ram_available_mb": 100918.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.2, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.69, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 34.31, "peak": 45.69, "min": 26.0}, "VIN": {"avg": 79.02, "peak": 124.6, "min": 63.77}}, "power_watts_avg": 34.31, "energy_joules_est": 121.35, "sample_count": 27, "duration_seconds": 3.537}, "timestamp": "2026-01-17T14:42:30.213386"}
{"image_index": 107, "image_name": "000000010995.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000010995.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2559.481, "latencies_ms": [2559.481], "images_per_second": 0.391, "prompt_tokens": 9, "response_tokens_est": 28, "n_tiles": 12, "output_text": "The image shows a neatly made bed with a plaid bedspread, a window with curtains, and a lamp on the wall.", "error": null, "sys_before": {"cpu_percent": 43.1, "ram_used_mb": 24844.2, "ram_available_mb": 100928.0, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.6, "ram_used_mb": 24845.0, "ram_available_mb": 100927.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.16, "peak": 16.76, "min": 13.31}, "VDD_CPU_SOC_MSS": {"avg": 15.9, "peak": 17.71, "min": 14.17}, "VDD_GPU": {"avg": 36.08, "peak": 44.12, "min": 27.57}, "VIN": {"avg": 76.31, "peak": 98.76, "min": 66.22}}, "power_watts_avg": 36.08, "energy_joules_est": 92.36, "sample_count": 20, "duration_seconds": 2.56}, "timestamp": "2026-01-17T14:42:32.869682"}
{"image_index": 107, "image_name": "000000010995.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000010995.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2584.313, "latencies_ms": [2584.313], "images_per_second": 0.387, "prompt_tokens": 23, "response_tokens_est": 27, "n_tiles": 12, "output_text": "bed: 1\ncurtain: 1\nlamp: 1\nwindow: 1\nwall: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24845.0, "ram_available_mb": 100927.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.6, "ram_used_mb": 24845.5, "ram_available_mb": 100926.7, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.28, "peak": 16.56, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.84, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 37.06, "peak": 45.68, "min": 27.57}, "VIN": {"avg": 75.79, "peak": 112.26, "min": 54.27}}, "power_watts_avg": 37.06, "energy_joules_est": 95.79, "sample_count": 20, "duration_seconds": 2.585}, "timestamp": "2026-01-17T14:42:35.460437"}
{"image_index": 107, "image_name": "000000010995.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000010995.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 5004.082, "latencies_ms": [5004.082], "images_per_second": 0.2, "prompt_tokens": 27, "response_tokens_est": 98, "n_tiles": 12, "output_text": "The image shows a room with a window on the left side, through which daylight is entering. The window is framed by a dark curtain. The main focus of the image is a bed with a plaid bedspread in the foreground. The bed is positioned against a wall, and there is a lamp on the wall to the right of the bed. The lamp is turned on, illuminating the room. The room appears to be well-lit, with the window providing natural light.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24845.5, "ram_available_mb": 100926.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 24845.6, "ram_available_mb": 100926.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.15, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.6, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 31.74, "peak": 45.68, "min": 25.6}, "VIN": {"avg": 72.9, "peak": 108.95, "min": 59.0}}, "power_watts_avg": 31.74, "energy_joules_est": 158.84, "sample_count": 39, "duration_seconds": 5.004}, "timestamp": "2026-01-17T14:42:40.471028"}
{"image_index": 107, "image_name": "000000010995.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000010995.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3595.401, "latencies_ms": [3595.401], "images_per_second": 0.278, "prompt_tokens": 21, "response_tokens_est": 57, "n_tiles": 12, "output_text": "The image depicts a dimly lit bedroom with a single bed in the center. The bed is covered with a plaid bedspread, and the room is illuminated by a small window with curtains. The overall scene suggests a quiet, possibly early morning or late evening setting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24845.6, "ram_available_mb": 100926.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 24845.3, "ram_available_mb": 100926.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.2, "peak": 16.66, "min": 14.12}, "VDD_CPU_SOC_MSS": {"avg": 15.7, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 34.06, "peak": 45.68, "min": 26.0}, "VIN": {"avg": 75.42, "peak": 110.38, "min": 63.09}}, "power_watts_avg": 34.06, "energy_joules_est": 122.49, "sample_count": 28, "duration_seconds": 3.596}, "timestamp": "2026-01-17T14:42:44.073522"}
{"image_index": 107, "image_name": "000000010995.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000010995.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3293.717, "latencies_ms": [3293.717], "images_per_second": 0.304, "prompt_tokens": 19, "response_tokens_est": 48, "n_tiles": 12, "output_text": "The image shows a plaid bedspread with a yellow and green color scheme, featuring a checkered pattern. The room is dimly lit, with natural light coming through a window with white trim, creating a cozy and warm atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24845.3, "ram_available_mb": 100926.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 24845.8, "ram_available_mb": 100926.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.22, "peak": 16.56, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.81, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 34.9, "peak": 45.3, "min": 26.0}, "VIN": {"avg": 78.63, "peak": 135.32, "min": 59.6}}, "power_watts_avg": 34.9, "energy_joules_est": 114.96, "sample_count": 25, "duration_seconds": 3.294}, "timestamp": "2026-01-17T14:42:47.373959"}
{"image_index": 108, "image_name": "000000011051.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011051.jpg", "image_width": 640, "image_height": 536, "image_resolution": "640x536", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2931.106, "latencies_ms": [2931.106], "images_per_second": 0.341, "prompt_tokens": 9, "response_tokens_est": 39, "n_tiles": 12, "output_text": "A young couple is posing for a photo at a formal event, with the man in a black suit and the woman in a black dress with a sparkling top and a beige belt.", "error": null, "sys_before": {"cpu_percent": 46.5, "ram_used_mb": 24823.8, "ram_available_mb": 100948.4, "ram_percent": 19.7}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 24829.0, "ram_available_mb": 100943.2, "ram_percent": 19.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.16, "peak": 16.66, "min": 13.29}, "VDD_CPU_SOC_MSS": {"avg": 15.91, "peak": 17.71, "min": 14.17}, "VDD_GPU": {"avg": 34.99, "peak": 44.12, "min": 26.79}, "VIN": {"avg": 81.01, "peak": 125.74, "min": 62.2}}, "power_watts_avg": 34.99, "energy_joules_est": 102.57, "sample_count": 22, "duration_seconds": 2.931}, "timestamp": "2026-01-17T14:42:50.409741"}
{"image_index": 108, "image_name": "000000011051.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011051.jpg", "image_width": 640, "image_height": 536, "image_resolution": "640x536", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2823.071, "latencies_ms": [2823.071], "images_per_second": 0.354, "prompt_tokens": 23, "response_tokens_est": 34, "n_tiles": 12, "output_text": "1. Woman\n2. Man\n3. Rose\n4. Necklace\n5. Dress\n6. Bangle\n7. Ring\n8. Background", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24829.0, "ram_available_mb": 100943.2, "ram_percent": 19.7}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 24830.2, "ram_available_mb": 100941.9, "ram_percent": 19.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.27, "peak": 16.56, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.99, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 36.48, "peak": 44.89, "min": 27.18}, "VIN": {"avg": 83.5, "peak": 125.44, "min": 66.76}}, "power_watts_avg": 36.48, "energy_joules_est": 103.0, "sample_count": 21, "duration_seconds": 2.824}, "timestamp": "2026-01-17T14:42:53.240112"}
{"image_index": 108, "image_name": "000000011051.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011051.jpg", "image_width": 640, "image_height": 536, "image_resolution": "640x536", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3371.7, "latencies_ms": [3371.7], "images_per_second": 0.297, "prompt_tokens": 27, "response_tokens_est": 50, "n_tiles": 12, "output_text": "The main objects in the image are a man and a woman. The man is in the foreground, while the woman is slightly behind him. The man is holding a rose, and the woman is wearing a black dress with a light-colored belt.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 24830.2, "ram_available_mb": 100941.9, "ram_percent": 19.7}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 24831.5, "ram_available_mb": 100940.7, "ram_percent": 19.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.22, "peak": 16.66, "min": 14.12}, "VDD_CPU_SOC_MSS": {"avg": 15.76, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 34.62, "peak": 44.91, "min": 26.0}, "VIN": {"avg": 76.56, "peak": 103.5, "min": 60.35}}, "power_watts_avg": 34.62, "energy_joules_est": 116.74, "sample_count": 26, "duration_seconds": 3.372}, "timestamp": "2026-01-17T14:42:56.618016"}
{"image_index": 108, "image_name": "000000011051.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011051.jpg", "image_width": 640, "image_height": 536, "image_resolution": "640x536", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 4653.217, "latencies_ms": [4653.217], "images_per_second": 0.215, "prompt_tokens": 21, "response_tokens_est": 87, "n_tiles": 12, "output_text": "The image depicts a young couple dressed in formal attire, likely at a wedding or a similar event. The man is wearing a black suit and tie, while the woman is dressed in a black dress with a sequined top and a beige belt. They are both smiling and appear to be enjoying the moment, with the man holding a white rose. The background is softly lit, suggesting an indoor setting with warm lighting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24831.5, "ram_available_mb": 100940.7, "ram_percent": 19.7}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 24831.5, "ram_available_mb": 100940.7, "ram_percent": 19.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.1, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.59, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 31.99, "peak": 45.28, "min": 25.6}, "VIN": {"avg": 75.4, "peak": 141.56, "min": 54.88}}, "power_watts_avg": 31.99, "energy_joules_est": 148.87, "sample_count": 36, "duration_seconds": 4.654}, "timestamp": "2026-01-17T14:43:01.277677"}
{"image_index": 108, "image_name": "000000011051.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011051.jpg", "image_width": 640, "image_height": 536, "image_resolution": "640x536", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3584.405, "latencies_ms": [3584.405], "images_per_second": 0.279, "prompt_tokens": 19, "response_tokens_est": 56, "n_tiles": 12, "output_text": "The image features a young woman with dark hair styled in a bun, wearing a black dress with a sequined top and a beige belt. The lighting is warm, casting a soft glow on her face, and the background is neutral, ensuring the focus remains on her.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24831.5, "ram_available_mb": 100940.7, "ram_percent": 19.7}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 24832.4, "ram_available_mb": 100939.7, "ram_percent": 19.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.91, "peak": 16.26, "min": 14.02}, "VDD_CPU_SOC_MSS": {"avg": 15.48, "peak": 16.92, "min": 14.56}, "VDD_GPU": {"avg": 33.68, "peak": 44.91, "min": 25.6}, "VIN": {"avg": 74.7, "peak": 128.67, "min": 59.32}}, "power_watts_avg": 33.68, "energy_joules_est": 120.73, "sample_count": 28, "duration_seconds": 3.585}, "timestamp": "2026-01-17T14:43:04.870454"}
{"image_index": 109, "image_name": "000000011122.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011122.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2903.389, "latencies_ms": [2903.389], "images_per_second": 0.344, "prompt_tokens": 9, "response_tokens_est": 38, "n_tiles": 12, "output_text": "The image shows a chain-link fence enclosing a grassy area with a stop sign attached to it, and behind the fence, there are palm trees and a building with balconies.", "error": null, "sys_before": {"cpu_percent": 42.1, "ram_used_mb": 24849.2, "ram_available_mb": 100923.0, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 24849.3, "ram_available_mb": 100922.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.09, "peak": 16.66, "min": 13.09}, "VDD_CPU_SOC_MSS": {"avg": 15.84, "peak": 17.71, "min": 14.17}, "VDD_GPU": {"avg": 34.91, "peak": 44.1, "min": 26.39}, "VIN": {"avg": 76.12, "peak": 103.76, "min": 63.36}}, "power_watts_avg": 34.91, "energy_joules_est": 101.37, "sample_count": 22, "duration_seconds": 2.904}, "timestamp": "2026-01-17T14:43:07.879186"}
{"image_index": 109, "image_name": "000000011122.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011122.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 3261.067, "latencies_ms": [3261.067], "images_per_second": 0.307, "prompt_tokens": 23, "response_tokens_est": 47, "n_tiles": 12, "output_text": "- Stop sign: 1\n- Fence: 1\n- Bushes: 1\n- Greenery: 1\n- House: 1\n- Palm trees: 1\n- Building: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24849.3, "ram_available_mb": 100922.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 24844.5, "ram_available_mb": 100927.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.23, "peak": 16.56, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.9, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 34.82, "peak": 44.91, "min": 26.0}, "VIN": {"avg": 73.3, "peak": 92.37, "min": 57.95}}, "power_watts_avg": 34.82, "energy_joules_est": 113.56, "sample_count": 25, "duration_seconds": 3.261}, "timestamp": "2026-01-17T14:43:11.146636"}
{"image_index": 109, "image_name": "000000011122.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011122.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3734.97, "latencies_ms": [3734.97], "images_per_second": 0.268, "prompt_tokens": 27, "response_tokens_est": 61, "n_tiles": 12, "output_text": "The main objects in the image are a chain-link fence, a stop sign, and a bush. The stop sign is positioned in the foreground, while the bush is located near the fence. The background features a building and palm trees, indicating that the scene is set in an urban or suburban area.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24844.5, "ram_available_mb": 100927.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 24845.0, "ram_available_mb": 100927.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.24, "peak": 16.66, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.87, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 33.89, "peak": 44.91, "min": 26.0}, "VIN": {"avg": 78.48, "peak": 136.08, "min": 58.78}}, "power_watts_avg": 33.89, "energy_joules_est": 126.59, "sample_count": 28, "duration_seconds": 3.735}, "timestamp": "2026-01-17T14:43:14.887811"}
{"image_index": 109, "image_name": "000000011122.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011122.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3224.567, "latencies_ms": [3224.567], "images_per_second": 0.31, "prompt_tokens": 21, "response_tokens_est": 46, "n_tiles": 12, "output_text": "The image depicts a chain-link fence enclosing a grassy area with a few scattered objects and plants. The setting appears to be a residential or park-like area, with a building and palm trees visible in the background.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24845.0, "ram_available_mb": 100927.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 24846.5, "ram_available_mb": 100925.7, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.27, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.8, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 34.99, "peak": 44.91, "min": 26.39}, "VIN": {"avg": 80.6, "peak": 120.59, "min": 58.68}}, "power_watts_avg": 34.99, "energy_joules_est": 112.84, "sample_count": 24, "duration_seconds": 3.225}, "timestamp": "2026-01-17T14:43:18.118641"}
{"image_index": 109, "image_name": "000000011122.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011122.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3466.219, "latencies_ms": [3466.219], "images_per_second": 0.288, "prompt_tokens": 19, "response_tokens_est": 53, "n_tiles": 12, "output_text": "The image shows a chain-link fence with a red stop sign attached to it. The stop sign is prominently displayed in the foreground, surrounded by a green metal fence. The lighting is bright, indicating it is daytime, and the weather appears to be clear.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24846.5, "ram_available_mb": 100925.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 24846.5, "ram_available_mb": 100925.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.23, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.78, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 34.39, "peak": 44.91, "min": 26.0}, "VIN": {"avg": 76.02, "peak": 141.37, "min": 56.77}}, "power_watts_avg": 34.39, "energy_joules_est": 119.21, "sample_count": 26, "duration_seconds": 3.467}, "timestamp": "2026-01-17T14:43:21.591300"}
{"image_index": 110, "image_name": "000000011149.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011149.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2726.591, "latencies_ms": [2726.591], "images_per_second": 0.367, "prompt_tokens": 9, "response_tokens_est": 33, "n_tiles": 12, "output_text": "A man wearing a gray t-shirt and brown shorts is standing next to a bicycle with a basket, while another man in a black jacket is riding a motorcycle.", "error": null, "sys_before": {"cpu_percent": 43.3, "ram_used_mb": 24854.5, "ram_available_mb": 100917.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.6, "ram_used_mb": 24850.8, "ram_available_mb": 100921.3, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.19, "peak": 16.76, "min": 13.51}, "VDD_CPU_SOC_MSS": {"avg": 15.96, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 35.88, "peak": 44.12, "min": 27.18}, "VIN": {"avg": 79.64, "peak": 113.98, "min": 66.03}}, "power_watts_avg": 35.88, "energy_joules_est": 97.85, "sample_count": 20, "duration_seconds": 2.727}, "timestamp": "2026-01-17T14:43:24.412110"}
{"image_index": 110, "image_name": "000000011149.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011149.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 3029.487, "latencies_ms": [3029.487], "images_per_second": 0.33, "prompt_tokens": 23, "response_tokens_est": 40, "n_tiles": 12, "output_text": "1. Bicycle\n2. Bicycle\n3. Bicycle\n4. Bicycle\n5. Bicycle\n6. Bicycle\n7. Bicycle\n8. Bicycle", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24850.8, "ram_available_mb": 100921.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 24850.8, "ram_available_mb": 100921.3, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.26, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.9, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 35.74, "peak": 44.91, "min": 26.39}, "VIN": {"avg": 80.4, "peak": 118.37, "min": 66.73}}, "power_watts_avg": 35.74, "energy_joules_est": 108.29, "sample_count": 23, "duration_seconds": 3.03}, "timestamp": "2026-01-17T14:43:27.447888"}
{"image_index": 110, "image_name": "000000011149.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011149.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 4383.133, "latencies_ms": [4383.133], "images_per_second": 0.228, "prompt_tokens": 27, "response_tokens_est": 80, "n_tiles": 12, "output_text": "The main objects in the image are a bicycle and a person. The bicycle is positioned in the foreground, with its front wheel and part of the frame visible. The person is standing near the bicycle, slightly to the right, and is wearing a helmet and a gray t-shirt. The background features a fence and some greenery, indicating that the scene takes place in a park or a garden.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24848.4, "ram_available_mb": 100923.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 24843.8, "ram_available_mb": 100928.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.21, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.85, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 32.61, "peak": 45.28, "min": 26.0}, "VIN": {"avg": 75.9, "peak": 142.56, "min": 61.75}}, "power_watts_avg": 32.61, "energy_joules_est": 142.95, "sample_count": 34, "duration_seconds": 4.384}, "timestamp": "2026-01-17T14:43:31.837970"}
{"image_index": 110, "image_name": "000000011149.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011149.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 4116.972, "latencies_ms": [4116.972], "images_per_second": 0.243, "prompt_tokens": 21, "response_tokens_est": 72, "n_tiles": 12, "output_text": "The scene depicts a man standing beside a bicycle in a park-like setting. The man is wearing a gray t-shirt, khaki shorts, and black shoes with red laces. The bicycle is yellow and black, and there is a basket attached to the front. The ground is paved, and there are trees and bushes in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24843.8, "ram_available_mb": 100928.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 24840.5, "ram_available_mb": 100931.7, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.19, "peak": 16.56, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.7, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 32.92, "peak": 44.89, "min": 26.0}, "VIN": {"avg": 73.8, "peak": 110.76, "min": 54.37}}, "power_watts_avg": 32.92, "energy_joules_est": 135.54, "sample_count": 32, "duration_seconds": 4.117}, "timestamp": "2026-01-17T14:43:35.961441"}
{"image_index": 110, "image_name": "000000011149.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011149.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3164.189, "latencies_ms": [3164.189], "images_per_second": 0.316, "prompt_tokens": 19, "response_tokens_est": 44, "n_tiles": 12, "output_text": "The image features a person riding a yellow bicycle, which stands out against the green foliage and concrete path. The lighting is bright, indicating it is daytime, and the weather appears to be clear and sunny.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24840.5, "ram_available_mb": 100931.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 24842.4, "ram_available_mb": 100929.7, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.24, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.84, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 35.09, "peak": 44.89, "min": 26.39}, "VIN": {"avg": 78.11, "peak": 117.14, "min": 62.68}}, "power_watts_avg": 35.09, "energy_joules_est": 111.04, "sample_count": 24, "duration_seconds": 3.165}, "timestamp": "2026-01-17T14:43:39.131238"}
{"image_index": 111, "image_name": "000000011197.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011197.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1389.681, "latencies_ms": [1389.681], "images_per_second": 0.72, "prompt_tokens": 9, "response_tokens_est": 29, "n_tiles": 6, "output_text": "A man in a blue shirt is standing on a sidewalk next to a black street lamp, while a red car is driving on the street.", "error": null, "sys_before": {"cpu_percent": 42.7, "ram_used_mb": 24859.9, "ram_available_mb": 100912.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 24862.1, "ram_available_mb": 100910.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.61, "peak": 15.04, "min": 13.82}, "VDD_CPU_SOC_MSS": {"avg": 15.28, "peak": 15.75, "min": 14.18}, "VDD_GPU": {"avg": 34.15, "peak": 40.56, "min": 26.79}, "VIN": {"avg": 73.31, "peak": 98.45, "min": 56.2}}, "power_watts_avg": 34.15, "energy_joules_est": 47.47, "sample_count": 10, "duration_seconds": 1.39}, "timestamp": "2026-01-17T14:43:40.584119"}
{"image_index": 111, "image_name": "000000011197.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011197.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2511.908, "latencies_ms": [2511.908], "images_per_second": 0.398, "prompt_tokens": 23, "response_tokens_est": 66, "n_tiles": 6, "output_text": "- Pedestrian: 1\n- Car: 1\n- Street light: 1\n- Building: 1\n- Traffic sign: 2\n- Street sign: 2\n- Bicycle: 1\n- Person: 1\n- Street: 1\n- Car: 1", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 24862.1, "ram_available_mb": 100910.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 24863.0, "ram_available_mb": 100909.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.87, "peak": 15.34, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.56, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 29.59, "peak": 41.34, "min": 22.86}, "VIN": {"avg": 70.0, "peak": 115.62, "min": 60.99}}, "power_watts_avg": 29.59, "energy_joules_est": 74.34, "sample_count": 19, "duration_seconds": 2.512}, "timestamp": "2026-01-17T14:43:43.102576"}
{"image_index": 111, "image_name": "000000011197.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011197.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2315.972, "latencies_ms": [2315.972], "images_per_second": 0.432, "prompt_tokens": 27, "response_tokens_est": 59, "n_tiles": 6, "output_text": "The main objects in the image are a black street lamp, a red car, and a person standing near the sidewalk. The street lamp is positioned in the foreground, while the red car is in the background. The person is standing near the sidewalk, which is located in the middle ground.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24863.0, "ram_available_mb": 100909.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 24864.0, "ram_available_mb": 100908.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.83, "peak": 15.24, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.35, "peak": 15.75, "min": 14.56}, "VDD_GPU": {"avg": 29.87, "peak": 40.16, "min": 23.25}, "VIN": {"avg": 69.42, "peak": 95.48, "min": 55.76}}, "power_watts_avg": 29.87, "energy_joules_est": 69.19, "sample_count": 17, "duration_seconds": 2.316}, "timestamp": "2026-01-17T14:43:45.428666"}
{"image_index": 111, "image_name": "000000011197.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011197.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3143.393, "latencies_ms": [3143.393], "images_per_second": 0.318, "prompt_tokens": 21, "response_tokens_est": 88, "n_tiles": 6, "output_text": "The image depicts an urban street scene with a pedestrian crossing sign indicating \"Procter & Gamble\" and a \"No Pedestrian\" sign. A man in a blue shirt is standing near the crossing, while a woman in a black jacket is walking across the street. The street is lined with buildings, trees, and a red car is visible in the background. The setting appears to be a city intersection during daytime.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24864.0, "ram_available_mb": 100908.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 24864.3, "ram_available_mb": 100907.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.84, "peak": 15.24, "min": 14.2}, "VDD_CPU_SOC_MSS": {"avg": 15.52, "peak": 15.75, "min": 14.56}, "VDD_GPU": {"avg": 27.81, "peak": 40.18, "min": 22.86}, "VIN": {"avg": 70.08, "peak": 107.83, "min": 60.77}}, "power_watts_avg": 27.81, "energy_joules_est": 87.43, "sample_count": 24, "duration_seconds": 3.144}, "timestamp": "2026-01-17T14:43:48.582012"}
{"image_index": 111, "image_name": "000000011197.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011197.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2770.29, "latencies_ms": [2770.29], "images_per_second": 0.361, "prompt_tokens": 19, "response_tokens_est": 75, "n_tiles": 6, "output_text": "The image depicts a city street scene with a mix of urban and natural elements. Notable visual attributes include a red car driving on the road, a black metal street lamp, a red trash can, a brick sidewalk, and a building with a red brick facade. The lighting is natural, likely from the overcast sky, and the weather appears to be clear.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24864.3, "ram_available_mb": 100907.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 24865.1, "ram_available_mb": 100907.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.84, "peak": 15.14, "min": 14.2}, "VDD_CPU_SOC_MSS": {"avg": 15.32, "peak": 15.74, "min": 14.56}, "VDD_GPU": {"avg": 28.38, "peak": 40.95, "min": 22.86}, "VIN": {"avg": 70.43, "peak": 110.57, "min": 62.94}}, "power_watts_avg": 28.38, "energy_joules_est": 78.63, "sample_count": 22, "duration_seconds": 2.771}, "timestamp": "2026-01-17T14:43:51.358274"}
{"image_index": 112, "image_name": "000000011511.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011511.jpg", "image_width": 640, "image_height": 464, "image_resolution": "640x464", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2390.669, "latencies_ms": [2390.669], "images_per_second": 0.418, "prompt_tokens": 9, "response_tokens_est": 23, "n_tiles": 12, "output_text": "The image shows a bronze statue of two women sitting on a bench, with one of them holding a bag.", "error": null, "sys_before": {"cpu_percent": 41.3, "ram_used_mb": 24841.4, "ram_available_mb": 100930.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.4, "ram_used_mb": 24842.5, "ram_available_mb": 100929.7, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.22, "peak": 16.76, "min": 13.59}, "VDD_CPU_SOC_MSS": {"avg": 15.96, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 36.33, "peak": 43.73, "min": 27.97}, "VIN": {"avg": 81.31, "peak": 122.87, "min": 60.97}}, "power_watts_avg": 36.33, "energy_joules_est": 86.87, "sample_count": 18, "duration_seconds": 2.391}, "timestamp": "2026-01-17T14:43:53.829253"}
{"image_index": 112, "image_name": "000000011511.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011511.jpg", "image_width": 640, "image_height": 464, "image_resolution": "640x464", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2826.597, "latencies_ms": [2826.597], "images_per_second": 0.354, "prompt_tokens": 23, "response_tokens_est": 34, "n_tiles": 12, "output_text": "1. Statue\n2. Woman\n3. Man\n4. Woman\n5. Woman\n6. Woman\n7. Woman\n8. Statue", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24842.5, "ram_available_mb": 100929.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 24844.0, "ram_available_mb": 100928.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.26, "peak": 16.66, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.89, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 36.22, "peak": 45.69, "min": 26.8}, "VIN": {"avg": 83.31, "peak": 128.73, "min": 57.14}}, "power_watts_avg": 36.22, "energy_joules_est": 102.4, "sample_count": 22, "duration_seconds": 2.827}, "timestamp": "2026-01-17T14:43:56.662420"}
{"image_index": 112, "image_name": "000000011511.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011511.jpg", "image_width": 640, "image_height": 464, "image_resolution": "640x464", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 4213.889, "latencies_ms": [4213.889], "images_per_second": 0.237, "prompt_tokens": 27, "response_tokens_est": 74, "n_tiles": 12, "output_text": "The main object in the foreground is a bronze statue of a woman sitting on a bench. The statue is positioned near the center of the image, with a concrete bench and a metal trash can nearby. In the background, there are two people standing near a closed garage door. The statue and the bench are closer to the camera, while the people are further away.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24844.0, "ram_available_mb": 100928.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 24843.9, "ram_available_mb": 100928.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.14, "peak": 16.56, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.72, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 32.62, "peak": 44.91, "min": 25.6}, "VIN": {"avg": 72.59, "peak": 99.74, "min": 58.63}}, "power_watts_avg": 32.62, "energy_joules_est": 137.47, "sample_count": 32, "duration_seconds": 4.214}, "timestamp": "2026-01-17T14:44:00.882775"}
{"image_index": 112, "image_name": "000000011511.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011511.jpg", "image_width": 640, "image_height": 464, "image_resolution": "640x464", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 4320.979, "latencies_ms": [4320.979], "images_per_second": 0.231, "prompt_tokens": 21, "response_tokens_est": 77, "n_tiles": 12, "output_text": "The image depicts a scene in an outdoor urban setting, possibly a park or a public square, with a large, weathered bronze statue of two women seated on a bench. The ground is paved with bricks, and there is a metal trash can nearby. The lighting suggests it is daytime, and the shadows indicate the sun is shining from the right side of the frame.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24843.9, "ram_available_mb": 100928.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 24844.2, "ram_available_mb": 100928.0, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.85, "peak": 16.36, "min": 13.92}, "VDD_CPU_SOC_MSS": {"avg": 15.43, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 32.29, "peak": 44.91, "min": 25.22}, "VIN": {"avg": 72.81, "peak": 100.58, "min": 56.45}}, "power_watts_avg": 32.29, "energy_joules_est": 139.53, "sample_count": 34, "duration_seconds": 4.321}, "timestamp": "2026-01-17T14:44:05.212203"}
{"image_index": 112, "image_name": "000000011511.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011511.jpg", "image_width": 640, "image_height": 464, "image_resolution": "640x464", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3140.064, "latencies_ms": [3140.064], "images_per_second": 0.318, "prompt_tokens": 19, "response_tokens_est": 43, "n_tiles": 12, "output_text": "The bronze statue of two women is notable for its weathered appearance, with visible signs of wear and tear. The lighting in the image is natural, casting shadows on the ground, indicating that it is daytime.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24844.2, "ram_available_mb": 100928.0, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 24843.9, "ram_available_mb": 100928.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.93, "peak": 16.26, "min": 13.92}, "VDD_CPU_SOC_MSS": {"avg": 15.47, "peak": 17.32, "min": 14.17}, "VDD_GPU": {"avg": 34.91, "peak": 44.91, "min": 26.0}, "VIN": {"avg": 77.77, "peak": 119.66, "min": 60.91}}, "power_watts_avg": 34.91, "energy_joules_est": 109.64, "sample_count": 24, "duration_seconds": 3.141}, "timestamp": "2026-01-17T14:44:08.361035"}
{"image_index": 113, "image_name": "000000011615.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011615.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 3708.613, "latencies_ms": [3708.613], "images_per_second": 0.27, "prompt_tokens": 9, "response_tokens_est": 62, "n_tiles": 12, "output_text": "The image shows a set of road signs, including a blue directional arrow indicating a left turn, a yellow bus icon, and a green directional arrow indicating a right turn. Additionally, there is a parking sign with a blue \"P\" symbol, and a circular sign indicating a no parking zone.", "error": null, "sys_before": {"cpu_percent": 45.8, "ram_used_mb": 24841.9, "ram_available_mb": 100930.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 24841.6, "ram_available_mb": 100930.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.11, "peak": 16.66, "min": 13.19}, "VDD_CPU_SOC_MSS": {"avg": 15.8, "peak": 17.71, "min": 14.17}, "VDD_GPU": {"avg": 33.1, "peak": 44.12, "min": 26.0}, "VIN": {"avg": 74.06, "peak": 133.92, "min": 54.7}}, "power_watts_avg": 33.1, "energy_joules_est": 122.77, "sample_count": 28, "duration_seconds": 3.709}, "timestamp": "2026-01-17T14:44:12.179103"}
{"image_index": 113, "image_name": "000000011615.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011615.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 3497.994, "latencies_ms": [3497.994], "images_per_second": 0.286, "prompt_tokens": 23, "response_tokens_est": 54, "n_tiles": 12, "output_text": "- Airplane: 1\n- Bus: 1\n- Car: 1\n- Parking sign: 1\n- Traffic sign: 1\n- Traffic light: 1\n- Street sign: 1\n- Road sign: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24841.6, "ram_available_mb": 100930.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 24841.6, "ram_available_mb": 100930.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.2, "peak": 16.56, "min": 14.02}, "VDD_CPU_SOC_MSS": {"avg": 15.83, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 34.29, "peak": 45.69, "min": 26.0}, "VIN": {"avg": 81.34, "peak": 129.64, "min": 64.19}}, "power_watts_avg": 34.29, "energy_joules_est": 119.97, "sample_count": 27, "duration_seconds": 3.499}, "timestamp": "2026-01-17T14:44:15.683974"}
{"image_index": 113, "image_name": "000000011615.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011615.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3921.167, "latencies_ms": [3921.167], "images_per_second": 0.255, "prompt_tokens": 27, "response_tokens_est": 66, "n_tiles": 12, "output_text": "The main objects in the image are a series of road signs. The signs are arranged in a way that the sign on the left is closer to the foreground, while the sign on the right is closer to the background. The sign on the left is closer to the camera, while the sign on the right is further away.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 24841.6, "ram_available_mb": 100930.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 24840.6, "ram_available_mb": 100931.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.18, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.75, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 33.28, "peak": 45.3, "min": 26.0}, "VIN": {"avg": 77.46, "peak": 113.25, "min": 63.49}}, "power_watts_avg": 33.28, "energy_joules_est": 130.51, "sample_count": 31, "duration_seconds": 3.922}, "timestamp": "2026-01-17T14:44:19.611590"}
{"image_index": 113, "image_name": "000000011615.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011615.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3805.646, "latencies_ms": [3805.646], "images_per_second": 0.263, "prompt_tokens": 21, "response_tokens_est": 63, "n_tiles": 12, "output_text": "The image depicts a collection of road signs mounted on a pole, likely in an urban or suburban setting. The signs provide directions and information for drivers, with one sign indicating a parking area and another directing to a specific location. The background features trees and a cloudy sky, suggesting an overcast day.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24840.6, "ram_available_mb": 100931.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 24841.4, "ram_available_mb": 100930.7, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.2, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.68, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 33.55, "peak": 44.89, "min": 26.0}, "VIN": {"avg": 78.41, "peak": 138.96, "min": 55.07}}, "power_watts_avg": 33.55, "energy_joules_est": 127.7, "sample_count": 29, "duration_seconds": 3.806}, "timestamp": "2026-01-17T14:44:23.423614"}
{"image_index": 113, "image_name": "000000011615.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011615.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 4861.074, "latencies_ms": [4861.074], "images_per_second": 0.206, "prompt_tokens": 19, "response_tokens_est": 94, "n_tiles": 12, "output_text": "The image features a collection of road signs, including a blue and white sign with an airplane icon, a green and white sign with a bus icon, and a white sign with a parking sign. The signs are mounted on a brown pole and are illuminated by natural daylight, suggesting a clear and sunny day. The colors are vibrant, with the blue and white sign contrasting against the green and white sign, and the white sign standing out against the brown pole.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24841.4, "ram_available_mb": 100930.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 24842.4, "ram_available_mb": 100929.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.16, "peak": 16.56, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.68, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 31.88, "peak": 45.28, "min": 26.0}, "VIN": {"avg": 73.75, "peak": 117.43, "min": 62.35}}, "power_watts_avg": 31.88, "energy_joules_est": 154.99, "sample_count": 38, "duration_seconds": 4.862}, "timestamp": "2026-01-17T14:44:28.291284"}
{"image_index": 114, "image_name": "000000011699.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011699.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2286.618, "latencies_ms": [2286.618], "images_per_second": 0.437, "prompt_tokens": 9, "response_tokens_est": 20, "n_tiles": 12, "output_text": "A woman and a young girl are standing next to a suitcase, both wearing backpacks.", "error": null, "sys_before": {"cpu_percent": 44.8, "ram_used_mb": 24843.4, "ram_available_mb": 100928.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.3, "ram_used_mb": 24843.7, "ram_available_mb": 100928.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.21, "peak": 16.66, "min": 13.39}, "VDD_CPU_SOC_MSS": {"avg": 15.95, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 37.56, "peak": 44.12, "min": 29.54}, "VIN": {"avg": 84.71, "peak": 128.08, "min": 66.45}}, "power_watts_avg": 37.56, "energy_joules_est": 85.9, "sample_count": 17, "duration_seconds": 2.287}, "timestamp": "2026-01-17T14:44:30.682276"}
{"image_index": 114, "image_name": "000000011699.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011699.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2825.365, "latencies_ms": [2825.365], "images_per_second": 0.354, "prompt_tokens": 23, "response_tokens_est": 33, "n_tiles": 12, "output_text": "1. Woman\n2. Girl\n3. Bag\n4. Suitcase\n5. Bag\n6. Woman\n7. Bag\n8. Bag", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24843.7, "ram_available_mb": 100928.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 24840.9, "ram_available_mb": 100931.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.27, "peak": 16.56, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.85, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 36.5, "peak": 46.07, "min": 26.79}, "VIN": {"avg": 76.73, "peak": 106.37, "min": 67.13}}, "power_watts_avg": 36.5, "energy_joules_est": 103.14, "sample_count": 22, "duration_seconds": 2.826}, "timestamp": "2026-01-17T14:44:33.513700"}
{"image_index": 114, "image_name": "000000011699.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011699.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 4839.612, "latencies_ms": [4839.612], "images_per_second": 0.207, "prompt_tokens": 27, "response_tokens_est": 93, "n_tiles": 12, "output_text": "The main objects in the image are a woman and a young girl standing next to a suitcase. The woman is wearing a red shirt and has a black bag with a colorful pattern. The young girl is wearing a light blue shirt and has a black bag with a colorful design. The suitcase is positioned in the foreground, with the woman and girl standing near it. The background features a ceiling with a red structure and a yellow tactile paving strip.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24840.9, "ram_available_mb": 100931.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 24839.2, "ram_available_mb": 100933.0, "ram_percent": 19.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.15, "peak": 16.56, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.74, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 31.92, "peak": 46.07, "min": 26.0}, "VIN": {"avg": 72.38, "peak": 115.29, "min": 61.72}}, "power_watts_avg": 31.92, "energy_joules_est": 154.49, "sample_count": 38, "duration_seconds": 4.84}, "timestamp": "2026-01-17T14:44:38.359982"}
{"image_index": 114, "image_name": "000000011699.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011699.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 5140.607, "latencies_ms": [5140.607], "images_per_second": 0.195, "prompt_tokens": 21, "response_tokens_est": 102, "n_tiles": 12, "output_text": "The image depicts a scene inside a subway station where a woman and a young girl are standing next to a black suitcase. The woman is wearing a red shirt and has a backpack, while the girl is dressed in a light blue shirt and is also carrying a backpack. They appear to be waiting for their train, with the woman holding onto the suitcase handle and the girl making a peace sign with her hand. The setting is a subway station with yellow tactile paving along the platform edge.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24839.2, "ram_available_mb": 100933.0, "ram_percent": 19.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 24839.4, "ram_available_mb": 100932.7, "ram_percent": 19.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.16, "peak": 16.66, "min": 14.12}, "VDD_CPU_SOC_MSS": {"avg": 15.61, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 31.48, "peak": 44.91, "min": 26.0}, "VIN": {"avg": 74.92, "peak": 112.08, "min": 57.45}}, "power_watts_avg": 31.48, "energy_joules_est": 161.84, "sample_count": 40, "duration_seconds": 5.141}, "timestamp": "2026-01-17T14:44:43.506986"}
{"image_index": 114, "image_name": "000000011699.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011699.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 4143.907, "latencies_ms": [4143.907], "images_per_second": 0.241, "prompt_tokens": 19, "response_tokens_est": 73, "n_tiles": 12, "output_text": "The image features a woman and a young girl standing next to a black suitcase. The woman is wearing a red shirt, and the girl is dressed in a light blue shirt. The scene is well-lit, with natural light illuminating the surroundings. The suitcase is made of black material, and the ground is covered with yellow tactile paving.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24839.4, "ram_available_mb": 100932.7, "ram_percent": 19.7}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 24839.7, "ram_available_mb": 100932.5, "ram_percent": 19.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.18, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.8, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 32.83, "peak": 44.49, "min": 26.0}, "VIN": {"avg": 75.41, "peak": 115.93, "min": 62.02}}, "power_watts_avg": 32.83, "energy_joules_est": 136.07, "sample_count": 32, "duration_seconds": 4.145}, "timestamp": "2026-01-17T14:44:47.661146"}
{"image_index": 115, "image_name": "000000011760.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011760.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1442.259, "latencies_ms": [1442.259], "images_per_second": 0.693, "prompt_tokens": 9, "response_tokens_est": 31, "n_tiles": 6, "output_text": "The image depicts a zebra standing in a natural environment with trees and bushes in the background, showcasing its distinctive black and white stripes.", "error": null, "sys_before": {"cpu_percent": 44.1, "ram_used_mb": 24852.1, "ram_available_mb": 100920.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 24852.8, "ram_available_mb": 100919.3, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.66, "peak": 15.11, "min": 13.72}, "VDD_CPU_SOC_MSS": {"avg": 15.35, "peak": 16.14, "min": 14.17}, "VDD_GPU": {"avg": 33.05, "peak": 40.57, "min": 25.6}, "VIN": {"avg": 71.56, "peak": 116.89, "min": 49.58}}, "power_watts_avg": 33.05, "energy_joules_est": 47.68, "sample_count": 11, "duration_seconds": 1.443}, "timestamp": "2026-01-17T14:44:49.171641"}
{"image_index": 115, "image_name": "000000011760.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011760.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1376.143, "latencies_ms": [1376.143], "images_per_second": 0.727, "prompt_tokens": 23, "response_tokens_est": 27, "n_tiles": 6, "output_text": "zebra: 2\ntree: 1\nbush: 1\nlog: 1\nground: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24852.8, "ram_available_mb": 100919.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 24854.1, "ram_available_mb": 100918.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.98, "peak": 15.54, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.55, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 34.66, "peak": 40.97, "min": 27.18}, "VIN": {"avg": 74.66, "peak": 107.5, "min": 63.62}}, "power_watts_avg": 34.66, "energy_joules_est": 47.71, "sample_count": 10, "duration_seconds": 1.377}, "timestamp": "2026-01-17T14:44:50.553811"}
{"image_index": 115, "image_name": "000000011760.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011760.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2351.82, "latencies_ms": [2351.82], "images_per_second": 0.425, "prompt_tokens": 27, "response_tokens_est": 63, "n_tiles": 6, "output_text": "The main objects in the image are two zebras. The foreground features a zebra with a prominent black and white striped pattern, while the background shows another zebra with a similar pattern. The zebra in the foreground is closer to the camera, while the one in the background is further away.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24854.1, "ram_available_mb": 100918.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 24857.4, "ram_available_mb": 100914.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.09, "peak": 15.54, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.81, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 30.18, "peak": 40.95, "min": 23.64}, "VIN": {"avg": 70.61, "peak": 98.76, "min": 58.28}}, "power_watts_avg": 30.18, "energy_joules_est": 70.99, "sample_count": 18, "duration_seconds": 2.352}, "timestamp": "2026-01-17T14:44:52.911418"}
{"image_index": 115, "image_name": "000000011760.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011760.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2077.072, "latencies_ms": [2077.072], "images_per_second": 0.481, "prompt_tokens": 21, "response_tokens_est": 53, "n_tiles": 6, "output_text": "The image depicts a scene in a natural habitat, likely a savanna or grassland, where two zebras are walking. The zebras are surrounded by a mix of dry grass and scattered trees, with some purple flowers visible in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24857.4, "ram_available_mb": 100914.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 24858.5, "ram_available_mb": 100913.7, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.08, "peak": 15.44, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.77, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 30.75, "peak": 40.57, "min": 24.03}, "VIN": {"avg": 72.44, "peak": 112.86, "min": 61.62}}, "power_watts_avg": 30.75, "energy_joules_est": 63.88, "sample_count": 16, "duration_seconds": 2.078}, "timestamp": "2026-01-17T14:44:54.994620"}
{"image_index": 115, "image_name": "000000011760.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011760.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2245.422, "latencies_ms": [2245.422], "images_per_second": 0.445, "prompt_tokens": 19, "response_tokens_est": 59, "n_tiles": 6, "output_text": "The image features two zebras standing in a natural environment. The zebras have distinctive black and white stripes on their bodies, and their fur appears to be a mix of brown and white. The lighting is natural, suggesting it is daytime, and the ground is covered in dirt.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24858.5, "ram_available_mb": 100913.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 24858.8, "ram_available_mb": 100913.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.08, "peak": 15.54, "min": 14.43}, "VDD_CPU_SOC_MSS": {"avg": 15.84, "peak": 16.53, "min": 14.96}, "VDD_GPU": {"avg": 30.22, "peak": 40.18, "min": 23.64}, "VIN": {"avg": 70.35, "peak": 99.01, "min": 56.21}}, "power_watts_avg": 30.22, "energy_joules_est": 67.88, "sample_count": 17, "duration_seconds": 2.246}, "timestamp": "2026-01-17T14:44:57.245990"}
{"image_index": 116, "image_name": "000000011813.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011813.jpg", "image_width": 333, "image_height": 500, "image_resolution": "333x500", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1468.051, "latencies_ms": [1468.051], "images_per_second": 0.681, "prompt_tokens": 9, "response_tokens_est": 32, "n_tiles": 6, "output_text": "The image shows a professional video camera mounted on a tripod, with a laptop placed on a folding chair nearby, indicating a setup for filming or recording.", "error": null, "sys_before": {"cpu_percent": 42.6, "ram_used_mb": 24858.5, "ram_available_mb": 100913.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 24859.7, "ram_available_mb": 100912.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.79, "peak": 15.24, "min": 14.1}, "VDD_CPU_SOC_MSS": {"avg": 15.53, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 33.02, "peak": 40.57, "min": 26.0}, "VIN": {"avg": 78.94, "peak": 118.29, "min": 64.14}}, "power_watts_avg": 33.02, "energy_joules_est": 48.49, "sample_count": 11, "duration_seconds": 1.468}, "timestamp": "2026-01-17T14:44:58.760245"}
{"image_index": 116, "image_name": "000000011813.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011813.jpg", "image_width": 333, "image_height": 500, "image_resolution": "333x500", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1599.55, "latencies_ms": [1599.55], "images_per_second": 0.625, "prompt_tokens": 23, "response_tokens_est": 34, "n_tiles": 6, "output_text": "1. Laptop\n2. Chair\n3. Tripod\n4. Camera\n5. Microphone\n6. Wall\n7. Floor\n8. Table", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24859.7, "ram_available_mb": 100912.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 24860.7, "ram_available_mb": 100911.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.84, "peak": 15.34, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.39, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 33.02, "peak": 40.95, "min": 24.82}, "VIN": {"avg": 70.87, "peak": 98.65, "min": 61.66}}, "power_watts_avg": 33.02, "energy_joules_est": 52.83, "sample_count": 12, "duration_seconds": 1.6}, "timestamp": "2026-01-17T14:45:00.366018"}
{"image_index": 116, "image_name": "000000011813.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011813.jpg", "image_width": 333, "image_height": 500, "image_resolution": "333x500", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1910.843, "latencies_ms": [1910.843], "images_per_second": 0.523, "prompt_tokens": 27, "response_tokens_est": 45, "n_tiles": 6, "output_text": "The main objects in the image are a laptop and a tripod. The laptop is positioned on a folding chair, which is located in the foreground. The tripod is positioned in the background, slightly to the right.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24860.7, "ram_available_mb": 100911.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 24860.7, "ram_available_mb": 100911.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.84, "peak": 15.34, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.47, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 31.51, "peak": 40.95, "min": 23.64}, "VIN": {"avg": 72.03, "peak": 100.27, "min": 58.57}}, "power_watts_avg": 31.51, "energy_joules_est": 60.22, "sample_count": 14, "duration_seconds": 1.911}, "timestamp": "2026-01-17T14:45:02.283978"}
{"image_index": 116, "image_name": "000000011813.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011813.jpg", "image_width": 333, "image_height": 500, "image_resolution": "333x500", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2051.353, "latencies_ms": [2051.353], "images_per_second": 0.487, "prompt_tokens": 21, "response_tokens_est": 50, "n_tiles": 6, "output_text": "The image depicts a professional video production setup in a well-lit room. A camera is mounted on a tripod, and a laptop is placed on a folding chair, indicating that the scene is likely set up for filming or recording purposes.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 24860.7, "ram_available_mb": 100911.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 24860.9, "ram_available_mb": 100911.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.84, "peak": 15.24, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.51, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 30.91, "peak": 40.56, "min": 23.64}, "VIN": {"avg": 70.98, "peak": 95.45, "min": 59.11}}, "power_watts_avg": 30.91, "energy_joules_est": 63.42, "sample_count": 15, "duration_seconds": 2.052}, "timestamp": "2026-01-17T14:45:04.345439"}
{"image_index": 116, "image_name": "000000011813.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011813.jpg", "image_width": 333, "image_height": 500, "image_resolution": "333x500", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1736.158, "latencies_ms": [1736.158], "images_per_second": 0.576, "prompt_tokens": 19, "response_tokens_est": 39, "n_tiles": 6, "output_text": "The image features a well-lit indoor setting with a white wall and a black laptop on a folding chair. The lighting is bright, casting clear shadows and highlighting the objects in the scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24860.9, "ram_available_mb": 100911.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 24860.9, "ram_available_mb": 100911.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.81, "peak": 15.34, "min": 14.2}, "VDD_CPU_SOC_MSS": {"avg": 15.41, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 31.81, "peak": 40.56, "min": 24.04}, "VIN": {"avg": 74.49, "peak": 112.74, "min": 60.55}}, "power_watts_avg": 31.81, "energy_joules_est": 55.24, "sample_count": 13, "duration_seconds": 1.737}, "timestamp": "2026-01-17T14:45:06.091691"}
{"image_index": 117, "image_name": "000000012062.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012062.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1492.237, "latencies_ms": [1492.237], "images_per_second": 0.67, "prompt_tokens": 9, "response_tokens_est": 33, "n_tiles": 6, "output_text": "A flock of sheep is being kept in a metal pen, with some sheep lying on the ground and others standing, all surrounded by a thick layer of wool.", "error": null, "sys_before": {"cpu_percent": 34.5, "ram_used_mb": 24860.9, "ram_available_mb": 100911.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 24860.9, "ram_available_mb": 100911.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.73, "peak": 15.24, "min": 13.79}, "VDD_CPU_SOC_MSS": {"avg": 15.46, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 32.77, "peak": 40.16, "min": 25.6}, "VIN": {"avg": 75.27, "peak": 118.73, "min": 56.34}}, "power_watts_avg": 32.77, "energy_joules_est": 48.91, "sample_count": 11, "duration_seconds": 1.493}, "timestamp": "2026-01-17T14:45:07.631662"}
{"image_index": 117, "image_name": "000000012062.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012062.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1765.722, "latencies_ms": [1765.722], "images_per_second": 0.566, "prompt_tokens": 23, "response_tokens_est": 40, "n_tiles": 6, "output_text": "1. Sheep\n2. Sheep\n3. Sheep\n4. Sheep\n5. Sheep\n6. Sheep\n7. Sheep\n8. Sheep", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 24860.9, "ram_available_mb": 100911.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 24860.9, "ram_available_mb": 100911.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.87, "peak": 15.34, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.41, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 32.39, "peak": 40.97, "min": 24.42}, "VIN": {"avg": 70.92, "peak": 96.0, "min": 55.14}}, "power_watts_avg": 32.39, "energy_joules_est": 57.2, "sample_count": 13, "duration_seconds": 1.766}, "timestamp": "2026-01-17T14:45:09.404044"}
{"image_index": 117, "image_name": "000000012062.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012062.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2511.54, "latencies_ms": [2511.54], "images_per_second": 0.398, "prompt_tokens": 27, "response_tokens_est": 66, "n_tiles": 6, "output_text": "The main objects in the image are sheep, which are located in the foreground. The sheep are surrounded by a metal wire fence, and there is a pile of sheep's wool in the foreground. The background features a paved surface and a small, white object, possibly a ball, which is not part of the sheep.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24860.9, "ram_available_mb": 100911.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 24860.9, "ram_available_mb": 100911.3, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.85, "peak": 15.24, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.48, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 29.32, "peak": 40.57, "min": 22.85}, "VIN": {"avg": 71.1, "peak": 112.26, "min": 59.01}}, "power_watts_avg": 29.32, "energy_joules_est": 73.65, "sample_count": 19, "duration_seconds": 2.512}, "timestamp": "2026-01-17T14:45:11.922928"}
{"image_index": 117, "image_name": "000000012062.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012062.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2352.972, "latencies_ms": [2352.972], "images_per_second": 0.425, "prompt_tokens": 21, "response_tokens_est": 61, "n_tiles": 6, "output_text": "The image depicts a sheep in a pen, surrounded by a variety of sheep's wool. The sheep are in a fenced area, likely a sheep farm, with a concrete floor and metal bars. The sheep appear to be in a relaxed state, with some wool on their backs and heads.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 24860.9, "ram_available_mb": 100911.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 24860.9, "ram_available_mb": 100911.3, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.87, "peak": 15.24, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.51, "peak": 16.14, "min": 14.57}, "VDD_GPU": {"avg": 29.55, "peak": 40.16, "min": 23.25}, "VIN": {"avg": 74.31, "peak": 114.91, "min": 62.76}}, "power_watts_avg": 29.55, "energy_joules_est": 69.54, "sample_count": 18, "duration_seconds": 2.353}, "timestamp": "2026-01-17T14:45:14.282156"}
{"image_index": 117, "image_name": "000000012062.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012062.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1563.486, "latencies_ms": [1563.486], "images_per_second": 0.64, "prompt_tokens": 19, "response_tokens_est": 33, "n_tiles": 6, "output_text": "The image shows a sheep in a wire pen, with a predominantly white woolly coat. The lighting is natural, and the weather appears to be overcast.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24860.9, "ram_available_mb": 100911.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 24860.9, "ram_available_mb": 100911.3, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.83, "peak": 15.24, "min": 14.3}, "VDD_CPU_SOC_MSS": {"avg": 15.42, "peak": 15.75, "min": 14.56}, "VDD_GPU": {"avg": 32.56, "peak": 40.56, "min": 24.82}, "VIN": {"avg": 71.86, "peak": 103.13, "min": 62.63}}, "power_watts_avg": 32.56, "energy_joules_est": 50.92, "sample_count": 12, "duration_seconds": 1.564}, "timestamp": "2026-01-17T14:45:15.851899"}
{"image_index": 118, "image_name": "000000012120.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012120.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1804.864, "latencies_ms": [1804.864], "images_per_second": 0.554, "prompt_tokens": 9, "response_tokens_est": 43, "n_tiles": 6, "output_text": "A female tennis player in a purple outfit is preparing to hit a tennis ball on a blue court, with a scoreboard showing the names of the players and the score, and a camera operator capturing the action.", "error": null, "sys_before": {"cpu_percent": 48.1, "ram_used_mb": 24860.9, "ram_available_mb": 100911.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 24861.4, "ram_available_mb": 100910.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.66, "peak": 15.04, "min": 13.79}, "VDD_CPU_SOC_MSS": {"avg": 15.45, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 31.54, "peak": 40.56, "min": 24.43}, "VIN": {"avg": 76.35, "peak": 118.08, "min": 60.19}}, "power_watts_avg": 31.54, "energy_joules_est": 56.94, "sample_count": 13, "duration_seconds": 1.805}, "timestamp": "2026-01-17T14:45:17.716296"}
{"image_index": 118, "image_name": "000000012120.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012120.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1942.191, "latencies_ms": [1942.191], "images_per_second": 0.515, "prompt_tokens": 23, "response_tokens_est": 46, "n_tiles": 6, "output_text": "1. Tennis player\n2. Tennis racket\n3. Tennis ball\n4. Blue tennis court\n5. Blue net\n6. Blue advertising boards\n7. Blue seating area\n8. Blue spectator seats", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24861.4, "ram_available_mb": 100910.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 24862.1, "ram_available_mb": 100910.0, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.83, "peak": 15.24, "min": 14.3}, "VDD_CPU_SOC_MSS": {"avg": 15.49, "peak": 15.75, "min": 14.56}, "VDD_GPU": {"avg": 31.09, "peak": 41.34, "min": 23.64}, "VIN": {"avg": 72.57, "peak": 103.36, "min": 58.45}}, "power_watts_avg": 31.09, "energy_joules_est": 60.39, "sample_count": 15, "duration_seconds": 1.942}, "timestamp": "2026-01-17T14:45:19.664793"}
{"image_index": 118, "image_name": "000000012120.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012120.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3042.33, "latencies_ms": [3042.33], "images_per_second": 0.329, "prompt_tokens": 27, "response_tokens_est": 85, "n_tiles": 6, "output_text": "The main object in the foreground is a tennis player in a purple outfit, preparing to hit a tennis ball. The player is positioned near the net, which is the central focus of the image. In the background, there is a scoreboard displaying the names of the players and the score, with a crowd of spectators watching the match. The net is between the two players, dividing the court into two halves.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24862.1, "ram_available_mb": 100910.0, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 24863.8, "ram_available_mb": 100908.3, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.88, "peak": 15.34, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.56, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 28.17, "peak": 40.56, "min": 22.85}, "VIN": {"avg": 68.42, "peak": 110.43, "min": 53.88}}, "power_watts_avg": 28.17, "energy_joules_est": 85.71, "sample_count": 23, "duration_seconds": 3.043}, "timestamp": "2026-01-17T14:45:22.717053"}
{"image_index": 118, "image_name": "000000012120.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012120.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2515.573, "latencies_ms": [2515.573], "images_per_second": 0.398, "prompt_tokens": 21, "response_tokens_est": 66, "n_tiles": 6, "output_text": "The image depicts a professional tennis match taking place on a blue hard court. The court is surrounded by a blue net, and there are several people in the background, including a referee and a cameraman. The match is being broadcasted, as indicated by the presence of a Sony Ericsson logo on the court.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24863.8, "ram_available_mb": 100908.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 24868.8, "ram_available_mb": 100903.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.82, "peak": 15.14, "min": 14.2}, "VDD_CPU_SOC_MSS": {"avg": 15.32, "peak": 15.75, "min": 14.57}, "VDD_GPU": {"avg": 28.94, "peak": 40.95, "min": 22.86}, "VIN": {"avg": 71.53, "peak": 112.43, "min": 59.21}}, "power_watts_avg": 28.94, "energy_joules_est": 72.81, "sample_count": 20, "duration_seconds": 2.516}, "timestamp": "2026-01-17T14:45:25.238694"}
{"image_index": 118, "image_name": "000000012120.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012120.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1823.665, "latencies_ms": [1823.665], "images_per_second": 0.548, "prompt_tokens": 19, "response_tokens_est": 42, "n_tiles": 6, "output_text": "The image depicts a brightly lit blue tennis court with a vibrant blue surface. The court is surrounded by a white net, and the lighting is even, highlighting the players and the court's details.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24868.8, "ram_available_mb": 100903.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 24869.1, "ram_available_mb": 100903.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.83, "peak": 15.14, "min": 14.3}, "VDD_CPU_SOC_MSS": {"avg": 15.3, "peak": 15.74, "min": 14.56}, "VDD_GPU": {"avg": 31.54, "peak": 40.95, "min": 24.04}, "VIN": {"avg": 72.02, "peak": 105.67, "min": 54.09}}, "power_watts_avg": 31.54, "energy_joules_est": 57.54, "sample_count": 14, "duration_seconds": 1.824}, "timestamp": "2026-01-17T14:45:27.068790"}
{"image_index": 119, "image_name": "000000012280.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012280.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2225.924, "latencies_ms": [2225.924], "images_per_second": 0.449, "prompt_tokens": 9, "response_tokens_est": 18, "n_tiles": 12, "output_text": "A man in a suit is carrying a suitcase and walking through an airport terminal.", "error": null, "sys_before": {"cpu_percent": 47.7, "ram_used_mb": 24838.7, "ram_available_mb": 100933.5, "ram_percent": 19.7}, "sys_after": {"cpu_percent": 2.2, "ram_used_mb": 24839.9, "ram_available_mb": 100932.3, "ram_percent": 19.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.17, "peak": 16.66, "min": 13.39}, "VDD_CPU_SOC_MSS": {"avg": 15.98, "peak": 17.71, "min": 14.18}, "VDD_GPU": {"avg": 37.12, "peak": 43.73, "min": 29.15}, "VIN": {"avg": 81.71, "peak": 120.68, "min": 58.77}}, "power_watts_avg": 37.12, "energy_joules_est": 82.64, "sample_count": 17, "duration_seconds": 2.226}, "timestamp": "2026-01-17T14:45:29.390916"}
{"image_index": 119, "image_name": "000000012280.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012280.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 6028.172, "latencies_ms": [6028.172], "images_per_second": 0.166, "prompt_tokens": 23, "response_tokens_est": 129, "n_tiles": 12, "output_text": "- Exit sign: 1\n- Exit sign: 1\n- Exit sign: 1\n- Exit sign: 1\n- Exit sign: 1\n- Exit sign: 1\n- Exit sign: 1\n- Exit sign: 1\n- Exit sign: 1\n- Exit sign: 1\n- Exit sign: 1\n- Exit sign: 1\n- Exit sign: 1\n- Exit sign: 1\n- Exit sign: 1\n- Exit sign: 1\n- Exit sign: 1\n- Exit sign: 1\n- Exit", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 24839.9, "ram_available_mb": 100932.3, "ram_percent": 19.7}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 24840.9, "ram_available_mb": 100931.3, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.13, "peak": 16.56, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.68, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 30.89, "peak": 46.07, "min": 26.0}, "VIN": {"avg": 72.73, "peak": 133.36, "min": 55.18}}, "power_watts_avg": 30.89, "energy_joules_est": 186.22, "sample_count": 48, "duration_seconds": 6.028}, "timestamp": "2026-01-17T14:45:35.425361"}
{"image_index": 119, "image_name": "000000012280.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012280.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3776.928, "latencies_ms": [3776.928], "images_per_second": 0.265, "prompt_tokens": 27, "response_tokens_est": 62, "n_tiles": 12, "output_text": "The main objects in the image are a man in a suit carrying a suitcase and a glass door. The man is positioned in the foreground, while the glass door is located in the background. The suitcase is near the man, indicating that he is likely about to enter or exit through the door.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24840.9, "ram_available_mb": 100931.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 24841.1, "ram_available_mb": 100931.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.21, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.81, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 33.46, "peak": 44.91, "min": 26.01}, "VIN": {"avg": 79.88, "peak": 126.75, "min": 61.73}}, "power_watts_avg": 33.46, "energy_joules_est": 126.39, "sample_count": 29, "duration_seconds": 3.777}, "timestamp": "2026-01-17T14:45:39.210913"}
{"image_index": 119, "image_name": "000000012280.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012280.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3442.261, "latencies_ms": [3442.261], "images_per_second": 0.291, "prompt_tokens": 21, "response_tokens_est": 53, "n_tiles": 12, "output_text": "The image depicts an airport terminal with a modern, clean design. A man is seen walking towards the terminal, carrying a suitcase. The terminal is well-lit with a mix of natural and artificial lighting, and there are signs and information boards visible.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24841.1, "ram_available_mb": 100931.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 24841.1, "ram_available_mb": 100931.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.26, "peak": 16.66, "min": 14.12}, "VDD_CPU_SOC_MSS": {"avg": 15.82, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 34.53, "peak": 44.91, "min": 26.39}, "VIN": {"avg": 75.17, "peak": 117.25, "min": 64.97}}, "power_watts_avg": 34.53, "energy_joules_est": 118.88, "sample_count": 26, "duration_seconds": 3.443}, "timestamp": "2026-01-17T14:45:42.660356"}
{"image_index": 119, "image_name": "000000012280.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012280.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3296.085, "latencies_ms": [3296.085], "images_per_second": 0.303, "prompt_tokens": 19, "response_tokens_est": 49, "n_tiles": 12, "output_text": "The image depicts a modern, well-lit indoor setting with a sleek, metallic staircase and a glass door. The floor is made of polished concrete, and the overall lighting is bright and even, creating a clean and contemporary atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24841.1, "ram_available_mb": 100931.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 24842.7, "ram_available_mb": 100929.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.3, "peak": 16.66, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.87, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 34.94, "peak": 44.89, "min": 26.39}, "VIN": {"avg": 78.55, "peak": 115.76, "min": 61.95}}, "power_watts_avg": 34.94, "energy_joules_est": 115.18, "sample_count": 25, "duration_seconds": 3.297}, "timestamp": "2026-01-17T14:45:45.963415"}
{"image_index": 120, "image_name": "000000012576.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012576.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2931.381, "latencies_ms": [2931.381], "images_per_second": 0.341, "prompt_tokens": 9, "response_tokens_est": 39, "n_tiles": 12, "output_text": "The image shows a table with a pizza box, a glass of water, a glass of red wine, and a plate of pizza, all set on a colorful checkered tablecloth.", "error": null, "sys_before": {"cpu_percent": 42.6, "ram_used_mb": 24846.0, "ram_available_mb": 100926.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 24843.9, "ram_available_mb": 100928.3, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.14, "peak": 16.66, "min": 13.29}, "VDD_CPU_SOC_MSS": {"avg": 15.89, "peak": 17.71, "min": 14.18}, "VDD_GPU": {"avg": 35.16, "peak": 44.49, "min": 26.39}, "VIN": {"avg": 80.24, "peak": 128.79, "min": 64.09}}, "power_watts_avg": 35.16, "energy_joules_est": 103.08, "sample_count": 22, "duration_seconds": 2.932}, "timestamp": "2026-01-17T14:45:49.006616"}
{"image_index": 120, "image_name": "000000012576.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012576.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 6028.279, "latencies_ms": [6028.279], "images_per_second": 0.166, "prompt_tokens": 23, "response_tokens_est": 128, "n_tiles": 12, "output_text": "object: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject:", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24843.9, "ram_available_mb": 100928.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 24844.6, "ram_available_mb": 100927.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.13, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.66, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 30.77, "peak": 44.89, "min": 26.0}, "VIN": {"avg": 71.97, "peak": 115.18, "min": 56.39}}, "power_watts_avg": 30.77, "energy_joules_est": 185.5, "sample_count": 47, "duration_seconds": 6.029}, "timestamp": "2026-01-17T14:45:55.042120"}
{"image_index": 120, "image_name": "000000012576.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012576.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3951.677, "latencies_ms": [3951.677], "images_per_second": 0.253, "prompt_tokens": 27, "response_tokens_est": 67, "n_tiles": 12, "output_text": "The main objects in the image are a pizza and a glass of water. The pizza is placed on a pizza box, while the glass of water is placed on a tablecloth. The pizza box is positioned in the background, slightly out of focus, while the glass of water is in the foreground, closer to the viewer.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24844.6, "ram_available_mb": 100927.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 24844.8, "ram_available_mb": 100927.3, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.18, "peak": 16.66, "min": 14.12}, "VDD_CPU_SOC_MSS": {"avg": 15.66, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 33.11, "peak": 45.28, "min": 26.0}, "VIN": {"avg": 71.56, "peak": 105.15, "min": 60.62}}, "power_watts_avg": 33.11, "energy_joules_est": 130.86, "sample_count": 31, "duration_seconds": 3.952}, "timestamp": "2026-01-17T14:45:59.000907"}
{"image_index": 120, "image_name": "000000012576.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012576.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 4150.286, "latencies_ms": [4150.286], "images_per_second": 0.241, "prompt_tokens": 21, "response_tokens_est": 72, "n_tiles": 12, "output_text": "The image depicts a dining scene with a focus on a pizza box and a glass of water on a table. In the background, there is a television set on a stand, and various other items are scattered around the table, including a can of soda and a glass of red wine. The setting appears to be a home or a casual dining area.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24844.8, "ram_available_mb": 100927.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 24845.6, "ram_available_mb": 100926.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.96, "peak": 16.46, "min": 14.12}, "VDD_CPU_SOC_MSS": {"avg": 15.43, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 32.84, "peak": 45.28, "min": 25.6}, "VIN": {"avg": 76.62, "peak": 142.06, "min": 65.35}}, "power_watts_avg": 32.84, "energy_joules_est": 136.31, "sample_count": 32, "duration_seconds": 4.151}, "timestamp": "2026-01-17T14:46:03.157713"}
{"image_index": 120, "image_name": "000000012576.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012576.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3558.941, "latencies_ms": [3558.941], "images_per_second": 0.281, "prompt_tokens": 19, "response_tokens_est": 56, "n_tiles": 12, "output_text": "The image depicts a dining table with a pizza and a glass of water. The pizza has a golden-brown crust and a red tomato sauce, while the glass of water is clear and contains a few bubbles. The lighting is warm and soft, creating a cozy atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24845.6, "ram_available_mb": 100926.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 24850.6, "ram_available_mb": 100921.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.96, "peak": 16.26, "min": 13.92}, "VDD_CPU_SOC_MSS": {"avg": 15.53, "peak": 17.32, "min": 14.17}, "VDD_GPU": {"avg": 34.29, "peak": 45.69, "min": 26.0}, "VIN": {"avg": 76.26, "peak": 116.08, "min": 64.8}}, "power_watts_avg": 34.29, "energy_joules_est": 122.05, "sample_count": 27, "duration_seconds": 3.559}, "timestamp": "2026-01-17T14:46:06.726986"}
{"image_index": 121, "image_name": "000000012639.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012639.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2554.527, "latencies_ms": [2554.527], "images_per_second": 0.391, "prompt_tokens": 9, "response_tokens_est": 28, "n_tiles": 12, "output_text": "A young boy is crouched down, wearing a protective helmet and a red shirt, preparing to catch a baseball with his mitt.", "error": null, "sys_before": {"cpu_percent": 43.0, "ram_used_mb": 24836.3, "ram_available_mb": 100935.9, "ram_percent": 19.7}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 24837.7, "ram_available_mb": 100934.4, "ram_percent": 19.7}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.1, "peak": 16.66, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 15.79, "peak": 17.71, "min": 13.78}, "VDD_GPU": {"avg": 36.37, "peak": 44.12, "min": 27.97}, "VIN": {"avg": 74.92, "peak": 105.6, "min": 61.45}}, "power_watts_avg": 36.37, "energy_joules_est": 92.92, "sample_count": 19, "duration_seconds": 2.555}, "timestamp": "2026-01-17T14:46:09.397352"}
{"image_index": 121, "image_name": "000000012639.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012639.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 5966.826, "latencies_ms": [5966.826], "images_per_second": 0.168, "prompt_tokens": 23, "response_tokens_est": 128, "n_tiles": 12, "output_text": "baseball bat: 1\nbaseball glove: 1\ncatcher's mask: 1\ncatcher's uniform: 1\nbaseball uniform: 1\nbaseball uniform: 1\nbaseball uniform: 1\nbaseball uniform: 1\nbaseball uniform: 1\nbaseball uniform: 1\nbaseball uniform: 1\nbaseball uniform: 1\nbaseball uniform: 1\nbaseball uniform: 1\nbaseball uniform: 1\nbaseball uniform: 1\nbaseball uniform: 1\nbaseball uniform: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24837.7, "ram_available_mb": 100934.4, "ram_percent": 19.7}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 24845.9, "ram_available_mb": 100926.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.24, "peak": 16.56, "min": 14.43}, "VDD_CPU_SOC_MSS": {"avg": 15.86, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 30.95, "peak": 45.28, "min": 26.0}, "VIN": {"avg": 76.07, "peak": 138.54, "min": 64.97}}, "power_watts_avg": 30.95, "energy_joules_est": 184.69, "sample_count": 46, "duration_seconds": 5.967}, "timestamp": "2026-01-17T14:46:15.370900"}
{"image_index": 121, "image_name": "000000012639.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012639.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 4716.388, "latencies_ms": [4716.388], "images_per_second": 0.212, "prompt_tokens": 27, "response_tokens_est": 90, "n_tiles": 12, "output_text": "In the image, the baseball player is positioned in the foreground, crouched down near the dirt area. The baseball bat is held by the player, and the catcher is positioned behind him, wearing a protective helmet and mitt. The background features a group of people sitting on the grass, with a few standing and watching the game. The setting is a park with trees and a clear sky, providing a serene atmosphere for the game.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24845.9, "ram_available_mb": 100926.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 24846.4, "ram_available_mb": 100925.7, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.22, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.83, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 32.19, "peak": 44.49, "min": 25.61}, "VIN": {"avg": 74.51, "peak": 115.31, "min": 63.0}}, "power_watts_avg": 32.19, "energy_joules_est": 151.83, "sample_count": 36, "duration_seconds": 4.717}, "timestamp": "2026-01-17T14:46:20.095760"}
{"image_index": 121, "image_name": "000000012639.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012639.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 4148.002, "latencies_ms": [4148.002], "images_per_second": 0.241, "prompt_tokens": 21, "response_tokens_est": 73, "n_tiles": 12, "output_text": "The image depicts a lively outdoor scene at a park where a group of people is gathered. A young boy, wearing a baseball uniform and helmet, is preparing to bat, while a group of spectators watches from the sidelines. The setting is a sunny day, and the atmosphere is filled with anticipation and excitement as the boy takes his turn at bat.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24846.4, "ram_available_mb": 100925.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 24846.4, "ram_available_mb": 100925.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.19, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.8, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 32.86, "peak": 44.49, "min": 26.0}, "VIN": {"avg": 74.68, "peak": 114.74, "min": 61.79}}, "power_watts_avg": 32.86, "energy_joules_est": 136.31, "sample_count": 32, "duration_seconds": 4.148}, "timestamp": "2026-01-17T14:46:24.250795"}
{"image_index": 121, "image_name": "000000012639.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012639.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3846.729, "latencies_ms": [3846.729], "images_per_second": 0.26, "prompt_tokens": 19, "response_tokens_est": 64, "n_tiles": 12, "output_text": "The image depicts a sunny day at a park with a young baseball player in a red jersey and gray pants, equipped with a black helmet and protective gear, preparing to bat. The player is surrounded by a group of spectators, some seated on the grass, and others standing, all enjoying the outdoor activity.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24846.4, "ram_available_mb": 100925.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 24856.1, "ram_available_mb": 100916.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.18, "peak": 16.66, "min": 14.12}, "VDD_CPU_SOC_MSS": {"avg": 15.79, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 33.39, "peak": 44.89, "min": 26.0}, "VIN": {"avg": 78.97, "peak": 133.67, "min": 60.96}}, "power_watts_avg": 33.39, "energy_joules_est": 128.46, "sample_count": 30, "duration_seconds": 3.847}, "timestamp": "2026-01-17T14:46:28.104102"}
{"image_index": 122, "image_name": "000000012667.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012667.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2629.75, "latencies_ms": [2629.75], "images_per_second": 0.38, "prompt_tokens": 9, "response_tokens_est": 30, "n_tiles": 12, "output_text": "The image shows a black telephone with a banana placed on top of it, along with a calculator and a piece of paper with handwritten notes.", "error": null, "sys_before": {"cpu_percent": 43.0, "ram_used_mb": 24849.3, "ram_available_mb": 100922.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 24851.3, "ram_available_mb": 100920.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.14, "peak": 16.66, "min": 13.31}, "VDD_CPU_SOC_MSS": {"avg": 15.9, "peak": 17.71, "min": 14.18}, "VDD_GPU": {"avg": 35.77, "peak": 44.1, "min": 27.18}, "VIN": {"avg": 74.9, "peak": 98.55, "min": 57.46}}, "power_watts_avg": 35.77, "energy_joules_est": 94.08, "sample_count": 20, "duration_seconds": 2.63}, "timestamp": "2026-01-17T14:46:30.847945"}
{"image_index": 122, "image_name": "000000012667.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012667.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2453.533, "latencies_ms": [2453.533], "images_per_second": 0.408, "prompt_tokens": 23, "response_tokens_est": 23, "n_tiles": 12, "output_text": "- Phone\n- Banana\n- Calculator\n- Notebook\n- Cable\n- Keyboard\n- Paper", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24851.3, "ram_available_mb": 100920.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.4, "ram_used_mb": 24852.0, "ram_available_mb": 100920.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.28, "peak": 16.56, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.85, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 37.66, "peak": 45.68, "min": 28.36}, "VIN": {"avg": 84.96, "peak": 130.6, "min": 65.15}}, "power_watts_avg": 37.66, "energy_joules_est": 92.41, "sample_count": 19, "duration_seconds": 2.454}, "timestamp": "2026-01-17T14:46:33.308207"}
{"image_index": 122, "image_name": "000000012667.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012667.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 4504.824, "latencies_ms": [4504.824], "images_per_second": 0.222, "prompt_tokens": 27, "response_tokens_est": 83, "n_tiles": 12, "output_text": "The main objects in the image are a black telephone, a banana, and a calculator. The telephone is positioned in the foreground, slightly to the left, with its cord extending towards the right side of the image. The banana is placed to the right of the telephone, leaning against the calculator. The calculator is in the background, slightly to the right, with its screen facing the left side of the image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24852.0, "ram_available_mb": 100920.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 24856.8, "ram_available_mb": 100915.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.16, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.61, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 32.48, "peak": 45.28, "min": 26.0}, "VIN": {"avg": 76.06, "peak": 113.39, "min": 61.62}}, "power_watts_avg": 32.48, "energy_joules_est": 146.33, "sample_count": 35, "duration_seconds": 4.505}, "timestamp": "2026-01-17T14:46:37.819229"}
{"image_index": 122, "image_name": "000000012667.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012667.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 4464.738, "latencies_ms": [4464.738], "images_per_second": 0.224, "prompt_tokens": 21, "response_tokens_est": 82, "n_tiles": 12, "output_text": "The image depicts a cluttered desk with various items scattered around. The desk surface is white, and there are several objects present, including a black telephone with a cord, a black calculator, a yellow banana, and a piece of paper with handwritten text. The setting appears to be a workspace or study area, with the objects suggesting that someone might be engaged in some form of work or study.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 24856.8, "ram_available_mb": 100915.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 24856.7, "ram_available_mb": 100915.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.17, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.62, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 32.17, "peak": 45.28, "min": 25.6}, "VIN": {"avg": 76.79, "peak": 140.1, "min": 59.98}}, "power_watts_avg": 32.17, "energy_joules_est": 143.65, "sample_count": 35, "duration_seconds": 4.465}, "timestamp": "2026-01-17T14:46:42.291922"}
{"image_index": 122, "image_name": "000000012667.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012667.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3164.175, "latencies_ms": [3164.175], "images_per_second": 0.316, "prompt_tokens": 19, "response_tokens_est": 44, "n_tiles": 12, "output_text": "The image features a white desk with a yellow banana placed on it. The lighting is bright, casting soft shadows on the desk. The banana's vibrant color contrasts with the white surface, making it stand out.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 24856.7, "ram_available_mb": 100915.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 24856.9, "ram_available_mb": 100915.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.21, "peak": 16.56, "min": 14.12}, "VDD_CPU_SOC_MSS": {"avg": 15.78, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 35.12, "peak": 44.89, "min": 26.4}, "VIN": {"avg": 74.11, "peak": 95.13, "min": 58.3}}, "power_watts_avg": 35.12, "energy_joules_est": 111.14, "sample_count": 24, "duration_seconds": 3.164}, "timestamp": "2026-01-17T14:46:45.462260"}
{"image_index": 123, "image_name": "000000012670.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012670.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1799.843, "latencies_ms": [1799.843], "images_per_second": 0.556, "prompt_tokens": 9, "response_tokens_est": 44, "n_tiles": 6, "output_text": "The image depicts a bustling crowd of people, with a focus on a group of young women and girls gathered closely together, some holding stuffed animals, while others are engaged in conversation or looking at their phones.", "error": null, "sys_before": {"cpu_percent": 44.0, "ram_used_mb": 24881.1, "ram_available_mb": 100891.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 24882.6, "ram_available_mb": 100889.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.72, "peak": 15.11, "min": 13.72}, "VDD_CPU_SOC_MSS": {"avg": 15.35, "peak": 15.75, "min": 14.17}, "VDD_GPU": {"avg": 32.0, "peak": 40.57, "min": 24.82}, "VIN": {"avg": 80.09, "peak": 123.18, "min": 59.17}}, "power_watts_avg": 32.0, "energy_joules_est": 57.61, "sample_count": 13, "duration_seconds": 1.8}, "timestamp": "2026-01-17T14:46:47.329450"}
{"image_index": 123, "image_name": "000000012670.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012670.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1648.987, "latencies_ms": [1648.987], "images_per_second": 0.606, "prompt_tokens": 23, "response_tokens_est": 37, "n_tiles": 6, "output_text": "1. People\n2. Teddy bear\n3. Paper\n4. Bag\n5. Backpack\n6. Clothing\n7. Green bag\n8. Green bag", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24882.6, "ram_available_mb": 100889.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 24882.8, "ram_available_mb": 100889.3, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.99, "peak": 15.44, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.58, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 32.99, "peak": 40.56, "min": 25.21}, "VIN": {"avg": 74.03, "peak": 106.5, "min": 61.69}}, "power_watts_avg": 32.99, "energy_joules_est": 54.42, "sample_count": 12, "duration_seconds": 1.649}, "timestamp": "2026-01-17T14:46:48.984525"}
{"image_index": 123, "image_name": "000000012670.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012670.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2390.577, "latencies_ms": [2390.577], "images_per_second": 0.418, "prompt_tokens": 27, "response_tokens_est": 64, "n_tiles": 6, "output_text": "The main objects in the image are a group of people, with the foreground and background objects being more distant from the camera. The foreground includes a group of young women, while the background features a crowd of people. The people in the foreground are closer to the camera, while those in the background are further away.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24882.8, "ram_available_mb": 100889.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 24883.3, "ram_available_mb": 100888.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.04, "peak": 15.44, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.72, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 30.11, "peak": 40.95, "min": 23.64}, "VIN": {"avg": 68.45, "peak": 91.49, "min": 53.8}}, "power_watts_avg": 30.11, "energy_joules_est": 71.99, "sample_count": 18, "duration_seconds": 2.391}, "timestamp": "2026-01-17T14:46:51.380944"}
{"image_index": 123, "image_name": "000000012670.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012670.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2938.521, "latencies_ms": [2938.521], "images_per_second": 0.34, "prompt_tokens": 21, "response_tokens_est": 84, "n_tiles": 6, "output_text": "The image depicts a bustling outdoor scene with a large crowd of people gathered, likely at a public event or gathering. The setting appears to be a park or a similar open area, with greenery and trees in the background. The crowd is diverse, consisting of individuals of various ages and genders, some holding items like books or bags, and others engaged in conversation or looking at something out of frame.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24883.3, "ram_available_mb": 100888.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 24884.3, "ram_available_mb": 100887.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.07, "peak": 15.54, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.68, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 28.62, "peak": 40.18, "min": 23.25}, "VIN": {"avg": 70.4, "peak": 102.49, "min": 62.12}}, "power_watts_avg": 28.62, "energy_joules_est": 84.11, "sample_count": 22, "duration_seconds": 2.939}, "timestamp": "2026-01-17T14:46:54.329437"}
{"image_index": 123, "image_name": "000000012670.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012670.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2746.592, "latencies_ms": [2746.592], "images_per_second": 0.364, "prompt_tokens": 19, "response_tokens_est": 77, "n_tiles": 6, "output_text": "The image depicts a bustling outdoor scene with a diverse crowd of people. Notable visual attributes include the vibrant colors of the clothing worn by the individuals, such as red, blue, green, and black, which stand out against the overcast sky. The lighting is soft and diffused, likely due to the cloudy weather, casting a gentle glow over the scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24884.3, "ram_available_mb": 100887.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 24888.7, "ram_available_mb": 100883.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.03, "peak": 15.44, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.64, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 28.79, "peak": 39.77, "min": 23.24}, "VIN": {"avg": 71.56, "peak": 106.96, "min": 62.25}}, "power_watts_avg": 28.79, "energy_joules_est": 79.08, "sample_count": 21, "duration_seconds": 2.747}, "timestamp": "2026-01-17T14:46:57.081883"}
{"image_index": 124, "image_name": "000000012748.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012748.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2289.886, "latencies_ms": [2289.886], "images_per_second": 0.437, "prompt_tokens": 9, "response_tokens_est": 20, "n_tiles": 12, "output_text": "A man is holding a baby while standing next to a horse, both looking at each other.", "error": null, "sys_before": {"cpu_percent": 41.1, "ram_used_mb": 24852.3, "ram_available_mb": 100919.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.4, "ram_used_mb": 24853.7, "ram_available_mb": 100918.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.23, "peak": 16.66, "min": 13.69}, "VDD_CPU_SOC_MSS": {"avg": 16.07, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 37.17, "peak": 44.12, "min": 29.15}, "VIN": {"avg": 85.6, "peak": 124.0, "min": 66.64}}, "power_watts_avg": 37.17, "energy_joules_est": 85.13, "sample_count": 17, "duration_seconds": 2.29}, "timestamp": "2026-01-17T14:46:59.456117"}
{"image_index": 124, "image_name": "000000012748.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012748.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2747.869, "latencies_ms": [2747.869], "images_per_second": 0.364, "prompt_tokens": 23, "response_tokens_est": 32, "n_tiles": 12, "output_text": "1. Man\n2. Baby\n3. Horse\n4. Building\n5. Door\n6. Window\n7. Floor\n8. Chain", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24853.7, "ram_available_mb": 100918.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 24855.2, "ram_available_mb": 100917.0, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.1, "peak": 16.46, "min": 14.43}, "VDD_CPU_SOC_MSS": {"avg": 15.71, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 36.93, "peak": 45.3, "min": 26.79}, "VIN": {"avg": 81.1, "peak": 116.61, "min": 62.61}}, "power_watts_avg": 36.93, "energy_joules_est": 101.49, "sample_count": 21, "duration_seconds": 2.748}, "timestamp": "2026-01-17T14:47:02.210042"}
{"image_index": 124, "image_name": "000000012748.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012748.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3685.692, "latencies_ms": [3685.692], "images_per_second": 0.271, "prompt_tokens": 27, "response_tokens_est": 59, "n_tiles": 12, "output_text": "The main objects in the image are a man and a baby. The man is holding the baby in his arms. The baby is standing near the man, with the man's left arm extended towards the baby. The baby is positioned in the foreground, while the man is in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24855.2, "ram_available_mb": 100917.0, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 24863.4, "ram_available_mb": 100908.7, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.94, "peak": 16.36, "min": 14.02}, "VDD_CPU_SOC_MSS": {"avg": 15.55, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 33.97, "peak": 44.91, "min": 25.6}, "VIN": {"avg": 71.8, "peak": 113.29, "min": 56.95}}, "power_watts_avg": 33.97, "energy_joules_est": 125.21, "sample_count": 28, "duration_seconds": 3.686}, "timestamp": "2026-01-17T14:47:05.903316"}
{"image_index": 124, "image_name": "000000012748.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012748.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3477.663, "latencies_ms": [3477.663], "images_per_second": 0.288, "prompt_tokens": 21, "response_tokens_est": 53, "n_tiles": 12, "output_text": "The image depicts a man and a young child standing in an outdoor setting, likely a stable or barn, with a horse visible in the background. The man is holding the child, and both are smiling, suggesting a moment of joy and connection between them.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24863.4, "ram_available_mb": 100908.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 24864.4, "ram_available_mb": 100907.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.93, "peak": 16.36, "min": 13.92}, "VDD_CPU_SOC_MSS": {"avg": 15.53, "peak": 17.32, "min": 14.17}, "VDD_GPU": {"avg": 34.23, "peak": 45.3, "min": 25.61}, "VIN": {"avg": 76.09, "peak": 116.06, "min": 66.2}}, "power_watts_avg": 34.23, "energy_joules_est": 119.05, "sample_count": 27, "duration_seconds": 3.478}, "timestamp": "2026-01-17T14:47:09.387375"}
{"image_index": 124, "image_name": "000000012748.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012748.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3544.895, "latencies_ms": [3544.895], "images_per_second": 0.282, "prompt_tokens": 19, "response_tokens_est": 55, "n_tiles": 12, "output_text": "The image features a man and a baby in a rustic, stone-walled enclosure. The man is wearing a red shirt, and the baby is dressed in a light blue shirt. The lighting is natural, suggesting it is daytime, and the weather appears to be clear.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24864.4, "ram_available_mb": 100907.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 24865.6, "ram_available_mb": 100906.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.94, "peak": 16.36, "min": 14.02}, "VDD_CPU_SOC_MSS": {"avg": 15.47, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 34.27, "peak": 45.3, "min": 25.6}, "VIN": {"avg": 76.41, "peak": 119.84, "min": 50.76}}, "power_watts_avg": 34.27, "energy_joules_est": 121.49, "sample_count": 27, "duration_seconds": 3.545}, "timestamp": "2026-01-17T14:47:12.942606"}
{"image_index": 125, "image_name": "000000013004.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013004.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2356.802, "latencies_ms": [2356.802], "images_per_second": 0.424, "prompt_tokens": 9, "response_tokens_est": 22, "n_tiles": 12, "output_text": "The image shows a white plate with a small, round, and slightly translucent banana placed on it.", "error": null, "sys_before": {"cpu_percent": 42.2, "ram_used_mb": 24875.6, "ram_available_mb": 100896.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.4, "ram_used_mb": 24871.3, "ram_available_mb": 100900.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.1, "peak": 16.76, "min": 13.09}, "VDD_CPU_SOC_MSS": {"avg": 15.9, "peak": 17.71, "min": 14.17}, "VDD_GPU": {"avg": 36.96, "peak": 44.12, "min": 28.76}, "VIN": {"avg": 77.06, "peak": 108.65, "min": 57.32}}, "power_watts_avg": 36.96, "energy_joules_est": 87.12, "sample_count": 18, "duration_seconds": 2.357}, "timestamp": "2026-01-17T14:47:15.412375"}
{"image_index": 125, "image_name": "000000013004.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013004.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2748.055, "latencies_ms": [2748.055], "images_per_second": 0.364, "prompt_tokens": 23, "response_tokens_est": 32, "n_tiles": 12, "output_text": "1. plate\n2. banana\n3. bowl\n4. bowl\n5. bowl\n6. bowl\n7. bowl\n8. bowl", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 24871.3, "ram_available_mb": 100900.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 24878.7, "ram_available_mb": 100893.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.29, "peak": 16.56, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.86, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 36.81, "peak": 45.68, "min": 27.18}, "VIN": {"avg": 76.36, "peak": 120.21, "min": 63.0}}, "power_watts_avg": 36.81, "energy_joules_est": 101.17, "sample_count": 21, "duration_seconds": 2.749}, "timestamp": "2026-01-17T14:47:18.167279"}
{"image_index": 125, "image_name": "000000013004.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013004.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3359.642, "latencies_ms": [3359.642], "images_per_second": 0.298, "prompt_tokens": 27, "response_tokens_est": 50, "n_tiles": 12, "output_text": "The main object in the image is a white plate with a banana on it. The banana is placed on the plate, which is positioned on a wooden surface. The banana is the closest object to the viewer, while the plate is further away.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24878.7, "ram_available_mb": 100893.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 24875.2, "ram_available_mb": 100897.0, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.27, "peak": 16.66, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.82, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 34.7, "peak": 44.89, "min": 26.0}, "VIN": {"avg": 78.21, "peak": 141.76, "min": 59.78}}, "power_watts_avg": 34.7, "energy_joules_est": 116.59, "sample_count": 26, "duration_seconds": 3.36}, "timestamp": "2026-01-17T14:47:21.534658"}
{"image_index": 125, "image_name": "000000013004.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013004.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2818.684, "latencies_ms": [2818.684], "images_per_second": 0.355, "prompt_tokens": 21, "response_tokens_est": 34, "n_tiles": 12, "output_text": "The image shows a white plate with a banana on it, placed on a wooden table. The banana appears to be fresh and ripe, with a slightly curved shape.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24875.2, "ram_available_mb": 100897.0, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 24875.4, "ram_available_mb": 100896.7, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.27, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.87, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 36.11, "peak": 44.89, "min": 26.8}, "VIN": {"avg": 76.43, "peak": 103.55, "min": 67.13}}, "power_watts_avg": 36.11, "energy_joules_est": 101.8, "sample_count": 22, "duration_seconds": 2.819}, "timestamp": "2026-01-17T14:47:24.361849"}
{"image_index": 125, "image_name": "000000013004.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013004.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2817.161, "latencies_ms": [2817.161], "images_per_second": 0.355, "prompt_tokens": 19, "response_tokens_est": 34, "n_tiles": 12, "output_text": "The image features a white plate with a light brown rim, placed on a wooden table. The lighting is soft and natural, casting a warm glow on the scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24875.4, "ram_available_mb": 100896.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 24884.4, "ram_available_mb": 100887.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.28, "peak": 16.56, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.86, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 36.46, "peak": 45.3, "min": 26.79}, "VIN": {"avg": 80.53, "peak": 125.09, "min": 59.3}}, "power_watts_avg": 36.46, "energy_joules_est": 102.72, "sample_count": 21, "duration_seconds": 2.817}, "timestamp": "2026-01-17T14:47:27.185011"}
{"image_index": 126, "image_name": "000000013177.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013177.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1305.21, "latencies_ms": [1305.21], "images_per_second": 0.766, "prompt_tokens": 9, "response_tokens_est": 26, "n_tiles": 6, "output_text": "A man is kneeling on the ground, working on a metal wheel with a spoke wrench, while another person stands nearby.", "error": null, "sys_before": {"cpu_percent": 40.3, "ram_used_mb": 24903.4, "ram_available_mb": 100868.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 24904.3, "ram_available_mb": 100867.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.64, "peak": 15.24, "min": 13.82}, "VDD_CPU_SOC_MSS": {"avg": 15.31, "peak": 16.14, "min": 14.17}, "VDD_GPU": {"avg": 35.01, "peak": 40.95, "min": 27.97}, "VIN": {"avg": 78.49, "peak": 117.69, "min": 63.94}}, "power_watts_avg": 35.01, "energy_joules_est": 45.71, "sample_count": 9, "duration_seconds": 1.306}, "timestamp": "2026-01-17T14:47:28.551252"}
{"image_index": 126, "image_name": "000000013177.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013177.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1510.289, "latencies_ms": [1510.289], "images_per_second": 0.662, "prompt_tokens": 23, "response_tokens_est": 32, "n_tiles": 6, "output_text": "1. Wheel\n2. Wheel\n3. Wheel\n4. Wheel\n5. Wheel\n6. Wheel\n7. Wheel\n8. Wheel", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24904.3, "ram_available_mb": 100867.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 24904.8, "ram_available_mb": 100867.3, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.99, "peak": 15.54, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.57, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 34.19, "peak": 40.95, "min": 26.0}, "VIN": {"avg": 75.18, "peak": 110.15, "min": 60.72}}, "power_watts_avg": 34.19, "energy_joules_est": 51.65, "sample_count": 11, "duration_seconds": 1.511}, "timestamp": "2026-01-17T14:47:30.067152"}
{"image_index": 126, "image_name": "000000013177.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013177.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2245.321, "latencies_ms": [2245.321], "images_per_second": 0.445, "prompt_tokens": 27, "response_tokens_est": 59, "n_tiles": 6, "output_text": "The main object in the foreground is a man crouched down, working on a metal wheel. The wheel is placed on a flat surface, likely a workshop floor. In the background, there is a motorcycle and a bicycle, indicating that the man is working in a workshop or garage.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24904.8, "ram_available_mb": 100867.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 24905.1, "ram_available_mb": 100867.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.06, "peak": 15.54, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.68, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 30.56, "peak": 40.56, "min": 23.64}, "VIN": {"avg": 71.1, "peak": 107.04, "min": 60.4}}, "power_watts_avg": 30.56, "energy_joules_est": 68.63, "sample_count": 17, "duration_seconds": 2.246}, "timestamp": "2026-01-17T14:47:32.323768"}
{"image_index": 126, "image_name": "000000013177.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013177.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2376.833, "latencies_ms": [2376.833], "images_per_second": 0.421, "prompt_tokens": 21, "response_tokens_est": 64, "n_tiles": 6, "output_text": "The image depicts a man working on a motorcycle in a workshop or garage setting. He is focused on cleaning or repairing the front wheel of the motorcycle, which is supported by a metal stand. The man is wearing casual clothing and sandals, and the workshop environment is cluttered with various tools and parts.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24905.1, "ram_available_mb": 100867.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 24905.3, "ram_available_mb": 100866.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.07, "peak": 15.44, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.77, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 29.89, "peak": 40.16, "min": 23.64}, "VIN": {"avg": 72.12, "peak": 118.57, "min": 61.07}}, "power_watts_avg": 29.89, "energy_joules_est": 71.06, "sample_count": 18, "duration_seconds": 2.377}, "timestamp": "2026-01-17T14:47:34.706499"}
{"image_index": 126, "image_name": "000000013177.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013177.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2617.216, "latencies_ms": [2617.216], "images_per_second": 0.382, "prompt_tokens": 19, "response_tokens_est": 73, "n_tiles": 6, "output_text": "The image features a man wearing a green shirt and dark pants, crouched down in a workshop or garage. He is working on a metal wheel with a circular pattern, surrounded by various tools and machinery. The lighting is dim, with a subdued ambiance, and the man is wearing sandals, suggesting a warm, possibly humid environment.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24905.3, "ram_available_mb": 100866.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 24910.3, "ram_available_mb": 100861.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.1, "peak": 15.34, "min": 14.43}, "VDD_CPU_SOC_MSS": {"avg": 15.88, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 29.21, "peak": 40.95, "min": 23.64}, "VIN": {"avg": 71.01, "peak": 101.75, "min": 62.73}}, "power_watts_avg": 29.21, "energy_joules_est": 76.47, "sample_count": 21, "duration_seconds": 2.618}, "timestamp": "2026-01-17T14:47:37.329970"}
{"image_index": 127, "image_name": "000000013201.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013201.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1282.038, "latencies_ms": [1282.038], "images_per_second": 0.78, "prompt_tokens": 9, "response_tokens_est": 25, "n_tiles": 6, "output_text": "A skateboarder is performing a trick on a ramp in a park, with trees and a fence in the background.", "error": null, "sys_before": {"cpu_percent": 41.6, "ram_used_mb": 24910.3, "ram_available_mb": 100861.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 24910.3, "ram_available_mb": 100861.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.63, "peak": 15.14, "min": 13.9}, "VDD_CPU_SOC_MSS": {"avg": 15.4, "peak": 16.14, "min": 14.57}, "VDD_GPU": {"avg": 34.09, "peak": 40.16, "min": 27.57}, "VIN": {"avg": 77.53, "peak": 110.54, "min": 61.31}}, "power_watts_avg": 34.09, "energy_joules_est": 43.72, "sample_count": 9, "duration_seconds": 1.282}, "timestamp": "2026-01-17T14:47:38.671122"}
{"image_index": 127, "image_name": "000000013201.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013201.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2280.125, "latencies_ms": [2280.125], "images_per_second": 0.439, "prompt_tokens": 23, "response_tokens_est": 58, "n_tiles": 6, "output_text": "skateboard: 1\nskateboarder: 1\nskateboard: 1\nskateboarder: 1\nskateboard: 1\nskateboard: 1\nskateboard: 1\nskateboard: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24910.3, "ram_available_mb": 100861.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 24910.5, "ram_available_mb": 100861.7, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.87, "peak": 15.34, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.58, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 30.47, "peak": 41.34, "min": 23.24}, "VIN": {"avg": 71.23, "peak": 104.83, "min": 61.4}}, "power_watts_avg": 30.47, "energy_joules_est": 69.49, "sample_count": 17, "duration_seconds": 2.281}, "timestamp": "2026-01-17T14:47:40.957520"}
{"image_index": 127, "image_name": "000000013201.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013201.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2451.379, "latencies_ms": [2451.379], "images_per_second": 0.408, "prompt_tokens": 27, "response_tokens_est": 64, "n_tiles": 6, "output_text": "The main subject, a skateboarder, is in the foreground, performing a trick on a ramp. The skateboarder is positioned near the center of the image, with the ramp and the background elements slightly out of focus. The background features a fence and a building, providing context to the skateboarding scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24910.5, "ram_available_mb": 100861.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 24911.5, "ram_available_mb": 100860.7, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.85, "peak": 15.24, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.64, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 29.29, "peak": 40.56, "min": 23.24}, "VIN": {"avg": 66.66, "peak": 83.27, "min": 60.06}}, "power_watts_avg": 29.29, "energy_joules_est": 71.81, "sample_count": 19, "duration_seconds": 2.452}, "timestamp": "2026-01-17T14:47:43.415511"}
{"image_index": 127, "image_name": "000000013201.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013201.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2756.295, "latencies_ms": [2756.295], "images_per_second": 0.363, "prompt_tokens": 21, "response_tokens_est": 75, "n_tiles": 6, "output_text": "The image captures a dynamic moment of a skateboarder performing a trick on a concrete ramp in an outdoor setting. The skateboarder, dressed in a black t-shirt and black pants, is captured mid-air, with their body angled towards the ground and their arms extended. The background features a fence and a building, suggesting an urban park or skate park environment.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24911.5, "ram_available_mb": 100860.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 24911.7, "ram_available_mb": 100860.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.9, "peak": 15.24, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.56, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 28.57, "peak": 40.18, "min": 22.86}, "VIN": {"avg": 71.05, "peak": 114.99, "min": 62.46}}, "power_watts_avg": 28.57, "energy_joules_est": 78.76, "sample_count": 21, "duration_seconds": 2.757}, "timestamp": "2026-01-17T14:47:46.177731"}
{"image_index": 127, "image_name": "000000013201.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013201.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1966.499, "latencies_ms": [1966.499], "images_per_second": 0.509, "prompt_tokens": 19, "response_tokens_est": 47, "n_tiles": 6, "output_text": "The notable visual attributes in the image include the vibrant green grass, the clear blue sky, and the bright sunlight. The lighting is natural, casting a warm glow on the scene, and the weather appears to be sunny and clear.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24911.7, "ram_available_mb": 100860.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 24914.9, "ram_available_mb": 100857.3, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.85, "peak": 15.24, "min": 14.3}, "VDD_CPU_SOC_MSS": {"avg": 15.41, "peak": 15.75, "min": 14.56}, "VDD_GPU": {"avg": 30.91, "peak": 40.95, "min": 23.64}, "VIN": {"avg": 73.72, "peak": 113.34, "min": 58.87}}, "power_watts_avg": 30.91, "energy_joules_est": 60.8, "sample_count": 15, "duration_seconds": 1.967}, "timestamp": "2026-01-17T14:47:48.150419"}
{"image_index": 128, "image_name": "000000013291.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013291.jpg", "image_width": 500, "image_height": 335, "image_resolution": "500x335", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1444.017, "latencies_ms": [1444.017], "images_per_second": 0.693, "prompt_tokens": 9, "response_tokens_est": 31, "n_tiles": 6, "output_text": "The image depicts a group of people playing a game of frisbee outdoors during sunset, with the sky painted in shades of blue and orange.", "error": null, "sys_before": {"cpu_percent": 43.4, "ram_used_mb": 24914.9, "ram_available_mb": 100857.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 24916.1, "ram_available_mb": 100856.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.67, "peak": 15.14, "min": 13.69}, "VDD_CPU_SOC_MSS": {"avg": 15.39, "peak": 16.14, "min": 14.17}, "VDD_GPU": {"avg": 32.62, "peak": 40.16, "min": 25.6}, "VIN": {"avg": 80.95, "peak": 117.64, "min": 61.9}}, "power_watts_avg": 32.62, "energy_joules_est": 47.12, "sample_count": 11, "duration_seconds": 1.445}, "timestamp": "2026-01-17T14:47:49.652062"}
{"image_index": 128, "image_name": "000000013291.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013291.jpg", "image_width": 500, "image_height": 335, "image_resolution": "500x335", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1993.692, "latencies_ms": [1993.692], "images_per_second": 0.502, "prompt_tokens": 23, "response_tokens_est": 48, "n_tiles": 6, "output_text": "frisbee: 3\npeople: 4\nshorts: 2\nsocks: 2\nshoes: 2\nt-shirt: 2\nglasses: 1\ntrunks: 1", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 24916.1, "ram_available_mb": 100856.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 24916.3, "ram_available_mb": 100855.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.9, "peak": 15.34, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.54, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 31.3, "peak": 40.95, "min": 23.64}, "VIN": {"avg": 72.55, "peak": 106.92, "min": 57.91}}, "power_watts_avg": 31.3, "energy_joules_est": 62.42, "sample_count": 15, "duration_seconds": 1.994}, "timestamp": "2026-01-17T14:47:51.651777"}
{"image_index": 128, "image_name": "000000013291.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013291.jpg", "image_width": 500, "image_height": 335, "image_resolution": "500x335", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2081.055, "latencies_ms": [2081.055], "images_per_second": 0.481, "prompt_tokens": 27, "response_tokens_est": 51, "n_tiles": 6, "output_text": "The main objects in the image are a group of people and a frisbee. The people are positioned in the foreground, with two men and a woman holding the frisbees. The background features a clear sky and a distant tree line.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24916.3, "ram_available_mb": 100855.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 24916.3, "ram_available_mb": 100855.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.84, "peak": 15.24, "min": 14.3}, "VDD_CPU_SOC_MSS": {"avg": 15.46, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 30.86, "peak": 40.18, "min": 23.64}, "VIN": {"avg": 74.37, "peak": 113.51, "min": 62.42}}, "power_watts_avg": 30.86, "energy_joules_est": 64.24, "sample_count": 15, "duration_seconds": 2.082}, "timestamp": "2026-01-17T14:47:53.738716"}
{"image_index": 128, "image_name": "000000013291.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013291.jpg", "image_width": 500, "image_height": 335, "image_resolution": "500x335", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2022.285, "latencies_ms": [2022.285], "images_per_second": 0.494, "prompt_tokens": 21, "response_tokens_est": 49, "n_tiles": 6, "output_text": "The scene is set in an outdoor park during twilight, with a group of people gathered on a grassy field. They are holding and displaying various colored Frisbees, suggesting they are engaged in a game or a casual gathering.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24916.3, "ram_available_mb": 100855.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 24916.3, "ram_available_mb": 100855.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.83, "peak": 15.24, "min": 14.3}, "VDD_CPU_SOC_MSS": {"avg": 15.41, "peak": 15.75, "min": 14.56}, "VDD_GPU": {"avg": 31.04, "peak": 41.34, "min": 23.64}, "VIN": {"avg": 72.41, "peak": 99.53, "min": 57.17}}, "power_watts_avg": 31.04, "energy_joules_est": 62.78, "sample_count": 15, "duration_seconds": 2.023}, "timestamp": "2026-01-17T14:47:55.766748"}
{"image_index": 128, "image_name": "000000013291.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013291.jpg", "image_width": 500, "image_height": 335, "image_resolution": "500x335", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1938.951, "latencies_ms": [1938.951], "images_per_second": 0.516, "prompt_tokens": 19, "response_tokens_est": 46, "n_tiles": 6, "output_text": "The image depicts a group of people playing frisbee outdoors during sunset. The sky is clear with a gradient of blue, and the grass is green. The lighting is soft, casting a warm glow on the scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24916.3, "ram_available_mb": 100855.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 24920.9, "ram_available_mb": 100851.3, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.83, "peak": 15.24, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.38, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 31.4, "peak": 40.57, "min": 24.04}, "VIN": {"avg": 70.66, "peak": 104.43, "min": 60.16}}, "power_watts_avg": 31.4, "energy_joules_est": 60.89, "sample_count": 14, "duration_seconds": 1.939}, "timestamp": "2026-01-17T14:47:57.711589"}
{"image_index": 129, "image_name": "000000013348.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013348.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1602.501, "latencies_ms": [1602.501], "images_per_second": 0.624, "prompt_tokens": 9, "response_tokens_est": 37, "n_tiles": 6, "output_text": "The image shows a large, white airplane with red accents parked on a tarmac, with various ground support vehicles and personnel nearby, indicating that it is likely at an airport.", "error": null, "sys_before": {"cpu_percent": 42.5, "ram_used_mb": 24920.9, "ram_available_mb": 100851.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 24920.7, "ram_available_mb": 100851.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.7, "peak": 15.14, "min": 13.69}, "VDD_CPU_SOC_MSS": {"avg": 15.42, "peak": 16.14, "min": 14.18}, "VDD_GPU": {"avg": 32.24, "peak": 40.18, "min": 25.22}, "VIN": {"avg": 75.53, "peak": 115.27, "min": 60.93}}, "power_watts_avg": 32.24, "energy_joules_est": 51.68, "sample_count": 12, "duration_seconds": 1.603}, "timestamp": "2026-01-17T14:47:59.374842"}
{"image_index": 129, "image_name": "000000013348.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013348.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 4273.707, "latencies_ms": [4273.707], "images_per_second": 0.234, "prompt_tokens": 23, "response_tokens_est": 128, "n_tiles": 6, "output_text": "object: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject:", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24920.7, "ram_available_mb": 100851.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 24920.6, "ram_available_mb": 100851.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.96, "peak": 15.34, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.69, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 26.63, "peak": 41.34, "min": 22.85}, "VIN": {"avg": 67.9, "peak": 113.21, "min": 60.72}}, "power_watts_avg": 26.63, "energy_joules_est": 113.83, "sample_count": 34, "duration_seconds": 4.274}, "timestamp": "2026-01-17T14:48:03.654879"}
{"image_index": 129, "image_name": "000000013348.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013348.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2907.197, "latencies_ms": [2907.197], "images_per_second": 0.344, "prompt_tokens": 27, "response_tokens_est": 80, "n_tiles": 6, "output_text": "The main object in the foreground is a large airplane with a red tail and a Japanese flag on its fuselage. The airplane is parked on the tarmac, with its landing gear extended. In the background, there are other aircraft and airport infrastructure, including a building and a palm tree. The airplane is positioned near the tarmac, with other ground vehicles and personnel nearby.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24920.6, "ram_available_mb": 100851.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 24920.9, "ram_available_mb": 100851.3, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.88, "peak": 15.14, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.46, "peak": 15.75, "min": 14.57}, "VDD_GPU": {"avg": 28.4, "peak": 40.97, "min": 22.85}, "VIN": {"avg": 70.98, "peak": 115.41, "min": 57.0}}, "power_watts_avg": 28.4, "energy_joules_est": 82.58, "sample_count": 22, "duration_seconds": 2.908}, "timestamp": "2026-01-17T14:48:06.572314"}
{"image_index": 129, "image_name": "000000013348.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013348.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2424.052, "latencies_ms": [2424.052], "images_per_second": 0.413, "prompt_tokens": 21, "response_tokens_est": 63, "n_tiles": 6, "output_text": "The image depicts a large commercial airplane parked on a tarmac at an airport. The scene is set under a partly cloudy sky, with various airport ground support vehicles and personnel present. The airplane, featuring a red tail and a logo, is likely preparing for or has just arrived from a flight.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24920.9, "ram_available_mb": 100851.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 24925.7, "ram_available_mb": 100846.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.83, "peak": 15.24, "min": 14.2}, "VDD_CPU_SOC_MSS": {"avg": 15.51, "peak": 15.75, "min": 14.56}, "VDD_GPU": {"avg": 29.39, "peak": 40.18, "min": 23.24}, "VIN": {"avg": 69.32, "peak": 110.32, "min": 61.42}}, "power_watts_avg": 29.39, "energy_joules_est": 71.26, "sample_count": 18, "duration_seconds": 2.425}, "timestamp": "2026-01-17T14:48:09.006364"}
{"image_index": 129, "image_name": "000000013348.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013348.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2392.4, "latencies_ms": [2392.4], "images_per_second": 0.418, "prompt_tokens": 19, "response_tokens_est": 62, "n_tiles": 6, "output_text": "The notable visual attributes of the image include a large, white airplane with red accents on its tail and fuselage, parked on a tarmac under a clear blue sky with scattered white clouds. The lighting is bright and natural, indicating daytime, and the weather appears to be clear and sunny.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24925.7, "ram_available_mb": 100846.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 24925.3, "ram_available_mb": 100846.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.83, "peak": 15.24, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.49, "peak": 15.75, "min": 14.56}, "VDD_GPU": {"avg": 29.54, "peak": 40.56, "min": 23.24}, "VIN": {"avg": 71.93, "peak": 105.56, "min": 60.11}}, "power_watts_avg": 29.54, "energy_joules_est": 70.69, "sample_count": 18, "duration_seconds": 2.393}, "timestamp": "2026-01-17T14:48:11.405002"}
{"image_index": 130, "image_name": "000000013546.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013546.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1385.389, "latencies_ms": [1385.389], "images_per_second": 0.722, "prompt_tokens": 9, "response_tokens_est": 29, "n_tiles": 6, "output_text": "A young person is performing a skateboard trick on a concrete ledge in a park, surrounded by green grass and a few benches.", "error": null, "sys_before": {"cpu_percent": 40.6, "ram_used_mb": 24924.8, "ram_available_mb": 100847.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 24924.7, "ram_available_mb": 100847.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.68, "peak": 15.24, "min": 13.9}, "VDD_CPU_SOC_MSS": {"avg": 15.43, "peak": 16.14, "min": 14.57}, "VDD_GPU": {"avg": 33.24, "peak": 40.16, "min": 26.39}, "VIN": {"avg": 82.39, "peak": 120.9, "min": 63.71}}, "power_watts_avg": 33.24, "energy_joules_est": 46.06, "sample_count": 10, "duration_seconds": 1.386}, "timestamp": "2026-01-17T14:48:12.839841"}
{"image_index": 130, "image_name": "000000013546.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013546.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1826.898, "latencies_ms": [1826.898], "images_per_second": 0.547, "prompt_tokens": 23, "response_tokens_est": 42, "n_tiles": 6, "output_text": "1. Person\n2. Skateboard\n3. Park bench\n4. Sidewalk\n5. Street\n6. Green fence\n7. Green trash can\n8. Green metal structure", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24924.7, "ram_available_mb": 100847.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 24924.7, "ram_available_mb": 100847.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.84, "peak": 15.34, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.41, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 31.63, "peak": 40.95, "min": 24.04}, "VIN": {"avg": 68.67, "peak": 113.32, "min": 60.81}}, "power_watts_avg": 31.63, "energy_joules_est": 57.8, "sample_count": 14, "duration_seconds": 1.827}, "timestamp": "2026-01-17T14:48:14.672814"}
{"image_index": 130, "image_name": "000000013546.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013546.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3393.428, "latencies_ms": [3393.428], "images_per_second": 0.295, "prompt_tokens": 27, "response_tokens_est": 97, "n_tiles": 6, "output_text": "In the image, the main subject is a young person skateboarding on a concrete ledge. The skateboarder is positioned near the foreground, with their body leaning forward and arms outstretched for balance. The concrete ledge is located in the middle ground, and the background features a park setting with green grass, a wooden fence, and a bench. The skateboarder is near the fence, and there is a trash can and a street sign visible in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24924.7, "ram_available_mb": 100847.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 24925.8, "ram_available_mb": 100846.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.88, "peak": 15.24, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.61, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 27.59, "peak": 40.56, "min": 22.86}, "VIN": {"avg": 70.11, "peak": 105.1, "min": 56.84}}, "power_watts_avg": 27.59, "energy_joules_est": 93.64, "sample_count": 26, "duration_seconds": 3.394}, "timestamp": "2026-01-17T14:48:18.072413"}
{"image_index": 130, "image_name": "000000013546.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013546.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2424.306, "latencies_ms": [2424.306], "images_per_second": 0.412, "prompt_tokens": 21, "response_tokens_est": 63, "n_tiles": 6, "output_text": "The image depicts a young person skateboarding in a park-like setting. The skateboarder is performing a trick on a concrete ledge, surrounded by green grass and a few benches. The scene is set in a public area with a paved path, trees, and a fence in the background.", "error": null, "sys_before": {"cpu_percent": 50.0, "ram_used_mb": 24925.8, "ram_available_mb": 100846.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 24925.6, "ram_available_mb": 100846.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.84, "peak": 15.24, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.31, "peak": 15.74, "min": 14.56}, "VDD_GPU": {"avg": 29.42, "peak": 40.18, "min": 22.86}, "VIN": {"avg": 71.08, "peak": 106.77, "min": 57.16}}, "power_watts_avg": 29.42, "energy_joules_est": 71.33, "sample_count": 18, "duration_seconds": 2.425}, "timestamp": "2026-01-17T14:48:20.502595"}
{"image_index": 130, "image_name": "000000013546.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013546.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3819.081, "latencies_ms": [3819.081], "images_per_second": 0.262, "prompt_tokens": 19, "response_tokens_est": 112, "n_tiles": 6, "output_text": "The image depicts a young person skateboarding on a concrete ramp in a park. The skateboarder is wearing a yellow t-shirt, black pants, and a black cap, and is captured mid-action, with their body leaning forward and arms outstretched for balance. The scene is bathed in natural sunlight, casting long shadows on the ground, indicating it is likely a bright, sunny day. The park is well-maintained with green grass, a wooden fence, benches, and a trash can, creating a vibrant and inviting atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24925.6, "ram_available_mb": 100846.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 24925.8, "ram_available_mb": 100846.3, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.88, "peak": 15.24, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.46, "peak": 15.75, "min": 14.56}, "VDD_GPU": {"avg": 26.9, "peak": 40.18, "min": 22.85}, "VIN": {"avg": 69.23, "peak": 104.2, "min": 52.57}}, "power_watts_avg": 26.9, "energy_joules_est": 102.74, "sample_count": 29, "duration_seconds": 3.819}, "timestamp": "2026-01-17T14:48:24.327164"}
{"image_index": 131, "image_name": "000000013597.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013597.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1333.772, "latencies_ms": [1333.772], "images_per_second": 0.75, "prompt_tokens": 9, "response_tokens_est": 27, "n_tiles": 6, "output_text": "The image shows a close-up of a chocolate cake with a dollop of cream on a plate, which has a decorative pattern.", "error": null, "sys_before": {"cpu_percent": 47.6, "ram_used_mb": 24925.8, "ram_available_mb": 100846.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 24926.8, "ram_available_mb": 100845.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.64, "peak": 15.14, "min": 13.69}, "VDD_CPU_SOC_MSS": {"avg": 15.35, "peak": 16.14, "min": 14.18}, "VDD_GPU": {"avg": 33.05, "peak": 39.77, "min": 26.4}, "VIN": {"avg": 77.14, "peak": 109.89, "min": 61.38}}, "power_watts_avg": 33.05, "energy_joules_est": 44.09, "sample_count": 10, "duration_seconds": 1.334}, "timestamp": "2026-01-17T14:48:25.723134"}
{"image_index": 131, "image_name": "000000013597.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013597.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1764.196, "latencies_ms": [1764.196], "images_per_second": 0.567, "prompt_tokens": 23, "response_tokens_est": 40, "n_tiles": 6, "output_text": "object: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24926.8, "ram_available_mb": 100845.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 24927.1, "ram_available_mb": 100845.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.82, "peak": 15.24, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.51, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 32.42, "peak": 40.97, "min": 24.42}, "VIN": {"avg": 73.27, "peak": 115.78, "min": 62.82}}, "power_watts_avg": 32.42, "energy_joules_est": 57.22, "sample_count": 13, "duration_seconds": 1.765}, "timestamp": "2026-01-17T14:48:27.493908"}
{"image_index": 131, "image_name": "000000013597.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013597.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2762.992, "latencies_ms": [2762.992], "images_per_second": 0.362, "prompt_tokens": 27, "response_tokens_est": 75, "n_tiles": 6, "output_text": "The main object in the image is a chocolate cake, which is positioned in the foreground. The cake is placed on a white plate with a floral pattern, making it the central focus of the image. The cake is being cut by a knife, which is in the foreground and slightly to the right. The background is blurred, emphasizing the cake and the knife.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24927.1, "ram_available_mb": 100845.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 24932.4, "ram_available_mb": 100839.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.87, "peak": 15.24, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.6, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 28.79, "peak": 40.56, "min": 22.86}, "VIN": {"avg": 69.9, "peak": 110.18, "min": 61.1}}, "power_watts_avg": 28.79, "energy_joules_est": 79.55, "sample_count": 21, "duration_seconds": 2.763}, "timestamp": "2026-01-17T14:48:30.262927"}
{"image_index": 131, "image_name": "000000013597.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013597.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2274.968, "latencies_ms": [2274.968], "images_per_second": 0.44, "prompt_tokens": 21, "response_tokens_est": 58, "n_tiles": 6, "output_text": "The image depicts a close-up view of a chocolate cake with a dollop of cream on a white plate adorned with gold floral patterns. The cake is being cut with a knife, and the cream is being poured onto the cake, creating a visually appealing and appetizing scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24932.4, "ram_available_mb": 100839.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 24932.6, "ram_available_mb": 100839.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.86, "peak": 15.24, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.54, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 29.89, "peak": 40.18, "min": 23.25}, "VIN": {"avg": 72.02, "peak": 115.13, "min": 59.61}}, "power_watts_avg": 29.89, "energy_joules_est": 68.01, "sample_count": 17, "duration_seconds": 2.275}, "timestamp": "2026-01-17T14:48:32.544418"}
{"image_index": 131, "image_name": "000000013597.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013597.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2189.225, "latencies_ms": [2189.225], "images_per_second": 0.457, "prompt_tokens": 19, "response_tokens_est": 55, "n_tiles": 6, "output_text": "The image features a chocolate cake with a glossy, smooth chocolate glaze and a golden-brown frosting. The cake is placed on a white plate with a floral pattern, and the lighting is warm and soft, highlighting the rich colors and textures of the cake.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24932.6, "ram_available_mb": 100839.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 24933.6, "ram_available_mb": 100838.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.84, "peak": 15.24, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.53, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 30.26, "peak": 40.18, "min": 23.24}, "VIN": {"avg": 69.13, "peak": 89.97, "min": 61.5}}, "power_watts_avg": 30.26, "energy_joules_est": 66.25, "sample_count": 16, "duration_seconds": 2.19}, "timestamp": "2026-01-17T14:48:34.739636"}
{"image_index": 132, "image_name": "000000013659.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013659.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2860.212, "latencies_ms": [2860.212], "images_per_second": 0.35, "prompt_tokens": 9, "response_tokens_est": 37, "n_tiles": 12, "output_text": "The image depicts a cluttered office space with various items scattered across the floor and tables, including a laptop, a person working on it, and other office supplies and equipment.", "error": null, "sys_before": {"cpu_percent": 42.9, "ram_used_mb": 24909.5, "ram_available_mb": 100862.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 24909.7, "ram_available_mb": 100862.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.17, "peak": 16.76, "min": 13.59}, "VDD_CPU_SOC_MSS": {"avg": 15.94, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 35.02, "peak": 44.12, "min": 26.79}, "VIN": {"avg": 76.73, "peak": 107.02, "min": 65.51}}, "power_watts_avg": 35.02, "energy_joules_est": 100.18, "sample_count": 22, "duration_seconds": 2.861}, "timestamp": "2026-01-17T14:48:37.680069"}
{"image_index": 132, "image_name": "000000013659.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013659.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2790.867, "latencies_ms": [2790.867], "images_per_second": 0.358, "prompt_tokens": 23, "response_tokens_est": 33, "n_tiles": 12, "output_text": "1. Laptop\n2. Computer monitor\n3. Paper\n4. Chair\n5. Box\n6. Box\n7. Box\n8. Box", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24909.7, "ram_available_mb": 100862.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 24911.1, "ram_available_mb": 100861.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.24, "peak": 16.66, "min": 14.12}, "VDD_CPU_SOC_MSS": {"avg": 15.89, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 36.44, "peak": 44.49, "min": 26.79}, "VIN": {"avg": 75.78, "peak": 112.47, "min": 57.83}}, "power_watts_avg": 36.44, "energy_joules_est": 101.72, "sample_count": 21, "duration_seconds": 2.791}, "timestamp": "2026-01-17T14:48:40.478652"}
{"image_index": 132, "image_name": "000000013659.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013659.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 4698.564, "latencies_ms": [4698.564], "images_per_second": 0.213, "prompt_tokens": 27, "response_tokens_est": 89, "n_tiles": 12, "output_text": "In the image, the main objects are arranged in a somewhat cluttered workspace. The foreground features a man seated at a desk, working on a laptop. To his left, there is a red chair and a small table with various items on it. In the background, there are more desks and chairs, with a person standing and another person sitting at a table. The overall spatial arrangement suggests a busy and somewhat disorganized workspace.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 24911.1, "ram_available_mb": 100861.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 24910.9, "ram_available_mb": 100861.3, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.16, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.63, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 32.19, "peak": 45.69, "min": 26.0}, "VIN": {"avg": 73.45, "peak": 112.99, "min": 66.6}}, "power_watts_avg": 32.19, "energy_joules_est": 151.26, "sample_count": 37, "duration_seconds": 4.699}, "timestamp": "2026-01-17T14:48:45.183990"}
{"image_index": 132, "image_name": "000000013659.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013659.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 4015.078, "latencies_ms": [4015.078], "images_per_second": 0.249, "prompt_tokens": 21, "response_tokens_est": 69, "n_tiles": 12, "output_text": "The image depicts a cluttered office or workshop environment with various items scattered across the floor and tables. A group of people is engaged in work, with some seated at desks and others standing. The setting appears to be a workspace, possibly for electronics or technical work, with a mix of electronic devices, papers, and tools visible.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24910.9, "ram_available_mb": 100861.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 24920.4, "ram_available_mb": 100851.7, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.17, "peak": 16.66, "min": 14.12}, "VDD_CPU_SOC_MSS": {"avg": 15.66, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 33.13, "peak": 44.89, "min": 26.0}, "VIN": {"avg": 74.35, "peak": 112.75, "min": 60.14}}, "power_watts_avg": 33.13, "energy_joules_est": 133.04, "sample_count": 31, "duration_seconds": 4.016}, "timestamp": "2026-01-17T14:48:49.205507"}
{"image_index": 132, "image_name": "000000013659.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013659.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2771.816, "latencies_ms": [2771.816], "images_per_second": 0.361, "prompt_tokens": 19, "response_tokens_est": 33, "n_tiles": 12, "output_text": "The image depicts a cluttered office space with a yellowish floor and walls. The lighting is dim, with a mix of natural and artificial light sources.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24920.4, "ram_available_mb": 100851.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 24921.2, "ram_available_mb": 100851.0, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.29, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.82, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 36.45, "peak": 44.91, "min": 27.18}, "VIN": {"avg": 83.76, "peak": 141.43, "min": 65.64}}, "power_watts_avg": 36.45, "energy_joules_est": 101.04, "sample_count": 21, "duration_seconds": 2.772}, "timestamp": "2026-01-17T14:48:51.983571"}
{"image_index": 133, "image_name": "000000013729.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013729.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2786.739, "latencies_ms": [2786.739], "images_per_second": 0.359, "prompt_tokens": 9, "response_tokens_est": 35, "n_tiles": 12, "output_text": "The image depicts a group of people in a living room, playing a video game together, with one person standing on the floor and the others standing on a couch.", "error": null, "sys_before": {"cpu_percent": 42.7, "ram_used_mb": 24922.6, "ram_available_mb": 100849.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 24918.5, "ram_available_mb": 100853.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.23, "peak": 16.66, "min": 13.61}, "VDD_CPU_SOC_MSS": {"avg": 15.97, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 35.79, "peak": 44.51, "min": 26.8}, "VIN": {"avg": 78.06, "peak": 110.49, "min": 63.88}}, "power_watts_avg": 35.79, "energy_joules_est": 99.75, "sample_count": 21, "duration_seconds": 2.787}, "timestamp": "2026-01-17T14:48:54.861296"}
{"image_index": 133, "image_name": "000000013729.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013729.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 3326.684, "latencies_ms": [3326.684], "images_per_second": 0.301, "prompt_tokens": 23, "response_tokens_est": 49, "n_tiles": 12, "output_text": "- woman: 1\n- man: 3\n- game controller: 1\n- couch: 1\n- table: 1\n- chair: 1\n- floor: 1\n- wall: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24918.5, "ram_available_mb": 100853.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 24918.4, "ram_available_mb": 100853.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.24, "peak": 16.66, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.9, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 34.71, "peak": 44.91, "min": 26.0}, "VIN": {"avg": 77.23, "peak": 113.29, "min": 62.46}}, "power_watts_avg": 34.71, "energy_joules_est": 115.48, "sample_count": 26, "duration_seconds": 3.327}, "timestamp": "2026-01-17T14:48:58.193979"}
{"image_index": 133, "image_name": "000000013729.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013729.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 4372.298, "latencies_ms": [4372.298], "images_per_second": 0.229, "prompt_tokens": 27, "response_tokens_est": 78, "n_tiles": 12, "output_text": "The main objects in the image are a group of people playing a video game in a living room. The individuals are positioned in the foreground, with the woman in the center foreground and the two men on either side. The background features a couch, a wooden cabinet, and a television stand. The room is well-lit, with a carpeted floor and various personal items scattered around.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24918.4, "ram_available_mb": 100853.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 24926.1, "ram_available_mb": 100846.0, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.02, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.5, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 32.38, "peak": 45.28, "min": 25.6}, "VIN": {"avg": 74.01, "peak": 106.28, "min": 61.58}}, "power_watts_avg": 32.38, "energy_joules_est": 141.59, "sample_count": 34, "duration_seconds": 4.373}, "timestamp": "2026-01-17T14:49:02.572756"}
{"image_index": 133, "image_name": "000000013729.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013729.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 5010.633, "latencies_ms": [5010.633], "images_per_second": 0.2, "prompt_tokens": 21, "response_tokens_est": 97, "n_tiles": 12, "output_text": "The image depicts a casual indoor scene where three individuals are engaged in a video game. The setting appears to be a living room with a carpeted floor, a couch, a wooden table, and various personal items scattered around. The individuals are casually dressed, with one person in a white tank top and shorts, another in a striped shirt and shorts, and the third in a green shirt and shorts. They are all barefoot, suggesting a relaxed and informal atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24926.1, "ram_available_mb": 100846.0, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 24924.7, "ram_available_mb": 100847.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.85, "peak": 16.36, "min": 13.92}, "VDD_CPU_SOC_MSS": {"avg": 15.29, "peak": 17.32, "min": 14.17}, "VDD_GPU": {"avg": 31.57, "peak": 45.69, "min": 25.21}, "VIN": {"avg": 72.5, "peak": 117.03, "min": 53.21}}, "power_watts_avg": 31.57, "energy_joules_est": 158.2, "sample_count": 40, "duration_seconds": 5.011}, "timestamp": "2026-01-17T14:49:07.590172"}
{"image_index": 133, "image_name": "000000013729.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013729.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3448.888, "latencies_ms": [3448.888], "images_per_second": 0.29, "prompt_tokens": 19, "response_tokens_est": 52, "n_tiles": 12, "output_text": "The image depicts a cozy living room with a beige carpeted floor. The room is well-lit, with a warm and inviting ambiance. The walls are painted in a light color, and there is a wooden coffee table in the foreground.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24924.7, "ram_available_mb": 100847.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 24931.0, "ram_available_mb": 100841.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.92, "peak": 16.36, "min": 13.92}, "VDD_CPU_SOC_MSS": {"avg": 15.47, "peak": 17.32, "min": 14.17}, "VDD_GPU": {"avg": 34.41, "peak": 45.3, "min": 25.6}, "VIN": {"avg": 72.82, "peak": 115.98, "min": 51.42}}, "power_watts_avg": 34.41, "energy_joules_est": 118.69, "sample_count": 26, "duration_seconds": 3.449}, "timestamp": "2026-01-17T14:49:11.046144"}
{"image_index": 134, "image_name": "000000013774.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013774.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2256.079, "latencies_ms": [2256.079], "images_per_second": 0.443, "prompt_tokens": 9, "response_tokens_est": 19, "n_tiles": 12, "output_text": "A lone person stands on a snowy beach, watching the sun set over the ocean.", "error": null, "sys_before": {"cpu_percent": 41.4, "ram_used_mb": 24921.0, "ram_available_mb": 100851.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.2, "ram_used_mb": 24921.7, "ram_available_mb": 100850.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.12, "peak": 16.66, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 15.91, "peak": 17.71, "min": 14.17}, "VDD_GPU": {"avg": 37.05, "peak": 43.73, "min": 29.15}, "VIN": {"avg": 78.99, "peak": 112.87, "min": 66.37}}, "power_watts_avg": 37.05, "energy_joules_est": 83.6, "sample_count": 17, "duration_seconds": 2.256}, "timestamp": "2026-01-17T14:49:13.417164"}
{"image_index": 134, "image_name": "000000013774.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013774.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 3030.865, "latencies_ms": [3030.865], "images_per_second": 0.33, "prompt_tokens": 23, "response_tokens_est": 40, "n_tiles": 12, "output_text": "object: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24921.7, "ram_available_mb": 100850.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 24922.7, "ram_available_mb": 100849.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.25, "peak": 16.66, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.8, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 36.0, "peak": 45.68, "min": 26.39}, "VIN": {"avg": 79.27, "peak": 115.32, "min": 66.25}}, "power_watts_avg": 36.0, "energy_joules_est": 109.13, "sample_count": 23, "duration_seconds": 3.031}, "timestamp": "2026-01-17T14:49:16.454819"}
{"image_index": 134, "image_name": "000000013774.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013774.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3481.248, "latencies_ms": [3481.248], "images_per_second": 0.287, "prompt_tokens": 27, "response_tokens_est": 53, "n_tiles": 12, "output_text": "The main objects in the image are the sun, the person, and the beach. The person is standing near the water's edge, with the sun positioned in the background. The beach is the foreground, with the person and the sun being the main subjects.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24922.7, "ram_available_mb": 100849.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 24922.9, "ram_available_mb": 100849.3, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.2, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.72, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 34.4, "peak": 44.49, "min": 26.0}, "VIN": {"avg": 76.67, "peak": 129.03, "min": 55.97}}, "power_watts_avg": 34.4, "energy_joules_est": 119.77, "sample_count": 26, "duration_seconds": 3.482}, "timestamp": "2026-01-17T14:49:19.943284"}
{"image_index": 134, "image_name": "000000013774.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013774.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3709.457, "latencies_ms": [3709.457], "images_per_second": 0.27, "prompt_tokens": 21, "response_tokens_est": 60, "n_tiles": 12, "output_text": "The image captures a serene beach scene during sunset. A lone person stands on the sandy shore, gazing at the sun as it sets behind the horizon. The tranquil atmosphere is enhanced by the calm ocean and the soft glow of the setting sun, creating a peaceful and reflective mood.", "error": null, "sys_before": {"cpu_percent": 14.3, "ram_used_mb": 24922.9, "ram_available_mb": 100849.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 24922.6, "ram_available_mb": 100849.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.11, "peak": 16.56, "min": 14.02}, "VDD_CPU_SOC_MSS": {"avg": 15.62, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 33.71, "peak": 45.28, "min": 26.0}, "VIN": {"avg": 73.91, "peak": 119.8, "min": 55.09}}, "power_watts_avg": 33.71, "energy_joules_est": 125.07, "sample_count": 29, "duration_seconds": 3.71}, "timestamp": "2026-01-17T14:49:23.659966"}
{"image_index": 134, "image_name": "000000013774.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013774.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 4287.982, "latencies_ms": [4287.982], "images_per_second": 0.233, "prompt_tokens": 19, "response_tokens_est": 77, "n_tiles": 12, "output_text": "The image captures a serene beach scene during sunset, with the sun low on the horizon casting a warm, golden glow across the sky and reflecting off the water. The sky is a gradient of warm colors, transitioning from a deep blue to a soft orange near the horizon. The beach is covered in a thin layer of snow, and the water is calm with gentle waves.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24922.6, "ram_available_mb": 100849.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 24922.9, "ram_available_mb": 100849.3, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.08, "peak": 16.56, "min": 14.12}, "VDD_CPU_SOC_MSS": {"avg": 15.57, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 32.54, "peak": 45.28, "min": 25.6}, "VIN": {"avg": 75.77, "peak": 123.83, "min": 59.3}}, "power_watts_avg": 32.54, "energy_joules_est": 139.54, "sample_count": 34, "duration_seconds": 4.288}, "timestamp": "2026-01-17T14:49:27.954534"}
{"image_index": 135, "image_name": "000000013923.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013923.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1876.994, "latencies_ms": [1876.994], "images_per_second": 0.533, "prompt_tokens": 9, "response_tokens_est": 47, "n_tiles": 6, "output_text": "The image depicts a cozy living room with a white sofa adorned with colorful pillows, a round coffee table, and a red chair, all set against a backdrop of a large window with a view of greenery outside.", "error": null, "sys_before": {"cpu_percent": 40.0, "ram_used_mb": 24938.1, "ram_available_mb": 100834.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 24938.6, "ram_available_mb": 100833.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.76, "peak": 15.14, "min": 13.82}, "VDD_CPU_SOC_MSS": {"avg": 15.47, "peak": 16.14, "min": 14.17}, "VDD_GPU": {"avg": 31.4, "peak": 40.57, "min": 24.42}, "VIN": {"avg": 73.8, "peak": 107.79, "min": 60.04}}, "power_watts_avg": 31.4, "energy_joules_est": 58.95, "sample_count": 14, "duration_seconds": 1.877}, "timestamp": "2026-01-17T14:49:29.885028"}
{"image_index": 135, "image_name": "000000013923.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013923.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1719.925, "latencies_ms": [1719.925], "images_per_second": 0.581, "prompt_tokens": 23, "response_tokens_est": 38, "n_tiles": 6, "output_text": "1. Living room\n2. Sofa\n3. Coffee table\n4. Rug\n5. Chairs\n6. TV stand\n7. Decorative plates\n8. Plants", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24938.6, "ram_available_mb": 100833.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 24938.8, "ram_available_mb": 100833.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.82, "peak": 15.24, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.41, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 32.18, "peak": 40.57, "min": 24.42}, "VIN": {"avg": 75.32, "peak": 110.53, "min": 58.05}}, "power_watts_avg": 32.18, "energy_joules_est": 55.36, "sample_count": 13, "duration_seconds": 1.72}, "timestamp": "2026-01-17T14:49:31.611330"}
{"image_index": 135, "image_name": "000000013923.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013923.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3605.247, "latencies_ms": [3605.247], "images_per_second": 0.277, "prompt_tokens": 27, "response_tokens_est": 104, "n_tiles": 6, "output_text": "The main objects in the image are a living room and a dining area. The living room features a white sofa with yellow and striped pillows, a black and white patterned rug, a small round coffee table, and a television on a stand. The dining area includes a white round table with a red chair, a vase with a green plant, and a black and white patterned rug. The living room is positioned to the left of the dining area, with the dining area situated to the right.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24938.8, "ram_available_mb": 100833.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 24940.0, "ram_available_mb": 100832.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.87, "peak": 15.24, "min": 14.3}, "VDD_CPU_SOC_MSS": {"avg": 15.61, "peak": 15.75, "min": 14.56}, "VDD_GPU": {"avg": 27.27, "peak": 40.95, "min": 22.85}, "VIN": {"avg": 67.11, "peak": 107.9, "min": 49.87}}, "power_watts_avg": 27.27, "energy_joules_est": 98.33, "sample_count": 28, "duration_seconds": 3.606}, "timestamp": "2026-01-17T14:49:35.223086"}
{"image_index": 135, "image_name": "000000013923.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013923.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3783.69, "latencies_ms": [3783.69], "images_per_second": 0.264, "prompt_tokens": 21, "response_tokens_est": 111, "n_tiles": 6, "output_text": "The image depicts a cozy, well-lit living room with a modern and stylish decor. The room features a white sofa adorned with colorful pillows, a black and white patterned rug, and a round white coffee table with a vase of flowers. The space is furnished with red chairs and a television on a stand, and there are various decorative items on the walls, including a circular wall clock and a wall-mounted shelf. The room is brightly lit by natural light coming through large windows, creating a warm and inviting atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24940.0, "ram_available_mb": 100832.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 24940.5, "ram_available_mb": 100831.7, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.89, "peak": 15.24, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.56, "peak": 15.75, "min": 14.56}, "VDD_GPU": {"avg": 26.99, "peak": 40.18, "min": 22.86}, "VIN": {"avg": 68.18, "peak": 113.49, "min": 63.06}}, "power_watts_avg": 26.99, "energy_joules_est": 102.13, "sample_count": 29, "duration_seconds": 3.784}, "timestamp": "2026-01-17T14:49:39.013703"}
{"image_index": 135, "image_name": "000000013923.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013923.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2275.805, "latencies_ms": [2275.805], "images_per_second": 0.439, "prompt_tokens": 19, "response_tokens_est": 58, "n_tiles": 6, "output_text": "The room is brightly lit with natural light streaming in through large windows, creating a warm and inviting atmosphere. The walls are painted white, and the furniture is a mix of modern and traditional styles, featuring a white sofa, red chairs, and a black and white patterned rug.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24940.5, "ram_available_mb": 100831.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 24941.2, "ram_available_mb": 100830.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.84, "peak": 15.24, "min": 14.2}, "VDD_CPU_SOC_MSS": {"avg": 15.47, "peak": 16.14, "min": 14.57}, "VDD_GPU": {"avg": 29.91, "peak": 40.18, "min": 23.25}, "VIN": {"avg": 68.42, "peak": 88.96, "min": 55.67}}, "power_watts_avg": 29.91, "energy_joules_est": 68.08, "sample_count": 17, "duration_seconds": 2.276}, "timestamp": "2026-01-17T14:49:41.295244"}
{"image_index": 136, "image_name": "000000014007.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014007.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 983.647, "latencies_ms": [983.647], "images_per_second": 1.017, "prompt_tokens": 9, "response_tokens_est": 14, "n_tiles": 6, "output_text": "A cat is sitting on top of a refrigerator, looking around.", "error": null, "sys_before": {"cpu_percent": 44.4, "ram_used_mb": 24941.2, "ram_available_mb": 100830.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 24942.0, "ram_available_mb": 100830.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.55, "peak": 15.24, "min": 13.79}, "VDD_CPU_SOC_MSS": {"avg": 15.18, "peak": 16.14, "min": 14.17}, "VDD_GPU": {"avg": 35.67, "peak": 40.16, "min": 30.73}, "VIN": {"avg": 81.94, "peak": 110.52, "min": 60.65}}, "power_watts_avg": 35.67, "energy_joules_est": 35.1, "sample_count": 7, "duration_seconds": 0.984}, "timestamp": "2026-01-17T14:49:42.327816"}
{"image_index": 136, "image_name": "000000014007.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014007.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1741.151, "latencies_ms": [1741.151], "images_per_second": 0.574, "prompt_tokens": 23, "response_tokens_est": 40, "n_tiles": 6, "output_text": "1. Cat\n2. Refrigerator\n3. Toy\n4. Toy box\n5. Toy car\n6. Toy blocks\n7. Toy blocks\n8. Toy blocks", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24942.0, "ram_available_mb": 100830.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 24942.7, "ram_available_mb": 100829.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.98, "peak": 15.44, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.59, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 33.39, "peak": 41.74, "min": 24.42}, "VIN": {"avg": 76.97, "peak": 120.06, "min": 62.65}}, "power_watts_avg": 33.39, "energy_joules_est": 58.15, "sample_count": 13, "duration_seconds": 1.742}, "timestamp": "2026-01-17T14:49:44.075012"}
{"image_index": 136, "image_name": "000000014007.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014007.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1949.085, "latencies_ms": [1949.085], "images_per_second": 0.513, "prompt_tokens": 27, "response_tokens_est": 46, "n_tiles": 6, "output_text": "The main object in the foreground is a cat sitting on top of a refrigerator. The refrigerator is located to the right of the cat. The background includes a wall and a ceiling, which are not directly visible in the image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24942.7, "ram_available_mb": 100829.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 24943.7, "ram_available_mb": 100828.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.83, "peak": 15.24, "min": 14.3}, "VDD_CPU_SOC_MSS": {"avg": 15.3, "peak": 15.74, "min": 14.56}, "VDD_GPU": {"avg": 30.99, "peak": 40.56, "min": 23.64}, "VIN": {"avg": 69.06, "peak": 97.5, "min": 51.81}}, "power_watts_avg": 30.99, "energy_joules_est": 60.41, "sample_count": 15, "duration_seconds": 1.949}, "timestamp": "2026-01-17T14:49:46.030163"}
{"image_index": 136, "image_name": "000000014007.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014007.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2289.821, "latencies_ms": [2289.821], "images_per_second": 0.437, "prompt_tokens": 21, "response_tokens_est": 58, "n_tiles": 6, "output_text": "The image depicts a domestic scene featuring a cat sitting on top of a refrigerator. The cat is wearing a blue collar and appears to be looking towards the camera. The setting is a kitchen, as evidenced by the presence of a refrigerator and various kitchen items on top of it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24943.7, "ram_available_mb": 100828.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 24943.9, "ram_available_mb": 100828.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.84, "peak": 15.24, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.33, "peak": 15.74, "min": 14.56}, "VDD_GPU": {"avg": 29.91, "peak": 40.18, "min": 23.25}, "VIN": {"avg": 67.58, "peak": 110.66, "min": 56.16}}, "power_watts_avg": 29.91, "energy_joules_est": 68.5, "sample_count": 17, "duration_seconds": 2.29}, "timestamp": "2026-01-17T14:49:48.330348"}
{"image_index": 136, "image_name": "000000014007.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014007.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2460.52, "latencies_ms": [2460.52], "images_per_second": 0.406, "prompt_tokens": 19, "response_tokens_est": 64, "n_tiles": 6, "output_text": "The image features a cat sitting on top of a refrigerator, which is painted in a deep blue color. The lighting in the room is soft and warm, casting a gentle glow on the cat and the refrigerator. The background is neutral, with a beige wall and a ceiling that has a subtle, rounded design.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24943.9, "ram_available_mb": 100828.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 24944.2, "ram_available_mb": 100828.0, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.82, "peak": 15.24, "min": 14.12}, "VDD_CPU_SOC_MSS": {"avg": 15.29, "peak": 15.74, "min": 14.56}, "VDD_GPU": {"avg": 29.05, "peak": 40.16, "min": 22.85}, "VIN": {"avg": 69.22, "peak": 95.05, "min": 56.38}}, "power_watts_avg": 29.05, "energy_joules_est": 71.5, "sample_count": 19, "duration_seconds": 2.461}, "timestamp": "2026-01-17T14:49:50.797467"}
{"image_index": 137, "image_name": "000000014038.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014038.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2729.561, "latencies_ms": [2729.561], "images_per_second": 0.366, "prompt_tokens": 9, "response_tokens_est": 78, "n_tiles": 6, "output_text": "The image depicts a cozy, well-organized living room with a mix of modern and rustic elements, featuring a white refrigerator adorned with various magnets and a blue balloon, a wooden table with a green balloon, a bed with a yellow balloon, a bookshelf filled with books and decorative items, a black speaker, and a yellow wall with framed pictures and a lamp.", "error": null, "sys_before": {"cpu_percent": 45.1, "ram_used_mb": 24944.2, "ram_available_mb": 100828.0, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 24944.2, "ram_available_mb": 100828.0, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.88, "peak": 15.14, "min": 13.69}, "VDD_CPU_SOC_MSS": {"avg": 15.56, "peak": 16.14, "min": 14.17}, "VDD_GPU": {"avg": 28.18, "peak": 40.16, "min": 23.24}, "VIN": {"avg": 69.84, "peak": 109.66, "min": 60.49}}, "power_watts_avg": 28.18, "energy_joules_est": 76.93, "sample_count": 21, "duration_seconds": 2.73}, "timestamp": "2026-01-17T14:49:53.586548"}
{"image_index": 137, "image_name": "000000014038.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014038.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2035.721, "latencies_ms": [2035.721], "images_per_second": 0.491, "prompt_tokens": 23, "response_tokens_est": 51, "n_tiles": 6, "output_text": "- refrigerator: 1\n- microwave: 1\n- table: 1\n- chair: 1\n- bookshelf: 1\n- lamp: 1\n- wall art: 1\n- balloons: 3", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24944.2, "ram_available_mb": 100828.0, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 24944.4, "ram_available_mb": 100827.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.99, "peak": 15.44, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.62, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 30.9, "peak": 39.77, "min": 24.03}, "VIN": {"avg": 69.36, "peak": 98.57, "min": 60.35}}, "power_watts_avg": 30.9, "energy_joules_est": 62.92, "sample_count": 15, "duration_seconds": 2.036}, "timestamp": "2026-01-17T14:49:55.628463"}
{"image_index": 137, "image_name": "000000014038.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014038.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2913.647, "latencies_ms": [2913.647], "images_per_second": 0.343, "prompt_tokens": 27, "response_tokens_est": 83, "n_tiles": 6, "output_text": "The main objects in the image are a refrigerator, a bed, and a desk. The refrigerator is located on the left side of the image, near the foreground. The bed is in the background, slightly to the right. The desk is located in the middle of the image, near the right side. The yellow balloons are placed on the desk and the bed, adding a playful touch to the room.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24944.4, "ram_available_mb": 100827.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 24944.4, "ram_available_mb": 100827.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.04, "peak": 15.54, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.7, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 28.65, "peak": 40.16, "min": 23.24}, "VIN": {"avg": 69.82, "peak": 106.71, "min": 59.86}}, "power_watts_avg": 28.65, "energy_joules_est": 83.48, "sample_count": 22, "duration_seconds": 2.914}, "timestamp": "2026-01-17T14:49:58.548402"}
{"image_index": 137, "image_name": "000000014038.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014038.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3372.906, "latencies_ms": [3372.906], "images_per_second": 0.296, "prompt_tokens": 21, "response_tokens_est": 97, "n_tiles": 6, "output_text": "The image depicts a cozy, well-organized living room with a mix of personal and decorative items. The room is furnished with a wooden coffee table, a bed with a yellow balloon, a white refrigerator with various magnets and notes, a black speaker, and a bookshelf filled with books and decorative items. The walls are adorned with framed pictures and a yellow wall decoration, and there are balloons floating in the air, adding a festive touch to the space.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24944.4, "ram_available_mb": 100827.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 24944.7, "ram_available_mb": 100827.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.97, "peak": 15.44, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.67, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 27.59, "peak": 40.18, "min": 22.86}, "VIN": {"avg": 67.67, "peak": 95.56, "min": 54.2}}, "power_watts_avg": 27.59, "energy_joules_est": 93.07, "sample_count": 26, "duration_seconds": 3.373}, "timestamp": "2026-01-17T14:50:01.927365"}
{"image_index": 137, "image_name": "000000014038.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014038.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2255.926, "latencies_ms": [2255.926], "images_per_second": 0.443, "prompt_tokens": 19, "response_tokens_est": 57, "n_tiles": 6, "output_text": "The room is brightly lit with a modern chandelier hanging from the ceiling. The walls are painted in a light color, and there are various decorations, including balloons in yellow and blue. The floor is wooden, and there are shelves filled with books and other items.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24944.7, "ram_available_mb": 100827.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 24944.9, "ram_available_mb": 100827.3, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.83, "peak": 15.24, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.35, "peak": 16.14, "min": 14.57}, "VDD_GPU": {"avg": 29.78, "peak": 40.18, "min": 23.24}, "VIN": {"avg": 71.83, "peak": 122.4, "min": 53.94}}, "power_watts_avg": 29.78, "energy_joules_est": 67.2, "sample_count": 17, "duration_seconds": 2.256}, "timestamp": "2026-01-17T14:50:04.189947"}
{"image_index": 138, "image_name": "000000014226.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014226.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2313.413, "latencies_ms": [2313.413], "images_per_second": 0.432, "prompt_tokens": 9, "response_tokens_est": 21, "n_tiles": 12, "output_text": "A man is sitting in a train, using a laptop on a table, with a focused expression.", "error": null, "sys_before": {"cpu_percent": 44.4, "ram_used_mb": 24914.2, "ram_available_mb": 100858.0, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.3, "ram_used_mb": 24914.9, "ram_available_mb": 100857.3, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.2, "peak": 16.66, "min": 13.49}, "VDD_CPU_SOC_MSS": {"avg": 15.99, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 36.7, "peak": 43.71, "min": 28.76}, "VIN": {"avg": 81.16, "peak": 112.68, "min": 65.97}}, "power_watts_avg": 36.7, "energy_joules_est": 84.91, "sample_count": 18, "duration_seconds": 2.314}, "timestamp": "2026-01-17T14:50:06.595451"}
{"image_index": 138, "image_name": "000000014226.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014226.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2788.422, "latencies_ms": [2788.422], "images_per_second": 0.359, "prompt_tokens": 23, "response_tokens_est": 33, "n_tiles": 12, "output_text": "1. Laptop\n2. Man\n3. Headphones\n4. Window\n5. Chair\n6. Table\n7. Keyboard\n8. Mouse", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24914.9, "ram_available_mb": 100857.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.6, "ram_used_mb": 24915.6, "ram_available_mb": 100856.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.26, "peak": 16.56, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.87, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 36.45, "peak": 46.46, "min": 26.79}, "VIN": {"avg": 74.34, "peak": 104.05, "min": 59.05}}, "power_watts_avg": 36.45, "energy_joules_est": 101.65, "sample_count": 22, "duration_seconds": 2.789}, "timestamp": "2026-01-17T14:50:09.390374"}
{"image_index": 138, "image_name": "000000014226.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014226.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3437.405, "latencies_ms": [3437.405], "images_per_second": 0.291, "prompt_tokens": 27, "response_tokens_est": 52, "n_tiles": 12, "output_text": "The main object in the foreground is a man sitting on a train seat, holding a laptop. The laptop is placed on a small table or bench near the man. The background features blurred train tracks and a window, indicating the train is in motion.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24915.6, "ram_available_mb": 100856.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 24916.9, "ram_available_mb": 100855.3, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.19, "peak": 16.56, "min": 14.12}, "VDD_CPU_SOC_MSS": {"avg": 15.83, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 34.33, "peak": 45.68, "min": 26.0}, "VIN": {"avg": 72.19, "peak": 96.95, "min": 59.68}}, "power_watts_avg": 34.33, "energy_joules_est": 118.03, "sample_count": 27, "duration_seconds": 3.438}, "timestamp": "2026-01-17T14:50:12.834364"}
{"image_index": 138, "image_name": "000000014226.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014226.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3439.036, "latencies_ms": [3439.036], "images_per_second": 0.291, "prompt_tokens": 21, "response_tokens_est": 53, "n_tiles": 12, "output_text": "The image depicts a man sitting in a train carriage, engrossed in using a laptop. The setting is inside a train, with the man seated on a bench, and the train's interior is visible, including a window and a cushioned seat.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24916.9, "ram_available_mb": 100855.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 24917.1, "ram_available_mb": 100855.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.25, "peak": 16.66, "min": 14.12}, "VDD_CPU_SOC_MSS": {"avg": 15.85, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 34.31, "peak": 45.68, "min": 26.39}, "VIN": {"avg": 73.41, "peak": 105.95, "min": 66.08}}, "power_watts_avg": 34.31, "energy_joules_est": 118.02, "sample_count": 27, "duration_seconds": 3.44}, "timestamp": "2026-01-17T14:50:16.280341"}
{"image_index": 138, "image_name": "000000014226.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014226.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3959.316, "latencies_ms": [3959.316], "images_per_second": 0.253, "prompt_tokens": 19, "response_tokens_est": 68, "n_tiles": 12, "output_text": "The image features a man sitting in a train, holding a laptop. The laptop is silver with a visible Apple logo. The man is wearing a dark jacket and has red hair. The train interior has a vintage design with a pink and white cushioned seat. The lighting is dim, suggesting it is either early morning or late evening.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24917.4, "ram_available_mb": 100854.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 24917.6, "ram_available_mb": 100854.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.23, "peak": 16.56, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.87, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 33.3, "peak": 44.89, "min": 26.0}, "VIN": {"avg": 75.49, "peak": 114.12, "min": 58.82}}, "power_watts_avg": 33.3, "energy_joules_est": 131.86, "sample_count": 30, "duration_seconds": 3.96}, "timestamp": "2026-01-17T14:50:20.246454"}
{"image_index": 139, "image_name": "000000014380.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014380.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1712.242, "latencies_ms": [1712.242], "images_per_second": 0.584, "prompt_tokens": 9, "response_tokens_est": 41, "n_tiles": 6, "output_text": "The image depicts a modern train station with multiple tracks and a sleek, silver train moving along them, set against a backdrop of a cloudy sky and a cityscape with buildings in the distance.", "error": null, "sys_before": {"cpu_percent": 37.0, "ram_used_mb": 24937.3, "ram_available_mb": 100834.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 24938.3, "ram_available_mb": 100833.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.74, "peak": 15.11, "min": 13.72}, "VDD_CPU_SOC_MSS": {"avg": 15.35, "peak": 15.75, "min": 14.17}, "VDD_GPU": {"avg": 31.97, "peak": 40.57, "min": 24.83}, "VIN": {"avg": 74.43, "peak": 116.95, "min": 60.07}}, "power_watts_avg": 31.97, "energy_joules_est": 54.75, "sample_count": 13, "duration_seconds": 1.713}, "timestamp": "2026-01-17T14:50:22.024686"}
{"image_index": 139, "image_name": "000000014380.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014380.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1728.944, "latencies_ms": [1728.944], "images_per_second": 0.578, "prompt_tokens": 23, "response_tokens_est": 40, "n_tiles": 6, "output_text": "object: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 24938.3, "ram_available_mb": 100833.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 24938.5, "ram_available_mb": 100833.7, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.02, "peak": 15.54, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.57, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 32.33, "peak": 40.56, "min": 24.83}, "VIN": {"avg": 74.2, "peak": 105.93, "min": 64.1}}, "power_watts_avg": 32.33, "energy_joules_est": 55.91, "sample_count": 13, "duration_seconds": 1.729}, "timestamp": "2026-01-17T14:50:23.759598"}
{"image_index": 139, "image_name": "000000014380.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014380.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2593.953, "latencies_ms": [2593.953], "images_per_second": 0.386, "prompt_tokens": 27, "response_tokens_est": 69, "n_tiles": 6, "output_text": "The main objects in the image are a train and a bridge. The train is positioned in the foreground, with its front end closer to the viewer. The bridge is in the background, spanning across the image and connecting the two sides. The train is on the tracks, while the bridge is supported by metal beams and spans over the tracks.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24938.5, "ram_available_mb": 100833.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 24938.5, "ram_available_mb": 100833.7, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.87, "peak": 15.34, "min": 14.43}, "VDD_CPU_SOC_MSS": {"avg": 15.47, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 29.19, "peak": 40.56, "min": 22.86}, "VIN": {"avg": 69.51, "peak": 116.15, "min": 56.77}}, "power_watts_avg": 29.19, "energy_joules_est": 75.73, "sample_count": 20, "duration_seconds": 2.594}, "timestamp": "2026-01-17T14:50:26.359505"}
{"image_index": 139, "image_name": "000000014380.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014380.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1766.338, "latencies_ms": [1766.338], "images_per_second": 0.566, "prompt_tokens": 21, "response_tokens_est": 40, "n_tiles": 6, "output_text": "The image depicts a modern train station with a sleek, silver train moving along the tracks. The station is surrounded by a series of metal structures and platforms, with a clear blue sky overhead.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24938.5, "ram_available_mb": 100833.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 24938.5, "ram_available_mb": 100833.7, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.81, "peak": 15.24, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.44, "peak": 15.75, "min": 14.56}, "VDD_GPU": {"avg": 31.88, "peak": 40.57, "min": 24.42}, "VIN": {"avg": 73.3, "peak": 107.09, "min": 56.56}}, "power_watts_avg": 31.88, "energy_joules_est": 56.33, "sample_count": 13, "duration_seconds": 1.767}, "timestamp": "2026-01-17T14:50:28.131903"}
{"image_index": 139, "image_name": "000000014380.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014380.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2360.896, "latencies_ms": [2360.896], "images_per_second": 0.424, "prompt_tokens": 19, "response_tokens_est": 61, "n_tiles": 6, "output_text": "The image showcases a modern train station with a sleek, silver train in motion on the tracks. The station is surrounded by a clear blue sky with scattered white clouds, indicating a bright and sunny day. The lighting is natural, with shadows cast by the train and structures, suggesting it is daytime.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24938.5, "ram_available_mb": 100833.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 24939.5, "ram_available_mb": 100832.7, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.85, "peak": 15.24, "min": 14.3}, "VDD_CPU_SOC_MSS": {"avg": 15.51, "peak": 15.75, "min": 14.56}, "VDD_GPU": {"avg": 29.78, "peak": 40.56, "min": 23.24}, "VIN": {"avg": 71.91, "peak": 112.61, "min": 52.63}}, "power_watts_avg": 29.78, "energy_joules_est": 70.32, "sample_count": 18, "duration_seconds": 2.361}, "timestamp": "2026-01-17T14:50:30.499026"}
{"image_index": 140, "image_name": "000000014439.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014439.jpg", "image_width": 640, "image_height": 404, "image_resolution": "640x404", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1438.29, "latencies_ms": [1438.29], "images_per_second": 0.695, "prompt_tokens": 9, "response_tokens_est": 31, "n_tiles": 6, "output_text": "A group of people are gathered in a park, with a colorful kite being flown by a woman and a child, while others sit and watch.", "error": null, "sys_before": {"cpu_percent": 40.0, "ram_used_mb": 24939.5, "ram_available_mb": 100832.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 24939.7, "ram_available_mb": 100832.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.65, "peak": 15.14, "min": 13.79}, "VDD_CPU_SOC_MSS": {"avg": 15.39, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 33.32, "peak": 40.16, "min": 26.39}, "VIN": {"avg": 72.51, "peak": 90.41, "min": 56.68}}, "power_watts_avg": 33.32, "energy_joules_est": 47.94, "sample_count": 10, "duration_seconds": 1.439}, "timestamp": "2026-01-17T14:50:31.994981"}
{"image_index": 140, "image_name": "000000014439.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014439.jpg", "image_width": 640, "image_height": 404, "image_resolution": "640x404", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2186.029, "latencies_ms": [2186.029], "images_per_second": 0.457, "prompt_tokens": 23, "response_tokens_est": 57, "n_tiles": 6, "output_text": "1. Kite: 1\n2. Person: 3\n3. Person: 2\n4. Person: 1\n5. Person: 1\n6. Person: 1\n7. Person: 1\n8. Person: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24939.7, "ram_available_mb": 100832.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 24939.7, "ram_available_mb": 100832.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.05, "peak": 15.44, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.66, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 30.86, "peak": 41.34, "min": 24.03}, "VIN": {"avg": 73.56, "peak": 112.78, "min": 60.25}}, "power_watts_avg": 30.86, "energy_joules_est": 67.48, "sample_count": 17, "duration_seconds": 2.187}, "timestamp": "2026-01-17T14:50:34.187487"}
{"image_index": 140, "image_name": "000000014439.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014439.jpg", "image_width": 640, "image_height": 404, "image_resolution": "640x404", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3032.272, "latencies_ms": [3032.272], "images_per_second": 0.33, "prompt_tokens": 27, "response_tokens_est": 88, "n_tiles": 6, "output_text": "In the image, the main object, a colorful kite, is prominently positioned in the foreground. The kite is being flown by a person standing to the right of the frame. In the background, there are several people scattered across the grassy field, some of whom are walking and others sitting or standing. The kite is the central focus of the image, with the people and the field serving as the background elements.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24939.7, "ram_available_mb": 100832.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 24940.7, "ram_available_mb": 100831.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.1, "peak": 15.54, "min": 14.43}, "VDD_CPU_SOC_MSS": {"avg": 15.8, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 28.52, "peak": 40.16, "min": 23.25}, "VIN": {"avg": 69.42, "peak": 94.99, "min": 62.49}}, "power_watts_avg": 28.52, "energy_joules_est": 86.49, "sample_count": 23, "duration_seconds": 3.033}, "timestamp": "2026-01-17T14:50:37.230096"}
{"image_index": 140, "image_name": "000000014439.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014439.jpg", "image_width": 640, "image_height": 404, "image_resolution": "640x404", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2779.196, "latencies_ms": [2779.196], "images_per_second": 0.36, "prompt_tokens": 21, "response_tokens_est": 79, "n_tiles": 6, "output_text": "The image depicts a vibrant outdoor scene in a park, where a group of people is gathered to fly a colorful kite. The kite, with its vivid colors and intricate patterns, is being held aloft by a person in the foreground. In the background, other individuals are scattered around the grassy field, some engaged in conversation, while others are simply enjoying the sunny day.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24940.7, "ram_available_mb": 100831.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 24941.0, "ram_available_mb": 100831.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.09, "peak": 15.44, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.69, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 28.81, "peak": 40.57, "min": 23.25}, "VIN": {"avg": 72.05, "peak": 114.6, "min": 61.68}}, "power_watts_avg": 28.81, "energy_joules_est": 80.08, "sample_count": 22, "duration_seconds": 2.78}, "timestamp": "2026-01-17T14:50:40.015680"}
{"image_index": 140, "image_name": "000000014439.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014439.jpg", "image_width": 640, "image_height": 404, "image_resolution": "640x404", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2023.008, "latencies_ms": [2023.008], "images_per_second": 0.494, "prompt_tokens": 19, "response_tokens_est": 51, "n_tiles": 6, "output_text": "The kite in the image is vibrant and colorful, featuring a combination of blue, purple, yellow, and orange hues. It is being flown by a person in a light-colored shirt, and the lighting suggests it is daytime with clear skies.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24941.0, "ram_available_mb": 100831.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 24941.2, "ram_available_mb": 100831.0, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.06, "peak": 15.44, "min": 14.43}, "VDD_CPU_SOC_MSS": {"avg": 15.64, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 31.04, "peak": 40.57, "min": 24.03}, "VIN": {"avg": 75.32, "peak": 111.71, "min": 62.8}}, "power_watts_avg": 31.04, "energy_joules_est": 62.81, "sample_count": 15, "duration_seconds": 2.023}, "timestamp": "2026-01-17T14:50:42.048579"}
{"image_index": 141, "image_name": "000000014473.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014473.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1145.204, "latencies_ms": [1145.204], "images_per_second": 0.873, "prompt_tokens": 9, "response_tokens_est": 20, "n_tiles": 6, "output_text": "A model of a red and black train is seen on tracks with workers in orange uniforms nearby.", "error": null, "sys_before": {"cpu_percent": 43.4, "ram_used_mb": 24941.2, "ram_available_mb": 100831.0, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 24941.2, "ram_available_mb": 100831.0, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.56, "peak": 15.14, "min": 13.79}, "VDD_CPU_SOC_MSS": {"avg": 15.35, "peak": 16.14, "min": 14.57}, "VDD_GPU": {"avg": 35.01, "peak": 40.57, "min": 29.15}, "VIN": {"avg": 84.62, "peak": 114.96, "min": 57.4}}, "power_watts_avg": 35.01, "energy_joules_est": 40.1, "sample_count": 8, "duration_seconds": 1.146}, "timestamp": "2026-01-17T14:50:43.255378"}
{"image_index": 141, "image_name": "000000014473.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014473.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1730.422, "latencies_ms": [1730.422], "images_per_second": 0.578, "prompt_tokens": 23, "response_tokens_est": 40, "n_tiles": 6, "output_text": "object: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24941.2, "ram_available_mb": 100831.0, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 24942.2, "ram_available_mb": 100830.0, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.98, "peak": 15.34, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.63, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 33.24, "peak": 41.76, "min": 24.82}, "VIN": {"avg": 75.08, "peak": 118.36, "min": 57.9}}, "power_watts_avg": 33.24, "energy_joules_est": 57.53, "sample_count": 13, "duration_seconds": 1.731}, "timestamp": "2026-01-17T14:50:44.991949"}
{"image_index": 141, "image_name": "000000014473.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014473.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2189.12, "latencies_ms": [2189.12], "images_per_second": 0.457, "prompt_tokens": 27, "response_tokens_est": 57, "n_tiles": 6, "output_text": "The main objects in the image are a model train and a group of railway workers. The model train is positioned in the foreground, while the railway workers are in the background. The workers are near the tracks, working on the ground, while the model train is on the tracks.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24942.2, "ram_available_mb": 100830.0, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 24942.4, "ram_available_mb": 100829.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.04, "peak": 15.54, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.7, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 30.8, "peak": 40.16, "min": 24.03}, "VIN": {"avg": 72.93, "peak": 115.64, "min": 59.84}}, "power_watts_avg": 30.8, "energy_joules_est": 67.43, "sample_count": 16, "duration_seconds": 2.189}, "timestamp": "2026-01-17T14:50:47.190983"}
{"image_index": 141, "image_name": "000000014473.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014473.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2079.761, "latencies_ms": [2079.761], "images_per_second": 0.481, "prompt_tokens": 21, "response_tokens_est": 53, "n_tiles": 6, "output_text": "The image depicts a miniature model of a train and railway tracks, with a group of railway workers in orange uniforms working on the tracks. The scene is set in a rural or semi-rural area, with greenery and a fence visible in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24942.4, "ram_available_mb": 100829.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 24942.7, "ram_available_mb": 100829.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.02, "peak": 15.34, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.65, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 30.75, "peak": 40.97, "min": 24.03}, "VIN": {"avg": 74.77, "peak": 108.7, "min": 59.46}}, "power_watts_avg": 30.75, "energy_joules_est": 63.97, "sample_count": 16, "duration_seconds": 2.08}, "timestamp": "2026-01-17T14:50:49.277515"}
{"image_index": 141, "image_name": "000000014473.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014473.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1861.113, "latencies_ms": [1861.113], "images_per_second": 0.537, "prompt_tokens": 19, "response_tokens_est": 45, "n_tiles": 6, "output_text": "The image features a red and black train with the Virgin logo on its side, moving on tracks surrounded by greenery. The scene is well-lit, with natural sunlight casting shadows on the train and the surrounding landscape.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24942.7, "ram_available_mb": 100829.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 24946.1, "ram_available_mb": 100826.0, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.05, "peak": 15.44, "min": 14.43}, "VDD_CPU_SOC_MSS": {"avg": 15.64, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 31.62, "peak": 40.16, "min": 24.42}, "VIN": {"avg": 76.74, "peak": 115.58, "min": 63.32}}, "power_watts_avg": 31.62, "energy_joules_est": 58.86, "sample_count": 14, "duration_seconds": 1.861}, "timestamp": "2026-01-17T14:50:51.144574"}
{"image_index": 142, "image_name": "000000014831.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014831.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2493.489, "latencies_ms": [2493.489], "images_per_second": 0.401, "prompt_tokens": 9, "response_tokens_est": 26, "n_tiles": 12, "output_text": "The image shows a close-up of a cat's fur, which appears to be a mix of light brown and white colors.", "error": null, "sys_before": {"cpu_percent": 43.0, "ram_used_mb": 24924.0, "ram_available_mb": 100848.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.5, "ram_used_mb": 24924.8, "ram_available_mb": 100847.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.19, "peak": 16.76, "min": 13.49}, "VDD_CPU_SOC_MSS": {"avg": 15.97, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 36.1, "peak": 43.73, "min": 27.57}, "VIN": {"avg": 82.71, "peak": 120.45, "min": 64.89}}, "power_watts_avg": 36.1, "energy_joules_est": 90.02, "sample_count": 19, "duration_seconds": 2.494}, "timestamp": "2026-01-17T14:50:53.736914"}
{"image_index": 142, "image_name": "000000014831.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014831.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2045.118, "latencies_ms": [2045.118], "images_per_second": 0.489, "prompt_tokens": 23, "response_tokens_est": 11, "n_tiles": 12, "output_text": "cat: 1\nblanket: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24924.8, "ram_available_mb": 100847.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.0, "ram_used_mb": 24925.8, "ram_available_mb": 100846.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.33, "peak": 16.66, "min": 14.12}, "VDD_CPU_SOC_MSS": {"avg": 15.95, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 39.75, "peak": 45.28, "min": 33.09}, "VIN": {"avg": 86.61, "peak": 130.68, "min": 67.4}}, "power_watts_avg": 39.75, "energy_joules_est": 81.31, "sample_count": 15, "duration_seconds": 2.045}, "timestamp": "2026-01-17T14:50:55.788511"}
{"image_index": 142, "image_name": "000000014831.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014831.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3438.815, "latencies_ms": [3438.815], "images_per_second": 0.291, "prompt_tokens": 27, "response_tokens_est": 52, "n_tiles": 12, "output_text": "The main object in the image is a cat's fur, which occupies the foreground. The fur is predominantly light brown with some white areas. The background is a textured fabric, possibly a couch or a blanket, which is slightly out of focus.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24926.0, "ram_available_mb": 100846.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 24926.8, "ram_available_mb": 100845.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.22, "peak": 16.56, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.78, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 35.13, "peak": 46.46, "min": 26.0}, "VIN": {"avg": 74.76, "peak": 113.0, "min": 65.48}}, "power_watts_avg": 35.13, "energy_joules_est": 120.82, "sample_count": 26, "duration_seconds": 3.439}, "timestamp": "2026-01-17T14:50:59.233829"}
{"image_index": 142, "image_name": "000000014831.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014831.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 4139.882, "latencies_ms": [4139.882], "images_per_second": 0.242, "prompt_tokens": 21, "response_tokens_est": 72, "n_tiles": 12, "output_text": "The image shows a close-up of a cat's fur, which appears to be a mix of light brown and white colors. The cat is lying down on a textured surface, possibly a bed or a couch, with its head turned slightly to the side. The overall setting suggests a cozy and comfortable environment, with the cat appearing relaxed and at ease.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24926.8, "ram_available_mb": 100845.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 24926.5, "ram_available_mb": 100845.7, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.94, "peak": 16.36, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.4, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 32.92, "peak": 44.91, "min": 25.6}, "VIN": {"avg": 72.48, "peak": 116.76, "min": 55.93}}, "power_watts_avg": 32.92, "energy_joules_est": 136.3, "sample_count": 32, "duration_seconds": 4.14}, "timestamp": "2026-01-17T14:51:03.381284"}
{"image_index": 142, "image_name": "000000014831.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014831.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3729.745, "latencies_ms": [3729.745], "images_per_second": 0.268, "prompt_tokens": 19, "response_tokens_est": 60, "n_tiles": 12, "output_text": "The image shows a close-up of a cat's fur with a mix of light brown and white colors. The lighting is soft and diffused, creating a gentle and warm ambiance. The fur appears to be well-groomed and smooth, suggesting that the cat is well-cared for.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24926.0, "ram_available_mb": 100846.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 24926.0, "ram_available_mb": 100846.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.9, "peak": 16.36, "min": 13.92}, "VDD_CPU_SOC_MSS": {"avg": 15.34, "peak": 17.32, "min": 14.17}, "VDD_GPU": {"avg": 33.65, "peak": 44.91, "min": 25.6}, "VIN": {"avg": 77.46, "peak": 118.63, "min": 58.69}}, "power_watts_avg": 33.65, "energy_joules_est": 125.52, "sample_count": 28, "duration_seconds": 3.73}, "timestamp": "2026-01-17T14:51:07.117962"}
{"image_index": 143, "image_name": "000000014888.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014888.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2864.567, "latencies_ms": [2864.567], "images_per_second": 0.349, "prompt_tokens": 9, "response_tokens_est": 37, "n_tiles": 12, "output_text": "The image shows a close-up of a black and white cow's head, with a red and white bottle attached to its ear, possibly containing a liquid for the cow to drink.", "error": null, "sys_before": {"cpu_percent": 43.0, "ram_used_mb": 24948.0, "ram_available_mb": 100824.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 24942.1, "ram_available_mb": 100830.0, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.11, "peak": 16.66, "min": 13.19}, "VDD_CPU_SOC_MSS": {"avg": 15.82, "peak": 17.71, "min": 14.17}, "VDD_GPU": {"avg": 35.33, "peak": 44.1, "min": 26.79}, "VIN": {"avg": 78.15, "peak": 120.78, "min": 65.93}}, "power_watts_avg": 35.33, "energy_joules_est": 101.22, "sample_count": 21, "duration_seconds": 2.865}, "timestamp": "2026-01-17T14:51:10.088659"}
{"image_index": 143, "image_name": "000000014888.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014888.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 3325.731, "latencies_ms": [3325.731], "images_per_second": 0.301, "prompt_tokens": 23, "response_tokens_est": 49, "n_tiles": 12, "output_text": "1. Dog\n2. Dog's ear\n3. Dog's paw\n4. Dog's nose\n5. Dog's mouth\n6. Dog's tail\n7. Dog's paw\n8. Dog's paw", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24942.1, "ram_available_mb": 100830.0, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 24941.3, "ram_available_mb": 100830.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.22, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.78, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 34.69, "peak": 45.28, "min": 26.0}, "VIN": {"avg": 77.76, "peak": 111.82, "min": 57.55}}, "power_watts_avg": 34.69, "energy_joules_est": 115.38, "sample_count": 26, "duration_seconds": 3.326}, "timestamp": "2026-01-17T14:51:13.422907"}
{"image_index": 143, "image_name": "000000014888.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014888.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 4688.565, "latencies_ms": [4688.565], "images_per_second": 0.213, "prompt_tokens": 27, "response_tokens_est": 89, "n_tiles": 12, "output_text": "The main object in the foreground is a black and white cow, which is positioned near the center of the image. The cow is interacting with a red and white plastic bottle, which is placed to the right of the cow. The bottle is situated on a black surface, likely a floor or a platform, and is in close proximity to the cow. The background is out of focus, emphasizing the cow and the bottle in the foreground.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24941.3, "ram_available_mb": 100830.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 24941.4, "ram_available_mb": 100830.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.19, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.69, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 32.1, "peak": 44.49, "min": 26.0}, "VIN": {"avg": 72.16, "peak": 107.25, "min": 59.57}}, "power_watts_avg": 32.1, "energy_joules_est": 150.52, "sample_count": 37, "duration_seconds": 4.689}, "timestamp": "2026-01-17T14:51:18.118564"}
{"image_index": 143, "image_name": "000000014888.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014888.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 4621.136, "latencies_ms": [4621.136], "images_per_second": 0.216, "prompt_tokens": 21, "response_tokens_est": 87, "n_tiles": 12, "output_text": "The image depicts a close-up view of a black and white cow's head, with its nose and mouth visible. The cow is standing on a concrete surface, surrounded by various objects and debris, including a black container, a red and white container, and a piece of wood. The setting appears to be a farm or livestock area, with the cow likely being part of a herd or being observed for its health and behavior.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24939.6, "ram_available_mb": 100832.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 24939.0, "ram_available_mb": 100833.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.18, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.66, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 32.02, "peak": 44.49, "min": 25.61}, "VIN": {"avg": 74.96, "peak": 125.22, "min": 60.33}}, "power_watts_avg": 32.02, "energy_joules_est": 147.98, "sample_count": 36, "duration_seconds": 4.621}, "timestamp": "2026-01-17T14:51:22.746545"}
{"image_index": 143, "image_name": "000000014888.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014888.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2993.304, "latencies_ms": [2993.304], "images_per_second": 0.334, "prompt_tokens": 19, "response_tokens_est": 39, "n_tiles": 12, "output_text": "The image shows a black and white cow with a shiny, wet coat, indicating recent rain. The lighting is dim, with a soft, natural light source illuminating the cow's fur.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24939.0, "ram_available_mb": 100833.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 24940.0, "ram_available_mb": 100832.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.23, "peak": 16.56, "min": 14.12}, "VDD_CPU_SOC_MSS": {"avg": 15.8, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 35.77, "peak": 44.91, "min": 26.79}, "VIN": {"avg": 78.89, "peak": 109.91, "min": 65.75}}, "power_watts_avg": 35.77, "energy_joules_est": 107.08, "sample_count": 22, "duration_seconds": 2.994}, "timestamp": "2026-01-17T14:51:25.746427"}
{"image_index": 144, "image_name": "000000015079.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015079.jpg", "image_width": 640, "image_height": 424, "image_resolution": "640x424", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1527.887, "latencies_ms": [1527.887], "images_per_second": 0.654, "prompt_tokens": 9, "response_tokens_est": 34, "n_tiles": 6, "output_text": "The image shows a plate of savory sandwiches with a side of pickles, placed on a patterned tablecloth, with a fork resting on the plate.", "error": null, "sys_before": {"cpu_percent": 45.3, "ram_used_mb": 24948.4, "ram_available_mb": 100823.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 24949.9, "ram_available_mb": 100822.3, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.66, "peak": 15.11, "min": 13.72}, "VDD_CPU_SOC_MSS": {"avg": 15.28, "peak": 15.75, "min": 14.18}, "VDD_GPU": {"avg": 33.23, "peak": 40.56, "min": 26.0}, "VIN": {"avg": 75.5, "peak": 101.56, "min": 60.2}}, "power_watts_avg": 33.23, "energy_joules_est": 50.79, "sample_count": 11, "duration_seconds": 1.528}, "timestamp": "2026-01-17T14:51:27.343988"}
{"image_index": 144, "image_name": "000000015079.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015079.jpg", "image_width": 640, "image_height": 424, "image_resolution": "640x424", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1298.036, "latencies_ms": [1298.036], "images_per_second": 0.77, "prompt_tokens": 23, "response_tokens_est": 24, "n_tiles": 6, "output_text": "- plate: 1\n- sandwich: 1\n- knife: 1\n- fork: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24949.9, "ram_available_mb": 100822.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 24950.2, "ram_available_mb": 100822.0, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.94, "peak": 15.44, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.53, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 35.53, "peak": 40.56, "min": 28.36}, "VIN": {"avg": 84.36, "peak": 113.69, "min": 63.21}}, "power_watts_avg": 35.53, "energy_joules_est": 46.13, "sample_count": 9, "duration_seconds": 1.298}, "timestamp": "2026-01-17T14:51:28.647975"}
{"image_index": 144, "image_name": "000000015079.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015079.jpg", "image_width": 640, "image_height": 424, "image_resolution": "640x424", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1998.86, "latencies_ms": [1998.86], "images_per_second": 0.5, "prompt_tokens": 27, "response_tokens_est": 48, "n_tiles": 6, "output_text": "The main object in the foreground is a plate with a sandwich and a knife. The sandwich is placed on the plate, and the knife is positioned near the plate. The background is dark, which makes the plate and sandwich stand out.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24950.2, "ram_available_mb": 100822.0, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 24951.4, "ram_available_mb": 100820.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.87, "peak": 15.34, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.48, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 31.54, "peak": 41.34, "min": 23.64}, "VIN": {"avg": 68.91, "peak": 91.52, "min": 60.75}}, "power_watts_avg": 31.54, "energy_joules_est": 63.06, "sample_count": 15, "duration_seconds": 1.999}, "timestamp": "2026-01-17T14:51:30.652858"}
{"image_index": 144, "image_name": "000000015079.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015079.jpg", "image_width": 640, "image_height": 424, "image_resolution": "640x424", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2814.43, "latencies_ms": [2814.43], "images_per_second": 0.355, "prompt_tokens": 21, "response_tokens_est": 77, "n_tiles": 6, "output_text": "The image depicts a plate of sandwiches, which are placed on a patterned tablecloth. The sandwiches are cut in half, revealing their contents, and a knife with a black handle is resting on the plate, suggesting that the sandwiches are ready to be eaten. The overall setting appears to be a cozy, intimate dining environment, possibly in a home or a small restaurant.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24951.4, "ram_available_mb": 100820.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 24958.0, "ram_available_mb": 100814.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.89, "peak": 15.24, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.55, "peak": 16.14, "min": 14.57}, "VDD_GPU": {"avg": 28.49, "peak": 40.18, "min": 22.85}, "VIN": {"avg": 68.3, "peak": 114.93, "min": 54.23}}, "power_watts_avg": 28.49, "energy_joules_est": 80.19, "sample_count": 22, "duration_seconds": 2.815}, "timestamp": "2026-01-17T14:51:33.473386"}
{"image_index": 144, "image_name": "000000015079.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015079.jpg", "image_width": 640, "image_height": 424, "image_resolution": "640x424", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1990.809, "latencies_ms": [1990.809], "images_per_second": 0.502, "prompt_tokens": 19, "response_tokens_est": 48, "n_tiles": 6, "output_text": "The image features a plate of food with a light-colored bread and a dark, possibly red or purple filling. The plate is adorned with a decorative pattern, and the lighting is warm and soft, creating a cozy and inviting atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24958.0, "ram_available_mb": 100814.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 24957.5, "ram_available_mb": 100814.7, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.83, "peak": 15.24, "min": 14.3}, "VDD_CPU_SOC_MSS": {"avg": 15.46, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 30.85, "peak": 40.16, "min": 23.64}, "VIN": {"avg": 75.28, "peak": 113.74, "min": 61.73}}, "power_watts_avg": 30.85, "energy_joules_est": 61.43, "sample_count": 15, "duration_seconds": 1.991}, "timestamp": "2026-01-17T14:51:35.470398"}
{"image_index": 145, "image_name": "000000015254.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015254.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1921.238, "latencies_ms": [1921.238], "images_per_second": 0.52, "prompt_tokens": 9, "response_tokens_est": 49, "n_tiles": 6, "output_text": "The image shows a colorful assortment of food items arranged in various colored containers, including a red container with spaghetti and meatballs, a green container with carrots, a blue container with grapes, and a purple container with a salad.", "error": null, "sys_before": {"cpu_percent": 44.3, "ram_used_mb": 24957.5, "ram_available_mb": 100814.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 24957.8, "ram_available_mb": 100814.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.83, "peak": 15.22, "min": 13.79}, "VDD_CPU_SOC_MSS": {"avg": 15.48, "peak": 16.14, "min": 14.57}, "VDD_GPU": {"avg": 30.76, "peak": 40.16, "min": 24.04}, "VIN": {"avg": 74.12, "peak": 117.44, "min": 62.11}}, "power_watts_avg": 30.76, "energy_joules_est": 59.11, "sample_count": 15, "duration_seconds": 1.922}, "timestamp": "2026-01-17T14:51:37.444082"}
{"image_index": 145, "image_name": "000000015254.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015254.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1485.642, "latencies_ms": [1485.642], "images_per_second": 0.673, "prompt_tokens": 23, "response_tokens_est": 31, "n_tiles": 6, "output_text": "- pasta: 1\n- meat: 1\n- cheese: 1\n- vegetables: 3\n- grapes: 8", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24957.8, "ram_available_mb": 100814.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 24957.5, "ram_available_mb": 100814.7, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.99, "peak": 15.54, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.55, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 34.11, "peak": 40.16, "min": 26.79}, "VIN": {"avg": 71.83, "peak": 94.17, "min": 61.87}}, "power_watts_avg": 34.11, "energy_joules_est": 50.69, "sample_count": 10, "duration_seconds": 1.486}, "timestamp": "2026-01-17T14:51:38.935873"}
{"image_index": 145, "image_name": "000000015254.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015254.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3275.718, "latencies_ms": [3275.718], "images_per_second": 0.305, "prompt_tokens": 27, "response_tokens_est": 97, "n_tiles": 6, "output_text": "The main objects in the image are a variety of food items arranged in a colorful and organized manner. The food items are placed in different colored trays, with the leftmost tray containing a pasta dish, the center tray featuring a salad, and the rightmost tray holding a selection of grapes. The background is dark, which helps to highlight the vibrant colors of the food items. The foreground is occupied by the pasta dish, while the background features the salad and grapes.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24957.5, "ram_available_mb": 100814.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 24957.8, "ram_available_mb": 100814.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.11, "peak": 15.54, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.69, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 28.38, "peak": 40.95, "min": 23.25}, "VIN": {"avg": 70.85, "peak": 116.12, "min": 57.28}}, "power_watts_avg": 28.38, "energy_joules_est": 92.98, "sample_count": 25, "duration_seconds": 3.276}, "timestamp": "2026-01-17T14:51:42.218534"}
{"image_index": 145, "image_name": "000000015254.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015254.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2566.656, "latencies_ms": [2566.656], "images_per_second": 0.39, "prompt_tokens": 21, "response_tokens_est": 71, "n_tiles": 6, "output_text": "The image shows a colorful assortment of food items arranged neatly in various colored trays. The scene appears to be set on a table or countertop, with the food items including a pasta dish, a salad, grapes, and carrots. The setting suggests a meal or snack time, with the food items being prepared and ready to be eaten.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24957.8, "ram_available_mb": 100814.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 24957.3, "ram_available_mb": 100814.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.1, "peak": 15.54, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.67, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 29.44, "peak": 40.18, "min": 23.64}, "VIN": {"avg": 72.35, "peak": 121.69, "min": 63.12}}, "power_watts_avg": 29.44, "energy_joules_est": 75.58, "sample_count": 19, "duration_seconds": 2.567}, "timestamp": "2026-01-17T14:51:44.791458"}
{"image_index": 145, "image_name": "000000015254.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015254.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2530.586, "latencies_ms": [2530.586], "images_per_second": 0.395, "prompt_tokens": 19, "response_tokens_est": 70, "n_tiles": 6, "output_text": "The image showcases a colorful and well-organized meal in a purple and blue container. The vibrant colors of the food, such as the red sauce in the meat dish and the green grapes in the green container, contrast beautifully with the dark background. The lighting is bright and even, highlighting the textures and colors of the food items.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24957.3, "ram_available_mb": 100814.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 24957.3, "ram_available_mb": 100814.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.1, "peak": 15.44, "min": 14.43}, "VDD_CPU_SOC_MSS": {"avg": 15.66, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 29.54, "peak": 40.95, "min": 23.64}, "VIN": {"avg": 71.56, "peak": 118.21, "min": 59.8}}, "power_watts_avg": 29.54, "energy_joules_est": 74.77, "sample_count": 19, "duration_seconds": 2.531}, "timestamp": "2026-01-17T14:51:47.328202"}
{"image_index": 146, "image_name": "000000015272.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015272.jpg", "image_width": 424, "image_height": 640, "image_resolution": "424x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1784.799, "latencies_ms": [1784.799], "images_per_second": 0.56, "prompt_tokens": 9, "response_tokens_est": 44, "n_tiles": 6, "output_text": "The image captures a vibrant scene of cherry blossoms in full bloom, with the blossoms forming a dense canopy that partially obscures the view of a nearby building, creating a picturesque and serene atmosphere.", "error": null, "sys_before": {"cpu_percent": 42.0, "ram_used_mb": 24957.3, "ram_available_mb": 100814.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 24958.3, "ram_available_mb": 100813.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.86, "peak": 15.22, "min": 14.0}, "VDD_CPU_SOC_MSS": {"avg": 15.52, "peak": 16.14, "min": 14.57}, "VDD_GPU": {"avg": 31.2, "peak": 40.16, "min": 24.42}, "VIN": {"avg": 75.44, "peak": 121.67, "min": 61.66}}, "power_watts_avg": 31.2, "energy_joules_est": 55.7, "sample_count": 14, "duration_seconds": 1.785}, "timestamp": "2026-01-17T14:51:49.163938"}
{"image_index": 146, "image_name": "000000015272.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015272.jpg", "image_width": 424, "image_height": 640, "image_resolution": "424x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1721.065, "latencies_ms": [1721.065], "images_per_second": 0.581, "prompt_tokens": 23, "response_tokens_est": 40, "n_tiles": 6, "output_text": "object: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24958.3, "ram_available_mb": 100813.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 24958.0, "ram_available_mb": 100814.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.04, "peak": 15.44, "min": 14.43}, "VDD_CPU_SOC_MSS": {"avg": 15.6, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 32.48, "peak": 40.95, "min": 24.83}, "VIN": {"avg": 71.97, "peak": 94.27, "min": 62.05}}, "power_watts_avg": 32.48, "energy_joules_est": 55.91, "sample_count": 13, "duration_seconds": 1.722}, "timestamp": "2026-01-17T14:51:50.891189"}
{"image_index": 146, "image_name": "000000015272.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015272.jpg", "image_width": 424, "image_height": 640, "image_resolution": "424x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2485.058, "latencies_ms": [2485.058], "images_per_second": 0.402, "prompt_tokens": 27, "response_tokens_est": 68, "n_tiles": 6, "output_text": "The main objects in the image are the cherry blossoms in full bloom, which are in the foreground. The background features a traffic light, which is slightly out of focus. The cherry blossoms are densely packed, creating a lush and vibrant foreground, while the traffic light is positioned in the middle ground, slightly out of focus.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24958.0, "ram_available_mb": 100814.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 24958.2, "ram_available_mb": 100813.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.07, "peak": 15.44, "min": 14.43}, "VDD_CPU_SOC_MSS": {"avg": 15.64, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 29.9, "peak": 41.34, "min": 23.64}, "VIN": {"avg": 72.55, "peak": 104.63, "min": 62.75}}, "power_watts_avg": 29.9, "energy_joules_est": 74.31, "sample_count": 19, "duration_seconds": 2.485}, "timestamp": "2026-01-17T14:51:53.382494"}
{"image_index": 146, "image_name": "000000015272.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015272.jpg", "image_width": 424, "image_height": 640, "image_resolution": "424x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1936.643, "latencies_ms": [1936.643], "images_per_second": 0.516, "prompt_tokens": 21, "response_tokens_est": 48, "n_tiles": 6, "output_text": "The image captures a vibrant scene of cherry blossoms in full bloom, with their delicate pink petals swaying gently in the wind. The blossoms are densely packed on a tree, creating a picturesque and serene atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24958.2, "ram_available_mb": 100813.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 24958.7, "ram_available_mb": 100813.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.06, "peak": 15.44, "min": 14.43}, "VDD_CPU_SOC_MSS": {"avg": 15.62, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 31.38, "peak": 40.97, "min": 24.42}, "VIN": {"avg": 74.76, "peak": 107.38, "min": 57.08}}, "power_watts_avg": 31.38, "energy_joules_est": 60.79, "sample_count": 15, "duration_seconds": 1.937}, "timestamp": "2026-01-17T14:51:55.325335"}
{"image_index": 146, "image_name": "000000015272.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015272.jpg", "image_width": 424, "image_height": 640, "image_resolution": "424x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1856.697, "latencies_ms": [1856.697], "images_per_second": 0.539, "prompt_tokens": 19, "response_tokens_est": 45, "n_tiles": 6, "output_text": "The image showcases a tree in full bloom, with its branches adorned with delicate pink flowers. The lighting is soft and diffused, likely due to an overcast sky, creating a gentle and serene atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24958.7, "ram_available_mb": 100813.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 24958.7, "ram_available_mb": 100813.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.06, "peak": 15.44, "min": 14.43}, "VDD_CPU_SOC_MSS": {"avg": 15.63, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 31.76, "peak": 40.56, "min": 24.43}, "VIN": {"avg": 71.66, "peak": 94.6, "min": 64.18}}, "power_watts_avg": 31.76, "energy_joules_est": 58.98, "sample_count": 14, "duration_seconds": 1.857}, "timestamp": "2026-01-17T14:51:57.188436"}
{"image_index": 147, "image_name": "000000015278.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015278.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2532.33, "latencies_ms": [2532.33], "images_per_second": 0.395, "prompt_tokens": 9, "response_tokens_est": 27, "n_tiles": 12, "output_text": "The image shows a plate of broccoli and other vegetables, including pieces of potato, with a piece of grilled salmon on top.", "error": null, "sys_before": {"cpu_percent": 34.8, "ram_used_mb": 24923.4, "ram_available_mb": 100848.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.6, "ram_used_mb": 24925.4, "ram_available_mb": 100846.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.23, "peak": 16.76, "min": 13.69}, "VDD_CPU_SOC_MSS": {"avg": 15.95, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 36.22, "peak": 44.12, "min": 27.57}, "VIN": {"avg": 81.79, "peak": 123.63, "min": 56.85}}, "power_watts_avg": 36.22, "energy_joules_est": 91.74, "sample_count": 19, "duration_seconds": 2.533}, "timestamp": "2026-01-17T14:51:59.796517"}
{"image_index": 147, "image_name": "000000015278.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015278.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2859.154, "latencies_ms": [2859.154], "images_per_second": 0.35, "prompt_tokens": 23, "response_tokens_est": 35, "n_tiles": 12, "output_text": "broccoli: 10\nonion: 1\ngarlic: 1\nchili: 1\npepper: 1\nrice: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24925.4, "ram_available_mb": 100846.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 24926.4, "ram_available_mb": 100845.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.01, "peak": 16.36, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.58, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 36.43, "peak": 45.3, "min": 26.39}, "VIN": {"avg": 83.26, "peak": 131.07, "min": 64.1}}, "power_watts_avg": 36.43, "energy_joules_est": 104.17, "sample_count": 21, "duration_seconds": 2.859}, "timestamp": "2026-01-17T14:52:02.662054"}
{"image_index": 147, "image_name": "000000015278.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015278.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 4210.677, "latencies_ms": [4210.677], "images_per_second": 0.237, "prompt_tokens": 27, "response_tokens_est": 74, "n_tiles": 12, "output_text": "The main object in the foreground is a plate of broccoli and other vegetables. The broccoli is the most prominent vegetable, occupying the center of the plate. The other vegetables, such as carrots and possibly other greens, are scattered around the broccoli. The background is slightly blurred, indicating that the focus is on the broccoli and the other vegetables.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24926.4, "ram_available_mb": 100845.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 24927.6, "ram_available_mb": 100844.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.89, "peak": 16.26, "min": 14.02}, "VDD_CPU_SOC_MSS": {"avg": 15.4, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 32.81, "peak": 44.91, "min": 25.6}, "VIN": {"avg": 74.48, "peak": 113.84, "min": 54.0}}, "power_watts_avg": 32.81, "energy_joules_est": 138.16, "sample_count": 32, "duration_seconds": 4.211}, "timestamp": "2026-01-17T14:52:06.880032"}
{"image_index": 147, "image_name": "000000015278.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015278.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3026.752, "latencies_ms": [3026.752], "images_per_second": 0.33, "prompt_tokens": 21, "response_tokens_est": 40, "n_tiles": 12, "output_text": "The image depicts a plate of broccoli and other vegetables, possibly a part of a meal. The setting appears to be a dining table, with the focus on the colorful and healthy meal.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 24927.6, "ram_available_mb": 100844.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 24927.8, "ram_available_mb": 100844.3, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.99, "peak": 16.36, "min": 14.02}, "VDD_CPU_SOC_MSS": {"avg": 15.54, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 35.47, "peak": 45.3, "min": 26.0}, "VIN": {"avg": 77.17, "peak": 116.77, "min": 63.7}}, "power_watts_avg": 35.47, "energy_joules_est": 107.37, "sample_count": 23, "duration_seconds": 3.027}, "timestamp": "2026-01-17T14:52:09.913515"}
{"image_index": 147, "image_name": "000000015278.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015278.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 4003.132, "latencies_ms": [4003.132], "images_per_second": 0.25, "prompt_tokens": 19, "response_tokens_est": 68, "n_tiles": 12, "output_text": "The image showcases a plate of broccoli and other vegetables, with the broccoli being the most prominent color due to its vibrant green hue. The lighting is bright and even, highlighting the textures and colors of the food. The plate itself is white, providing a clean and neutral background that makes the colors of the food stand out.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24927.8, "ram_available_mb": 100844.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 24928.1, "ram_available_mb": 100844.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.91, "peak": 16.36, "min": 13.92}, "VDD_CPU_SOC_MSS": {"avg": 15.38, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 33.02, "peak": 44.91, "min": 25.6}, "VIN": {"avg": 72.71, "peak": 118.2, "min": 57.25}}, "power_watts_avg": 33.02, "energy_joules_est": 132.19, "sample_count": 31, "duration_seconds": 4.003}, "timestamp": "2026-01-17T14:52:13.927151"}
{"image_index": 148, "image_name": "000000015335.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015335.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 3043.737, "latencies_ms": [3043.737], "images_per_second": 0.329, "prompt_tokens": 9, "response_tokens_est": 42, "n_tiles": 12, "output_text": "The image depicts a dimly lit indoor setting with a group of people seated around a table, some of whom are engaged in conversation, while others appear to be focused on their phones or other devices.", "error": null, "sys_before": {"cpu_percent": 28.3, "ram_used_mb": 24923.4, "ram_available_mb": 100848.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 24919.6, "ram_available_mb": 100852.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.12, "peak": 16.76, "min": 13.19}, "VDD_CPU_SOC_MSS": {"avg": 15.68, "peak": 17.71, "min": 13.78}, "VDD_GPU": {"avg": 34.73, "peak": 44.91, "min": 26.39}, "VIN": {"avg": 78.81, "peak": 113.98, "min": 62.23}}, "power_watts_avg": 34.73, "energy_joules_est": 105.73, "sample_count": 23, "duration_seconds": 3.044}, "timestamp": "2026-01-17T14:52:17.056575"}
{"image_index": 148, "image_name": "000000015335.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015335.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2760.864, "latencies_ms": [2760.864], "images_per_second": 0.362, "prompt_tokens": 23, "response_tokens_est": 32, "n_tiles": 12, "output_text": "1. Person\n2. Person\n3. Person\n4. Person\n5. Person\n6. Person\n7. Person\n8. Person", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24919.6, "ram_available_mb": 100852.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 24920.6, "ram_available_mb": 100851.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.23, "peak": 16.56, "min": 14.02}, "VDD_CPU_SOC_MSS": {"avg": 15.82, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 36.41, "peak": 45.68, "min": 27.18}, "VIN": {"avg": 80.4, "peak": 113.36, "min": 58.41}}, "power_watts_avg": 36.41, "energy_joules_est": 100.54, "sample_count": 22, "duration_seconds": 2.761}, "timestamp": "2026-01-17T14:52:19.824838"}
{"image_index": 148, "image_name": "000000015335.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015335.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3779.639, "latencies_ms": [3779.639], "images_per_second": 0.265, "prompt_tokens": 27, "response_tokens_est": 62, "n_tiles": 12, "output_text": "The main objects in the image are a group of people sitting at a table. The person in the foreground is a young boy eating, while the other people are seated in the background. The table is located in the center of the image, with the boy in the foreground being the closest to the camera.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24920.6, "ram_available_mb": 100851.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 24921.6, "ram_available_mb": 100850.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.22, "peak": 16.56, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.76, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 33.79, "peak": 45.28, "min": 26.0}, "VIN": {"avg": 78.94, "peak": 142.76, "min": 63.36}}, "power_watts_avg": 33.79, "energy_joules_est": 127.72, "sample_count": 29, "duration_seconds": 3.78}, "timestamp": "2026-01-17T14:52:23.612086"}
{"image_index": 148, "image_name": "000000015335.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015335.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 4011.9, "latencies_ms": [4011.9], "images_per_second": 0.249, "prompt_tokens": 21, "response_tokens_est": 69, "n_tiles": 12, "output_text": "The image depicts a dimly lit indoor setting, likely a restaurant or bar, with patrons seated at tables. The atmosphere is cozy and intimate, with warm lighting creating a relaxed ambiance. The focus is on a young boy and a man, both engaged in conversation, with the boy appearing to be eating or playing with his hands.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24921.6, "ram_available_mb": 100850.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 24918.5, "ram_available_mb": 100853.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.18, "peak": 16.66, "min": 14.12}, "VDD_CPU_SOC_MSS": {"avg": 15.75, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 33.12, "peak": 44.89, "min": 26.0}, "VIN": {"avg": 75.34, "peak": 118.67, "min": 54.97}}, "power_watts_avg": 33.12, "energy_joules_est": 132.88, "sample_count": 31, "duration_seconds": 4.012}, "timestamp": "2026-01-17T14:52:27.630503"}
{"image_index": 148, "image_name": "000000015335.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015335.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2786.419, "latencies_ms": [2786.419], "images_per_second": 0.359, "prompt_tokens": 19, "response_tokens_est": 33, "n_tiles": 12, "output_text": "The image depicts a dimly lit indoor setting with warm, orange lighting. The individuals are seated, and the background features a wrought iron railing.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24918.5, "ram_available_mb": 100853.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 24919.5, "ram_available_mb": 100852.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.25, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.82, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 36.41, "peak": 45.69, "min": 27.18}, "VIN": {"avg": 76.08, "peak": 117.84, "min": 56.08}}, "power_watts_avg": 36.41, "energy_joules_est": 101.48, "sample_count": 21, "duration_seconds": 2.787}, "timestamp": "2026-01-17T14:52:30.423755"}
{"image_index": 149, "image_name": "000000015338.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015338.jpg", "image_width": 640, "image_height": 424, "image_resolution": "640x424", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1252.412, "latencies_ms": [1252.412], "images_per_second": 0.798, "prompt_tokens": 9, "response_tokens_est": 24, "n_tiles": 6, "output_text": "A yellow bus is parked on the side of a street, with a white van and a building in the background.", "error": null, "sys_before": {"cpu_percent": 41.1, "ram_used_mb": 24938.2, "ram_available_mb": 100833.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 24939.7, "ram_available_mb": 100832.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.66, "peak": 15.14, "min": 13.92}, "VDD_CPU_SOC_MSS": {"avg": 15.35, "peak": 16.14, "min": 14.17}, "VDD_GPU": {"avg": 35.15, "peak": 41.36, "min": 27.97}, "VIN": {"avg": 83.99, "peak": 122.84, "min": 58.72}}, "power_watts_avg": 35.15, "energy_joules_est": 44.05, "sample_count": 9, "duration_seconds": 1.253}, "timestamp": "2026-01-17T14:52:31.731648"}
{"image_index": 149, "image_name": "000000015338.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015338.jpg", "image_width": 640, "image_height": 424, "image_resolution": "640x424", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1542.197, "latencies_ms": [1542.197], "images_per_second": 0.648, "prompt_tokens": 23, "response_tokens_est": 32, "n_tiles": 6, "output_text": "1. Bus\n2. Bus\n3. Bus\n4. Bus\n5. Bus\n6. Bus\n7. Bus\n8. Bus", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24939.7, "ram_available_mb": 100832.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 24940.4, "ram_available_mb": 100831.7, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.83, "peak": 15.24, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.32, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 34.05, "peak": 41.34, "min": 25.61}, "VIN": {"avg": 74.47, "peak": 109.46, "min": 62.98}}, "power_watts_avg": 34.05, "energy_joules_est": 52.52, "sample_count": 11, "duration_seconds": 1.543}, "timestamp": "2026-01-17T14:52:33.281088"}
{"image_index": 149, "image_name": "000000015338.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015338.jpg", "image_width": 640, "image_height": 424, "image_resolution": "640x424", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2054.312, "latencies_ms": [2054.312], "images_per_second": 0.487, "prompt_tokens": 27, "response_tokens_est": 50, "n_tiles": 6, "output_text": "The main objects in the image are a bus and a building. The bus is located in the foreground, near the sidewalk, while the building is in the background. The bus is positioned closer to the foreground, while the building is further back.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 24940.4, "ram_available_mb": 100831.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 24941.6, "ram_available_mb": 100830.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.83, "peak": 15.24, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.45, "peak": 15.75, "min": 14.56}, "VDD_GPU": {"avg": 30.92, "peak": 41.74, "min": 23.64}, "VIN": {"avg": 73.93, "peak": 112.74, "min": 59.05}}, "power_watts_avg": 30.92, "energy_joules_est": 63.53, "sample_count": 16, "duration_seconds": 2.055}, "timestamp": "2026-01-17T14:52:35.341646"}
{"image_index": 149, "image_name": "000000015338.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015338.jpg", "image_width": 640, "image_height": 424, "image_resolution": "640x424", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2387.877, "latencies_ms": [2387.877], "images_per_second": 0.419, "prompt_tokens": 21, "response_tokens_est": 62, "n_tiles": 6, "output_text": "The image depicts a city street scene with a yellow bus in the foreground, moving along a curved road. The setting appears to be a modern urban area with a mix of buildings, trees, and a sidewalk. The bus is the main focus, and the scene is captured during daylight with clear skies.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24941.6, "ram_available_mb": 100830.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 24942.1, "ram_available_mb": 100830.0, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.88, "peak": 15.24, "min": 14.3}, "VDD_CPU_SOC_MSS": {"avg": 15.49, "peak": 15.75, "min": 14.56}, "VDD_GPU": {"avg": 29.72, "peak": 40.57, "min": 23.24}, "VIN": {"avg": 72.64, "peak": 106.11, "min": 62.38}}, "power_watts_avg": 29.72, "energy_joules_est": 70.98, "sample_count": 18, "duration_seconds": 2.388}, "timestamp": "2026-01-17T14:52:37.739715"}
{"image_index": 149, "image_name": "000000015338.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015338.jpg", "image_width": 640, "image_height": 424, "image_resolution": "640x424", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2275.18, "latencies_ms": [2275.18], "images_per_second": 0.44, "prompt_tokens": 19, "response_tokens_est": 58, "n_tiles": 6, "output_text": "The image depicts a sunny day with clear blue skies. The scene is characterized by the bright yellow bus in the foreground, contrasting sharply with the gray building in the background. The lighting is natural, with shadows cast by the trees and buildings, indicating a bright and clear day.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 24942.1, "ram_available_mb": 100830.0, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 24942.9, "ram_available_mb": 100829.3, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.84, "peak": 15.24, "min": 14.2}, "VDD_CPU_SOC_MSS": {"avg": 15.52, "peak": 16.14, "min": 14.57}, "VDD_GPU": {"avg": 29.87, "peak": 40.18, "min": 23.25}, "VIN": {"avg": 72.4, "peak": 100.64, "min": 63.02}}, "power_watts_avg": 29.87, "energy_joules_est": 67.97, "sample_count": 17, "duration_seconds": 2.275}, "timestamp": "2026-01-17T14:52:40.021468"}
{"image_index": 150, "image_name": "000000015440.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015440.jpg", "image_width": 404, "image_height": 640, "image_resolution": "404x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1557.479, "latencies_ms": [1557.479], "images_per_second": 0.642, "prompt_tokens": 9, "response_tokens_est": 35, "n_tiles": 6, "output_text": "The image depicts a stop sign mounted on a metal pole, with a blue car visible in the background, and a clear, sunny sky casting shadows on the ground.", "error": null, "sys_before": {"cpu_percent": 39.7, "ram_used_mb": 24942.9, "ram_available_mb": 100829.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 24943.1, "ram_available_mb": 100829.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.7, "peak": 15.14, "min": 13.79}, "VDD_CPU_SOC_MSS": {"avg": 15.43, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 32.7, "peak": 40.57, "min": 25.61}, "VIN": {"avg": 76.47, "peak": 112.07, "min": 63.25}}, "power_watts_avg": 32.7, "energy_joules_est": 50.94, "sample_count": 11, "duration_seconds": 1.558}, "timestamp": "2026-01-17T14:52:41.627417"}
{"image_index": 150, "image_name": "000000015440.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015440.jpg", "image_width": 404, "image_height": 640, "image_resolution": "404x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1773.929, "latencies_ms": [1773.929], "images_per_second": 0.564, "prompt_tokens": 23, "response_tokens_est": 40, "n_tiles": 6, "output_text": "object: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24943.1, "ram_available_mb": 100829.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 24944.1, "ram_available_mb": 100828.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.85, "peak": 15.34, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.32, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 32.27, "peak": 40.95, "min": 24.43}, "VIN": {"avg": 71.23, "peak": 111.83, "min": 61.57}}, "power_watts_avg": 32.27, "energy_joules_est": 57.25, "sample_count": 13, "duration_seconds": 1.774}, "timestamp": "2026-01-17T14:52:43.407342"}
{"image_index": 150, "image_name": "000000015440.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015440.jpg", "image_width": 404, "image_height": 640, "image_resolution": "404x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2657.999, "latencies_ms": [2657.999], "images_per_second": 0.376, "prompt_tokens": 27, "response_tokens_est": 71, "n_tiles": 6, "output_text": "The main objects in the image are a stop sign and a metal fence. The stop sign is positioned in the foreground, near the bottom left corner of the image. The metal fence is situated in the background, to the right of the stop sign. The fence is partially obscured by the stop sign, creating a sense of depth in the scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24944.1, "ram_available_mb": 100828.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 24944.1, "ram_available_mb": 100828.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.83, "peak": 15.24, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.45, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 28.93, "peak": 40.57, "min": 22.86}, "VIN": {"avg": 71.0, "peak": 112.45, "min": 53.14}}, "power_watts_avg": 28.93, "energy_joules_est": 76.9, "sample_count": 20, "duration_seconds": 2.658}, "timestamp": "2026-01-17T14:52:46.075575"}
{"image_index": 150, "image_name": "000000015440.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015440.jpg", "image_width": 404, "image_height": 640, "image_resolution": "404x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2886.614, "latencies_ms": [2886.614], "images_per_second": 0.346, "prompt_tokens": 21, "response_tokens_est": 79, "n_tiles": 6, "output_text": "The image depicts a scene in an urban setting during the daytime, with a stop sign prominently displayed on a pole. The sign is red with white lettering, and it is situated in a paved area with a metal railing and some greenery in the foreground. The background shows a blurred cityscape with buildings and a clear sky, suggesting a bustling city environment.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24944.1, "ram_available_mb": 100828.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 24944.1, "ram_available_mb": 100828.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.84, "peak": 15.24, "min": 14.2}, "VDD_CPU_SOC_MSS": {"avg": 15.32, "peak": 15.74, "min": 14.56}, "VDD_GPU": {"avg": 28.33, "peak": 40.57, "min": 22.86}, "VIN": {"avg": 67.83, "peak": 103.71, "min": 61.24}}, "power_watts_avg": 28.33, "energy_joules_est": 81.79, "sample_count": 22, "duration_seconds": 2.887}, "timestamp": "2026-01-17T14:52:48.968361"}
{"image_index": 150, "image_name": "000000015440.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015440.jpg", "image_width": 404, "image_height": 640, "image_resolution": "404x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2997.423, "latencies_ms": [2997.423], "images_per_second": 0.334, "prompt_tokens": 19, "response_tokens_est": 83, "n_tiles": 6, "output_text": "The image features a stop sign with a red octagonal shape and white lettering, mounted on a metal pole. The sign is illuminated by the warm, golden light of a setting or rising sun, casting long shadows and creating a serene atmosphere. The surrounding area includes a paved road, a metal fence with arches, and some greenery, all bathed in the soft, golden light.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 24944.1, "ram_available_mb": 100828.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 24944.3, "ram_available_mb": 100827.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.86, "peak": 15.24, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.32, "peak": 15.74, "min": 14.56}, "VDD_GPU": {"avg": 28.07, "peak": 40.18, "min": 22.86}, "VIN": {"avg": 70.95, "peak": 117.26, "min": 55.68}}, "power_watts_avg": 28.07, "energy_joules_est": 84.15, "sample_count": 23, "duration_seconds": 2.998}, "timestamp": "2026-01-17T14:52:51.971370"}
{"image_index": 151, "image_name": "000000015497.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015497.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2485.874, "latencies_ms": [2485.874], "images_per_second": 0.402, "prompt_tokens": 9, "response_tokens_est": 26, "n_tiles": 12, "output_text": "A cat is lying on a black surface, with its paws curled up and a white and brown striped body.", "error": null, "sys_before": {"cpu_percent": 36.0, "ram_used_mb": 24910.4, "ram_available_mb": 100861.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.6, "ram_used_mb": 24928.0, "ram_available_mb": 100844.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.24, "peak": 16.66, "min": 13.69}, "VDD_CPU_SOC_MSS": {"avg": 15.99, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 36.16, "peak": 43.73, "min": 27.57}, "VIN": {"avg": 80.35, "peak": 111.57, "min": 67.19}}, "power_watts_avg": 36.16, "energy_joules_est": 89.91, "sample_count": 19, "duration_seconds": 2.486}, "timestamp": "2026-01-17T14:52:54.522186"}
{"image_index": 151, "image_name": "000000015497.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015497.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2104.26, "latencies_ms": [2104.26], "images_per_second": 0.475, "prompt_tokens": 23, "response_tokens_est": 13, "n_tiles": 12, "output_text": "1. Cat\n2. Mouse\n3. Power cord", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24928.0, "ram_available_mb": 100844.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.0, "ram_used_mb": 24928.8, "ram_available_mb": 100843.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.37, "peak": 16.66, "min": 14.43}, "VDD_CPU_SOC_MSS": {"avg": 16.01, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 39.19, "peak": 45.3, "min": 31.51}, "VIN": {"avg": 85.23, "peak": 116.93, "min": 66.66}}, "power_watts_avg": 39.19, "energy_joules_est": 82.53, "sample_count": 16, "duration_seconds": 2.106}, "timestamp": "2026-01-17T14:52:56.636197"}
{"image_index": 151, "image_name": "000000015497.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015497.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3574.08, "latencies_ms": [3574.08], "images_per_second": 0.28, "prompt_tokens": 27, "response_tokens_est": 56, "n_tiles": 12, "output_text": "The main object in the foreground is a white cat with black and brown stripes, sitting on a black surface. The cat is holding a white cord in its paws. In the background, there is a black object, possibly a piece of furniture or clothing, partially visible.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24928.8, "ram_available_mb": 100843.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 24928.8, "ram_available_mb": 100843.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.22, "peak": 16.56, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.84, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 34.61, "peak": 46.86, "min": 26.01}, "VIN": {"avg": 77.53, "peak": 110.07, "min": 60.91}}, "power_watts_avg": 34.61, "energy_joules_est": 123.71, "sample_count": 28, "duration_seconds": 3.575}, "timestamp": "2026-01-17T14:53:00.216755"}
{"image_index": 151, "image_name": "000000015497.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015497.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3724.116, "latencies_ms": [3724.116], "images_per_second": 0.269, "prompt_tokens": 21, "response_tokens_est": 60, "n_tiles": 12, "output_text": "The image depicts a domestic cat lying on a dark surface, possibly a bed or couch. The cat is positioned with its front paws resting on a white mouse, which is connected to a power cord. The setting appears to be indoors, and the cat seems to be relaxed and comfortable.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24928.8, "ram_available_mb": 100843.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 24930.0, "ram_available_mb": 100842.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.94, "peak": 16.36, "min": 14.02}, "VDD_CPU_SOC_MSS": {"avg": 15.5, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 33.61, "peak": 44.91, "min": 25.6}, "VIN": {"avg": 75.72, "peak": 111.47, "min": 60.16}}, "power_watts_avg": 33.61, "energy_joules_est": 125.18, "sample_count": 29, "duration_seconds": 3.725}, "timestamp": "2026-01-17T14:53:03.947380"}
{"image_index": 151, "image_name": "000000015497.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015497.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3411.937, "latencies_ms": [3411.937], "images_per_second": 0.293, "prompt_tokens": 19, "response_tokens_est": 51, "n_tiles": 12, "output_text": "The cat in the image has a striking appearance with a white and brown striped coat. The lighting is dim, casting a soft glow on the cat's fur, and the background is dark, providing a stark contrast that highlights the cat's features.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24930.0, "ram_available_mb": 100842.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 24930.0, "ram_available_mb": 100842.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.91, "peak": 16.36, "min": 13.92}, "VDD_CPU_SOC_MSS": {"avg": 15.56, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 34.24, "peak": 45.69, "min": 25.6}, "VIN": {"avg": 75.65, "peak": 116.31, "min": 59.75}}, "power_watts_avg": 34.24, "energy_joules_est": 116.84, "sample_count": 27, "duration_seconds": 3.412}, "timestamp": "2026-01-17T14:53:07.366104"}
{"image_index": 152, "image_name": "000000015517.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015517.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2660.136, "latencies_ms": [2660.136], "images_per_second": 0.376, "prompt_tokens": 9, "response_tokens_est": 31, "n_tiles": 12, "output_text": "The image depicts a bustling urban scene with a variety of buses and vehicles navigating through a busy city street, surrounded by modern buildings and infrastructure.", "error": null, "sys_before": {"cpu_percent": 44.5, "ram_used_mb": 24932.7, "ram_available_mb": 100839.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.5, "ram_used_mb": 24929.2, "ram_available_mb": 100843.0, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.12, "peak": 16.66, "min": 13.19}, "VDD_CPU_SOC_MSS": {"avg": 15.88, "peak": 17.71, "min": 14.17}, "VDD_GPU": {"avg": 35.93, "peak": 44.51, "min": 27.18}, "VIN": {"avg": 77.68, "peak": 107.91, "min": 60.17}}, "power_watts_avg": 35.93, "energy_joules_est": 95.59, "sample_count": 20, "duration_seconds": 2.66}, "timestamp": "2026-01-17T14:53:10.129835"}
{"image_index": 152, "image_name": "000000015517.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015517.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2383.31, "latencies_ms": [2383.31], "images_per_second": 0.42, "prompt_tokens": 23, "response_tokens_est": 21, "n_tiles": 12, "output_text": "bus: 5\nbuilding: 1\ntrees: 1\nsign: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24929.2, "ram_available_mb": 100843.0, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.3, "ram_used_mb": 24926.1, "ram_available_mb": 100846.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.3, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.86, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 38.09, "peak": 44.89, "min": 29.15}, "VIN": {"avg": 84.56, "peak": 133.47, "min": 61.82}}, "power_watts_avg": 38.09, "energy_joules_est": 90.79, "sample_count": 18, "duration_seconds": 2.384}, "timestamp": "2026-01-17T14:53:12.519482"}
{"image_index": 152, "image_name": "000000015517.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015517.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 4052.389, "latencies_ms": [4052.389], "images_per_second": 0.247, "prompt_tokens": 27, "response_tokens_est": 70, "n_tiles": 12, "output_text": "The main objects in the image are a bus and a cityscape. The bus is in the foreground, near the bottom left corner, while the cityscape is in the background, stretching across the entire image. The bus is parked on the street, and the cityscape features various buildings, including a tall tower with a red and white antenna.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24926.1, "ram_available_mb": 100846.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 24926.1, "ram_available_mb": 100846.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.2, "peak": 16.66, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.72, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 33.39, "peak": 45.28, "min": 26.0}, "VIN": {"avg": 77.43, "peak": 128.53, "min": 56.97}}, "power_watts_avg": 33.39, "energy_joules_est": 135.32, "sample_count": 31, "duration_seconds": 4.053}, "timestamp": "2026-01-17T14:53:16.578386"}
{"image_index": 152, "image_name": "000000015517.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015517.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3598.109, "latencies_ms": [3598.109], "images_per_second": 0.278, "prompt_tokens": 21, "response_tokens_est": 57, "n_tiles": 12, "output_text": "The image depicts a bustling urban scene with a busy street filled with various buses and vehicles. The setting appears to be a city with modern buildings and infrastructure, including a bridge and a roadway. The weather is overcast, and the overall atmosphere is busy and dynamic.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24926.1, "ram_available_mb": 100846.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 24927.5, "ram_available_mb": 100844.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.24, "peak": 16.66, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.78, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 34.14, "peak": 44.91, "min": 26.0}, "VIN": {"avg": 78.75, "peak": 121.58, "min": 61.61}}, "power_watts_avg": 34.14, "energy_joules_est": 122.85, "sample_count": 27, "duration_seconds": 3.598}, "timestamp": "2026-01-17T14:53:20.183566"}
{"image_index": 152, "image_name": "000000015517.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015517.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3777.025, "latencies_ms": [3777.025], "images_per_second": 0.265, "prompt_tokens": 19, "response_tokens_est": 62, "n_tiles": 12, "output_text": "The image depicts a bustling urban scene with a mix of modern and traditional architectural styles. The sky is partly cloudy, providing a soft, diffused light that enhances the colors and textures of the scene. The buses, with their varied colors and designs, add vibrancy to the urban landscape.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24927.5, "ram_available_mb": 100844.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 24927.8, "ram_available_mb": 100844.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.19, "peak": 16.56, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.73, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 33.63, "peak": 44.51, "min": 26.0}, "VIN": {"avg": 76.08, "peak": 112.46, "min": 56.39}}, "power_watts_avg": 33.63, "energy_joules_est": 127.03, "sample_count": 29, "duration_seconds": 3.777}, "timestamp": "2026-01-17T14:53:23.967216"}
{"image_index": 153, "image_name": "000000015597.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015597.jpg", "image_width": 433, "image_height": 640, "image_resolution": "433x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1274.352, "latencies_ms": [1274.352], "images_per_second": 0.785, "prompt_tokens": 9, "response_tokens_est": 25, "n_tiles": 6, "output_text": "A young man is performing a skateboard trick on a ramp, with a large tent and other equipment in the background.", "error": null, "sys_before": {"cpu_percent": 39.4, "ram_used_mb": 24946.4, "ram_available_mb": 100825.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 24946.7, "ram_available_mb": 100825.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.65, "peak": 15.11, "min": 13.82}, "VDD_CPU_SOC_MSS": {"avg": 15.35, "peak": 16.14, "min": 14.17}, "VDD_GPU": {"avg": 34.23, "peak": 40.56, "min": 27.18}, "VIN": {"avg": 76.79, "peak": 118.28, "min": 56.1}}, "power_watts_avg": 34.23, "energy_joules_est": 43.64, "sample_count": 10, "duration_seconds": 1.275}, "timestamp": "2026-01-17T14:53:25.297238"}
{"image_index": 153, "image_name": "000000015597.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015597.jpg", "image_width": 433, "image_height": 640, "image_resolution": "433x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2176.588, "latencies_ms": [2176.588], "images_per_second": 0.459, "prompt_tokens": 23, "response_tokens_est": 57, "n_tiles": 6, "output_text": "skateboard: 1\nskateboarder: 1\nskateboard: 1\nskateboard: 1\nskateboard: 1\nskateboard: 1\nskateboard: 1\nskateboard: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24946.7, "ram_available_mb": 100825.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 24947.4, "ram_available_mb": 100824.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.08, "peak": 15.44, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.77, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 31.05, "peak": 41.74, "min": 24.04}, "VIN": {"avg": 70.23, "peak": 89.75, "min": 59.64}}, "power_watts_avg": 31.05, "energy_joules_est": 67.6, "sample_count": 17, "duration_seconds": 2.177}, "timestamp": "2026-01-17T14:53:27.480123"}
{"image_index": 153, "image_name": "000000015597.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015597.jpg", "image_width": 433, "image_height": 640, "image_resolution": "433x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2688.673, "latencies_ms": [2688.673], "images_per_second": 0.372, "prompt_tokens": 27, "response_tokens_est": 76, "n_tiles": 6, "output_text": "The main object in the foreground is a skateboarder performing a trick on a ramp. The skateboarder is positioned near the center of the image, with their skateboard and feet clearly visible. The background features a large tent structure, which is slightly out of focus. The skateboarder's shadow is cast on the ground, indicating the direction of the light source.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24947.6, "ram_available_mb": 100824.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 24948.4, "ram_available_mb": 100823.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.16, "peak": 15.54, "min": 14.43}, "VDD_CPU_SOC_MSS": {"avg": 15.81, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 29.31, "peak": 40.16, "min": 23.64}, "VIN": {"avg": 68.91, "peak": 80.18, "min": 62.39}}, "power_watts_avg": 29.31, "energy_joules_est": 78.82, "sample_count": 20, "duration_seconds": 2.689}, "timestamp": "2026-01-17T14:53:30.178937"}
{"image_index": 153, "image_name": "000000015597.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015597.jpg", "image_width": 433, "image_height": 640, "image_resolution": "433x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2468.37, "latencies_ms": [2468.37], "images_per_second": 0.405, "prompt_tokens": 21, "response_tokens_est": 68, "n_tiles": 6, "output_text": "The image captures a skateboarder performing a trick on a concrete ramp at an outdoor skate park. The skate park is surrounded by large, tarp-covered structures, and there are other skateboards and bicycles scattered around. The setting is sunny, and the skateboarder is wearing a hat and shorts, indicating warm weather.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24948.4, "ram_available_mb": 100823.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 24948.1, "ram_available_mb": 100824.0, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.12, "peak": 15.44, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.79, "peak": 16.14, "min": 14.57}, "VDD_GPU": {"avg": 29.69, "peak": 40.97, "min": 23.64}, "VIN": {"avg": 71.06, "peak": 91.07, "min": 63.6}}, "power_watts_avg": 29.69, "energy_joules_est": 73.3, "sample_count": 19, "duration_seconds": 2.469}, "timestamp": "2026-01-17T14:53:32.653634"}
{"image_index": 153, "image_name": "000000015597.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015597.jpg", "image_width": 433, "image_height": 640, "image_resolution": "433x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2219.061, "latencies_ms": [2219.061], "images_per_second": 0.451, "prompt_tokens": 19, "response_tokens_est": 59, "n_tiles": 6, "output_text": "The image features a skateboarder performing a trick on a concrete ramp, with a clear blue sky overhead. The skateboarder is wearing a light-colored hat and shorts, and the ramp is covered with a tarp. The lighting is bright and natural, casting shadows on the ramp.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24948.1, "ram_available_mb": 100824.0, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 24948.9, "ram_available_mb": 100823.3, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.17, "peak": 15.54, "min": 14.43}, "VDD_CPU_SOC_MSS": {"avg": 15.96, "peak": 16.53, "min": 14.96}, "VDD_GPU": {"avg": 30.36, "peak": 40.57, "min": 24.03}, "VIN": {"avg": 70.23, "peak": 100.95, "min": 62.06}}, "power_watts_avg": 30.36, "energy_joules_est": 67.38, "sample_count": 17, "duration_seconds": 2.219}, "timestamp": "2026-01-17T14:53:34.878770"}
{"image_index": 154, "image_name": "000000015660.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015660.jpg", "image_width": 640, "image_height": 348, "image_resolution": "640x348", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 872.961, "latencies_ms": [872.961], "images_per_second": 1.146, "prompt_tokens": 9, "response_tokens_est": 31, "n_tiles": 2, "output_text": "A person is windsurfing on the ocean, holding onto a board with a sail, while other windsurfers are also seen in the distance.", "error": null, "sys_before": {"cpu_percent": 30.3, "ram_used_mb": 24948.9, "ram_available_mb": 100823.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 24949.1, "ram_available_mb": 100823.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5055.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.71, "peak": 15.22, "min": 14.2}, "VDD_CPU_SOC_MSS": {"avg": 15.55, "peak": 16.15, "min": 14.96}, "VDD_GPU": {"avg": 27.05, "peak": 32.3, "min": 23.24}, "VIN": {"avg": 65.12, "peak": 78.88, "min": 58.25}}, "power_watts_avg": 27.05, "energy_joules_est": 23.62, "sample_count": 6, "duration_seconds": 0.873}, "timestamp": "2026-01-17T14:53:35.779602"}
{"image_index": 154, "image_name": "000000015660.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015660.jpg", "image_width": 640, "image_height": 348, "image_resolution": "640x348", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1081.57, "latencies_ms": [1081.57], "images_per_second": 0.925, "prompt_tokens": 23, "response_tokens_est": 40, "n_tiles": 2, "output_text": "object: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24949.1, "ram_available_mb": 100823.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 24949.6, "ram_available_mb": 100822.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5064.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.86, "peak": 15.22, "min": 14.51}, "VDD_CPU_SOC_MSS": {"avg": 16.2, "peak": 16.95, "min": 15.36}, "VDD_GPU": {"avg": 25.95, "peak": 31.89, "min": 22.45}, "VIN": {"avg": 68.58, "peak": 103.39, "min": 61.27}}, "power_watts_avg": 25.95, "energy_joules_est": 28.08, "sample_count": 8, "duration_seconds": 1.082}, "timestamp": "2026-01-17T14:53:36.867053"}
{"image_index": 154, "image_name": "000000015660.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015660.jpg", "image_width": 640, "image_height": 348, "image_resolution": "640x348", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1380.777, "latencies_ms": [1380.777], "images_per_second": 0.724, "prompt_tokens": 27, "response_tokens_est": 51, "n_tiles": 2, "output_text": "The main objects in the image are a person kiteboarding and several kites flying in the sky. The person is positioned in the foreground, on a kiteboard, while the kites are in the background, flying high above the water.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24949.6, "ram_available_mb": 100822.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24949.6, "ram_available_mb": 100822.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5067.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.12, "peak": 15.42, "min": 14.51}, "VDD_CPU_SOC_MSS": {"avg": 16.1, "peak": 16.53, "min": 15.75}, "VDD_GPU": {"avg": 24.7, "peak": 31.12, "min": 21.27}, "VIN": {"avg": 65.31, "peak": 88.3, "min": 61.44}}, "power_watts_avg": 24.7, "energy_joules_est": 34.11, "sample_count": 10, "duration_seconds": 1.381}, "timestamp": "2026-01-17T14:53:38.253473"}
{"image_index": 154, "image_name": "000000015660.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015660.jpg", "image_width": 640, "image_height": 348, "image_resolution": "640x348", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1906.738, "latencies_ms": [1906.738], "images_per_second": 0.524, "prompt_tokens": 21, "response_tokens_est": 73, "n_tiles": 2, "output_text": "The image captures a serene beach scene with a person windsurfing in the ocean. The person is wearing a wetsuit and is holding onto a windsurf board, which is partially submerged in the water. The ocean is calm with gentle waves, and several kites are flying in the sky, adding to the picturesque atmosphere of the beach.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24949.6, "ram_available_mb": 100822.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 24950.8, "ram_available_mb": 100821.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5063.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.16, "peak": 15.42, "min": 14.61}, "VDD_CPU_SOC_MSS": {"avg": 16.37, "peak": 16.93, "min": 15.35}, "VDD_GPU": {"avg": 23.25, "peak": 30.73, "min": 20.49}, "VIN": {"avg": 65.18, "peak": 99.03, "min": 55.54}}, "power_watts_avg": 23.25, "energy_joules_est": 44.34, "sample_count": 14, "duration_seconds": 1.907}, "timestamp": "2026-01-17T14:53:40.169991"}
{"image_index": 154, "image_name": "000000015660.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015660.jpg", "image_width": 640, "image_height": 348, "image_resolution": "640x348", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1068.609, "latencies_ms": [1068.609], "images_per_second": 0.936, "prompt_tokens": 19, "response_tokens_est": 38, "n_tiles": 2, "output_text": "The image depicts a vibrant scene at the beach with clear blue skies and a few scattered clouds. The ocean waves are gently crashing onto the shore, creating a dynamic and lively atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24950.8, "ram_available_mb": 100821.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24950.6, "ram_available_mb": 100821.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5062.6, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.05, "peak": 15.32, "min": 14.81}, "VDD_CPU_SOC_MSS": {"avg": 16.24, "peak": 16.54, "min": 15.75}, "VDD_GPU": {"avg": 25.02, "peak": 29.54, "min": 21.67}, "VIN": {"avg": 70.56, "peak": 104.72, "min": 62.83}}, "power_watts_avg": 25.02, "energy_joules_est": 26.74, "sample_count": 8, "duration_seconds": 1.069}, "timestamp": "2026-01-17T14:53:41.244716"}
{"image_index": 155, "image_name": "000000015746.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015746.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1515.079, "latencies_ms": [1515.079], "images_per_second": 0.66, "prompt_tokens": 9, "response_tokens_est": 34, "n_tiles": 6, "output_text": "The image shows a vibrant red fire hydrant situated in a lush green grassy area, with a backdrop of a residential building and a variety of trees and flowers.", "error": null, "sys_before": {"cpu_percent": 44.3, "ram_used_mb": 24950.6, "ram_available_mb": 100821.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 24951.5, "ram_available_mb": 100820.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.81, "peak": 15.24, "min": 14.1}, "VDD_CPU_SOC_MSS": {"avg": 15.82, "peak": 16.53, "min": 14.96}, "VDD_GPU": {"avg": 32.59, "peak": 40.18, "min": 25.6}, "VIN": {"avg": 77.36, "peak": 118.08, "min": 62.66}}, "power_watts_avg": 32.59, "energy_joules_est": 49.39, "sample_count": 11, "duration_seconds": 1.515}, "timestamp": "2026-01-17T14:53:42.816928"}
{"image_index": 155, "image_name": "000000015746.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015746.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1772.791, "latencies_ms": [1772.791], "images_per_second": 0.564, "prompt_tokens": 23, "response_tokens_est": 40, "n_tiles": 6, "output_text": "object: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24951.5, "ram_available_mb": 100820.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 24951.8, "ram_available_mb": 100820.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.83, "peak": 15.24, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.63, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 32.45, "peak": 41.36, "min": 24.42}, "VIN": {"avg": 75.3, "peak": 112.88, "min": 59.84}}, "power_watts_avg": 32.45, "energy_joules_est": 57.54, "sample_count": 13, "duration_seconds": 1.773}, "timestamp": "2026-01-17T14:53:44.595818"}
{"image_index": 155, "image_name": "000000015746.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015746.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2761.712, "latencies_ms": [2761.712], "images_per_second": 0.362, "prompt_tokens": 27, "response_tokens_est": 75, "n_tiles": 6, "output_text": "The main object in the foreground is a red fire hydrant, which is situated on the grass. The hydrant is positioned near the center of the image, slightly to the left. In the background, there is a house with a white exterior and a window with purple flowers. The house is slightly out of focus, indicating that the focus is on the hydrant.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24951.8, "ram_available_mb": 100820.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 24951.5, "ram_available_mb": 100820.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.89, "peak": 15.24, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.64, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 28.74, "peak": 40.56, "min": 22.86}, "VIN": {"avg": 69.54, "peak": 117.82, "min": 55.71}}, "power_watts_avg": 28.74, "energy_joules_est": 79.38, "sample_count": 21, "duration_seconds": 2.762}, "timestamp": "2026-01-17T14:53:47.363695"}
{"image_index": 155, "image_name": "000000015746.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015746.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3017.685, "latencies_ms": [3017.685], "images_per_second": 0.331, "prompt_tokens": 21, "response_tokens_est": 84, "n_tiles": 6, "output_text": "The image depicts a vibrant red fire hydrant situated in a lush, green grassy area with a backdrop of a residential building and trees. The hydrant appears weathered, with some rust and wear, and is surrounded by various plants and flowers, including dandelions and small purple flowers. The scene suggests a peaceful, suburban setting with a focus on the hydrant as a central object amidst nature.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24951.5, "ram_available_mb": 100820.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 24951.8, "ram_available_mb": 100820.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.9, "peak": 15.24, "min": 14.3}, "VDD_CPU_SOC_MSS": {"avg": 15.32, "peak": 15.74, "min": 14.56}, "VDD_GPU": {"avg": 27.94, "peak": 40.16, "min": 22.86}, "VIN": {"avg": 70.7, "peak": 111.6, "min": 61.44}}, "power_watts_avg": 27.94, "energy_joules_est": 84.33, "sample_count": 24, "duration_seconds": 3.018}, "timestamp": "2026-01-17T14:53:50.387534"}
{"image_index": 155, "image_name": "000000015746.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015746.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2165.162, "latencies_ms": [2165.162], "images_per_second": 0.462, "prompt_tokens": 19, "response_tokens_est": 54, "n_tiles": 6, "output_text": "The notable visual attributes of the image include a vivid red fire hydrant with a black cap and a black hose attachment, set against a lush green grassy background. The lighting is natural, suggesting it is daytime, and the weather appears to be clear and sunny.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 24951.8, "ram_available_mb": 100820.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 24951.8, "ram_available_mb": 100820.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.84, "peak": 15.24, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.38, "peak": 15.75, "min": 14.56}, "VDD_GPU": {"avg": 30.38, "peak": 40.57, "min": 23.25}, "VIN": {"avg": 69.8, "peak": 97.59, "min": 60.94}}, "power_watts_avg": 30.38, "energy_joules_est": 65.8, "sample_count": 16, "duration_seconds": 2.166}, "timestamp": "2026-01-17T14:53:52.558767"}
{"image_index": 156, "image_name": "000000015751.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015751.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1443.751, "latencies_ms": [1443.751], "images_per_second": 0.693, "prompt_tokens": 9, "response_tokens_est": 31, "n_tiles": 6, "output_text": "The image depicts a close-up view of a blue wooden surface with a few birds perched on it, including a bird in mid-flight.", "error": null, "sys_before": {"cpu_percent": 35.6, "ram_used_mb": 24951.8, "ram_available_mb": 100820.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 24952.5, "ram_available_mb": 100819.7, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.66, "peak": 15.14, "min": 13.69}, "VDD_CPU_SOC_MSS": {"avg": 15.42, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 32.8, "peak": 40.18, "min": 25.61}, "VIN": {"avg": 75.69, "peak": 110.96, "min": 53.71}}, "power_watts_avg": 32.8, "energy_joules_est": 47.37, "sample_count": 11, "duration_seconds": 1.444}, "timestamp": "2026-01-17T14:53:54.064189"}
{"image_index": 156, "image_name": "000000015751.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015751.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1947.048, "latencies_ms": [1947.048], "images_per_second": 0.514, "prompt_tokens": 23, "response_tokens_est": 48, "n_tiles": 6, "output_text": "birds: 3\nbirds: 2\nbirds: 1\nbirds: 1\nbirds: 1\nbirds: 1\nbirds: 1\nbirds: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24952.5, "ram_available_mb": 100819.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 24953.0, "ram_available_mb": 100819.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.03, "peak": 15.54, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.63, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 31.63, "peak": 40.56, "min": 24.42}, "VIN": {"avg": 71.42, "peak": 103.2, "min": 60.46}}, "power_watts_avg": 31.63, "energy_joules_est": 61.6, "sample_count": 14, "duration_seconds": 1.947}, "timestamp": "2026-01-17T14:53:56.017783"}
{"image_index": 156, "image_name": "000000015751.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015751.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2270.938, "latencies_ms": [2270.938], "images_per_second": 0.44, "prompt_tokens": 27, "response_tokens_est": 60, "n_tiles": 6, "output_text": "The main objects in the image are a pair of birds and a piece of wood. The birds are positioned near the bottom left corner of the image, while the wood is in the background. The birds are in close proximity to the wood, suggesting they may be perched or resting on it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24953.0, "ram_available_mb": 100819.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 24953.3, "ram_available_mb": 100818.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.06, "peak": 15.44, "min": 14.43}, "VDD_CPU_SOC_MSS": {"avg": 15.63, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 30.52, "peak": 40.95, "min": 23.64}, "VIN": {"avg": 71.7, "peak": 108.23, "min": 48.82}}, "power_watts_avg": 30.52, "energy_joules_est": 69.32, "sample_count": 17, "duration_seconds": 2.271}, "timestamp": "2026-01-17T14:53:58.294716"}
{"image_index": 156, "image_name": "000000015751.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015751.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2659.628, "latencies_ms": [2659.628], "images_per_second": 0.376, "prompt_tokens": 21, "response_tokens_est": 73, "n_tiles": 6, "output_text": "The image depicts a close-up view of a weathered wooden surface with a blue paint finish. Birds are perched on the surface, with one bird in the foreground appearing to be in the process of eating or catching something. The setting suggests an outdoor or rustic environment, possibly a park or a backyard, with the birds engaging in typical avian behavior.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24953.3, "ram_available_mb": 100818.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 24953.0, "ram_available_mb": 100819.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.06, "peak": 15.54, "min": 14.43}, "VDD_CPU_SOC_MSS": {"avg": 15.67, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 29.17, "peak": 40.18, "min": 23.24}, "VIN": {"avg": 69.44, "peak": 108.08, "min": 55.84}}, "power_watts_avg": 29.17, "energy_joules_est": 77.59, "sample_count": 20, "duration_seconds": 2.66}, "timestamp": "2026-01-17T14:54:00.964569"}
{"image_index": 156, "image_name": "000000015751.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015751.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1770.496, "latencies_ms": [1770.496], "images_per_second": 0.565, "prompt_tokens": 19, "response_tokens_est": 40, "n_tiles": 6, "output_text": "The image features a weathered, teal-colored wooden surface with visible signs of wear and age. The lighting is soft and diffused, casting gentle shadows and highlighting the texture of the wood.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24953.0, "ram_available_mb": 100819.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 24953.0, "ram_available_mb": 100819.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.81, "peak": 15.24, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.35, "peak": 15.75, "min": 14.56}, "VDD_GPU": {"avg": 31.79, "peak": 40.18, "min": 24.42}, "VIN": {"avg": 67.93, "peak": 77.67, "min": 63.32}}, "power_watts_avg": 31.79, "energy_joules_est": 56.3, "sample_count": 13, "duration_seconds": 1.771}, "timestamp": "2026-01-17T14:54:02.741191"}
{"image_index": 157, "image_name": "000000015956.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015956.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2172.88, "latencies_ms": [2172.88], "images_per_second": 0.46, "prompt_tokens": 9, "response_tokens_est": 17, "n_tiles": 12, "output_text": "A woman is walking through a rustic barn with a brown horse in the background.", "error": null, "sys_before": {"cpu_percent": 45.9, "ram_used_mb": 24920.3, "ram_available_mb": 100851.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.1, "ram_used_mb": 24921.8, "ram_available_mb": 100850.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.0, "peak": 16.46, "min": 13.39}, "VDD_CPU_SOC_MSS": {"avg": 15.72, "peak": 17.32, "min": 14.18}, "VDD_GPU": {"avg": 37.69, "peak": 43.73, "min": 29.94}, "VIN": {"avg": 81.78, "peak": 114.4, "min": 65.71}}, "power_watts_avg": 37.69, "energy_joules_est": 81.91, "sample_count": 16, "duration_seconds": 2.173}, "timestamp": "2026-01-17T14:54:05.016022"}
{"image_index": 157, "image_name": "000000015956.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015956.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2824.785, "latencies_ms": [2824.785], "images_per_second": 0.354, "prompt_tokens": 23, "response_tokens_est": 34, "n_tiles": 12, "output_text": "1. Horse\n2. Barn\n3. Horses\n4. Woman\n5. Horse\n6. Horses\n7. Horse\n8. Horse", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24921.8, "ram_available_mb": 100850.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 24921.8, "ram_available_mb": 100850.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.99, "peak": 16.26, "min": 14.02}, "VDD_CPU_SOC_MSS": {"avg": 15.6, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 36.86, "peak": 45.3, "min": 26.79}, "VIN": {"avg": 79.36, "peak": 119.6, "min": 61.27}}, "power_watts_avg": 36.86, "energy_joules_est": 104.14, "sample_count": 21, "duration_seconds": 2.825}, "timestamp": "2026-01-17T14:54:07.847271"}
{"image_index": 157, "image_name": "000000015956.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015956.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3616.759, "latencies_ms": [3616.759], "images_per_second": 0.276, "prompt_tokens": 27, "response_tokens_est": 57, "n_tiles": 12, "output_text": "The main object in the foreground is a woman wearing blue jeans and white shoes, standing near the dirt ground. The background features a horse in a stable with a wooden wall and a red door. The stable has various items like a bucket, a ladder, and a wooden bench.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24921.8, "ram_available_mb": 100850.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 24922.0, "ram_available_mb": 100850.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.94, "peak": 16.36, "min": 14.02}, "VDD_CPU_SOC_MSS": {"avg": 15.45, "peak": 17.32, "min": 14.17}, "VDD_GPU": {"avg": 33.9, "peak": 45.3, "min": 25.6}, "VIN": {"avg": 76.06, "peak": 118.53, "min": 59.93}}, "power_watts_avg": 33.9, "energy_joules_est": 122.62, "sample_count": 28, "duration_seconds": 3.617}, "timestamp": "2026-01-17T14:54:11.471796"}
{"image_index": 157, "image_name": "000000015956.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015956.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3166.912, "latencies_ms": [3166.912], "images_per_second": 0.316, "prompt_tokens": 21, "response_tokens_est": 44, "n_tiles": 12, "output_text": "The image depicts a rustic, rural setting with a brown horse in a stable. A woman is walking through the stable, and the environment is filled with various tools and equipment, indicating a working farm or ranch.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24922.0, "ram_available_mb": 100850.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 24923.3, "ram_available_mb": 100848.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.93, "peak": 16.26, "min": 13.92}, "VDD_CPU_SOC_MSS": {"avg": 15.37, "peak": 17.32, "min": 14.17}, "VDD_GPU": {"avg": 35.12, "peak": 46.09, "min": 26.01}, "VIN": {"avg": 76.61, "peak": 124.39, "min": 63.92}}, "power_watts_avg": 35.12, "energy_joules_est": 111.23, "sample_count": 25, "duration_seconds": 3.167}, "timestamp": "2026-01-17T14:54:14.645049"}
{"image_index": 157, "image_name": "000000015956.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015956.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2882.807, "latencies_ms": [2882.807], "images_per_second": 0.347, "prompt_tokens": 19, "response_tokens_est": 36, "n_tiles": 12, "output_text": "The image depicts a rustic barn with a dirt floor and wooden walls. The lighting is natural, with sunlight streaming in through a window, creating a warm and inviting atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24923.3, "ram_available_mb": 100848.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 24923.3, "ram_available_mb": 100848.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.99, "peak": 16.36, "min": 14.02}, "VDD_CPU_SOC_MSS": {"avg": 15.55, "peak": 17.32, "min": 14.17}, "VDD_GPU": {"avg": 36.13, "peak": 44.91, "min": 26.4}, "VIN": {"avg": 74.66, "peak": 91.17, "min": 62.64}}, "power_watts_avg": 36.13, "energy_joules_est": 104.17, "sample_count": 22, "duration_seconds": 2.883}, "timestamp": "2026-01-17T14:54:17.534390"}
{"image_index": 158, "image_name": "000000016010.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016010.jpg", "image_width": 640, "image_height": 471, "image_resolution": "640x471", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 3001.863, "latencies_ms": [3001.863], "images_per_second": 0.333, "prompt_tokens": 9, "response_tokens_est": 41, "n_tiles": 12, "output_text": "The image depicts a serene and lush green meadow with a variety of wildlife, including a zebra grazing and a herd of elephants and rhinos peacefully coexisting in the background.", "error": null, "sys_before": {"cpu_percent": 40.3, "ram_used_mb": 24911.3, "ram_available_mb": 100860.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 24912.0, "ram_available_mb": 100860.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.11, "peak": 16.66, "min": 13.19}, "VDD_CPU_SOC_MSS": {"avg": 15.85, "peak": 17.71, "min": 14.17}, "VDD_GPU": {"avg": 34.83, "peak": 44.12, "min": 26.39}, "VIN": {"avg": 79.66, "peak": 114.8, "min": 62.29}}, "power_watts_avg": 34.83, "energy_joules_est": 104.57, "sample_count": 23, "duration_seconds": 3.002}, "timestamp": "2026-01-17T14:54:20.640951"}
{"image_index": 158, "image_name": "000000016010.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016010.jpg", "image_width": 640, "image_height": 471, "image_resolution": "640x471", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 3814.201, "latencies_ms": [3814.201], "images_per_second": 0.262, "prompt_tokens": 23, "response_tokens_est": 63, "n_tiles": 12, "output_text": "1. Rhino: 1\n2. Elephant: 1\n3. Zebra: 1\n4. Buffalo: 1\n5. Deer: 1\n6. Antelope: 1\n7. Giraffe: 1\n8. Lion: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24912.0, "ram_available_mb": 100860.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 24912.7, "ram_available_mb": 100859.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.2, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.77, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 33.71, "peak": 45.3, "min": 26.0}, "VIN": {"avg": 79.96, "peak": 124.26, "min": 64.0}}, "power_watts_avg": 33.71, "energy_joules_est": 128.59, "sample_count": 29, "duration_seconds": 3.815}, "timestamp": "2026-01-17T14:54:24.462945"}
{"image_index": 158, "image_name": "000000016010.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016010.jpg", "image_width": 640, "image_height": 471, "image_resolution": "640x471", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 4025.354, "latencies_ms": [4025.354], "images_per_second": 0.248, "prompt_tokens": 27, "response_tokens_est": 69, "n_tiles": 12, "output_text": "The main objects in the image are a herd of zebras and a group of rhinos. The zebras are in the foreground, grazing on the green grass, while the rhinos are in the background, standing on the hill. The trees and rocks are in the midground, providing a natural backdrop to the scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24912.7, "ram_available_mb": 100859.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 24913.4, "ram_available_mb": 100858.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.18, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.72, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 33.14, "peak": 44.89, "min": 26.0}, "VIN": {"avg": 77.21, "peak": 134.28, "min": 55.77}}, "power_watts_avg": 33.14, "energy_joules_est": 133.41, "sample_count": 31, "duration_seconds": 4.026}, "timestamp": "2026-01-17T14:54:28.494908"}
{"image_index": 158, "image_name": "000000016010.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016010.jpg", "image_width": 640, "image_height": 471, "image_resolution": "640x471", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3575.557, "latencies_ms": [3575.557], "images_per_second": 0.28, "prompt_tokens": 21, "response_tokens_est": 56, "n_tiles": 12, "output_text": "The image depicts a serene and lush green meadow with a variety of wildlife, including a herd of zebras grazing peacefully. The meadow is bordered by a stone wall and is surrounded by tall trees, creating a tranquil and natural environment.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24913.4, "ram_available_mb": 100858.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 24913.7, "ram_available_mb": 100858.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.19, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.7, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 33.91, "peak": 45.68, "min": 26.0}, "VIN": {"avg": 74.54, "peak": 109.01, "min": 60.69}}, "power_watts_avg": 33.91, "energy_joules_est": 121.26, "sample_count": 28, "duration_seconds": 3.576}, "timestamp": "2026-01-17T14:54:32.080975"}
{"image_index": 158, "image_name": "000000016010.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016010.jpg", "image_width": 640, "image_height": 471, "image_resolution": "640x471", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2864.434, "latencies_ms": [2864.434], "images_per_second": 0.349, "prompt_tokens": 19, "response_tokens_est": 35, "n_tiles": 12, "output_text": "The image depicts a lush, green meadow with a variety of trees in the background. The sky is partly cloudy, providing a mix of sunlight and shade.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24913.7, "ram_available_mb": 100858.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 24913.9, "ram_available_mb": 100858.3, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.22, "peak": 16.66, "min": 14.02}, "VDD_CPU_SOC_MSS": {"avg": 15.84, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 35.99, "peak": 44.91, "min": 26.8}, "VIN": {"avg": 75.47, "peak": 106.81, "min": 62.41}}, "power_watts_avg": 35.99, "energy_joules_est": 103.1, "sample_count": 22, "duration_seconds": 2.865}, "timestamp": "2026-01-17T14:54:34.951686"}
{"image_index": 159, "image_name": "000000016228.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016228.jpg", "image_width": 640, "image_height": 440, "image_resolution": "640x440", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1395.967, "latencies_ms": [1395.967], "images_per_second": 0.716, "prompt_tokens": 9, "response_tokens_est": 29, "n_tiles": 6, "output_text": "A white horse is being pulled by a carriage with the word \"Disneyland\" on it, as people walk by in the background.", "error": null, "sys_before": {"cpu_percent": 39.1, "ram_used_mb": 24937.5, "ram_available_mb": 100834.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 24938.5, "ram_available_mb": 100833.7, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.62, "peak": 15.14, "min": 13.72}, "VDD_CPU_SOC_MSS": {"avg": 15.35, "peak": 16.14, "min": 14.17}, "VDD_GPU": {"avg": 33.79, "peak": 40.95, "min": 26.39}, "VIN": {"avg": 78.51, "peak": 113.72, "min": 65.53}}, "power_watts_avg": 33.79, "energy_joules_est": 47.18, "sample_count": 10, "duration_seconds": 1.396}, "timestamp": "2026-01-17T14:54:36.416395"}
{"image_index": 159, "image_name": "000000016228.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016228.jpg", "image_width": 640, "image_height": 440, "image_resolution": "640x440", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1733.096, "latencies_ms": [1733.096], "images_per_second": 0.577, "prompt_tokens": 23, "response_tokens_est": 40, "n_tiles": 6, "output_text": "object: 1\nobject: 2\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24938.5, "ram_available_mb": 100833.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 24939.0, "ram_available_mb": 100833.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.0, "peak": 15.44, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.59, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 32.51, "peak": 40.56, "min": 24.82}, "VIN": {"avg": 71.76, "peak": 107.76, "min": 59.48}}, "power_watts_avg": 32.51, "energy_joules_est": 56.35, "sample_count": 13, "duration_seconds": 1.733}, "timestamp": "2026-01-17T14:54:38.155763"}
{"image_index": 159, "image_name": "000000016228.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016228.jpg", "image_width": 640, "image_height": 440, "image_resolution": "640x440", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3095.374, "latencies_ms": [3095.374], "images_per_second": 0.323, "prompt_tokens": 27, "response_tokens_est": 90, "n_tiles": 6, "output_text": "The main object in the foreground is a white horse being pulled by a carriage. The carriage is decorated with green and gold colors and has the word \"Disneyland\" on it. In the background, there is a crowd of people and a trolley. The trolley is moving along the street, and there is a yellow and white umbrella nearby. The scene is set in an outdoor area with trees and a park-like setting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24939.0, "ram_available_mb": 100833.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 24940.2, "ram_available_mb": 100831.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.1, "peak": 15.44, "min": 14.43}, "VDD_CPU_SOC_MSS": {"avg": 15.68, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 28.4, "peak": 40.95, "min": 23.25}, "VIN": {"avg": 70.8, "peak": 111.28, "min": 55.76}}, "power_watts_avg": 28.4, "energy_joules_est": 87.92, "sample_count": 24, "duration_seconds": 3.096}, "timestamp": "2026-01-17T14:54:41.257543"}
{"image_index": 159, "image_name": "000000016228.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016228.jpg", "image_width": 640, "image_height": 440, "image_resolution": "640x440", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2847.127, "latencies_ms": [2847.127], "images_per_second": 0.351, "prompt_tokens": 21, "response_tokens_est": 81, "n_tiles": 6, "output_text": "The image depicts a scene from a parade or public event, likely in a park or open area, with a white horse being pulled by a carriage. The carriage is adorned with the Disneyland logo and the number 1, and it is being led by a person in a suit. The setting is outdoors, with trees and a crowd of people in the background, suggesting a festive atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24940.2, "ram_available_mb": 100831.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 24940.0, "ram_available_mb": 100832.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.1, "peak": 15.54, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.68, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 28.62, "peak": 40.18, "min": 23.25}, "VIN": {"avg": 68.89, "peak": 86.79, "min": 60.45}}, "power_watts_avg": 28.62, "energy_joules_est": 81.49, "sample_count": 22, "duration_seconds": 2.847}, "timestamp": "2026-01-17T14:54:44.110954"}
{"image_index": 159, "image_name": "000000016228.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016228.jpg", "image_width": 640, "image_height": 440, "image_resolution": "640x440", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2355.388, "latencies_ms": [2355.388], "images_per_second": 0.425, "prompt_tokens": 19, "response_tokens_est": 63, "n_tiles": 6, "output_text": "The image depicts a vintage scene with a horse-drawn carriage, a white horse, and a crowd of people. The carriage is adorned with green and gold colors, and the horse is wearing a harness. The scene is well-lit by natural sunlight, creating a bright and cheerful atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24940.0, "ram_available_mb": 100832.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 24941.2, "ram_available_mb": 100831.0, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.05, "peak": 15.44, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.64, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 29.76, "peak": 40.16, "min": 23.64}, "VIN": {"avg": 72.63, "peak": 103.87, "min": 60.33}}, "power_watts_avg": 29.76, "energy_joules_est": 70.11, "sample_count": 18, "duration_seconds": 2.356}, "timestamp": "2026-01-17T14:54:46.472753"}
{"image_index": 160, "image_name": "000000016249.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016249.jpg", "image_width": 500, "image_height": 365, "image_resolution": "500x365", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2526.888, "latencies_ms": [2526.888], "images_per_second": 0.396, "prompt_tokens": 9, "response_tokens_est": 27, "n_tiles": 12, "output_text": "The image depicts a group of elderly men sitting on benches in a public square, each engrossed in reading newspapers.", "error": null, "sys_before": {"cpu_percent": 45.0, "ram_used_mb": 24911.4, "ram_available_mb": 100860.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.6, "ram_used_mb": 24913.4, "ram_available_mb": 100858.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.19, "peak": 16.76, "min": 13.59}, "VDD_CPU_SOC_MSS": {"avg": 15.96, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 35.98, "peak": 44.12, "min": 27.57}, "VIN": {"avg": 75.95, "peak": 104.51, "min": 53.13}}, "power_watts_avg": 35.98, "energy_joules_est": 90.93, "sample_count": 20, "duration_seconds": 2.527}, "timestamp": "2026-01-17T14:54:49.093816"}
{"image_index": 160, "image_name": "000000016249.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016249.jpg", "image_width": 500, "image_height": 365, "image_resolution": "500x365", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 3571.28, "latencies_ms": [3571.28], "images_per_second": 0.28, "prompt_tokens": 23, "response_tokens_est": 56, "n_tiles": 12, "output_text": "1. Man: 2\n2. Man: 2\n3. Man: 2\n4. Man: 2\n5. Man: 2\n6. Man: 2\n7. Man: 2\n8. Man: 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24913.4, "ram_available_mb": 100858.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 24913.8, "ram_available_mb": 100858.3, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.19, "peak": 16.56, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.68, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 34.06, "peak": 45.68, "min": 26.01}, "VIN": {"avg": 76.81, "peak": 116.33, "min": 62.1}}, "power_watts_avg": 34.06, "energy_joules_est": 121.66, "sample_count": 28, "duration_seconds": 3.572}, "timestamp": "2026-01-17T14:54:52.674802"}
{"image_index": 160, "image_name": "000000016249.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016249.jpg", "image_width": 500, "image_height": 365, "image_resolution": "500x365", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3542.937, "latencies_ms": [3542.937], "images_per_second": 0.282, "prompt_tokens": 27, "response_tokens_est": 55, "n_tiles": 12, "output_text": "The main objects in the image are a group of elderly men sitting on benches. The benches are positioned in the foreground, with the men seated on them. The background features a building with a large sign, and a few other people are visible in the distance.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24913.8, "ram_available_mb": 100858.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 24915.3, "ram_available_mb": 100856.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.2, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.66, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 34.2, "peak": 44.89, "min": 26.01}, "VIN": {"avg": 74.88, "peak": 116.66, "min": 59.31}}, "power_watts_avg": 34.2, "energy_joules_est": 121.18, "sample_count": 27, "duration_seconds": 3.543}, "timestamp": "2026-01-17T14:54:56.224850"}
{"image_index": 160, "image_name": "000000016249.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016249.jpg", "image_width": 500, "image_height": 365, "image_resolution": "500x365", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3814.863, "latencies_ms": [3814.863], "images_per_second": 0.262, "prompt_tokens": 21, "response_tokens_est": 63, "n_tiles": 12, "output_text": "The image depicts an outdoor scene with several elderly individuals seated on benches in a public space. The setting appears to be a city square or park, with a mix of modern and traditional architectural elements. The individuals are engaged in various activities, such as reading newspapers, conversing, and enjoying the outdoors.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 24915.3, "ram_available_mb": 100856.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 24916.8, "ram_available_mb": 100855.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.18, "peak": 16.66, "min": 14.12}, "VDD_CPU_SOC_MSS": {"avg": 15.65, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 33.4, "peak": 44.89, "min": 26.0}, "VIN": {"avg": 75.02, "peak": 94.57, "min": 65.29}}, "power_watts_avg": 33.4, "energy_joules_est": 127.43, "sample_count": 30, "duration_seconds": 3.815}, "timestamp": "2026-01-17T14:55:00.047049"}
{"image_index": 160, "image_name": "000000016249.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016249.jpg", "image_width": 500, "image_height": 365, "image_resolution": "500x365", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3831.762, "latencies_ms": [3831.762], "images_per_second": 0.261, "prompt_tokens": 19, "response_tokens_est": 63, "n_tiles": 12, "output_text": "The image depicts a sunny day with clear blue skies and bright sunlight. The scene is characterized by the vibrant colors of the buildings, the green metal benches, and the red and white signage. The lighting is natural, casting shadows on the ground, and the weather appears to be pleasant and warm.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24916.8, "ram_available_mb": 100855.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 24916.8, "ram_available_mb": 100855.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.9, "peak": 16.36, "min": 14.12}, "VDD_CPU_SOC_MSS": {"avg": 15.41, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 33.41, "peak": 45.69, "min": 25.6}, "VIN": {"avg": 74.16, "peak": 121.01, "min": 60.22}}, "power_watts_avg": 33.41, "energy_joules_est": 128.04, "sample_count": 30, "duration_seconds": 3.832}, "timestamp": "2026-01-17T14:55:03.884939"}
{"image_index": 161, "image_name": "000000016439.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016439.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2830.072, "latencies_ms": [2830.072], "images_per_second": 0.353, "prompt_tokens": 9, "response_tokens_est": 36, "n_tiles": 12, "output_text": "The image shows a well-organized desk with a laptop, a glass of orange juice, a phone, and a lamp, all set against a warmly lit background.", "error": null, "sys_before": {"cpu_percent": 39.7, "ram_used_mb": 24917.9, "ram_available_mb": 100854.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 24919.7, "ram_available_mb": 100852.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.16, "peak": 16.76, "min": 13.29}, "VDD_CPU_SOC_MSS": {"avg": 15.87, "peak": 17.71, "min": 14.18}, "VDD_GPU": {"avg": 35.26, "peak": 44.12, "min": 26.79}, "VIN": {"avg": 75.99, "peak": 109.86, "min": 59.54}}, "power_watts_avg": 35.26, "energy_joules_est": 99.8, "sample_count": 22, "duration_seconds": 2.83}, "timestamp": "2026-01-17T14:55:06.811694"}
{"image_index": 161, "image_name": "000000016439.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016439.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 3330.62, "latencies_ms": [3330.62], "images_per_second": 0.3, "prompt_tokens": 23, "response_tokens_est": 49, "n_tiles": 12, "output_text": "- Laptop: 1\n- Phone: 1\n- Glass: 1\n- Lamp: 1\n- Desk: 1\n- Magazine: 1\n- Book: 1\n- Candle: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24919.7, "ram_available_mb": 100852.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 24914.2, "ram_available_mb": 100857.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.22, "peak": 16.56, "min": 14.12}, "VDD_CPU_SOC_MSS": {"avg": 15.75, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 34.82, "peak": 45.3, "min": 26.39}, "VIN": {"avg": 77.98, "peak": 126.93, "min": 64.02}}, "power_watts_avg": 34.82, "energy_joules_est": 115.98, "sample_count": 26, "duration_seconds": 3.331}, "timestamp": "2026-01-17T14:55:10.149954"}
{"image_index": 161, "image_name": "000000016439.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016439.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 4190.394, "latencies_ms": [4190.394], "images_per_second": 0.239, "prompt_tokens": 27, "response_tokens_est": 74, "n_tiles": 12, "output_text": "The main objects in the image are a desk, a laptop, a lamp, and a cup of orange juice. The desk is positioned in the foreground, with the laptop and the lamp placed on top of it. The cup of orange juice is placed on the desk, near the lamp. The lamp is positioned in the background, providing light to the desk area.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24914.2, "ram_available_mb": 100857.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 24915.7, "ram_available_mb": 100856.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.18, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.78, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 32.89, "peak": 45.69, "min": 26.0}, "VIN": {"avg": 72.52, "peak": 110.46, "min": 55.82}}, "power_watts_avg": 32.89, "energy_joules_est": 137.83, "sample_count": 33, "duration_seconds": 4.191}, "timestamp": "2026-01-17T14:55:14.347010"}
{"image_index": 161, "image_name": "000000016439.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016439.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 4144.582, "latencies_ms": [4144.582], "images_per_second": 0.241, "prompt_tokens": 21, "response_tokens_est": 73, "n_tiles": 12, "output_text": "The image depicts a well-organized workspace with a desk featuring a laptop, a phone, and a glass of orange juice. The desk is set against a warmly lit wall, and there is a decorative lamp with a classic design on the desk. The overall scene suggests a calm and productive environment, possibly in a home office or a study room.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24915.7, "ram_available_mb": 100856.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 24917.4, "ram_available_mb": 100854.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.18, "peak": 16.56, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.66, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 32.83, "peak": 45.28, "min": 26.0}, "VIN": {"avg": 76.0, "peak": 111.71, "min": 55.91}}, "power_watts_avg": 32.83, "energy_joules_est": 136.08, "sample_count": 33, "duration_seconds": 4.145}, "timestamp": "2026-01-17T14:55:18.498614"}
{"image_index": 161, "image_name": "000000016439.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016439.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3841.735, "latencies_ms": [3841.735], "images_per_second": 0.26, "prompt_tokens": 19, "response_tokens_est": 64, "n_tiles": 12, "output_text": "The image depicts a well-lit, cozy office desk with a warm yellowish light illuminating the scene. The desk features a black laptop, a glass of orange juice, a phone, and various papers and books. The desk is made of dark wood, and the lighting creates a cozy and inviting atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24917.4, "ram_available_mb": 100854.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 24917.2, "ram_available_mb": 100855.0, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.18, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.84, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 33.42, "peak": 45.28, "min": 26.0}, "VIN": {"avg": 75.78, "peak": 106.62, "min": 54.82}}, "power_watts_avg": 33.42, "energy_joules_est": 128.41, "sample_count": 30, "duration_seconds": 3.842}, "timestamp": "2026-01-17T14:55:22.346956"}
{"image_index": 162, "image_name": "000000016451.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016451.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1038.695, "latencies_ms": [1038.695], "images_per_second": 0.963, "prompt_tokens": 9, "response_tokens_est": 44, "n_tiles": 1, "output_text": "The image depicts a beach scene with various beach equipment and accessories spread out on the sand, including a striped beach towel, a blue surfboard, a red bag, and a green and white beach umbrella.", "error": null, "sys_before": {"cpu_percent": 21.9, "ram_used_mb": 24917.2, "ram_available_mb": 100855.0, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24917.4, "ram_available_mb": 100854.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 4845.7, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.18, "peak": 15.62, "min": 14.4}, "VDD_CPU_SOC_MSS": {"avg": 16.0, "peak": 16.54, "min": 14.96}, "VDD_GPU": {"avg": 23.05, "peak": 26.79, "min": 20.89}, "VIN": {"avg": 62.67, "peak": 69.03, "min": 55.13}}, "power_watts_avg": 23.05, "energy_joules_est": 23.95, "sample_count": 8, "duration_seconds": 1.039}, "timestamp": "2026-01-17T14:55:23.414620"}
{"image_index": 162, "image_name": "000000016451.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016451.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2839.122, "latencies_ms": [2839.122], "images_per_second": 0.352, "prompt_tokens": 23, "response_tokens_est": 128, "n_tiles": 1, "output_text": "object: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject:", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24917.4, "ram_available_mb": 100854.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24917.4, "ram_available_mb": 100854.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 4856.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.6, "peak": 15.72, "min": 15.11}, "VDD_CPU_SOC_MSS": {"avg": 16.64, "peak": 16.93, "min": 16.14}, "VDD_GPU": {"avg": 20.76, "peak": 24.42, "min": 20.09}, "VIN": {"avg": 63.96, "peak": 71.42, "min": 61.01}}, "power_watts_avg": 20.76, "energy_joules_est": 58.95, "sample_count": 22, "duration_seconds": 2.84}, "timestamp": "2026-01-17T14:55:26.259660"}
{"image_index": 162, "image_name": "000000016451.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016451.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1880.572, "latencies_ms": [1880.572], "images_per_second": 0.532, "prompt_tokens": 27, "response_tokens_est": 84, "n_tiles": 1, "output_text": "The main objects in the image are located in the foreground, with a variety of beach equipment and accessories spread out on the sand. The beach umbrella, surfboards, and towels are positioned near the center of the image, while the ocean and distant beachgoers are in the background. The foreground objects are closer to the viewer, while the background elements are further away, creating a sense of depth in the scene.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 24917.4, "ram_available_mb": 100854.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24918.9, "ram_available_mb": 100853.3, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 4859.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.61, "peak": 15.82, "min": 15.32}, "VDD_CPU_SOC_MSS": {"avg": 16.71, "peak": 16.93, "min": 16.14}, "VDD_GPU": {"avg": 21.05, "peak": 24.03, "min": 20.09}, "VIN": {"avg": 64.8, "peak": 68.0, "min": 61.39}}, "power_watts_avg": 21.05, "energy_joules_est": 39.59, "sample_count": 14, "duration_seconds": 1.881}, "timestamp": "2026-01-17T14:55:28.145948"}
{"image_index": 162, "image_name": "000000016451.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016451.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1686.459, "latencies_ms": [1686.459], "images_per_second": 0.593, "prompt_tokens": 21, "response_tokens_est": 75, "n_tiles": 1, "output_text": "The image depicts a serene beach scene with a clear blue sky overhead. The sandy beach is dotted with various beach gear, including a striped beach towel, a blue surfboard, a red bag, and a green and white beach umbrella. In the background, the ocean is calm with gentle waves, and a few people can be seen enjoying the beach.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24918.9, "ram_available_mb": 100853.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24919.4, "ram_available_mb": 100852.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 4854.7, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.56, "peak": 15.72, "min": 15.32}, "VDD_CPU_SOC_MSS": {"avg": 16.69, "peak": 16.93, "min": 16.14}, "VDD_GPU": {"avg": 21.1, "peak": 24.03, "min": 20.1}, "VIN": {"avg": 63.12, "peak": 67.39, "min": 56.86}}, "power_watts_avg": 21.1, "energy_joules_est": 35.59, "sample_count": 13, "duration_seconds": 1.687}, "timestamp": "2026-01-17T14:55:29.838176"}
{"image_index": 162, "image_name": "000000016451.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016451.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1749.728, "latencies_ms": [1749.728], "images_per_second": 0.572, "prompt_tokens": 19, "response_tokens_est": 78, "n_tiles": 1, "output_text": "The image depicts a beach scene with a clear blue sky, calm ocean waves, and a sandy shore. The sand is light-colored, and the beach is adorned with various beach gear, including a striped towel, a blue surfboard, a red bag, and a green and white umbrella. The lighting is bright and sunny, casting a warm glow over the scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24919.4, "ram_available_mb": 100852.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24920.1, "ram_available_mb": 100852.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 4853.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.57, "peak": 15.72, "min": 15.11}, "VDD_CPU_SOC_MSS": {"avg": 16.57, "peak": 16.93, "min": 16.54}, "VDD_GPU": {"avg": 21.09, "peak": 24.02, "min": 20.1}, "VIN": {"avg": 62.59, "peak": 65.08, "min": 56.98}}, "power_watts_avg": 21.09, "energy_joules_est": 36.91, "sample_count": 13, "duration_seconds": 1.75}, "timestamp": "2026-01-17T14:55:31.593604"}
{"image_index": 163, "image_name": "000000016502.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016502.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2185.793, "latencies_ms": [2185.793], "images_per_second": 0.457, "prompt_tokens": 9, "response_tokens_est": 17, "n_tiles": 12, "output_text": "A sheep stands on a rocky outcrop with a blue sky in the background.", "error": null, "sys_before": {"cpu_percent": 51.2, "ram_used_mb": 24919.5, "ram_available_mb": 100852.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.2, "ram_used_mb": 24920.1, "ram_available_mb": 100852.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.3, "peak": 16.76, "min": 13.9}, "VDD_CPU_SOC_MSS": {"avg": 16.16, "peak": 17.71, "min": 14.96}, "VDD_GPU": {"avg": 36.77, "peak": 43.33, "min": 27.59}, "VIN": {"avg": 79.64, "peak": 114.16, "min": 59.07}}, "power_watts_avg": 36.77, "energy_joules_est": 80.38, "sample_count": 17, "duration_seconds": 2.186}, "timestamp": "2026-01-17T14:55:33.867823"}
{"image_index": 163, "image_name": "000000016502.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016502.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 3027.688, "latencies_ms": [3027.688], "images_per_second": 0.33, "prompt_tokens": 23, "response_tokens_est": 40, "n_tiles": 12, "output_text": "object: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24920.1, "ram_available_mb": 100852.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 24915.3, "ram_available_mb": 100856.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.24, "peak": 16.56, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.85, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 36.01, "peak": 45.68, "min": 26.79}, "VIN": {"avg": 75.52, "peak": 109.7, "min": 60.55}}, "power_watts_avg": 36.01, "energy_joules_est": 109.04, "sample_count": 23, "duration_seconds": 3.028}, "timestamp": "2026-01-17T14:55:36.902086"}
{"image_index": 163, "image_name": "000000016502.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016502.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3429.775, "latencies_ms": [3429.775], "images_per_second": 0.292, "prompt_tokens": 27, "response_tokens_est": 52, "n_tiles": 12, "output_text": "The main objects in the image are a sheep and a rocky landscape. The sheep is positioned in the foreground, standing on a small mound of grass. The rocky landscape is in the background, with large, jagged rocks and a clear blue sky.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24915.3, "ram_available_mb": 100856.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 24916.4, "ram_available_mb": 100855.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.23, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.71, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 34.66, "peak": 44.89, "min": 26.0}, "VIN": {"avg": 75.88, "peak": 110.3, "min": 60.54}}, "power_watts_avg": 34.66, "energy_joules_est": 118.89, "sample_count": 26, "duration_seconds": 3.43}, "timestamp": "2026-01-17T14:55:40.342479"}
{"image_index": 163, "image_name": "000000016502.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016502.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3356.743, "latencies_ms": [3356.743], "images_per_second": 0.298, "prompt_tokens": 21, "response_tokens_est": 50, "n_tiles": 12, "output_text": "The image depicts a serene landscape featuring a sheep standing on a rocky outcrop under a clear blue sky. The sheep appears to be grazing on the grassy area, surrounded by large, rugged rocks and a lush green meadow.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24916.4, "ram_available_mb": 100855.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 24916.1, "ram_available_mb": 100856.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.34, "peak": 16.66, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.84, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 34.4, "peak": 44.89, "min": 26.0}, "VIN": {"avg": 75.35, "peak": 106.39, "min": 64.9}}, "power_watts_avg": 34.4, "energy_joules_est": 115.49, "sample_count": 24, "duration_seconds": 3.357}, "timestamp": "2026-01-17T14:55:43.707264"}
{"image_index": 163, "image_name": "000000016502.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016502.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3568.85, "latencies_ms": [3568.85], "images_per_second": 0.28, "prompt_tokens": 19, "response_tokens_est": 56, "n_tiles": 12, "output_text": "The image features a serene landscape with a clear blue sky adorned with fluffy white clouds. The ground is covered in lush green grass, and the rocky terrain is rugged and textured. The lighting is bright and natural, suggesting a sunny day with minimal cloud cover.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24916.1, "ram_available_mb": 100856.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 24917.3, "ram_available_mb": 100854.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.21, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.73, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 34.13, "peak": 44.89, "min": 26.0}, "VIN": {"avg": 75.92, "peak": 102.38, "min": 66.65}}, "power_watts_avg": 34.13, "energy_joules_est": 121.82, "sample_count": 27, "duration_seconds": 3.569}, "timestamp": "2026-01-17T14:55:47.282828"}
{"image_index": 164, "image_name": "000000016598.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016598.jpg", "image_width": 478, "image_height": 640, "image_resolution": "478x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2394.456, "latencies_ms": [2394.456], "images_per_second": 0.418, "prompt_tokens": 9, "response_tokens_est": 23, "n_tiles": 12, "output_text": "The image shows a person with blue hair and a blue shirt, holding a smartphone and taking a selfie.", "error": null, "sys_before": {"cpu_percent": 45.3, "ram_used_mb": 24914.4, "ram_available_mb": 100857.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.5, "ram_used_mb": 24915.6, "ram_available_mb": 100856.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.12, "peak": 16.66, "min": 13.19}, "VDD_CPU_SOC_MSS": {"avg": 15.88, "peak": 17.71, "min": 14.18}, "VDD_GPU": {"avg": 36.61, "peak": 43.73, "min": 27.96}, "VIN": {"avg": 80.34, "peak": 116.8, "min": 66.4}}, "power_watts_avg": 36.61, "energy_joules_est": 87.67, "sample_count": 18, "duration_seconds": 2.395}, "timestamp": "2026-01-17T14:55:49.798567"}
{"image_index": 164, "image_name": "000000016598.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016598.jpg", "image_width": 478, "image_height": 640, "image_resolution": "478x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2827.816, "latencies_ms": [2827.816], "images_per_second": 0.354, "prompt_tokens": 23, "response_tokens_est": 34, "n_tiles": 12, "output_text": "1. Woman\n2. Blue hair\n3. Blue shirt\n4. Phone\n5. Ring\n6. Background\n7. Wall\n8. Light", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24915.6, "ram_available_mb": 100856.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 24916.8, "ram_available_mb": 100855.3, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.23, "peak": 16.56, "min": 14.12}, "VDD_CPU_SOC_MSS": {"avg": 15.78, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 36.28, "peak": 46.07, "min": 26.79}, "VIN": {"avg": 77.11, "peak": 111.31, "min": 67.32}}, "power_watts_avg": 36.28, "energy_joules_est": 102.61, "sample_count": 22, "duration_seconds": 2.828}, "timestamp": "2026-01-17T14:55:52.632594"}
{"image_index": 164, "image_name": "000000016598.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016598.jpg", "image_width": 478, "image_height": 640, "image_resolution": "478x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3575.932, "latencies_ms": [3575.932], "images_per_second": 0.28, "prompt_tokens": 27, "response_tokens_est": 56, "n_tiles": 12, "output_text": "The main object in the foreground is a person holding a smartphone. The person's face is in the center of the frame, and the smartphone is held up to their face. The background is out of focus, with a light-colored wall and a small object on the wall.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24916.8, "ram_available_mb": 100855.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 24918.1, "ram_available_mb": 100854.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.22, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.85, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 34.16, "peak": 45.68, "min": 26.0}, "VIN": {"avg": 76.31, "peak": 113.17, "min": 61.76}}, "power_watts_avg": 34.16, "energy_joules_est": 122.17, "sample_count": 28, "duration_seconds": 3.576}, "timestamp": "2026-01-17T14:55:56.215110"}
{"image_index": 164, "image_name": "000000016598.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016598.jpg", "image_width": 478, "image_height": 640, "image_resolution": "478x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2836.604, "latencies_ms": [2836.604], "images_per_second": 0.353, "prompt_tokens": 21, "response_tokens_est": 35, "n_tiles": 12, "output_text": "The image depicts a person with blue hair and a blue shirt standing in front of a light-colored wall. The person is holding a smartphone and taking a selfie.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24918.1, "ram_available_mb": 100854.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 24917.8, "ram_available_mb": 100854.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.3, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.91, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 36.11, "peak": 44.89, "min": 27.18}, "VIN": {"avg": 79.87, "peak": 119.24, "min": 62.13}}, "power_watts_avg": 36.11, "energy_joules_est": 102.45, "sample_count": 22, "duration_seconds": 2.837}, "timestamp": "2026-01-17T14:55:59.059846"}
{"image_index": 164, "image_name": "000000016598.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016598.jpg", "image_width": 478, "image_height": 640, "image_resolution": "478x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3741.365, "latencies_ms": [3741.365], "images_per_second": 0.267, "prompt_tokens": 19, "response_tokens_est": 61, "n_tiles": 12, "output_text": "The notable visual attributes of the image include the striking blue hair of the person, which stands out against the neutral background. The lighting is soft and diffused, creating a gentle ambiance. The person is wearing a blue shirt, and the image has a watermark indicating copyright by Janie Henderson.", "error": null, "sys_before": {"cpu_percent": 16.7, "ram_used_mb": 24917.8, "ram_available_mb": 100854.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 24919.3, "ram_available_mb": 100852.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.04, "peak": 16.36, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.62, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 33.88, "peak": 44.89, "min": 25.6}, "VIN": {"avg": 74.53, "peak": 116.97, "min": 61.89}}, "power_watts_avg": 33.88, "energy_joules_est": 126.77, "sample_count": 29, "duration_seconds": 3.742}, "timestamp": "2026-01-17T14:56:02.807938"}
{"image_index": 165, "image_name": "000000016958.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016958.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2111.991, "latencies_ms": [2111.991], "images_per_second": 0.473, "prompt_tokens": 9, "response_tokens_est": 56, "n_tiles": 6, "output_text": "The image depicts a classic, well-appointed room with a large, ornate fireplace, a wooden cabinet, a wooden table with a book, and several antique chairs, all set against a backdrop of a framed painting and a wall adorned with various decorative items.", "error": null, "sys_before": {"cpu_percent": 45.2, "ram_used_mb": 24934.5, "ram_available_mb": 100837.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 24936.7, "ram_available_mb": 100835.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.84, "peak": 15.22, "min": 13.72}, "VDD_CPU_SOC_MSS": {"avg": 15.45, "peak": 16.14, "min": 14.17}, "VDD_GPU": {"avg": 30.53, "peak": 40.56, "min": 24.03}, "VIN": {"avg": 77.31, "peak": 121.26, "min": 60.95}}, "power_watts_avg": 30.53, "energy_joules_est": 64.49, "sample_count": 16, "duration_seconds": 2.112}, "timestamp": "2026-01-17T14:56:04.973979"}
{"image_index": 165, "image_name": "000000016958.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016958.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1729.288, "latencies_ms": [1729.288], "images_per_second": 0.578, "prompt_tokens": 23, "response_tokens_est": 40, "n_tiles": 6, "output_text": "object: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24936.7, "ram_available_mb": 100835.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 24938.2, "ram_available_mb": 100834.0, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.02, "peak": 15.54, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.69, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 32.24, "peak": 40.16, "min": 24.82}, "VIN": {"avg": 75.47, "peak": 108.49, "min": 63.93}}, "power_watts_avg": 32.24, "energy_joules_est": 55.76, "sample_count": 13, "duration_seconds": 1.73}, "timestamp": "2026-01-17T14:56:06.709676"}
{"image_index": 165, "image_name": "000000016958.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016958.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2902.058, "latencies_ms": [2902.058], "images_per_second": 0.345, "prompt_tokens": 27, "response_tokens_est": 80, "n_tiles": 6, "output_text": "The main objects in the image are a fireplace, a wooden cabinet, a chair, and a small round table. The fireplace is positioned in the foreground, with a chair and a small round table placed near it. The wooden cabinet is located to the right of the fireplace, and the chair is positioned to the right of the cabinet. The small round table is placed in front of the cabinet.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24938.2, "ram_available_mb": 100834.0, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 24938.7, "ram_available_mb": 100833.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.93, "peak": 15.34, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.68, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 28.54, "peak": 40.95, "min": 22.86}, "VIN": {"avg": 71.42, "peak": 119.61, "min": 56.38}}, "power_watts_avg": 28.54, "energy_joules_est": 82.84, "sample_count": 22, "duration_seconds": 2.902}, "timestamp": "2026-01-17T14:56:09.618154"}
{"image_index": 165, "image_name": "000000016958.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016958.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3013.707, "latencies_ms": [3013.707], "images_per_second": 0.332, "prompt_tokens": 21, "response_tokens_est": 84, "n_tiles": 6, "output_text": "The image depicts a classic, well-appointed room with a traditional fireplace, a wooden cabinet, and a patterned carpet. The room appears to be a study or a living room with antique furniture, including a leather armchair and a wooden table with a book on it. The setting exudes a sense of history and elegance, with the room's decor and furnishings suggesting a bygone era.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 24938.7, "ram_available_mb": 100833.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 24938.9, "ram_available_mb": 100833.3, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.9, "peak": 15.24, "min": 14.2}, "VDD_CPU_SOC_MSS": {"avg": 15.57, "peak": 16.14, "min": 14.57}, "VDD_GPU": {"avg": 27.9, "peak": 40.18, "min": 22.85}, "VIN": {"avg": 70.74, "peak": 114.87, "min": 59.86}}, "power_watts_avg": 27.9, "energy_joules_est": 84.1, "sample_count": 24, "duration_seconds": 3.014}, "timestamp": "2026-01-17T14:56:12.639291"}
{"image_index": 165, "image_name": "000000016958.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016958.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2502.253, "latencies_ms": [2502.253], "images_per_second": 0.4, "prompt_tokens": 19, "response_tokens_est": 66, "n_tiles": 6, "output_text": "The room features a classic, well-kept fireplace with a brick surround and a black mantel. The walls are painted white, and there is a wooden cabinet with glass doors. The room is well-lit, with soft, ambient lighting that enhances the warm tones of the furniture and the rich colors of the carpet.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 24938.9, "ram_available_mb": 100833.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 24939.6, "ram_available_mb": 100832.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.86, "peak": 15.24, "min": 14.3}, "VDD_CPU_SOC_MSS": {"avg": 15.6, "peak": 16.14, "min": 14.57}, "VDD_GPU": {"avg": 29.13, "peak": 40.16, "min": 23.25}, "VIN": {"avg": 67.49, "peak": 85.67, "min": 55.48}}, "power_watts_avg": 29.13, "energy_joules_est": 72.91, "sample_count": 19, "duration_seconds": 2.503}, "timestamp": "2026-01-17T14:56:15.147460"}
{"image_index": 166, "image_name": "000000017029.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017029.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1388.657, "latencies_ms": [1388.657], "images_per_second": 0.72, "prompt_tokens": 9, "response_tokens_est": 41, "n_tiles": 4, "output_text": "A brown dog is energetically jumping in the air, holding a bright red Frisbee in its mouth, while two black cars are parked in the background on a well-maintained lawn.", "error": null, "sys_before": {"cpu_percent": 36.7, "ram_used_mb": 24939.6, "ram_available_mb": 100832.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 24939.6, "ram_available_mb": 100832.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4606.7, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5444.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.64, "peak": 15.22, "min": 13.9}, "VDD_CPU_SOC_MSS": {"avg": 15.43, "peak": 16.14, "min": 14.57}, "VDD_GPU": {"avg": 29.19, "peak": 36.24, "min": 24.04}, "VIN": {"avg": 71.29, "peak": 104.84, "min": 58.77}}, "power_watts_avg": 29.19, "energy_joules_est": 40.54, "sample_count": 10, "duration_seconds": 1.389}, "timestamp": "2026-01-17T14:56:16.584987"}
{"image_index": 166, "image_name": "000000017029.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017029.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1346.438, "latencies_ms": [1346.438], "images_per_second": 0.743, "prompt_tokens": 23, "response_tokens_est": 39, "n_tiles": 4, "output_text": "dog: 1\nfrisbee: 1\ncar: 1\ntree: 1\ngrass: 1\nbushes: 1\nhouse: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24939.6, "ram_available_mb": 100832.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 24939.9, "ram_available_mb": 100832.3, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4606.7, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5455.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.9, "peak": 15.22, "min": 14.61}, "VDD_CPU_SOC_MSS": {"avg": 15.71, "peak": 16.14, "min": 15.36}, "VDD_GPU": {"avg": 29.82, "peak": 37.42, "min": 24.03}, "VIN": {"avg": 71.28, "peak": 103.46, "min": 60.18}}, "power_watts_avg": 29.82, "energy_joules_est": 40.16, "sample_count": 10, "duration_seconds": 1.347}, "timestamp": "2026-01-17T14:56:17.937490"}
{"image_index": 166, "image_name": "000000017029.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017029.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3564.36, "latencies_ms": [3564.36], "images_per_second": 0.281, "prompt_tokens": 27, "response_tokens_est": 128, "n_tiles": 4, "output_text": "In the image, the main object is a dog in the foreground, positioned near the center of the frame. The dog is facing to the right, with its body slightly turned towards the camera. In the background, there is a black car parked on the left side of the image, slightly behind the dog. The car is also facing to the right, and it is positioned near the edge of the frame. The dog and the car are both in the foreground, with the dog being closer to the camera than the car. The background features a well-maintained lawn and a hedge, which are further away from the main subjects.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24939.9, "ram_available_mb": 100832.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 24941.1, "ram_available_mb": 100831.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4606.7, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5458.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.16, "peak": 15.32, "min": 14.61}, "VDD_CPU_SOC_MSS": {"avg": 15.97, "peak": 16.14, "min": 15.35}, "VDD_GPU": {"avg": 25.19, "peak": 37.8, "min": 22.06}, "VIN": {"avg": 66.25, "peak": 109.81, "min": 53.65}}, "power_watts_avg": 25.19, "energy_joules_est": 89.8, "sample_count": 28, "duration_seconds": 3.565}, "timestamp": "2026-01-17T14:56:21.508089"}
{"image_index": 166, "image_name": "000000017029.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017029.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1836.786, "latencies_ms": [1836.786], "images_per_second": 0.544, "prompt_tokens": 21, "response_tokens_est": 59, "n_tiles": 4, "output_text": "The image captures a lively scene in a well-maintained backyard, featuring a brown dog mid-leap, seemingly in the midst of a playful game of fetch with a red frisbee. The lush green lawn and the presence of a black car in the background suggest a suburban setting.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 24941.1, "ram_available_mb": 100831.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 24941.6, "ram_available_mb": 100830.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4606.7, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5453.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.0, "peak": 15.32, "min": 14.61}, "VDD_CPU_SOC_MSS": {"avg": 15.83, "peak": 16.14, "min": 15.35}, "VDD_GPU": {"avg": 27.8, "peak": 37.41, "min": 22.85}, "VIN": {"avg": 66.98, "peak": 104.16, "min": 54.3}}, "power_watts_avg": 27.8, "energy_joules_est": 51.07, "sample_count": 14, "duration_seconds": 1.837}, "timestamp": "2026-01-17T14:56:23.350850"}
{"image_index": 166, "image_name": "000000017029.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017029.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1913.548, "latencies_ms": [1913.548], "images_per_second": 0.523, "prompt_tokens": 19, "response_tokens_est": 62, "n_tiles": 4, "output_text": "The image features a black car parked on a well-maintained lawn, with a dog in mid-air, leaping towards a red frisbee. The scene is bathed in natural daylight, highlighting the vibrant green grass and the contrasting colors of the dog's fur and the frisbee.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24941.6, "ram_available_mb": 100830.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 24941.6, "ram_available_mb": 100830.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4606.7, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5451.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.04, "peak": 15.32, "min": 14.61}, "VDD_CPU_SOC_MSS": {"avg": 15.8, "peak": 16.14, "min": 15.35}, "VDD_GPU": {"avg": 27.42, "peak": 37.03, "min": 22.46}, "VIN": {"avg": 67.2, "peak": 108.77, "min": 48.08}}, "power_watts_avg": 27.42, "energy_joules_est": 52.48, "sample_count": 15, "duration_seconds": 1.914}, "timestamp": "2026-01-17T14:56:25.270417"}
{"image_index": 167, "image_name": "000000017031.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017031.jpg", "image_width": 500, "image_height": 334, "image_resolution": "500x334", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1601.504, "latencies_ms": [1601.504], "images_per_second": 0.624, "prompt_tokens": 9, "response_tokens_est": 37, "n_tiles": 6, "output_text": "The image features a close-up of a giraffe's face, with its distinctive brown and white spotted pattern, and its large, expressive eyes gazing directly at the viewer.", "error": null, "sys_before": {"cpu_percent": 21.2, "ram_used_mb": 24941.6, "ram_available_mb": 100830.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 24941.6, "ram_available_mb": 100830.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.89, "peak": 15.34, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.58, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 32.37, "peak": 40.57, "min": 25.22}, "VIN": {"avg": 76.36, "peak": 118.59, "min": 61.46}}, "power_watts_avg": 32.37, "energy_joules_est": 51.86, "sample_count": 12, "duration_seconds": 1.602}, "timestamp": "2026-01-17T14:56:26.900060"}
{"image_index": 167, "image_name": "000000017031.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017031.jpg", "image_width": 500, "image_height": 334, "image_resolution": "500x334", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 971.011, "latencies_ms": [971.011], "images_per_second": 1.03, "prompt_tokens": 23, "response_tokens_est": 12, "n_tiles": 6, "output_text": "giraffe: 1\ntree: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24941.6, "ram_available_mb": 100830.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 24942.6, "ram_available_mb": 100829.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.91, "peak": 15.44, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.46, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 37.08, "peak": 40.57, "min": 32.7}, "VIN": {"avg": 80.88, "peak": 113.51, "min": 64.58}}, "power_watts_avg": 37.08, "energy_joules_est": 36.02, "sample_count": 7, "duration_seconds": 0.971}, "timestamp": "2026-01-17T14:56:27.877272"}
{"image_index": 167, "image_name": "000000017031.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017031.jpg", "image_width": 500, "image_height": 334, "image_resolution": "500x334", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2219.573, "latencies_ms": [2219.573], "images_per_second": 0.451, "prompt_tokens": 27, "response_tokens_est": 58, "n_tiles": 6, "output_text": "The main object in the foreground is a giraffe, which is positioned near the center of the image. The background features blurred greenery, indicating that the giraffe is situated in a natural environment. The blurred background suggests that the giraffe is further away from the viewer.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24942.6, "ram_available_mb": 100829.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 24942.6, "ram_available_mb": 100829.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.03, "peak": 15.34, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.65, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 31.81, "peak": 42.53, "min": 24.03}, "VIN": {"avg": 72.13, "peak": 123.64, "min": 64.49}}, "power_watts_avg": 31.81, "energy_joules_est": 70.62, "sample_count": 17, "duration_seconds": 2.22}, "timestamp": "2026-01-17T14:56:30.107699"}
{"image_index": 167, "image_name": "000000017031.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017031.jpg", "image_width": 500, "image_height": 334, "image_resolution": "500x334", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2463.351, "latencies_ms": [2463.351], "images_per_second": 0.406, "prompt_tokens": 21, "response_tokens_est": 67, "n_tiles": 6, "output_text": "The image captures a giraffe standing in a lush, green environment, likely a savanna or a similar habitat. The giraffe is facing the camera, with its head slightly tilted, and its long neck and legs are prominently visible. The background is filled with various shades of green, indicating a dense, natural setting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24942.6, "ram_available_mb": 100829.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 24942.6, "ram_available_mb": 100829.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.04, "peak": 15.44, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.64, "peak": 16.14, "min": 14.57}, "VDD_GPU": {"avg": 29.61, "peak": 40.57, "min": 23.64}, "VIN": {"avg": 72.29, "peak": 111.52, "min": 61.36}}, "power_watts_avg": 29.61, "energy_joules_est": 72.95, "sample_count": 19, "duration_seconds": 2.464}, "timestamp": "2026-01-17T14:56:32.577476"}
{"image_index": 167, "image_name": "000000017031.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017031.jpg", "image_width": 500, "image_height": 334, "image_resolution": "500x334", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1455.485, "latencies_ms": [1455.485], "images_per_second": 0.687, "prompt_tokens": 19, "response_tokens_est": 30, "n_tiles": 6, "output_text": "The giraffe in the image has a brown and white coat with distinctive black spots. The lighting is bright and natural, suggesting it is daytime.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24942.6, "ram_available_mb": 100829.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 24942.8, "ram_available_mb": 100829.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.98, "peak": 15.44, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.57, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 33.59, "peak": 40.18, "min": 26.01}, "VIN": {"avg": 74.31, "peak": 96.0, "min": 61.65}}, "power_watts_avg": 33.59, "energy_joules_est": 48.9, "sample_count": 11, "duration_seconds": 1.456}, "timestamp": "2026-01-17T14:56:34.039675"}
{"image_index": 168, "image_name": "000000017115.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017115.jpg", "image_width": 443, "image_height": 640, "image_resolution": "443x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1194.431, "latencies_ms": [1194.431], "images_per_second": 0.837, "prompt_tokens": 9, "response_tokens_est": 22, "n_tiles": 6, "output_text": "Two zebras stand in a fenced enclosure, with their distinctive black and white stripes clearly visible.", "error": null, "sys_before": {"cpu_percent": 36.1, "ram_used_mb": 24942.8, "ram_available_mb": 100829.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 24943.5, "ram_available_mb": 100828.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.71, "peak": 15.24, "min": 14.02}, "VDD_CPU_SOC_MSS": {"avg": 15.35, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 35.02, "peak": 40.97, "min": 27.98}, "VIN": {"avg": 75.97, "peak": 102.88, "min": 61.14}}, "power_watts_avg": 35.02, "energy_joules_est": 41.84, "sample_count": 9, "duration_seconds": 1.195}, "timestamp": "2026-01-17T14:56:35.282783"}
{"image_index": 168, "image_name": "000000017115.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017115.jpg", "image_width": 443, "image_height": 640, "image_resolution": "443x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1237.147, "latencies_ms": [1237.147], "images_per_second": 0.808, "prompt_tokens": 23, "response_tokens_est": 22, "n_tiles": 6, "output_text": "zebra: 2\nfence: 1\nrock: 1\nground: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24943.5, "ram_available_mb": 100828.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 24944.3, "ram_available_mb": 100827.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.95, "peak": 15.44, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.53, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 36.32, "peak": 41.34, "min": 28.76}, "VIN": {"avg": 81.0, "peak": 116.79, "min": 60.54}}, "power_watts_avg": 36.32, "energy_joules_est": 44.94, "sample_count": 9, "duration_seconds": 1.237}, "timestamp": "2026-01-17T14:56:36.527033"}
{"image_index": 168, "image_name": "000000017115.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017115.jpg", "image_width": 443, "image_height": 640, "image_resolution": "443x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2277.487, "latencies_ms": [2277.487], "images_per_second": 0.439, "prompt_tokens": 27, "response_tokens_est": 58, "n_tiles": 6, "output_text": "The main objects in the image are two zebras standing in a zoo enclosure. The foreground features the backs of the zebras, with the left one slightly closer to the camera. The background consists of a chain-link fence and a rocky ground, indicating the enclosure's location.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24944.3, "ram_available_mb": 100827.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 24944.3, "ram_available_mb": 100827.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.92, "peak": 15.34, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.33, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 30.66, "peak": 41.74, "min": 23.25}, "VIN": {"avg": 68.8, "peak": 93.96, "min": 57.21}}, "power_watts_avg": 30.66, "energy_joules_est": 69.84, "sample_count": 17, "duration_seconds": 2.278}, "timestamp": "2026-01-17T14:56:38.810745"}
{"image_index": 168, "image_name": "000000017115.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017115.jpg", "image_width": 443, "image_height": 640, "image_resolution": "443x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2702.448, "latencies_ms": [2702.448], "images_per_second": 0.37, "prompt_tokens": 21, "response_tokens_est": 73, "n_tiles": 6, "output_text": "The image depicts a scene in a zoo or a wildlife sanctuary, where two zebras are standing in a fenced area. The zebras are facing the camera, and their distinctive black and white stripes are clearly visible. The ground is covered with dirt and scattered rocks, and the fence encloses the area, separating it from the natural environment.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24944.3, "ram_available_mb": 100827.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 24944.7, "ram_available_mb": 100827.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.88, "peak": 15.24, "min": 14.3}, "VDD_CPU_SOC_MSS": {"avg": 15.49, "peak": 15.75, "min": 14.56}, "VDD_GPU": {"avg": 28.72, "peak": 40.97, "min": 22.85}, "VIN": {"avg": 69.55, "peak": 108.9, "min": 53.66}}, "power_watts_avg": 28.72, "energy_joules_est": 77.64, "sample_count": 21, "duration_seconds": 2.703}, "timestamp": "2026-01-17T14:56:41.519701"}
{"image_index": 168, "image_name": "000000017115.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017115.jpg", "image_width": 443, "image_height": 640, "image_resolution": "443x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2193.735, "latencies_ms": [2193.735], "images_per_second": 0.456, "prompt_tokens": 19, "response_tokens_est": 55, "n_tiles": 6, "output_text": "The zebras in the image have black and white stripes, which are a notable visual attribute. The lighting is bright and natural, suggesting it is daytime. The ground is covered with dry leaves and patches of grass, indicating a dry season or a lack of rainfall.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24944.7, "ram_available_mb": 100827.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 24945.0, "ram_available_mb": 100827.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.83, "peak": 15.24, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.47, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 29.91, "peak": 40.16, "min": 23.25}, "VIN": {"avg": 69.42, "peak": 116.84, "min": 59.31}}, "power_watts_avg": 29.91, "energy_joules_est": 65.63, "sample_count": 17, "duration_seconds": 2.194}, "timestamp": "2026-01-17T14:56:43.719895"}
{"image_index": 169, "image_name": "000000017178.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017178.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1038.403, "latencies_ms": [1038.403], "images_per_second": 0.963, "prompt_tokens": 9, "response_tokens_est": 16, "n_tiles": 6, "output_text": "A group of horses stands in a field, with a car parked nearby.", "error": null, "sys_before": {"cpu_percent": 32.1, "ram_used_mb": 24945.0, "ram_available_mb": 100827.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 24944.7, "ram_available_mb": 100827.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.55, "peak": 15.04, "min": 13.9}, "VDD_CPU_SOC_MSS": {"avg": 15.2, "peak": 15.75, "min": 14.56}, "VDD_GPU": {"avg": 35.26, "peak": 40.18, "min": 29.94}, "VIN": {"avg": 81.97, "peak": 112.19, "min": 55.8}}, "power_watts_avg": 35.26, "energy_joules_est": 36.64, "sample_count": 8, "duration_seconds": 1.039}, "timestamp": "2026-01-17T14:56:44.808317"}
{"image_index": 169, "image_name": "000000017178.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017178.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1541.005, "latencies_ms": [1541.005], "images_per_second": 0.649, "prompt_tokens": 23, "response_tokens_est": 32, "n_tiles": 6, "output_text": "1. Horse\n2. Horse\n3. Horse\n4. Horse\n5. Horse\n6. Horse\n7. Horse\n8. Car", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 24944.7, "ram_available_mb": 100827.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 24945.2, "ram_available_mb": 100826.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.79, "peak": 15.24, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.35, "peak": 15.75, "min": 14.56}, "VDD_GPU": {"avg": 34.14, "peak": 42.13, "min": 25.21}, "VIN": {"avg": 73.92, "peak": 112.36, "min": 57.77}}, "power_watts_avg": 34.14, "energy_joules_est": 52.63, "sample_count": 12, "duration_seconds": 1.541}, "timestamp": "2026-01-17T14:56:46.356026"}
{"image_index": 169, "image_name": "000000017178.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017178.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2080.016, "latencies_ms": [2080.016], "images_per_second": 0.481, "prompt_tokens": 27, "response_tokens_est": 51, "n_tiles": 6, "output_text": "The main objects in the image are a wooden fence, a road, and a car. The wooden fence is on the left side of the image, the road is in the foreground, and the car is on the right side, near the fence.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24945.2, "ram_available_mb": 100826.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 24946.5, "ram_available_mb": 100825.7, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.87, "peak": 15.34, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.35, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 31.12, "peak": 40.95, "min": 23.64}, "VIN": {"avg": 70.57, "peak": 94.63, "min": 53.18}}, "power_watts_avg": 31.12, "energy_joules_est": 64.74, "sample_count": 15, "duration_seconds": 2.08}, "timestamp": "2026-01-17T14:56:48.446568"}
{"image_index": 169, "image_name": "000000017178.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017178.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2449.384, "latencies_ms": [2449.384], "images_per_second": 0.408, "prompt_tokens": 21, "response_tokens_est": 64, "n_tiles": 6, "output_text": "The image depicts a serene rural scene with a dirt road flanked by a wooden fence on one side and lush green trees on the other. A dark-colored car is parked on the right side of the road, and a group of horses is standing in the middle of the road, appearing calm and peaceful.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24946.5, "ram_available_mb": 100825.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 24946.5, "ram_available_mb": 100825.7, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.83, "peak": 15.14, "min": 14.2}, "VDD_CPU_SOC_MSS": {"avg": 15.48, "peak": 15.75, "min": 14.56}, "VDD_GPU": {"avg": 29.52, "peak": 40.57, "min": 23.24}, "VIN": {"avg": 69.81, "peak": 102.14, "min": 61.22}}, "power_watts_avg": 29.52, "energy_joules_est": 72.33, "sample_count": 18, "duration_seconds": 2.45}, "timestamp": "2026-01-17T14:56:50.903136"}
{"image_index": 169, "image_name": "000000017178.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017178.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1941.072, "latencies_ms": [1941.072], "images_per_second": 0.515, "prompt_tokens": 19, "response_tokens_est": 46, "n_tiles": 6, "output_text": "The image features a serene scene with a wooden fence lining the road, a lush green tree providing shade, and a parked car in the foreground. The lighting is soft and natural, suggesting a sunny day with clear skies.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24946.5, "ram_available_mb": 100825.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 24946.2, "ram_available_mb": 100826.0, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.83, "peak": 15.24, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.33, "peak": 15.75, "min": 14.56}, "VDD_GPU": {"avg": 30.96, "peak": 40.56, "min": 23.64}, "VIN": {"avg": 72.68, "peak": 114.88, "min": 61.45}}, "power_watts_avg": 30.96, "energy_joules_est": 60.11, "sample_count": 15, "duration_seconds": 1.941}, "timestamp": "2026-01-17T14:56:52.850441"}
{"image_index": 170, "image_name": "000000017182.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017182.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1783.902, "latencies_ms": [1783.902], "images_per_second": 0.561, "prompt_tokens": 9, "response_tokens_est": 42, "n_tiles": 6, "output_text": "The image shows a wooden desk with a stack of hardcover books on top, a small green bottle, and a red apple on the desk, with a framed picture hanging on the wall to the right.", "error": null, "sys_before": {"cpu_percent": 45.7, "ram_used_mb": 24946.2, "ram_available_mb": 100826.0, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 24946.2, "ram_available_mb": 100826.0, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.66, "peak": 15.01, "min": 13.79}, "VDD_CPU_SOC_MSS": {"avg": 15.36, "peak": 15.75, "min": 14.57}, "VDD_GPU": {"avg": 31.06, "peak": 39.77, "min": 24.04}, "VIN": {"avg": 71.1, "peak": 109.27, "min": 62.53}}, "power_watts_avg": 31.06, "energy_joules_est": 55.42, "sample_count": 14, "duration_seconds": 1.784}, "timestamp": "2026-01-17T14:56:54.685958"}
{"image_index": 170, "image_name": "000000017182.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017182.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1399.575, "latencies_ms": [1399.575], "images_per_second": 0.715, "prompt_tokens": 23, "response_tokens_est": 27, "n_tiles": 6, "output_text": "bookcase: 1\nbook: 1\napple: 1\nvase: 1\nchair: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24946.2, "ram_available_mb": 100826.0, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 24946.4, "ram_available_mb": 100825.7, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.82, "peak": 15.24, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.28, "peak": 15.74, "min": 14.56}, "VDD_GPU": {"avg": 33.63, "peak": 41.34, "min": 26.0}, "VIN": {"avg": 72.22, "peak": 90.32, "min": 62.94}}, "power_watts_avg": 33.63, "energy_joules_est": 47.09, "sample_count": 11, "duration_seconds": 1.4}, "timestamp": "2026-01-17T14:56:56.092146"}
{"image_index": 170, "image_name": "000000017182.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017182.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2738.976, "latencies_ms": [2738.976], "images_per_second": 0.365, "prompt_tokens": 27, "response_tokens_est": 74, "n_tiles": 6, "output_text": "The main objects in the image are a wooden desk and a chair. The desk is positioned in the foreground, with a stack of books placed on top of it. The chair is located to the right of the desk, partially visible in the image. The background features a chalkboard, a framed picture on the wall, and a doorway leading to another room.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24946.4, "ram_available_mb": 100825.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 24946.7, "ram_available_mb": 100825.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.86, "peak": 15.24, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.49, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 28.91, "peak": 40.95, "min": 22.86}, "VIN": {"avg": 70.46, "peak": 114.13, "min": 59.23}}, "power_watts_avg": 28.91, "energy_joules_est": 79.19, "sample_count": 21, "duration_seconds": 2.739}, "timestamp": "2026-01-17T14:56:58.837672"}
{"image_index": 170, "image_name": "000000017182.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017182.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2536.208, "latencies_ms": [2536.208], "images_per_second": 0.394, "prompt_tokens": 21, "response_tokens_est": 67, "n_tiles": 6, "output_text": "The image depicts a cozy, vintage library setting. The room is filled with wooden bookshelves, a wooden desk, and a chair, creating a warm and inviting atmosphere. The books on the desk and shelves suggest a rich collection of literature, while the overall ambiance conveys a sense of tranquility and intellectual pursuit.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24946.7, "ram_available_mb": 100825.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 24946.7, "ram_available_mb": 100825.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.85, "peak": 15.24, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.31, "peak": 15.74, "min": 14.56}, "VDD_GPU": {"avg": 28.94, "peak": 40.56, "min": 22.86}, "VIN": {"avg": 69.66, "peak": 110.68, "min": 62.05}}, "power_watts_avg": 28.94, "energy_joules_est": 73.41, "sample_count": 20, "duration_seconds": 2.537}, "timestamp": "2026-01-17T14:57:01.380048"}
{"image_index": 170, "image_name": "000000017182.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017182.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2164.461, "latencies_ms": [2164.461], "images_per_second": 0.462, "prompt_tokens": 19, "response_tokens_est": 54, "n_tiles": 6, "output_text": "The image depicts a room with a dark, textured wall and a wooden desk. The desk is made of dark wood and has a few books and a small green bottle on top. The lighting in the room is soft and warm, creating a cozy atmosphere.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 24946.7, "ram_available_mb": 100825.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 24947.7, "ram_available_mb": 100824.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.84, "peak": 15.24, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.43, "peak": 15.75, "min": 14.56}, "VDD_GPU": {"avg": 30.28, "peak": 40.16, "min": 23.24}, "VIN": {"avg": 75.19, "peak": 117.62, "min": 63.01}}, "power_watts_avg": 30.28, "energy_joules_est": 65.55, "sample_count": 16, "duration_seconds": 2.165}, "timestamp": "2026-01-17T14:57:03.550046"}
{"image_index": 171, "image_name": "000000017207.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017207.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2760.998, "latencies_ms": [2760.998], "images_per_second": 0.362, "prompt_tokens": 9, "response_tokens_est": 34, "n_tiles": 12, "output_text": "A yellow and white bus with the number 475 is seen in motion on a city street, with a white van and a motorcyclist in the background.", "error": null, "sys_before": {"cpu_percent": 45.8, "ram_used_mb": 24909.3, "ram_available_mb": 100862.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 24910.0, "ram_available_mb": 100862.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.16, "peak": 16.66, "min": 13.39}, "VDD_CPU_SOC_MSS": {"avg": 15.92, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 35.3, "peak": 43.71, "min": 26.8}, "VIN": {"avg": 77.67, "peak": 114.17, "min": 65.7}}, "power_watts_avg": 35.3, "energy_joules_est": 97.48, "sample_count": 21, "duration_seconds": 2.761}, "timestamp": "2026-01-17T14:57:06.411119"}
{"image_index": 171, "image_name": "000000017207.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017207.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 3708.227, "latencies_ms": [3708.227], "images_per_second": 0.27, "prompt_tokens": 23, "response_tokens_est": 60, "n_tiles": 12, "output_text": "1. Bus: 1\n2. Motorcycle: 1\n3. Car: 1\n4. Street: 1\n5. Road: 1\n6. Pedestrian: 1\n7. Building: 1\n8. Street sign: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24910.0, "ram_available_mb": 100862.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 24911.2, "ram_available_mb": 100860.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.2, "peak": 16.56, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.8, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 33.76, "peak": 44.89, "min": 26.0}, "VIN": {"avg": 76.52, "peak": 114.93, "min": 58.57}}, "power_watts_avg": 33.76, "energy_joules_est": 125.22, "sample_count": 28, "duration_seconds": 3.709}, "timestamp": "2026-01-17T14:57:10.126249"}
{"image_index": 171, "image_name": "000000017207.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017207.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 4008.716, "latencies_ms": [4008.716], "images_per_second": 0.249, "prompt_tokens": 27, "response_tokens_est": 69, "n_tiles": 12, "output_text": "The main object in the foreground is a yellow bus with red and black text and graphics. The bus is parked on the side of the road, with its rear facing the camera. In the background, there is a white van driving on the road. The van is further away from the camera, indicating a significant distance between the two objects.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24911.2, "ram_available_mb": 100860.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 24912.5, "ram_available_mb": 100859.7, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.2, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.81, "peak": 17.71, "min": 14.96}, "VDD_GPU": {"avg": 33.21, "peak": 44.89, "min": 26.0}, "VIN": {"avg": 78.22, "peak": 131.96, "min": 65.83}}, "power_watts_avg": 33.21, "energy_joules_est": 133.14, "sample_count": 31, "duration_seconds": 4.009}, "timestamp": "2026-01-17T14:57:14.141906"}
{"image_index": 171, "image_name": "000000017207.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017207.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3595.466, "latencies_ms": [3595.466], "images_per_second": 0.278, "prompt_tokens": 21, "response_tokens_est": 57, "n_tiles": 12, "output_text": "The image depicts a scene on a city street with a yellow and white bus in motion, indicating it is likely a bus stop. The bus has a sign with Hindi text and a number \"475\" displayed on its side, suggesting it is part of a public transportation system.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24912.5, "ram_available_mb": 100859.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 24913.0, "ram_available_mb": 100859.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.21, "peak": 16.56, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.81, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 34.13, "peak": 44.89, "min": 26.0}, "VIN": {"avg": 76.09, "peak": 109.28, "min": 58.8}}, "power_watts_avg": 34.13, "energy_joules_est": 122.73, "sample_count": 27, "duration_seconds": 3.596}, "timestamp": "2026-01-17T14:57:17.744097"}
{"image_index": 171, "image_name": "000000017207.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017207.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3500.147, "latencies_ms": [3500.147], "images_per_second": 0.286, "prompt_tokens": 19, "response_tokens_est": 54, "n_tiles": 12, "output_text": "The image features a yellow and white bus with red and black text, indicating it is likely a public transport vehicle. The bus is parked on a wet road, suggesting recent rain. The lighting is natural, indicating daytime, and the weather appears to be overcast.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24913.0, "ram_available_mb": 100859.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 24913.9, "ram_available_mb": 100858.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.22, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.78, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 34.22, "peak": 44.49, "min": 26.0}, "VIN": {"avg": 75.7, "peak": 117.76, "min": 61.74}}, "power_watts_avg": 34.22, "energy_joules_est": 119.79, "sample_count": 27, "duration_seconds": 3.5}, "timestamp": "2026-01-17T14:57:21.250879"}
{"image_index": 172, "image_name": "000000017379.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017379.jpg", "image_width": 478, "image_height": 640, "image_resolution": "478x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2448.912, "latencies_ms": [2448.912], "images_per_second": 0.408, "prompt_tokens": 9, "response_tokens_est": 25, "n_tiles": 12, "output_text": "The image shows a bathroom with a sink, a mirror, and a toilet, all set against a tiled wall.", "error": null, "sys_before": {"cpu_percent": 44.9, "ram_used_mb": 24927.0, "ram_available_mb": 100845.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.4, "ram_used_mb": 24927.5, "ram_available_mb": 100844.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.2, "peak": 16.66, "min": 13.49}, "VDD_CPU_SOC_MSS": {"avg": 15.97, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 36.57, "peak": 44.49, "min": 27.97}, "VIN": {"avg": 82.48, "peak": 123.21, "min": 60.77}}, "power_watts_avg": 36.57, "energy_joules_est": 89.57, "sample_count": 19, "duration_seconds": 2.449}, "timestamp": "2026-01-17T14:57:23.802032"}
{"image_index": 172, "image_name": "000000017379.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017379.jpg", "image_width": 478, "image_height": 640, "image_resolution": "478x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 3530.736, "latencies_ms": [3530.736], "images_per_second": 0.283, "prompt_tokens": 23, "response_tokens_est": 55, "n_tiles": 12, "output_text": "- TV: 1\n- TV stand: 1\n- TV screen: 1\n- Mirror: 1\n- Sink: 1\n- Faucet: 1\n- Trash can: 1\n- Wall tiles: 1", "error": null, "sys_before": {"cpu_percent": 16.7, "ram_used_mb": 24927.5, "ram_available_mb": 100844.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 24928.0, "ram_available_mb": 100844.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.23, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.7, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 34.24, "peak": 45.68, "min": 26.0}, "VIN": {"avg": 76.06, "peak": 131.11, "min": 52.62}}, "power_watts_avg": 34.24, "energy_joules_est": 120.92, "sample_count": 28, "duration_seconds": 3.531}, "timestamp": "2026-01-17T14:57:27.339622"}
{"image_index": 172, "image_name": "000000017379.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017379.jpg", "image_width": 478, "image_height": 640, "image_resolution": "478x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3882.115, "latencies_ms": [3882.115], "images_per_second": 0.258, "prompt_tokens": 27, "response_tokens_est": 65, "n_tiles": 12, "output_text": "The main objects in the image are a bathroom sink, a toilet paper holder, and a mirror. The sink is located in the foreground, with the toilet paper holder and mirror positioned near it. The toilet paper holder is situated to the right of the sink, and the mirror is positioned to the left of the sink.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24928.0, "ram_available_mb": 100844.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 24928.5, "ram_available_mb": 100843.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.2, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.72, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 33.36, "peak": 45.28, "min": 26.0}, "VIN": {"avg": 80.79, "peak": 134.12, "min": 64.99}}, "power_watts_avg": 33.36, "energy_joules_est": 129.52, "sample_count": 30, "duration_seconds": 3.883}, "timestamp": "2026-01-17T14:57:31.228411"}
{"image_index": 172, "image_name": "000000017379.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017379.jpg", "image_width": 478, "image_height": 640, "image_resolution": "478x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3839.997, "latencies_ms": [3839.997], "images_per_second": 0.26, "prompt_tokens": 21, "response_tokens_est": 64, "n_tiles": 12, "output_text": "The image depicts a bathroom with a modern and clean design. The setting is a bathroom with a sink, a mirror, and a tiled wall. The sink is empty, and there is a white soap dispenser on the counter. The wall behind the sink is tiled with beige and brown tiles.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24928.5, "ram_available_mb": 100843.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 24928.8, "ram_available_mb": 100843.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.2, "peak": 16.56, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.71, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 33.37, "peak": 44.89, "min": 26.0}, "VIN": {"avg": 75.94, "peak": 120.93, "min": 61.7}}, "power_watts_avg": 33.37, "energy_joules_est": 128.15, "sample_count": 30, "duration_seconds": 3.84}, "timestamp": "2026-01-17T14:57:35.074936"}
{"image_index": 172, "image_name": "000000017379.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017379.jpg", "image_width": 478, "image_height": 640, "image_resolution": "478x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2823.335, "latencies_ms": [2823.335], "images_per_second": 0.354, "prompt_tokens": 19, "response_tokens_est": 34, "n_tiles": 12, "output_text": "The image shows a bathroom with beige tiled walls and a brown door. The lighting is bright, and the materials used include tiles and a granite countertop.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24928.8, "ram_available_mb": 100843.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 24929.0, "ram_available_mb": 100843.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.26, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.88, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 36.27, "peak": 44.89, "min": 26.79}, "VIN": {"avg": 78.63, "peak": 110.72, "min": 57.55}}, "power_watts_avg": 36.27, "energy_joules_est": 102.41, "sample_count": 21, "duration_seconds": 2.824}, "timestamp": "2026-01-17T14:57:37.908696"}
{"image_index": 173, "image_name": "000000017436.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017436.jpg", "image_width": 481, "image_height": 640, "image_resolution": "481x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2355.911, "latencies_ms": [2355.911], "images_per_second": 0.424, "prompt_tokens": 9, "response_tokens_est": 22, "n_tiles": 12, "output_text": "A man is sitting on a bench in a park, looking down and appearing to be deep in thought.", "error": null, "sys_before": {"cpu_percent": 42.9, "ram_used_mb": 24920.9, "ram_available_mb": 100851.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.4, "ram_used_mb": 24921.6, "ram_available_mb": 100850.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.14, "peak": 16.76, "min": 13.19}, "VDD_CPU_SOC_MSS": {"avg": 15.92, "peak": 17.71, "min": 14.17}, "VDD_GPU": {"avg": 37.11, "peak": 44.49, "min": 28.76}, "VIN": {"avg": 76.88, "peak": 114.86, "min": 59.8}}, "power_watts_avg": 37.11, "energy_joules_est": 87.45, "sample_count": 18, "duration_seconds": 2.356}, "timestamp": "2026-01-17T14:57:40.377447"}
{"image_index": 173, "image_name": "000000017436.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017436.jpg", "image_width": 481, "image_height": 640, "image_resolution": "481x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 3124.221, "latencies_ms": [3124.221], "images_per_second": 0.32, "prompt_tokens": 23, "response_tokens_est": 43, "n_tiles": 12, "output_text": "1. Church\n2. Church steeple\n3. Church building\n4. Church clock\n5. Church clock tower\n6. Church bell\n7. Church spire\n8. Church arch", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24921.6, "ram_available_mb": 100850.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 24922.1, "ram_available_mb": 100850.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.24, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.86, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 35.43, "peak": 46.07, "min": 26.39}, "VIN": {"avg": 72.08, "peak": 86.63, "min": 66.3}}, "power_watts_avg": 35.43, "energy_joules_est": 110.71, "sample_count": 24, "duration_seconds": 3.125}, "timestamp": "2026-01-17T14:57:43.508191"}
{"image_index": 173, "image_name": "000000017436.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017436.jpg", "image_width": 481, "image_height": 640, "image_resolution": "481x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 4149.978, "latencies_ms": [4149.978], "images_per_second": 0.241, "prompt_tokens": 27, "response_tokens_est": 73, "n_tiles": 12, "output_text": "The main objects in the image are a man sitting on a bench and a church tower. The man is positioned in the foreground, sitting on the bench, while the church tower is in the background, towering over the scene. The bench is located near the man, and the church tower is further away, creating a clear spatial relationship between the two objects.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24922.1, "ram_available_mb": 100850.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 24922.8, "ram_available_mb": 100849.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.2, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.81, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 33.09, "peak": 45.68, "min": 26.0}, "VIN": {"avg": 73.8, "peak": 116.06, "min": 47.32}}, "power_watts_avg": 33.09, "energy_joules_est": 137.33, "sample_count": 32, "duration_seconds": 4.15}, "timestamp": "2026-01-17T14:57:47.668745"}
{"image_index": 173, "image_name": "000000017436.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017436.jpg", "image_width": 481, "image_height": 640, "image_resolution": "481x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 4143.428, "latencies_ms": [4143.428], "images_per_second": 0.241, "prompt_tokens": 21, "response_tokens_est": 73, "n_tiles": 12, "output_text": "The image depicts a serene, monochromatic scene of a park or garden with a man sitting on a bench. The setting appears to be a quiet, possibly urban area with well-maintained greenery and a small building in the background. The man seems to be resting or contemplating, while the surrounding environment is peaceful and orderly.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24922.8, "ram_available_mb": 100849.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 24924.0, "ram_available_mb": 100848.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.19, "peak": 16.56, "min": 14.12}, "VDD_CPU_SOC_MSS": {"avg": 15.67, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 32.83, "peak": 45.3, "min": 26.0}, "VIN": {"avg": 75.12, "peak": 138.3, "min": 63.68}}, "power_watts_avg": 32.83, "energy_joules_est": 136.04, "sample_count": 32, "duration_seconds": 4.144}, "timestamp": "2026-01-17T14:57:51.819080"}
{"image_index": 173, "image_name": "000000017436.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017436.jpg", "image_width": 481, "image_height": 640, "image_resolution": "481x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3600.194, "latencies_ms": [3600.194], "images_per_second": 0.278, "prompt_tokens": 19, "response_tokens_est": 57, "n_tiles": 12, "output_text": "The image is a black and white photograph featuring a man sitting on a bench in a park-like setting. The man is dressed in casual attire, and the bench is made of concrete. The lighting is soft and diffused, suggesting an overcast day with no direct sunlight.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24924.0, "ram_available_mb": 100848.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 24925.0, "ram_available_mb": 100847.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.23, "peak": 16.56, "min": 14.12}, "VDD_CPU_SOC_MSS": {"avg": 15.7, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 34.15, "peak": 44.49, "min": 26.0}, "VIN": {"avg": 75.87, "peak": 106.82, "min": 60.22}}, "power_watts_avg": 34.15, "energy_joules_est": 122.96, "sample_count": 27, "duration_seconds": 3.601}, "timestamp": "2026-01-17T14:57:55.427373"}
{"image_index": 174, "image_name": "000000017627.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017627.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2558.592, "latencies_ms": [2558.592], "images_per_second": 0.391, "prompt_tokens": 9, "response_tokens_est": 28, "n_tiles": 12, "output_text": "The image depicts a busy street scene with several cars parked along the side, a person walking, and a building in the background.", "error": null, "sys_before": {"cpu_percent": 45.3, "ram_used_mb": 24925.7, "ram_available_mb": 100846.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.6, "ram_used_mb": 24924.2, "ram_available_mb": 100848.0, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.17, "peak": 16.76, "min": 13.39}, "VDD_CPU_SOC_MSS": {"avg": 15.93, "peak": 17.71, "min": 14.18}, "VDD_GPU": {"avg": 36.22, "peak": 43.73, "min": 27.57}, "VIN": {"avg": 81.51, "peak": 114.58, "min": 67.07}}, "power_watts_avg": 36.22, "energy_joules_est": 92.69, "sample_count": 19, "duration_seconds": 2.559}, "timestamp": "2026-01-17T14:57:58.091306"}
{"image_index": 174, "image_name": "000000017627.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017627.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 3313.825, "latencies_ms": [3313.825], "images_per_second": 0.302, "prompt_tokens": 23, "response_tokens_est": 48, "n_tiles": 12, "output_text": "- Car: 5\n- Car: 2\n- Car: 1\n- Car: 1\n- Car: 1\n- Car: 1\n- Car: 1\n- Car: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24924.2, "ram_available_mb": 100848.0, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 24920.5, "ram_available_mb": 100851.7, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.18, "peak": 16.66, "min": 14.12}, "VDD_CPU_SOC_MSS": {"avg": 15.7, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 34.97, "peak": 45.28, "min": 26.01}, "VIN": {"avg": 77.99, "peak": 118.97, "min": 64.41}}, "power_watts_avg": 34.97, "energy_joules_est": 115.91, "sample_count": 25, "duration_seconds": 3.315}, "timestamp": "2026-01-17T14:58:01.416347"}
{"image_index": 174, "image_name": "000000017627.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017627.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 4041.048, "latencies_ms": [4041.048], "images_per_second": 0.247, "prompt_tokens": 27, "response_tokens_est": 69, "n_tiles": 12, "output_text": "The main objects in the image are parked cars and a building. The cars are parked in the foreground, with the closest one being a white sedan. The building is in the background, with a stone facade and a sign on it. The parking lot is located near the building, with a yellow line marking the edge of the parking area.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24920.5, "ram_available_mb": 100851.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 24920.5, "ram_available_mb": 100851.7, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.89, "peak": 16.26, "min": 13.92}, "VDD_CPU_SOC_MSS": {"avg": 15.39, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 33.07, "peak": 45.3, "min": 25.6}, "VIN": {"avg": 73.87, "peak": 119.54, "min": 54.96}}, "power_watts_avg": 33.07, "energy_joules_est": 133.65, "sample_count": 30, "duration_seconds": 4.041}, "timestamp": "2026-01-17T14:58:05.464296"}
{"image_index": 174, "image_name": "000000017627.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017627.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 4318.636, "latencies_ms": [4318.636], "images_per_second": 0.232, "prompt_tokens": 21, "response_tokens_est": 77, "n_tiles": 12, "output_text": "The image depicts a bustling urban street scene, likely in a Middle Eastern city, given the architecture and the presence of a sign in Hebrew. The setting is characterized by a mix of modern and traditional elements, with cars parked along the side of the street and pedestrians walking. The sky is clear, suggesting a sunny day, and the overall atmosphere is busy yet orderly.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24920.5, "ram_available_mb": 100851.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 24920.5, "ram_available_mb": 100851.7, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.87, "peak": 16.36, "min": 13.82}, "VDD_CPU_SOC_MSS": {"avg": 15.34, "peak": 17.32, "min": 14.17}, "VDD_GPU": {"avg": 32.32, "peak": 45.69, "min": 25.6}, "VIN": {"avg": 71.31, "peak": 116.65, "min": 51.18}}, "power_watts_avg": 32.32, "energy_joules_est": 139.59, "sample_count": 34, "duration_seconds": 4.319}, "timestamp": "2026-01-17T14:58:09.789631"}
{"image_index": 174, "image_name": "000000017627.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017627.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3376.083, "latencies_ms": [3376.083], "images_per_second": 0.296, "prompt_tokens": 19, "response_tokens_est": 50, "n_tiles": 12, "output_text": "The image depicts a sunny day with clear blue skies. The scene is characterized by a mix of modern and traditional elements. The parking lot is filled with cars, and there are various signs and structures in the background, including a stone building.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24920.5, "ram_available_mb": 100851.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 24920.5, "ram_available_mb": 100851.7, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.94, "peak": 16.36, "min": 14.02}, "VDD_CPU_SOC_MSS": {"avg": 15.49, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 34.56, "peak": 45.3, "min": 25.6}, "VIN": {"avg": 78.56, "peak": 132.54, "min": 54.08}}, "power_watts_avg": 34.56, "energy_joules_est": 116.69, "sample_count": 26, "duration_seconds": 3.377}, "timestamp": "2026-01-17T14:58:13.172296"}
{"image_index": 175, "image_name": "000000017714.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017714.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2870.674, "latencies_ms": [2870.674], "images_per_second": 0.348, "prompt_tokens": 9, "response_tokens_est": 37, "n_tiles": 12, "output_text": "The image shows a plate of food consisting of a slice of banana bread, a cup of coffee, and a bowl of watermelon slices, all placed on a wooden table.", "error": null, "sys_before": {"cpu_percent": 44.1, "ram_used_mb": 24914.9, "ram_available_mb": 100857.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 24915.9, "ram_available_mb": 100856.3, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.09, "peak": 16.76, "min": 13.09}, "VDD_CPU_SOC_MSS": {"avg": 15.78, "peak": 17.71, "min": 13.78}, "VDD_GPU": {"avg": 35.17, "peak": 44.12, "min": 26.79}, "VIN": {"avg": 76.88, "peak": 104.52, "min": 63.26}}, "power_watts_avg": 35.17, "energy_joules_est": 100.97, "sample_count": 21, "duration_seconds": 2.871}, "timestamp": "2026-01-17T14:58:16.153260"}
{"image_index": 175, "image_name": "000000017714.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017714.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2961.714, "latencies_ms": [2961.714], "images_per_second": 0.338, "prompt_tokens": 23, "response_tokens_est": 38, "n_tiles": 12, "output_text": "1. Plate\n2. Cereal\n3. Cup\n4. Cutting board\n5. Knife\n6. Fork\n7. Watermelon\n8. Banana", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24915.9, "ram_available_mb": 100856.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 24916.4, "ram_available_mb": 100855.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.23, "peak": 16.56, "min": 14.12}, "VDD_CPU_SOC_MSS": {"avg": 15.89, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 35.98, "peak": 44.89, "min": 26.79}, "VIN": {"avg": 73.93, "peak": 92.91, "min": 51.71}}, "power_watts_avg": 35.98, "energy_joules_est": 106.57, "sample_count": 22, "duration_seconds": 2.962}, "timestamp": "2026-01-17T14:58:19.121972"}
{"image_index": 175, "image_name": "000000017714.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017714.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 4358.907, "latencies_ms": [4358.907], "images_per_second": 0.229, "prompt_tokens": 27, "response_tokens_est": 79, "n_tiles": 12, "output_text": "The main objects in the image are a plate of food and a cup of coffee. The plate of food is positioned in the foreground, with a slice of banana and a piece of fruit on it. The cup of coffee is placed to the left of the plate, with a spoon inside it. The background features a wooden table, and the shadow of a person is cast on the floor.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24916.4, "ram_available_mb": 100855.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 24916.9, "ram_available_mb": 100855.3, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.18, "peak": 16.56, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.69, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 32.74, "peak": 44.89, "min": 26.0}, "VIN": {"avg": 74.63, "peak": 117.45, "min": 66.17}}, "power_watts_avg": 32.74, "energy_joules_est": 142.73, "sample_count": 32, "duration_seconds": 4.359}, "timestamp": "2026-01-17T14:58:23.487825"}
{"image_index": 175, "image_name": "000000017714.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017714.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3909.932, "latencies_ms": [3909.932], "images_per_second": 0.256, "prompt_tokens": 21, "response_tokens_est": 66, "n_tiles": 12, "output_text": "The image depicts a breakfast scene set on a wooden table with a marble floor. The table is adorned with a plate of food, including a slice of pancake, a bowl of fruit, and a cup of coffee. The setting suggests a cozy and relaxed atmosphere, likely in a home or a casual dining environment.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24916.9, "ram_available_mb": 100855.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 24917.1, "ram_available_mb": 100855.0, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.22, "peak": 16.56, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.77, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 33.43, "peak": 44.89, "min": 26.01}, "VIN": {"avg": 75.38, "peak": 130.57, "min": 58.33}}, "power_watts_avg": 33.43, "energy_joules_est": 130.72, "sample_count": 29, "duration_seconds": 3.91}, "timestamp": "2026-01-17T14:58:27.405954"}
{"image_index": 175, "image_name": "000000017714.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017714.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 4283.508, "latencies_ms": [4283.508], "images_per_second": 0.233, "prompt_tokens": 19, "response_tokens_est": 77, "n_tiles": 12, "output_text": "The image depicts a warm, sunny day with sunlight casting shadows on the tiled floor. The scene features a wooden table with a rustic appearance, holding a plate of food including a slice of banana bread, a bowl of coffee, and a plate of watermelon slices. The lighting is bright and natural, enhancing the colors and textures of the food and the wooden table.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 24917.1, "ram_available_mb": 100855.0, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 24917.9, "ram_available_mb": 100854.3, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.2, "peak": 16.56, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.71, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 32.79, "peak": 44.89, "min": 26.0}, "VIN": {"avg": 76.68, "peak": 136.66, "min": 62.75}}, "power_watts_avg": 32.79, "energy_joules_est": 140.47, "sample_count": 32, "duration_seconds": 4.284}, "timestamp": "2026-01-17T14:58:31.699307"}
{"image_index": 176, "image_name": "000000017899.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017899.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2432.258, "latencies_ms": [2432.258], "images_per_second": 0.411, "prompt_tokens": 9, "response_tokens_est": 24, "n_tiles": 12, "output_text": "An elderly woman is preparing a batch of homemade cookies on a wooden table, surrounded by various baking tools and ingredients.", "error": null, "sys_before": {"cpu_percent": 31.4, "ram_used_mb": 24913.4, "ram_available_mb": 100858.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.4, "ram_used_mb": 24912.7, "ram_available_mb": 100859.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.16, "peak": 16.76, "min": 13.29}, "VDD_CPU_SOC_MSS": {"avg": 15.83, "peak": 17.71, "min": 14.17}, "VDD_GPU": {"avg": 36.61, "peak": 44.91, "min": 27.97}, "VIN": {"avg": 82.32, "peak": 113.9, "min": 57.92}}, "power_watts_avg": 36.61, "energy_joules_est": 89.06, "sample_count": 19, "duration_seconds": 2.433}, "timestamp": "2026-01-17T14:58:34.226162"}
{"image_index": 176, "image_name": "000000017899.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017899.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 6025.373, "latencies_ms": [6025.373], "images_per_second": 0.166, "prompt_tokens": 23, "response_tokens_est": 129, "n_tiles": 12, "output_text": "- plate: 1\n- bowl: 1\n- knife: 1\n- cookies: 8\n- plate: 1\n- glass: 1\n- cup: 1\n- plate: 1\n- plate: 1\n- plate: 1\n- plate: 1\n- plate: 1\n- plate: 1\n- plate: 1\n- plate: 1\n- plate: 1\n- plate: 1\n- plate: 1\n- plate: 1\n- plate: 1\n- plate: 1\n- plate", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24912.7, "ram_available_mb": 100859.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 24913.7, "ram_available_mb": 100858.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.19, "peak": 16.56, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.6, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 30.8, "peak": 45.28, "min": 26.0}, "VIN": {"avg": 73.6, "peak": 134.17, "min": 53.22}}, "power_watts_avg": 30.8, "energy_joules_est": 185.6, "sample_count": 47, "duration_seconds": 6.026}, "timestamp": "2026-01-17T14:58:40.258511"}
{"image_index": 176, "image_name": "000000017899.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017899.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 4739.838, "latencies_ms": [4739.838], "images_per_second": 0.211, "prompt_tokens": 27, "response_tokens_est": 91, "n_tiles": 12, "output_text": "The main objects in the image are a woman, a table, and various baking items. The woman is seated at the table, focused on baking. The table is the central object, with the baking items placed on it. The background includes a couch and a door, while the foreground features a glass of water and a book. The woman is positioned near the table, with the baking items in the foreground and the couch and door in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24913.7, "ram_available_mb": 100858.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 24914.5, "ram_available_mb": 100857.7, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.24, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.76, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 32.13, "peak": 44.91, "min": 26.0}, "VIN": {"avg": 74.35, "peak": 107.9, "min": 62.49}}, "power_watts_avg": 32.13, "energy_joules_est": 152.31, "sample_count": 37, "duration_seconds": 4.74}, "timestamp": "2026-01-17T14:58:45.004619"}
{"image_index": 176, "image_name": "000000017899.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017899.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 4179.0, "latencies_ms": [4179.0], "images_per_second": 0.239, "prompt_tokens": 21, "response_tokens_est": 74, "n_tiles": 12, "output_text": "The image depicts an elderly woman in a kitchen setting, engaged in the process of baking cookies. She is surrounded by various baking tools and ingredients, including a wooden table, a metal cooling rack, and a variety of cookies on the rack. The room appears to be a cozy, well-used kitchen with a wooden door and a couch visible in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24914.5, "ram_available_mb": 100857.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 24915.5, "ram_available_mb": 100856.7, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.2, "peak": 16.66, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.7, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 32.88, "peak": 44.51, "min": 26.0}, "VIN": {"avg": 74.08, "peak": 115.0, "min": 58.62}}, "power_watts_avg": 32.88, "energy_joules_est": 137.42, "sample_count": 32, "duration_seconds": 4.179}, "timestamp": "2026-01-17T14:58:49.190152"}
{"image_index": 176, "image_name": "000000017899.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017899.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 4804.042, "latencies_ms": [4804.042], "images_per_second": 0.208, "prompt_tokens": 19, "response_tokens_est": 92, "n_tiles": 12, "output_text": "The image depicts a cozy, well-lit kitchen scene with a warm, inviting ambiance. The lighting is soft and natural, casting gentle shadows that enhance the textures and colors of the objects present. The kitchen features a wooden table, a white metal cooling rack, and various baking items, including a bowl, a tray of cookies, and a glass of water. The overall atmosphere is cheerful and homely, with a touch of rustic charm.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24915.5, "ram_available_mb": 100856.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 24915.7, "ram_available_mb": 100856.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.15, "peak": 16.66, "min": 14.12}, "VDD_CPU_SOC_MSS": {"avg": 15.63, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 31.99, "peak": 45.69, "min": 25.61}, "VIN": {"avg": 74.17, "peak": 128.44, "min": 59.17}}, "power_watts_avg": 31.99, "energy_joules_est": 153.7, "sample_count": 38, "duration_seconds": 4.805}, "timestamp": "2026-01-17T14:58:54.001324"}
{"image_index": 177, "image_name": "000000017905.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017905.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2736.146, "latencies_ms": [2736.146], "images_per_second": 0.365, "prompt_tokens": 9, "response_tokens_est": 33, "n_tiles": 12, "output_text": "A man wearing a white t-shirt and khaki shorts stands in front of a traffic light, which is illuminated in red, indicating that vehicles should stop.", "error": null, "sys_before": {"cpu_percent": 42.4, "ram_used_mb": 24901.4, "ram_available_mb": 100870.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 24903.6, "ram_available_mb": 100868.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.15, "peak": 16.76, "min": 13.29}, "VDD_CPU_SOC_MSS": {"avg": 15.82, "peak": 17.71, "min": 14.17}, "VDD_GPU": {"avg": 35.62, "peak": 44.89, "min": 26.79}, "VIN": {"avg": 78.74, "peak": 116.27, "min": 59.6}}, "power_watts_avg": 35.62, "energy_joules_est": 97.47, "sample_count": 21, "duration_seconds": 2.736}, "timestamp": "2026-01-17T14:58:56.833000"}
{"image_index": 177, "image_name": "000000017905.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017905.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 3471.014, "latencies_ms": [3471.014], "images_per_second": 0.288, "prompt_tokens": 23, "response_tokens_est": 53, "n_tiles": 12, "output_text": "- man: 1\n- white t-shirt: 1\n- shorts: 1\n- sandals: 1\n- traffic light: 1\n- sign: 1\n- tree: 1\n- bushes: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24903.6, "ram_available_mb": 100868.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 24904.5, "ram_available_mb": 100867.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.21, "peak": 16.66, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.8, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 34.43, "peak": 45.3, "min": 26.01}, "VIN": {"avg": 77.88, "peak": 117.55, "min": 59.32}}, "power_watts_avg": 34.43, "energy_joules_est": 119.52, "sample_count": 27, "duration_seconds": 3.471}, "timestamp": "2026-01-17T14:59:00.313847"}
{"image_index": 177, "image_name": "000000017905.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017905.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3658.049, "latencies_ms": [3658.049], "images_per_second": 0.273, "prompt_tokens": 27, "response_tokens_est": 58, "n_tiles": 12, "output_text": "The main object in the foreground is a man standing next to a traffic light. The traffic light is positioned to the right of the man. The background features lush green plants and a sign that reads \"AUSTRALIA TRAFFIC LIGHT.\" The sign is located near the man.", "error": null, "sys_before": {"cpu_percent": 16.7, "ram_used_mb": 24904.5, "ram_available_mb": 100867.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 24905.5, "ram_available_mb": 100866.7, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.94, "peak": 16.36, "min": 14.02}, "VDD_CPU_SOC_MSS": {"avg": 15.46, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 33.79, "peak": 45.69, "min": 25.6}, "VIN": {"avg": 76.72, "peak": 121.34, "min": 55.58}}, "power_watts_avg": 33.79, "energy_joules_est": 123.62, "sample_count": 28, "duration_seconds": 3.659}, "timestamp": "2026-01-17T14:59:03.978573"}
{"image_index": 177, "image_name": "000000017905.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017905.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3405.258, "latencies_ms": [3405.258], "images_per_second": 0.294, "prompt_tokens": 21, "response_tokens_est": 51, "n_tiles": 12, "output_text": "The image depicts a man standing in front of a traffic light, which is currently showing red. The setting appears to be outdoors, possibly in a park or garden area, with lush green foliage and a variety of plants surrounding the man.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24905.5, "ram_available_mb": 100866.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 24906.3, "ram_available_mb": 100865.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.94, "peak": 16.26, "min": 14.02}, "VDD_CPU_SOC_MSS": {"avg": 15.5, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 34.31, "peak": 44.91, "min": 25.6}, "VIN": {"avg": 75.06, "peak": 113.76, "min": 65.03}}, "power_watts_avg": 34.31, "energy_joules_est": 116.86, "sample_count": 26, "duration_seconds": 3.406}, "timestamp": "2026-01-17T14:59:07.390687"}
{"image_index": 177, "image_name": "000000017905.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017905.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2958.832, "latencies_ms": [2958.832], "images_per_second": 0.338, "prompt_tokens": 19, "response_tokens_est": 38, "n_tiles": 12, "output_text": "The image features a brightly lit scene with a man standing in front of a traffic light. The traffic light is red, and the surrounding area is lush with green foliage.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24906.3, "ram_available_mb": 100865.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 24906.3, "ram_available_mb": 100865.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.99, "peak": 16.36, "min": 14.02}, "VDD_CPU_SOC_MSS": {"avg": 15.56, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 35.72, "peak": 45.3, "min": 26.39}, "VIN": {"avg": 78.69, "peak": 127.45, "min": 55.24}}, "power_watts_avg": 35.72, "energy_joules_est": 105.72, "sample_count": 23, "duration_seconds": 2.96}, "timestamp": "2026-01-17T14:59:10.357083"}
{"image_index": 178, "image_name": "000000017959.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017959.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1736.11, "latencies_ms": [1736.11], "images_per_second": 0.576, "prompt_tokens": 9, "response_tokens_est": 42, "n_tiles": 6, "output_text": "The image depicts a vibrant scene of a kite festival with a large, colorful kite flying high in the sky, featuring a fish design, amidst a crowd of people and other kites.", "error": null, "sys_before": {"cpu_percent": 45.9, "ram_used_mb": 24923.9, "ram_available_mb": 100848.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 24925.4, "ram_available_mb": 100846.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.74, "peak": 15.22, "min": 13.51}, "VDD_CPU_SOC_MSS": {"avg": 15.38, "peak": 16.14, "min": 14.17}, "VDD_GPU": {"avg": 31.85, "peak": 40.56, "min": 24.83}, "VIN": {"avg": 72.96, "peak": 109.52, "min": 60.18}}, "power_watts_avg": 31.85, "energy_joules_est": 55.31, "sample_count": 13, "duration_seconds": 1.736}, "timestamp": "2026-01-17T14:59:12.156960"}
{"image_index": 178, "image_name": "000000017959.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017959.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1726.059, "latencies_ms": [1726.059], "images_per_second": 0.579, "prompt_tokens": 23, "response_tokens_est": 40, "n_tiles": 6, "output_text": "object: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24925.4, "ram_available_mb": 100846.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 24926.6, "ram_available_mb": 100845.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.05, "peak": 15.44, "min": 14.43}, "VDD_CPU_SOC_MSS": {"avg": 15.63, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 32.42, "peak": 40.57, "min": 24.83}, "VIN": {"avg": 75.6, "peak": 110.14, "min": 63.97}}, "power_watts_avg": 32.42, "energy_joules_est": 55.97, "sample_count": 13, "duration_seconds": 1.726}, "timestamp": "2026-01-17T14:59:13.889955"}
{"image_index": 178, "image_name": "000000017959.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017959.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3486.479, "latencies_ms": [3486.479], "images_per_second": 0.287, "prompt_tokens": 27, "response_tokens_est": 105, "n_tiles": 6, "output_text": "The main objects in the image are kites, with the most prominent one being a large, colorful fish kite in the foreground. The fish kite is positioned near the center of the image, with its tail extending towards the right side. In the background, there are other kites, including a red and black one and a blue and purple one, which are positioned further away from the fish kite. The kites are spread out across the field, with some closer to the foreground and others further back.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24926.6, "ram_available_mb": 100845.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 24927.8, "ram_available_mb": 100844.3, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.13, "peak": 15.34, "min": 14.43}, "VDD_CPU_SOC_MSS": {"avg": 15.7, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 27.95, "peak": 40.95, "min": 23.64}, "VIN": {"avg": 68.59, "peak": 101.02, "min": 56.09}}, "power_watts_avg": 27.95, "energy_joules_est": 97.47, "sample_count": 27, "duration_seconds": 3.487}, "timestamp": "2026-01-17T14:59:17.383003"}
{"image_index": 178, "image_name": "000000017959.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017959.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2622.019, "latencies_ms": [2622.019], "images_per_second": 0.381, "prompt_tokens": 21, "response_tokens_est": 73, "n_tiles": 6, "output_text": "The image depicts a vibrant outdoor scene at a public park where numerous people are gathered, enjoying a day of leisure and festivities. The setting is characterized by a large, open grassy area, with a few scattered people and a few kites in the air. The kites, featuring colorful designs, add a lively and festive atmosphere to the scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24927.8, "ram_available_mb": 100844.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 24928.8, "ram_available_mb": 100843.3, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.12, "peak": 15.54, "min": 14.43}, "VDD_CPU_SOC_MSS": {"avg": 15.67, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 29.15, "peak": 40.16, "min": 23.64}, "VIN": {"avg": 72.23, "peak": 106.69, "min": 61.37}}, "power_watts_avg": 29.15, "energy_joules_est": 76.45, "sample_count": 20, "duration_seconds": 2.622}, "timestamp": "2026-01-17T14:59:20.011195"}
{"image_index": 178, "image_name": "000000017959.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017959.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2236.919, "latencies_ms": [2236.919], "images_per_second": 0.447, "prompt_tokens": 19, "response_tokens_est": 59, "n_tiles": 6, "output_text": "The image showcases a vibrant kite flying high in the sky, featuring a striking red fish with black and white stripes. The kite is illuminated by the bright sunlight, casting a warm glow on the scene. The sky is clear, indicating a sunny day with good weather conditions.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24928.8, "ram_available_mb": 100843.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 24929.3, "ram_available_mb": 100842.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.09, "peak": 15.44, "min": 14.43}, "VDD_CPU_SOC_MSS": {"avg": 15.65, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 30.24, "peak": 40.18, "min": 23.64}, "VIN": {"avg": 70.36, "peak": 103.4, "min": 55.1}}, "power_watts_avg": 30.24, "energy_joules_est": 67.66, "sample_count": 17, "duration_seconds": 2.237}, "timestamp": "2026-01-17T14:59:22.254384"}
{"image_index": 179, "image_name": "000000018150.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018150.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2565.392, "latencies_ms": [2565.392], "images_per_second": 0.39, "prompt_tokens": 9, "response_tokens_est": 28, "n_tiles": 12, "output_text": "A young boy is eating a slice of pizza from a box, while a man sits beside him, both in a cozy indoor setting.", "error": null, "sys_before": {"cpu_percent": 28.4, "ram_used_mb": 24904.7, "ram_available_mb": 100867.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.6, "ram_used_mb": 24905.7, "ram_available_mb": 100866.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.29, "peak": 16.76, "min": 13.9}, "VDD_CPU_SOC_MSS": {"avg": 15.97, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 36.24, "peak": 43.73, "min": 27.57}, "VIN": {"avg": 82.57, "peak": 134.16, "min": 64.58}}, "power_watts_avg": 36.24, "energy_joules_est": 92.99, "sample_count": 19, "duration_seconds": 2.566}, "timestamp": "2026-01-17T14:59:24.878440"}
{"image_index": 179, "image_name": "000000018150.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018150.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2995.905, "latencies_ms": [2995.905], "images_per_second": 0.334, "prompt_tokens": 23, "response_tokens_est": 39, "n_tiles": 12, "output_text": "1. Pizza\n2. Pizza box\n3. Pizza slice\n4. Pizza box\n5. Pizza box\n6. Pizza box\n7. Pizza box\n8. Pizza box", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24905.7, "ram_available_mb": 100866.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 24906.9, "ram_available_mb": 100865.3, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.24, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.81, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 35.62, "peak": 45.68, "min": 26.39}, "VIN": {"avg": 75.44, "peak": 107.01, "min": 61.0}}, "power_watts_avg": 35.62, "energy_joules_est": 106.74, "sample_count": 23, "duration_seconds": 2.997}, "timestamp": "2026-01-17T14:59:27.881039"}
{"image_index": 179, "image_name": "000000018150.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018150.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 4696.189, "latencies_ms": [4696.189], "images_per_second": 0.213, "prompt_tokens": 27, "response_tokens_est": 89, "n_tiles": 12, "output_text": "The main objects in the image are a man and a young child. The man is seated on the left side of the image, while the child is on the right side. The man is holding a pizza slice in his right hand, and the child is holding a green toy in their left hand. The pizza slice is in the foreground, and the child is slightly behind it. The background includes a couch and a window with blinds.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24906.9, "ram_available_mb": 100865.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 24907.1, "ram_available_mb": 100865.0, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.19, "peak": 16.56, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.66, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 32.13, "peak": 45.28, "min": 26.0}, "VIN": {"avg": 76.35, "peak": 133.3, "min": 61.59}}, "power_watts_avg": 32.13, "energy_joules_est": 150.9, "sample_count": 37, "duration_seconds": 4.697}, "timestamp": "2026-01-17T14:59:32.583859"}
{"image_index": 179, "image_name": "000000018150.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018150.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3844.191, "latencies_ms": [3844.191], "images_per_second": 0.26, "prompt_tokens": 21, "response_tokens_est": 64, "n_tiles": 12, "output_text": "The image depicts a cozy indoor setting with a young boy and a man seated on a couch. The boy is holding a slice of pizza, while the man is eating from a box of pizza. The room has warm lighting, and there are various items on the couch, including a stuffed animal and a bottle.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24907.1, "ram_available_mb": 100865.0, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 24908.1, "ram_available_mb": 100864.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.2, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.75, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 33.34, "peak": 45.3, "min": 26.0}, "VIN": {"avg": 76.67, "peak": 115.22, "min": 59.19}}, "power_watts_avg": 33.34, "energy_joules_est": 128.18, "sample_count": 30, "duration_seconds": 3.845}, "timestamp": "2026-01-17T14:59:36.434406"}
{"image_index": 179, "image_name": "000000018150.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018150.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2958.921, "latencies_ms": [2958.921], "images_per_second": 0.338, "prompt_tokens": 19, "response_tokens_est": 38, "n_tiles": 12, "output_text": "The image features a young boy with curly hair, wearing a blue hoodie, sitting on a red and brown striped blanket. The lighting is warm, creating a cozy atmosphere.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24908.1, "ram_available_mb": 100864.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 24908.4, "ram_available_mb": 100863.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.24, "peak": 16.56, "min": 14.12}, "VDD_CPU_SOC_MSS": {"avg": 15.81, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 35.85, "peak": 45.28, "min": 26.79}, "VIN": {"avg": 79.21, "peak": 116.36, "min": 60.42}}, "power_watts_avg": 35.85, "energy_joules_est": 106.11, "sample_count": 23, "duration_seconds": 2.96}, "timestamp": "2026-01-17T14:59:39.400315"}
{"image_index": 180, "image_name": "000000018193.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018193.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2333.267, "latencies_ms": [2333.267], "images_per_second": 0.429, "prompt_tokens": 9, "response_tokens_est": 21, "n_tiles": 12, "output_text": "A young woman is eating a hot dog from a sandwich, with a plate of chips beside her.", "error": null, "sys_before": {"cpu_percent": 31.4, "ram_used_mb": 24907.1, "ram_available_mb": 100865.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.4, "ram_used_mb": 24908.5, "ram_available_mb": 100863.7, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.21, "peak": 16.76, "min": 13.51}, "VDD_CPU_SOC_MSS": {"avg": 15.9, "peak": 17.71, "min": 14.17}, "VDD_GPU": {"avg": 37.09, "peak": 44.49, "min": 28.76}, "VIN": {"avg": 79.52, "peak": 117.13, "min": 66.04}}, "power_watts_avg": 37.09, "energy_joules_est": 86.56, "sample_count": 18, "duration_seconds": 2.334}, "timestamp": "2026-01-17T14:59:41.809281"}
{"image_index": 180, "image_name": "000000018193.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018193.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2825.415, "latencies_ms": [2825.415], "images_per_second": 0.354, "prompt_tokens": 23, "response_tokens_est": 34, "n_tiles": 12, "output_text": "1. Plate\n2. Sandwich\n3. Bag\n4. Woman\n5. Chair\n6. Food\n7. Plate\n8. Sandwich", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24908.5, "ram_available_mb": 100863.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 24907.7, "ram_available_mb": 100864.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.27, "peak": 16.56, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.85, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 36.36, "peak": 46.07, "min": 26.8}, "VIN": {"avg": 77.47, "peak": 121.23, "min": 66.82}}, "power_watts_avg": 36.36, "energy_joules_est": 102.74, "sample_count": 22, "duration_seconds": 2.826}, "timestamp": "2026-01-17T14:59:44.641121"}
{"image_index": 180, "image_name": "000000018193.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018193.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3437.908, "latencies_ms": [3437.908], "images_per_second": 0.291, "prompt_tokens": 27, "response_tokens_est": 52, "n_tiles": 12, "output_text": "The main object in the foreground is a plate with a few pieces of potato chips. The plate is placed on a person's lap, which is in the foreground. The background features a dark, possibly outdoor setting with a chair and a tree branch visible.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24907.7, "ram_available_mb": 100864.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 24909.2, "ram_available_mb": 100863.0, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.22, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.77, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 34.48, "peak": 45.68, "min": 26.0}, "VIN": {"avg": 74.44, "peak": 118.69, "min": 61.46}}, "power_watts_avg": 34.48, "energy_joules_est": 118.55, "sample_count": 27, "duration_seconds": 3.438}, "timestamp": "2026-01-17T14:59:48.085303"}
{"image_index": 180, "image_name": "000000018193.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018193.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3193.231, "latencies_ms": [3193.231], "images_per_second": 0.313, "prompt_tokens": 21, "response_tokens_est": 45, "n_tiles": 12, "output_text": "The image depicts a young woman sitting on a blue chair, holding a sandwich in her hand. She is surrounded by a dark, outdoor setting with a small pile of potato chips on a plate in front of her.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24909.2, "ram_available_mb": 100863.0, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 24907.4, "ram_available_mb": 100864.7, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.24, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.81, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 35.07, "peak": 45.28, "min": 26.4}, "VIN": {"avg": 75.94, "peak": 103.7, "min": 65.99}}, "power_watts_avg": 35.07, "energy_joules_est": 112.0, "sample_count": 25, "duration_seconds": 3.194}, "timestamp": "2026-01-17T14:59:51.285174"}
{"image_index": 180, "image_name": "000000018193.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018193.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3158.874, "latencies_ms": [3158.874], "images_per_second": 0.317, "prompt_tokens": 19, "response_tokens_est": 44, "n_tiles": 12, "output_text": "The image depicts a young woman with light skin and dark hair, wearing a striped shirt and holding a sandwich. The setting appears to be outdoors at night, with dim lighting and a dark, rocky ground.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24907.4, "ram_available_mb": 100864.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 24907.4, "ram_available_mb": 100864.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.26, "peak": 16.56, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.81, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 35.43, "peak": 45.28, "min": 26.39}, "VIN": {"avg": 78.4, "peak": 118.79, "min": 63.76}}, "power_watts_avg": 35.43, "energy_joules_est": 111.93, "sample_count": 24, "duration_seconds": 3.159}, "timestamp": "2026-01-17T14:59:54.450744"}
{"image_index": 181, "image_name": "000000018380.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018380.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1312.686, "latencies_ms": [1312.686], "images_per_second": 0.762, "prompt_tokens": 9, "response_tokens_est": 26, "n_tiles": 6, "output_text": "A group of people are gathered around a table, enjoying a meal together, with various dishes and drinks visible on the table.", "error": null, "sys_before": {"cpu_percent": 23.0, "ram_used_mb": 24926.3, "ram_available_mb": 100845.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 24927.0, "ram_available_mb": 100845.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.71, "peak": 15.14, "min": 13.92}, "VDD_CPU_SOC_MSS": {"avg": 15.31, "peak": 16.14, "min": 14.17}, "VDD_GPU": {"avg": 34.19, "peak": 40.95, "min": 26.8}, "VIN": {"avg": 76.24, "peak": 115.55, "min": 60.64}}, "power_watts_avg": 34.19, "energy_joules_est": 44.9, "sample_count": 10, "duration_seconds": 1.313}, "timestamp": "2026-01-17T14:59:55.811539"}
{"image_index": 181, "image_name": "000000018380.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018380.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 4220.237, "latencies_ms": [4220.237], "images_per_second": 0.237, "prompt_tokens": 23, "response_tokens_est": 126, "n_tiles": 6, "output_text": "- People: 8\n- Table: 1\n- Plate: 1\n- Glass: 1\n- Wine: 1\n- Bowl: 1\n- Cutlery: 1\n- Napkin: 1\n- Fork: 1\n- Knife: 1\n- Bread: 1\n- Meat: 1\n- Carrots: 1\n- Potatoes: 1\n- Chicken: 1\n- Dessert: 1\n- Beverage: 1\n- Glassware: 1\n- Tablecloth: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24927.0, "ram_available_mb": 100845.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 24927.0, "ram_available_mb": 100845.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.94, "peak": 15.34, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.57, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 26.83, "peak": 41.36, "min": 22.85}, "VIN": {"avg": 67.45, "peak": 102.93, "min": 54.2}}, "power_watts_avg": 26.83, "energy_joules_est": 113.24, "sample_count": 33, "duration_seconds": 4.221}, "timestamp": "2026-01-17T15:00:00.038334"}
{"image_index": 181, "image_name": "000000018380.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018380.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3167.833, "latencies_ms": [3167.833], "images_per_second": 0.316, "prompt_tokens": 27, "response_tokens_est": 89, "n_tiles": 6, "output_text": "The main objects in the image are a family gathered around a dining table. The table is covered with a purple tablecloth, and various dishes, plates, and glasses are spread out on it. The family members are seated in the foreground, with the woman in the center and the children on either side. The background features a staircase and a doorway, with a decorative piece on the wall and a framed picture above the door.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24927.0, "ram_available_mb": 100845.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 24927.8, "ram_available_mb": 100844.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.87, "peak": 15.24, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.55, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 27.84, "peak": 40.16, "min": 22.85}, "VIN": {"avg": 69.09, "peak": 109.19, "min": 54.09}}, "power_watts_avg": 27.84, "energy_joules_est": 88.21, "sample_count": 24, "duration_seconds": 3.168}, "timestamp": "2026-01-17T15:00:03.216765"}
{"image_index": 181, "image_name": "000000018380.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018380.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2646.633, "latencies_ms": [2646.633], "images_per_second": 0.378, "prompt_tokens": 21, "response_tokens_est": 71, "n_tiles": 6, "output_text": "The image depicts a family gathering in a warmly lit, cozy dining room. The family is seated around a large table covered with a purple tablecloth, filled with various dishes, including a plate of food, a bowl of fruit, and plates of food. The family members are smiling and posing for the camera, enjoying a meal together.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24927.8, "ram_available_mb": 100844.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 24928.3, "ram_available_mb": 100843.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.89, "peak": 15.24, "min": 14.2}, "VDD_CPU_SOC_MSS": {"avg": 15.32, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 28.85, "peak": 40.16, "min": 22.85}, "VIN": {"avg": 70.71, "peak": 113.58, "min": 62.03}}, "power_watts_avg": 28.85, "energy_joules_est": 76.36, "sample_count": 20, "duration_seconds": 2.647}, "timestamp": "2026-01-17T15:00:05.870268"}
{"image_index": 181, "image_name": "000000018380.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018380.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2162.663, "latencies_ms": [2162.663], "images_per_second": 0.462, "prompt_tokens": 19, "response_tokens_est": 54, "n_tiles": 6, "output_text": "The image depicts a family gathering in a warmly lit, cozy kitchen. The table is covered with a purple tablecloth, and various dishes, including a plate of food, are spread out. The lighting is soft and warm, creating a welcoming atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24928.3, "ram_available_mb": 100843.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 24929.5, "ram_available_mb": 100842.7, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.86, "peak": 15.34, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.48, "peak": 16.14, "min": 14.57}, "VDD_GPU": {"avg": 30.23, "peak": 40.16, "min": 23.24}, "VIN": {"avg": 68.59, "peak": 111.44, "min": 50.73}}, "power_watts_avg": 30.23, "energy_joules_est": 65.39, "sample_count": 16, "duration_seconds": 2.163}, "timestamp": "2026-01-17T15:00:08.039327"}
{"image_index": 182, "image_name": "000000018491.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018491.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1576.426, "latencies_ms": [1576.426], "images_per_second": 0.634, "prompt_tokens": 9, "response_tokens_est": 36, "n_tiles": 6, "output_text": "A baseball game is in progress on a sunny day, with players in protective gear on the field, one attempting to catch the ball while the other is sliding into the base.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 24929.5, "ram_available_mb": 100842.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 24929.7, "ram_available_mb": 100842.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.8, "peak": 15.24, "min": 14.0}, "VDD_CPU_SOC_MSS": {"avg": 15.48, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 32.3, "peak": 40.56, "min": 25.22}, "VIN": {"avg": 75.71, "peak": 111.98, "min": 60.06}}, "power_watts_avg": 32.3, "energy_joules_est": 50.94, "sample_count": 12, "duration_seconds": 1.577}, "timestamp": "2026-01-17T15:00:09.649238"}
{"image_index": 182, "image_name": "000000018491.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018491.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2104.783, "latencies_ms": [2104.783], "images_per_second": 0.475, "prompt_tokens": 23, "response_tokens_est": 52, "n_tiles": 6, "output_text": "baseball player: 1\numpire: 1\ncatcher: 1\nbatter: 1\npitcher: 1\nhome plate: 1\ndirt infield: 1\ngrass outfield: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24929.7, "ram_available_mb": 100842.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 24930.5, "ram_available_mb": 100841.7, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.91, "peak": 15.34, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.35, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 30.72, "peak": 40.56, "min": 23.25}, "VIN": {"avg": 69.41, "peak": 88.18, "min": 57.98}}, "power_watts_avg": 30.72, "energy_joules_est": 64.67, "sample_count": 16, "duration_seconds": 2.105}, "timestamp": "2026-01-17T15:00:11.760267"}
{"image_index": 182, "image_name": "000000018491.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018491.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2622.796, "latencies_ms": [2622.796], "images_per_second": 0.381, "prompt_tokens": 27, "response_tokens_est": 70, "n_tiles": 6, "output_text": "In the image, the main objects are the baseball players and the baseball field. The players are positioned in the foreground, with one player sliding into the base and another player standing near the base. The baseball field is in the background, with the fence and spectators visible. The players are near the base, while the field is farther away.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 24930.5, "ram_available_mb": 100841.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 24930.2, "ram_available_mb": 100842.0, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.87, "peak": 15.24, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.47, "peak": 15.75, "min": 14.56}, "VDD_GPU": {"avg": 29.0, "peak": 40.57, "min": 22.86}, "VIN": {"avg": 71.37, "peak": 112.77, "min": 62.53}}, "power_watts_avg": 29.0, "energy_joules_est": 76.07, "sample_count": 20, "duration_seconds": 2.623}, "timestamp": "2026-01-17T15:00:14.389498"}
{"image_index": 182, "image_name": "000000018491.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018491.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2363.004, "latencies_ms": [2363.004], "images_per_second": 0.423, "prompt_tokens": 21, "response_tokens_est": 61, "n_tiles": 6, "output_text": "The image captures a moment during a baseball game, with players in action on the field. The scene is set in a baseball field with a dirt infield and green grass surrounding it. The focus is on a player sliding into first base, while another player is in the process of throwing the ball.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24930.2, "ram_available_mb": 100842.0, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 24931.9, "ram_available_mb": 100840.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.87, "peak": 15.34, "min": 14.3}, "VDD_CPU_SOC_MSS": {"avg": 15.59, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 29.73, "peak": 40.56, "min": 23.24}, "VIN": {"avg": 72.9, "peak": 112.84, "min": 61.97}}, "power_watts_avg": 29.73, "energy_joules_est": 70.27, "sample_count": 17, "duration_seconds": 2.363}, "timestamp": "2026-01-17T15:00:16.762973"}
{"image_index": 182, "image_name": "000000018491.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018491.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2070.766, "latencies_ms": [2070.766], "images_per_second": 0.483, "prompt_tokens": 19, "response_tokens_est": 51, "n_tiles": 6, "output_text": "The image captures a baseball game in action, with players in dark and light-colored uniforms, and a clear blue sky overhead. The lighting is bright, indicating it is daytime, and the field is well-maintained with green grass and brown dirt.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24931.9, "ram_available_mb": 100840.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24932.7, "ram_available_mb": 100839.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.87, "peak": 15.24, "min": 14.2}, "VDD_CPU_SOC_MSS": {"avg": 15.55, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 30.35, "peak": 40.16, "min": 23.64}, "VIN": {"avg": 71.97, "peak": 108.04, "min": 58.09}}, "power_watts_avg": 30.35, "energy_joules_est": 62.86, "sample_count": 16, "duration_seconds": 2.071}, "timestamp": "2026-01-17T15:00:18.844517"}
{"image_index": 183, "image_name": "000000018519.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018519.jpg", "image_width": 515, "image_height": 640, "image_resolution": "515x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2362.638, "latencies_ms": [2362.638], "images_per_second": 0.423, "prompt_tokens": 9, "response_tokens_est": 22, "n_tiles": 12, "output_text": "A skateboarder is performing a trick on a concrete ramp, with his shadow visible on the ground.", "error": null, "sys_before": {"cpu_percent": 44.9, "ram_used_mb": 24902.4, "ram_available_mb": 100869.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 24904.1, "ram_available_mb": 100868.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.25, "peak": 16.76, "min": 13.61}, "VDD_CPU_SOC_MSS": {"avg": 16.05, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 37.2, "peak": 44.49, "min": 28.76}, "VIN": {"avg": 79.79, "peak": 116.8, "min": 59.5}}, "power_watts_avg": 37.2, "energy_joules_est": 87.9, "sample_count": 17, "duration_seconds": 2.363}, "timestamp": "2026-01-17T15:00:21.304605"}
{"image_index": 183, "image_name": "000000018519.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018519.jpg", "image_width": 515, "image_height": 640, "image_resolution": "515x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 3300.251, "latencies_ms": [3300.251], "images_per_second": 0.303, "prompt_tokens": 23, "response_tokens_est": 48, "n_tiles": 12, "output_text": "1. Skateboarder\n2. Helmet\n3. Gloves\n4. Skateboard\n5. Concrete wall\n6. Skateboard ramp\n7. Skateboard\n8. Skateboarder", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24904.1, "ram_available_mb": 100868.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 24905.3, "ram_available_mb": 100866.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.24, "peak": 16.66, "min": 14.12}, "VDD_CPU_SOC_MSS": {"avg": 15.9, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 35.29, "peak": 45.28, "min": 26.39}, "VIN": {"avg": 77.3, "peak": 116.32, "min": 58.62}}, "power_watts_avg": 35.29, "energy_joules_est": 116.48, "sample_count": 25, "duration_seconds": 3.301}, "timestamp": "2026-01-17T15:00:24.615767"}
{"image_index": 183, "image_name": "000000018519.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018519.jpg", "image_width": 515, "image_height": 640, "image_resolution": "515x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 4496.543, "latencies_ms": [4496.543], "images_per_second": 0.222, "prompt_tokens": 27, "response_tokens_est": 83, "n_tiles": 12, "output_text": "The main object in the foreground is a skateboarder performing a trick on a concrete ramp. The skateboarder is positioned near the center of the image, with their body leaning forward and arms extended. The background features a metal fence and a grassy area, with trees and a building visible in the distance. The skateboarder is near the fence, and the grassy area is in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24905.3, "ram_available_mb": 100866.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 24906.5, "ram_available_mb": 100865.7, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.14, "peak": 16.56, "min": 14.02}, "VDD_CPU_SOC_MSS": {"avg": 15.75, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 32.34, "peak": 45.68, "min": 26.0}, "VIN": {"avg": 73.02, "peak": 111.11, "min": 54.46}}, "power_watts_avg": 32.34, "energy_joules_est": 145.43, "sample_count": 35, "duration_seconds": 4.497}, "timestamp": "2026-01-17T15:00:29.118990"}
{"image_index": 183, "image_name": "000000018519.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018519.jpg", "image_width": 515, "image_height": 640, "image_resolution": "515x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 4074.147, "latencies_ms": [4074.147], "images_per_second": 0.245, "prompt_tokens": 21, "response_tokens_est": 71, "n_tiles": 12, "output_text": "The image captures a skateboarder performing a trick on a concrete ramp in an outdoor setting. The skateboarder is wearing a black helmet, black gloves, and black shorts, and is in mid-air, executing a maneuver on the ramp. The background features a metal fence, green grass, and a clear blue sky, indicating a sunny day.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24906.5, "ram_available_mb": 100865.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 24907.7, "ram_available_mb": 100864.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.19, "peak": 16.56, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.67, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 33.04, "peak": 44.89, "min": 26.0}, "VIN": {"avg": 75.42, "peak": 117.77, "min": 62.12}}, "power_watts_avg": 33.04, "energy_joules_est": 134.63, "sample_count": 32, "duration_seconds": 4.075}, "timestamp": "2026-01-17T15:00:33.200447"}
{"image_index": 183, "image_name": "000000018519.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018519.jpg", "image_width": 515, "image_height": 640, "image_resolution": "515x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 4041.669, "latencies_ms": [4041.669], "images_per_second": 0.247, "prompt_tokens": 19, "response_tokens_est": 70, "n_tiles": 12, "output_text": "The image depicts a skateboarder performing a trick on a concrete ramp. The skateboarder is wearing a black helmet, black gloves, and black shorts, and the shadow of the skateboarder is cast on the concrete surface. The lighting is bright, indicating a sunny day, and the skateboarder's shadow is clearly visible.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24907.7, "ram_available_mb": 100864.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 24908.7, "ram_available_mb": 100863.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.21, "peak": 16.66, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.68, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 33.14, "peak": 44.91, "min": 26.0}, "VIN": {"avg": 76.76, "peak": 127.58, "min": 61.89}}, "power_watts_avg": 33.14, "energy_joules_est": 133.95, "sample_count": 31, "duration_seconds": 4.042}, "timestamp": "2026-01-17T15:00:37.248726"}
{"image_index": 184, "image_name": "000000018575.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018575.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 3000.526, "latencies_ms": [3000.526], "images_per_second": 0.333, "prompt_tokens": 9, "response_tokens_est": 41, "n_tiles": 12, "output_text": "The image shows a plate of food consisting of a slice of bread, a side of fries, a small bowl of pickles, a slice of tomato, and a small bowl of mayonnaise.", "error": null, "sys_before": {"cpu_percent": 44.5, "ram_used_mb": 24920.2, "ram_available_mb": 100852.0, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 24917.8, "ram_available_mb": 100854.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.16, "peak": 16.66, "min": 13.39}, "VDD_CPU_SOC_MSS": {"avg": 15.88, "peak": 17.71, "min": 14.17}, "VDD_GPU": {"avg": 34.78, "peak": 44.12, "min": 26.39}, "VIN": {"avg": 75.9, "peak": 104.32, "min": 66.53}}, "power_watts_avg": 34.78, "energy_joules_est": 104.37, "sample_count": 23, "duration_seconds": 3.001}, "timestamp": "2026-01-17T15:00:40.344958"}
{"image_index": 184, "image_name": "000000018575.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018575.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 3090.421, "latencies_ms": [3090.421], "images_per_second": 0.324, "prompt_tokens": 23, "response_tokens_est": 42, "n_tiles": 12, "output_text": "- bread: 1\n- fries: 1\n- tomato: 1\n- pickle: 1\n- salad: 1\n- lemon: 1\n- dip: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24917.8, "ram_available_mb": 100854.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 24917.0, "ram_available_mb": 100855.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.26, "peak": 16.66, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.76, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 35.31, "peak": 45.28, "min": 26.4}, "VIN": {"avg": 78.02, "peak": 139.04, "min": 56.88}}, "power_watts_avg": 35.31, "energy_joules_est": 109.14, "sample_count": 24, "duration_seconds": 3.091}, "timestamp": "2026-01-17T15:00:43.442020"}
{"image_index": 184, "image_name": "000000018575.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018575.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3981.098, "latencies_ms": [3981.098], "images_per_second": 0.251, "prompt_tokens": 27, "response_tokens_est": 68, "n_tiles": 12, "output_text": "The main objects in the image are a plate of food, a glass of water, and a napkin. The plate of food is placed in the foreground, with the glass of water and napkin positioned behind it. The glass of water is on the right side of the plate, while the napkin is on the left side.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24917.0, "ram_available_mb": 100855.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 24918.9, "ram_available_mb": 100853.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.2, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.68, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 33.4, "peak": 45.3, "min": 26.0}, "VIN": {"avg": 79.29, "peak": 130.01, "min": 58.89}}, "power_watts_avg": 33.4, "energy_joules_est": 132.99, "sample_count": 30, "duration_seconds": 3.982}, "timestamp": "2026-01-17T15:00:47.430150"}
{"image_index": 184, "image_name": "000000018575.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018575.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 4974.486, "latencies_ms": [4974.486], "images_per_second": 0.201, "prompt_tokens": 21, "response_tokens_est": 97, "n_tiles": 12, "output_text": "The image depicts a well-arranged dining table with a variety of food items. The setting appears to be a restaurant or a dining area, as indicated by the presence of a tablecloth and a menu card. The food items on the table include a plate of French fries, a slice of bread, a pickle, a small bowl of salad, a glass of water, and a lemon. The overall scene suggests a casual dining experience, possibly in a restaurant setting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24918.9, "ram_available_mb": 100853.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 24919.4, "ram_available_mb": 100852.7, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.15, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.6, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 31.81, "peak": 45.3, "min": 25.6}, "VIN": {"avg": 73.96, "peak": 117.95, "min": 63.37}}, "power_watts_avg": 31.81, "energy_joules_est": 158.26, "sample_count": 38, "duration_seconds": 4.975}, "timestamp": "2026-01-17T15:00:52.415739"}
{"image_index": 184, "image_name": "000000018575.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018575.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3326.689, "latencies_ms": [3326.689], "images_per_second": 0.301, "prompt_tokens": 19, "response_tokens_est": 49, "n_tiles": 12, "output_text": "The image features a plate of food with a white plate, a glass of water, a napkin, and a small bowl of salad. The lighting is bright, and the colors are vibrant, with the food items appearing fresh and inviting.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24919.4, "ram_available_mb": 100852.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 24920.2, "ram_available_mb": 100852.0, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.23, "peak": 16.56, "min": 14.12}, "VDD_CPU_SOC_MSS": {"avg": 15.73, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 34.7, "peak": 45.28, "min": 26.0}, "VIN": {"avg": 74.74, "peak": 112.84, "min": 62.04}}, "power_watts_avg": 34.7, "energy_joules_est": 115.45, "sample_count": 26, "duration_seconds": 3.327}, "timestamp": "2026-01-17T15:00:55.749515"}
{"image_index": 185, "image_name": "000000018737.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018737.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1711.779, "latencies_ms": [1711.779], "images_per_second": 0.584, "prompt_tokens": 9, "response_tokens_est": 41, "n_tiles": 6, "output_text": "A classic red motorcycle with chrome accents is parked on a dusty road, with a wooden saddle and a side case on its back, surrounded by a fence and sparse vegetation under a clear blue sky.", "error": null, "sys_before": {"cpu_percent": 25.9, "ram_used_mb": 24934.3, "ram_available_mb": 100837.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 24935.3, "ram_available_mb": 100836.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.81, "peak": 15.14, "min": 13.92}, "VDD_CPU_SOC_MSS": {"avg": 15.44, "peak": 16.14, "min": 14.17}, "VDD_GPU": {"avg": 32.12, "peak": 40.95, "min": 24.83}, "VIN": {"avg": 73.58, "peak": 108.92, "min": 53.86}}, "power_watts_avg": 32.12, "energy_joules_est": 55.0, "sample_count": 13, "duration_seconds": 1.712}, "timestamp": "2026-01-17T15:00:57.504743"}
{"image_index": 185, "image_name": "000000018737.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018737.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 4181.742, "latencies_ms": [4181.742], "images_per_second": 0.239, "prompt_tokens": 23, "response_tokens_est": 129, "n_tiles": 6, "output_text": "motorcycle: 1\nseat: 1\nsteering wheel: 1\nrear-view mirror: 1\nrear fender: 1\nrear tire: 1\nrear shock absorber: 1\nrear exhaust: 1\nrear bumper: 1\nrear light: 1\nrear license plate: 1\nrear fender: 1\nrear tire: 1\nrear shock absorber: 1\nrear exhaust: 1\nrear bumper: 1\nrear light: 1\nrear license plate: 1\nseat: 1\nsteering wheel: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24935.3, "ram_available_mb": 100836.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 24936.5, "ram_available_mb": 100835.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.08, "peak": 15.44, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.66, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 26.92, "peak": 40.57, "min": 22.85}, "VIN": {"avg": 66.67, "peak": 84.37, "min": 61.6}}, "power_watts_avg": 26.92, "energy_joules_est": 112.58, "sample_count": 33, "duration_seconds": 4.182}, "timestamp": "2026-01-17T15:01:01.692823"}
{"image_index": 185, "image_name": "000000018737.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018737.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2272.747, "latencies_ms": [2272.747], "images_per_second": 0.44, "prompt_tokens": 27, "response_tokens_est": 58, "n_tiles": 6, "output_text": "The main object in the foreground is a red motorcycle with a black seat and chrome details. The motorcycle is parked on a paved road, with a wooden fence and sparse vegetation in the background. The background features a clear blue sky and a few palm trees, indicating a sunny day.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24936.5, "ram_available_mb": 100835.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 24937.0, "ram_available_mb": 100835.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.88, "peak": 15.24, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.35, "peak": 15.75, "min": 14.56}, "VDD_GPU": {"avg": 29.94, "peak": 40.16, "min": 23.25}, "VIN": {"avg": 71.17, "peak": 109.76, "min": 62.56}}, "power_watts_avg": 29.94, "energy_joules_est": 68.06, "sample_count": 17, "duration_seconds": 2.273}, "timestamp": "2026-01-17T15:01:03.971742"}
{"image_index": 185, "image_name": "000000018737.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018737.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1973.023, "latencies_ms": [1973.023], "images_per_second": 0.507, "prompt_tokens": 21, "response_tokens_est": 47, "n_tiles": 6, "output_text": "The image depicts a red motorcycle parked on a dusty road with a clear blue sky in the background. The setting appears to be a desert or arid region, with sparse vegetation and a wooden fence visible in the distance.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24937.0, "ram_available_mb": 100835.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 24941.8, "ram_available_mb": 100830.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.85, "peak": 15.34, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.3, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 30.95, "peak": 40.16, "min": 23.64}, "VIN": {"avg": 70.33, "peak": 96.1, "min": 62.71}}, "power_watts_avg": 30.95, "energy_joules_est": 61.07, "sample_count": 14, "duration_seconds": 1.973}, "timestamp": "2026-01-17T15:01:05.951379"}
{"image_index": 185, "image_name": "000000018737.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018737.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1652.552, "latencies_ms": [1652.552], "images_per_second": 0.605, "prompt_tokens": 19, "response_tokens_est": 36, "n_tiles": 6, "output_text": "The motorcycle in the image is painted in a vibrant red color, with chrome accents and a shiny finish. The lighting is bright and sunny, casting distinct shadows on the road.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24941.8, "ram_available_mb": 100830.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 24942.0, "ram_available_mb": 100830.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.85, "peak": 15.34, "min": 14.3}, "VDD_CPU_SOC_MSS": {"avg": 15.32, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 32.56, "peak": 40.57, "min": 24.82}, "VIN": {"avg": 75.35, "peak": 115.83, "min": 62.06}}, "power_watts_avg": 32.56, "energy_joules_est": 53.82, "sample_count": 12, "duration_seconds": 1.653}, "timestamp": "2026-01-17T15:01:07.610179"}
{"image_index": 186, "image_name": "000000018770.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018770.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1279.803, "latencies_ms": [1279.803], "images_per_second": 0.781, "prompt_tokens": 9, "response_tokens_est": 24, "n_tiles": 6, "output_text": "The image shows a close-up of a man wearing a dark suit and tie, with a white shirt visible underneath.", "error": null, "sys_before": {"cpu_percent": 28.6, "ram_used_mb": 24942.0, "ram_available_mb": 100830.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 24942.0, "ram_available_mb": 100830.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.64, "peak": 15.14, "min": 14.02}, "VDD_CPU_SOC_MSS": {"avg": 15.22, "peak": 16.14, "min": 14.17}, "VDD_GPU": {"avg": 34.13, "peak": 40.56, "min": 27.18}, "VIN": {"avg": 73.41, "peak": 114.26, "min": 57.62}}, "power_watts_avg": 34.13, "energy_joules_est": 43.7, "sample_count": 9, "duration_seconds": 1.28}, "timestamp": "2026-01-17T15:01:08.925965"}
{"image_index": 186, "image_name": "000000018770.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018770.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1542.695, "latencies_ms": [1542.695], "images_per_second": 0.648, "prompt_tokens": 23, "response_tokens_est": 32, "n_tiles": 6, "output_text": "1. Man\n2. Suit\n3. Tie\n4. Shirt\n5. Background\n6. Wall\n7. Light\n8. Person", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24942.0, "ram_available_mb": 100830.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 24942.0, "ram_available_mb": 100830.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.8, "peak": 15.24, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.28, "peak": 15.74, "min": 14.56}, "VDD_GPU": {"avg": 34.02, "peak": 41.34, "min": 25.6}, "VIN": {"avg": 76.99, "peak": 111.68, "min": 63.16}}, "power_watts_avg": 34.02, "energy_joules_est": 52.51, "sample_count": 11, "duration_seconds": 1.543}, "timestamp": "2026-01-17T15:01:10.475150"}
{"image_index": 186, "image_name": "000000018770.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018770.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1914.227, "latencies_ms": [1914.227], "images_per_second": 0.522, "prompt_tokens": 27, "response_tokens_est": 45, "n_tiles": 6, "output_text": "The main object in the image is a man wearing a dark suit and tie. The man is positioned in the foreground, slightly to the right. The background is blurred, making it difficult to discern any specific details.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24942.0, "ram_available_mb": 100830.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 24943.0, "ram_available_mb": 100829.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.84, "peak": 15.24, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.3, "peak": 15.74, "min": 14.56}, "VDD_GPU": {"avg": 31.56, "peak": 40.95, "min": 24.03}, "VIN": {"avg": 72.71, "peak": 111.65, "min": 58.51}}, "power_watts_avg": 31.56, "energy_joules_est": 60.42, "sample_count": 14, "duration_seconds": 1.915}, "timestamp": "2026-01-17T15:01:12.395491"}
{"image_index": 186, "image_name": "000000018770.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018770.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1964.989, "latencies_ms": [1964.989], "images_per_second": 0.509, "prompt_tokens": 21, "response_tokens_est": 47, "n_tiles": 6, "output_text": "The image depicts a man dressed in a formal black suit and tie, standing in an indoor setting. The background is dark, and the man appears to be in a formal or professional environment, possibly a business or office setting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24943.0, "ram_available_mb": 100829.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 24943.2, "ram_available_mb": 100829.0, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.87, "peak": 15.24, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.33, "peak": 15.75, "min": 14.56}, "VDD_GPU": {"avg": 30.94, "peak": 40.56, "min": 23.64}, "VIN": {"avg": 70.34, "peak": 99.53, "min": 58.84}}, "power_watts_avg": 30.94, "energy_joules_est": 60.81, "sample_count": 15, "duration_seconds": 1.965}, "timestamp": "2026-01-17T15:01:14.366293"}
{"image_index": 186, "image_name": "000000018770.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018770.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2304.685, "latencies_ms": [2304.685], "images_per_second": 0.434, "prompt_tokens": 19, "response_tokens_est": 59, "n_tiles": 6, "output_text": "The man in the image is wearing a dark suit with a white shirt and a dark tie. The lighting is soft and even, highlighting the contours of his face and the texture of his suit. The background is dark, providing a stark contrast that makes the man's attire stand out.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24943.2, "ram_available_mb": 100829.0, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 24942.9, "ram_available_mb": 100829.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.88, "peak": 15.24, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.4, "peak": 15.75, "min": 14.56}, "VDD_GPU": {"avg": 29.67, "peak": 40.95, "min": 23.24}, "VIN": {"avg": 73.36, "peak": 116.3, "min": 49.45}}, "power_watts_avg": 29.67, "energy_joules_est": 68.4, "sample_count": 18, "duration_seconds": 2.305}, "timestamp": "2026-01-17T15:01:16.677537"}
{"image_index": 187, "image_name": "000000018833.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018833.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1227.503, "latencies_ms": [1227.503], "images_per_second": 0.815, "prompt_tokens": 9, "response_tokens_est": 23, "n_tiles": 6, "output_text": "A cat is sleeping on a pair of sneakers, with its head resting on the toe of one shoe.", "error": null, "sys_before": {"cpu_percent": 19.0, "ram_used_mb": 24942.9, "ram_available_mb": 100829.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 24943.9, "ram_available_mb": 100828.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.72, "peak": 15.24, "min": 14.0}, "VDD_CPU_SOC_MSS": {"avg": 15.35, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 34.05, "peak": 39.77, "min": 27.57}, "VIN": {"avg": 76.12, "peak": 106.37, "min": 62.77}}, "power_watts_avg": 34.05, "energy_joules_est": 41.82, "sample_count": 9, "duration_seconds": 1.228}, "timestamp": "2026-01-17T15:01:17.939446"}
{"image_index": 187, "image_name": "000000018833.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018833.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 946.043, "latencies_ms": [946.043], "images_per_second": 1.057, "prompt_tokens": 23, "response_tokens_est": 11, "n_tiles": 6, "output_text": "cat: 1\nshoe: 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24943.9, "ram_available_mb": 100828.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.6, "ram_used_mb": 24944.4, "ram_available_mb": 100827.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.91, "peak": 15.44, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.46, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 37.81, "peak": 40.95, "min": 32.7}, "VIN": {"avg": 75.58, "peak": 100.82, "min": 62.74}}, "power_watts_avg": 37.81, "energy_joules_est": 35.78, "sample_count": 7, "duration_seconds": 0.946}, "timestamp": "2026-01-17T15:01:18.892483"}
{"image_index": 187, "image_name": "000000018833.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018833.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2001.772, "latencies_ms": [2001.772], "images_per_second": 0.5, "prompt_tokens": 27, "response_tokens_est": 50, "n_tiles": 6, "output_text": "The main object in the foreground is a cat lying on a shoe. The shoe is positioned near the cat, with its toe pointing towards the left side of the image. The background is out of focus, emphasizing the cat and the shoe.", "error": null, "sys_before": {"cpu_percent": 16.7, "ram_used_mb": 24944.4, "ram_available_mb": 100827.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 24944.2, "ram_available_mb": 100828.0, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.04, "peak": 15.44, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.62, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 32.51, "peak": 42.53, "min": 24.04}, "VIN": {"avg": 71.08, "peak": 92.56, "min": 62.42}}, "power_watts_avg": 32.51, "energy_joules_est": 65.1, "sample_count": 15, "duration_seconds": 2.002}, "timestamp": "2026-01-17T15:01:20.901000"}
{"image_index": 187, "image_name": "000000018833.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018833.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2456.624, "latencies_ms": [2456.624], "images_per_second": 0.407, "prompt_tokens": 21, "response_tokens_est": 67, "n_tiles": 6, "output_text": "The image depicts a serene scene of a cat resting on a pair of sneakers. The cat appears to be sleeping peacefully, with its head resting on the white sole of the sneakers. The setting is indoors, likely in a home or a cozy space, with a soft, textured wall in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24944.2, "ram_available_mb": 100828.0, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 24944.9, "ram_available_mb": 100827.3, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.09, "peak": 15.44, "min": 14.43}, "VDD_CPU_SOC_MSS": {"avg": 15.64, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 29.69, "peak": 40.97, "min": 23.64}, "VIN": {"avg": 73.47, "peak": 121.51, "min": 59.69}}, "power_watts_avg": 29.69, "energy_joules_est": 72.95, "sample_count": 19, "duration_seconds": 2.457}, "timestamp": "2026-01-17T15:01:23.364065"}
{"image_index": 187, "image_name": "000000018833.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018833.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1915.896, "latencies_ms": [1915.896], "images_per_second": 0.522, "prompt_tokens": 19, "response_tokens_est": 47, "n_tiles": 6, "output_text": "The image features a cat lying on a pair of sneakers, which are predominantly gray with black accents. The lighting is soft and natural, casting gentle shadows that highlight the cat's relaxed posture and the texture of the sneakers.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24944.7, "ram_available_mb": 100827.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 24944.9, "ram_available_mb": 100827.3, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.06, "peak": 15.54, "min": 14.43}, "VDD_CPU_SOC_MSS": {"avg": 15.61, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 31.4, "peak": 40.16, "min": 24.43}, "VIN": {"avg": 71.88, "peak": 104.14, "min": 57.58}}, "power_watts_avg": 31.4, "energy_joules_est": 60.17, "sample_count": 14, "duration_seconds": 1.916}, "timestamp": "2026-01-17T15:01:25.286004"}
{"image_index": 188, "image_name": "000000018837.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018837.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2970.863, "latencies_ms": [2970.863], "images_per_second": 0.337, "prompt_tokens": 9, "response_tokens_est": 40, "n_tiles": 12, "output_text": "A green garbage truck is parked on the street, with two workers wearing safety vests and helmets, one of whom is standing next to the truck, and another is sitting inside the truck.", "error": null, "sys_before": {"cpu_percent": 31.7, "ram_used_mb": 24922.1, "ram_available_mb": 100850.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 24923.5, "ram_available_mb": 100848.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.25, "peak": 16.66, "min": 13.92}, "VDD_CPU_SOC_MSS": {"avg": 15.87, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 34.78, "peak": 44.1, "min": 26.39}, "VIN": {"avg": 79.6, "peak": 117.28, "min": 59.35}}, "power_watts_avg": 34.78, "energy_joules_est": 103.34, "sample_count": 23, "duration_seconds": 2.971}, "timestamp": "2026-01-17T15:01:28.318488"}
{"image_index": 188, "image_name": "000000018837.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018837.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 3504.788, "latencies_ms": [3504.788], "images_per_second": 0.285, "prompt_tokens": 23, "response_tokens_est": 54, "n_tiles": 12, "output_text": "1. Green truck\n2. Person in green vest\n3. Person in green hat\n4. Person in green jacket\n5. Person in green jacket\n6. Person in green jacket\n7. Person in green jacket\n8. Person in green jacket", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24923.5, "ram_available_mb": 100848.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 24924.8, "ram_available_mb": 100847.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.21, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.72, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 34.18, "peak": 44.89, "min": 26.0}, "VIN": {"avg": 74.64, "peak": 115.16, "min": 51.75}}, "power_watts_avg": 34.18, "energy_joules_est": 119.81, "sample_count": 27, "duration_seconds": 3.505}, "timestamp": "2026-01-17T15:01:31.829821"}
{"image_index": 188, "image_name": "000000018837.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018837.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 4217.487, "latencies_ms": [4217.487], "images_per_second": 0.237, "prompt_tokens": 27, "response_tokens_est": 75, "n_tiles": 12, "output_text": "The main object in the foreground is a green truck with a person wearing a safety vest and a cap. The truck is parked on the street, and the person is standing near the front of the truck. In the background, there is a building with a sign that reads \"Power Exchange.\" The truck's license plate is visible, and there is another vehicle parked nearby.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24924.8, "ram_available_mb": 100847.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 24925.7, "ram_available_mb": 100846.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.18, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.69, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 32.76, "peak": 45.68, "min": 26.0}, "VIN": {"avg": 76.42, "peak": 127.18, "min": 58.38}}, "power_watts_avg": 32.76, "energy_joules_est": 138.18, "sample_count": 33, "duration_seconds": 4.218}, "timestamp": "2026-01-17T15:01:36.054339"}
{"image_index": 188, "image_name": "000000018837.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018837.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3638.268, "latencies_ms": [3638.268], "images_per_second": 0.275, "prompt_tokens": 21, "response_tokens_est": 58, "n_tiles": 12, "output_text": "The image depicts a scene on a city street where a green garbage truck is parked. Two individuals, one wearing a green vest and the other in a yellow cap, are seen interacting with the truck. The setting appears to be a city environment with buildings and trees in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24925.7, "ram_available_mb": 100846.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 24926.7, "ram_available_mb": 100845.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.2, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.73, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 33.92, "peak": 44.89, "min": 26.0}, "VIN": {"avg": 76.26, "peak": 111.24, "min": 56.02}}, "power_watts_avg": 33.92, "energy_joules_est": 123.42, "sample_count": 28, "duration_seconds": 3.639}, "timestamp": "2026-01-17T15:01:39.699342"}
{"image_index": 188, "image_name": "000000018837.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018837.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2925.125, "latencies_ms": [2925.125], "images_per_second": 0.342, "prompt_tokens": 19, "response_tokens_est": 37, "n_tiles": 12, "output_text": "The image features a green and white truck with a red and white striped front bumper. The truck is parked on a street with a clear sky overhead, indicating a sunny day.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24926.7, "ram_available_mb": 100845.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 24928.2, "ram_available_mb": 100844.0, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.25, "peak": 16.66, "min": 14.12}, "VDD_CPU_SOC_MSS": {"avg": 15.8, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 35.91, "peak": 44.89, "min": 26.79}, "VIN": {"avg": 75.85, "peak": 110.7, "min": 59.16}}, "power_watts_avg": 35.91, "energy_joules_est": 105.06, "sample_count": 22, "duration_seconds": 2.926}, "timestamp": "2026-01-17T15:01:42.631200"}
{"image_index": 189, "image_name": "000000019042.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019042.jpg", "image_width": 640, "image_height": 371, "image_resolution": "640x371", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1166.434, "latencies_ms": [1166.434], "images_per_second": 0.857, "prompt_tokens": 9, "response_tokens_est": 21, "n_tiles": 6, "output_text": "A solitary bird stands on a rocky shoreline, surrounded by a calm river with gentle ripples.", "error": null, "sys_before": {"cpu_percent": 40.7, "ram_used_mb": 24943.8, "ram_available_mb": 100828.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 24944.6, "ram_available_mb": 100827.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.6, "peak": 15.14, "min": 13.82}, "VDD_CPU_SOC_MSS": {"avg": 15.4, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 35.9, "peak": 40.97, "min": 29.54}, "VIN": {"avg": 78.91, "peak": 120.93, "min": 62.62}}, "power_watts_avg": 35.9, "energy_joules_est": 41.89, "sample_count": 8, "duration_seconds": 1.167}, "timestamp": "2026-01-17T15:01:43.857218"}
{"image_index": 189, "image_name": "000000019042.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019042.jpg", "image_width": 640, "image_height": 371, "image_resolution": "640x371", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1772.1, "latencies_ms": [1772.1], "images_per_second": 0.564, "prompt_tokens": 23, "response_tokens_est": 40, "n_tiles": 6, "output_text": "object: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24944.6, "ram_available_mb": 100827.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 24944.3, "ram_available_mb": 100827.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.84, "peak": 15.24, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.47, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 33.0, "peak": 41.36, "min": 24.43}, "VIN": {"avg": 72.5, "peak": 112.54, "min": 63.13}}, "power_watts_avg": 33.0, "energy_joules_est": 58.49, "sample_count": 13, "duration_seconds": 1.773}, "timestamp": "2026-01-17T15:01:45.639661"}
{"image_index": 189, "image_name": "000000019042.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019042.jpg", "image_width": 640, "image_height": 371, "image_resolution": "640x371", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2250.756, "latencies_ms": [2250.756], "images_per_second": 0.444, "prompt_tokens": 27, "response_tokens_est": 57, "n_tiles": 6, "output_text": "The main objects in the image are a river, a bridge, and a bird standing on the riverbank. The bird is positioned near the riverbank, while the bridge is in the background. The foreground consists of the river and the bird, while the background features the bridge.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24944.3, "ram_available_mb": 100827.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 24945.3, "ram_available_mb": 100826.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.85, "peak": 15.24, "min": 14.2}, "VDD_CPU_SOC_MSS": {"avg": 15.54, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 30.24, "peak": 40.57, "min": 23.24}, "VIN": {"avg": 70.77, "peak": 115.39, "min": 62.56}}, "power_watts_avg": 30.24, "energy_joules_est": 68.08, "sample_count": 17, "duration_seconds": 2.251}, "timestamp": "2026-01-17T15:01:47.896777"}
{"image_index": 189, "image_name": "000000019042.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019042.jpg", "image_width": 640, "image_height": 371, "image_resolution": "640x371", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2194.193, "latencies_ms": [2194.193], "images_per_second": 0.456, "prompt_tokens": 21, "response_tokens_est": 55, "n_tiles": 6, "output_text": "The image depicts a serene river scene with a rocky shoreline in the foreground. A solitary bird is standing on the rocks, seemingly surveying its surroundings. The setting is peaceful, with a bridge visible in the background, suggesting a location near a populated area.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24945.3, "ram_available_mb": 100826.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 24945.3, "ram_available_mb": 100826.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.86, "peak": 15.24, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.59, "peak": 16.14, "min": 14.57}, "VDD_GPU": {"avg": 29.91, "peak": 40.16, "min": 23.24}, "VIN": {"avg": 71.31, "peak": 115.82, "min": 59.42}}, "power_watts_avg": 29.91, "energy_joules_est": 65.64, "sample_count": 17, "duration_seconds": 2.195}, "timestamp": "2026-01-17T15:01:50.097494"}
{"image_index": 189, "image_name": "000000019042.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019042.jpg", "image_width": 640, "image_height": 371, "image_resolution": "640x371", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2246.804, "latencies_ms": [2246.804], "images_per_second": 0.445, "prompt_tokens": 19, "response_tokens_est": 57, "n_tiles": 6, "output_text": "The image depicts a serene river scene with a clear blue sky overhead. The lighting is bright and natural, casting soft shadows on the riverbanks. The river is surrounded by lush greenery, and the water appears calm with a few small rocks scattered along the shore.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24945.3, "ram_available_mb": 100826.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 24945.8, "ram_available_mb": 100826.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.85, "peak": 15.24, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.56, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 30.07, "peak": 40.95, "min": 23.24}, "VIN": {"avg": 69.75, "peak": 106.62, "min": 54.71}}, "power_watts_avg": 30.07, "energy_joules_est": 67.57, "sample_count": 17, "duration_seconds": 2.247}, "timestamp": "2026-01-17T15:01:52.350636"}
{"image_index": 190, "image_name": "000000019109.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019109.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1464.518, "latencies_ms": [1464.518], "images_per_second": 0.683, "prompt_tokens": 9, "response_tokens_est": 32, "n_tiles": 6, "output_text": "A row of parked motorcycles is lined up in front of a building with a red awning, while a few people are seen walking around the area.", "error": null, "sys_before": {"cpu_percent": 34.8, "ram_used_mb": 24945.8, "ram_available_mb": 100826.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 24946.5, "ram_available_mb": 100825.7, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.72, "peak": 15.14, "min": 13.79}, "VDD_CPU_SOC_MSS": {"avg": 15.42, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 32.59, "peak": 40.16, "min": 25.6}, "VIN": {"avg": 77.81, "peak": 119.72, "min": 59.46}}, "power_watts_avg": 32.59, "energy_joules_est": 47.74, "sample_count": 11, "duration_seconds": 1.465}, "timestamp": "2026-01-17T15:01:53.877674"}
{"image_index": 190, "image_name": "000000019109.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019109.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2441.024, "latencies_ms": [2441.024], "images_per_second": 0.41, "prompt_tokens": 23, "response_tokens_est": 64, "n_tiles": 6, "output_text": "1. Motorcycles: 8\n2. Motorcycle: 1\n3. Motorcycle: 1\n4. Motorcycle: 1\n5. Motorcycle: 1\n6. Motorcycle: 1\n7. Motorcycle: 1\n8. Motorcycle: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24946.5, "ram_available_mb": 100825.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 24947.2, "ram_available_mb": 100824.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.9, "peak": 15.34, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.54, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 29.46, "peak": 40.97, "min": 23.24}, "VIN": {"avg": 69.97, "peak": 103.17, "min": 54.82}}, "power_watts_avg": 29.46, "energy_joules_est": 71.93, "sample_count": 19, "duration_seconds": 2.441}, "timestamp": "2026-01-17T15:01:56.325156"}
{"image_index": 190, "image_name": "000000019109.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019109.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2275.318, "latencies_ms": [2275.318], "images_per_second": 0.439, "prompt_tokens": 27, "response_tokens_est": 58, "n_tiles": 6, "output_text": "The main objects in the image are a row of parked motorcycles and a building with a red awning. The motorcycles are parked in the foreground, with their fronts facing the camera. The building with the red awning is located in the background, slightly to the right.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24947.2, "ram_available_mb": 100824.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 24947.7, "ram_available_mb": 100824.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.88, "peak": 15.14, "min": 14.3}, "VDD_CPU_SOC_MSS": {"avg": 15.31, "peak": 15.74, "min": 14.56}, "VDD_GPU": {"avg": 29.74, "peak": 40.97, "min": 23.25}, "VIN": {"avg": 68.93, "peak": 90.37, "min": 57.65}}, "power_watts_avg": 29.74, "energy_joules_est": 67.68, "sample_count": 18, "duration_seconds": 2.276}, "timestamp": "2026-01-17T15:01:58.606864"}
{"image_index": 190, "image_name": "000000019109.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019109.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2359.971, "latencies_ms": [2359.971], "images_per_second": 0.424, "prompt_tokens": 21, "response_tokens_est": 61, "n_tiles": 6, "output_text": "The image depicts a street scene in a city, likely Paris, with a row of parked motorcycles lining the curb. The setting is urban, with buildings and shops in the background. People are seen walking or standing near the motorcycles, and there are posters and advertisements visible on the buildings.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24947.7, "ram_available_mb": 100824.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 24948.9, "ram_available_mb": 100823.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.87, "peak": 15.14, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.44, "peak": 15.75, "min": 14.56}, "VDD_GPU": {"avg": 29.76, "peak": 41.34, "min": 23.25}, "VIN": {"avg": 68.87, "peak": 90.11, "min": 58.51}}, "power_watts_avg": 29.76, "energy_joules_est": 70.25, "sample_count": 18, "duration_seconds": 2.361}, "timestamp": "2026-01-17T15:02:00.974344"}
{"image_index": 190, "image_name": "000000019109.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019109.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2163.797, "latencies_ms": [2163.797], "images_per_second": 0.462, "prompt_tokens": 19, "response_tokens_est": 54, "n_tiles": 6, "output_text": "The image depicts a row of parked motorcycles lined up against a backdrop of a historic building. The scene is illuminated by natural daylight, casting soft shadows on the pavement. The motorcycles are predominantly black, with some featuring sleek designs and modern features.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24948.9, "ram_available_mb": 100823.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 24949.2, "ram_available_mb": 100823.0, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.87, "peak": 15.24, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.35, "peak": 15.75, "min": 14.56}, "VDD_GPU": {"avg": 30.17, "peak": 40.57, "min": 23.25}, "VIN": {"avg": 73.98, "peak": 108.45, "min": 60.65}}, "power_watts_avg": 30.17, "energy_joules_est": 65.3, "sample_count": 17, "duration_seconds": 2.164}, "timestamp": "2026-01-17T15:02:03.143793"}
{"image_index": 191, "image_name": "000000019221.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019221.jpg", "image_width": 640, "image_height": 478, "image_resolution": "640x478", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2729.483, "latencies_ms": [2729.483], "images_per_second": 0.366, "prompt_tokens": 9, "response_tokens_est": 33, "n_tiles": 12, "output_text": "The image shows a close-up of a person's hand holding a small, green, leafy vegetable, possibly a broccoli, with a blurred background.", "error": null, "sys_before": {"cpu_percent": 45.3, "ram_used_mb": 24919.4, "ram_available_mb": 100852.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 24920.9, "ram_available_mb": 100851.3, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.16, "peak": 16.66, "min": 13.29}, "VDD_CPU_SOC_MSS": {"avg": 15.86, "peak": 17.71, "min": 14.17}, "VDD_GPU": {"avg": 35.32, "peak": 43.73, "min": 27.18}, "VIN": {"avg": 75.01, "peak": 112.83, "min": 61.07}}, "power_watts_avg": 35.32, "energy_joules_est": 96.42, "sample_count": 21, "duration_seconds": 2.73}, "timestamp": "2026-01-17T15:02:05.970577"}
{"image_index": 191, "image_name": "000000019221.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019221.jpg", "image_width": 640, "image_height": 478, "image_resolution": "640x478", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2043.342, "latencies_ms": [2043.342], "images_per_second": 0.489, "prompt_tokens": 23, "response_tokens_est": 11, "n_tiles": 12, "output_text": "broccoli: 1\npot: 1", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24920.9, "ram_available_mb": 100851.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.0, "ram_used_mb": 24921.4, "ram_available_mb": 100850.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.3, "peak": 16.56, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.94, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 39.11, "peak": 45.68, "min": 31.51}, "VIN": {"avg": 78.75, "peak": 109.13, "min": 60.61}}, "power_watts_avg": 39.11, "energy_joules_est": 79.94, "sample_count": 16, "duration_seconds": 2.044}, "timestamp": "2026-01-17T15:02:08.020846"}
{"image_index": 191, "image_name": "000000019221.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019221.jpg", "image_width": 640, "image_height": 478, "image_resolution": "640x478", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3880.919, "latencies_ms": [3880.919], "images_per_second": 0.258, "prompt_tokens": 27, "response_tokens_est": 65, "n_tiles": 12, "output_text": "The main object in the foreground is a person's hand holding a broccoli. The broccoli is positioned in the center of the image, with the person's hand slightly to the right and in the foreground. The background is blurred, with a dark, indistinct area that makes the broccoli stand out.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24921.4, "ram_available_mb": 100850.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 24922.1, "ram_available_mb": 100850.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.22, "peak": 16.66, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.73, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 33.83, "peak": 46.46, "min": 26.0}, "VIN": {"avg": 77.2, "peak": 112.67, "min": 61.8}}, "power_watts_avg": 33.83, "energy_joules_est": 131.3, "sample_count": 31, "duration_seconds": 3.881}, "timestamp": "2026-01-17T15:02:11.908534"}
{"image_index": 191, "image_name": "000000019221.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019221.jpg", "image_width": 640, "image_height": 478, "image_resolution": "640x478", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3262.15, "latencies_ms": [3262.15], "images_per_second": 0.307, "prompt_tokens": 21, "response_tokens_est": 47, "n_tiles": 12, "output_text": "The image shows a close-up of a person's hand holding a small, green, broccoli-like vegetable. The background is blurred, but it appears to be an indoor setting with a dark, possibly wooden or metal surface.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24922.1, "ram_available_mb": 100850.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 24922.9, "ram_available_mb": 100849.3, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.24, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.71, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 34.79, "peak": 44.91, "min": 26.0}, "VIN": {"avg": 77.26, "peak": 109.49, "min": 67.12}}, "power_watts_avg": 34.79, "energy_joules_est": 113.5, "sample_count": 25, "duration_seconds": 3.262}, "timestamp": "2026-01-17T15:02:15.177594"}
{"image_index": 191, "image_name": "000000019221.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019221.jpg", "image_width": 640, "image_height": 478, "image_resolution": "640x478", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3840.064, "latencies_ms": [3840.064], "images_per_second": 0.26, "prompt_tokens": 19, "response_tokens_est": 64, "n_tiles": 12, "output_text": "The image features a close-up of a person's hand holding a green broccoli floret. The broccoli is bright green with small, tightly packed florets, and the hand is holding it with a slightly wrinkled skin texture. The lighting is natural, suggesting the photo was taken outdoors during the day.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24922.9, "ram_available_mb": 100849.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 24923.3, "ram_available_mb": 100848.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.19, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.65, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 33.43, "peak": 45.28, "min": 26.0}, "VIN": {"avg": 79.07, "peak": 121.11, "min": 66.0}}, "power_watts_avg": 33.43, "energy_joules_est": 128.38, "sample_count": 30, "duration_seconds": 3.84}, "timestamp": "2026-01-17T15:02:19.027493"}
{"image_index": 192, "image_name": "000000019402.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019402.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2460.246, "latencies_ms": [2460.246], "images_per_second": 0.406, "prompt_tokens": 9, "response_tokens_est": 25, "n_tiles": 12, "output_text": "The image shows a person wearing a green hat and jacket, with their mouth open as if they are shouting or speaking.", "error": null, "sys_before": {"cpu_percent": 40.4, "ram_used_mb": 24921.6, "ram_available_mb": 100850.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.5, "ram_used_mb": 24921.1, "ram_available_mb": 100851.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.16, "peak": 16.76, "min": 13.29}, "VDD_CPU_SOC_MSS": {"avg": 15.87, "peak": 17.71, "min": 14.17}, "VDD_GPU": {"avg": 36.5, "peak": 44.1, "min": 27.97}, "VIN": {"avg": 76.27, "peak": 106.73, "min": 58.83}}, "power_watts_avg": 36.5, "energy_joules_est": 89.81, "sample_count": 19, "duration_seconds": 2.461}, "timestamp": "2026-01-17T15:02:21.591549"}
{"image_index": 192, "image_name": "000000019402.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019402.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2745.118, "latencies_ms": [2745.118], "images_per_second": 0.364, "prompt_tokens": 23, "response_tokens_est": 32, "n_tiles": 12, "output_text": "1. Woman\n2. Hat\n3. Light\n4. Background\n5. Person\n6. Light\n7. Person\n8. Light", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24921.1, "ram_available_mb": 100851.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 24920.1, "ram_available_mb": 100852.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.33, "peak": 16.66, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.84, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 36.7, "peak": 45.28, "min": 27.18}, "VIN": {"avg": 76.76, "peak": 117.36, "min": 59.35}}, "power_watts_avg": 36.7, "energy_joules_est": 100.76, "sample_count": 21, "duration_seconds": 2.746}, "timestamp": "2026-01-17T15:02:24.343940"}
{"image_index": 192, "image_name": "000000019402.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019402.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3329.452, "latencies_ms": [3329.452], "images_per_second": 0.3, "prompt_tokens": 27, "response_tokens_est": 49, "n_tiles": 12, "output_text": "The main object in the foreground is a person wearing a green hat. The person's face is partially visible, with their mouth open. The background is out of focus, but there appears to be a lamp and some indistinct objects.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24920.1, "ram_available_mb": 100852.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 24920.8, "ram_available_mb": 100851.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.24, "peak": 16.66, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.76, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 34.74, "peak": 45.3, "min": 26.0}, "VIN": {"avg": 75.5, "peak": 105.83, "min": 58.91}}, "power_watts_avg": 34.74, "energy_joules_est": 115.68, "sample_count": 26, "duration_seconds": 3.33}, "timestamp": "2026-01-17T15:02:27.680197"}
{"image_index": 192, "image_name": "000000019402.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019402.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3024.566, "latencies_ms": [3024.566], "images_per_second": 0.331, "prompt_tokens": 21, "response_tokens_est": 40, "n_tiles": 12, "output_text": "The image depicts a person wearing a green jacket, with their face partially obscured by the jacket. The background is blurred, but it appears to be an indoor setting with warm lighting.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 24920.8, "ram_available_mb": 100851.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 24922.0, "ram_available_mb": 100850.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.27, "peak": 16.66, "min": 14.12}, "VDD_CPU_SOC_MSS": {"avg": 15.9, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 35.6, "peak": 44.89, "min": 26.39}, "VIN": {"avg": 76.8, "peak": 108.54, "min": 57.39}}, "power_watts_avg": 35.6, "energy_joules_est": 107.69, "sample_count": 23, "duration_seconds": 3.025}, "timestamp": "2026-01-17T15:02:30.711391"}
{"image_index": 192, "image_name": "000000019402.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019402.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3230.774, "latencies_ms": [3230.774], "images_per_second": 0.31, "prompt_tokens": 19, "response_tokens_est": 46, "n_tiles": 12, "output_text": "The image features a person wearing a green jacket with a fur-lined hood, creating a striking contrast against the warm, yellowish lighting. The background is blurred, emphasizing the subject and the texture of the jacket.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 24922.0, "ram_available_mb": 100850.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 24922.7, "ram_available_mb": 100849.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.25, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.83, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 34.88, "peak": 44.91, "min": 26.39}, "VIN": {"avg": 77.65, "peak": 110.97, "min": 63.51}}, "power_watts_avg": 34.88, "energy_joules_est": 112.71, "sample_count": 25, "duration_seconds": 3.231}, "timestamp": "2026-01-17T15:02:33.949806"}
{"image_index": 193, "image_name": "000000019432.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019432.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2130.102, "latencies_ms": [2130.102], "images_per_second": 0.469, "prompt_tokens": 9, "response_tokens_est": 15, "n_tiles": 12, "output_text": "A tennis player is preparing to serve a ball on a blue court.", "error": null, "sys_before": {"cpu_percent": 39.1, "ram_used_mb": 24919.0, "ram_available_mb": 100853.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.2, "ram_used_mb": 24921.4, "ram_available_mb": 100850.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.2, "peak": 16.66, "min": 13.39}, "VDD_CPU_SOC_MSS": {"avg": 16.02, "peak": 17.71, "min": 14.18}, "VDD_GPU": {"avg": 38.01, "peak": 44.1, "min": 30.73}, "VIN": {"avg": 80.55, "peak": 114.52, "min": 65.34}}, "power_watts_avg": 38.01, "energy_joules_est": 80.98, "sample_count": 16, "duration_seconds": 2.131}, "timestamp": "2026-01-17T15:02:36.175262"}
{"image_index": 193, "image_name": "000000019432.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019432.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2988.374, "latencies_ms": [2988.374], "images_per_second": 0.335, "prompt_tokens": 23, "response_tokens_est": 39, "n_tiles": 12, "output_text": "1. Tennis player\n2. Tennis racket\n3. Tennis ball\n4. Court\n5. Chairs\n6. Stadium\n7. Blue surface\n8. White lines", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24921.4, "ram_available_mb": 100850.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 24922.4, "ram_available_mb": 100849.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.28, "peak": 16.66, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.97, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 36.18, "peak": 45.68, "min": 26.39}, "VIN": {"avg": 75.69, "peak": 117.02, "min": 54.24}}, "power_watts_avg": 36.18, "energy_joules_est": 108.14, "sample_count": 23, "duration_seconds": 2.989}, "timestamp": "2026-01-17T15:02:39.170195"}
{"image_index": 193, "image_name": "000000019432.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019432.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3748.995, "latencies_ms": [3748.995], "images_per_second": 0.267, "prompt_tokens": 27, "response_tokens_est": 61, "n_tiles": 12, "output_text": "The main object in the foreground is a tennis player holding a tennis racket. The player is standing on a blue tennis court with white lines marking the playing area. In the background, there are empty white chairs arranged in a row, suggesting that the court is set up for an event or match.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24922.4, "ram_available_mb": 100849.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 24922.6, "ram_available_mb": 100849.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.2, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.8, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 33.79, "peak": 45.68, "min": 26.0}, "VIN": {"avg": 78.67, "peak": 116.78, "min": 65.55}}, "power_watts_avg": 33.79, "energy_joules_est": 126.69, "sample_count": 29, "duration_seconds": 3.749}, "timestamp": "2026-01-17T15:02:42.926097"}
{"image_index": 193, "image_name": "000000019432.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019432.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3539.858, "latencies_ms": [3539.858], "images_per_second": 0.282, "prompt_tokens": 21, "response_tokens_est": 55, "n_tiles": 12, "output_text": "The image depicts a tennis player on a blue court, preparing to serve the ball. The player is dressed in a white shirt and black shorts, with a tennis racket in hand. The court is surrounded by white chairs, and the background shows a green wall.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24922.6, "ram_available_mb": 100849.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 24922.9, "ram_available_mb": 100849.3, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.22, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.75, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 34.16, "peak": 44.49, "min": 26.0}, "VIN": {"avg": 79.04, "peak": 121.47, "min": 63.24}}, "power_watts_avg": 34.16, "energy_joules_est": 120.94, "sample_count": 27, "duration_seconds": 3.54}, "timestamp": "2026-01-17T15:02:46.472956"}
{"image_index": 193, "image_name": "000000019432.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019432.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2657.896, "latencies_ms": [2657.896], "images_per_second": 0.376, "prompt_tokens": 19, "response_tokens_est": 29, "n_tiles": 12, "output_text": "The tennis court is a vibrant blue, with white lines marking the playing area. The lighting is bright and natural, suggesting it is daytime.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24922.9, "ram_available_mb": 100849.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 24923.9, "ram_available_mb": 100848.3, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.24, "peak": 16.66, "min": 14.12}, "VDD_CPU_SOC_MSS": {"avg": 15.8, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 36.91, "peak": 45.68, "min": 27.57}, "VIN": {"avg": 76.76, "peak": 112.07, "min": 66.5}}, "power_watts_avg": 36.91, "energy_joules_est": 98.12, "sample_count": 20, "duration_seconds": 2.658}, "timestamp": "2026-01-17T15:02:49.138949"}
{"image_index": 194, "image_name": "000000019742.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019742.jpg", "image_width": 500, "image_height": 374, "image_resolution": "500x374", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 3099.86, "latencies_ms": [3099.86], "images_per_second": 0.323, "prompt_tokens": 9, "response_tokens_est": 44, "n_tiles": 12, "output_text": "The image shows a candle with a lit wick on a white plate, placed on a wooden table, with a glass of amber-colored liquid beside it, and a string of white lights hanging above the table.", "error": null, "sys_before": {"cpu_percent": 44.6, "ram_used_mb": 24920.2, "ram_available_mb": 100852.0, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 24921.1, "ram_available_mb": 100851.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.16, "peak": 16.66, "min": 13.51}, "VDD_CPU_SOC_MSS": {"avg": 15.93, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 34.43, "peak": 44.49, "min": 26.39}, "VIN": {"avg": 75.92, "peak": 99.61, "min": 61.46}}, "power_watts_avg": 34.43, "energy_joules_est": 106.74, "sample_count": 24, "duration_seconds": 3.1}, "timestamp": "2026-01-17T15:02:52.334439"}
{"image_index": 194, "image_name": "000000019742.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019742.jpg", "image_width": 500, "image_height": 374, "image_resolution": "500x374", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 3030.293, "latencies_ms": [3030.293], "images_per_second": 0.33, "prompt_tokens": 23, "response_tokens_est": 40, "n_tiles": 12, "output_text": "object: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24921.1, "ram_available_mb": 100851.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 24921.6, "ram_available_mb": 100850.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.25, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.88, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 35.72, "peak": 45.28, "min": 26.39}, "VIN": {"avg": 77.79, "peak": 126.74, "min": 66.68}}, "power_watts_avg": 35.72, "energy_joules_est": 108.26, "sample_count": 23, "duration_seconds": 3.031}, "timestamp": "2026-01-17T15:02:55.371706"}
{"image_index": 194, "image_name": "000000019742.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019742.jpg", "image_width": 500, "image_height": 374, "image_resolution": "500x374", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 4393.276, "latencies_ms": [4393.276], "images_per_second": 0.228, "prompt_tokens": 27, "response_tokens_est": 80, "n_tiles": 12, "output_text": "The main objects in the image are a candle and a glass of amber-colored liquid. The candle is placed on a small, round, green plate near the left side of the image, while the glass of liquid is positioned in the foreground, slightly to the right. The candle and the glass are the central focus of the image, with the plate serving as a supporting element in the foreground.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24921.6, "ram_available_mb": 100850.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 24921.6, "ram_available_mb": 100850.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.19, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.83, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 32.53, "peak": 44.89, "min": 26.0}, "VIN": {"avg": 77.37, "peak": 126.09, "min": 61.4}}, "power_watts_avg": 32.53, "energy_joules_est": 142.92, "sample_count": 34, "duration_seconds": 4.394}, "timestamp": "2026-01-17T15:02:59.771503"}
{"image_index": 194, "image_name": "000000019742.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019742.jpg", "image_width": 500, "image_height": 374, "image_resolution": "500x374", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 4073.145, "latencies_ms": [4073.145], "images_per_second": 0.246, "prompt_tokens": 21, "response_tokens_est": 70, "n_tiles": 12, "output_text": "The image depicts a cozy indoor setting with a warm, inviting ambiance. A glass of amber-colored liquid, possibly tea or a warm beverage, is placed on a small, round glass plate on a wooden table. The table is adorned with a string of small, white, string lights that add a festive touch to the scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24921.6, "ram_available_mb": 100850.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 24921.8, "ram_available_mb": 100850.3, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.93, "peak": 16.36, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.56, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 33.15, "peak": 45.3, "min": 25.6}, "VIN": {"avg": 76.52, "peak": 132.27, "min": 52.46}}, "power_watts_avg": 33.15, "energy_joules_est": 135.04, "sample_count": 31, "duration_seconds": 4.074}, "timestamp": "2026-01-17T15:03:03.851244"}
{"image_index": 194, "image_name": "000000019742.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019742.jpg", "image_width": 500, "image_height": 374, "image_resolution": "500x374", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3204.32, "latencies_ms": [3204.32], "images_per_second": 0.312, "prompt_tokens": 19, "response_tokens_est": 45, "n_tiles": 12, "output_text": "The image features a candle with a white base and a lit wick, placed on a green glass plate. The candle is surrounded by a string of small, white, string lights, creating a warm and cozy atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24921.8, "ram_available_mb": 100850.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 24921.8, "ram_available_mb": 100850.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.94, "peak": 16.36, "min": 13.92}, "VDD_CPU_SOC_MSS": {"avg": 15.59, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 34.82, "peak": 45.3, "min": 26.0}, "VIN": {"avg": 75.82, "peak": 127.23, "min": 57.25}}, "power_watts_avg": 34.82, "energy_joules_est": 111.59, "sample_count": 25, "duration_seconds": 3.205}, "timestamp": "2026-01-17T15:03:07.062506"}
{"image_index": 195, "image_name": "000000019786.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019786.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2194.259, "latencies_ms": [2194.259], "images_per_second": 0.456, "prompt_tokens": 9, "response_tokens_est": 17, "n_tiles": 12, "output_text": "A man is playing a video game while sitting on a couch in a room.", "error": null, "sys_before": {"cpu_percent": 45.5, "ram_used_mb": 24918.8, "ram_available_mb": 100853.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.1, "ram_used_mb": 24920.3, "ram_available_mb": 100851.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.12, "peak": 16.76, "min": 13.19}, "VDD_CPU_SOC_MSS": {"avg": 15.84, "peak": 17.71, "min": 14.17}, "VDD_GPU": {"avg": 37.65, "peak": 44.89, "min": 29.94}, "VIN": {"avg": 83.93, "peak": 124.57, "min": 57.32}}, "power_watts_avg": 37.65, "energy_joules_est": 82.62, "sample_count": 17, "duration_seconds": 2.195}, "timestamp": "2026-01-17T15:03:09.353293"}
{"image_index": 195, "image_name": "000000019786.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019786.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2992.616, "latencies_ms": [2992.616], "images_per_second": 0.334, "prompt_tokens": 23, "response_tokens_est": 39, "n_tiles": 12, "output_text": "1. man\n2. game controller\n3. game console\n4. game controller\n5. game controller\n6. game controller\n7. game controller\n8. game controller", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24920.3, "ram_available_mb": 100851.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 24920.8, "ram_available_mb": 100851.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.28, "peak": 16.56, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.92, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 36.15, "peak": 45.68, "min": 26.79}, "VIN": {"avg": 76.75, "peak": 109.6, "min": 59.5}}, "power_watts_avg": 36.15, "energy_joules_est": 108.19, "sample_count": 23, "duration_seconds": 2.993}, "timestamp": "2026-01-17T15:03:12.352346"}
{"image_index": 195, "image_name": "000000019786.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019786.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3266.979, "latencies_ms": [3266.979], "images_per_second": 0.306, "prompt_tokens": 27, "response_tokens_est": 47, "n_tiles": 12, "output_text": "The main object in the foreground is a black tripod stand with a white object on it. The white object is a laptop. The background features a person standing next to a couch, with a brown jacket hanging on a chair.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24920.8, "ram_available_mb": 100851.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 24921.8, "ram_available_mb": 100850.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.22, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.82, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 34.94, "peak": 45.68, "min": 26.39}, "VIN": {"avg": 73.69, "peak": 106.03, "min": 58.19}}, "power_watts_avg": 34.94, "energy_joules_est": 114.16, "sample_count": 25, "duration_seconds": 3.267}, "timestamp": "2026-01-17T15:03:15.626207"}
{"image_index": 195, "image_name": "000000019786.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019786.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 4113.707, "latencies_ms": [4113.707], "images_per_second": 0.243, "prompt_tokens": 21, "response_tokens_est": 72, "n_tiles": 12, "output_text": "The image depicts a man in a casual setting, possibly a living room or a small office, wearing a brown jacket and jeans. He is holding a white object in his hand and appears to be in motion, possibly walking or moving around. The background includes a couch, a table, and some electronic equipment, suggesting a home or small office environment.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24921.8, "ram_available_mb": 100850.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 24922.0, "ram_available_mb": 100850.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.19, "peak": 16.56, "min": 14.12}, "VDD_CPU_SOC_MSS": {"avg": 15.7, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 33.0, "peak": 45.69, "min": 26.0}, "VIN": {"avg": 75.76, "peak": 140.5, "min": 56.99}}, "power_watts_avg": 33.0, "energy_joules_est": 135.77, "sample_count": 33, "duration_seconds": 4.114}, "timestamp": "2026-01-17T15:03:19.747940"}
{"image_index": 195, "image_name": "000000019786.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019786.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3162.53, "latencies_ms": [3162.53], "images_per_second": 0.316, "prompt_tokens": 19, "response_tokens_est": 44, "n_tiles": 12, "output_text": "The image features a man in a gray shirt and dark pants standing in a room with a brown couch and a black tripod stand. The lighting is bright, and the overall atmosphere appears to be casual and comfortable.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24922.0, "ram_available_mb": 100850.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 24922.8, "ram_available_mb": 100849.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.24, "peak": 16.66, "min": 14.12}, "VDD_CPU_SOC_MSS": {"avg": 15.81, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 35.4, "peak": 45.68, "min": 26.4}, "VIN": {"avg": 78.87, "peak": 129.16, "min": 56.07}}, "power_watts_avg": 35.4, "energy_joules_est": 111.98, "sample_count": 24, "duration_seconds": 3.163}, "timestamp": "2026-01-17T15:03:22.916957"}
{"image_index": 196, "image_name": "000000019924.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019924.jpg", "image_width": 458, "image_height": 500, "image_resolution": "458x500", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 975.832, "latencies_ms": [975.832], "images_per_second": 1.025, "prompt_tokens": 9, "response_tokens_est": 41, "n_tiles": 1, "output_text": "The image features a black-and-white photograph of a woman wearing a wide-brimmed hat, a striped tank top, and a tie, holding a cigarette in her right hand and smiling broadly.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24922.5, "ram_available_mb": 100849.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 24922.8, "ram_available_mb": 100849.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 4845.7, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.11, "peak": 15.52, "min": 14.51}, "VDD_CPU_SOC_MSS": {"avg": 15.92, "peak": 16.54, "min": 14.96}, "VDD_GPU": {"avg": 23.59, "peak": 27.59, "min": 21.28}, "VIN": {"avg": 63.59, "peak": 67.77, "min": 61.92}}, "power_watts_avg": 23.59, "energy_joules_est": 23.04, "sample_count": 7, "duration_seconds": 0.977}, "timestamp": "2026-01-17T15:03:23.914267"}
{"image_index": 196, "image_name": "000000019924.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019924.jpg", "image_width": 458, "image_height": 500, "image_resolution": "458x500", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 911.51, "latencies_ms": [911.51], "images_per_second": 1.097, "prompt_tokens": 23, "response_tokens_est": 39, "n_tiles": 1, "output_text": "1. Woman\n2. Hat\n3. Cigarette\n4. Necktie\n5. Sunglasses\n6. Sunglasses\n7. Sunglasses\n8. Sunglasses", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24922.8, "ram_available_mb": 100849.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24924.0, "ram_available_mb": 100848.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 4856.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.48, "peak": 15.72, "min": 15.11}, "VDD_CPU_SOC_MSS": {"avg": 16.42, "peak": 16.54, "min": 16.14}, "VDD_GPU": {"avg": 22.24, "peak": 24.83, "min": 20.89}, "VIN": {"avg": 63.62, "peak": 70.28, "min": 59.34}}, "power_watts_avg": 22.24, "energy_joules_est": 20.28, "sample_count": 7, "duration_seconds": 0.912}, "timestamp": "2026-01-17T15:03:24.831889"}
{"image_index": 196, "image_name": "000000019924.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019924.jpg", "image_width": 458, "image_height": 500, "image_resolution": "458x500", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1210.141, "latencies_ms": [1210.141], "images_per_second": 0.826, "prompt_tokens": 27, "response_tokens_est": 53, "n_tiles": 1, "output_text": "The main object in the image is a woman wearing a wide-brimmed hat and a striped tank top. She is holding a cigarette in her right hand, which is near her mouth. The background is plain and does not contain any distinguishable features.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24924.0, "ram_available_mb": 100848.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24923.7, "ram_available_mb": 100848.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 4859.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.55, "peak": 15.72, "min": 15.22}, "VDD_CPU_SOC_MSS": {"avg": 16.54, "peak": 16.93, "min": 16.14}, "VDD_GPU": {"avg": 21.8, "peak": 24.43, "min": 20.49}, "VIN": {"avg": 64.73, "peak": 71.76, "min": 60.92}}, "power_watts_avg": 21.8, "energy_joules_est": 26.39, "sample_count": 9, "duration_seconds": 1.211}, "timestamp": "2026-01-17T15:03:26.048157"}
{"image_index": 196, "image_name": "000000019924.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019924.jpg", "image_width": 458, "image_height": 500, "image_resolution": "458x500", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1708.475, "latencies_ms": [1708.475], "images_per_second": 0.585, "prompt_tokens": 21, "response_tokens_est": 76, "n_tiles": 1, "output_text": "The image depicts a black-and-white photograph of a woman wearing a wide-brimmed hat and a striped tank top. She is holding a cigarette in her right hand and appears to be smiling while looking at the camera. The setting is not clearly defined, but the woman's attire and the presence of the cigarette suggest a casual, possibly outdoor or relaxed environment.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24923.7, "ram_available_mb": 100848.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24923.7, "ram_available_mb": 100848.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 4854.7, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.61, "peak": 15.82, "min": 15.22}, "VDD_CPU_SOC_MSS": {"avg": 16.6, "peak": 16.93, "min": 16.53}, "VDD_GPU": {"avg": 21.22, "peak": 24.42, "min": 20.1}, "VIN": {"avg": 64.36, "peak": 68.81, "min": 60.8}}, "power_watts_avg": 21.22, "energy_joules_est": 36.26, "sample_count": 13, "duration_seconds": 1.709}, "timestamp": "2026-01-17T15:03:27.762428"}
{"image_index": 196, "image_name": "000000019924.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019924.jpg", "image_width": 458, "image_height": 500, "image_resolution": "458x500", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1380.003, "latencies_ms": [1380.003], "images_per_second": 0.725, "prompt_tokens": 19, "response_tokens_est": 61, "n_tiles": 1, "output_text": "The notable visual attributes of the image include a black and white color scheme, a soft and diffused lighting that highlights the subject's features, and a woman wearing a wide-brimmed hat and a striped tank top. The hat casts a shadow over her face, adding depth to the image.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24923.7, "ram_available_mb": 100848.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24924.0, "ram_available_mb": 100848.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 4853.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.69, "peak": 15.82, "min": 15.32}, "VDD_CPU_SOC_MSS": {"avg": 16.53, "peak": 16.93, "min": 16.14}, "VDD_GPU": {"avg": 21.39, "peak": 24.03, "min": 20.09}, "VIN": {"avg": 64.31, "peak": 70.11, "min": 60.66}}, "power_watts_avg": 21.39, "energy_joules_est": 29.53, "sample_count": 10, "duration_seconds": 1.381}, "timestamp": "2026-01-17T15:03:29.150827"}
{"image_index": 197, "image_name": "000000020059.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000020059.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1220.436, "latencies_ms": [1220.436], "images_per_second": 0.819, "prompt_tokens": 9, "response_tokens_est": 23, "n_tiles": 6, "output_text": "A zebra is grazing on the green grass in a natural environment with trees and rocks in the background.", "error": null, "sys_before": {"cpu_percent": 43.9, "ram_used_mb": 24938.9, "ram_available_mb": 100833.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 24938.9, "ram_available_mb": 100833.3, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.9, "peak": 15.34, "min": 14.43}, "VDD_CPU_SOC_MSS": {"avg": 15.7, "peak": 16.53, "min": 14.96}, "VDD_GPU": {"avg": 33.75, "peak": 39.39, "min": 27.97}, "VIN": {"avg": 80.97, "peak": 111.24, "min": 61.27}}, "power_watts_avg": 33.75, "energy_joules_est": 41.21, "sample_count": 9, "duration_seconds": 1.221}, "timestamp": "2026-01-17T15:03:30.415400"}
{"image_index": 197, "image_name": "000000020059.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000020059.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1351.07, "latencies_ms": [1351.07], "images_per_second": 0.74, "prompt_tokens": 23, "response_tokens_est": 26, "n_tiles": 6, "output_text": "zebra: 1\nrock: 1\ntree: 1\nwall: 1\ndoor: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24938.9, "ram_available_mb": 100833.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 24940.1, "ram_available_mb": 100832.0, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.96, "peak": 15.44, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.59, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 35.33, "peak": 41.74, "min": 27.57}, "VIN": {"avg": 73.64, "peak": 94.74, "min": 61.86}}, "power_watts_avg": 35.33, "energy_joules_est": 47.74, "sample_count": 10, "duration_seconds": 1.351}, "timestamp": "2026-01-17T15:03:31.772739"}
{"image_index": 197, "image_name": "000000020059.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000020059.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3500.312, "latencies_ms": [3500.312], "images_per_second": 0.286, "prompt_tokens": 27, "response_tokens_est": 101, "n_tiles": 6, "output_text": "The main object in the foreground is a zebra, which is positioned near the center of the image. The zebra is the primary focus of the scene. In the background, there is another zebra, slightly further away, which is also grazing. The zebra in the background is positioned on the right side of the image. The zebra in the foreground is closer to the viewer, while the zebra in the background is further away, creating a sense of depth in the image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24940.1, "ram_available_mb": 100832.0, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 24941.4, "ram_available_mb": 100830.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.94, "peak": 15.34, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.66, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 27.79, "peak": 41.34, "min": 22.85}, "VIN": {"avg": 66.88, "peak": 112.21, "min": 54.41}}, "power_watts_avg": 27.79, "energy_joules_est": 97.29, "sample_count": 27, "duration_seconds": 3.501}, "timestamp": "2026-01-17T15:03:35.280588"}
{"image_index": 197, "image_name": "000000020059.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000020059.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2587.366, "latencies_ms": [2587.366], "images_per_second": 0.386, "prompt_tokens": 21, "response_tokens_est": 69, "n_tiles": 6, "output_text": "The image depicts a serene scene in a natural habitat, likely a zoo or wildlife reserve, where a zebra is grazing on the lush green grass. The zebra is the central focus, with its distinctive black and white stripes clearly visible. The background features a rocky outcrop and a tree, adding to the natural setting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24941.4, "ram_available_mb": 100830.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 24941.4, "ram_available_mb": 100830.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.88, "peak": 15.24, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.47, "peak": 15.75, "min": 14.56}, "VDD_GPU": {"avg": 29.07, "peak": 40.97, "min": 22.85}, "VIN": {"avg": 70.18, "peak": 107.14, "min": 59.91}}, "power_watts_avg": 29.07, "energy_joules_est": 75.23, "sample_count": 20, "duration_seconds": 2.588}, "timestamp": "2026-01-17T15:03:37.874860"}
{"image_index": 197, "image_name": "000000020059.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000020059.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1823.039, "latencies_ms": [1823.039], "images_per_second": 0.549, "prompt_tokens": 19, "response_tokens_est": 42, "n_tiles": 6, "output_text": "The image features a zebra standing in a lush, green meadow. The zebra has a striking black and white striped pattern, and the lighting is bright and natural, suggesting it is daytime.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24941.4, "ram_available_mb": 100830.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 24942.1, "ram_available_mb": 100830.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.82, "peak": 15.24, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.44, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 31.87, "peak": 40.18, "min": 24.03}, "VIN": {"avg": 71.57, "peak": 101.16, "min": 55.14}}, "power_watts_avg": 31.87, "energy_joules_est": 58.11, "sample_count": 13, "duration_seconds": 1.823}, "timestamp": "2026-01-17T15:03:39.704494"}
{"image_index": 198, "image_name": "000000020107.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000020107.jpg", "image_width": 333, "image_height": 500, "image_resolution": "333x500", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1549.046, "latencies_ms": [1549.046], "images_per_second": 0.646, "prompt_tokens": 9, "response_tokens_est": 35, "n_tiles": 6, "output_text": "The image shows an old, rusty fire hydrant with a chain attached to it, situated on a concrete surface with some greenery and a wall in the background.", "error": null, "sys_before": {"cpu_percent": 44.9, "ram_used_mb": 24942.1, "ram_available_mb": 100830.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 24942.3, "ram_available_mb": 100829.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.73, "peak": 15.11, "min": 13.79}, "VDD_CPU_SOC_MSS": {"avg": 15.35, "peak": 15.75, "min": 14.18}, "VDD_GPU": {"avg": 32.36, "peak": 39.77, "min": 25.21}, "VIN": {"avg": 68.74, "peak": 75.29, "min": 63.73}}, "power_watts_avg": 32.36, "energy_joules_est": 50.14, "sample_count": 12, "duration_seconds": 1.549}, "timestamp": "2026-01-17T15:03:41.306553"}
{"image_index": 198, "image_name": "000000020107.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000020107.jpg", "image_width": 333, "image_height": 500, "image_resolution": "333x500", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 4105.854, "latencies_ms": [4105.854], "images_per_second": 0.244, "prompt_tokens": 23, "response_tokens_est": 129, "n_tiles": 6, "output_text": "fire hydrant: 1\nchain: 1\npipe: 1\npipe cap: 1\npipe bolt: 1\npipe nut: 1\npipe flange: 1\npipe flange bolt: 1\npipe flange nut: 1\npipe flange bolt cap: 1\npipe flange bolt cap nut: 1\npipe flange bolt cap nut chain: 1\npipe flange bolt cap nut chain: 1\npipe flange bolt cap nut chain: 1\npipe flange bolt cap nut chain: 1\npipe flange bolt cap nut chain", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24942.3, "ram_available_mb": 100829.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 24943.3, "ram_available_mb": 100828.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.15, "peak": 15.44, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.69, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 27.32, "peak": 40.56, "min": 23.25}, "VIN": {"avg": 69.41, "peak": 108.99, "min": 60.99}}, "power_watts_avg": 27.32, "energy_joules_est": 112.18, "sample_count": 32, "duration_seconds": 4.106}, "timestamp": "2026-01-17T15:03:45.418836"}
{"image_index": 198, "image_name": "000000020107.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000020107.jpg", "image_width": 333, "image_height": 500, "image_resolution": "333x500", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2557.581, "latencies_ms": [2557.581], "images_per_second": 0.391, "prompt_tokens": 27, "response_tokens_est": 71, "n_tiles": 6, "output_text": "The main object in the foreground is a rusty, beige fire hydrant with a chain attached to it. The chain is attached to the hydrant's cap. In the background, there is a wall with a mural depicting green trees. The hydrant is situated on a concrete surface, and there are some leaves and twigs nearby.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24943.3, "ram_available_mb": 100828.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 24943.6, "ram_available_mb": 100828.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.1, "peak": 15.44, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.65, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 29.41, "peak": 40.56, "min": 23.64}, "VIN": {"avg": 69.76, "peak": 105.92, "min": 58.77}}, "power_watts_avg": 29.41, "energy_joules_est": 75.23, "sample_count": 20, "duration_seconds": 2.558}, "timestamp": "2026-01-17T15:03:47.983186"}
{"image_index": 198, "image_name": "000000020107.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000020107.jpg", "image_width": 333, "image_height": 500, "image_resolution": "333x500", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2478.111, "latencies_ms": [2478.111], "images_per_second": 0.404, "prompt_tokens": 21, "response_tokens_est": 68, "n_tiles": 6, "output_text": "The image depicts an old, rusty fire hydrant situated outdoors on a concrete surface. The hydrant is weathered and shows signs of wear, with a chain attached to it. The surrounding area appears to be a public space, possibly a park or a public area, with some greenery and a wall in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24943.6, "ram_available_mb": 100828.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 24944.8, "ram_available_mb": 100827.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.11, "peak": 15.44, "min": 14.43}, "VDD_CPU_SOC_MSS": {"avg": 15.69, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 29.75, "peak": 40.95, "min": 23.64}, "VIN": {"avg": 71.15, "peak": 96.99, "min": 61.38}}, "power_watts_avg": 29.75, "energy_joules_est": 73.75, "sample_count": 19, "duration_seconds": 2.479}, "timestamp": "2026-01-17T15:03:50.472272"}
{"image_index": 198, "image_name": "000000020107.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000020107.jpg", "image_width": 333, "image_height": 500, "image_resolution": "333x500", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2266.514, "latencies_ms": [2266.514], "images_per_second": 0.441, "prompt_tokens": 19, "response_tokens_est": 60, "n_tiles": 6, "output_text": "The fire hydrant in the image is painted in a faded orange color, with signs of rust and wear. The lighting is natural, suggesting it might be taken during the day. The weather appears to be overcast, as the overall lighting is subdued and there are no harsh shadows.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24944.8, "ram_available_mb": 100827.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 24944.8, "ram_available_mb": 100827.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.06, "peak": 15.44, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.63, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 30.28, "peak": 40.56, "min": 23.64}, "VIN": {"avg": 73.78, "peak": 110.28, "min": 58.76}}, "power_watts_avg": 30.28, "energy_joules_est": 68.64, "sample_count": 17, "duration_seconds": 2.267}, "timestamp": "2026-01-17T15:03:52.749220"}
{"image_index": 199, "image_name": "000000020247.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000020247.jpg", "image_width": 640, "image_height": 457, "image_resolution": "640x457", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2192.815, "latencies_ms": [2192.815], "images_per_second": 0.456, "prompt_tokens": 9, "response_tokens_est": 17, "n_tiles": 12, "output_text": "A brown bear is standing on a dirt ground, looking directly at the camera.", "error": null, "sys_before": {"cpu_percent": 41.0, "ram_used_mb": 24915.1, "ram_available_mb": 100857.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.2, "ram_used_mb": 24916.1, "ram_available_mb": 100856.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.18, "peak": 16.66, "min": 13.49}, "VDD_CPU_SOC_MSS": {"avg": 15.98, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 37.07, "peak": 43.73, "min": 29.16}, "VIN": {"avg": 80.84, "peak": 114.86, "min": 58.36}}, "power_watts_avg": 37.07, "energy_joules_est": 81.3, "sample_count": 17, "duration_seconds": 2.193}, "timestamp": "2026-01-17T15:03:55.031308"}
{"image_index": 199, "image_name": "000000020247.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000020247.jpg", "image_width": 640, "image_height": 457, "image_resolution": "640x457", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2047.422, "latencies_ms": [2047.422], "images_per_second": 0.488, "prompt_tokens": 23, "response_tokens_est": 11, "n_tiles": 12, "output_text": "bear: 1\nground: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24916.1, "ram_available_mb": 100856.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 1.9, "ram_used_mb": 24917.3, "ram_available_mb": 100854.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.33, "peak": 16.56, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 16.09, "peak": 17.71, "min": 14.96}, "VDD_GPU": {"avg": 39.48, "peak": 45.68, "min": 31.91}, "VIN": {"avg": 86.83, "peak": 124.64, "min": 60.72}}, "power_watts_avg": 39.48, "energy_joules_est": 80.85, "sample_count": 16, "duration_seconds": 2.048}, "timestamp": "2026-01-17T15:03:57.087965"}
{"image_index": 199, "image_name": "000000020247.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000020247.jpg", "image_width": 640, "image_height": 457, "image_resolution": "640x457", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3968.578, "latencies_ms": [3968.578], "images_per_second": 0.252, "prompt_tokens": 27, "response_tokens_est": 67, "n_tiles": 12, "output_text": "The main object in the foreground is a brown bear, which is positioned near the center of the image. The bear is facing the camera and appears to be walking. In the background, there is a blurred brown bear, which is further away from the camera. The blurred bear is partially obscured by the foreground bear.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24917.3, "ram_available_mb": 100854.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 24918.0, "ram_available_mb": 100854.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.16, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.75, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 33.74, "peak": 46.86, "min": 25.6}, "VIN": {"avg": 72.6, "peak": 111.71, "min": 52.85}}, "power_watts_avg": 33.74, "energy_joules_est": 133.92, "sample_count": 31, "duration_seconds": 3.969}, "timestamp": "2026-01-17T15:04:01.063362"}
{"image_index": 199, "image_name": "000000020247.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000020247.jpg", "image_width": 640, "image_height": 457, "image_resolution": "640x457", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3956.853, "latencies_ms": [3956.853], "images_per_second": 0.253, "prompt_tokens": 21, "response_tokens_est": 67, "n_tiles": 12, "output_text": "The image depicts a brown bear standing on a barren, rocky ground, likely in a natural habitat. The bear appears alert and is facing the camera, with its fur slightly tousled by the wind. The setting is a rugged, rocky terrain, possibly a forest or wilderness area, with sparse vegetation and a clear sky.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24918.0, "ram_available_mb": 100854.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 24919.0, "ram_available_mb": 100853.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.92, "peak": 16.36, "min": 14.02}, "VDD_CPU_SOC_MSS": {"avg": 15.4, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 33.14, "peak": 45.3, "min": 25.6}, "VIN": {"avg": 75.1, "peak": 118.22, "min": 59.61}}, "power_watts_avg": 33.14, "energy_joules_est": 131.15, "sample_count": 31, "duration_seconds": 3.957}, "timestamp": "2026-01-17T15:04:05.026905"}
{"image_index": 199, "image_name": "000000020247.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000020247.jpg", "image_width": 640, "image_height": 457, "image_resolution": "640x457", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2928.97, "latencies_ms": [2928.97], "images_per_second": 0.341, "prompt_tokens": 19, "response_tokens_est": 37, "n_tiles": 12, "output_text": "The image features a brown bear standing on a dusty, rocky ground. The lighting is natural, casting shadows on the ground, and the weather appears to be clear and sunny.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24919.0, "ram_available_mb": 100853.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 24919.0, "ram_available_mb": 100853.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.96, "peak": 16.36, "min": 13.82}, "VDD_CPU_SOC_MSS": {"avg": 15.6, "peak": 17.32, "min": 14.17}, "VDD_GPU": {"avg": 35.88, "peak": 45.69, "min": 26.39}, "VIN": {"avg": 74.14, "peak": 116.59, "min": 59.39}}, "power_watts_avg": 35.88, "energy_joules_est": 105.12, "sample_count": 22, "duration_seconds": 2.93}, "timestamp": "2026-01-17T15:04:07.963368"}
{"image_index": 200, "image_name": "000000020333.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000020333.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1221.181, "latencies_ms": [1221.181], "images_per_second": 0.819, "prompt_tokens": 9, "response_tokens_est": 23, "n_tiles": 6, "output_text": "A young child is kneeling in a shallow pit filled with soil, examining a small object in their hands.", "error": null, "sys_before": {"cpu_percent": 41.6, "ram_used_mb": 24933.1, "ram_available_mb": 100839.0, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 24934.4, "ram_available_mb": 100837.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.57, "peak": 15.14, "min": 13.72}, "VDD_CPU_SOC_MSS": {"avg": 15.31, "peak": 16.14, "min": 14.17}, "VDD_GPU": {"avg": 34.92, "peak": 40.95, "min": 27.97}, "VIN": {"avg": 75.98, "peak": 102.97, "min": 60.35}}, "power_watts_avg": 34.92, "energy_joules_est": 42.65, "sample_count": 9, "duration_seconds": 1.221}, "timestamp": "2026-01-17T15:04:09.243526"}
{"image_index": 200, "image_name": "000000020333.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000020333.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1806.803, "latencies_ms": [1806.803], "images_per_second": 0.553, "prompt_tokens": 23, "response_tokens_est": 43, "n_tiles": 6, "output_text": "1. Child\n2. Shovel\n3. Bucket\n4. Shovel handle\n5. Shovel blade\n6. Shovel base\n7. Shovel handle\n8. Shovel blade", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24934.4, "ram_available_mb": 100837.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 24934.6, "ram_available_mb": 100837.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.05, "peak": 15.54, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.63, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 32.93, "peak": 41.34, "min": 24.82}, "VIN": {"avg": 73.93, "peak": 121.08, "min": 58.14}}, "power_watts_avg": 32.93, "energy_joules_est": 59.51, "sample_count": 13, "duration_seconds": 1.807}, "timestamp": "2026-01-17T15:04:11.061444"}
{"image_index": 200, "image_name": "000000020333.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000020333.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2489.047, "latencies_ms": [2489.047], "images_per_second": 0.402, "prompt_tokens": 27, "response_tokens_est": 68, "n_tiles": 6, "output_text": "The main object in the foreground is a small child with light-colored hair, wearing a white shirt and a colorful tie. The child is kneeling on the ground, interacting with a large metal bowl filled with dark soil. The background features a dark, leafy bush, providing a contrast to the bright colors of the child's clothing.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24934.6, "ram_available_mb": 100837.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 24935.3, "ram_available_mb": 100836.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.08, "peak": 15.44, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.71, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 29.69, "peak": 40.56, "min": 23.64}, "VIN": {"avg": 70.15, "peak": 108.03, "min": 59.71}}, "power_watts_avg": 29.69, "energy_joules_est": 73.91, "sample_count": 19, "duration_seconds": 2.489}, "timestamp": "2026-01-17T15:04:13.557122"}
{"image_index": 200, "image_name": "000000020333.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000020333.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2778.719, "latencies_ms": [2778.719], "images_per_second": 0.36, "prompt_tokens": 21, "response_tokens_est": 79, "n_tiles": 6, "output_text": "The image depicts a young child, likely a toddler, engaged in an outdoor activity. The child is kneeling on a patch of dirt, surrounded by a large metal bucket filled with dark soil. The child appears to be playing with the soil, possibly digging or examining it. The setting is outdoors, with a backdrop of lush green foliage, indicating a garden or park environment.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24935.3, "ram_available_mb": 100836.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 24935.3, "ram_available_mb": 100836.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.11, "peak": 15.44, "min": 14.43}, "VDD_CPU_SOC_MSS": {"avg": 15.75, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 29.06, "peak": 40.57, "min": 23.64}, "VIN": {"avg": 70.62, "peak": 107.68, "min": 62.59}}, "power_watts_avg": 29.06, "energy_joules_est": 80.76, "sample_count": 21, "duration_seconds": 2.779}, "timestamp": "2026-01-17T15:04:16.342752"}
{"image_index": 200, "image_name": "000000020333.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000020333.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1833.729, "latencies_ms": [1833.729], "images_per_second": 0.545, "prompt_tokens": 19, "response_tokens_est": 44, "n_tiles": 6, "output_text": "The notable visual attributes of the image include a child with light-colored hair, wearing a white shirt and a colorful tie, and a black and white photograph. The lighting is natural, with shadows indicating a sunny day.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24935.3, "ram_available_mb": 100836.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 24936.1, "ram_available_mb": 100836.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.06, "peak": 15.54, "min": 14.43}, "VDD_CPU_SOC_MSS": {"avg": 15.58, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 31.51, "peak": 40.16, "min": 24.42}, "VIN": {"avg": 71.54, "peak": 106.0, "min": 60.12}}, "power_watts_avg": 31.51, "energy_joules_est": 57.8, "sample_count": 14, "duration_seconds": 1.834}, "timestamp": "2026-01-17T15:04:18.182719"}
{"image_index": 201, "image_name": "000000020553.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000020553.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2691.175, "latencies_ms": [2691.175], "images_per_second": 0.372, "prompt_tokens": 9, "response_tokens_est": 32, "n_tiles": 12, "output_text": "A small table is placed on the ground with a few items on it, including a bottle of beer, a small plant, and a few other objects.", "error": null, "sys_before": {"cpu_percent": 43.1, "ram_used_mb": 24916.5, "ram_available_mb": 100855.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 24917.0, "ram_available_mb": 100855.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.25, "peak": 16.76, "min": 13.79}, "VDD_CPU_SOC_MSS": {"avg": 15.96, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 35.88, "peak": 44.1, "min": 27.18}, "VIN": {"avg": 75.3, "peak": 98.06, "min": 62.38}}, "power_watts_avg": 35.88, "energy_joules_est": 96.58, "sample_count": 20, "duration_seconds": 2.692}, "timestamp": "2026-01-17T15:04:20.953734"}
{"image_index": 201, "image_name": "000000020553.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000020553.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 3636.455, "latencies_ms": [3636.455], "images_per_second": 0.275, "prompt_tokens": 23, "response_tokens_est": 58, "n_tiles": 12, "output_text": "1. Stuffed animals: 3\n2. Bottle: 1\n3. Bottle: 1\n4. Bottle: 1\n5. Bottle: 1\n6. Bottle: 1\n7. Bottle: 1\n8. Bottle: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24917.0, "ram_available_mb": 100855.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 24917.7, "ram_available_mb": 100854.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.2, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.73, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 33.92, "peak": 45.68, "min": 26.0}, "VIN": {"avg": 74.65, "peak": 117.38, "min": 57.56}}, "power_watts_avg": 33.92, "energy_joules_est": 123.36, "sample_count": 29, "duration_seconds": 3.637}, "timestamp": "2026-01-17T15:04:24.596992"}
{"image_index": 201, "image_name": "000000020553.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000020553.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 4257.505, "latencies_ms": [4257.505], "images_per_second": 0.235, "prompt_tokens": 27, "response_tokens_est": 76, "n_tiles": 12, "output_text": "The main objects in the image are a collection of stuffed animals and a small table. The stuffed animals are positioned on the table, with one bear sitting on top of the table and another bear standing on the table. The table is placed in the foreground, with the stuffed animals and other items on it. The background features a barren, sandy landscape with sparse vegetation.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24917.7, "ram_available_mb": 100854.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 24918.5, "ram_available_mb": 100853.7, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.18, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.85, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 32.94, "peak": 45.68, "min": 26.0}, "VIN": {"avg": 73.79, "peak": 109.97, "min": 59.37}}, "power_watts_avg": 32.94, "energy_joules_est": 140.25, "sample_count": 33, "duration_seconds": 4.258}, "timestamp": "2026-01-17T15:04:28.865315"}
{"image_index": 201, "image_name": "000000020553.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000020553.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2928.515, "latencies_ms": [2928.515], "images_per_second": 0.341, "prompt_tokens": 21, "response_tokens_est": 37, "n_tiles": 12, "output_text": "The image depicts a barren, dusty landscape with a small table and a few items placed on it. The table is surrounded by sparse vegetation and a few scattered rocks.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24918.5, "ram_available_mb": 100853.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 24919.2, "ram_available_mb": 100853.0, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.2, "peak": 16.66, "min": 14.02}, "VDD_CPU_SOC_MSS": {"avg": 15.85, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 35.95, "peak": 45.28, "min": 26.79}, "VIN": {"avg": 73.75, "peak": 101.39, "min": 55.57}}, "power_watts_avg": 35.95, "energy_joules_est": 105.29, "sample_count": 22, "duration_seconds": 2.929}, "timestamp": "2026-01-17T15:04:31.804670"}
{"image_index": 201, "image_name": "000000020553.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000020553.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3473.704, "latencies_ms": [3473.704], "images_per_second": 0.288, "prompt_tokens": 19, "response_tokens_est": 53, "n_tiles": 12, "output_text": "The image depicts a desert scene with a sandy ground and sparse vegetation. The lighting is bright and natural, casting shadows on the ground. The materials used include a wooden table, a red cross pillow, a green plastic bottle, and various stuffed animals.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24919.2, "ram_available_mb": 100853.0, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 24919.4, "ram_available_mb": 100852.7, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.21, "peak": 16.56, "min": 14.12}, "VDD_CPU_SOC_MSS": {"avg": 15.79, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 34.34, "peak": 45.28, "min": 26.0}, "VIN": {"avg": 77.98, "peak": 117.41, "min": 65.99}}, "power_watts_avg": 34.34, "energy_joules_est": 119.3, "sample_count": 26, "duration_seconds": 3.474}, "timestamp": "2026-01-17T15:04:35.289370"}
{"image_index": 202, "image_name": "000000020571.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000020571.jpg", "image_width": 539, "image_height": 640, "image_resolution": "539x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 3209.28, "latencies_ms": [3209.28], "images_per_second": 0.312, "prompt_tokens": 9, "response_tokens_est": 47, "n_tiles": 12, "output_text": "The image depicts a harbor scene with a variety of boats and a calm body of water, featuring a rusty, weathered boat with a number \"19\" on its side, alongside other boats and a rocky shoreline.", "error": null, "sys_before": {"cpu_percent": 43.2, "ram_used_mb": 24934.3, "ram_available_mb": 100837.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 24932.1, "ram_available_mb": 100840.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.11, "peak": 16.66, "min": 13.19}, "VDD_CPU_SOC_MSS": {"avg": 15.76, "peak": 17.71, "min": 14.17}, "VDD_GPU": {"avg": 34.13, "peak": 44.12, "min": 26.0}, "VIN": {"avg": 75.7, "peak": 114.81, "min": 65.22}}, "power_watts_avg": 34.13, "energy_joules_est": 109.55, "sample_count": 25, "duration_seconds": 3.21}, "timestamp": "2026-01-17T15:04:38.608715"}
{"image_index": 202, "image_name": "000000020571.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000020571.jpg", "image_width": 539, "image_height": 640, "image_resolution": "539x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 6038.777, "latencies_ms": [6038.777], "images_per_second": 0.166, "prompt_tokens": 23, "response_tokens_est": 129, "n_tiles": 12, "output_text": "boat: 1\nbuoy: 1\nlifebuoy: 1\npole: 1\npole: 1\npole: 1\npole: 1\npole: 1\npole: 1\npole: 1\npole: 1\npole: 1\npole: 1\npole: 1\npole: 1\npole: 1\npole: 1\npole: 1\npole: 1\npole: 1\npole: 1\npole", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24932.1, "ram_available_mb": 100840.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 24927.4, "ram_available_mb": 100844.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.15, "peak": 16.56, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.61, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 30.76, "peak": 45.69, "min": 25.6}, "VIN": {"avg": 71.86, "peak": 126.11, "min": 54.83}}, "power_watts_avg": 30.76, "energy_joules_est": 185.77, "sample_count": 47, "duration_seconds": 6.039}, "timestamp": "2026-01-17T15:04:44.654430"}
{"image_index": 202, "image_name": "000000020571.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000020571.jpg", "image_width": 539, "image_height": 640, "image_resolution": "539x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 4256.902, "latencies_ms": [4256.902], "images_per_second": 0.235, "prompt_tokens": 27, "response_tokens_est": 76, "n_tiles": 12, "output_text": "The main objects in the image are a weathered boat and a rusty metal sign. The boat is situated in the foreground, with its deck and various items like ropes and buoys visible. The sign is located near the boat, attached to a metal structure. The background features a calm body of water, a rocky shoreline, and a distant mountain range.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24927.4, "ram_available_mb": 100844.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 24927.2, "ram_available_mb": 100844.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.19, "peak": 16.56, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.71, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 32.76, "peak": 44.89, "min": 26.0}, "VIN": {"avg": 74.98, "peak": 110.64, "min": 61.51}}, "power_watts_avg": 32.76, "energy_joules_est": 139.47, "sample_count": 32, "duration_seconds": 4.257}, "timestamp": "2026-01-17T15:04:48.917828"}
{"image_index": 202, "image_name": "000000020571.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000020571.jpg", "image_width": 539, "image_height": 640, "image_resolution": "539x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3734.965, "latencies_ms": [3734.965], "images_per_second": 0.268, "prompt_tokens": 21, "response_tokens_est": 61, "n_tiles": 12, "output_text": "The image depicts a harbor scene with a variety of boats and a calm body of water. The setting appears to be a coastal area with green hills in the background and a cloudy sky. There are several boats moored at the dock, and a few people can be seen near the water.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24927.2, "ram_available_mb": 100844.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 24927.7, "ram_available_mb": 100844.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.21, "peak": 16.66, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.68, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 33.83, "peak": 45.28, "min": 26.0}, "VIN": {"avg": 76.05, "peak": 139.78, "min": 55.86}}, "power_watts_avg": 33.83, "energy_joules_est": 126.38, "sample_count": 28, "duration_seconds": 3.736}, "timestamp": "2026-01-17T15:04:52.659985"}
{"image_index": 202, "image_name": "000000020571.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000020571.jpg", "image_width": 539, "image_height": 640, "image_resolution": "539x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 4251.64, "latencies_ms": [4251.64], "images_per_second": 0.235, "prompt_tokens": 19, "response_tokens_est": 76, "n_tiles": 12, "output_text": "The image depicts a harbor scene with a variety of boats and a calm body of water. The boats are painted in different colors, including green, yellow, and red, and are equipped with various equipment such as lifebuoys and ropes. The lighting is soft and diffused, suggesting an overcast day, and the weather appears to be mild and pleasant.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24927.7, "ram_available_mb": 100844.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 24928.5, "ram_available_mb": 100843.7, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.19, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.65, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 32.7, "peak": 44.89, "min": 26.0}, "VIN": {"avg": 72.94, "peak": 92.65, "min": 55.59}}, "power_watts_avg": 32.7, "energy_joules_est": 139.04, "sample_count": 33, "duration_seconds": 4.252}, "timestamp": "2026-01-17T15:04:56.918552"}
{"image_index": 203, "image_name": "000000020992.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000020992.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1194.202, "latencies_ms": [1194.202], "images_per_second": 0.837, "prompt_tokens": 9, "response_tokens_est": 22, "n_tiles": 6, "output_text": "A woman is eating a hot dog with her mouth open, and she is wearing a black scarf.", "error": null, "sys_before": {"cpu_percent": 36.3, "ram_used_mb": 24945.6, "ram_available_mb": 100826.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 24947.8, "ram_available_mb": 100824.3, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.59, "peak": 15.14, "min": 13.72}, "VDD_CPU_SOC_MSS": {"avg": 15.21, "peak": 16.14, "min": 14.17}, "VDD_GPU": {"avg": 35.4, "peak": 40.56, "min": 29.15}, "VIN": {"avg": 79.46, "peak": 116.25, "min": 63.16}}, "power_watts_avg": 35.4, "energy_joules_est": 42.29, "sample_count": 8, "duration_seconds": 1.195}, "timestamp": "2026-01-17T15:04:58.177899"}
{"image_index": 203, "image_name": "000000020992.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000020992.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1508.498, "latencies_ms": [1508.498], "images_per_second": 0.663, "prompt_tokens": 23, "response_tokens_est": 32, "n_tiles": 6, "output_text": "1. Woman\n2. Food\n3. Plate\n4. Bowl\n5. Bowl\n6. Bowl\n7. Bowl\n8. Bowl", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24947.8, "ram_available_mb": 100824.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 24949.1, "ram_available_mb": 100823.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.0, "peak": 15.44, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.57, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 34.66, "peak": 41.74, "min": 26.39}, "VIN": {"avg": 75.18, "peak": 99.35, "min": 63.88}}, "power_watts_avg": 34.66, "energy_joules_est": 52.29, "sample_count": 11, "duration_seconds": 1.509}, "timestamp": "2026-01-17T15:04:59.692690"}
{"image_index": 203, "image_name": "000000020992.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000020992.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1996.961, "latencies_ms": [1996.961], "images_per_second": 0.501, "prompt_tokens": 27, "response_tokens_est": 48, "n_tiles": 6, "output_text": "The main object in the foreground is a person holding a hot dog. The person is wearing a black scarf and has a focused expression. The background is blurred, with indistinct shapes and lights, suggesting an indoor setting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24949.1, "ram_available_mb": 100823.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 24949.0, "ram_available_mb": 100823.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.93, "peak": 15.34, "min": 14.43}, "VDD_CPU_SOC_MSS": {"avg": 15.38, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 31.41, "peak": 40.95, "min": 23.64}, "VIN": {"avg": 69.32, "peak": 89.66, "min": 59.42}}, "power_watts_avg": 31.41, "energy_joules_est": 62.73, "sample_count": 15, "duration_seconds": 1.997}, "timestamp": "2026-01-17T15:05:01.696254"}
{"image_index": 203, "image_name": "000000020992.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000020992.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2130.703, "latencies_ms": [2130.703], "images_per_second": 0.469, "prompt_tokens": 21, "response_tokens_est": 53, "n_tiles": 6, "output_text": "The image depicts a close-up of a person eating a hot dog, with their mouth open and tongue visible. The setting appears to be indoors, possibly in a dimly lit area, as suggested by the blurred background and the warm, soft lighting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24949.0, "ram_available_mb": 100823.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 24949.5, "ram_available_mb": 100822.7, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.89, "peak": 15.24, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.38, "peak": 15.75, "min": 14.56}, "VDD_GPU": {"avg": 30.55, "peak": 40.56, "min": 23.64}, "VIN": {"avg": 72.03, "peak": 111.76, "min": 60.64}}, "power_watts_avg": 30.55, "energy_joules_est": 65.11, "sample_count": 16, "duration_seconds": 2.131}, "timestamp": "2026-01-17T15:05:03.833264"}
{"image_index": 203, "image_name": "000000020992.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000020992.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1568.766, "latencies_ms": [1568.766], "images_per_second": 0.637, "prompt_tokens": 19, "response_tokens_est": 33, "n_tiles": 6, "output_text": "The image features a person with short hair, wearing a dark-colored scarf. The lighting is dim, with a warm glow illuminating the subject's face.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24949.5, "ram_available_mb": 100822.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 24949.5, "ram_available_mb": 100822.7, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.83, "peak": 15.34, "min": 14.3}, "VDD_CPU_SOC_MSS": {"avg": 15.32, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 32.57, "peak": 40.18, "min": 24.83}, "VIN": {"avg": 72.31, "peak": 106.6, "min": 61.01}}, "power_watts_avg": 32.57, "energy_joules_est": 51.1, "sample_count": 12, "duration_seconds": 1.569}, "timestamp": "2026-01-17T15:05:05.410610"}
{"image_index": 204, "image_name": "000000021167.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000021167.jpg", "image_width": 426, "image_height": 640, "image_resolution": "426x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1387.71, "latencies_ms": [1387.71], "images_per_second": 0.721, "prompt_tokens": 9, "response_tokens_est": 29, "n_tiles": 6, "output_text": "A man and a woman are standing in a room, with the man holding a martini glass and the woman looking off to the side.", "error": null, "sys_before": {"cpu_percent": 36.5, "ram_used_mb": 24949.3, "ram_available_mb": 100822.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 24949.3, "ram_available_mb": 100822.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.66, "peak": 15.14, "min": 13.69}, "VDD_CPU_SOC_MSS": {"avg": 15.31, "peak": 16.14, "min": 14.18}, "VDD_GPU": {"avg": 33.52, "peak": 40.16, "min": 26.39}, "VIN": {"avg": 77.54, "peak": 116.37, "min": 65.25}}, "power_watts_avg": 33.52, "energy_joules_est": 46.52, "sample_count": 10, "duration_seconds": 1.388}, "timestamp": "2026-01-17T15:05:06.859286"}
{"image_index": 204, "image_name": "000000021167.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000021167.jpg", "image_width": 426, "image_height": 640, "image_resolution": "426x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2217.711, "latencies_ms": [2217.711], "images_per_second": 0.451, "prompt_tokens": 23, "response_tokens_est": 56, "n_tiles": 6, "output_text": "1. Man: 1\n2. Woman: 1\n3. Man: 1\n4. Woman: 1\n5. Man: 1\n6. Woman: 1\n7. Man: 1\n8. Woman: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24949.3, "ram_available_mb": 100822.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 24950.3, "ram_available_mb": 100821.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.86, "peak": 15.34, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.38, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 30.8, "peak": 41.34, "min": 23.24}, "VIN": {"avg": 71.87, "peak": 100.28, "min": 58.62}}, "power_watts_avg": 30.8, "energy_joules_est": 68.31, "sample_count": 16, "duration_seconds": 2.218}, "timestamp": "2026-01-17T15:05:09.083272"}
{"image_index": 204, "image_name": "000000021167.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000021167.jpg", "image_width": 426, "image_height": 640, "image_resolution": "426x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2138.193, "latencies_ms": [2138.193], "images_per_second": 0.468, "prompt_tokens": 27, "response_tokens_est": 53, "n_tiles": 6, "output_text": "The main objects in the image are a man and a woman standing close to each other. The man is in the foreground, while the woman is slightly behind him. The background is slightly blurred, indicating that the focus is on the man and the woman.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24950.3, "ram_available_mb": 100821.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 24950.0, "ram_available_mb": 100822.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.86, "peak": 15.24, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.3, "peak": 15.75, "min": 14.56}, "VDD_GPU": {"avg": 30.41, "peak": 40.97, "min": 23.64}, "VIN": {"avg": 70.86, "peak": 113.15, "min": 54.05}}, "power_watts_avg": 30.41, "energy_joules_est": 65.04, "sample_count": 16, "duration_seconds": 2.139}, "timestamp": "2026-01-17T15:05:11.228528"}
{"image_index": 204, "image_name": "000000021167.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000021167.jpg", "image_width": 426, "image_height": 640, "image_resolution": "426x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2877.662, "latencies_ms": [2877.662], "images_per_second": 0.348, "prompt_tokens": 21, "response_tokens_est": 79, "n_tiles": 6, "output_text": "The image depicts a man and a woman standing in an indoor setting, possibly a hallway or a room. The man is dressed in a formal black suit and tie, holding a martini glass with a drink. The woman is wearing a dark, sleeveless dress and has her hair tied back. They appear to be engaged in a conversation, with the man looking slightly to his right.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24950.0, "ram_available_mb": 100822.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 24951.2, "ram_available_mb": 100821.0, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.88, "peak": 15.24, "min": 14.3}, "VDD_CPU_SOC_MSS": {"avg": 15.34, "peak": 15.75, "min": 14.56}, "VDD_GPU": {"avg": 28.51, "peak": 40.16, "min": 22.85}, "VIN": {"avg": 71.74, "peak": 105.67, "min": 61.83}}, "power_watts_avg": 28.51, "energy_joules_est": 82.05, "sample_count": 21, "duration_seconds": 2.878}, "timestamp": "2026-01-17T15:05:14.112469"}
{"image_index": 204, "image_name": "000000021167.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000021167.jpg", "image_width": 426, "image_height": 640, "image_resolution": "426x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2446.914, "latencies_ms": [2446.914], "images_per_second": 0.409, "prompt_tokens": 19, "response_tokens_est": 64, "n_tiles": 6, "output_text": "The image features a man and a woman standing indoors. The man is dressed in a black suit with a white shirt and a dark tie, while the woman is wearing a dark, sleeveless dress. The lighting is dim, and the overall atmosphere appears to be indoors with a warm, subdued ambiance.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24951.2, "ram_available_mb": 100821.0, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 24951.2, "ram_available_mb": 100821.0, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.86, "peak": 15.34, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.42, "peak": 16.14, "min": 14.57}, "VDD_GPU": {"avg": 29.3, "peak": 40.16, "min": 23.24}, "VIN": {"avg": 68.76, "peak": 93.96, "min": 53.06}}, "power_watts_avg": 29.3, "energy_joules_est": 71.7, "sample_count": 18, "duration_seconds": 2.447}, "timestamp": "2026-01-17T15:05:16.565378"}
{"image_index": 205, "image_name": "000000021465.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000021465.jpg", "image_width": 500, "image_height": 281, "image_resolution": "500x281", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1380.229, "latencies_ms": [1380.229], "images_per_second": 0.725, "prompt_tokens": 9, "response_tokens_est": 53, "n_tiles": 2, "output_text": "The image shows a cluttered blue table with various items scattered on it, including a metal pitcher, a small plate, a basket, and a wooden chair, all set against a backdrop of a rustic wooden table and a green surface with a small figurine.", "error": null, "sys_before": {"cpu_percent": 29.3, "ram_used_mb": 24951.2, "ram_available_mb": 100821.0, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24951.2, "ram_available_mb": 100821.0, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5055.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.85, "peak": 15.22, "min": 14.0}, "VDD_CPU_SOC_MSS": {"avg": 15.55, "peak": 16.14, "min": 14.57}, "VDD_GPU": {"avg": 24.51, "peak": 31.51, "min": 21.28}, "VIN": {"avg": 66.43, "peak": 78.56, "min": 61.09}}, "power_watts_avg": 24.51, "energy_joules_est": 33.85, "sample_count": 10, "duration_seconds": 1.381}, "timestamp": "2026-01-17T15:05:17.978448"}
{"image_index": 205, "image_name": "000000021465.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000021465.jpg", "image_width": 500, "image_height": 281, "image_resolution": "500x281", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1083.097, "latencies_ms": [1083.097], "images_per_second": 0.923, "prompt_tokens": 23, "response_tokens_est": 40, "n_tiles": 2, "output_text": "object: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24951.2, "ram_available_mb": 100821.0, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 24951.2, "ram_available_mb": 100821.0, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5064.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.09, "peak": 15.42, "min": 14.61}, "VDD_CPU_SOC_MSS": {"avg": 15.95, "peak": 16.54, "min": 15.75}, "VDD_GPU": {"avg": 25.41, "peak": 30.73, "min": 22.06}, "VIN": {"avg": 68.02, "peak": 94.36, "min": 58.73}}, "power_watts_avg": 25.41, "energy_joules_est": 27.53, "sample_count": 8, "duration_seconds": 1.083}, "timestamp": "2026-01-17T15:05:19.069793"}
{"image_index": 205, "image_name": "000000021465.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000021465.jpg", "image_width": 500, "image_height": 281, "image_resolution": "500x281", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1457.923, "latencies_ms": [1457.923], "images_per_second": 0.686, "prompt_tokens": 27, "response_tokens_est": 56, "n_tiles": 2, "output_text": "The main objects in the image are a blue table and a green table. The blue table is in the foreground, while the green table is in the background. The objects on the blue table are closer to the viewer, while the objects on the green table are farther away.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24951.2, "ram_available_mb": 100821.0, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 24951.2, "ram_available_mb": 100821.0, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5067.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.09, "peak": 15.32, "min": 14.61}, "VDD_CPU_SOC_MSS": {"avg": 16.0, "peak": 16.54, "min": 15.36}, "VDD_GPU": {"avg": 24.43, "peak": 31.12, "min": 21.28}, "VIN": {"avg": 66.54, "peak": 104.71, "min": 58.94}}, "power_watts_avg": 24.43, "energy_joules_est": 35.62, "sample_count": 11, "duration_seconds": 1.458}, "timestamp": "2026-01-17T15:05:20.533894"}
{"image_index": 205, "image_name": "000000021465.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000021465.jpg", "image_width": 500, "image_height": 281, "image_resolution": "500x281", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1901.898, "latencies_ms": [1901.898], "images_per_second": 0.526, "prompt_tokens": 21, "response_tokens_est": 76, "n_tiles": 2, "output_text": "The image depicts a cluttered and somewhat disorganized outdoor setting, possibly a market or a flea market. Various items are scattered on tables and shelves, including a blue table with a few items on it, a yellow chair, a basket, and a wooden sculpture. The scene suggests a casual, possibly temporary, environment where people are selling or displaying various objects.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24951.2, "ram_available_mb": 100821.0, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24951.2, "ram_available_mb": 100821.0, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5063.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.24, "peak": 15.82, "min": 14.71}, "VDD_CPU_SOC_MSS": {"avg": 16.14, "peak": 16.54, "min": 15.75}, "VDD_GPU": {"avg": 23.56, "peak": 30.33, "min": 20.89}, "VIN": {"avg": 67.13, "peak": 109.9, "min": 57.62}}, "power_watts_avg": 23.56, "energy_joules_est": 44.83, "sample_count": 14, "duration_seconds": 1.903}, "timestamp": "2026-01-17T15:05:22.442112"}
{"image_index": 205, "image_name": "000000021465.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000021465.jpg", "image_width": 500, "image_height": 281, "image_resolution": "500x281", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1558.406, "latencies_ms": [1558.406], "images_per_second": 0.642, "prompt_tokens": 19, "response_tokens_est": 61, "n_tiles": 2, "output_text": "The image showcases a vibrant blue table with a rustic, weathered look. The table is adorned with various metallic objects, including vases and cups, and is surrounded by a mix of wooden and metal furniture pieces. The lighting is soft and natural, casting a warm glow over the scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24951.2, "ram_available_mb": 100821.0, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24951.2, "ram_available_mb": 100821.0, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5062.6, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.25, "peak": 15.82, "min": 14.61}, "VDD_CPU_SOC_MSS": {"avg": 15.81, "peak": 16.14, "min": 15.35}, "VDD_GPU": {"avg": 23.67, "peak": 29.94, "min": 20.88}, "VIN": {"avg": 66.22, "peak": 102.52, "min": 55.08}}, "power_watts_avg": 23.67, "energy_joules_est": 36.9, "sample_count": 12, "duration_seconds": 1.559}, "timestamp": "2026-01-17T15:05:24.005915"}
{"image_index": 206, "image_name": "000000021503.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000021503.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2789.096, "latencies_ms": [2789.096], "images_per_second": 0.359, "prompt_tokens": 9, "response_tokens_est": 35, "n_tiles": 12, "output_text": "The image shows a close-up view of a plate with a slice of cheesecake, which is topped with a layer of cream and has a golden-brown crust.", "error": null, "sys_before": {"cpu_percent": 43.1, "ram_used_mb": 24932.7, "ram_available_mb": 100839.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 24933.0, "ram_available_mb": 100839.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.29, "peak": 16.76, "min": 14.0}, "VDD_CPU_SOC_MSS": {"avg": 15.97, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 35.15, "peak": 43.33, "min": 26.79}, "VIN": {"avg": 77.32, "peak": 117.86, "min": 66.01}}, "power_watts_avg": 35.15, "energy_joules_est": 98.05, "sample_count": 21, "duration_seconds": 2.789}, "timestamp": "2026-01-17T15:05:26.885578"}
{"image_index": 206, "image_name": "000000021503.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000021503.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 3487.263, "latencies_ms": [3487.263], "images_per_second": 0.287, "prompt_tokens": 23, "response_tokens_est": 54, "n_tiles": 12, "output_text": "- 1 pizza\n- 1 slice of pizza\n- 1 slice of bread\n- 1 slice of bread\n- 1 slice of bread\n- 1 slice of bread\n- 1 slice of bread\n- 1 slice of bread", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24933.0, "ram_available_mb": 100839.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 24933.7, "ram_available_mb": 100838.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.26, "peak": 16.56, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.7, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 34.3, "peak": 44.89, "min": 26.0}, "VIN": {"avg": 78.2, "peak": 140.95, "min": 61.74}}, "power_watts_avg": 34.3, "energy_joules_est": 119.63, "sample_count": 27, "duration_seconds": 3.488}, "timestamp": "2026-01-17T15:05:30.379847"}
{"image_index": 206, "image_name": "000000021503.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000021503.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 4861.478, "latencies_ms": [4861.478], "images_per_second": 0.206, "prompt_tokens": 27, "response_tokens_est": 94, "n_tiles": 12, "output_text": "The main objects in the image are a plate of cheesy breadsticks. The cheesy breadsticks are positioned in the foreground, with the plate serving as the central point of focus. The background features a blurred computer keyboard and mouse, indicating that the cheesy breadsticks are likely placed on a desk or table. The cheesy breadsticks are near the center of the image, with the computer equipment slightly out of focus in the background.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 24933.7, "ram_available_mb": 100838.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 24933.7, "ram_available_mb": 100838.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.18, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.67, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 31.93, "peak": 44.89, "min": 26.0}, "VIN": {"avg": 73.61, "peak": 111.54, "min": 59.34}}, "power_watts_avg": 31.93, "energy_joules_est": 155.24, "sample_count": 38, "duration_seconds": 4.862}, "timestamp": "2026-01-17T15:05:35.248325"}
{"image_index": 206, "image_name": "000000021503.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000021503.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 4072.056, "latencies_ms": [4072.056], "images_per_second": 0.246, "prompt_tokens": 21, "response_tokens_est": 71, "n_tiles": 12, "output_text": "The image depicts a close-up view of a plate with several pieces of a cheesy, golden-brown pastry, likely a type of bread or pastry. The setting appears to be indoors, possibly in a kitchen or dining area, with a blurred background featuring a computer mouse and a keyboard, suggesting a workspace or a home environment.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24933.7, "ram_available_mb": 100838.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 24933.7, "ram_available_mb": 100838.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.19, "peak": 16.56, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.66, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 33.15, "peak": 44.89, "min": 26.0}, "VIN": {"avg": 74.23, "peak": 117.24, "min": 55.48}}, "power_watts_avg": 33.15, "energy_joules_est": 135.0, "sample_count": 32, "duration_seconds": 4.072}, "timestamp": "2026-01-17T15:05:39.327202"}
{"image_index": 206, "image_name": "000000021503.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000021503.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3944.258, "latencies_ms": [3944.258], "images_per_second": 0.254, "prompt_tokens": 19, "response_tokens_est": 67, "n_tiles": 12, "output_text": "The image features a close-up view of a plate with a slice of cheesecake, which is light in color with a creamy white topping. The background is softly blurred, with a hint of a computer keyboard and a mouse, suggesting a home or office setting. The lighting is warm and soft, creating a cozy atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24933.7, "ram_available_mb": 100838.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 24933.7, "ram_available_mb": 100838.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.19, "peak": 16.66, "min": 14.12}, "VDD_CPU_SOC_MSS": {"avg": 15.63, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 33.27, "peak": 44.89, "min": 26.0}, "VIN": {"avg": 72.74, "peak": 92.06, "min": 65.83}}, "power_watts_avg": 33.27, "energy_joules_est": 131.25, "sample_count": 30, "duration_seconds": 3.945}, "timestamp": "2026-01-17T15:05:43.278013"}
{"image_index": 207, "image_name": "000000021604.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000021604.jpg", "image_width": 512, "image_height": 640, "image_resolution": "512x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2529.891, "latencies_ms": [2529.891], "images_per_second": 0.395, "prompt_tokens": 9, "response_tokens_est": 27, "n_tiles": 12, "output_text": "The image shows a man dressed in a formal black suit, white shirt, and a tie adorned with small, colorful lights.", "error": null, "sys_before": {"cpu_percent": 43.0, "ram_used_mb": 24942.4, "ram_available_mb": 100829.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 24940.9, "ram_available_mb": 100831.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.16, "peak": 16.66, "min": 13.29}, "VDD_CPU_SOC_MSS": {"avg": 15.87, "peak": 17.71, "min": 14.17}, "VDD_GPU": {"avg": 36.23, "peak": 43.71, "min": 27.57}, "VIN": {"avg": 80.63, "peak": 118.41, "min": 66.09}}, "power_watts_avg": 36.23, "energy_joules_est": 91.67, "sample_count": 19, "duration_seconds": 2.53}, "timestamp": "2026-01-17T15:05:45.915320"}
{"image_index": 207, "image_name": "000000021604.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000021604.jpg", "image_width": 512, "image_height": 640, "image_resolution": "512x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2826.044, "latencies_ms": [2826.044], "images_per_second": 0.354, "prompt_tokens": 23, "response_tokens_est": 34, "n_tiles": 12, "output_text": "1. Man\n2. Suits\n3. Tie\n4. Glasses\n5. Shirt\n6. Background\n7. Lighting\n8. Background", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24940.9, "ram_available_mb": 100831.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 24935.5, "ram_available_mb": 100836.7, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.25, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.89, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 36.29, "peak": 45.28, "min": 26.79}, "VIN": {"avg": 76.97, "peak": 104.99, "min": 61.85}}, "power_watts_avg": 36.29, "energy_joules_est": 102.57, "sample_count": 22, "duration_seconds": 2.827}, "timestamp": "2026-01-17T15:05:48.748137"}
{"image_index": 207, "image_name": "000000021604.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000021604.jpg", "image_width": 512, "image_height": 640, "image_resolution": "512x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 4126.631, "latencies_ms": [4126.631], "images_per_second": 0.242, "prompt_tokens": 27, "response_tokens_est": 72, "n_tiles": 12, "output_text": "The main object in the image is a man wearing a black suit and tie. The tie is adorned with small, red LED lights. The man is positioned in the foreground, slightly off-center to the right. The background is a plain, dark gray color, providing a stark contrast to the man's attire and the LED lights on the tie.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24935.5, "ram_available_mb": 100836.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 24934.3, "ram_available_mb": 100837.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.18, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.64, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 32.92, "peak": 45.3, "min": 26.0}, "VIN": {"avg": 76.45, "peak": 117.95, "min": 55.62}}, "power_watts_avg": 32.92, "energy_joules_est": 135.86, "sample_count": 32, "duration_seconds": 4.127}, "timestamp": "2026-01-17T15:05:52.882219"}
{"image_index": 207, "image_name": "000000021604.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000021604.jpg", "image_width": 512, "image_height": 640, "image_resolution": "512x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3609.185, "latencies_ms": [3609.185], "images_per_second": 0.277, "prompt_tokens": 21, "response_tokens_est": 57, "n_tiles": 12, "output_text": "The image depicts a man dressed in a formal black suit and tie, standing against a dark, neutral background. He is adjusting his tie, with a focused expression on his face. The lighting is soft and focused on him, highlighting his attire and creating a dramatic effect.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24934.3, "ram_available_mb": 100837.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 24934.2, "ram_available_mb": 100838.0, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.2, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.73, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 33.89, "peak": 44.91, "min": 26.0}, "VIN": {"avg": 75.33, "peak": 106.71, "min": 61.29}}, "power_watts_avg": 33.89, "energy_joules_est": 122.33, "sample_count": 28, "duration_seconds": 3.609}, "timestamp": "2026-01-17T15:05:56.497627"}
{"image_index": 207, "image_name": "000000021604.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000021604.jpg", "image_width": 512, "image_height": 640, "image_resolution": "512x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3503.662, "latencies_ms": [3503.662], "images_per_second": 0.285, "prompt_tokens": 19, "response_tokens_est": 54, "n_tiles": 12, "output_text": "The man in the image is wearing a black suit with a white shirt and a tie that has a pattern of small, red lights. The lighting is soft and focused on him, casting a gentle glow on his face and creating a dramatic contrast with the dark background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24934.2, "ram_available_mb": 100838.0, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 24934.7, "ram_available_mb": 100837.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.2, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.76, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 34.19, "peak": 45.28, "min": 26.0}, "VIN": {"avg": 73.79, "peak": 102.7, "min": 51.47}}, "power_watts_avg": 34.19, "energy_joules_est": 119.81, "sample_count": 27, "duration_seconds": 3.504}, "timestamp": "2026-01-17T15:06:00.008215"}
{"image_index": 208, "image_name": "000000021839.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000021839.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2522.04, "latencies_ms": [2522.04], "images_per_second": 0.397, "prompt_tokens": 9, "response_tokens_est": 27, "n_tiles": 12, "output_text": "A woman is walking down a street at night, passing by a building with a sign that reads \"TADU RIA.\"", "error": null, "sys_before": {"cpu_percent": 39.4, "ram_used_mb": 24937.1, "ram_available_mb": 100835.0, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.5, "ram_used_mb": 24930.6, "ram_available_mb": 100841.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.19, "peak": 16.66, "min": 13.39}, "VDD_CPU_SOC_MSS": {"avg": 15.97, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 36.4, "peak": 44.12, "min": 27.97}, "VIN": {"avg": 82.78, "peak": 129.41, "min": 60.67}}, "power_watts_avg": 36.4, "energy_joules_est": 91.81, "sample_count": 19, "duration_seconds": 2.522}, "timestamp": "2026-01-17T15:06:02.634736"}
{"image_index": 208, "image_name": "000000021839.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000021839.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2647.566, "latencies_ms": [2647.566], "images_per_second": 0.378, "prompt_tokens": 23, "response_tokens_est": 29, "n_tiles": 12, "output_text": "- Street light\n- Street sign\n- Pedestrian\n- Woman\n- Car\n- Building\n- Street\n- Storefront", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24930.6, "ram_available_mb": 100841.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.5, "ram_used_mb": 24932.1, "ram_available_mb": 100840.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.31, "peak": 16.66, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.94, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 37.22, "peak": 45.28, "min": 27.57}, "VIN": {"avg": 78.08, "peak": 113.1, "min": 60.16}}, "power_watts_avg": 37.22, "energy_joules_est": 98.56, "sample_count": 20, "duration_seconds": 2.648}, "timestamp": "2026-01-17T15:06:05.288626"}
{"image_index": 208, "image_name": "000000021839.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000021839.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3461.444, "latencies_ms": [3461.444], "images_per_second": 0.289, "prompt_tokens": 27, "response_tokens_est": 53, "n_tiles": 12, "output_text": "The main object in the foreground is a person walking on the street. The person is near the street, with a clear view of the street and buildings in the background. The street is the main focus of the image, with the person walking in the foreground.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24932.1, "ram_available_mb": 100840.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 24932.5, "ram_available_mb": 100839.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.25, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.82, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 34.77, "peak": 45.28, "min": 26.39}, "VIN": {"avg": 78.93, "peak": 112.93, "min": 60.18}}, "power_watts_avg": 34.77, "energy_joules_est": 120.37, "sample_count": 26, "duration_seconds": 3.462}, "timestamp": "2026-01-17T15:06:08.761586"}
{"image_index": 208, "image_name": "000000021839.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000021839.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2986.619, "latencies_ms": [2986.619], "images_per_second": 0.335, "prompt_tokens": 21, "response_tokens_est": 39, "n_tiles": 12, "output_text": "The image depicts a nighttime scene on a city street, with a person walking across the street. The street is lined with buildings, and there are streetlights illuminating the area.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24932.5, "ram_available_mb": 100839.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 24933.8, "ram_available_mb": 100838.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.27, "peak": 16.66, "min": 14.12}, "VDD_CPU_SOC_MSS": {"avg": 15.88, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 35.62, "peak": 44.49, "min": 26.4}, "VIN": {"avg": 76.18, "peak": 93.02, "min": 65.53}}, "power_watts_avg": 35.62, "energy_joules_est": 106.4, "sample_count": 23, "duration_seconds": 2.987}, "timestamp": "2026-01-17T15:06:11.754853"}
{"image_index": 208, "image_name": "000000021839.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000021839.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3533.692, "latencies_ms": [3533.692], "images_per_second": 0.283, "prompt_tokens": 19, "response_tokens_est": 55, "n_tiles": 12, "output_text": "The image depicts a nighttime scene with a building illuminated by warm, yellowish lights. The building's exterior is white, and it has multiple balconies with glass windows. The street is dark, and the sky is clear, indicating it is nighttime.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24933.8, "ram_available_mb": 100838.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 24934.0, "ram_available_mb": 100838.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.24, "peak": 16.56, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.76, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 34.47, "peak": 44.91, "min": 26.0}, "VIN": {"avg": 77.8, "peak": 125.34, "min": 63.28}}, "power_watts_avg": 34.47, "energy_joules_est": 121.82, "sample_count": 26, "duration_seconds": 3.534}, "timestamp": "2026-01-17T15:06:15.296976"}
{"image_index": 209, "image_name": "000000021879.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000021879.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1361.533, "latencies_ms": [1361.533], "images_per_second": 0.734, "prompt_tokens": 9, "response_tokens_est": 28, "n_tiles": 6, "output_text": "A young woman is surfing on a blue surfboard, balancing on the waves, while another person is also surfing on a blue board.", "error": null, "sys_before": {"cpu_percent": 42.3, "ram_used_mb": 24950.2, "ram_available_mb": 100822.0, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 24950.2, "ram_available_mb": 100822.0, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.67, "peak": 15.14, "min": 13.82}, "VDD_CPU_SOC_MSS": {"avg": 15.35, "peak": 16.14, "min": 14.17}, "VDD_GPU": {"avg": 33.88, "peak": 41.34, "min": 26.39}, "VIN": {"avg": 73.98, "peak": 96.93, "min": 62.91}}, "power_watts_avg": 33.88, "energy_joules_est": 46.14, "sample_count": 10, "duration_seconds": 1.362}, "timestamp": "2026-01-17T15:06:16.716594"}
{"image_index": 209, "image_name": "000000021879.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000021879.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1313.249, "latencies_ms": [1313.249], "images_per_second": 0.761, "prompt_tokens": 23, "response_tokens_est": 24, "n_tiles": 6, "output_text": "surfboard: 1\nwoman: 1\nman: 1\nwetsuit: 1", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24950.2, "ram_available_mb": 100822.0, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 24951.4, "ram_available_mb": 100820.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.84, "peak": 15.34, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.31, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 35.54, "peak": 41.34, "min": 27.97}, "VIN": {"avg": 81.85, "peak": 118.97, "min": 59.55}}, "power_watts_avg": 35.54, "energy_joules_est": 46.69, "sample_count": 9, "duration_seconds": 1.314}, "timestamp": "2026-01-17T15:06:18.035691"}
{"image_index": 209, "image_name": "000000021879.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000021879.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1972.505, "latencies_ms": [1972.505], "images_per_second": 0.507, "prompt_tokens": 27, "response_tokens_est": 47, "n_tiles": 6, "output_text": "The main objects in the image are a blue surfboard and a person riding it. The person is in the foreground, riding the surfboard. The background features a person lying on a surfboard, further back in the water.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24951.4, "ram_available_mb": 100820.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 24951.9, "ram_available_mb": 100820.3, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.83, "peak": 15.24, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.33, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 31.48, "peak": 41.34, "min": 23.64}, "VIN": {"avg": 73.34, "peak": 114.63, "min": 51.56}}, "power_watts_avg": 31.48, "energy_joules_est": 62.1, "sample_count": 15, "duration_seconds": 1.973}, "timestamp": "2026-01-17T15:06:20.014865"}
{"image_index": 209, "image_name": "000000021879.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000021879.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3044.638, "latencies_ms": [3044.638], "images_per_second": 0.328, "prompt_tokens": 21, "response_tokens_est": 85, "n_tiles": 6, "output_text": "The image captures a dynamic scene at the beach, where a young woman is surfing on a blue surfboard. She is skillfully riding a wave, with her body leaning forward and her arms outstretched for balance. In the background, another person is seen lying on a surfboard, seemingly relaxed and enjoying the ocean. The setting is a sunny day at the beach, with clear blue water and a calm atmosphere.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24951.9, "ram_available_mb": 100820.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 24952.4, "ram_available_mb": 100819.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.91, "peak": 15.24, "min": 14.3}, "VDD_CPU_SOC_MSS": {"avg": 15.54, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 28.24, "peak": 40.95, "min": 22.85}, "VIN": {"avg": 70.43, "peak": 119.31, "min": 56.93}}, "power_watts_avg": 28.24, "energy_joules_est": 86.0, "sample_count": 23, "duration_seconds": 3.045}, "timestamp": "2026-01-17T15:06:23.065860"}
{"image_index": 209, "image_name": "000000021879.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000021879.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2046.311, "latencies_ms": [2046.311], "images_per_second": 0.489, "prompt_tokens": 19, "response_tokens_est": 50, "n_tiles": 6, "output_text": "The image depicts a scene of a young woman surfing on a blue surfboard in the ocean. The water is a light blue-green color, and the waves are small and gentle. The lighting is bright and natural, suggesting it is daytime.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24952.4, "ram_available_mb": 100819.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 24952.4, "ram_available_mb": 100819.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.88, "peak": 15.34, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.33, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 30.73, "peak": 40.18, "min": 23.64}, "VIN": {"avg": 74.51, "peak": 114.05, "min": 56.72}}, "power_watts_avg": 30.73, "energy_joules_est": 62.89, "sample_count": 15, "duration_seconds": 2.047}, "timestamp": "2026-01-17T15:06:25.118081"}
{"image_index": 210, "image_name": "000000021903.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000021903.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2723.911, "latencies_ms": [2723.911], "images_per_second": 0.367, "prompt_tokens": 9, "response_tokens_est": 33, "n_tiles": 12, "output_text": "A man in a white shirt is seen in a zoo, attempting to catch a large elephant with his hands, while another person observes from behind a fence.", "error": null, "sys_before": {"cpu_percent": 41.9, "ram_used_mb": 24919.7, "ram_available_mb": 100852.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 24920.4, "ram_available_mb": 100851.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.22, "peak": 16.66, "min": 13.59}, "VDD_CPU_SOC_MSS": {"avg": 15.93, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 35.58, "peak": 44.1, "min": 26.79}, "VIN": {"avg": 83.92, "peak": 136.01, "min": 67.06}}, "power_watts_avg": 35.58, "energy_joules_est": 96.93, "sample_count": 21, "duration_seconds": 2.724}, "timestamp": "2026-01-17T15:06:27.923913"}
{"image_index": 210, "image_name": "000000021903.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000021903.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 3603.255, "latencies_ms": [3603.255], "images_per_second": 0.278, "prompt_tokens": 23, "response_tokens_est": 57, "n_tiles": 12, "output_text": "1. Elephant: 1\n2. Man: 1\n3. Person: 1\n4. Person: 1\n5. Person: 1\n6. Person: 1\n7. Person: 1\n8. Person: 1", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24920.4, "ram_available_mb": 100851.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 24921.8, "ram_available_mb": 100850.3, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.22, "peak": 16.66, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.68, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 34.11, "peak": 45.28, "min": 26.0}, "VIN": {"avg": 79.26, "peak": 141.17, "min": 63.72}}, "power_watts_avg": 34.11, "energy_joules_est": 122.92, "sample_count": 28, "duration_seconds": 3.604}, "timestamp": "2026-01-17T15:06:31.533357"}
{"image_index": 210, "image_name": "000000021903.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000021903.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3533.943, "latencies_ms": [3533.943], "images_per_second": 0.283, "prompt_tokens": 27, "response_tokens_est": 55, "n_tiles": 12, "output_text": "The main object in the foreground is a man standing near a fence, with his right hand extended towards an elephant. The elephant is positioned to the left of the man. In the background, there is a dense green hedge and a building partially obscured by the hedge.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24921.8, "ram_available_mb": 100850.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 24923.1, "ram_available_mb": 100849.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.23, "peak": 16.66, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.75, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 34.23, "peak": 45.68, "min": 26.0}, "VIN": {"avg": 75.14, "peak": 114.05, "min": 50.33}}, "power_watts_avg": 34.23, "energy_joules_est": 120.98, "sample_count": 28, "duration_seconds": 3.534}, "timestamp": "2026-01-17T15:06:35.073419"}
{"image_index": 210, "image_name": "000000021903.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000021903.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3531.966, "latencies_ms": [3531.966], "images_per_second": 0.283, "prompt_tokens": 21, "response_tokens_est": 55, "n_tiles": 12, "output_text": "The image depicts an outdoor scene with a man in a white shirt and khaki pants standing on a concrete platform, holding onto a metal fence. In the background, there is a lush green area with trees and foliage, suggesting a park or garden setting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24923.1, "ram_available_mb": 100849.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 24923.8, "ram_available_mb": 100848.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.24, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.73, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 34.32, "peak": 45.28, "min": 26.0}, "VIN": {"avg": 83.12, "peak": 136.66, "min": 56.23}}, "power_watts_avg": 34.32, "energy_joules_est": 121.23, "sample_count": 27, "duration_seconds": 3.532}, "timestamp": "2026-01-17T15:06:38.611348"}
{"image_index": 210, "image_name": "000000021903.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000021903.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3600.765, "latencies_ms": [3600.765], "images_per_second": 0.278, "prompt_tokens": 19, "response_tokens_est": 57, "n_tiles": 12, "output_text": "The image depicts an elephant standing on a concrete platform, with a man in the foreground reaching out to touch its trunk. The elephant is gray, and the man is wearing a white shirt and khaki pants. The lighting is natural, and the weather appears to be clear.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24923.8, "ram_available_mb": 100848.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 24923.8, "ram_available_mb": 100848.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.23, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.72, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 34.04, "peak": 45.28, "min": 26.0}, "VIN": {"avg": 79.24, "peak": 124.81, "min": 57.95}}, "power_watts_avg": 34.04, "energy_joules_est": 122.58, "sample_count": 28, "duration_seconds": 3.601}, "timestamp": "2026-01-17T15:06:42.218068"}
{"image_index": 211, "image_name": "000000022192.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022192.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1086.208, "latencies_ms": [1086.208], "images_per_second": 0.921, "prompt_tokens": 9, "response_tokens_est": 18, "n_tiles": 6, "output_text": "A brown dog is sitting on a bed with various clothes and items scattered around it.", "error": null, "sys_before": {"cpu_percent": 46.3, "ram_used_mb": 24943.5, "ram_available_mb": 100828.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 24945.2, "ram_available_mb": 100827.0, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.55, "peak": 15.04, "min": 13.82}, "VDD_CPU_SOC_MSS": {"avg": 15.1, "peak": 15.74, "min": 14.17}, "VDD_GPU": {"avg": 35.84, "peak": 40.95, "min": 29.94}, "VIN": {"avg": 79.53, "peak": 116.81, "min": 56.61}}, "power_watts_avg": 35.84, "energy_joules_est": 38.94, "sample_count": 8, "duration_seconds": 1.087}, "timestamp": "2026-01-17T15:06:43.365632"}
{"image_index": 211, "image_name": "000000022192.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022192.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1537.826, "latencies_ms": [1537.826], "images_per_second": 0.65, "prompt_tokens": 23, "response_tokens_est": 33, "n_tiles": 6, "output_text": "dog: 1\npillow: 1\nblanket: 1\nclothes: 1\nbox: 1\npaper: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24945.2, "ram_available_mb": 100827.0, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 24945.7, "ram_available_mb": 100826.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.97, "peak": 15.44, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.57, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 34.91, "peak": 41.74, "min": 26.39}, "VIN": {"avg": 78.66, "peak": 115.92, "min": 59.59}}, "power_watts_avg": 34.91, "energy_joules_est": 53.7, "sample_count": 11, "duration_seconds": 1.538}, "timestamp": "2026-01-17T15:06:44.909266"}
{"image_index": 211, "image_name": "000000022192.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022192.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2337.456, "latencies_ms": [2337.456], "images_per_second": 0.428, "prompt_tokens": 27, "response_tokens_est": 60, "n_tiles": 6, "output_text": "The main object in the foreground is a brown dog with a white patch on its chest. The dog is sitting on a bed with various items scattered around it, including a red bag, a white pillow, and a pink blanket. The background consists of a white curtain and a wooden door frame.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24945.7, "ram_available_mb": 100826.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 24945.7, "ram_available_mb": 100826.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.91, "peak": 15.34, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.35, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 30.26, "peak": 40.95, "min": 23.24}, "VIN": {"avg": 70.26, "peak": 109.99, "min": 56.8}}, "power_watts_avg": 30.26, "energy_joules_est": 70.75, "sample_count": 17, "duration_seconds": 2.338}, "timestamp": "2026-01-17T15:06:47.253137"}
{"image_index": 211, "image_name": "000000022192.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022192.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1939.151, "latencies_ms": [1939.151], "images_per_second": 0.516, "prompt_tokens": 21, "response_tokens_est": 46, "n_tiles": 6, "output_text": "The image depicts a cluttered bedroom setting with a brown dog sitting on a bed. The bed is covered with various items, including clothes, pillows, and a blanket, creating a messy and disorganized appearance.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24945.7, "ram_available_mb": 100826.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 24946.2, "ram_available_mb": 100826.0, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.84, "peak": 15.24, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.32, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 31.34, "peak": 40.16, "min": 24.03}, "VIN": {"avg": 74.14, "peak": 106.04, "min": 62.52}}, "power_watts_avg": 31.34, "energy_joules_est": 60.78, "sample_count": 14, "duration_seconds": 1.939}, "timestamp": "2026-01-17T15:06:49.198211"}
{"image_index": 211, "image_name": "000000022192.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022192.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2299.376, "latencies_ms": [2299.376], "images_per_second": 0.435, "prompt_tokens": 19, "response_tokens_est": 59, "n_tiles": 6, "output_text": "The image depicts a cluttered room with a brown dog sitting on a bed. The dog has a dark coat and is looking directly at the camera. The room is dimly lit, with a soft, natural light coming from the left side, creating a cozy and somewhat mysterious atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24946.2, "ram_available_mb": 100826.0, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 24946.2, "ram_available_mb": 100826.0, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.9, "peak": 15.24, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.49, "peak": 15.75, "min": 14.57}, "VDD_GPU": {"avg": 29.8, "peak": 41.34, "min": 23.24}, "VIN": {"avg": 72.01, "peak": 114.46, "min": 55.69}}, "power_watts_avg": 29.8, "energy_joules_est": 68.54, "sample_count": 18, "duration_seconds": 2.3}, "timestamp": "2026-01-17T15:06:51.503831"}
{"image_index": 212, "image_name": "000000022371.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022371.jpg", "image_width": 425, "image_height": 282, "image_resolution": "425x282", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1683.273, "latencies_ms": [1683.273], "images_per_second": 0.594, "prompt_tokens": 9, "response_tokens_est": 40, "n_tiles": 6, "output_text": "The image depicts a man dressed in a white shirt and blue tie, seated at a desk with a laptop, pen, and some papers, appearing to be deep in thought or contemplation.", "error": null, "sys_before": {"cpu_percent": 50.0, "ram_used_mb": 24946.4, "ram_available_mb": 100825.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 24947.4, "ram_available_mb": 100824.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.8, "peak": 15.22, "min": 13.9}, "VDD_CPU_SOC_MSS": {"avg": 15.47, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 31.57, "peak": 40.16, "min": 24.82}, "VIN": {"avg": 69.43, "peak": 86.0, "min": 59.24}}, "power_watts_avg": 31.57, "energy_joules_est": 53.15, "sample_count": 13, "duration_seconds": 1.684}, "timestamp": "2026-01-17T15:06:53.234740"}
{"image_index": 212, "image_name": "000000022371.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022371.jpg", "image_width": 425, "image_height": 282, "image_resolution": "425x282", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1942.836, "latencies_ms": [1942.836], "images_per_second": 0.515, "prompt_tokens": 23, "response_tokens_est": 48, "n_tiles": 6, "output_text": "object: 1 laptop\nobject: 1 pen\nobject: 1 notebook\nobject: 1 pen\nobject: 1 notebook\nobject: 1 notebook\nobject: 1 notebook\nobject: 1 notebook", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24947.4, "ram_available_mb": 100824.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 24947.6, "ram_available_mb": 100824.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.05, "peak": 15.44, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.62, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 31.49, "peak": 40.97, "min": 24.03}, "VIN": {"avg": 72.69, "peak": 113.77, "min": 52.76}}, "power_watts_avg": 31.49, "energy_joules_est": 61.19, "sample_count": 15, "duration_seconds": 1.943}, "timestamp": "2026-01-17T15:06:55.184787"}
{"image_index": 212, "image_name": "000000022371.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022371.jpg", "image_width": 425, "image_height": 282, "image_resolution": "425x282", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2762.256, "latencies_ms": [2762.256], "images_per_second": 0.362, "prompt_tokens": 27, "response_tokens_est": 78, "n_tiles": 6, "output_text": "The main objects in the image are a laptop, a pen, and a person. The laptop is positioned on the left side of the image, while the person is sitting in front of it. The pen is placed on the right side of the person, near their hand. The background is blurred, indicating that the focus is on the person and the objects in front of them.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 24947.6, "ram_available_mb": 100824.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 24948.6, "ram_available_mb": 100823.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.1, "peak": 15.44, "min": 14.43}, "VDD_CPU_SOC_MSS": {"avg": 15.66, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 28.95, "peak": 40.95, "min": 23.64}, "VIN": {"avg": 69.7, "peak": 98.14, "min": 54.94}}, "power_watts_avg": 28.95, "energy_joules_est": 79.97, "sample_count": 22, "duration_seconds": 2.763}, "timestamp": "2026-01-17T15:06:57.954155"}
{"image_index": 212, "image_name": "000000022371.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022371.jpg", "image_width": 425, "image_height": 282, "image_resolution": "425x282", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2331.185, "latencies_ms": [2331.185], "images_per_second": 0.429, "prompt_tokens": 21, "response_tokens_est": 62, "n_tiles": 6, "output_text": "The image depicts a man in a professional setting, seated at a desk with a laptop and a pen. He appears to be engaged in a task, possibly reviewing or writing something on a piece of paper. The background is minimalistic, with a light blue wall, suggesting a calm and focused environment.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24948.6, "ram_available_mb": 100823.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 24948.6, "ram_available_mb": 100823.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.08, "peak": 15.54, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.64, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 29.85, "peak": 40.16, "min": 23.64}, "VIN": {"avg": 71.47, "peak": 118.0, "min": 57.48}}, "power_watts_avg": 29.85, "energy_joules_est": 69.6, "sample_count": 18, "duration_seconds": 2.332}, "timestamp": "2026-01-17T15:07:00.291834"}
{"image_index": 212, "image_name": "000000022371.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022371.jpg", "image_width": 425, "image_height": 282, "image_resolution": "425x282", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1735.915, "latencies_ms": [1735.915], "images_per_second": 0.576, "prompt_tokens": 19, "response_tokens_est": 39, "n_tiles": 6, "output_text": "The man in the image is wearing a white shirt with a blue tie. The background is a light blue wall, and the lighting is bright, creating a clear and well-lit scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24948.6, "ram_available_mb": 100823.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 24948.6, "ram_available_mb": 100823.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.88, "peak": 15.24, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.35, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 32.03, "peak": 40.57, "min": 24.43}, "VIN": {"avg": 74.97, "peak": 106.85, "min": 62.91}}, "power_watts_avg": 32.03, "energy_joules_est": 55.62, "sample_count": 13, "duration_seconds": 1.736}, "timestamp": "2026-01-17T15:07:02.034085"}
{"image_index": 213, "image_name": "000000022396.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022396.jpg", "image_width": 640, "image_height": 495, "image_resolution": "640x495", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2320.932, "latencies_ms": [2320.932], "images_per_second": 0.431, "prompt_tokens": 9, "response_tokens_est": 21, "n_tiles": 12, "output_text": "A large, bright moon is visible in the sky, casting a soft glow over the landscape below.", "error": null, "sys_before": {"cpu_percent": 50.0, "ram_used_mb": 24914.5, "ram_available_mb": 100857.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.2, "ram_used_mb": 24915.2, "ram_available_mb": 100857.0, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.22, "peak": 16.66, "min": 13.59}, "VDD_CPU_SOC_MSS": {"avg": 15.92, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 36.7, "peak": 43.73, "min": 28.37}, "VIN": {"avg": 77.59, "peak": 100.15, "min": 65.0}}, "power_watts_avg": 36.7, "energy_joules_est": 85.19, "sample_count": 18, "duration_seconds": 2.321}, "timestamp": "2026-01-17T15:07:04.443354"}
{"image_index": 213, "image_name": "000000022396.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022396.jpg", "image_width": 640, "image_height": 495, "image_resolution": "640x495", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2858.009, "latencies_ms": [2858.009], "images_per_second": 0.35, "prompt_tokens": 23, "response_tokens_est": 35, "n_tiles": 12, "output_text": "1. Moon\n2. Airplane\n3. Sky\n4. Earth\n5. Sun\n6. Stars\n7. Clouds\n8. Atmosphere", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24915.2, "ram_available_mb": 100857.0, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 24916.2, "ram_available_mb": 100856.0, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.28, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.85, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 36.36, "peak": 45.68, "min": 26.8}, "VIN": {"avg": 82.86, "peak": 115.64, "min": 61.95}}, "power_watts_avg": 36.36, "energy_joules_est": 103.93, "sample_count": 22, "duration_seconds": 2.858}, "timestamp": "2026-01-17T15:07:07.308903"}
{"image_index": 213, "image_name": "000000022396.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022396.jpg", "image_width": 640, "image_height": 495, "image_resolution": "640x495", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3197.426, "latencies_ms": [3197.426], "images_per_second": 0.313, "prompt_tokens": 27, "response_tokens_est": 45, "n_tiles": 12, "output_text": "The main objects in the image are the moon and the airplane. The moon is positioned in the background, while the airplane is in the foreground. The airplane is closer to the viewer, while the moon is further away.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24916.2, "ram_available_mb": 100856.0, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 24916.4, "ram_available_mb": 100855.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.24, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.82, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 35.09, "peak": 45.68, "min": 26.39}, "VIN": {"avg": 74.86, "peak": 112.69, "min": 66.35}}, "power_watts_avg": 35.09, "energy_joules_est": 112.21, "sample_count": 25, "duration_seconds": 3.198}, "timestamp": "2026-01-17T15:07:10.512663"}
{"image_index": 213, "image_name": "000000022396.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022396.jpg", "image_width": 640, "image_height": 495, "image_resolution": "640x495", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3837.528, "latencies_ms": [3837.528], "images_per_second": 0.261, "prompt_tokens": 21, "response_tokens_est": 64, "n_tiles": 12, "output_text": "The image captures a clear blue sky with a single, large, and brightly lit moon visible in the background. The moon is positioned slightly to the left of the center of the image, and it appears to be a full moon, with its surface showing a range of colors from deep crimson to lighter shades.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24916.4, "ram_available_mb": 100855.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 24917.4, "ram_available_mb": 100854.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.2, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.88, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 33.47, "peak": 45.28, "min": 26.0}, "VIN": {"avg": 78.42, "peak": 125.13, "min": 63.37}}, "power_watts_avg": 33.47, "energy_joules_est": 128.46, "sample_count": 30, "duration_seconds": 3.838}, "timestamp": "2026-01-17T15:07:14.356408"}
{"image_index": 213, "image_name": "000000022396.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022396.jpg", "image_width": 640, "image_height": 495, "image_resolution": "640x495", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3530.948, "latencies_ms": [3530.948], "images_per_second": 0.283, "prompt_tokens": 19, "response_tokens_est": 55, "n_tiles": 12, "output_text": "The image features a large, partially illuminated moon with a reddish hue, set against a clear blue sky. The lighting is soft and diffused, with no harsh shadows, indicating either a cloudy day or a time when the sun is not directly overhead.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24917.4, "ram_available_mb": 100854.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 24917.1, "ram_available_mb": 100855.0, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.22, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.82, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 34.19, "peak": 44.49, "min": 26.01}, "VIN": {"avg": 78.79, "peak": 117.33, "min": 64.85}}, "power_watts_avg": 34.19, "energy_joules_est": 120.73, "sample_count": 27, "duration_seconds": 3.531}, "timestamp": "2026-01-17T15:07:17.893716"}
{"image_index": 214, "image_name": "000000022479.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022479.jpg", "image_width": 640, "image_height": 605, "image_resolution": "640x605", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 632.335, "latencies_ms": [632.335], "images_per_second": 1.581, "prompt_tokens": 9, "response_tokens_est": 25, "n_tiles": 1, "output_text": "A skateboarder is performing a trick in a skate park, with a colorful board and a focus on the action.", "error": null, "sys_before": {"cpu_percent": 24.1, "ram_used_mb": 24917.1, "ram_available_mb": 100855.0, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24918.6, "ram_available_mb": 100853.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 4845.7, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.89, "peak": 15.22, "min": 14.51}, "VDD_CPU_SOC_MSS": {"avg": 15.55, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 25.01, "peak": 27.57, "min": 22.85}, "VIN": {"avg": 62.23, "peak": 67.99, "min": 58.67}}, "power_watts_avg": 25.01, "energy_joules_est": 15.83, "sample_count": 4, "duration_seconds": 0.633}, "timestamp": "2026-01-17T15:07:18.551414"}
{"image_index": 214, "image_name": "000000022479.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022479.jpg", "image_width": 640, "image_height": 605, "image_resolution": "640x605", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1450.244, "latencies_ms": [1450.244], "images_per_second": 0.69, "prompt_tokens": 23, "response_tokens_est": 64, "n_tiles": 1, "output_text": "- skateboarder: 1\n- skateboard: 1\n- shorts: 1\n- shirt: 1\n- pants: 1\n- headphones: 1\n- park: 1\n- palm trees: 2\n- buildings: 1\n- playground: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24918.6, "ram_available_mb": 100853.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24918.6, "ram_available_mb": 100853.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 4856.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.53, "peak": 15.72, "min": 15.01}, "VDD_CPU_SOC_MSS": {"avg": 16.53, "peak": 16.93, "min": 15.74}, "VDD_GPU": {"avg": 21.85, "peak": 25.61, "min": 20.1}, "VIN": {"avg": 64.48, "peak": 70.88, "min": 56.46}}, "power_watts_avg": 21.85, "energy_joules_est": 31.69, "sample_count": 11, "duration_seconds": 1.451}, "timestamp": "2026-01-17T15:07:20.007167"}
{"image_index": 214, "image_name": "000000022479.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022479.jpg", "image_width": 640, "image_height": 605, "image_resolution": "640x605", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2122.919, "latencies_ms": [2122.919], "images_per_second": 0.471, "prompt_tokens": 27, "response_tokens_est": 95, "n_tiles": 1, "output_text": "The main object in the foreground is a skateboarder performing a trick on a concrete ramp. The skateboarder is positioned near the center of the image, with the skateboard tilted at an angle, indicating a mid-air maneuver. In the background, there are palm trees and a building, suggesting an outdoor setting. The skateboarder is slightly off-center to the left, and the background elements are further away, creating a sense of depth in the image.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24918.6, "ram_available_mb": 100853.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24919.6, "ram_available_mb": 100852.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 4859.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.59, "peak": 15.72, "min": 15.11}, "VDD_CPU_SOC_MSS": {"avg": 16.77, "peak": 16.93, "min": 16.14}, "VDD_GPU": {"avg": 20.97, "peak": 24.42, "min": 20.09}, "VIN": {"avg": 64.24, "peak": 70.18, "min": 58.67}}, "power_watts_avg": 20.97, "energy_joules_est": 44.53, "sample_count": 17, "duration_seconds": 2.123}, "timestamp": "2026-01-17T15:07:22.135950"}
{"image_index": 214, "image_name": "000000022479.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022479.jpg", "image_width": 640, "image_height": 605, "image_resolution": "640x605", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1663.34, "latencies_ms": [1663.34], "images_per_second": 0.601, "prompt_tokens": 21, "response_tokens_est": 74, "n_tiles": 1, "output_text": "The image captures a dynamic moment at a skate park, where a skateboarder is performing a trick. The skateboarder, dressed in a colorful t-shirt and black pants, is airborne, showcasing impressive skill and control. The background features a park setting with palm trees, a concrete ramp, and other recreational structures, creating a vibrant and energetic atmosphere.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 24919.6, "ram_available_mb": 100852.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24919.8, "ram_available_mb": 100852.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 4854.7, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.6, "peak": 15.72, "min": 15.22}, "VDD_CPU_SOC_MSS": {"avg": 16.63, "peak": 16.93, "min": 16.14}, "VDD_GPU": {"avg": 21.19, "peak": 24.04, "min": 20.09}, "VIN": {"avg": 63.05, "peak": 69.89, "min": 55.72}}, "power_watts_avg": 21.19, "energy_joules_est": 35.26, "sample_count": 13, "duration_seconds": 1.664}, "timestamp": "2026-01-17T15:07:23.805023"}
{"image_index": 214, "image_name": "000000022479.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022479.jpg", "image_width": 640, "image_height": 605, "image_resolution": "640x605", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 948.44, "latencies_ms": [948.44], "images_per_second": 1.054, "prompt_tokens": 19, "response_tokens_est": 41, "n_tiles": 1, "output_text": "The skateboarder is wearing a colorful tie-dye shirt and black pants. The scene is well-lit with natural sunlight, casting a vibrant glow on the skateboarder and the surrounding environment.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24919.8, "ram_available_mb": 100852.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24919.8, "ram_available_mb": 100852.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 4853.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.56, "peak": 15.62, "min": 15.32}, "VDD_CPU_SOC_MSS": {"avg": 16.54, "peak": 16.93, "min": 16.14}, "VDD_GPU": {"avg": 22.06, "peak": 24.42, "min": 20.49}, "VIN": {"avg": 63.08, "peak": 67.57, "min": 60.85}}, "power_watts_avg": 22.06, "energy_joules_est": 20.93, "sample_count": 7, "duration_seconds": 0.949}, "timestamp": "2026-01-17T15:07:24.758891"}
{"image_index": 215, "image_name": "000000022589.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022589.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2659.414, "latencies_ms": [2659.414], "images_per_second": 0.376, "prompt_tokens": 9, "response_tokens_est": 31, "n_tiles": 12, "output_text": "The image shows a group of sheep in a pastoral setting, with a wire fence enclosing them and a lush, green forest in the background.", "error": null, "sys_before": {"cpu_percent": 41.7, "ram_used_mb": 24932.0, "ram_available_mb": 100840.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 24925.5, "ram_available_mb": 100846.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.28, "peak": 16.76, "min": 13.9}, "VDD_CPU_SOC_MSS": {"avg": 16.12, "peak": 17.71, "min": 14.96}, "VDD_GPU": {"avg": 35.35, "peak": 42.94, "min": 27.19}, "VIN": {"avg": 82.94, "peak": 127.76, "min": 58.44}}, "power_watts_avg": 35.35, "energy_joules_est": 94.02, "sample_count": 20, "duration_seconds": 2.66}, "timestamp": "2026-01-17T15:07:27.507434"}
{"image_index": 215, "image_name": "000000022589.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022589.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 3031.876, "latencies_ms": [3031.876], "images_per_second": 0.33, "prompt_tokens": 23, "response_tokens_est": 40, "n_tiles": 12, "output_text": "1. Sheep\n2. Sheep\n3. Sheep\n4. Sheep\n5. Sheep\n6. Sheep\n7. Sheep\n8. Sheep", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24925.5, "ram_available_mb": 100846.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 24923.5, "ram_available_mb": 100848.7, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.24, "peak": 16.66, "min": 14.12}, "VDD_CPU_SOC_MSS": {"avg": 15.9, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 35.77, "peak": 45.28, "min": 26.39}, "VIN": {"avg": 79.49, "peak": 121.52, "min": 62.29}}, "power_watts_avg": 35.77, "energy_joules_est": 108.47, "sample_count": 23, "duration_seconds": 3.032}, "timestamp": "2026-01-17T15:07:30.546762"}
{"image_index": 215, "image_name": "000000022589.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022589.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3473.483, "latencies_ms": [3473.483], "images_per_second": 0.288, "prompt_tokens": 27, "response_tokens_est": 53, "n_tiles": 12, "output_text": "The main objects in the image are sheep, which are located in the foreground. The sheep are positioned near a wire fence, with the wire extending from the foreground to the background. The background features a lush, green forest with trees and a grassy area.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24923.5, "ram_available_mb": 100848.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 24922.9, "ram_available_mb": 100849.3, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.21, "peak": 16.56, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.86, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 34.39, "peak": 45.69, "min": 26.0}, "VIN": {"avg": 75.23, "peak": 108.45, "min": 62.73}}, "power_watts_avg": 34.39, "energy_joules_est": 119.47, "sample_count": 27, "duration_seconds": 3.474}, "timestamp": "2026-01-17T15:07:34.026816"}
{"image_index": 215, "image_name": "000000022589.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022589.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 4283.555, "latencies_ms": [4283.555], "images_per_second": 0.233, "prompt_tokens": 21, "response_tokens_est": 77, "n_tiles": 12, "output_text": "The image depicts a pastoral scene with a flock of sheep in a lush, green field. The sheep are gathered closely together, with their woolly coats visible, and they appear to be resting or grazing in a serene, natural environment. The setting is likely a rural area, with trees and grass surrounding the sheep, creating a peaceful and idyllic atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24922.9, "ram_available_mb": 100849.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 24922.9, "ram_available_mb": 100849.3, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.18, "peak": 16.56, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.69, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 32.67, "peak": 45.3, "min": 26.0}, "VIN": {"avg": 77.05, "peak": 135.39, "min": 65.94}}, "power_watts_avg": 32.67, "energy_joules_est": 139.96, "sample_count": 34, "duration_seconds": 4.284}, "timestamp": "2026-01-17T15:07:38.316698"}
{"image_index": 215, "image_name": "000000022589.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022589.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3535.04, "latencies_ms": [3535.04], "images_per_second": 0.283, "prompt_tokens": 19, "response_tokens_est": 55, "n_tiles": 12, "output_text": "The image features a flock of sheep with fluffy, light brown wool. The sheep are situated in a lush, green forested area, with a clear sky overhead. The lighting is natural, suggesting it is daytime, and the overall atmosphere is serene and peaceful.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24922.9, "ram_available_mb": 100849.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 24923.6, "ram_available_mb": 100848.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.2, "peak": 16.56, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.67, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 34.09, "peak": 45.28, "min": 26.0}, "VIN": {"avg": 77.73, "peak": 113.02, "min": 62.35}}, "power_watts_avg": 34.09, "energy_joules_est": 120.52, "sample_count": 27, "duration_seconds": 3.535}, "timestamp": "2026-01-17T15:07:41.857762"}
{"image_index": 216, "image_name": "000000022623.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022623.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2625.271, "latencies_ms": [2625.271], "images_per_second": 0.381, "prompt_tokens": 9, "response_tokens_est": 30, "n_tiles": 12, "output_text": "The image shows a close-up of a vintage-style, metallic, circular button on a computer keyboard, with a red and white symbol on it.", "error": null, "sys_before": {"cpu_percent": 41.0, "ram_used_mb": 24917.2, "ram_available_mb": 100855.0, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 24913.7, "ram_available_mb": 100858.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.19, "peak": 16.76, "min": 13.59}, "VDD_CPU_SOC_MSS": {"avg": 15.96, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 36.14, "peak": 44.12, "min": 27.57}, "VIN": {"avg": 76.86, "peak": 104.96, "min": 60.79}}, "power_watts_avg": 36.14, "energy_joules_est": 94.89, "sample_count": 20, "duration_seconds": 2.626}, "timestamp": "2026-01-17T15:07:44.573387"}
{"image_index": 216, "image_name": "000000022623.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022623.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 3129.92, "latencies_ms": [3129.92], "images_per_second": 0.319, "prompt_tokens": 23, "response_tokens_est": 43, "n_tiles": 12, "output_text": "1. Keyboard\n2. Mouse\n3. Computer mouse\n4. Computer mouse pad\n5. Computer mouse pad\n6. Computer mouse pad\n7. Computer mouse pad\n8. Computer mouse pad", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24913.7, "ram_available_mb": 100858.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 24912.9, "ram_available_mb": 100859.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.23, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.73, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 35.35, "peak": 45.69, "min": 26.39}, "VIN": {"avg": 72.89, "peak": 101.72, "min": 62.0}}, "power_watts_avg": 35.35, "energy_joules_est": 110.66, "sample_count": 24, "duration_seconds": 3.13}, "timestamp": "2026-01-17T15:07:47.709756"}
{"image_index": 216, "image_name": "000000022623.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022623.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 4012.626, "latencies_ms": [4012.626], "images_per_second": 0.249, "prompt_tokens": 27, "response_tokens_est": 69, "n_tiles": 12, "output_text": "The main object in the foreground is a close-up of a silver-colored electronic device with a button on its surface. The button has a red symbol on it. The background is slightly blurred, but it appears to be a dark surface. The device is positioned near the center of the image, with the button being the most prominent feature.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24913.4, "ram_available_mb": 100858.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 24914.4, "ram_available_mb": 100857.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.21, "peak": 16.56, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.71, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 33.13, "peak": 45.68, "min": 26.0}, "VIN": {"avg": 75.04, "peak": 112.17, "min": 59.23}}, "power_watts_avg": 33.13, "energy_joules_est": 132.95, "sample_count": 32, "duration_seconds": 4.013}, "timestamp": "2026-01-17T15:07:51.729243"}
{"image_index": 216, "image_name": "000000022623.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022623.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3705.96, "latencies_ms": [3705.96], "images_per_second": 0.27, "prompt_tokens": 21, "response_tokens_est": 60, "n_tiles": 12, "output_text": "The image shows a close-up view of a computer keyboard with a focus on the keys. The keys are illuminated by a light source, creating a warm and inviting atmosphere. The background is blurred, but it appears to be a dark room, possibly a computer lab or a home office.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24914.4, "ram_available_mb": 100857.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 24915.6, "ram_available_mb": 100856.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.21, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.72, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 33.6, "peak": 44.89, "min": 26.01}, "VIN": {"avg": 74.27, "peak": 110.69, "min": 55.92}}, "power_watts_avg": 33.6, "energy_joules_est": 124.53, "sample_count": 29, "duration_seconds": 3.706}, "timestamp": "2026-01-17T15:07:55.441501"}
{"image_index": 216, "image_name": "000000022623.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022623.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3639.359, "latencies_ms": [3639.359], "images_per_second": 0.275, "prompt_tokens": 19, "response_tokens_est": 58, "n_tiles": 12, "output_text": "The image features a close-up of a silver-colored electronic device with a glossy finish. The lighting is soft and warm, casting a gentle glow on the device's surface. The device appears to be made of a metallic material, possibly aluminum, and has a sleek, modern design.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24915.6, "ram_available_mb": 100856.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 24915.6, "ram_available_mb": 100856.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.2, "peak": 16.56, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.83, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 33.99, "peak": 45.28, "min": 26.0}, "VIN": {"avg": 76.96, "peak": 127.64, "min": 64.57}}, "power_watts_avg": 33.99, "energy_joules_est": 123.71, "sample_count": 28, "duration_seconds": 3.64}, "timestamp": "2026-01-17T15:07:59.088107"}
{"image_index": 217, "image_name": "000000022705.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022705.jpg", "image_width": 482, "image_height": 640, "image_resolution": "482x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2427.698, "latencies_ms": [2427.698], "images_per_second": 0.412, "prompt_tokens": 9, "response_tokens_est": 24, "n_tiles": 12, "output_text": "A woman is standing in a kitchen, holding a glass of orange juice, wearing a black dress and black shoes.", "error": null, "sys_before": {"cpu_percent": 43.2, "ram_used_mb": 24911.1, "ram_available_mb": 100861.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.5, "ram_used_mb": 24913.3, "ram_available_mb": 100858.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.98, "peak": 16.46, "min": 13.29}, "VDD_CPU_SOC_MSS": {"avg": 15.68, "peak": 17.32, "min": 14.17}, "VDD_GPU": {"avg": 36.78, "peak": 44.1, "min": 27.97}, "VIN": {"avg": 77.43, "peak": 117.69, "min": 66.47}}, "power_watts_avg": 36.78, "energy_joules_est": 89.3, "sample_count": 18, "duration_seconds": 2.428}, "timestamp": "2026-01-17T15:08:01.617564"}
{"image_index": 217, "image_name": "000000022705.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022705.jpg", "image_width": 482, "image_height": 640, "image_resolution": "482x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 4624.66, "latencies_ms": [4624.66], "images_per_second": 0.216, "prompt_tokens": 23, "response_tokens_est": 86, "n_tiles": 12, "output_text": "- woman: 1\n- dress: 1\n- glass: 1\n- necklace: 1\n- purse: 0\n- handbag: 0\n- shoes: 1\n- table: 0\n- cabinet: 1\n- refrigerator: 1\n- wall: 0\n- door: 0\n- countertop: 0\n- sink: 0", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24913.3, "ram_available_mb": 100858.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 24914.5, "ram_available_mb": 100857.7, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.89, "peak": 16.36, "min": 14.02}, "VDD_CPU_SOC_MSS": {"avg": 15.33, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 32.3, "peak": 46.09, "min": 25.6}, "VIN": {"avg": 75.2, "peak": 120.18, "min": 64.86}}, "power_watts_avg": 32.3, "energy_joules_est": 149.39, "sample_count": 36, "duration_seconds": 4.625}, "timestamp": "2026-01-17T15:08:06.248587"}
{"image_index": 217, "image_name": "000000022705.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022705.jpg", "image_width": 482, "image_height": 640, "image_resolution": "482x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3341.935, "latencies_ms": [3341.935], "images_per_second": 0.299, "prompt_tokens": 27, "response_tokens_est": 49, "n_tiles": 12, "output_text": "The main object in the foreground is a woman standing next to a stainless steel refrigerator. She is holding a glass of orange juice and wearing a black dress with a starry pattern. The background features wooden cabinets and a tiled backsplash.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24914.5, "ram_available_mb": 100857.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 24915.7, "ram_available_mb": 100856.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.95, "peak": 16.36, "min": 13.92}, "VDD_CPU_SOC_MSS": {"avg": 15.44, "peak": 17.32, "min": 14.17}, "VDD_GPU": {"avg": 34.52, "peak": 45.69, "min": 25.6}, "VIN": {"avg": 76.11, "peak": 116.8, "min": 56.33}}, "power_watts_avg": 34.52, "energy_joules_est": 115.38, "sample_count": 26, "duration_seconds": 3.342}, "timestamp": "2026-01-17T15:08:09.597161"}
{"image_index": 217, "image_name": "000000022705.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022705.jpg", "image_width": 482, "image_height": 640, "image_resolution": "482x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3721.178, "latencies_ms": [3721.178], "images_per_second": 0.269, "prompt_tokens": 21, "response_tokens_est": 60, "n_tiles": 12, "output_text": "The image depicts a woman standing in a kitchen, holding a glass of orange juice. She is wearing a black dress and black high heels, and the kitchen has wooden cabinets and a stainless steel refrigerator. The woman appears to be in a celebratory mood, possibly at a party or event.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24915.7, "ram_available_mb": 100856.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 24916.0, "ram_available_mb": 100856.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.93, "peak": 16.26, "min": 14.02}, "VDD_CPU_SOC_MSS": {"avg": 15.45, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 33.98, "peak": 45.3, "min": 25.6}, "VIN": {"avg": 76.92, "peak": 114.42, "min": 63.26}}, "power_watts_avg": 33.98, "energy_joules_est": 126.46, "sample_count": 28, "duration_seconds": 3.721}, "timestamp": "2026-01-17T15:08:13.328712"}
{"image_index": 217, "image_name": "000000022705.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022705.jpg", "image_width": 482, "image_height": 640, "image_resolution": "482x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3127.879, "latencies_ms": [3127.879], "images_per_second": 0.32, "prompt_tokens": 19, "response_tokens_est": 43, "n_tiles": 12, "output_text": "The image features a woman standing in a kitchen with a silver refrigerator and wooden cabinets. The kitchen has a warm, cozy atmosphere with beige and brown tones. The lighting is soft and natural, suggesting daytime.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24916.0, "ram_available_mb": 100856.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 24916.2, "ram_available_mb": 100856.0, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.97, "peak": 16.36, "min": 13.92}, "VDD_CPU_SOC_MSS": {"avg": 15.52, "peak": 17.32, "min": 14.17}, "VDD_GPU": {"avg": 35.47, "peak": 45.3, "min": 26.0}, "VIN": {"avg": 74.75, "peak": 97.05, "min": 65.36}}, "power_watts_avg": 35.47, "energy_joules_est": 110.96, "sample_count": 24, "duration_seconds": 3.128}, "timestamp": "2026-01-17T15:08:16.462860"}
{"image_index": 218, "image_name": "000000022755.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022755.jpg", "image_width": 640, "image_height": 479, "image_resolution": "640x479", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2732.576, "latencies_ms": [2732.576], "images_per_second": 0.366, "prompt_tokens": 9, "response_tokens_est": 33, "n_tiles": 12, "output_text": "The image shows a close-up view of a bus mirror reflecting a yellow school bus on a road, with other vehicles and a cloudy sky in the background.", "error": null, "sys_before": {"cpu_percent": 40.5, "ram_used_mb": 24917.7, "ram_available_mb": 100854.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 24920.1, "ram_available_mb": 100852.0, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.17, "peak": 16.66, "min": 13.29}, "VDD_CPU_SOC_MSS": {"avg": 15.86, "peak": 17.71, "min": 14.17}, "VDD_GPU": {"avg": 35.7, "peak": 44.12, "min": 27.18}, "VIN": {"avg": 81.55, "peak": 117.59, "min": 66.31}}, "power_watts_avg": 35.7, "energy_joules_est": 97.56, "sample_count": 21, "duration_seconds": 2.733}, "timestamp": "2026-01-17T15:08:19.292062"}
{"image_index": 218, "image_name": "000000022755.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022755.jpg", "image_width": 640, "image_height": 479, "image_resolution": "640x479", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2756.257, "latencies_ms": [2756.257], "images_per_second": 0.363, "prompt_tokens": 23, "response_tokens_est": 32, "n_tiles": 12, "output_text": "1. Bus\n2. Car\n3. Car\n4. Car\n5. Car\n6. Car\n7. Car\n8. Car", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24920.4, "ram_available_mb": 100851.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 24922.1, "ram_available_mb": 100850.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.28, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.88, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 36.44, "peak": 45.28, "min": 27.19}, "VIN": {"avg": 77.84, "peak": 107.97, "min": 61.25}}, "power_watts_avg": 36.44, "energy_joules_est": 100.45, "sample_count": 21, "duration_seconds": 2.757}, "timestamp": "2026-01-17T15:08:22.055104"}
{"image_index": 218, "image_name": "000000022755.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022755.jpg", "image_width": 640, "image_height": 479, "image_resolution": "640x479", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 4564.199, "latencies_ms": [4564.199], "images_per_second": 0.219, "prompt_tokens": 27, "response_tokens_est": 85, "n_tiles": 12, "output_text": "The main object in the foreground is a bicycle handlebar, which is positioned near the center of the image. The bicycle handlebar is connected to the bicycle frame, which is slightly out of focus in the background. The background features a blurred city street scene with a yellow school bus, cars, and a cloudy sky. The focus is on the bicycle handlebar, with the background elements slightly out of focus.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24922.1, "ram_available_mb": 100850.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 24922.6, "ram_available_mb": 100849.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.2, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.69, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 32.57, "peak": 45.68, "min": 26.0}, "VIN": {"avg": 75.93, "peak": 123.89, "min": 61.71}}, "power_watts_avg": 32.57, "energy_joules_est": 148.67, "sample_count": 35, "duration_seconds": 4.565}, "timestamp": "2026-01-17T15:08:26.625882"}
{"image_index": 218, "image_name": "000000022755.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022755.jpg", "image_width": 640, "image_height": 479, "image_resolution": "640x479", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 4079.138, "latencies_ms": [4079.138], "images_per_second": 0.245, "prompt_tokens": 21, "response_tokens_est": 71, "n_tiles": 12, "output_text": "The image depicts a scene from a bus stop, with a focus on a bus mirror reflecting the surroundings. The bus stop is situated outdoors, and the mirror captures a blurry view of a city street with cars and buildings. The setting suggests a typical urban environment, with the bus stop being a common place for people to wait for public transportation.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24922.6, "ram_available_mb": 100849.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 24922.8, "ram_available_mb": 100849.3, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.2, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.8, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 33.14, "peak": 44.51, "min": 26.0}, "VIN": {"avg": 73.91, "peak": 108.05, "min": 56.4}}, "power_watts_avg": 33.14, "energy_joules_est": 135.19, "sample_count": 31, "duration_seconds": 4.079}, "timestamp": "2026-01-17T15:08:30.712414"}
{"image_index": 218, "image_name": "000000022755.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022755.jpg", "image_width": 640, "image_height": 479, "image_resolution": "640x479", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3227.202, "latencies_ms": [3227.202], "images_per_second": 0.31, "prompt_tokens": 19, "response_tokens_est": 46, "n_tiles": 12, "output_text": "The image features a bus with a yellow color and a reflective surface, likely made of glass or polished metal. The lighting is dim, suggesting it is either early morning or late afternoon, with a soft, diffused light.", "error": null, "sys_before": {"cpu_percent": 16.7, "ram_used_mb": 24922.8, "ram_available_mb": 100849.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 24923.1, "ram_available_mb": 100849.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.24, "peak": 16.56, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.83, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 35.01, "peak": 45.3, "min": 26.39}, "VIN": {"avg": 77.93, "peak": 118.19, "min": 60.36}}, "power_watts_avg": 35.01, "energy_joules_est": 113.0, "sample_count": 25, "duration_seconds": 3.228}, "timestamp": "2026-01-17T15:08:33.946003"}
{"image_index": 219, "image_name": "000000022892.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022892.jpg", "image_width": 500, "image_height": 334, "image_resolution": "500x334", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1279.169, "latencies_ms": [1279.169], "images_per_second": 0.782, "prompt_tokens": 9, "response_tokens_est": 25, "n_tiles": 6, "output_text": "A gray cat is sitting on a wooden table, observing a small plant with green leaves and a small pot of soil.", "error": null, "sys_before": {"cpu_percent": 42.5, "ram_used_mb": 24942.2, "ram_available_mb": 100830.0, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 24943.7, "ram_available_mb": 100828.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.64, "peak": 15.14, "min": 13.82}, "VDD_CPU_SOC_MSS": {"avg": 15.31, "peak": 16.14, "min": 14.17}, "VDD_GPU": {"avg": 35.01, "peak": 40.97, "min": 27.97}, "VIN": {"avg": 79.97, "peak": 115.29, "min": 60.53}}, "power_watts_avg": 35.01, "energy_joules_est": 44.79, "sample_count": 9, "duration_seconds": 1.279}, "timestamp": "2026-01-17T15:08:35.283368"}
{"image_index": 219, "image_name": "000000022892.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022892.jpg", "image_width": 500, "image_height": 334, "image_resolution": "500x334", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1511.56, "latencies_ms": [1511.56], "images_per_second": 0.662, "prompt_tokens": 23, "response_tokens_est": 32, "n_tiles": 6, "output_text": "cat: 1\ndog: 1\nplant: 2\npot: 2\ntray: 1\ncans: 1", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 24943.7, "ram_available_mb": 100828.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 24945.2, "ram_available_mb": 100827.0, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.03, "peak": 15.65, "min": 14.43}, "VDD_CPU_SOC_MSS": {"avg": 15.67, "peak": 16.53, "min": 14.96}, "VDD_GPU": {"avg": 34.02, "peak": 41.34, "min": 26.0}, "VIN": {"avg": 76.18, "peak": 97.48, "min": 63.98}}, "power_watts_avg": 34.02, "energy_joules_est": 51.43, "sample_count": 11, "duration_seconds": 1.512}, "timestamp": "2026-01-17T15:08:36.801950"}
{"image_index": 219, "image_name": "000000022892.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022892.jpg", "image_width": 500, "image_height": 334, "image_resolution": "500x334", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2076.776, "latencies_ms": [2076.776], "images_per_second": 0.482, "prompt_tokens": 27, "response_tokens_est": 51, "n_tiles": 6, "output_text": "The main objects in the image are a cat and a dog. The cat is positioned in the foreground, while the dog is slightly blurred in the background. The cat is sitting on a wooden table, and the dog is standing near the table.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24945.2, "ram_available_mb": 100827.0, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 24946.4, "ram_available_mb": 100825.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.95, "peak": 15.34, "min": 14.43}, "VDD_CPU_SOC_MSS": {"avg": 15.65, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 30.99, "peak": 41.34, "min": 23.64}, "VIN": {"avg": 76.64, "peak": 117.05, "min": 56.75}}, "power_watts_avg": 30.99, "energy_joules_est": 64.37, "sample_count": 16, "duration_seconds": 2.077}, "timestamp": "2026-01-17T15:08:38.885311"}
{"image_index": 219, "image_name": "000000022892.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022892.jpg", "image_width": 500, "image_height": 334, "image_resolution": "500x334", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1960.934, "latencies_ms": [1960.934], "images_per_second": 0.51, "prompt_tokens": 21, "response_tokens_est": 47, "n_tiles": 6, "output_text": "The image depicts a cozy indoor scene with a cat and a dog sitting on a wooden table. The cat is looking at the dog, and both are surrounded by various plants and gardening supplies, suggesting a relaxed and domestic atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24946.4, "ram_available_mb": 100825.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 24946.6, "ram_available_mb": 100825.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.89, "peak": 15.34, "min": 14.3}, "VDD_CPU_SOC_MSS": {"avg": 15.51, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 30.83, "peak": 40.56, "min": 23.64}, "VIN": {"avg": 72.93, "peak": 105.68, "min": 55.71}}, "power_watts_avg": 30.83, "energy_joules_est": 60.46, "sample_count": 15, "duration_seconds": 1.961}, "timestamp": "2026-01-17T15:08:40.853256"}
{"image_index": 219, "image_name": "000000022892.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022892.jpg", "image_width": 500, "image_height": 334, "image_resolution": "500x334", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1513.823, "latencies_ms": [1513.823], "images_per_second": 0.661, "prompt_tokens": 19, "response_tokens_est": 31, "n_tiles": 6, "output_text": "The cat in the image has a gray coat and is sitting on a wooden table. The lighting is bright, and the weather appears to be clear.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 24946.6, "ram_available_mb": 100825.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 24949.3, "ram_available_mb": 100822.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.81, "peak": 15.24, "min": 14.3}, "VDD_CPU_SOC_MSS": {"avg": 15.46, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 33.48, "peak": 40.56, "min": 25.6}, "VIN": {"avg": 75.88, "peak": 105.33, "min": 55.46}}, "power_watts_avg": 33.48, "energy_joules_est": 50.7, "sample_count": 11, "duration_seconds": 1.514}, "timestamp": "2026-01-17T15:08:42.373517"}
{"image_index": 220, "image_name": "000000022935.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022935.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2630.404, "latencies_ms": [2630.404], "images_per_second": 0.38, "prompt_tokens": 9, "response_tokens_est": 30, "n_tiles": 12, "output_text": "The image shows a female soccer player in a blue jersey with a logo on the chest, holding a soccer ball, and wearing a headband.", "error": null, "sys_before": {"cpu_percent": 43.4, "ram_used_mb": 24928.8, "ram_available_mb": 100843.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 24930.2, "ram_available_mb": 100841.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.19, "peak": 16.66, "min": 13.59}, "VDD_CPU_SOC_MSS": {"avg": 15.9, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 35.96, "peak": 44.1, "min": 27.19}, "VIN": {"avg": 81.71, "peak": 115.39, "min": 61.27}}, "power_watts_avg": 35.96, "energy_joules_est": 94.6, "sample_count": 20, "duration_seconds": 2.631}, "timestamp": "2026-01-17T15:08:45.086117"}
{"image_index": 220, "image_name": "000000022935.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022935.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2744.407, "latencies_ms": [2744.407], "images_per_second": 0.364, "prompt_tokens": 23, "response_tokens_est": 32, "n_tiles": 12, "output_text": "ball: 1\nwoman: 2\nshorts: 1\nshirt: 1\nheadband: 1\nhair: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24930.2, "ram_available_mb": 100841.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 24931.2, "ram_available_mb": 100841.0, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.29, "peak": 16.66, "min": 14.12}, "VDD_CPU_SOC_MSS": {"avg": 15.82, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 36.69, "peak": 45.3, "min": 27.57}, "VIN": {"avg": 83.0, "peak": 125.3, "min": 66.99}}, "power_watts_avg": 36.69, "energy_joules_est": 100.71, "sample_count": 21, "duration_seconds": 2.745}, "timestamp": "2026-01-17T15:08:47.836952"}
{"image_index": 220, "image_name": "000000022935.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022935.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 4309.001, "latencies_ms": [4309.001], "images_per_second": 0.232, "prompt_tokens": 27, "response_tokens_est": 79, "n_tiles": 12, "output_text": "The main object in the foreground is a young woman wearing a blue sports jersey with a logo on the left side of her chest. She is holding a volleyball in her right hand, which is near her body. The background features another person wearing a yellow shirt, slightly out of focus. The volleyball and the woman are in the foreground, while the other person is in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24931.2, "ram_available_mb": 100841.0, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 24932.7, "ram_available_mb": 100839.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.27, "peak": 16.66, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.87, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 32.99, "peak": 44.89, "min": 26.0}, "VIN": {"avg": 74.86, "peak": 115.81, "min": 62.47}}, "power_watts_avg": 32.99, "energy_joules_est": 142.17, "sample_count": 33, "duration_seconds": 4.309}, "timestamp": "2026-01-17T15:08:52.152496"}
{"image_index": 220, "image_name": "000000022935.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022935.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 4175.791, "latencies_ms": [4175.791], "images_per_second": 0.239, "prompt_tokens": 21, "response_tokens_est": 75, "n_tiles": 12, "output_text": "The image captures a moment on a sports field where two female athletes are engaged in a game. The setting is outdoors, and the focus is on the players, with one of them in a blue jersey and the other in a yellow jersey. The players are actively participating in the game, with the blue jersey player in motion, possibly running or preparing to strike the ball.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24932.4, "ram_available_mb": 100839.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 24932.7, "ram_available_mb": 100839.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.29, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.86, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 33.13, "peak": 44.91, "min": 26.39}, "VIN": {"avg": 76.4, "peak": 112.96, "min": 65.6}}, "power_watts_avg": 33.13, "energy_joules_est": 138.36, "sample_count": 32, "duration_seconds": 4.176}, "timestamp": "2026-01-17T15:08:56.334910"}
{"image_index": 220, "image_name": "000000022935.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022935.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3095.142, "latencies_ms": [3095.142], "images_per_second": 0.323, "prompt_tokens": 19, "response_tokens_est": 43, "n_tiles": 12, "output_text": "The image features a young woman in a blue sports jersey with white and red accents, standing on a grassy field. The lighting is natural, suggesting it is daytime, and the weather appears to be clear.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24932.7, "ram_available_mb": 100839.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 24932.2, "ram_available_mb": 100839.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.31, "peak": 16.66, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.89, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 35.43, "peak": 44.91, "min": 26.79}, "VIN": {"avg": 76.77, "peak": 110.63, "min": 64.35}}, "power_watts_avg": 35.43, "energy_joules_est": 109.68, "sample_count": 24, "duration_seconds": 3.096}, "timestamp": "2026-01-17T15:08:59.436517"}
{"image_index": 221, "image_name": "000000022969.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022969.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2141.376, "latencies_ms": [2141.376], "images_per_second": 0.467, "prompt_tokens": 9, "response_tokens_est": 16, "n_tiles": 12, "output_text": "A giraffe is standing in a pen, eating from a wooden fence.", "error": null, "sys_before": {"cpu_percent": 47.9, "ram_used_mb": 24929.9, "ram_available_mb": 100842.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.1, "ram_used_mb": 24932.1, "ram_available_mb": 100840.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.0, "peak": 16.36, "min": 13.29}, "VDD_CPU_SOC_MSS": {"avg": 15.67, "peak": 17.32, "min": 14.17}, "VDD_GPU": {"avg": 38.21, "peak": 44.91, "min": 29.56}, "VIN": {"avg": 79.61, "peak": 129.83, "min": 62.7}}, "power_watts_avg": 38.21, "energy_joules_est": 81.84, "sample_count": 16, "duration_seconds": 2.142}, "timestamp": "2026-01-17T15:09:01.686480"}
{"image_index": 221, "image_name": "000000022969.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022969.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2287.333, "latencies_ms": [2287.333], "images_per_second": 0.437, "prompt_tokens": 23, "response_tokens_est": 19, "n_tiles": 12, "output_text": "giraffe: 1\nfence: 1\ntrees: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24932.1, "ram_available_mb": 100840.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.3, "ram_used_mb": 24932.7, "ram_available_mb": 100839.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.13, "peak": 16.26, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.74, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 39.27, "peak": 46.09, "min": 29.94}, "VIN": {"avg": 82.69, "peak": 118.42, "min": 64.92}}, "power_watts_avg": 39.27, "energy_joules_est": 89.84, "sample_count": 17, "duration_seconds": 2.288}, "timestamp": "2026-01-17T15:09:03.980376"}
{"image_index": 221, "image_name": "000000022969.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022969.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3477.692, "latencies_ms": [3477.692], "images_per_second": 0.288, "prompt_tokens": 27, "response_tokens_est": 53, "n_tiles": 12, "output_text": "The giraffes are positioned in the foreground of the image, with the giraffe on the left and the other giraffe on the right. The giraffes are near a wooden fence, which separates them from the grassy area in the background.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24932.7, "ram_available_mb": 100839.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 24933.5, "ram_available_mb": 100838.7, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.98, "peak": 16.36, "min": 14.12}, "VDD_CPU_SOC_MSS": {"avg": 15.6, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 34.79, "peak": 46.48, "min": 26.0}, "VIN": {"avg": 77.18, "peak": 115.49, "min": 65.78}}, "power_watts_avg": 34.79, "energy_joules_est": 121.0, "sample_count": 27, "duration_seconds": 3.478}, "timestamp": "2026-01-17T15:09:07.465582"}
{"image_index": 221, "image_name": "000000022969.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022969.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3203.98, "latencies_ms": [3203.98], "images_per_second": 0.312, "prompt_tokens": 21, "response_tokens_est": 45, "n_tiles": 12, "output_text": "The image depicts a giraffe standing in a grassy area with a wooden fence in the background. The giraffe appears to be eating from a patch of dirt, surrounded by lush green trees and a clear sky.", "error": null, "sys_before": {"cpu_percent": 16.7, "ram_used_mb": 24933.5, "ram_available_mb": 100838.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 24934.0, "ram_available_mb": 100838.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.97, "peak": 16.26, "min": 14.02}, "VDD_CPU_SOC_MSS": {"avg": 15.56, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 35.24, "peak": 44.91, "min": 26.0}, "VIN": {"avg": 75.11, "peak": 116.13, "min": 63.15}}, "power_watts_avg": 35.24, "energy_joules_est": 112.93, "sample_count": 24, "duration_seconds": 3.205}, "timestamp": "2026-01-17T15:09:10.680362"}
{"image_index": 221, "image_name": "000000022969.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022969.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3029.697, "latencies_ms": [3029.697], "images_per_second": 0.33, "prompt_tokens": 19, "response_tokens_est": 40, "n_tiles": 12, "output_text": "The giraffes in the image have a brown and white spotted coat, standing out against the lush greenery of the surrounding trees. The lighting is bright and natural, indicating a sunny day.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24934.0, "ram_available_mb": 100838.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 24933.7, "ram_available_mb": 100838.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.96, "peak": 16.36, "min": 13.82}, "VDD_CPU_SOC_MSS": {"avg": 15.5, "peak": 17.32, "min": 14.18}, "VDD_GPU": {"avg": 35.58, "peak": 46.09, "min": 26.39}, "VIN": {"avg": 80.4, "peak": 119.47, "min": 65.13}}, "power_watts_avg": 35.58, "energy_joules_est": 107.81, "sample_count": 24, "duration_seconds": 3.03}, "timestamp": "2026-01-17T15:09:13.716538"}
{"image_index": 222, "image_name": "000000023023.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023023.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2135.974, "latencies_ms": [2135.974], "images_per_second": 0.468, "prompt_tokens": 9, "response_tokens_est": 94, "n_tiles": 1, "output_text": "The image depicts a black and white scene featuring a large, gray suitcase with a zippered compartment on top, positioned on a carpeted floor against a backdrop of draped curtains. To the left of the suitcase, there is a large trash bag with a reflective surface, and to the right, a smaller, black, zippered bag is visible. Additionally, a stack of books is placed on the carpeted floor in front of the suitcase.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24933.9, "ram_available_mb": 100838.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24933.9, "ram_available_mb": 100838.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 4845.7, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.45, "peak": 15.82, "min": 14.2}, "VDD_CPU_SOC_MSS": {"avg": 16.37, "peak": 16.94, "min": 14.56}, "VDD_GPU": {"avg": 21.62, "peak": 27.57, "min": 20.09}, "VIN": {"avg": 63.71, "peak": 68.19, "min": 58.03}}, "power_watts_avg": 21.62, "energy_joules_est": 46.19, "sample_count": 17, "duration_seconds": 2.137}, "timestamp": "2026-01-17T15:09:15.879128"}
{"image_index": 222, "image_name": "000000023023.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023023.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1313.116, "latencies_ms": [1313.116], "images_per_second": 0.762, "prompt_tokens": 23, "response_tokens_est": 57, "n_tiles": 1, "output_text": "1. Luggage: 2\n2. Bag: 2\n3. Bag: 1\n4. Bag: 1\n5. Bag: 1\n6. Bag: 1\n7. Bag: 1\n8. Bag: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24933.9, "ram_available_mb": 100838.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24933.9, "ram_available_mb": 100838.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 4856.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.57, "peak": 15.82, "min": 15.22}, "VDD_CPU_SOC_MSS": {"avg": 16.74, "peak": 16.94, "min": 16.14}, "VDD_GPU": {"avg": 21.4, "peak": 24.04, "min": 20.09}, "VIN": {"avg": 63.71, "peak": 71.6, "min": 57.96}}, "power_watts_avg": 21.4, "energy_joules_est": 28.11, "sample_count": 10, "duration_seconds": 1.314}, "timestamp": "2026-01-17T15:09:17.197845"}
{"image_index": 222, "image_name": "000000023023.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023023.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1823.285, "latencies_ms": [1823.285], "images_per_second": 0.548, "prompt_tokens": 27, "response_tokens_est": 80, "n_tiles": 1, "output_text": "The main objects in the image are a large gray suitcase, a black trash bag, and a black bag. The suitcase is positioned in the center and slightly to the right, with the trash bag to its left and the black bag to its right. The suitcase is the most prominent object, while the trash bag and the black bag are positioned in the foreground and background, respectively.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24933.9, "ram_available_mb": 100838.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 24934.9, "ram_available_mb": 100837.3, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 4859.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.64, "peak": 15.93, "min": 15.11}, "VDD_CPU_SOC_MSS": {"avg": 16.65, "peak": 16.93, "min": 16.14}, "VDD_GPU": {"avg": 21.08, "peak": 24.43, "min": 20.09}, "VIN": {"avg": 64.15, "peak": 69.42, "min": 57.25}}, "power_watts_avg": 21.08, "energy_joules_est": 38.44, "sample_count": 14, "duration_seconds": 1.824}, "timestamp": "2026-01-17T15:09:19.026855"}
{"image_index": 222, "image_name": "000000023023.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023023.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1120.791, "latencies_ms": [1120.791], "images_per_second": 0.892, "prompt_tokens": 21, "response_tokens_est": 48, "n_tiles": 1, "output_text": "The image depicts a black and white scene featuring a suitcase, a trash bag, and a small pile of books on a carpeted floor. The setting appears to be indoors, possibly in a room with curtains in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24934.9, "ram_available_mb": 100837.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24935.2, "ram_available_mb": 100837.0, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 4854.7, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.55, "peak": 15.72, "min": 15.32}, "VDD_CPU_SOC_MSS": {"avg": 16.53, "peak": 16.93, "min": 16.14}, "VDD_GPU": {"avg": 21.62, "peak": 24.03, "min": 20.49}, "VIN": {"avg": 64.96, "peak": 71.2, "min": 61.83}}, "power_watts_avg": 21.62, "energy_joules_est": 24.24, "sample_count": 8, "duration_seconds": 1.121}, "timestamp": "2026-01-17T15:09:20.153553"}
{"image_index": 222, "image_name": "000000023023.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023023.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1662.567, "latencies_ms": [1662.567], "images_per_second": 0.601, "prompt_tokens": 19, "response_tokens_est": 73, "n_tiles": 1, "output_text": "The image features a black and white photograph of a suitcase, a trash bag, and a small black bag on a carpeted floor. The suitcase is gray with a zipper and a handle, while the trash bag is black with a reflective surface and a zipper. The lighting is soft and even, and the background consists of draped curtains.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24935.2, "ram_available_mb": 100837.0, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24935.7, "ram_available_mb": 100836.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 4853.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.61, "peak": 15.82, "min": 15.11}, "VDD_CPU_SOC_MSS": {"avg": 16.72, "peak": 16.93, "min": 16.14}, "VDD_GPU": {"avg": 21.13, "peak": 24.04, "min": 20.09}, "VIN": {"avg": 64.22, "peak": 71.69, "min": 55.82}}, "power_watts_avg": 21.13, "energy_joules_est": 35.14, "sample_count": 13, "duration_seconds": 1.663}, "timestamp": "2026-01-17T15:09:21.821801"}
{"image_index": 223, "image_name": "000000023034.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023034.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1246.978, "latencies_ms": [1246.978], "images_per_second": 0.802, "prompt_tokens": 9, "response_tokens_est": 24, "n_tiles": 6, "output_text": "A man in a blue shirt and a red bandana is riding a horse through a rocky, forested area.", "error": null, "sys_before": {"cpu_percent": 48.4, "ram_used_mb": 24951.8, "ram_available_mb": 100820.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 24953.1, "ram_available_mb": 100819.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.86, "peak": 15.34, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.7, "peak": 16.53, "min": 14.96}, "VDD_GPU": {"avg": 33.52, "peak": 38.98, "min": 27.97}, "VIN": {"avg": 82.02, "peak": 126.47, "min": 56.07}}, "power_watts_avg": 33.52, "energy_joules_est": 41.81, "sample_count": 9, "duration_seconds": 1.247}, "timestamp": "2026-01-17T15:09:23.116915"}
{"image_index": 223, "image_name": "000000023034.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023034.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1513.022, "latencies_ms": [1513.022], "images_per_second": 0.661, "prompt_tokens": 23, "response_tokens_est": 32, "n_tiles": 6, "output_text": "1. Man\n2. Horse\n3. Horse\n4. Man\n5. Horse\n6. Man\n7. Horse\n8. Man", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24953.1, "ram_available_mb": 100819.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 24953.8, "ram_available_mb": 100818.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.98, "peak": 15.44, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.57, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 34.34, "peak": 40.95, "min": 26.39}, "VIN": {"avg": 73.04, "peak": 93.91, "min": 63.1}}, "power_watts_avg": 34.34, "energy_joules_est": 51.97, "sample_count": 11, "duration_seconds": 1.513}, "timestamp": "2026-01-17T15:09:24.635796"}
{"image_index": 223, "image_name": "000000023034.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023034.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2333.329, "latencies_ms": [2333.329], "images_per_second": 0.429, "prompt_tokens": 27, "response_tokens_est": 60, "n_tiles": 6, "output_text": "The main objects in the image are a man and two horses. The man is riding a horse in the foreground, while the other horse is in the background. The man is wearing a blue shirt and has a backpack on his back. The horses are walking on a rocky path surrounded by trees.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24954.0, "ram_available_mb": 100818.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 24955.3, "ram_available_mb": 100816.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.91, "peak": 15.34, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.59, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 30.11, "peak": 41.34, "min": 23.25}, "VIN": {"avg": 74.07, "peak": 111.86, "min": 59.02}}, "power_watts_avg": 30.11, "energy_joules_est": 70.27, "sample_count": 18, "duration_seconds": 2.334}, "timestamp": "2026-01-17T15:09:26.975850"}
{"image_index": 223, "image_name": "000000023034.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023034.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2301.16, "latencies_ms": [2301.16], "images_per_second": 0.435, "prompt_tokens": 21, "response_tokens_est": 59, "n_tiles": 6, "output_text": "The image depicts a man in a blue shirt and a red bandana riding a horse through a forested area. The man is smiling and appears to be enjoying the ride. The setting is a natural, outdoor environment with trees and a dirt path, suggesting a leisurely outdoor activity.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24955.3, "ram_available_mb": 100816.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 24956.5, "ram_available_mb": 100815.7, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.89, "peak": 15.24, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.31, "peak": 15.74, "min": 14.56}, "VDD_GPU": {"avg": 29.61, "peak": 40.18, "min": 23.24}, "VIN": {"avg": 70.34, "peak": 113.44, "min": 57.45}}, "power_watts_avg": 29.61, "energy_joules_est": 68.15, "sample_count": 18, "duration_seconds": 2.302}, "timestamp": "2026-01-17T15:09:29.283220"}
{"image_index": 223, "image_name": "000000023034.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023034.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1791.486, "latencies_ms": [1791.486], "images_per_second": 0.558, "prompt_tokens": 19, "response_tokens_est": 41, "n_tiles": 6, "output_text": "The image depicts a man in a blue shirt and an orange bandana riding a horse through a forested area. The sunlight filters through the trees, casting dappled shadows on the ground.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24956.7, "ram_available_mb": 100815.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 24957.5, "ram_available_mb": 100814.7, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.84, "peak": 15.24, "min": 14.3}, "VDD_CPU_SOC_MSS": {"avg": 15.38, "peak": 15.75, "min": 14.56}, "VDD_GPU": {"avg": 31.6, "peak": 41.36, "min": 24.04}, "VIN": {"avg": 72.53, "peak": 106.48, "min": 56.27}}, "power_watts_avg": 31.6, "energy_joules_est": 56.64, "sample_count": 14, "duration_seconds": 1.792}, "timestamp": "2026-01-17T15:09:31.082048"}
{"image_index": 224, "image_name": "000000023126.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023126.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1060.784, "latencies_ms": [1060.784], "images_per_second": 0.943, "prompt_tokens": 9, "response_tokens_est": 17, "n_tiles": 6, "output_text": "A man is riding a horse in a blurry, monochrome image.", "error": null, "sys_before": {"cpu_percent": 41.9, "ram_used_mb": 24957.5, "ram_available_mb": 100814.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 24958.0, "ram_available_mb": 100814.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.6, "peak": 15.14, "min": 13.9}, "VDD_CPU_SOC_MSS": {"avg": 15.3, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 35.5, "peak": 40.56, "min": 29.54}, "VIN": {"avg": 75.2, "peak": 110.52, "min": 60.33}}, "power_watts_avg": 35.5, "energy_joules_est": 37.67, "sample_count": 8, "duration_seconds": 1.061}, "timestamp": "2026-01-17T15:09:32.190659"}
{"image_index": 224, "image_name": "000000023126.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023126.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1887.695, "latencies_ms": [1887.695], "images_per_second": 0.53, "prompt_tokens": 23, "response_tokens_est": 46, "n_tiles": 6, "output_text": "1. Horse\n2. Rider\n3. Horse's mane\n4. Horse's tail\n5. Horse's hooves\n6. Horse's bridle\n7. Horse's saddle\n8. Horse's legs", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24958.0, "ram_available_mb": 100814.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 24957.7, "ram_available_mb": 100814.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.0, "peak": 15.44, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.61, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 32.74, "peak": 41.74, "min": 24.82}, "VIN": {"avg": 74.27, "peak": 112.33, "min": 62.28}}, "power_watts_avg": 32.74, "energy_joules_est": 61.82, "sample_count": 14, "duration_seconds": 1.888}, "timestamp": "2026-01-17T15:09:34.084985"}
{"image_index": 224, "image_name": "000000023126.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023126.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2377.359, "latencies_ms": [2377.359], "images_per_second": 0.421, "prompt_tokens": 27, "response_tokens_est": 64, "n_tiles": 6, "output_text": "The main object in the foreground is a horse, which is being ridden by a man. The horse is positioned near the center of the image, slightly to the right. The background features a blurred horse, indicating motion. The man is standing on the horse's back, near the center of the image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24957.7, "ram_available_mb": 100814.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 24957.7, "ram_available_mb": 100814.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.08, "peak": 15.54, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.66, "peak": 16.53, "min": 14.56}, "VDD_GPU": {"avg": 29.91, "peak": 40.16, "min": 23.64}, "VIN": {"avg": 69.72, "peak": 94.32, "min": 59.51}}, "power_watts_avg": 29.91, "energy_joules_est": 71.12, "sample_count": 18, "duration_seconds": 2.378}, "timestamp": "2026-01-17T15:09:36.468153"}
{"image_index": 224, "image_name": "000000023126.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023126.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2427.432, "latencies_ms": [2427.432], "images_per_second": 0.412, "prompt_tokens": 21, "response_tokens_est": 66, "n_tiles": 6, "output_text": "The image depicts a man riding a horse in what appears to be a race or competition. The horse is in motion, with its mane and tail flowing, and the man is dressed in a racing outfit. The setting is an outdoor area, likely a racetrack, with a blurred background suggesting speed and movement.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24957.7, "ram_available_mb": 100814.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 24957.7, "ram_available_mb": 100814.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.1, "peak": 15.44, "min": 14.43}, "VDD_CPU_SOC_MSS": {"avg": 15.68, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 29.98, "peak": 40.56, "min": 23.64}, "VIN": {"avg": 71.92, "peak": 111.29, "min": 52.16}}, "power_watts_avg": 29.98, "energy_joules_est": 72.79, "sample_count": 18, "duration_seconds": 2.428}, "timestamp": "2026-01-17T15:09:38.905813"}
{"image_index": 224, "image_name": "000000023126.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023126.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1912.958, "latencies_ms": [1912.958], "images_per_second": 0.523, "prompt_tokens": 19, "response_tokens_est": 47, "n_tiles": 6, "output_text": "The image is a black and white photograph featuring a man riding a horse. The horse is in motion, creating a blur effect in the background. The man is wearing a jacket with a logo on the left side of his chest.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24957.7, "ram_available_mb": 100814.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 24957.9, "ram_available_mb": 100814.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.02, "peak": 15.44, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.62, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 31.25, "peak": 40.56, "min": 24.42}, "VIN": {"avg": 74.04, "peak": 119.84, "min": 59.64}}, "power_watts_avg": 31.25, "energy_joules_est": 59.79, "sample_count": 15, "duration_seconds": 1.913}, "timestamp": "2026-01-17T15:09:40.824769"}
{"image_index": 225, "image_name": "000000023230.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023230.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2628.312, "latencies_ms": [2628.312], "images_per_second": 0.38, "prompt_tokens": 9, "response_tokens_est": 30, "n_tiles": 12, "output_text": "The image depicts a serene scene of a group of geese swimming in a calm body of water surrounded by tall grass and shrubs.", "error": null, "sys_before": {"cpu_percent": 45.7, "ram_used_mb": 24924.4, "ram_available_mb": 100847.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.6, "ram_used_mb": 24925.8, "ram_available_mb": 100846.3, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.95, "peak": 16.36, "min": 13.49}, "VDD_CPU_SOC_MSS": {"avg": 15.71, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 35.78, "peak": 44.12, "min": 26.79}, "VIN": {"avg": 78.71, "peak": 129.03, "min": 58.97}}, "power_watts_avg": 35.78, "energy_joules_est": 94.05, "sample_count": 20, "duration_seconds": 2.629}, "timestamp": "2026-01-17T15:09:43.549551"}
{"image_index": 225, "image_name": "000000023230.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023230.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2198.414, "latencies_ms": [2198.414], "images_per_second": 0.455, "prompt_tokens": 23, "response_tokens_est": 16, "n_tiles": 12, "output_text": "birds: 2\nwater: 1\ngrass: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24925.8, "ram_available_mb": 100846.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.2, "ram_used_mb": 24926.8, "ram_available_mb": 100845.3, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.09, "peak": 16.26, "min": 14.02}, "VDD_CPU_SOC_MSS": {"avg": 15.65, "peak": 16.92, "min": 14.56}, "VDD_GPU": {"avg": 39.09, "peak": 45.69, "min": 31.12}, "VIN": {"avg": 87.49, "peak": 137.93, "min": 66.19}}, "power_watts_avg": 39.09, "energy_joules_est": 85.95, "sample_count": 16, "duration_seconds": 2.199}, "timestamp": "2026-01-17T15:09:45.755211"}
{"image_index": 225, "image_name": "000000023230.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023230.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3477.215, "latencies_ms": [3477.215], "images_per_second": 0.288, "prompt_tokens": 27, "response_tokens_est": 53, "n_tiles": 12, "output_text": "The main objects in the image are geese swimming in a body of water. The geese are positioned in the foreground, with their heads and necks visible. The background consists of a dense, green bushy area, which is slightly out of focus.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24926.8, "ram_available_mb": 100845.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 24927.8, "ram_available_mb": 100844.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.94, "peak": 16.36, "min": 14.02}, "VDD_CPU_SOC_MSS": {"avg": 15.44, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 34.52, "peak": 46.48, "min": 25.6}, "VIN": {"avg": 75.94, "peak": 117.75, "min": 63.52}}, "power_watts_avg": 34.52, "energy_joules_est": 120.06, "sample_count": 28, "duration_seconds": 3.478}, "timestamp": "2026-01-17T15:09:49.239710"}
{"image_index": 225, "image_name": "000000023230.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023230.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3167.927, "latencies_ms": [3167.927], "images_per_second": 0.316, "prompt_tokens": 21, "response_tokens_est": 44, "n_tiles": 12, "output_text": "The image depicts a serene natural scene featuring a calm body of water with a group of geese swimming. The geese are surrounded by tall grass and shrubs, creating a peaceful and tranquil environment.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24928.1, "ram_available_mb": 100844.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 24929.0, "ram_available_mb": 100843.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.97, "peak": 16.36, "min": 13.92}, "VDD_CPU_SOC_MSS": {"avg": 15.55, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 35.33, "peak": 44.91, "min": 26.0}, "VIN": {"avg": 77.8, "peak": 126.13, "min": 57.07}}, "power_watts_avg": 35.33, "energy_joules_est": 111.94, "sample_count": 24, "duration_seconds": 3.168}, "timestamp": "2026-01-17T15:09:52.414510"}
{"image_index": 225, "image_name": "000000023230.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023230.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3689.242, "latencies_ms": [3689.242], "images_per_second": 0.271, "prompt_tokens": 19, "response_tokens_est": 59, "n_tiles": 12, "output_text": "The image depicts a serene scene of a calm body of water with a lush green shoreline. The lighting is soft and natural, suggesting a sunny day with clear skies. The colors are primarily shades of green and brown, with the water reflecting the surrounding greenery and the sky.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24929.0, "ram_available_mb": 100843.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 24930.3, "ram_available_mb": 100841.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.93, "peak": 16.36, "min": 13.92}, "VDD_CPU_SOC_MSS": {"avg": 15.44, "peak": 17.32, "min": 14.17}, "VDD_GPU": {"avg": 33.89, "peak": 45.3, "min": 25.6}, "VIN": {"avg": 75.81, "peak": 112.43, "min": 65.25}}, "power_watts_avg": 33.89, "energy_joules_est": 125.04, "sample_count": 28, "duration_seconds": 3.689}, "timestamp": "2026-01-17T15:09:56.109747"}
{"image_index": 226, "image_name": "000000023272.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023272.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2636.481, "latencies_ms": [2636.481], "images_per_second": 0.379, "prompt_tokens": 9, "response_tokens_est": 30, "n_tiles": 12, "output_text": "A black car with a shiny chrome grille and a Mercedes emblem is parked in front of a building with a green fence and a window.", "error": null, "sys_before": {"cpu_percent": 46.3, "ram_used_mb": 24916.3, "ram_available_mb": 100855.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.5, "ram_used_mb": 24917.3, "ram_available_mb": 100854.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.12, "peak": 16.76, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 15.81, "peak": 17.71, "min": 13.78}, "VDD_GPU": {"avg": 36.04, "peak": 44.12, "min": 27.57}, "VIN": {"avg": 78.89, "peak": 111.27, "min": 60.34}}, "power_watts_avg": 36.04, "energy_joules_est": 95.03, "sample_count": 20, "duration_seconds": 2.637}, "timestamp": "2026-01-17T15:09:58.859446"}
{"image_index": 226, "image_name": "000000023272.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023272.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2794.312, "latencies_ms": [2794.312], "images_per_second": 0.358, "prompt_tokens": 23, "response_tokens_est": 33, "n_tiles": 12, "output_text": "car: 1\nmirror: 1\nbuilding: 1\nwindow: 1\ntrees: 1\nfence: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24917.3, "ram_available_mb": 100854.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 24918.3, "ram_available_mb": 100853.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.14, "peak": 16.46, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.62, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 36.32, "peak": 45.68, "min": 26.79}, "VIN": {"avg": 78.93, "peak": 118.5, "min": 60.65}}, "power_watts_avg": 36.32, "energy_joules_est": 101.51, "sample_count": 22, "duration_seconds": 2.795}, "timestamp": "2026-01-17T15:10:01.660367"}
{"image_index": 226, "image_name": "000000023272.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023272.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3867.207, "latencies_ms": [3867.207], "images_per_second": 0.259, "prompt_tokens": 27, "response_tokens_est": 64, "n_tiles": 12, "output_text": "The main objects in the image are a car and a cat. The car is positioned in the foreground, with its front end visible. The cat is perched on the car's roof, near the windshield. The background features a building and a fence, which are further away from the car and the cat.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24918.3, "ram_available_mb": 100853.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 24918.3, "ram_available_mb": 100853.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.91, "peak": 16.36, "min": 14.02}, "VDD_CPU_SOC_MSS": {"avg": 15.33, "peak": 17.32, "min": 14.17}, "VDD_GPU": {"avg": 33.49, "peak": 45.69, "min": 25.61}, "VIN": {"avg": 75.42, "peak": 119.27, "min": 57.05}}, "power_watts_avg": 33.49, "energy_joules_est": 129.53, "sample_count": 30, "duration_seconds": 3.868}, "timestamp": "2026-01-17T15:10:05.534231"}
{"image_index": 226, "image_name": "000000023272.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023272.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3272.224, "latencies_ms": [3272.224], "images_per_second": 0.306, "prompt_tokens": 21, "response_tokens_est": 47, "n_tiles": 12, "output_text": "The image depicts a scene of a black car parked in front of a building with a green fence and a tree in the background. A ginger and white cat is perched on the car's roof, seemingly enjoying the view.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24918.3, "ram_available_mb": 100853.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 24918.8, "ram_available_mb": 100853.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.94, "peak": 16.36, "min": 13.9}, "VDD_CPU_SOC_MSS": {"avg": 15.5, "peak": 17.32, "min": 14.17}, "VDD_GPU": {"avg": 34.54, "peak": 45.69, "min": 26.0}, "VIN": {"avg": 74.8, "peak": 118.95, "min": 65.75}}, "power_watts_avg": 34.54, "energy_joules_est": 113.05, "sample_count": 26, "duration_seconds": 3.273}, "timestamp": "2026-01-17T15:10:08.813096"}
{"image_index": 226, "image_name": "000000023272.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023272.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2511.093, "latencies_ms": [2511.093], "images_per_second": 0.398, "prompt_tokens": 19, "response_tokens_est": 25, "n_tiles": 12, "output_text": "The car in the image is black with a shiny, reflective surface. The lighting is bright, indicating it is daytime.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24918.8, "ram_available_mb": 100853.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.6, "ram_used_mb": 24918.8, "ram_available_mb": 100853.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.04, "peak": 16.36, "min": 13.92}, "VDD_CPU_SOC_MSS": {"avg": 15.64, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 37.63, "peak": 45.69, "min": 27.97}, "VIN": {"avg": 83.17, "peak": 124.32, "min": 53.54}}, "power_watts_avg": 37.63, "energy_joules_est": 94.51, "sample_count": 19, "duration_seconds": 2.512}, "timestamp": "2026-01-17T15:10:11.330806"}
{"image_index": 227, "image_name": "000000023359.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023359.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1334.384, "latencies_ms": [1334.384], "images_per_second": 0.749, "prompt_tokens": 9, "response_tokens_est": 27, "n_tiles": 6, "output_text": "A snowboarder is captured mid-air, performing a trick against a clear blue sky, with snowflakes scattered around him.", "error": null, "sys_before": {"cpu_percent": 47.5, "ram_used_mb": 24934.9, "ram_available_mb": 100837.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 24936.4, "ram_available_mb": 100835.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.59, "peak": 15.04, "min": 13.61}, "VDD_CPU_SOC_MSS": {"avg": 15.31, "peak": 16.14, "min": 14.17}, "VDD_GPU": {"avg": 34.31, "peak": 41.34, "min": 26.79}, "VIN": {"avg": 75.14, "peak": 114.74, "min": 61.11}}, "power_watts_avg": 34.31, "energy_joules_est": 45.79, "sample_count": 10, "duration_seconds": 1.335}, "timestamp": "2026-01-17T15:10:12.723593"}
{"image_index": 227, "image_name": "000000023359.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023359.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2218.099, "latencies_ms": [2218.099], "images_per_second": 0.451, "prompt_tokens": 23, "response_tokens_est": 56, "n_tiles": 6, "output_text": "1. Snowboarder\n2. Snow\n3. Snowboard\n4. Snowboarder's clothing\n5. Snowboarder's hat\n6. Snowboarder's gloves\n7. Snowboarder's goggles\n8. Snowboarder's helmet", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24936.4, "ram_available_mb": 100835.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 24937.4, "ram_available_mb": 100834.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.91, "peak": 15.24, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.38, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 30.54, "peak": 41.34, "min": 23.24}, "VIN": {"avg": 69.85, "peak": 106.94, "min": 60.03}}, "power_watts_avg": 30.54, "energy_joules_est": 67.75, "sample_count": 17, "duration_seconds": 2.219}, "timestamp": "2026-01-17T15:10:14.947872"}
{"image_index": 227, "image_name": "000000023359.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023359.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2617.391, "latencies_ms": [2617.391], "images_per_second": 0.382, "prompt_tokens": 27, "response_tokens_est": 70, "n_tiles": 6, "output_text": "The main object in the image is a snowboarder performing a jump over a snowy slope. The snowboarder is positioned in the foreground, with the snowy slope extending into the background. The snowboarder is near the bottom of the slope, while the snow is accumulating around them, creating a dynamic and visually engaging scene.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 24937.4, "ram_available_mb": 100834.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 24938.6, "ram_available_mb": 100833.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.92, "peak": 15.34, "min": 14.3}, "VDD_CPU_SOC_MSS": {"avg": 15.47, "peak": 15.75, "min": 14.56}, "VDD_GPU": {"avg": 29.01, "peak": 40.16, "min": 23.25}, "VIN": {"avg": 70.18, "peak": 97.6, "min": 61.99}}, "power_watts_avg": 29.01, "energy_joules_est": 75.94, "sample_count": 20, "duration_seconds": 2.618}, "timestamp": "2026-01-17T15:10:17.571283"}
{"image_index": 227, "image_name": "000000023359.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023359.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2589.9, "latencies_ms": [2589.9], "images_per_second": 0.386, "prompt_tokens": 21, "response_tokens_est": 69, "n_tiles": 6, "output_text": "The image captures a snowboarder mid-air, performing a trick against a clear blue sky. The snowboarder is dressed in a brown jacket and bright yellow pants, and the snowboard is visible beneath them. The scene is set on a snowy slope, with the snowboarder's motion creating a dynamic and visually striking image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24938.6, "ram_available_mb": 100833.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 24938.6, "ram_available_mb": 100833.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.87, "peak": 15.24, "min": 14.3}, "VDD_CPU_SOC_MSS": {"avg": 15.47, "peak": 15.75, "min": 14.56}, "VDD_GPU": {"avg": 28.91, "peak": 40.56, "min": 22.85}, "VIN": {"avg": 70.27, "peak": 118.68, "min": 57.88}}, "power_watts_avg": 28.91, "energy_joules_est": 74.89, "sample_count": 20, "duration_seconds": 2.591}, "timestamp": "2026-01-17T15:10:20.167820"}
{"image_index": 227, "image_name": "000000023359.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023359.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2186.608, "latencies_ms": [2186.608], "images_per_second": 0.457, "prompt_tokens": 19, "response_tokens_est": 55, "n_tiles": 6, "output_text": "The snowboarder is dressed in a brown jacket and bright yellow pants, contrasting sharply against the clear blue sky. The snowboard is white, and the snowboarder is captured mid-air, with snowflakes scattered around, indicating a bright and sunny day.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24938.6, "ram_available_mb": 100833.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 24938.6, "ram_available_mb": 100833.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.89, "peak": 15.34, "min": 14.3}, "VDD_CPU_SOC_MSS": {"avg": 15.45, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 30.23, "peak": 40.16, "min": 23.64}, "VIN": {"avg": 68.98, "peak": 104.86, "min": 47.82}}, "power_watts_avg": 30.23, "energy_joules_est": 66.11, "sample_count": 16, "duration_seconds": 2.187}, "timestamp": "2026-01-17T15:10:22.360554"}
{"image_index": 228, "image_name": "000000023666.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023666.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2489.755, "latencies_ms": [2489.755], "images_per_second": 0.402, "prompt_tokens": 9, "response_tokens_est": 26, "n_tiles": 12, "output_text": "The image shows a small bathroom with a white bathtub and a wooden toilet, both placed on a tiled floor.", "error": null, "sys_before": {"cpu_percent": 45.6, "ram_used_mb": 24920.0, "ram_available_mb": 100852.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.5, "ram_used_mb": 24919.4, "ram_available_mb": 100852.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.25, "peak": 16.66, "min": 13.59}, "VDD_CPU_SOC_MSS": {"avg": 15.97, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 36.42, "peak": 44.1, "min": 27.97}, "VIN": {"avg": 82.75, "peak": 136.69, "min": 65.91}}, "power_watts_avg": 36.42, "energy_joules_est": 90.69, "sample_count": 19, "duration_seconds": 2.49}, "timestamp": "2026-01-17T15:10:24.938445"}
{"image_index": 228, "image_name": "000000023666.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023666.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2791.191, "latencies_ms": [2791.191], "images_per_second": 0.358, "prompt_tokens": 23, "response_tokens_est": 33, "n_tiles": 12, "output_text": "toilet: 1\nsink: 1\ntub: 1\npipe: 1\nwall: 1\ndoor: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24919.4, "ram_available_mb": 100852.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 24920.4, "ram_available_mb": 100851.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.28, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.86, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 36.72, "peak": 45.28, "min": 27.18}, "VIN": {"avg": 83.97, "peak": 125.41, "min": 67.11}}, "power_watts_avg": 36.72, "energy_joules_est": 102.5, "sample_count": 21, "duration_seconds": 2.792}, "timestamp": "2026-01-17T15:10:27.735816"}
{"image_index": 228, "image_name": "000000023666.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023666.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3781.165, "latencies_ms": [3781.165], "images_per_second": 0.264, "prompt_tokens": 27, "response_tokens_est": 62, "n_tiles": 12, "output_text": "The main objects in the image are a toilet and a bathtub. The toilet is located in the foreground, while the bathtub is positioned in the background. The toilet is situated near the bathtub, with the bathtub's faucet visible on the right side of the image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24920.4, "ram_available_mb": 100851.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 24920.4, "ram_available_mb": 100851.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.22, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.8, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 33.87, "peak": 45.3, "min": 26.01}, "VIN": {"avg": 77.63, "peak": 111.63, "min": 57.27}}, "power_watts_avg": 33.87, "energy_joules_est": 128.08, "sample_count": 29, "duration_seconds": 3.782}, "timestamp": "2026-01-17T15:10:31.523434"}
{"image_index": 228, "image_name": "000000023666.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023666.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3939.792, "latencies_ms": [3939.792], "images_per_second": 0.254, "prompt_tokens": 21, "response_tokens_est": 67, "n_tiles": 12, "output_text": "The image depicts a small, unfinished bathroom with a wooden toilet and a white bathtub. The setting appears to be in a room with a dark wooden door and a white wall, with some pipes and a small electrical outlet visible. The overall scene suggests that the bathroom is in the process of being built or renovated.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24920.4, "ram_available_mb": 100851.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 24920.7, "ram_available_mb": 100851.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.23, "peak": 16.66, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.83, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 33.39, "peak": 44.89, "min": 26.0}, "VIN": {"avg": 71.37, "peak": 90.04, "min": 60.19}}, "power_watts_avg": 33.39, "energy_joules_est": 131.57, "sample_count": 30, "duration_seconds": 3.94}, "timestamp": "2026-01-17T15:10:35.469808"}
{"image_index": 228, "image_name": "000000023666.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023666.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3643.647, "latencies_ms": [3643.647], "images_per_second": 0.274, "prompt_tokens": 19, "response_tokens_est": 58, "n_tiles": 12, "output_text": "The image depicts a small, rustic bathroom with a wooden toilet and a white bathtub. The room is dimly lit, with a warm, earthy color palette, and the wooden elements of the toilet and bathtub add a cozy, rustic charm to the space.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24920.7, "ram_available_mb": 100851.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 24921.9, "ram_available_mb": 100850.3, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.21, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.82, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 34.15, "peak": 45.69, "min": 26.01}, "VIN": {"avg": 79.22, "peak": 136.08, "min": 63.26}}, "power_watts_avg": 34.15, "energy_joules_est": 124.45, "sample_count": 27, "duration_seconds": 3.644}, "timestamp": "2026-01-17T15:10:39.122052"}
{"image_index": 229, "image_name": "000000023751.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023751.jpg", "image_width": 430, "image_height": 640, "image_resolution": "430x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1331.504, "latencies_ms": [1331.504], "images_per_second": 0.751, "prompt_tokens": 9, "response_tokens_est": 27, "n_tiles": 6, "output_text": "The image shows a statue of a child holding a kite, set against a backdrop of a building with a light-colored facade.", "error": null, "sys_before": {"cpu_percent": 42.7, "ram_used_mb": 24943.8, "ram_available_mb": 100828.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 24944.5, "ram_available_mb": 100827.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.64, "peak": 15.11, "min": 13.72}, "VDD_CPU_SOC_MSS": {"avg": 15.31, "peak": 16.14, "min": 14.17}, "VDD_GPU": {"avg": 33.84, "peak": 40.57, "min": 26.79}, "VIN": {"avg": 80.25, "peak": 110.13, "min": 61.22}}, "power_watts_avg": 33.84, "energy_joules_est": 45.08, "sample_count": 10, "duration_seconds": 1.332}, "timestamp": "2026-01-17T15:10:40.526755"}
{"image_index": 229, "image_name": "000000023751.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023751.jpg", "image_width": 430, "image_height": 640, "image_resolution": "430x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1731.27, "latencies_ms": [1731.27], "images_per_second": 0.578, "prompt_tokens": 23, "response_tokens_est": 40, "n_tiles": 6, "output_text": "1. Statue\n2. Statue\n3. Statue\n4. Statue\n5. Statue\n6. Statue\n7. Statue\n8. Statue", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24944.5, "ram_available_mb": 100827.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 24946.5, "ram_available_mb": 100825.7, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.0, "peak": 15.44, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.59, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 32.72, "peak": 40.95, "min": 24.82}, "VIN": {"avg": 73.47, "peak": 101.94, "min": 63.37}}, "power_watts_avg": 32.72, "energy_joules_est": 56.66, "sample_count": 13, "duration_seconds": 1.732}, "timestamp": "2026-01-17T15:10:42.263868"}
{"image_index": 229, "image_name": "000000023751.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023751.jpg", "image_width": 430, "image_height": 640, "image_resolution": "430x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1806.557, "latencies_ms": [1806.557], "images_per_second": 0.554, "prompt_tokens": 27, "response_tokens_est": 43, "n_tiles": 6, "output_text": "The main objects in the image are a statue and a kite. The statue is located in the foreground, standing on a raised platform. The kite is in the background, flying high above the statue.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24946.5, "ram_available_mb": 100825.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 24947.2, "ram_available_mb": 100824.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.04, "peak": 15.44, "min": 14.4}, "VDD_CPU_SOC_MSS": {"avg": 15.63, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 32.48, "peak": 40.56, "min": 24.82}, "VIN": {"avg": 76.84, "peak": 120.89, "min": 62.75}}, "power_watts_avg": 32.48, "energy_joules_est": 58.69, "sample_count": 13, "duration_seconds": 1.807}, "timestamp": "2026-01-17T15:10:44.080383"}
{"image_index": 229, "image_name": "000000023751.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023751.jpg", "image_width": 430, "image_height": 640, "image_resolution": "430x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1749.541, "latencies_ms": [1749.541], "images_per_second": 0.572, "prompt_tokens": 21, "response_tokens_est": 41, "n_tiles": 6, "output_text": "The image depicts a statue of a child holding a kite in a city setting. The statue is situated on a ledge or platform, with a building and a clear sky in the background.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 24947.2, "ram_available_mb": 100824.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 24947.0, "ram_available_mb": 100825.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.03, "peak": 15.44, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.63, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 32.42, "peak": 40.95, "min": 24.82}, "VIN": {"avg": 74.19, "peak": 109.95, "min": 55.59}}, "power_watts_avg": 32.42, "energy_joules_est": 56.73, "sample_count": 13, "duration_seconds": 1.75}, "timestamp": "2026-01-17T15:10:45.835913"}
{"image_index": 229, "image_name": "000000023751.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023751.jpg", "image_width": 430, "image_height": 640, "image_resolution": "430x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2240.754, "latencies_ms": [2240.754], "images_per_second": 0.446, "prompt_tokens": 19, "response_tokens_est": 57, "n_tiles": 6, "output_text": "The notable visual attributes of the image include a statue of a child holding a kite, which is made of metal. The statue is set against a backdrop of a building with a light blue tiled facade. The lighting is soft and diffused, suggesting an overcast day.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 24947.0, "ram_available_mb": 100825.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 24948.2, "ram_available_mb": 100824.0, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.94, "peak": 15.34, "min": 14.43}, "VDD_CPU_SOC_MSS": {"avg": 15.59, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 30.15, "peak": 40.95, "min": 23.25}, "VIN": {"avg": 72.6, "peak": 117.55, "min": 54.91}}, "power_watts_avg": 30.15, "energy_joules_est": 67.57, "sample_count": 17, "duration_seconds": 2.241}, "timestamp": "2026-01-17T15:10:48.082654"}
{"image_index": 230, "image_name": "000000023781.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023781.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2223.217, "latencies_ms": [2223.217], "images_per_second": 0.45, "prompt_tokens": 9, "response_tokens_est": 60, "n_tiles": 6, "output_text": "The image showcases a variety of fresh vegetables and fruits arranged neatly on a table, including a basket of green beans, a bunch of broccoli, a bunch of radishes, a bunch of carrots, and a bunch of strawberries, all presented in a colorful and appetizing manner.", "error": null, "sys_before": {"cpu_percent": 38.1, "ram_used_mb": 24948.2, "ram_available_mb": 100824.0, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 24948.7, "ram_available_mb": 100823.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.89, "peak": 15.24, "min": 13.9}, "VDD_CPU_SOC_MSS": {"avg": 15.56, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 29.87, "peak": 40.56, "min": 23.64}, "VIN": {"avg": 74.73, "peak": 113.1, "min": 61.44}}, "power_watts_avg": 29.87, "energy_joules_est": 66.41, "sample_count": 17, "duration_seconds": 2.223}, "timestamp": "2026-01-17T15:10:50.357920"}
{"image_index": 230, "image_name": "000000023781.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023781.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1839.271, "latencies_ms": [1839.271], "images_per_second": 0.544, "prompt_tokens": 23, "response_tokens_est": 44, "n_tiles": 6, "output_text": "strawberries: 10\nbroccoli: 1\ncucumber: 1\nradishes: 10\npeas: 1\ncarrots: 8\npotatoes: 8", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24948.7, "ram_available_mb": 100823.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 24948.4, "ram_available_mb": 100823.7, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.03, "peak": 15.54, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.63, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 31.59, "peak": 40.16, "min": 24.42}, "VIN": {"avg": 73.0, "peak": 103.05, "min": 62.95}}, "power_watts_avg": 31.59, "energy_joules_est": 58.11, "sample_count": 14, "duration_seconds": 1.84}, "timestamp": "2026-01-17T15:10:52.203164"}
{"image_index": 230, "image_name": "000000023781.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023781.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 4126.144, "latencies_ms": [4126.144], "images_per_second": 0.242, "prompt_tokens": 27, "response_tokens_est": 129, "n_tiles": 6, "output_text": "The main objects in the image are a variety of fresh vegetables and fruits arranged on a table. The vegetables and fruits are placed in a way that showcases their spatial relationships. In the foreground, there is a white plastic bag filled with green peas, carrots, and potatoes. To the left of the bag, there is a wooden crate filled with red strawberries. Behind the strawberries, there is a large green cucumber. In the background, there are more vegetables and fruits, including a bunch of green onions, a bunch of asparagus, and a bunch of radishes. The vegetables and fruits are arranged in a way that", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24948.4, "ram_available_mb": 100823.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 24948.7, "ram_available_mb": 100823.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.15, "peak": 15.54, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.7, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 27.08, "peak": 40.16, "min": 23.24}, "VIN": {"avg": 67.83, "peak": 95.42, "min": 58.36}}, "power_watts_avg": 27.08, "energy_joules_est": 111.74, "sample_count": 32, "duration_seconds": 4.126}, "timestamp": "2026-01-17T15:10:56.335379"}
{"image_index": 230, "image_name": "000000023781.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023781.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2834.993, "latencies_ms": [2834.993], "images_per_second": 0.353, "prompt_tokens": 21, "response_tokens_est": 81, "n_tiles": 6, "output_text": "The image depicts a vibrant and colorful assortment of fresh produce arranged on a table. The scene is set in a market or grocery store, showcasing a variety of vegetables and fruits, including green beans, carrots, beets, strawberries, and radishes. The produce is displayed in baskets and containers, with the vegetables and fruits arranged neatly, creating a visually appealing and inviting display.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24948.7, "ram_available_mb": 100823.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 24949.7, "ram_available_mb": 100822.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.11, "peak": 15.54, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.66, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 28.75, "peak": 40.16, "min": 23.64}, "VIN": {"avg": 68.54, "peak": 93.4, "min": 55.57}}, "power_watts_avg": 28.75, "energy_joules_est": 81.52, "sample_count": 22, "duration_seconds": 2.835}, "timestamp": "2026-01-17T15:10:59.176369"}
{"image_index": 230, "image_name": "000000023781.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023781.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2928.029, "latencies_ms": [2928.029], "images_per_second": 0.342, "prompt_tokens": 19, "response_tokens_est": 81, "n_tiles": 6, "output_text": "The image showcases a vibrant and colorful assortment of fresh produce, including bright red strawberries, green broccoli, red radishes, green peas, carrots, and potatoes. The lighting is natural, with a soft, diffused glow that highlights the freshness and vibrancy of the colors. The overall atmosphere is warm and inviting, with a focus on the freshness and quality of the produce.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24949.7, "ram_available_mb": 100822.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 24949.4, "ram_available_mb": 100822.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.98, "peak": 15.54, "min": 14.43}, "VDD_CPU_SOC_MSS": {"avg": 15.53, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 28.41, "peak": 40.16, "min": 22.85}, "VIN": {"avg": 71.12, "peak": 115.48, "min": 62.26}}, "power_watts_avg": 28.41, "energy_joules_est": 83.2, "sample_count": 22, "duration_seconds": 2.928}, "timestamp": "2026-01-17T15:11:02.110003"}
{"image_index": 231, "image_name": "000000023899.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023899.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1329.928, "latencies_ms": [1329.928], "images_per_second": 0.752, "prompt_tokens": 9, "response_tokens_est": 27, "n_tiles": 6, "output_text": "Two young men are sitting on a bed, laughing and holding controllers, while a projector is on a table in the background.", "error": null, "sys_before": {"cpu_percent": 41.5, "ram_used_mb": 24949.4, "ram_available_mb": 100822.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 24950.6, "ram_available_mb": 100821.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.69, "peak": 15.14, "min": 13.9}, "VDD_CPU_SOC_MSS": {"avg": 15.39, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 33.52, "peak": 40.18, "min": 26.79}, "VIN": {"avg": 77.15, "peak": 110.94, "min": 62.08}}, "power_watts_avg": 33.52, "energy_joules_est": 44.59, "sample_count": 10, "duration_seconds": 1.33}, "timestamp": "2026-01-17T15:11:03.488907"}
{"image_index": 231, "image_name": "000000023899.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023899.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2159.377, "latencies_ms": [2159.377], "images_per_second": 0.463, "prompt_tokens": 23, "response_tokens_est": 56, "n_tiles": 6, "output_text": "1. Person: 2\n2. Person: 2\n3. Person: 2\n4. Person: 2\n5. Person: 2\n6. Person: 2\n7. Person: 2\n8. Person: 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24950.6, "ram_available_mb": 100821.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 24950.6, "ram_available_mb": 100821.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.05, "peak": 15.44, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.62, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 31.04, "peak": 40.95, "min": 24.03}, "VIN": {"avg": 70.37, "peak": 93.7, "min": 61.56}}, "power_watts_avg": 31.04, "energy_joules_est": 67.04, "sample_count": 16, "duration_seconds": 2.16}, "timestamp": "2026-01-17T15:11:05.655026"}
{"image_index": 231, "image_name": "000000023899.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023899.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3028.603, "latencies_ms": [3028.603], "images_per_second": 0.33, "prompt_tokens": 27, "response_tokens_est": 88, "n_tiles": 6, "output_text": "The main objects in the image are three people sitting on a couch. The person in the foreground is holding a remote control, while the person in the middle is also holding a remote control. The person in the background is not holding a remote control. The couch is positioned in the center of the image, with the people sitting on it. The background is slightly blurred, indicating that the focus is on the people and their actions.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24950.6, "ram_available_mb": 100821.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 24950.9, "ram_available_mb": 100821.3, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.11, "peak": 15.54, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.66, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 28.47, "peak": 40.16, "min": 23.64}, "VIN": {"avg": 70.5, "peak": 108.11, "min": 56.37}}, "power_watts_avg": 28.47, "energy_joules_est": 86.24, "sample_count": 23, "duration_seconds": 3.029}, "timestamp": "2026-01-17T15:11:08.689931"}
{"image_index": 231, "image_name": "000000023899.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023899.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2402.995, "latencies_ms": [2402.995], "images_per_second": 0.416, "prompt_tokens": 21, "response_tokens_est": 65, "n_tiles": 6, "output_text": "The scene depicts a cozy, dimly lit room where three individuals are engaged in a shared activity. They are seated on a couch, each holding a white remote control, and appear to be playing a video game. The atmosphere is casual and relaxed, with the focus on their interaction and the shared experience of gaming.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24950.9, "ram_available_mb": 100821.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 24950.6, "ram_available_mb": 100821.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.08, "peak": 15.44, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.64, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 29.87, "peak": 40.18, "min": 23.64}, "VIN": {"avg": 71.0, "peak": 104.91, "min": 63.72}}, "power_watts_avg": 29.87, "energy_joules_est": 71.79, "sample_count": 18, "duration_seconds": 2.403}, "timestamp": "2026-01-17T15:11:11.098734"}
{"image_index": 231, "image_name": "000000023899.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023899.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1777.569, "latencies_ms": [1777.569], "images_per_second": 0.563, "prompt_tokens": 19, "response_tokens_est": 42, "n_tiles": 6, "output_text": "The image depicts a dimly lit room with a warm, soft glow from a projector. The individuals are seated on colorful, patterned cushions, and the overall atmosphere is cozy and intimate.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 24950.6, "ram_available_mb": 100821.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 24950.9, "ram_available_mb": 100821.3, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.06, "peak": 15.44, "min": 14.43}, "VDD_CPU_SOC_MSS": {"avg": 15.62, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 32.18, "peak": 40.56, "min": 24.82}, "VIN": {"avg": 73.43, "peak": 111.45, "min": 55.61}}, "power_watts_avg": 32.18, "energy_joules_est": 57.23, "sample_count": 13, "duration_seconds": 1.778}, "timestamp": "2026-01-17T15:11:12.882598"}
{"image_index": 232, "image_name": "000000023937.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023937.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1529.003, "latencies_ms": [1529.003], "images_per_second": 0.654, "prompt_tokens": 9, "response_tokens_est": 33, "n_tiles": 6, "output_text": "The image depicts a serene pastoral scene with a group of cows lying on a lush green field, surrounded by a tree trunk and some greenery.", "error": null, "sys_before": {"cpu_percent": 38.5, "ram_used_mb": 24950.9, "ram_available_mb": 100821.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 24950.6, "ram_available_mb": 100821.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.67, "peak": 15.14, "min": 14.0}, "VDD_CPU_SOC_MSS": {"avg": 15.28, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 32.87, "peak": 40.56, "min": 25.21}, "VIN": {"avg": 74.63, "peak": 102.64, "min": 60.25}}, "power_watts_avg": 32.87, "energy_joules_est": 50.27, "sample_count": 11, "duration_seconds": 1.529}, "timestamp": "2026-01-17T15:11:14.462900"}
{"image_index": 232, "image_name": "000000023937.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023937.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1572.909, "latencies_ms": [1572.909], "images_per_second": 0.636, "prompt_tokens": 23, "response_tokens_est": 33, "n_tiles": 6, "output_text": "1. Cows\n2. Grass\n3. Trees\n4. Trees\n5. Trees\n6. Trees\n7. Trees\n8. Trees", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 24950.6, "ram_available_mb": 100821.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 24951.1, "ram_available_mb": 100821.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.83, "peak": 15.34, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.32, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 32.89, "peak": 40.95, "min": 24.83}, "VIN": {"avg": 73.31, "peak": 112.1, "min": 55.88}}, "power_watts_avg": 32.89, "energy_joules_est": 51.74, "sample_count": 12, "duration_seconds": 1.573}, "timestamp": "2026-01-17T15:11:16.041656"}
{"image_index": 232, "image_name": "000000023937.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023937.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2337.232, "latencies_ms": [2337.232], "images_per_second": 0.428, "prompt_tokens": 27, "response_tokens_est": 60, "n_tiles": 6, "output_text": "In the image, the main objects are a group of cows lying on a lush green field. The cows are positioned in the foreground, with the foreground being the grassy area where they are resting. The background features a tree trunk and some greenery, indicating the presence of a natural environment.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24951.1, "ram_available_mb": 100821.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 24951.1, "ram_available_mb": 100821.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.86, "peak": 15.24, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.35, "peak": 15.75, "min": 14.56}, "VDD_GPU": {"avg": 29.85, "peak": 40.56, "min": 23.24}, "VIN": {"avg": 67.51, "peak": 105.05, "min": 52.72}}, "power_watts_avg": 29.85, "energy_joules_est": 69.78, "sample_count": 18, "duration_seconds": 2.338}, "timestamp": "2026-01-17T15:11:18.385017"}
{"image_index": 232, "image_name": "000000023937.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023937.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2675.964, "latencies_ms": [2675.964], "images_per_second": 0.374, "prompt_tokens": 21, "response_tokens_est": 72, "n_tiles": 6, "output_text": "The image depicts a serene pastoral scene with a lush green field in the foreground, where a group of cows is peacefully resting. The cows are scattered across the grass, with some lying down and others standing. The setting appears to be a rural area, possibly a farm, with a tree trunk visible in the right side of the image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24951.1, "ram_available_mb": 100821.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 24951.3, "ram_available_mb": 100820.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.86, "peak": 15.34, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.43, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 28.87, "peak": 40.16, "min": 22.85}, "VIN": {"avg": 68.58, "peak": 112.25, "min": 57.32}}, "power_watts_avg": 28.87, "energy_joules_est": 77.27, "sample_count": 20, "duration_seconds": 2.676}, "timestamp": "2026-01-17T15:11:21.067817"}
{"image_index": 232, "image_name": "000000023937.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023937.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2134.585, "latencies_ms": [2134.585], "images_per_second": 0.468, "prompt_tokens": 19, "response_tokens_est": 53, "n_tiles": 6, "output_text": "The image depicts a lush, green field with a few cows lying down, enjoying the sunshine. The cows are primarily white with black patches, and the grass is vibrant and fresh. The lighting is bright and natural, suggesting a sunny day with clear skies.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 24951.3, "ram_available_mb": 100820.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 24951.3, "ram_available_mb": 100820.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.84, "peak": 15.14, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.42, "peak": 15.75, "min": 14.56}, "VDD_GPU": {"avg": 30.12, "peak": 40.95, "min": 23.24}, "VIN": {"avg": 68.96, "peak": 119.22, "min": 50.51}}, "power_watts_avg": 30.12, "energy_joules_est": 64.31, "sample_count": 17, "duration_seconds": 2.135}, "timestamp": "2026-01-17T15:11:23.208483"}
{"image_index": 233, "image_name": "000000024021.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000024021.jpg", "image_width": 640, "image_height": 390, "image_resolution": "640x390", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1633.115, "latencies_ms": [1633.115], "images_per_second": 0.612, "prompt_tokens": 9, "response_tokens_est": 38, "n_tiles": 6, "output_text": "The image is a black and white photograph of a large group of children and a few adults posing for a school photo in Goodmayes Boys' School, dated April 1929.", "error": null, "sys_before": {"cpu_percent": 47.4, "ram_used_mb": 24951.3, "ram_available_mb": 100820.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 24952.1, "ram_available_mb": 100820.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.7, "peak": 15.14, "min": 13.79}, "VDD_CPU_SOC_MSS": {"avg": 15.39, "peak": 16.14, "min": 14.17}, "VDD_GPU": {"avg": 31.94, "peak": 40.18, "min": 24.82}, "VIN": {"avg": 75.69, "peak": 104.94, "min": 63.9}}, "power_watts_avg": 31.94, "energy_joules_est": 52.17, "sample_count": 12, "duration_seconds": 1.633}, "timestamp": "2026-01-17T15:11:24.899778"}
{"image_index": 233, "image_name": "000000024021.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000024021.jpg", "image_width": 640, "image_height": 390, "image_resolution": "640x390", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 4277.588, "latencies_ms": [4277.588], "images_per_second": 0.234, "prompt_tokens": 23, "response_tokens_est": 128, "n_tiles": 6, "output_text": "object: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject:", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 24952.1, "ram_available_mb": 100820.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 24952.3, "ram_available_mb": 100819.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.92, "peak": 15.34, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.4, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 26.55, "peak": 40.56, "min": 22.85}, "VIN": {"avg": 66.47, "peak": 91.51, "min": 59.04}}, "power_watts_avg": 26.55, "energy_joules_est": 113.58, "sample_count": 33, "duration_seconds": 4.278}, "timestamp": "2026-01-17T15:11:29.183510"}
{"image_index": 233, "image_name": "000000024021.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000024021.jpg", "image_width": 640, "image_height": 390, "image_resolution": "640x390", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2392.696, "latencies_ms": [2392.696], "images_per_second": 0.418, "prompt_tokens": 27, "response_tokens_est": 62, "n_tiles": 6, "output_text": "The main objects in the image are the students of Goodmayes Boys' School, arranged in a large group. The students are positioned in the foreground, with the school building serving as the background. The students are arranged in a somewhat staggered formation, with some sitting on the ground and others standing.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24952.3, "ram_available_mb": 100819.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 24952.8, "ram_available_mb": 100819.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.86, "peak": 15.24, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.51, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 29.48, "peak": 40.18, "min": 23.24}, "VIN": {"avg": 68.08, "peak": 89.46, "min": 59.03}}, "power_watts_avg": 29.48, "energy_joules_est": 70.55, "sample_count": 18, "duration_seconds": 2.393}, "timestamp": "2026-01-17T15:11:31.582541"}
{"image_index": 233, "image_name": "000000024021.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000024021.jpg", "image_width": 640, "image_height": 390, "image_resolution": "640x390", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2449.117, "latencies_ms": [2449.117], "images_per_second": 0.408, "prompt_tokens": 21, "response_tokens_est": 64, "n_tiles": 6, "output_text": "The image depicts a large group of children, likely from a school, gathered in a schoolyard or courtyard. They are dressed in school uniforms, with some sitting on the ground and others standing. The setting appears to be an outdoor location, possibly a school campus, with a brick building in the background.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24952.8, "ram_available_mb": 100819.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 24953.8, "ram_available_mb": 100818.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.88, "peak": 15.24, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.6, "peak": 16.14, "min": 14.57}, "VDD_GPU": {"avg": 29.3, "peak": 40.57, "min": 23.25}, "VIN": {"avg": 71.92, "peak": 112.63, "min": 60.6}}, "power_watts_avg": 29.3, "energy_joules_est": 71.77, "sample_count": 19, "duration_seconds": 2.45}, "timestamp": "2026-01-17T15:11:34.037887"}
{"image_index": 233, "image_name": "000000024021.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000024021.jpg", "image_width": 640, "image_height": 390, "image_resolution": "640x390", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1882.002, "latencies_ms": [1882.002], "images_per_second": 0.531, "prompt_tokens": 19, "response_tokens_est": 44, "n_tiles": 6, "output_text": "The black and white photograph features a large group of children and a few adults posing for a school photo. The lighting is natural, and the image has a vintage feel, likely from the early 20th century.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24953.8, "ram_available_mb": 100818.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 24953.8, "ram_available_mb": 100818.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.83, "peak": 15.34, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.44, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 31.18, "peak": 40.18, "min": 24.04}, "VIN": {"avg": 69.94, "peak": 88.99, "min": 63.22}}, "power_watts_avg": 31.18, "energy_joules_est": 58.69, "sample_count": 14, "duration_seconds": 1.882}, "timestamp": "2026-01-17T15:11:35.925945"}
{"image_index": 234, "image_name": "000000024027.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000024027.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2995.632, "latencies_ms": [2995.632], "images_per_second": 0.334, "prompt_tokens": 9, "response_tokens_est": 41, "n_tiles": 12, "output_text": "The image depicts a vibrant kite flying high in the sky, with a clear blue sky and fluffy white clouds in the background, surrounded by a park with green grass and a few trees.", "error": null, "sys_before": {"cpu_percent": 46.3, "ram_used_mb": 24914.8, "ram_available_mb": 100857.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 24916.0, "ram_available_mb": 100856.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.17, "peak": 16.66, "min": 13.39}, "VDD_CPU_SOC_MSS": {"avg": 15.83, "peak": 17.71, "min": 14.18}, "VDD_GPU": {"avg": 34.56, "peak": 44.12, "min": 26.39}, "VIN": {"avg": 75.67, "peak": 109.09, "min": 66.23}}, "power_watts_avg": 34.56, "energy_joules_est": 103.54, "sample_count": 23, "duration_seconds": 2.996}, "timestamp": "2026-01-17T15:11:39.005550"}
{"image_index": 234, "image_name": "000000024027.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000024027.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 3025.908, "latencies_ms": [3025.908], "images_per_second": 0.33, "prompt_tokens": 23, "response_tokens_est": 40, "n_tiles": 12, "output_text": "object: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24916.0, "ram_available_mb": 100856.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 24916.5, "ram_available_mb": 100855.7, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.26, "peak": 16.56, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.78, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 35.72, "peak": 45.68, "min": 26.39}, "VIN": {"avg": 77.33, "peak": 115.22, "min": 54.86}}, "power_watts_avg": 35.72, "energy_joules_est": 108.1, "sample_count": 23, "duration_seconds": 3.026}, "timestamp": "2026-01-17T15:11:42.037860"}
{"image_index": 234, "image_name": "000000024027.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000024027.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3839.844, "latencies_ms": [3839.844], "images_per_second": 0.26, "prompt_tokens": 27, "response_tokens_est": 64, "n_tiles": 12, "output_text": "The main objects in the image are a kite and a person. The kite is in the foreground, flying high in the sky, while the person is near the kite, standing on the grass. The background features a park with trees and buildings, providing a serene setting for the kite flying.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24916.5, "ram_available_mb": 100855.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 24917.7, "ram_available_mb": 100854.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.22, "peak": 16.66, "min": 14.12}, "VDD_CPU_SOC_MSS": {"avg": 15.79, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 33.44, "peak": 45.68, "min": 26.0}, "VIN": {"avg": 73.37, "peak": 117.25, "min": 56.95}}, "power_watts_avg": 33.44, "energy_joules_est": 128.41, "sample_count": 30, "duration_seconds": 3.84}, "timestamp": "2026-01-17T15:11:45.885390"}
{"image_index": 234, "image_name": "000000024027.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000024027.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3667.557, "latencies_ms": [3667.557], "images_per_second": 0.273, "prompt_tokens": 21, "response_tokens_est": 59, "n_tiles": 12, "output_text": "The image depicts a serene outdoor scene with a clear blue sky and scattered clouds. In the foreground, there is a grassy field with a few trees and a few buildings in the background. A person is seen walking on the grass, and there are a few cars parked nearby.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24917.7, "ram_available_mb": 100854.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 24933.7, "ram_available_mb": 100838.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.22, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.76, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 33.72, "peak": 44.89, "min": 26.0}, "VIN": {"avg": 73.31, "peak": 108.43, "min": 58.13}}, "power_watts_avg": 33.72, "energy_joules_est": 123.69, "sample_count": 28, "duration_seconds": 3.668}, "timestamp": "2026-01-17T15:11:49.559504"}
{"image_index": 234, "image_name": "000000024027.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000024027.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3978.017, "latencies_ms": [3978.017], "images_per_second": 0.251, "prompt_tokens": 19, "response_tokens_est": 68, "n_tiles": 12, "output_text": "The image depicts a vibrant kite flying in the sky, with a striking combination of colors including purple, blue, and orange. The kite is illuminated by the sunlight, casting a warm glow on the scene. The sky is partly cloudy, with patches of blue and white clouds, creating a dynamic and lively atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24933.7, "ram_available_mb": 100838.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 24932.6, "ram_available_mb": 100839.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.18, "peak": 16.66, "min": 14.12}, "VDD_CPU_SOC_MSS": {"avg": 15.71, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 33.19, "peak": 45.69, "min": 26.0}, "VIN": {"avg": 74.52, "peak": 118.88, "min": 56.4}}, "power_watts_avg": 33.19, "energy_joules_est": 132.04, "sample_count": 31, "duration_seconds": 3.978}, "timestamp": "2026-01-17T15:11:53.543966"}
{"image_index": 235, "image_name": "000000024144.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000024144.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2434.542, "latencies_ms": [2434.542], "images_per_second": 0.411, "prompt_tokens": 9, "response_tokens_est": 24, "n_tiles": 12, "output_text": "The image shows a slice of pizza with melted cheese and pepperoni toppings, resting on a cardboard pizza box.", "error": null, "sys_before": {"cpu_percent": 30.8, "ram_used_mb": 24925.9, "ram_available_mb": 100846.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.4, "ram_used_mb": 24924.3, "ram_available_mb": 100847.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.2, "peak": 16.66, "min": 13.61}, "VDD_CPU_SOC_MSS": {"avg": 15.9, "peak": 17.71, "min": 14.17}, "VDD_GPU": {"avg": 37.09, "peak": 44.49, "min": 28.36}, "VIN": {"avg": 77.87, "peak": 107.98, "min": 65.34}}, "power_watts_avg": 37.09, "energy_joules_est": 90.31, "sample_count": 18, "duration_seconds": 2.435}, "timestamp": "2026-01-17T15:11:56.054466"}
{"image_index": 235, "image_name": "000000024144.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000024144.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2919.938, "latencies_ms": [2919.938], "images_per_second": 0.342, "prompt_tokens": 23, "response_tokens_est": 37, "n_tiles": 12, "output_text": "1. Pizza\n2. Pizza box\n3. Pita bread\n4. Cheese\n5. Tomato\n6. Pepperoni\n7. Sauce\n8. Meat", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24924.3, "ram_available_mb": 100847.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 24924.6, "ram_available_mb": 100847.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.28, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.89, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 36.06, "peak": 45.28, "min": 26.79}, "VIN": {"avg": 79.95, "peak": 116.6, "min": 65.14}}, "power_watts_avg": 36.06, "energy_joules_est": 105.32, "sample_count": 22, "duration_seconds": 2.921}, "timestamp": "2026-01-17T15:11:58.981722"}
{"image_index": 235, "image_name": "000000024144.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000024144.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3590.276, "latencies_ms": [3590.276], "images_per_second": 0.279, "prompt_tokens": 27, "response_tokens_est": 56, "n_tiles": 12, "output_text": "The main object in the image is a pizza placed on a cardboard box. The pizza is positioned in the foreground, with its crust and toppings clearly visible. The cardboard box is situated in the background, providing a neutral backdrop that contrasts with the pizza's vibrant colors.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24924.6, "ram_available_mb": 100847.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 24924.4, "ram_available_mb": 100847.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.02, "peak": 16.46, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.54, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 34.04, "peak": 45.68, "min": 25.6}, "VIN": {"avg": 72.17, "peak": 113.25, "min": 55.47}}, "power_watts_avg": 34.04, "energy_joules_est": 122.23, "sample_count": 28, "duration_seconds": 3.591}, "timestamp": "2026-01-17T15:12:02.579552"}
{"image_index": 235, "image_name": "000000024144.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000024144.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3578.215, "latencies_ms": [3578.215], "images_per_second": 0.279, "prompt_tokens": 21, "response_tokens_est": 56, "n_tiles": 12, "output_text": "The image shows a slice of pizza placed on a cardboard pizza box. The pizza appears to be freshly baked, with melted cheese and a crispy crust. The setting suggests that the pizza is ready to be eaten, and the box is likely being used for transport or storage.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24924.4, "ram_available_mb": 100847.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 24924.4, "ram_available_mb": 100847.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.94, "peak": 16.26, "min": 14.02}, "VDD_CPU_SOC_MSS": {"avg": 15.47, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 34.13, "peak": 44.91, "min": 25.6}, "VIN": {"avg": 78.43, "peak": 112.82, "min": 65.11}}, "power_watts_avg": 34.13, "energy_joules_est": 122.14, "sample_count": 27, "duration_seconds": 3.579}, "timestamp": "2026-01-17T15:12:06.164886"}
{"image_index": 235, "image_name": "000000024144.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000024144.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3164.104, "latencies_ms": [3164.104], "images_per_second": 0.316, "prompt_tokens": 19, "response_tokens_est": 44, "n_tiles": 12, "output_text": "The image shows a freshly baked pizza with a golden-brown crust, melted cheese, and a red tomato topping. The lighting is bright, highlighting the colors of the pizza and the cardboard box it is placed on.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24924.4, "ram_available_mb": 100847.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 24925.4, "ram_available_mb": 100846.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.98, "peak": 16.26, "min": 14.02}, "VDD_CPU_SOC_MSS": {"avg": 15.5, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 35.12, "peak": 44.91, "min": 26.0}, "VIN": {"avg": 78.79, "peak": 125.2, "min": 64.97}}, "power_watts_avg": 35.12, "energy_joules_est": 111.14, "sample_count": 24, "duration_seconds": 3.165}, "timestamp": "2026-01-17T15:12:09.335381"}
{"image_index": 236, "image_name": "000000024243.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000024243.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2453.755, "latencies_ms": [2453.755], "images_per_second": 0.408, "prompt_tokens": 9, "response_tokens_est": 25, "n_tiles": 12, "output_text": "A woman is sitting on a bench outside a refrigerator, holding a beer in her hand, and smiling at the camera.", "error": null, "sys_before": {"cpu_percent": 41.5, "ram_used_mb": 24939.4, "ram_available_mb": 100832.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.5, "ram_used_mb": 24936.6, "ram_available_mb": 100835.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.16, "peak": 16.66, "min": 13.29}, "VDD_CPU_SOC_MSS": {"avg": 15.93, "peak": 17.71, "min": 14.18}, "VDD_GPU": {"avg": 36.63, "peak": 44.51, "min": 27.97}, "VIN": {"avg": 83.98, "peak": 136.34, "min": 67.11}}, "power_watts_avg": 36.63, "energy_joules_est": 89.9, "sample_count": 19, "duration_seconds": 2.454}, "timestamp": "2026-01-17T15:12:11.884686"}
{"image_index": 236, "image_name": "000000024243.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000024243.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 3572.133, "latencies_ms": [3572.133], "images_per_second": 0.28, "prompt_tokens": 23, "response_tokens_est": 56, "n_tiles": 12, "output_text": "1. Woman: 1\n2. Woman: 1\n3. Woman: 1\n4. Woman: 1\n5. Woman: 1\n6. Woman: 1\n7. Woman: 1\n8. Woman: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24936.6, "ram_available_mb": 100835.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 24930.6, "ram_available_mb": 100841.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.23, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.8, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 34.27, "peak": 45.28, "min": 26.0}, "VIN": {"avg": 74.04, "peak": 104.44, "min": 62.84}}, "power_watts_avg": 34.27, "energy_joules_est": 122.43, "sample_count": 28, "duration_seconds": 3.573}, "timestamp": "2026-01-17T15:12:15.463165"}
{"image_index": 236, "image_name": "000000024243.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000024243.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3984.442, "latencies_ms": [3984.442], "images_per_second": 0.251, "prompt_tokens": 27, "response_tokens_est": 68, "n_tiles": 12, "output_text": "The main objects in the image are a woman sitting on a bench and a refrigerator. The woman is positioned in the foreground, sitting on the bench, while the refrigerator is located in the background, slightly to the right. The bench is situated near the refrigerator, and the woman is interacting with the refrigerator, possibly using it for cooling.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24930.6, "ram_available_mb": 100841.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 24928.1, "ram_available_mb": 100844.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.21, "peak": 16.66, "min": 14.12}, "VDD_CPU_SOC_MSS": {"avg": 15.76, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 33.26, "peak": 45.68, "min": 26.0}, "VIN": {"avg": 73.88, "peak": 116.75, "min": 59.13}}, "power_watts_avg": 33.26, "energy_joules_est": 132.54, "sample_count": 31, "duration_seconds": 3.985}, "timestamp": "2026-01-17T15:12:19.454874"}
{"image_index": 236, "image_name": "000000024243.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000024243.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 4115.828, "latencies_ms": [4115.828], "images_per_second": 0.243, "prompt_tokens": 21, "response_tokens_est": 72, "n_tiles": 12, "output_text": "The image depicts a scene on a city street, likely in a European city given the architecture and the style of the buildings. A woman is sitting on a bench, leaning against a white refrigerator, with a smile on her face. She is holding a drink in her hand. The setting appears to be a casual, everyday moment in a public space.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24927.9, "ram_available_mb": 100844.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 24923.5, "ram_available_mb": 100848.7, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.19, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.73, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 33.04, "peak": 45.28, "min": 26.0}, "VIN": {"avg": 72.29, "peak": 94.47, "min": 62.42}}, "power_watts_avg": 33.04, "energy_joules_est": 136.01, "sample_count": 32, "duration_seconds": 4.117}, "timestamp": "2026-01-17T15:12:23.577634"}
{"image_index": 236, "image_name": "000000024243.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000024243.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3500.398, "latencies_ms": [3500.398], "images_per_second": 0.286, "prompt_tokens": 19, "response_tokens_est": 54, "n_tiles": 12, "output_text": "The image depicts a scene with a woman sitting on a bench next to a white refrigerator. The woman is wearing a brown jacket and jeans, and she is holding a beer in her hand. The lighting is dim, and the weather appears to be overcast.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24923.5, "ram_available_mb": 100848.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 24925.7, "ram_available_mb": 100846.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.21, "peak": 16.56, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.72, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 34.27, "peak": 45.28, "min": 26.0}, "VIN": {"avg": 76.22, "peak": 115.77, "min": 54.88}}, "power_watts_avg": 34.27, "energy_joules_est": 119.97, "sample_count": 27, "duration_seconds": 3.501}, "timestamp": "2026-01-17T15:12:27.084607"}
{"image_index": 237, "image_name": "000000024567.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000024567.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2799.598, "latencies_ms": [2799.598], "images_per_second": 0.357, "prompt_tokens": 9, "response_tokens_est": 35, "n_tiles": 12, "output_text": "The image shows a person holding a tray with several hot dogs, each with a red ketchup and a white bun, all placed on a piece of aluminum foil.", "error": null, "sys_before": {"cpu_percent": 36.5, "ram_used_mb": 24914.3, "ram_available_mb": 100857.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 24916.6, "ram_available_mb": 100855.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.19, "peak": 16.76, "min": 13.59}, "VDD_CPU_SOC_MSS": {"avg": 15.95, "peak": 17.71, "min": 14.18}, "VDD_GPU": {"avg": 35.81, "peak": 44.12, "min": 27.18}, "VIN": {"avg": 77.85, "peak": 107.08, "min": 66.78}}, "power_watts_avg": 35.81, "energy_joules_est": 100.26, "sample_count": 21, "duration_seconds": 2.8}, "timestamp": "2026-01-17T15:12:29.971680"}
{"image_index": 237, "image_name": "000000024567.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000024567.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 6036.514, "latencies_ms": [6036.514], "images_per_second": 0.166, "prompt_tokens": 23, "response_tokens_est": 129, "n_tiles": 12, "output_text": "hot dog: 8\nbuns: 8\nfork: 1\nfork: 1\nfork: 1\nfork: 1\nfork: 1\nfork: 1\nfork: 1\nfork: 1\nfork: 1\nfork: 1\nfork: 1\nfork: 1\nfork: 1\nfork: 1\nfork: 1\nfork: 1\nfork: 1\nfork: 1\nfork: 1\nfork: 1\nfork: 1\nfork: 1\nfork: 1\nfork", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24916.6, "ram_available_mb": 100855.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 24917.3, "ram_available_mb": 100854.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.15, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.6, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 30.8, "peak": 45.28, "min": 26.0}, "VIN": {"avg": 73.87, "peak": 127.72, "min": 57.58}}, "power_watts_avg": 30.8, "energy_joules_est": 185.94, "sample_count": 47, "duration_seconds": 6.037}, "timestamp": "2026-01-17T15:12:36.014428"}
{"image_index": 237, "image_name": "000000024567.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000024567.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 4083.464, "latencies_ms": [4083.464], "images_per_second": 0.245, "prompt_tokens": 27, "response_tokens_est": 71, "n_tiles": 12, "output_text": "The main objects in the image are a tray of hot dogs, which are placed on a blue-lined aluminum foil tray. The hot dogs are arranged in a row, with the closest ones to the foreground and the far ones in the background. The tray is situated on a wooden surface, and there is a white plastic chair visible in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24917.3, "ram_available_mb": 100854.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 24917.5, "ram_available_mb": 100854.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.18, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.7, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 32.95, "peak": 45.3, "min": 26.0}, "VIN": {"avg": 76.88, "peak": 120.41, "min": 64.29}}, "power_watts_avg": 32.95, "energy_joules_est": 134.57, "sample_count": 32, "duration_seconds": 4.084}, "timestamp": "2026-01-17T15:12:40.106319"}
{"image_index": 237, "image_name": "000000024567.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000024567.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3971.638, "latencies_ms": [3971.638], "images_per_second": 0.252, "prompt_tokens": 21, "response_tokens_est": 68, "n_tiles": 12, "output_text": "The image depicts a man holding a tray of hot dogs, which are placed on a piece of aluminum foil. The hot dogs are arranged in a neat row, with each bun holding a different type of hot dog. The man is wearing a green shirt and a hat, and the background shows a grassy area with a fence.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24917.5, "ram_available_mb": 100854.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 24917.8, "ram_available_mb": 100854.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.21, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.87, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 33.08, "peak": 44.49, "min": 26.0}, "VIN": {"avg": 74.13, "peak": 115.34, "min": 59.59}}, "power_watts_avg": 33.08, "energy_joules_est": 131.39, "sample_count": 31, "duration_seconds": 3.972}, "timestamp": "2026-01-17T15:12:44.084117"}
{"image_index": 237, "image_name": "000000024567.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000024567.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3584.269, "latencies_ms": [3584.269], "images_per_second": 0.279, "prompt_tokens": 19, "response_tokens_est": 57, "n_tiles": 12, "output_text": "The image features a tray of hot dogs, each coated in a glossy, red sauce. The hot dogs are placed on a light-colored, foil-lined tray, and the tray is placed on a wooden surface. The lighting is bright and natural, suggesting a sunny day.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24917.8, "ram_available_mb": 100854.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 24918.0, "ram_available_mb": 100854.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.26, "peak": 16.66, "min": 14.12}, "VDD_CPU_SOC_MSS": {"avg": 15.82, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 34.17, "peak": 45.68, "min": 26.0}, "VIN": {"avg": 73.28, "peak": 114.32, "min": 60.25}}, "power_watts_avg": 34.17, "energy_joules_est": 122.49, "sample_count": 28, "duration_seconds": 3.585}, "timestamp": "2026-01-17T15:12:47.675363"}
{"image_index": 238, "image_name": "000000024610.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000024610.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2896.419, "latencies_ms": [2896.419], "images_per_second": 0.345, "prompt_tokens": 9, "response_tokens_est": 38, "n_tiles": 12, "output_text": "The image depicts a cluttered living room with a bookshelf filled with various books, a computer on a table, a couch with a backpack, and a television on the floor.", "error": null, "sys_before": {"cpu_percent": 41.1, "ram_used_mb": 24933.7, "ram_available_mb": 100838.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 24933.2, "ram_available_mb": 100838.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.19, "peak": 16.66, "min": 13.51}, "VDD_CPU_SOC_MSS": {"avg": 15.94, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 35.05, "peak": 44.1, "min": 26.79}, "VIN": {"avg": 79.39, "peak": 126.55, "min": 66.32}}, "power_watts_avg": 35.05, "energy_joules_est": 101.54, "sample_count": 22, "duration_seconds": 2.897}, "timestamp": "2026-01-17T15:12:50.669703"}
{"image_index": 238, "image_name": "000000024610.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000024610.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2856.228, "latencies_ms": [2856.228], "images_per_second": 0.35, "prompt_tokens": 23, "response_tokens_est": 35, "n_tiles": 12, "output_text": "1. Laptop\n2. Chair\n3. Bookshelf\n4. Book\n5. Backpack\n6. TV stand\n7. TV\n8. Rug", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24933.2, "ram_available_mb": 100838.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 24934.5, "ram_available_mb": 100837.7, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.27, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.89, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 36.06, "peak": 45.3, "min": 26.79}, "VIN": {"avg": 80.41, "peak": 125.75, "min": 67.31}}, "power_watts_avg": 36.06, "energy_joules_est": 103.01, "sample_count": 22, "duration_seconds": 2.856}, "timestamp": "2026-01-17T15:12:53.533455"}
{"image_index": 238, "image_name": "000000024610.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000024610.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3979.357, "latencies_ms": [3979.357], "images_per_second": 0.251, "prompt_tokens": 27, "response_tokens_est": 68, "n_tiles": 12, "output_text": "The main objects in the image are a bookshelf filled with books, a computer on a desk, and a television on a couch. The bookshelf is located in the background, while the computer and television are in the foreground. The bookshelf is near the computer and television, indicating that they are part of the same room setup.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24934.5, "ram_available_mb": 100837.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 24934.6, "ram_available_mb": 100837.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.23, "peak": 16.66, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.92, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 33.46, "peak": 45.68, "min": 26.0}, "VIN": {"avg": 74.98, "peak": 125.85, "min": 55.19}}, "power_watts_avg": 33.46, "energy_joules_est": 133.16, "sample_count": 31, "duration_seconds": 3.98}, "timestamp": "2026-01-17T15:12:57.519631"}
{"image_index": 238, "image_name": "000000024610.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000024610.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3836.884, "latencies_ms": [3836.884], "images_per_second": 0.261, "prompt_tokens": 21, "response_tokens_est": 63, "n_tiles": 12, "output_text": "The image depicts a cozy, cluttered living room with a small table and chairs, a bookshelf filled with books, and a couch with various items on it. The room appears to be in a state of transition, possibly after moving in or out, as there are boxes and other items scattered around.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24934.6, "ram_available_mb": 100837.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 24935.8, "ram_available_mb": 100836.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.12, "peak": 16.66, "min": 14.12}, "VDD_CPU_SOC_MSS": {"avg": 15.69, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 33.28, "peak": 44.91, "min": 25.6}, "VIN": {"avg": 75.66, "peak": 128.38, "min": 60.39}}, "power_watts_avg": 33.28, "energy_joules_est": 127.7, "sample_count": 30, "duration_seconds": 3.837}, "timestamp": "2026-01-17T15:13:01.362744"}
{"image_index": 238, "image_name": "000000024610.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000024610.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3825.708, "latencies_ms": [3825.708], "images_per_second": 0.261, "prompt_tokens": 19, "response_tokens_est": 63, "n_tiles": 12, "output_text": "The image depicts a cozy, dimly lit room with a beige wall and a brown carpet. The lighting is soft and warm, creating a comfortable atmosphere. The room features a wooden desk with a laptop, a bookshelf filled with books, a chair, and a couch with various items on it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24935.8, "ram_available_mb": 100836.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 24936.0, "ram_available_mb": 100836.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.91, "peak": 16.36, "min": 13.92}, "VDD_CPU_SOC_MSS": {"avg": 15.5, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 33.54, "peak": 45.69, "min": 25.6}, "VIN": {"avg": 73.57, "peak": 119.19, "min": 64.11}}, "power_watts_avg": 33.54, "energy_joules_est": 128.33, "sample_count": 29, "duration_seconds": 3.826}, "timestamp": "2026-01-17T15:13:05.199027"}
{"image_index": 239, "image_name": "000000024919.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000024919.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1113.133, "latencies_ms": [1113.133], "images_per_second": 0.898, "prompt_tokens": 9, "response_tokens_est": 19, "n_tiles": 6, "output_text": "Two elephants are walking through a grassy savannah, surrounded by dense vegetation and trees.", "error": null, "sys_before": {"cpu_percent": 40.0, "ram_used_mb": 24954.2, "ram_available_mb": 100818.0, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 24954.4, "ram_available_mb": 100817.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.48, "peak": 15.04, "min": 13.59}, "VDD_CPU_SOC_MSS": {"avg": 15.15, "peak": 16.14, "min": 14.17}, "VDD_GPU": {"avg": 35.5, "peak": 40.56, "min": 29.54}, "VIN": {"avg": 81.23, "peak": 114.62, "min": 61.86}}, "power_watts_avg": 35.5, "energy_joules_est": 39.53, "sample_count": 8, "duration_seconds": 1.114}, "timestamp": "2026-01-17T15:13:06.373071"}
{"image_index": 239, "image_name": "000000024919.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000024919.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1160.487, "latencies_ms": [1160.487], "images_per_second": 0.862, "prompt_tokens": 23, "response_tokens_est": 19, "n_tiles": 6, "output_text": "elephant: 2\nbushes: 10\ntrees: 5", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24954.4, "ram_available_mb": 100817.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 24955.4, "ram_available_mb": 100816.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.9, "peak": 15.34, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.5, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 37.41, "peak": 41.34, "min": 30.73}, "VIN": {"avg": 81.69, "peak": 115.48, "min": 61.91}}, "power_watts_avg": 37.41, "energy_joules_est": 43.43, "sample_count": 8, "duration_seconds": 1.161}, "timestamp": "2026-01-17T15:13:07.543619"}
{"image_index": 239, "image_name": "000000024919.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000024919.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2079.435, "latencies_ms": [2079.435], "images_per_second": 0.481, "prompt_tokens": 27, "response_tokens_est": 51, "n_tiles": 6, "output_text": "The main objects in the image are two elephants. The elephant on the left is in the foreground, while the one on the right is slightly behind and to the right. The background features a hazy, misty landscape with trees and shrubs.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 24955.4, "ram_available_mb": 100816.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 24955.4, "ram_available_mb": 100816.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.88, "peak": 15.24, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.52, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 31.39, "peak": 41.74, "min": 23.64}, "VIN": {"avg": 68.66, "peak": 93.42, "min": 58.79}}, "power_watts_avg": 31.39, "energy_joules_est": 65.28, "sample_count": 16, "duration_seconds": 2.08}, "timestamp": "2026-01-17T15:13:09.629241"}
{"image_index": 239, "image_name": "000000024919.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000024919.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2389.068, "latencies_ms": [2389.068], "images_per_second": 0.419, "prompt_tokens": 21, "response_tokens_est": 62, "n_tiles": 6, "output_text": "The image depicts a serene African savanna landscape with two elephants walking through a dense thicket of greenery. The elephants appear to be in a natural, undisturbed environment, with a misty, hazy atmosphere that adds to the tranquil and peaceful ambiance of the scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24955.4, "ram_available_mb": 100816.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 24955.9, "ram_available_mb": 100816.3, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.87, "peak": 15.24, "min": 14.3}, "VDD_CPU_SOC_MSS": {"avg": 15.53, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 29.76, "peak": 40.56, "min": 23.24}, "VIN": {"avg": 72.58, "peak": 109.35, "min": 58.01}}, "power_watts_avg": 29.76, "energy_joules_est": 71.11, "sample_count": 18, "duration_seconds": 2.389}, "timestamp": "2026-01-17T15:13:12.024150"}
{"image_index": 239, "image_name": "000000024919.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000024919.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2473.577, "latencies_ms": [2473.577], "images_per_second": 0.404, "prompt_tokens": 19, "response_tokens_est": 65, "n_tiles": 6, "output_text": "The image depicts two elephants in a natural, grassy environment with a misty, overcast sky. The elephants are brown, with the larger one having prominent tusks and the smaller one having a smaller head. The lighting is soft and diffused, with the fog adding a layer of mystery to the scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24955.9, "ram_available_mb": 100816.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 24955.6, "ram_available_mb": 100816.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.85, "peak": 15.24, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.52, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 29.31, "peak": 40.95, "min": 22.86}, "VIN": {"avg": 69.99, "peak": 103.84, "min": 62.42}}, "power_watts_avg": 29.31, "energy_joules_est": 72.51, "sample_count": 19, "duration_seconds": 2.474}, "timestamp": "2026-01-17T15:13:14.503689"}
{"image_index": 240, "image_name": "000000025057.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025057.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1520.349, "latencies_ms": [1520.349], "images_per_second": 0.658, "prompt_tokens": 9, "response_tokens_est": 34, "n_tiles": 6, "output_text": "A shirtless man in sunglasses and a baseball cap is holding a frisbee in his right hand while standing on a grassy field with trees in the background.", "error": null, "sys_before": {"cpu_percent": 35.5, "ram_used_mb": 24955.6, "ram_available_mb": 100816.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 24955.6, "ram_available_mb": 100816.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.72, "peak": 15.14, "min": 13.9}, "VDD_CPU_SOC_MSS": {"avg": 15.46, "peak": 16.14, "min": 14.57}, "VDD_GPU": {"avg": 32.84, "peak": 40.16, "min": 26.0}, "VIN": {"avg": 77.05, "peak": 114.93, "min": 55.7}}, "power_watts_avg": 32.84, "energy_joules_est": 49.95, "sample_count": 11, "duration_seconds": 1.521}, "timestamp": "2026-01-17T15:13:16.079245"}
{"image_index": 240, "image_name": "000000025057.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025057.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1621.645, "latencies_ms": [1621.645], "images_per_second": 0.617, "prompt_tokens": 23, "response_tokens_est": 36, "n_tiles": 6, "output_text": "1. Frisbee\n2. Man\n3. Sunglasses\n4. Cap\n5. Shirt\n6. Shorts\n7. Belt\n8. Shoes", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24955.6, "ram_available_mb": 100816.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 24955.6, "ram_available_mb": 100816.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.98, "peak": 15.34, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.58, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 33.32, "peak": 41.36, "min": 25.6}, "VIN": {"avg": 73.4, "peak": 106.59, "min": 60.41}}, "power_watts_avg": 33.32, "energy_joules_est": 54.06, "sample_count": 12, "duration_seconds": 1.622}, "timestamp": "2026-01-17T15:13:17.707531"}
{"image_index": 240, "image_name": "000000025057.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025057.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2505.552, "latencies_ms": [2505.552], "images_per_second": 0.399, "prompt_tokens": 27, "response_tokens_est": 66, "n_tiles": 6, "output_text": "The main object in the foreground is a man standing on a grassy field. He is holding a white frisbee in his right hand and appears to be preparing to throw it. In the background, there is another person walking away from the camera. The trees and clear blue sky provide a natural backdrop to the scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24955.6, "ram_available_mb": 100816.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 24956.1, "ram_available_mb": 100816.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.89, "peak": 15.34, "min": 14.43}, "VDD_CPU_SOC_MSS": {"avg": 15.58, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 29.48, "peak": 40.95, "min": 22.86}, "VIN": {"avg": 69.83, "peak": 108.98, "min": 56.3}}, "power_watts_avg": 29.48, "energy_joules_est": 73.87, "sample_count": 19, "duration_seconds": 2.506}, "timestamp": "2026-01-17T15:13:20.218976"}
{"image_index": 240, "image_name": "000000025057.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025057.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2782.688, "latencies_ms": [2782.688], "images_per_second": 0.359, "prompt_tokens": 21, "response_tokens_est": 76, "n_tiles": 6, "output_text": "The image depicts a man standing on a grassy field, holding a white frisbee in his right hand. He is wearing a white baseball cap, sunglasses, and a necklace. In the background, there is a dense line of trees, and another person is visible walking away from the camera. The setting appears to be a park or recreational area during the daytime.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24956.1, "ram_available_mb": 100816.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 24956.6, "ram_available_mb": 100815.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.9, "peak": 15.24, "min": 14.3}, "VDD_CPU_SOC_MSS": {"avg": 15.54, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 28.68, "peak": 40.18, "min": 22.86}, "VIN": {"avg": 71.23, "peak": 106.67, "min": 59.65}}, "power_watts_avg": 28.68, "energy_joules_est": 79.82, "sample_count": 21, "duration_seconds": 2.783}, "timestamp": "2026-01-17T15:13:23.011678"}
{"image_index": 240, "image_name": "000000025057.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025057.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1911.297, "latencies_ms": [1911.297], "images_per_second": 0.523, "prompt_tokens": 19, "response_tokens_est": 45, "n_tiles": 6, "output_text": "The image depicts a man in a green shirt and khaki shorts, barefoot, standing in a grassy field under a clear blue sky. The lighting is bright and natural, suggesting it is a sunny day.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24956.6, "ram_available_mb": 100815.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 24956.6, "ram_available_mb": 100815.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.83, "peak": 15.24, "min": 14.2}, "VDD_CPU_SOC_MSS": {"avg": 15.27, "peak": 15.74, "min": 14.56}, "VDD_GPU": {"avg": 31.26, "peak": 40.18, "min": 24.03}, "VIN": {"avg": 72.32, "peak": 113.35, "min": 62.05}}, "power_watts_avg": 31.26, "energy_joules_est": 59.76, "sample_count": 14, "duration_seconds": 1.912}, "timestamp": "2026-01-17T15:13:24.929010"}
{"image_index": 241, "image_name": "000000025096.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025096.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2462.547, "latencies_ms": [2462.547], "images_per_second": 0.406, "prompt_tokens": 9, "response_tokens_est": 25, "n_tiles": 12, "output_text": "A young boy is sitting on a table, looking at a cake decorated with a toy airplane and a toy airplane figure.", "error": null, "sys_before": {"cpu_percent": 43.1, "ram_used_mb": 24918.4, "ram_available_mb": 100853.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.5, "ram_used_mb": 24921.4, "ram_available_mb": 100850.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.21, "peak": 16.66, "min": 13.59}, "VDD_CPU_SOC_MSS": {"avg": 15.94, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 36.65, "peak": 44.1, "min": 28.36}, "VIN": {"avg": 84.42, "peak": 117.85, "min": 64.36}}, "power_watts_avg": 36.65, "energy_joules_est": 90.26, "sample_count": 18, "duration_seconds": 2.463}, "timestamp": "2026-01-17T15:13:27.473486"}
{"image_index": 241, "image_name": "000000025096.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025096.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 6031.531, "latencies_ms": [6031.531], "images_per_second": 0.166, "prompt_tokens": 23, "response_tokens_est": 128, "n_tiles": 12, "output_text": "object: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject:", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24921.4, "ram_available_mb": 100850.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 24921.6, "ram_available_mb": 100850.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.14, "peak": 16.66, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.56, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 30.82, "peak": 45.68, "min": 26.0}, "VIN": {"avg": 73.83, "peak": 124.35, "min": 56.42}}, "power_watts_avg": 30.82, "energy_joules_est": 185.9, "sample_count": 47, "duration_seconds": 6.032}, "timestamp": "2026-01-17T15:13:33.515363"}
{"image_index": 241, "image_name": "000000025096.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025096.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 4019.301, "latencies_ms": [4019.301], "images_per_second": 0.249, "prompt_tokens": 27, "response_tokens_est": 69, "n_tiles": 12, "output_text": "The main object in the foreground is a cake with a chocolate icing design, which is placed on a table covered with a colorful tablecloth. In the background, there is a boy wearing a blue jersey, and a white plate is visible on the table. The boy is leaning over the table, possibly preparing to cut the cake.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24921.6, "ram_available_mb": 100850.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 24922.6, "ram_available_mb": 100849.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.17, "peak": 16.66, "min": 14.02}, "VDD_CPU_SOC_MSS": {"avg": 15.67, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 33.24, "peak": 45.28, "min": 26.0}, "VIN": {"avg": 72.76, "peak": 115.67, "min": 55.36}}, "power_watts_avg": 33.24, "energy_joules_est": 133.61, "sample_count": 31, "duration_seconds": 4.02}, "timestamp": "2026-01-17T15:13:37.543073"}
{"image_index": 241, "image_name": "000000025096.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025096.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3670.209, "latencies_ms": [3670.209], "images_per_second": 0.272, "prompt_tokens": 21, "response_tokens_est": 59, "n_tiles": 12, "output_text": "The image depicts a young boy sitting at a table, focused on a cake. The cake is decorated with various elements, including a toy airplane, a toy dinosaur, and chocolate drizzle. The boy is wearing a blue shirt and appears to be enjoying a moment of cake decorating.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24922.6, "ram_available_mb": 100849.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 24922.6, "ram_available_mb": 100849.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.2, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.69, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 33.96, "peak": 45.3, "min": 26.01}, "VIN": {"avg": 76.39, "peak": 119.25, "min": 55.08}}, "power_watts_avg": 33.96, "energy_joules_est": 124.66, "sample_count": 28, "duration_seconds": 3.671}, "timestamp": "2026-01-17T15:13:41.219574"}
{"image_index": 241, "image_name": "000000025096.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025096.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2856.96, "latencies_ms": [2856.96], "images_per_second": 0.35, "prompt_tokens": 19, "response_tokens_est": 35, "n_tiles": 12, "output_text": "The cake is richly decorated with chocolate and red icing, and it is placed on a colorful tablecloth. The lighting is warm, creating a cozy atmosphere.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24922.6, "ram_available_mb": 100849.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 24922.8, "ram_available_mb": 100849.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.24, "peak": 16.56, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.85, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 36.15, "peak": 45.68, "min": 26.79}, "VIN": {"avg": 75.94, "peak": 103.76, "min": 66.65}}, "power_watts_avg": 36.15, "energy_joules_est": 103.3, "sample_count": 22, "duration_seconds": 2.857}, "timestamp": "2026-01-17T15:13:44.083135"}
{"image_index": 242, "image_name": "000000025139.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025139.jpg", "image_width": 500, "image_height": 334, "image_resolution": "500x334", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1251.394, "latencies_ms": [1251.394], "images_per_second": 0.799, "prompt_tokens": 9, "response_tokens_est": 24, "n_tiles": 6, "output_text": "The image shows a close-up of a zebra's face, with its distinctive black and white stripes clearly visible.", "error": null, "sys_before": {"cpu_percent": 45.9, "ram_used_mb": 24940.0, "ram_available_mb": 100832.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 24941.7, "ram_available_mb": 100830.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.62, "peak": 15.14, "min": 13.92}, "VDD_CPU_SOC_MSS": {"avg": 15.35, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 35.14, "peak": 40.95, "min": 28.36}, "VIN": {"avg": 75.95, "peak": 100.49, "min": 57.34}}, "power_watts_avg": 35.14, "energy_joules_est": 44.0, "sample_count": 9, "duration_seconds": 1.252}, "timestamp": "2026-01-17T15:13:45.392253"}
{"image_index": 242, "image_name": "000000025139.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025139.jpg", "image_width": 500, "image_height": 334, "image_resolution": "500x334", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1107.252, "latencies_ms": [1107.252], "images_per_second": 0.903, "prompt_tokens": 23, "response_tokens_est": 17, "n_tiles": 6, "output_text": "zebra: 1\nfence: 1\nground: 1", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24941.7, "ram_available_mb": 100830.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 24942.2, "ram_available_mb": 100830.0, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.92, "peak": 15.44, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.5, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 36.93, "peak": 41.34, "min": 30.33}, "VIN": {"avg": 80.39, "peak": 118.41, "min": 59.11}}, "power_watts_avg": 36.93, "energy_joules_est": 40.91, "sample_count": 8, "duration_seconds": 1.108}, "timestamp": "2026-01-17T15:13:46.505769"}
{"image_index": 242, "image_name": "000000025139.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025139.jpg", "image_width": 500, "image_height": 334, "image_resolution": "500x334", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2984.897, "latencies_ms": [2984.897], "images_per_second": 0.335, "prompt_tokens": 27, "response_tokens_est": 83, "n_tiles": 6, "output_text": "The main object in the foreground is a zebra, which is positioned near the right side of the image. The zebra's stripes are clearly visible, and it appears to be standing close to a metal fence. In the background, there is another zebra, partially visible, and it seems to be further away from the foreground zebra. The zebra in the background is also near a metal fence.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24942.2, "ram_available_mb": 100830.0, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 24943.4, "ram_available_mb": 100828.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.93, "peak": 15.34, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.61, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 28.82, "peak": 42.13, "min": 22.85}, "VIN": {"avg": 69.83, "peak": 114.95, "min": 48.33}}, "power_watts_avg": 28.82, "energy_joules_est": 86.03, "sample_count": 23, "duration_seconds": 2.985}, "timestamp": "2026-01-17T15:13:49.496402"}
{"image_index": 242, "image_name": "000000025139.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025139.jpg", "image_width": 500, "image_height": 334, "image_resolution": "500x334", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2193.983, "latencies_ms": [2193.983], "images_per_second": 0.456, "prompt_tokens": 21, "response_tokens_est": 55, "n_tiles": 6, "output_text": "The image depicts a zebra standing in a pen, with its head turned slightly to the side. The zebra appears to be in a zoo or a similar controlled environment, as indicated by the presence of a metal fence and the ground covered with dirt and rocks.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24943.4, "ram_available_mb": 100828.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 24943.4, "ram_available_mb": 100828.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.85, "peak": 15.24, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.52, "peak": 15.75, "min": 14.56}, "VDD_GPU": {"avg": 29.89, "peak": 40.18, "min": 23.25}, "VIN": {"avg": 69.34, "peak": 91.02, "min": 58.05}}, "power_watts_avg": 29.89, "energy_joules_est": 65.59, "sample_count": 17, "duration_seconds": 2.194}, "timestamp": "2026-01-17T15:13:51.697440"}
{"image_index": 242, "image_name": "000000025139.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025139.jpg", "image_width": 500, "image_height": 334, "image_resolution": "500x334", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2022.702, "latencies_ms": [2022.702], "images_per_second": 0.494, "prompt_tokens": 19, "response_tokens_est": 49, "n_tiles": 6, "output_text": "The zebra in the image has a striking black and white striped pattern, which is highly visible against the natural backdrop. The lighting is bright and natural, casting shadows on the ground, indicating that the photo was taken during the day.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24943.4, "ram_available_mb": 100828.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 24943.4, "ram_available_mb": 100828.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.85, "peak": 15.24, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.51, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 30.72, "peak": 40.16, "min": 23.64}, "VIN": {"avg": 71.43, "peak": 117.52, "min": 58.72}}, "power_watts_avg": 30.72, "energy_joules_est": 62.15, "sample_count": 15, "duration_seconds": 2.023}, "timestamp": "2026-01-17T15:13:53.726303"}
{"image_index": 243, "image_name": "000000025181.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025181.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1379.478, "latencies_ms": [1379.478], "images_per_second": 0.725, "prompt_tokens": 9, "response_tokens_est": 29, "n_tiles": 6, "output_text": "The image depicts a black and white photograph of a train station platform with a train approaching, and a sign indicating the station's name.", "error": null, "sys_before": {"cpu_percent": 46.5, "ram_used_mb": 24943.4, "ram_available_mb": 100828.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 24944.6, "ram_available_mb": 100827.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.68, "peak": 15.14, "min": 13.79}, "VDD_CPU_SOC_MSS": {"avg": 15.43, "peak": 16.14, "min": 14.18}, "VDD_GPU": {"avg": 33.76, "peak": 40.57, "min": 26.79}, "VIN": {"avg": 76.11, "peak": 122.42, "min": 54.03}}, "power_watts_avg": 33.76, "energy_joules_est": 46.58, "sample_count": 10, "duration_seconds": 1.38}, "timestamp": "2026-01-17T15:13:55.162613"}
{"image_index": 243, "image_name": "000000025181.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025181.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1720.747, "latencies_ms": [1720.747], "images_per_second": 0.581, "prompt_tokens": 23, "response_tokens_est": 40, "n_tiles": 6, "output_text": "object: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24944.6, "ram_available_mb": 100827.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 24945.3, "ram_available_mb": 100826.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.03, "peak": 15.44, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.63, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 32.79, "peak": 40.97, "min": 24.83}, "VIN": {"avg": 71.69, "peak": 110.81, "min": 52.62}}, "power_watts_avg": 32.79, "energy_joules_est": 56.44, "sample_count": 13, "duration_seconds": 1.721}, "timestamp": "2026-01-17T15:13:56.889540"}
{"image_index": 243, "image_name": "000000025181.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025181.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2295.812, "latencies_ms": [2295.812], "images_per_second": 0.436, "prompt_tokens": 27, "response_tokens_est": 59, "n_tiles": 6, "output_text": "The main objects in the image are the train and the platform. The train is positioned on the right side of the image, while the platform is on the left side. The train is closer to the foreground, and the platform is further back, creating a clear spatial relationship between the two.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24945.3, "ram_available_mb": 100826.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 24945.6, "ram_available_mb": 100826.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.94, "peak": 15.44, "min": 14.43}, "VDD_CPU_SOC_MSS": {"avg": 15.61, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 30.19, "peak": 40.97, "min": 23.24}, "VIN": {"avg": 70.29, "peak": 105.78, "min": 61.98}}, "power_watts_avg": 30.19, "energy_joules_est": 69.32, "sample_count": 17, "duration_seconds": 2.296}, "timestamp": "2026-01-17T15:13:59.192217"}
{"image_index": 243, "image_name": "000000025181.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025181.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2127.306, "latencies_ms": [2127.306], "images_per_second": 0.47, "prompt_tokens": 21, "response_tokens_est": 53, "n_tiles": 6, "output_text": "The image depicts a black and white scene at a train station platform. A train is visible on the tracks, and there are people present on the platform. The station appears to be in a rural or semi-rural area, with a clear sky overhead.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 24945.6, "ram_available_mb": 100826.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 24946.1, "ram_available_mb": 100826.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.88, "peak": 15.24, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.48, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 30.45, "peak": 40.16, "min": 23.64}, "VIN": {"avg": 68.99, "peak": 94.78, "min": 54.74}}, "power_watts_avg": 30.45, "energy_joules_est": 64.79, "sample_count": 16, "duration_seconds": 2.128}, "timestamp": "2026-01-17T15:14:01.325799"}
{"image_index": 243, "image_name": "000000025181.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025181.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1928.075, "latencies_ms": [1928.075], "images_per_second": 0.519, "prompt_tokens": 19, "response_tokens_est": 46, "n_tiles": 6, "output_text": "The image is a black and white photograph of a train station platform. The platform is made of brick, and there is a concrete pillar in the center. The station has overhead signage, and the sky is overcast.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24946.1, "ram_available_mb": 100826.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 24946.3, "ram_available_mb": 100825.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.89, "peak": 15.34, "min": 14.3}, "VDD_CPU_SOC_MSS": {"avg": 15.44, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 31.48, "peak": 40.56, "min": 24.04}, "VIN": {"avg": 69.85, "peak": 91.58, "min": 59.96}}, "power_watts_avg": 31.48, "energy_joules_est": 60.71, "sample_count": 14, "duration_seconds": 1.928}, "timestamp": "2026-01-17T15:14:03.259903"}
{"image_index": 244, "image_name": "000000025228.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025228.jpg", "image_width": 640, "image_height": 436, "image_resolution": "640x436", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1164.908, "latencies_ms": [1164.908], "images_per_second": 0.858, "prompt_tokens": 9, "response_tokens_est": 21, "n_tiles": 6, "output_text": "A person is sitting on a surfboard in the water, with the sun setting in the background.", "error": null, "sys_before": {"cpu_percent": 32.7, "ram_used_mb": 24946.1, "ram_available_mb": 100826.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 24946.1, "ram_available_mb": 100826.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.65, "peak": 15.24, "min": 13.9}, "VDD_CPU_SOC_MSS": {"avg": 15.35, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 35.05, "peak": 40.56, "min": 29.15}, "VIN": {"avg": 81.71, "peak": 109.22, "min": 63.86}}, "power_watts_avg": 35.05, "energy_joules_est": 40.84, "sample_count": 8, "duration_seconds": 1.165}, "timestamp": "2026-01-17T15:14:04.473357"}
{"image_index": 244, "image_name": "000000025228.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025228.jpg", "image_width": 640, "image_height": 436, "image_resolution": "640x436", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1398.523, "latencies_ms": [1398.523], "images_per_second": 0.715, "prompt_tokens": 23, "response_tokens_est": 28, "n_tiles": 6, "output_text": "surfboard: 1\nwoman: 1\nwater: 1\nocean: 1\nsun: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24946.1, "ram_available_mb": 100826.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 24946.1, "ram_available_mb": 100826.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.98, "peak": 15.44, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.55, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 35.25, "peak": 41.34, "min": 27.18}, "VIN": {"avg": 73.8, "peak": 103.24, "min": 63.76}}, "power_watts_avg": 35.25, "energy_joules_est": 49.31, "sample_count": 10, "duration_seconds": 1.399}, "timestamp": "2026-01-17T15:14:05.878002"}
{"image_index": 244, "image_name": "000000025228.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025228.jpg", "image_width": 640, "image_height": 436, "image_resolution": "640x436", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3022.66, "latencies_ms": [3022.66], "images_per_second": 0.331, "prompt_tokens": 27, "response_tokens_est": 85, "n_tiles": 6, "output_text": "The main object in the foreground is a person sitting on a red surfboard. The person is positioned near the center of the image, with their back to the camera. The background features a vast body of water, likely the ocean, with a cloudy sky above. The surfboard is positioned in the foreground, closer to the camera, while the person is further away, creating a sense of depth in the image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24946.1, "ram_available_mb": 100826.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 24946.3, "ram_available_mb": 100825.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.96, "peak": 15.24, "min": 14.43}, "VDD_CPU_SOC_MSS": {"avg": 15.63, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 28.48, "peak": 41.74, "min": 22.86}, "VIN": {"avg": 66.65, "peak": 86.19, "min": 55.0}}, "power_watts_avg": 28.48, "energy_joules_est": 86.1, "sample_count": 24, "duration_seconds": 3.023}, "timestamp": "2026-01-17T15:14:08.906990"}
{"image_index": 244, "image_name": "000000025228.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025228.jpg", "image_width": 640, "image_height": 436, "image_resolution": "640x436", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2656.488, "latencies_ms": [2656.488], "images_per_second": 0.376, "prompt_tokens": 21, "response_tokens_est": 72, "n_tiles": 6, "output_text": "The image captures a serene scene of a person sitting on a surfboard, likely preparing to ride the waves. The setting is a calm ocean at sunset, with the sky painted in shades of orange and purple, reflecting off the water's surface. The person is dressed in a black wetsuit, suggesting they are engaged in surfing or water sports.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24946.3, "ram_available_mb": 100825.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 24946.3, "ram_available_mb": 100825.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.95, "peak": 15.34, "min": 14.3}, "VDD_CPU_SOC_MSS": {"avg": 15.61, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 28.82, "peak": 40.16, "min": 23.24}, "VIN": {"avg": 71.35, "peak": 116.62, "min": 59.54}}, "power_watts_avg": 28.82, "energy_joules_est": 76.57, "sample_count": 20, "duration_seconds": 2.657}, "timestamp": "2026-01-17T15:14:11.570187"}
{"image_index": 244, "image_name": "000000025228.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025228.jpg", "image_width": 640, "image_height": 436, "image_resolution": "640x436", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1842.469, "latencies_ms": [1842.469], "images_per_second": 0.543, "prompt_tokens": 19, "response_tokens_est": 43, "n_tiles": 6, "output_text": "The image captures a person in a black wetsuit sitting on a red surfboard, with the sun setting in the background. The sky is filled with dramatic clouds, casting a warm glow over the scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24946.3, "ram_available_mb": 100825.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 24946.5, "ram_available_mb": 100825.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.88, "peak": 15.34, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.55, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 31.37, "peak": 40.18, "min": 24.03}, "VIN": {"avg": 72.78, "peak": 109.98, "min": 62.28}}, "power_watts_avg": 31.37, "energy_joules_est": 57.81, "sample_count": 14, "duration_seconds": 1.843}, "timestamp": "2026-01-17T15:14:13.419139"}
{"image_index": 245, "image_name": "000000025386.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025386.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1414.538, "latencies_ms": [1414.538], "images_per_second": 0.707, "prompt_tokens": 9, "response_tokens_est": 30, "n_tiles": 6, "output_text": "A man and a woman are seated in a train carriage, with the man holding chopsticks and the woman holding a small bag of snacks.", "error": null, "sys_before": {"cpu_percent": 38.7, "ram_used_mb": 24946.5, "ram_available_mb": 100825.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 24947.3, "ram_available_mb": 100824.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.68, "peak": 15.14, "min": 13.9}, "VDD_CPU_SOC_MSS": {"avg": 15.39, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 33.6, "peak": 40.18, "min": 26.79}, "VIN": {"avg": 73.16, "peak": 101.54, "min": 56.77}}, "power_watts_avg": 33.6, "energy_joules_est": 47.55, "sample_count": 10, "duration_seconds": 1.415}, "timestamp": "2026-01-17T15:14:14.881673"}
{"image_index": 245, "image_name": "000000025386.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025386.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2360.366, "latencies_ms": [2360.366], "images_per_second": 0.424, "prompt_tokens": 23, "response_tokens_est": 61, "n_tiles": 6, "output_text": "1. Man: 1\n2. Woman: 1\n3. Chopsticks: 2\n4. Plate: 1\n5. Sushi: 1\n6. Bag: 1\n7. Tray: 1\n8. Table: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24947.3, "ram_available_mb": 100824.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 24947.5, "ram_available_mb": 100824.7, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.89, "peak": 15.34, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.42, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 30.07, "peak": 40.95, "min": 23.24}, "VIN": {"avg": 70.03, "peak": 106.36, "min": 60.5}}, "power_watts_avg": 30.07, "energy_joules_est": 71.0, "sample_count": 18, "duration_seconds": 2.361}, "timestamp": "2026-01-17T15:14:17.248465"}
{"image_index": 245, "image_name": "000000025386.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025386.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3677.158, "latencies_ms": [3677.158], "images_per_second": 0.272, "prompt_tokens": 27, "response_tokens_est": 107, "n_tiles": 6, "output_text": "The main objects in the image are a man and a woman seated next to each other. The man is on the left side, while the woman is on the right side. The man is holding chopsticks and appears to be eating something from a small plate. The woman is holding a small bag and is also eating. The background features a window with curtains, a yellow sign, and a green bag on the table. The table is situated in the middle of the image, with the man and woman sitting close to it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24947.5, "ram_available_mb": 100824.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 24948.5, "ram_available_mb": 100823.7, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.88, "peak": 15.24, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.42, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 27.01, "peak": 40.18, "min": 22.85}, "VIN": {"avg": 67.6, "peak": 92.68, "min": 58.64}}, "power_watts_avg": 27.01, "energy_joules_est": 99.33, "sample_count": 29, "duration_seconds": 3.678}, "timestamp": "2026-01-17T15:14:20.932308"}
{"image_index": 245, "image_name": "000000025386.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025386.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2730.143, "latencies_ms": [2730.143], "images_per_second": 0.366, "prompt_tokens": 21, "response_tokens_est": 74, "n_tiles": 6, "output_text": "The image depicts a man and a woman seated in a train carriage, sharing a meal. The man is holding chopsticks and appears to be eating a sushi roll, while the woman is holding a small bag and seems to be eating a different type of food. The setting is inside a train, with a window and a seat visible in the background.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24948.5, "ram_available_mb": 100823.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 24948.5, "ram_available_mb": 100823.7, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.88, "peak": 15.24, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.47, "peak": 15.75, "min": 14.56}, "VDD_GPU": {"avg": 28.51, "peak": 40.16, "min": 22.85}, "VIN": {"avg": 71.06, "peak": 114.73, "min": 58.75}}, "power_watts_avg": 28.51, "energy_joules_est": 77.85, "sample_count": 21, "duration_seconds": 2.731}, "timestamp": "2026-01-17T15:14:23.668688"}
{"image_index": 245, "image_name": "000000025386.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025386.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2329.606, "latencies_ms": [2329.606], "images_per_second": 0.429, "prompt_tokens": 19, "response_tokens_est": 60, "n_tiles": 6, "output_text": "The image depicts a man and a woman seated in a train carriage, with the man wearing a plaid shirt and the woman in a red and white checkered top. The lighting is bright, and the carriage appears to be well-lit, with a yellow sign visible in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24948.5, "ram_available_mb": 100823.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 24948.5, "ram_available_mb": 100823.7, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.87, "peak": 15.24, "min": 14.3}, "VDD_CPU_SOC_MSS": {"avg": 15.48, "peak": 15.75, "min": 14.56}, "VDD_GPU": {"avg": 29.61, "peak": 40.16, "min": 23.25}, "VIN": {"avg": 73.58, "peak": 120.36, "min": 60.05}}, "power_watts_avg": 29.61, "energy_joules_est": 68.99, "sample_count": 18, "duration_seconds": 2.33}, "timestamp": "2026-01-17T15:14:26.004803"}
{"image_index": 246, "image_name": "000000025393.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025393.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2155.949, "latencies_ms": [2155.949], "images_per_second": 0.464, "prompt_tokens": 9, "response_tokens_est": 16, "n_tiles": 12, "output_text": "A man in a suit and tie is walking down a street at night.", "error": null, "sys_before": {"cpu_percent": 42.9, "ram_used_mb": 24928.2, "ram_available_mb": 100844.0, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.2, "ram_used_mb": 24919.8, "ram_available_mb": 100852.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.2, "peak": 16.66, "min": 13.39}, "VDD_CPU_SOC_MSS": {"avg": 15.99, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 37.59, "peak": 43.73, "min": 29.15}, "VIN": {"avg": 88.53, "peak": 134.31, "min": 67.36}}, "power_watts_avg": 37.59, "energy_joules_est": 81.05, "sample_count": 16, "duration_seconds": 2.156}, "timestamp": "2026-01-17T15:14:28.257154"}
{"image_index": 246, "image_name": "000000025393.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025393.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2822.623, "latencies_ms": [2822.623], "images_per_second": 0.354, "prompt_tokens": 23, "response_tokens_est": 34, "n_tiles": 12, "output_text": "1. Man\n2. Shirt\n3. Tie\n4. Suits\n5. Shoes\n6. Street light\n7. Building\n8. Car", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24919.8, "ram_available_mb": 100852.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 24921.2, "ram_available_mb": 100851.0, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.31, "peak": 16.66, "min": 14.43}, "VDD_CPU_SOC_MSS": {"avg": 15.97, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 36.85, "peak": 45.68, "min": 27.18}, "VIN": {"avg": 80.02, "peak": 119.73, "min": 53.06}}, "power_watts_avg": 36.85, "energy_joules_est": 104.03, "sample_count": 21, "duration_seconds": 2.823}, "timestamp": "2026-01-17T15:14:31.087221"}
{"image_index": 246, "image_name": "000000025393.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025393.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 4194.31, "latencies_ms": [4194.31], "images_per_second": 0.238, "prompt_tokens": 27, "response_tokens_est": 74, "n_tiles": 12, "output_text": "The main object in the foreground is a man dressed in a dark suit and tie, walking on a sidewalk. The man is positioned near a street lamp, which is on the left side of the image. The background features a building with a sign that reads \"Hierro Albero,\" and there are other indistinct objects and structures in the distance.", "error": null, "sys_before": {"cpu_percent": 16.7, "ram_used_mb": 24921.2, "ram_available_mb": 100851.0, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 24922.2, "ram_available_mb": 100850.0, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.17, "peak": 16.66, "min": 14.12}, "VDD_CPU_SOC_MSS": {"avg": 15.66, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 32.88, "peak": 45.28, "min": 26.0}, "VIN": {"avg": 76.87, "peak": 117.25, "min": 57.52}}, "power_watts_avg": 32.88, "energy_joules_est": 137.94, "sample_count": 32, "duration_seconds": 4.195}, "timestamp": "2026-01-17T15:14:35.288754"}
{"image_index": 246, "image_name": "000000025393.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025393.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3367.465, "latencies_ms": [3367.465], "images_per_second": 0.297, "prompt_tokens": 21, "response_tokens_est": 50, "n_tiles": 12, "output_text": "The image depicts a nighttime scene on a city street, with a man dressed in a formal black suit and tie walking confidently. The setting appears to be a pedestrian crossing, as indicated by the zebra crossing pattern on the ground.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24922.2, "ram_available_mb": 100850.0, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 24922.7, "ram_available_mb": 100849.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.2, "peak": 16.56, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.84, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 34.63, "peak": 45.68, "min": 26.0}, "VIN": {"avg": 74.48, "peak": 102.46, "min": 65.79}}, "power_watts_avg": 34.63, "energy_joules_est": 116.63, "sample_count": 26, "duration_seconds": 3.368}, "timestamp": "2026-01-17T15:14:38.662691"}
{"image_index": 246, "image_name": "000000025393.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025393.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3805.437, "latencies_ms": [3805.437], "images_per_second": 0.263, "prompt_tokens": 19, "response_tokens_est": 63, "n_tiles": 12, "output_text": "The image depicts a man dressed in a formal black suit and tie walking on a city street at night. The lighting is dim, with streetlights illuminating the scene, casting a warm glow on the man's face. The background features buildings with lit signs, a crosswalk, and a metal pole.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24922.7, "ram_available_mb": 100849.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 24922.7, "ram_available_mb": 100849.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.18, "peak": 16.66, "min": 14.12}, "VDD_CPU_SOC_MSS": {"avg": 15.76, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 33.39, "peak": 45.28, "min": 26.0}, "VIN": {"avg": 73.97, "peak": 108.67, "min": 63.26}}, "power_watts_avg": 33.39, "energy_joules_est": 127.09, "sample_count": 30, "duration_seconds": 3.806}, "timestamp": "2026-01-17T15:14:42.475562"}
{"image_index": 247, "image_name": "000000025394.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025394.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2254.977, "latencies_ms": [2254.977], "images_per_second": 0.443, "prompt_tokens": 9, "response_tokens_est": 19, "n_tiles": 12, "output_text": "A man is holding a bottle of wine and a glass of wine in a bar setting.", "error": null, "sys_before": {"cpu_percent": 41.4, "ram_used_mb": 24925.5, "ram_available_mb": 100846.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.3, "ram_used_mb": 24924.2, "ram_available_mb": 100847.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.2, "peak": 16.66, "min": 13.39}, "VDD_CPU_SOC_MSS": {"avg": 15.95, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 37.39, "peak": 44.1, "min": 29.56}, "VIN": {"avg": 85.1, "peak": 128.1, "min": 54.28}}, "power_watts_avg": 37.39, "energy_joules_est": 84.33, "sample_count": 17, "duration_seconds": 2.255}, "timestamp": "2026-01-17T15:14:44.838062"}
{"image_index": 247, "image_name": "000000025394.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025394.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 3301.274, "latencies_ms": [3301.274], "images_per_second": 0.303, "prompt_tokens": 23, "response_tokens_est": 48, "n_tiles": 12, "output_text": "1. Glass of wine\n2. Glass of wine\n3. Glass of wine\n4. Glass of wine\n5. Glass of wine\n6. Glass of wine\n7. Glass of wine\n8. Glass of wine", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24924.2, "ram_available_mb": 100847.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 24919.6, "ram_available_mb": 100852.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.22, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.84, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 34.88, "peak": 46.09, "min": 26.39}, "VIN": {"avg": 76.51, "peak": 117.98, "min": 58.22}}, "power_watts_avg": 34.88, "energy_joules_est": 115.18, "sample_count": 26, "duration_seconds": 3.302}, "timestamp": "2026-01-17T15:14:48.146021"}
{"image_index": 247, "image_name": "000000025394.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025394.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3846.04, "latencies_ms": [3846.04], "images_per_second": 0.26, "prompt_tokens": 27, "response_tokens_est": 64, "n_tiles": 12, "output_text": "The main object in the foreground is a glass of red wine, held by a person's hand. The glass is placed on a wooden table. In the background, there is a man wearing glasses and a gray sweater, standing near a wooden cabinet filled with wine bottles. The cabinet is situated on a wooden counter.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24920.1, "ram_available_mb": 100852.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 24921.3, "ram_available_mb": 100850.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.2, "peak": 16.56, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.84, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 33.58, "peak": 45.68, "min": 26.0}, "VIN": {"avg": 74.14, "peak": 128.46, "min": 64.31}}, "power_watts_avg": 33.58, "energy_joules_est": 129.16, "sample_count": 30, "duration_seconds": 3.846}, "timestamp": "2026-01-17T15:14:51.997881"}
{"image_index": 247, "image_name": "000000025394.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025394.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3909.61, "latencies_ms": [3909.61], "images_per_second": 0.256, "prompt_tokens": 21, "response_tokens_est": 66, "n_tiles": 12, "output_text": "The image depicts a man in a cozy, dimly lit bar setting, holding a bottle of wine and a glass. He appears to be engaged in a conversation or perhaps tasting the wine. The background features a wooden bar counter, various bottles, and a shelf with more wine bottles, creating a warm and intimate atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24921.3, "ram_available_mb": 100850.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 24921.6, "ram_available_mb": 100850.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.19, "peak": 16.56, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.72, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 33.29, "peak": 45.68, "min": 26.0}, "VIN": {"avg": 72.86, "peak": 101.97, "min": 61.2}}, "power_watts_avg": 33.29, "energy_joules_est": 130.16, "sample_count": 31, "duration_seconds": 3.91}, "timestamp": "2026-01-17T15:14:55.913557"}
{"image_index": 247, "image_name": "000000025394.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025394.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3256.79, "latencies_ms": [3256.79], "images_per_second": 0.307, "prompt_tokens": 19, "response_tokens_est": 47, "n_tiles": 12, "output_text": "The image depicts a man in a cozy, dimly lit bar setting. He is holding a wine glass and a bottle, with a bottle opener nearby. The lighting is warm and subdued, creating a relaxed atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24921.6, "ram_available_mb": 100850.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 24921.8, "ram_available_mb": 100850.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.23, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.76, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 34.76, "peak": 45.68, "min": 26.0}, "VIN": {"avg": 75.53, "peak": 111.13, "min": 50.68}}, "power_watts_avg": 34.76, "energy_joules_est": 113.22, "sample_count": 26, "duration_seconds": 3.257}, "timestamp": "2026-01-17T15:14:59.181020"}
{"image_index": 248, "image_name": "000000025424.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025424.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1472.333, "latencies_ms": [1472.333], "images_per_second": 0.679, "prompt_tokens": 9, "response_tokens_est": 31, "n_tiles": 6, "output_text": "A tennis player is captured mid-action, with his racket in motion, preparing to hit a tennis ball on a well-maintained grass court.", "error": null, "sys_before": {"cpu_percent": 37.5, "ram_used_mb": 24941.4, "ram_available_mb": 100830.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 24943.4, "ram_available_mb": 100828.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.61, "peak": 15.04, "min": 13.72}, "VDD_CPU_SOC_MSS": {"avg": 15.21, "peak": 16.14, "min": 14.17}, "VDD_GPU": {"avg": 33.3, "peak": 40.95, "min": 25.6}, "VIN": {"avg": 76.23, "peak": 119.9, "min": 61.54}}, "power_watts_avg": 33.3, "energy_joules_est": 49.04, "sample_count": 11, "duration_seconds": 1.473}, "timestamp": "2026-01-17T15:15:00.717936"}
{"image_index": 248, "image_name": "000000025424.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025424.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1885.352, "latencies_ms": [1885.352], "images_per_second": 0.53, "prompt_tokens": 23, "response_tokens_est": 44, "n_tiles": 6, "output_text": "1. Tennis player\n2. Tennis racket\n3. Tennis ball\n4. White shirt\n5. White shorts\n6. White wristband\n7. White wristband\n8. White wristband", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24943.4, "ram_available_mb": 100828.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 24944.6, "ram_available_mb": 100827.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.82, "peak": 15.24, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.38, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 31.74, "peak": 40.95, "min": 24.04}, "VIN": {"avg": 77.24, "peak": 116.39, "min": 61.77}}, "power_watts_avg": 31.74, "energy_joules_est": 59.86, "sample_count": 14, "duration_seconds": 1.886}, "timestamp": "2026-01-17T15:15:02.609372"}
{"image_index": 248, "image_name": "000000025424.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025424.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2332.243, "latencies_ms": [2332.243], "images_per_second": 0.429, "prompt_tokens": 27, "response_tokens_est": 60, "n_tiles": 6, "output_text": "The main object in the foreground is a tennis player, who is captured mid-action, with their back to the camera. The player is wearing a white shirt and white shorts, and is holding a tennis racket. The background features a well-maintained grass tennis court with white boundary lines.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24944.6, "ram_available_mb": 100827.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 24944.9, "ram_available_mb": 100827.3, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.89, "peak": 15.24, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.4, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 29.7, "peak": 40.56, "min": 23.25}, "VIN": {"avg": 69.88, "peak": 95.56, "min": 55.83}}, "power_watts_avg": 29.7, "energy_joules_est": 69.28, "sample_count": 18, "duration_seconds": 2.333}, "timestamp": "2026-01-17T15:15:04.947422"}
{"image_index": 248, "image_name": "000000025424.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025424.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3518.618, "latencies_ms": [3518.618], "images_per_second": 0.284, "prompt_tokens": 21, "response_tokens_est": 102, "n_tiles": 6, "output_text": "The image captures a moment on a tennis court, where a player is in the midst of a powerful serve. The player is dressed in white attire, with their back turned to the camera, and is holding a tennis racket in their right hand, which is extended upwards and slightly to the side. The court is well-maintained with green grass, and the player appears to be in the process of executing a powerful serve, with the ball visible in the air to the right of the frame.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24944.9, "ram_available_mb": 100827.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 24945.4, "ram_available_mb": 100826.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.93, "peak": 15.24, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.55, "peak": 15.75, "min": 14.56}, "VDD_GPU": {"avg": 27.28, "peak": 40.57, "min": 22.85}, "VIN": {"avg": 68.78, "peak": 102.82, "min": 60.55}}, "power_watts_avg": 27.28, "energy_joules_est": 96.0, "sample_count": 28, "duration_seconds": 3.519}, "timestamp": "2026-01-17T15:15:08.472740"}
{"image_index": 248, "image_name": "000000025424.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025424.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2672.811, "latencies_ms": [2672.811], "images_per_second": 0.374, "prompt_tokens": 19, "response_tokens_est": 72, "n_tiles": 6, "output_text": "The image captures a tennis player in mid-action, wearing a white shirt and white shorts. The player is holding a tennis racket, and the background shows a well-maintained grass court with white boundary lines. The lighting is bright, indicating it is daytime, and the colors are vibrant, with the green grass and white lines providing a stark contrast.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24945.4, "ram_available_mb": 100826.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 24946.4, "ram_available_mb": 100825.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.9, "peak": 15.24, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.53, "peak": 15.75, "min": 14.56}, "VDD_GPU": {"avg": 28.95, "peak": 40.56, "min": 22.86}, "VIN": {"avg": 67.79, "peak": 90.08, "min": 57.47}}, "power_watts_avg": 28.95, "energy_joules_est": 77.39, "sample_count": 20, "duration_seconds": 2.673}, "timestamp": "2026-01-17T15:15:11.151567"}
{"image_index": 249, "image_name": "000000025560.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025560.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2364.111, "latencies_ms": [2364.111], "images_per_second": 0.423, "prompt_tokens": 9, "response_tokens_est": 22, "n_tiles": 12, "output_text": "A ginger and white cat is perched on a wooden shelf, seemingly watching something on the television screen.", "error": null, "sys_before": {"cpu_percent": 49.3, "ram_used_mb": 24917.8, "ram_available_mb": 100854.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.3, "ram_used_mb": 24920.2, "ram_available_mb": 100852.0, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.15, "peak": 16.66, "min": 13.29}, "VDD_CPU_SOC_MSS": {"avg": 15.94, "peak": 17.71, "min": 14.18}, "VDD_GPU": {"avg": 36.54, "peak": 43.73, "min": 28.37}, "VIN": {"avg": 78.66, "peak": 115.25, "min": 65.68}}, "power_watts_avg": 36.54, "energy_joules_est": 86.4, "sample_count": 18, "duration_seconds": 2.364}, "timestamp": "2026-01-17T15:15:13.615698"}
{"image_index": 249, "image_name": "000000025560.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025560.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2958.237, "latencies_ms": [2958.237], "images_per_second": 0.338, "prompt_tokens": 23, "response_tokens_est": 38, "n_tiles": 12, "output_text": "1. Cat\n2. TV\n3. Shelf\n4. Coffee cup\n5. TV stand\n6. Remote control\n7. DVD player\n8. Bookshelf", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 24920.2, "ram_available_mb": 100852.0, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 24921.7, "ram_available_mb": 100850.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.25, "peak": 16.56, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.9, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 35.92, "peak": 45.68, "min": 26.79}, "VIN": {"avg": 79.43, "peak": 135.42, "min": 59.19}}, "power_watts_avg": 35.92, "energy_joules_est": 106.27, "sample_count": 23, "duration_seconds": 2.959}, "timestamp": "2026-01-17T15:15:16.579945"}
{"image_index": 249, "image_name": "000000025560.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025560.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 4086.875, "latencies_ms": [4086.875], "images_per_second": 0.245, "prompt_tokens": 27, "response_tokens_est": 71, "n_tiles": 12, "output_text": "The main object in the foreground is a cat, which is perched on a wooden shelf. The cat is positioned near the bottom right corner of the image. In the background, there is a television set on a stand, slightly to the left of the cat. The television is positioned behind the cat, making it the focal point of the image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24921.7, "ram_available_mb": 100850.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 24921.9, "ram_available_mb": 100850.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.19, "peak": 16.56, "min": 14.12}, "VDD_CPU_SOC_MSS": {"avg": 15.87, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 33.2, "peak": 45.69, "min": 26.0}, "VIN": {"avg": 73.69, "peak": 104.04, "min": 64.98}}, "power_watts_avg": 33.2, "energy_joules_est": 135.69, "sample_count": 32, "duration_seconds": 4.087}, "timestamp": "2026-01-17T15:15:20.677576"}
{"image_index": 249, "image_name": "000000025560.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025560.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3913.388, "latencies_ms": [3913.388], "images_per_second": 0.256, "prompt_tokens": 21, "response_tokens_est": 66, "n_tiles": 12, "output_text": "The image depicts a cozy indoor setting with a television screen displaying a man in a suit. A cat is perched on a wooden shelf next to the TV, seemingly curious about the screen. The room appears to be a living room or a study, with a bookshelf and a cup of coffee visible in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24921.9, "ram_available_mb": 100850.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 24922.9, "ram_available_mb": 100849.3, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.18, "peak": 16.66, "min": 14.12}, "VDD_CPU_SOC_MSS": {"avg": 15.87, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 33.23, "peak": 45.3, "min": 26.0}, "VIN": {"avg": 75.15, "peak": 118.61, "min": 56.89}}, "power_watts_avg": 33.23, "energy_joules_est": 130.06, "sample_count": 31, "duration_seconds": 3.914}, "timestamp": "2026-01-17T15:15:24.597873"}
{"image_index": 249, "image_name": "000000025560.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025560.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 4285.22, "latencies_ms": [4285.22], "images_per_second": 0.233, "prompt_tokens": 19, "response_tokens_est": 77, "n_tiles": 12, "output_text": "The image features a cat with a white and orange coat, perched on a wooden shelf. The cat's fur appears soft and fluffy, and its tail is slightly curled. The background includes a television set with a dark screen, a white mug on a table, and a wooden bookshelf. The lighting in the room is dim, creating a cozy and intimate atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24922.9, "ram_available_mb": 100849.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 24923.2, "ram_available_mb": 100849.0, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.17, "peak": 16.66, "min": 14.12}, "VDD_CPU_SOC_MSS": {"avg": 15.82, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 32.72, "peak": 44.91, "min": 26.0}, "VIN": {"avg": 76.32, "peak": 137.59, "min": 55.2}}, "power_watts_avg": 32.72, "energy_joules_est": 140.23, "sample_count": 33, "duration_seconds": 4.286}, "timestamp": "2026-01-17T15:15:28.890157"}
{"image_index": 250, "image_name": "000000025593.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025593.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2491.937, "latencies_ms": [2491.937], "images_per_second": 0.401, "prompt_tokens": 9, "response_tokens_est": 26, "n_tiles": 12, "output_text": "The image shows a blue circular sign with white silhouettes of a person and a bicycle, indicating a pedestrian crossing area.", "error": null, "sys_before": {"cpu_percent": 40.6, "ram_used_mb": 24923.4, "ram_available_mb": 100848.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.5, "ram_used_mb": 24922.1, "ram_available_mb": 100850.0, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.18, "peak": 16.66, "min": 13.49}, "VDD_CPU_SOC_MSS": {"avg": 15.97, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 36.49, "peak": 44.12, "min": 27.97}, "VIN": {"avg": 79.3, "peak": 103.7, "min": 67.89}}, "power_watts_avg": 36.49, "energy_joules_est": 90.94, "sample_count": 19, "duration_seconds": 2.492}, "timestamp": "2026-01-17T15:15:31.475997"}
{"image_index": 250, "image_name": "000000025593.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025593.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2488.097, "latencies_ms": [2488.097], "images_per_second": 0.402, "prompt_tokens": 23, "response_tokens_est": 24, "n_tiles": 12, "output_text": "- sign: 2\n- bicycle: 1\n- person: 1\n- tree: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24922.1, "ram_available_mb": 100850.0, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.6, "ram_used_mb": 24921.9, "ram_available_mb": 100850.3, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.31, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.89, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 37.91, "peak": 45.68, "min": 28.36}, "VIN": {"avg": 83.86, "peak": 123.57, "min": 66.69}}, "power_watts_avg": 37.91, "energy_joules_est": 94.34, "sample_count": 19, "duration_seconds": 2.488}, "timestamp": "2026-01-17T15:15:33.970554"}
{"image_index": 250, "image_name": "000000025593.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025593.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 4627.457, "latencies_ms": [4627.457], "images_per_second": 0.216, "prompt_tokens": 27, "response_tokens_est": 87, "n_tiles": 12, "output_text": "The main objects in the image are a signpost with a bicycle symbol and a sign with Japanese characters. The bicycle symbol is located on the top of the signpost, while the sign with Japanese characters is positioned below it. The signpost is situated in the foreground, with the sign and its symbols being the most prominent features. The background consists of a clear blue sky, and the signpost is slightly elevated above the ground.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24921.9, "ram_available_mb": 100850.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 24923.1, "ram_available_mb": 100849.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.17, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.74, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 32.37, "peak": 45.68, "min": 26.0}, "VIN": {"avg": 74.57, "peak": 126.09, "min": 60.9}}, "power_watts_avg": 32.37, "energy_joules_est": 149.8, "sample_count": 36, "duration_seconds": 4.628}, "timestamp": "2026-01-17T15:15:38.604222"}
{"image_index": 250, "image_name": "000000025593.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025593.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 4045.732, "latencies_ms": [4045.732], "images_per_second": 0.247, "prompt_tokens": 21, "response_tokens_est": 70, "n_tiles": 12, "output_text": "The image depicts a street sign with a blue background and white silhouettes of a person and a bicycle. The sign is mounted on a pole, and there are green leaves and a clear blue sky in the background. The sign appears to be a pedestrian crossing sign, indicating that it is a designated area for pedestrians to cross the street.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24923.1, "ram_available_mb": 100849.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 24924.4, "ram_available_mb": 100847.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.19, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.85, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 33.19, "peak": 45.68, "min": 26.0}, "VIN": {"avg": 73.27, "peak": 116.23, "min": 55.48}}, "power_watts_avg": 33.19, "energy_joules_est": 134.29, "sample_count": 31, "duration_seconds": 4.046}, "timestamp": "2026-01-17T15:15:42.657095"}
{"image_index": 250, "image_name": "000000025593.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025593.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3063.569, "latencies_ms": [3063.569], "images_per_second": 0.326, "prompt_tokens": 19, "response_tokens_est": 41, "n_tiles": 12, "output_text": "The image features a blue circular sign with white silhouettes of a person and a bicycle. The sign is mounted on a pole and is under bright sunlight, casting a slight reflection on its surface.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 24924.4, "ram_available_mb": 100847.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 24924.1, "ram_available_mb": 100848.0, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.23, "peak": 16.56, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.87, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 35.76, "peak": 45.28, "min": 26.39}, "VIN": {"avg": 77.51, "peak": 127.15, "min": 60.04}}, "power_watts_avg": 35.76, "energy_joules_est": 109.57, "sample_count": 23, "duration_seconds": 3.064}, "timestamp": "2026-01-17T15:15:45.727544"}
{"image_index": 251, "image_name": "000000025603.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025603.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2601.503, "latencies_ms": [2601.503], "images_per_second": 0.384, "prompt_tokens": 9, "response_tokens_est": 29, "n_tiles": 12, "output_text": "A young girl with dark hair is sitting at a table in a restaurant, eating a slice of pizza and drinking water from a clear glass.", "error": null, "sys_before": {"cpu_percent": 42.4, "ram_used_mb": 24921.4, "ram_available_mb": 100850.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.6, "ram_used_mb": 24924.6, "ram_available_mb": 100847.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.17, "peak": 16.66, "min": 13.29}, "VDD_CPU_SOC_MSS": {"avg": 15.9, "peak": 17.71, "min": 14.17}, "VDD_GPU": {"avg": 35.98, "peak": 44.12, "min": 27.18}, "VIN": {"avg": 78.4, "peak": 116.69, "min": 60.64}}, "power_watts_avg": 35.98, "energy_joules_est": 93.62, "sample_count": 20, "duration_seconds": 2.602}, "timestamp": "2026-01-17T15:15:48.436770"}
{"image_index": 251, "image_name": "000000025603.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025603.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2995.771, "latencies_ms": [2995.771], "images_per_second": 0.334, "prompt_tokens": 23, "response_tokens_est": 39, "n_tiles": 12, "output_text": "1. Pizza\n2. Glass of water\n3. Napkin\n4. Glass\n5. Pieces of pizza\n6. Book\n7. Water bottle\n8. Glass", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24924.6, "ram_available_mb": 100847.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 24924.6, "ram_available_mb": 100847.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.26, "peak": 16.56, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.81, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 35.98, "peak": 45.68, "min": 26.8}, "VIN": {"avg": 77.99, "peak": 129.83, "min": 62.62}}, "power_watts_avg": 35.98, "energy_joules_est": 107.81, "sample_count": 23, "duration_seconds": 2.996}, "timestamp": "2026-01-17T15:15:51.438923"}
{"image_index": 251, "image_name": "000000025603.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025603.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 5115.546, "latencies_ms": [5115.546], "images_per_second": 0.195, "prompt_tokens": 27, "response_tokens_est": 101, "n_tiles": 12, "output_text": "The main objects in the image are a glass of water, a glass of soda, a pizza, a napkin, and a book. The pizza is on a plate, which is placed on a table. The glass of soda is on the table, near the pizza. The napkin is on the table, near the pizza. The book is on the table, near the glass of soda. The pizza is in the foreground, while the glass of soda and the book are in the background.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24924.6, "ram_available_mb": 100847.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 24924.6, "ram_available_mb": 100847.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.17, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.82, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 31.8, "peak": 45.68, "min": 26.0}, "VIN": {"avg": 73.33, "peak": 122.24, "min": 48.3}}, "power_watts_avg": 31.8, "energy_joules_est": 162.69, "sample_count": 40, "duration_seconds": 5.116}, "timestamp": "2026-01-17T15:15:56.560805"}
{"image_index": 251, "image_name": "000000025603.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025603.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 4024.473, "latencies_ms": [4024.473], "images_per_second": 0.248, "prompt_tokens": 21, "response_tokens_est": 69, "n_tiles": 12, "output_text": "The image depicts a young girl sitting at a table in a dimly lit restaurant, possibly a pizzeria, with a pizza on the table. She is holding a glass of water and appears to be enjoying her meal. The setting is cozy and intimate, with other patrons visible in the background, creating a casual dining atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24924.6, "ram_available_mb": 100847.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 24925.6, "ram_available_mb": 100846.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.17, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.69, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 33.07, "peak": 45.69, "min": 25.6}, "VIN": {"avg": 76.87, "peak": 117.47, "min": 56.0}}, "power_watts_avg": 33.07, "energy_joules_est": 133.11, "sample_count": 32, "duration_seconds": 4.025}, "timestamp": "2026-01-17T15:16:00.591381"}
{"image_index": 251, "image_name": "000000025603.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025603.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3517.416, "latencies_ms": [3517.416], "images_per_second": 0.284, "prompt_tokens": 19, "response_tokens_est": 54, "n_tiles": 12, "output_text": "The image depicts a young girl with dark hair and blue eyes, wearing a white sleeveless top and a gold bracelet. She is seated at a wooden table in a dimly lit restaurant, with a glass of water and a partially eaten pizza on the table.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24925.6, "ram_available_mb": 100846.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 24925.6, "ram_available_mb": 100846.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.93, "peak": 16.36, "min": 13.92}, "VDD_CPU_SOC_MSS": {"avg": 15.46, "peak": 17.32, "min": 14.17}, "VDD_GPU": {"avg": 34.24, "peak": 45.69, "min": 25.6}, "VIN": {"avg": 71.46, "peak": 96.18, "min": 48.91}}, "power_watts_avg": 34.24, "energy_joules_est": 120.45, "sample_count": 27, "duration_seconds": 3.518}, "timestamp": "2026-01-17T15:16:04.115162"}
{"image_index": 252, "image_name": "000000025986.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025986.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2965.43, "latencies_ms": [2965.43], "images_per_second": 0.337, "prompt_tokens": 9, "response_tokens_est": 40, "n_tiles": 12, "output_text": "The image shows a kitchen counter with various food items, including a bowl of broccoli, a plate of rice and vegetables, a glass of water, and a plate with a piece of bread.", "error": null, "sys_before": {"cpu_percent": 44.4, "ram_used_mb": 24924.8, "ram_available_mb": 100847.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 24921.0, "ram_available_mb": 100851.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.17, "peak": 16.66, "min": 13.41}, "VDD_CPU_SOC_MSS": {"avg": 15.85, "peak": 17.71, "min": 14.18}, "VDD_GPU": {"avg": 34.87, "peak": 44.1, "min": 26.39}, "VIN": {"avg": 77.36, "peak": 118.14, "min": 63.69}}, "power_watts_avg": 34.87, "energy_joules_est": 103.41, "sample_count": 23, "duration_seconds": 2.966}, "timestamp": "2026-01-17T15:16:07.172665"}
{"image_index": 252, "image_name": "000000025986.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025986.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 6031.204, "latencies_ms": [6031.204], "images_per_second": 0.166, "prompt_tokens": 23, "response_tokens_est": 129, "n_tiles": 12, "output_text": "- plate: 1\n- bowl: 2\n- spoon: 1\n- cup: 1\n- plate: 1\n- bowl: 1\n- plate: 1\n- bowl: 1\n- plate: 1\n- bowl: 1\n- plate: 1\n- bowl: 1\n- plate: 1\n- bowl: 1\n- plate: 1\n- bowl: 1\n- plate: 1\n- bowl: 1\n- plate: 1\n- bowl: 1\n- plate: 1\n- bowl", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24921.0, "ram_available_mb": 100851.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 24920.2, "ram_available_mb": 100852.0, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.17, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.7, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 30.82, "peak": 45.68, "min": 26.0}, "VIN": {"avg": 73.45, "peak": 113.56, "min": 58.91}}, "power_watts_avg": 30.82, "energy_joules_est": 185.89, "sample_count": 47, "duration_seconds": 6.032}, "timestamp": "2026-01-17T15:16:13.214296"}
{"image_index": 252, "image_name": "000000025986.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025986.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3947.064, "latencies_ms": [3947.064], "images_per_second": 0.253, "prompt_tokens": 27, "response_tokens_est": 67, "n_tiles": 12, "output_text": "The main objects in the image are a bowl of broccoli and a plate of food. The broccoli is in the bowl, while the plate of food is in the foreground. The plate of food is placed on a table, and there is a glass of water nearby. The background features a yellow box and a silver bowl.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24920.2, "ram_available_mb": 100852.0, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 24921.4, "ram_available_mb": 100850.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.18, "peak": 16.66, "min": 14.12}, "VDD_CPU_SOC_MSS": {"avg": 15.8, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 33.21, "peak": 44.89, "min": 26.0}, "VIN": {"avg": 73.11, "peak": 103.06, "min": 66.13}}, "power_watts_avg": 33.21, "energy_joules_est": 131.1, "sample_count": 31, "duration_seconds": 3.948}, "timestamp": "2026-01-17T15:16:17.167452"}
{"image_index": 252, "image_name": "000000025986.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025986.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 4002.7, "latencies_ms": [4002.7], "images_per_second": 0.25, "prompt_tokens": 21, "response_tokens_est": 69, "n_tiles": 12, "output_text": "The image depicts a kitchen scene with a focus on a plate of food. The plate contains a mix of vegetables, possibly including broccoli, and a portion of what appears to be a curry or stew. The setting is a kitchen with a stainless steel countertop, and there are other dishes and utensils visible in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24921.4, "ram_available_mb": 100850.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 24919.4, "ram_available_mb": 100852.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.22, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.78, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 33.2, "peak": 44.89, "min": 26.0}, "VIN": {"avg": 78.09, "peak": 113.56, "min": 58.01}}, "power_watts_avg": 33.2, "energy_joules_est": 132.91, "sample_count": 31, "duration_seconds": 4.003}, "timestamp": "2026-01-17T15:16:21.176410"}
{"image_index": 252, "image_name": "000000025986.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025986.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3840.082, "latencies_ms": [3840.082], "images_per_second": 0.26, "prompt_tokens": 19, "response_tokens_est": 64, "n_tiles": 12, "output_text": "The image features a kitchen counter with various food items. The countertop is made of stainless steel, and there is a clear glass of water on it. The lighting is bright, and the colors are vibrant, with the green broccoli, white onion, and orange tomato providing a striking contrast against the metallic surfaces.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24919.4, "ram_available_mb": 100852.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 24920.1, "ram_available_mb": 100852.0, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.19, "peak": 16.56, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.78, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 33.58, "peak": 45.68, "min": 26.0}, "VIN": {"avg": 75.24, "peak": 110.81, "min": 57.91}}, "power_watts_avg": 33.58, "energy_joules_est": 128.96, "sample_count": 30, "duration_seconds": 3.84}, "timestamp": "2026-01-17T15:16:25.022605"}
{"image_index": 253, "image_name": "000000026204.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000026204.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1276.153, "latencies_ms": [1276.153], "images_per_second": 0.784, "prompt_tokens": 9, "response_tokens_est": 25, "n_tiles": 6, "output_text": "A bustling city street scene with a variety of vehicles, including a bus and cars, is captured in the image.", "error": null, "sys_before": {"cpu_percent": 42.7, "ram_used_mb": 24941.3, "ram_available_mb": 100830.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 24942.5, "ram_available_mb": 100829.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.6, "peak": 15.14, "min": 13.82}, "VDD_CPU_SOC_MSS": {"avg": 15.31, "peak": 16.14, "min": 14.17}, "VDD_GPU": {"avg": 35.06, "peak": 40.95, "min": 28.36}, "VIN": {"avg": 78.18, "peak": 120.58, "min": 60.45}}, "power_watts_avg": 35.06, "energy_joules_est": 44.75, "sample_count": 9, "duration_seconds": 1.276}, "timestamp": "2026-01-17T15:16:26.365575"}
{"image_index": 253, "image_name": "000000026204.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000026204.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1512.054, "latencies_ms": [1512.054], "images_per_second": 0.661, "prompt_tokens": 23, "response_tokens_est": 32, "n_tiles": 6, "output_text": "1. Bus\n2. Car\n3. Car\n4. Car\n5. Car\n6. Car\n7. Car\n8. Car", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24942.5, "ram_available_mb": 100829.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 24943.3, "ram_available_mb": 100828.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.98, "peak": 15.44, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.57, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 34.44, "peak": 40.95, "min": 26.39}, "VIN": {"avg": 76.9, "peak": 114.05, "min": 56.89}}, "power_watts_avg": 34.44, "energy_joules_est": 52.09, "sample_count": 11, "duration_seconds": 1.512}, "timestamp": "2026-01-17T15:16:27.883441"}
{"image_index": 253, "image_name": "000000026204.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000026204.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3393.101, "latencies_ms": [3393.101], "images_per_second": 0.295, "prompt_tokens": 27, "response_tokens_est": 97, "n_tiles": 6, "output_text": "In the image, the main objects are a bus, a car, and a building. The bus is in the foreground, slightly to the right, and is moving towards the left side of the frame. The car is in the foreground, closer to the viewer, and is positioned in the center of the frame. The building is in the background, to the right, and is taller than the other objects. The street sign is also visible in the background, near the building.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24943.3, "ram_available_mb": 100828.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 24944.2, "ram_available_mb": 100827.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.94, "peak": 15.34, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.57, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 27.83, "peak": 40.95, "min": 22.85}, "VIN": {"avg": 69.05, "peak": 118.5, "min": 62.46}}, "power_watts_avg": 27.83, "energy_joules_est": 94.44, "sample_count": 26, "duration_seconds": 3.393}, "timestamp": "2026-01-17T15:16:31.287108"}
{"image_index": 253, "image_name": "000000026204.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000026204.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3217.496, "latencies_ms": [3217.496], "images_per_second": 0.311, "prompt_tokens": 21, "response_tokens_est": 91, "n_tiles": 6, "output_text": "The image depicts a bustling urban street scene, likely in a city with a mix of commercial and residential buildings. The street is busy with various vehicles, including a white bus with the number 1, a silver sedan, and several cars parked along the curb. Pedestrians can be seen walking on the sidewalks, and there are street signs indicating directions and traffic rules. The overall atmosphere is typical of a busy city street during daytime.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24944.2, "ram_available_mb": 100827.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 24944.7, "ram_available_mb": 100827.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.91, "peak": 15.24, "min": 14.2}, "VDD_CPU_SOC_MSS": {"avg": 15.5, "peak": 15.75, "min": 14.56}, "VDD_GPU": {"avg": 27.77, "peak": 40.57, "min": 22.85}, "VIN": {"avg": 70.15, "peak": 111.05, "min": 61.12}}, "power_watts_avg": 27.77, "energy_joules_est": 89.37, "sample_count": 25, "duration_seconds": 3.218}, "timestamp": "2026-01-17T15:16:34.511271"}
{"image_index": 253, "image_name": "000000026204.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000026204.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2675.151, "latencies_ms": [2675.151], "images_per_second": 0.374, "prompt_tokens": 19, "response_tokens_est": 72, "n_tiles": 6, "output_text": "The image depicts a bustling urban street scene with a mix of vehicles, including a white bus and several cars, all navigating through a busy intersection. The street is lined with buildings, trees, and a clear sky, indicating a sunny day with ample natural light. The overall atmosphere is lively and dynamic, capturing the essence of a typical city day.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24944.7, "ram_available_mb": 100827.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 24945.0, "ram_available_mb": 100827.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.88, "peak": 15.24, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.62, "peak": 16.14, "min": 14.57}, "VDD_GPU": {"avg": 28.63, "peak": 40.16, "min": 22.86}, "VIN": {"avg": 74.06, "peak": 114.27, "min": 62.31}}, "power_watts_avg": 28.63, "energy_joules_est": 76.6, "sample_count": 21, "duration_seconds": 2.675}, "timestamp": "2026-01-17T15:16:37.192257"}
{"image_index": 254, "image_name": "000000026465.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000026465.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2769.428, "latencies_ms": [2769.428], "images_per_second": 0.361, "prompt_tokens": 9, "response_tokens_est": 34, "n_tiles": 12, "output_text": "The image shows a laptop with a black keyboard and a screen displaying a desktop interface, alongside a smartphone and a small electronic device, all resting on a white surface.", "error": null, "sys_before": {"cpu_percent": 49.6, "ram_used_mb": 24913.9, "ram_available_mb": 100858.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 24915.9, "ram_available_mb": 100856.3, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.14, "peak": 16.66, "min": 13.29}, "VDD_CPU_SOC_MSS": {"avg": 15.97, "peak": 17.71, "min": 14.18}, "VDD_GPU": {"avg": 35.28, "peak": 43.71, "min": 26.79}, "VIN": {"avg": 78.11, "peak": 110.11, "min": 66.48}}, "power_watts_avg": 35.28, "energy_joules_est": 97.71, "sample_count": 21, "duration_seconds": 2.77}, "timestamp": "2026-01-17T15:16:40.062852"}
{"image_index": 254, "image_name": "000000026465.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000026465.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2480.259, "latencies_ms": [2480.259], "images_per_second": 0.403, "prompt_tokens": 23, "response_tokens_est": 24, "n_tiles": 12, "output_text": "- laptop: 1\n- smartphone: 1\n- calculator: 1\n- notebook: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24915.9, "ram_available_mb": 100856.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.5, "ram_used_mb": 24917.1, "ram_available_mb": 100855.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.27, "peak": 16.56, "min": 14.12}, "VDD_CPU_SOC_MSS": {"avg": 15.87, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 37.56, "peak": 45.68, "min": 28.36}, "VIN": {"avg": 80.26, "peak": 115.6, "min": 65.88}}, "power_watts_avg": 37.56, "energy_joules_est": 93.19, "sample_count": 19, "duration_seconds": 2.481}, "timestamp": "2026-01-17T15:16:42.551194"}
{"image_index": 254, "image_name": "000000026465.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000026465.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3950.312, "latencies_ms": [3950.312], "images_per_second": 0.253, "prompt_tokens": 27, "response_tokens_est": 67, "n_tiles": 12, "output_text": "The main objects in the image are a laptop, a smartphone, and a small electronic device. The laptop is positioned in the foreground, with the smartphone and the small electronic device placed near it. The smartphone is slightly to the right of the laptop, while the small electronic device is closer to the bottom left corner of the image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24917.1, "ram_available_mb": 100855.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 24917.6, "ram_available_mb": 100854.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.2, "peak": 16.56, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.77, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 33.66, "peak": 45.68, "min": 26.0}, "VIN": {"avg": 76.44, "peak": 124.43, "min": 66.27}}, "power_watts_avg": 33.66, "energy_joules_est": 132.98, "sample_count": 30, "duration_seconds": 3.951}, "timestamp": "2026-01-17T15:16:46.508453"}
{"image_index": 254, "image_name": "000000026465.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000026465.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 4355.559, "latencies_ms": [4355.559], "images_per_second": 0.23, "prompt_tokens": 21, "response_tokens_est": 79, "n_tiles": 12, "output_text": "The image shows a cluttered desk with various items scattered around. The desk appears to be in a room with a white wall and a window with a partially visible view of the outside. The items on the desk include a laptop, a smartphone, a small black object that could be a remote or a phone, and a black object that might be a remote control or a small electronic device.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24917.6, "ram_available_mb": 100854.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 24917.6, "ram_available_mb": 100854.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.21, "peak": 16.56, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.78, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 32.62, "peak": 44.89, "min": 26.0}, "VIN": {"avg": 78.09, "peak": 124.87, "min": 58.11}}, "power_watts_avg": 32.62, "energy_joules_est": 142.09, "sample_count": 33, "duration_seconds": 4.356}, "timestamp": "2026-01-17T15:16:50.871116"}
{"image_index": 254, "image_name": "000000026465.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000026465.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3052.94, "latencies_ms": [3052.94], "images_per_second": 0.328, "prompt_tokens": 19, "response_tokens_est": 41, "n_tiles": 12, "output_text": "The image features a laptop with a black keyboard and a silver-colored mouse. The laptop is placed on a white surface, and the lighting is bright, casting clear reflections on the laptop's surface.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24917.6, "ram_available_mb": 100854.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 24918.8, "ram_available_mb": 100853.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.26, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.88, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 35.55, "peak": 44.49, "min": 26.8}, "VIN": {"avg": 75.83, "peak": 112.72, "min": 53.4}}, "power_watts_avg": 35.55, "energy_joules_est": 108.54, "sample_count": 23, "duration_seconds": 3.053}, "timestamp": "2026-01-17T15:16:53.930779"}
{"image_index": 255, "image_name": "000000026564.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000026564.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1437.177, "latencies_ms": [1437.177], "images_per_second": 0.696, "prompt_tokens": 9, "response_tokens_est": 31, "n_tiles": 6, "output_text": "The image shows a well-organized workspace with a desktop computer, a keyboard, a mouse, and a laptop, all placed on a desk.", "error": null, "sys_before": {"cpu_percent": 40.2, "ram_used_mb": 24947.9, "ram_available_mb": 100824.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 24948.6, "ram_available_mb": 100823.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.65, "peak": 15.14, "min": 13.72}, "VDD_CPU_SOC_MSS": {"avg": 15.35, "peak": 16.14, "min": 14.17}, "VDD_GPU": {"avg": 33.87, "peak": 40.56, "min": 26.79}, "VIN": {"avg": 74.72, "peak": 111.01, "min": 52.0}}, "power_watts_avg": 33.87, "energy_joules_est": 48.69, "sample_count": 10, "duration_seconds": 1.437}, "timestamp": "2026-01-17T15:16:55.433656"}
{"image_index": 255, "image_name": "000000026564.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000026564.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1836.9, "latencies_ms": [1836.9], "images_per_second": 0.544, "prompt_tokens": 23, "response_tokens_est": 44, "n_tiles": 6, "output_text": "- computer monitor: 1\n- keyboard: 1\n- mouse: 1\n- laptop: 1\n- book: 5\n- pen: 1\n- water bottle: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24948.6, "ram_available_mb": 100823.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 24948.9, "ram_available_mb": 100823.3, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.05, "peak": 15.44, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.61, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 31.99, "peak": 40.56, "min": 24.42}, "VIN": {"avg": 73.6, "peak": 106.24, "min": 59.19}}, "power_watts_avg": 31.99, "energy_joules_est": 58.78, "sample_count": 14, "duration_seconds": 1.837}, "timestamp": "2026-01-17T15:16:57.276728"}
{"image_index": 255, "image_name": "000000026564.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000026564.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2764.889, "latencies_ms": [2764.889], "images_per_second": 0.362, "prompt_tokens": 27, "response_tokens_est": 78, "n_tiles": 6, "output_text": "The main objects in the image are a desk with various items on it, including a laptop, a keyboard, a mouse, and a water bottle. The laptop is positioned in the background, slightly to the right, while the keyboard and mouse are in the foreground, closer to the viewer. The water bottle is placed near the keyboard, indicating that it is easily accessible for the user.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24948.9, "ram_available_mb": 100823.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 24949.1, "ram_available_mb": 100823.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.11, "peak": 15.54, "min": 14.43}, "VDD_CPU_SOC_MSS": {"avg": 15.79, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 29.07, "peak": 40.16, "min": 23.64}, "VIN": {"avg": 70.11, "peak": 111.41, "min": 57.13}}, "power_watts_avg": 29.07, "energy_joules_est": 80.38, "sample_count": 21, "duration_seconds": 2.765}, "timestamp": "2026-01-17T15:17:00.047463"}
{"image_index": 255, "image_name": "000000026564.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000026564.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3243.117, "latencies_ms": [3243.117], "images_per_second": 0.308, "prompt_tokens": 21, "response_tokens_est": 92, "n_tiles": 6, "output_text": "The image depicts a well-organized workspace with a desk set up for a computer. The desk is equipped with a laptop, a keyboard, a mouse, and a cup of coffee. The workspace is brightly lit by natural light coming through a window, and there are various items scattered around, including books, a pen, and a small stuffed animal. The overall setting suggests a comfortable and functional workspace, likely in a home or office environment.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24949.1, "ram_available_mb": 100823.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 24950.1, "ram_available_mb": 100822.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.93, "peak": 15.34, "min": 14.4}, "VDD_CPU_SOC_MSS": {"avg": 15.53, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 27.75, "peak": 40.16, "min": 22.85}, "VIN": {"avg": 67.69, "peak": 102.1, "min": 57.0}}, "power_watts_avg": 27.75, "energy_joules_est": 90.0, "sample_count": 25, "duration_seconds": 3.243}, "timestamp": "2026-01-17T15:17:03.296641"}
{"image_index": 255, "image_name": "000000026564.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000026564.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2305.027, "latencies_ms": [2305.027], "images_per_second": 0.434, "prompt_tokens": 19, "response_tokens_est": 59, "n_tiles": 6, "output_text": "The image depicts a well-organized workspace with a light-colored desk. The desk is equipped with a laptop, a keyboard, a mouse, and a water bottle. The lighting is bright, likely from natural light coming through a window, and the overall atmosphere is clean and tidy.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24950.1, "ram_available_mb": 100822.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 24950.8, "ram_available_mb": 100821.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.85, "peak": 15.24, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.53, "peak": 15.75, "min": 14.56}, "VDD_GPU": {"avg": 29.74, "peak": 40.95, "min": 23.25}, "VIN": {"avg": 70.87, "peak": 95.37, "min": 59.7}}, "power_watts_avg": 29.74, "energy_joules_est": 68.57, "sample_count": 18, "duration_seconds": 2.305}, "timestamp": "2026-01-17T15:17:05.608446"}
{"image_index": 256, "image_name": "000000026690.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000026690.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1193.936, "latencies_ms": [1193.936], "images_per_second": 0.838, "prompt_tokens": 9, "response_tokens_est": 22, "n_tiles": 6, "output_text": "A skateboarder is performing a trick in an indoor arena, with spectators watching from the stands.", "error": null, "sys_before": {"cpu_percent": 34.2, "ram_used_mb": 24950.8, "ram_available_mb": 100821.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 24951.1, "ram_available_mb": 100821.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.6, "peak": 15.14, "min": 13.69}, "VDD_CPU_SOC_MSS": {"avg": 15.4, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 34.22, "peak": 40.16, "min": 27.97}, "VIN": {"avg": 75.25, "peak": 97.46, "min": 60.95}}, "power_watts_avg": 34.22, "energy_joules_est": 40.87, "sample_count": 9, "duration_seconds": 1.194}, "timestamp": "2026-01-17T15:17:06.864602"}
{"image_index": 256, "image_name": "000000026690.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000026690.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2670.969, "latencies_ms": [2670.969], "images_per_second": 0.374, "prompt_tokens": 23, "response_tokens_est": 72, "n_tiles": 6, "output_text": "1. Skateboarder\n2. Skateboard\n3. Skateboarder's helmet\n4. Skateboarder's knee pads\n5. Skateboarder's wrist guard\n6. Skateboarder's wrist guard\n7. Skateboarder's wrist guard\n8. Skateboarder's wrist guard", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24951.1, "ram_available_mb": 100821.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 24951.6, "ram_available_mb": 100820.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.89, "peak": 15.24, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.49, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 29.34, "peak": 41.74, "min": 22.86}, "VIN": {"avg": 70.74, "peak": 106.75, "min": 60.41}}, "power_watts_avg": 29.34, "energy_joules_est": 78.38, "sample_count": 21, "duration_seconds": 2.671}, "timestamp": "2026-01-17T15:17:09.541619"}
{"image_index": 256, "image_name": "000000026690.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000026690.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3420.799, "latencies_ms": [3420.799], "images_per_second": 0.292, "prompt_tokens": 27, "response_tokens_est": 98, "n_tiles": 6, "output_text": "The main object in the foreground is a skateboarder performing a trick in mid-air. The skateboarder is wearing a black shirt and a helmet, and is holding onto the skateboard with both hands. The skateboarder is positioned near the center of the image, with the audience in the background. The audience is seated on a balcony, with some individuals standing and others sitting. The skateboarder is near the center of the image, with the audience in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24951.6, "ram_available_mb": 100820.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 24952.0, "ram_available_mb": 100820.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.9, "peak": 15.24, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.48, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 27.55, "peak": 40.56, "min": 22.85}, "VIN": {"avg": 69.64, "peak": 108.87, "min": 59.96}}, "power_watts_avg": 27.55, "energy_joules_est": 94.26, "sample_count": 26, "duration_seconds": 3.421}, "timestamp": "2026-01-17T15:17:12.972691"}
{"image_index": 256, "image_name": "000000026690.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000026690.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2505.269, "latencies_ms": [2505.269], "images_per_second": 0.399, "prompt_tokens": 21, "response_tokens_est": 66, "n_tiles": 6, "output_text": "The image captures a dynamic moment at an indoor skateboarding event, likely a competition or exhibition. A skateboarder is in mid-air, performing a trick, while a crowd of spectators is gathered around the event area. The setting is an indoor sports arena, with a high ceiling and a large audience watching the performance.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24952.0, "ram_available_mb": 100820.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 24951.8, "ram_available_mb": 100820.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.86, "peak": 15.24, "min": 14.2}, "VDD_CPU_SOC_MSS": {"avg": 15.5, "peak": 15.75, "min": 14.56}, "VDD_GPU": {"avg": 29.13, "peak": 40.16, "min": 22.86}, "VIN": {"avg": 66.78, "peak": 80.52, "min": 57.86}}, "power_watts_avg": 29.13, "energy_joules_est": 72.99, "sample_count": 19, "duration_seconds": 2.506}, "timestamp": "2026-01-17T15:17:15.484194"}
{"image_index": 256, "image_name": "000000026690.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000026690.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2251.14, "latencies_ms": [2251.14], "images_per_second": 0.444, "prompt_tokens": 19, "response_tokens_est": 57, "n_tiles": 6, "output_text": "The image depicts a vibrant and dynamic scene in an indoor sports arena. The lighting is bright, highlighting the action taking place. The arena's ceiling is adorned with a patterned design, and the audience is seated on tiered seating, with some individuals wearing protective gear.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24951.8, "ram_available_mb": 100820.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 24952.0, "ram_available_mb": 100820.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.85, "peak": 15.24, "min": 14.3}, "VDD_CPU_SOC_MSS": {"avg": 15.56, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 29.84, "peak": 40.16, "min": 23.24}, "VIN": {"avg": 69.26, "peak": 110.78, "min": 60.34}}, "power_watts_avg": 29.84, "energy_joules_est": 67.19, "sample_count": 17, "duration_seconds": 2.252}, "timestamp": "2026-01-17T15:17:17.741515"}
{"image_index": 257, "image_name": "000000026926.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000026926.jpg", "image_width": 423, "image_height": 640, "image_resolution": "423x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1468.348, "latencies_ms": [1468.348], "images_per_second": 0.681, "prompt_tokens": 9, "response_tokens_est": 32, "n_tiles": 6, "output_text": "The image features a red fire hydrant with a black face painted on it, standing on a sidewalk with a yellow line and a tree in the background.", "error": null, "sys_before": {"cpu_percent": 35.5, "ram_used_mb": 24952.0, "ram_available_mb": 100820.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 24952.0, "ram_available_mb": 100820.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.66, "peak": 15.11, "min": 13.69}, "VDD_CPU_SOC_MSS": {"avg": 15.32, "peak": 16.14, "min": 14.17}, "VDD_GPU": {"avg": 32.69, "peak": 40.16, "min": 25.6}, "VIN": {"avg": 75.52, "peak": 120.13, "min": 59.81}}, "power_watts_avg": 32.69, "energy_joules_est": 48.01, "sample_count": 11, "duration_seconds": 1.469}, "timestamp": "2026-01-17T15:17:19.269530"}
{"image_index": 257, "image_name": "000000026926.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000026926.jpg", "image_width": 423, "image_height": 640, "image_resolution": "423x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1727.277, "latencies_ms": [1727.277], "images_per_second": 0.579, "prompt_tokens": 23, "response_tokens_est": 40, "n_tiles": 6, "output_text": "object: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24952.0, "ram_available_mb": 100820.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 24952.8, "ram_available_mb": 100819.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.03, "peak": 15.54, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.56, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 32.57, "peak": 40.95, "min": 24.82}, "VIN": {"avg": 72.61, "peak": 91.96, "min": 62.67}}, "power_watts_avg": 32.57, "energy_joules_est": 56.27, "sample_count": 13, "duration_seconds": 1.728}, "timestamp": "2026-01-17T15:17:21.002769"}
{"image_index": 257, "image_name": "000000026926.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000026926.jpg", "image_width": 423, "image_height": 640, "image_resolution": "423x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2486.128, "latencies_ms": [2486.128], "images_per_second": 0.402, "prompt_tokens": 27, "response_tokens_est": 68, "n_tiles": 6, "output_text": "The main object in the foreground is a red fire hydrant with a black handle and a black emblem on it. The fire hydrant is positioned on the right side of the image, near the sidewalk. In the background, there are trees and a few buildings, indicating that the fire hydrant is located in an urban setting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24952.8, "ram_available_mb": 100819.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 24952.8, "ram_available_mb": 100819.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.09, "peak": 15.44, "min": 14.43}, "VDD_CPU_SOC_MSS": {"avg": 15.67, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 29.71, "peak": 40.56, "min": 23.64}, "VIN": {"avg": 67.77, "peak": 79.0, "min": 59.37}}, "power_watts_avg": 29.71, "energy_joules_est": 73.87, "sample_count": 19, "duration_seconds": 2.486}, "timestamp": "2026-01-17T15:17:23.494810"}
{"image_index": 257, "image_name": "000000026926.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000026926.jpg", "image_width": 423, "image_height": 640, "image_resolution": "423x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2318.851, "latencies_ms": [2318.851], "images_per_second": 0.431, "prompt_tokens": 21, "response_tokens_est": 62, "n_tiles": 6, "output_text": "The image depicts a vibrant red fire hydrant situated on a city street. The hydrant is painted with a black smiley face, adding a playful and cheerful element to the urban scene. The background shows a street lined with buildings, trees, and parked cars, suggesting a typical city environment.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24952.8, "ram_available_mb": 100819.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 24953.0, "ram_available_mb": 100819.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.11, "peak": 15.54, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.64, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 29.96, "peak": 40.16, "min": 23.64}, "VIN": {"avg": 71.22, "peak": 117.57, "min": 61.85}}, "power_watts_avg": 29.96, "energy_joules_est": 69.48, "sample_count": 18, "duration_seconds": 2.319}, "timestamp": "2026-01-17T15:17:25.819533"}
{"image_index": 257, "image_name": "000000026926.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000026926.jpg", "image_width": 423, "image_height": 640, "image_resolution": "423x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1944.132, "latencies_ms": [1944.132], "images_per_second": 0.514, "prompt_tokens": 19, "response_tokens_est": 48, "n_tiles": 6, "output_text": "The notable visual attributes of the image include a vivid red fire hydrant with a black cap and handle, which stands out against the urban backdrop. The lighting is natural, suggesting daytime, and the weather appears to be clear and sunny.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24953.0, "ram_available_mb": 100819.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 24953.0, "ram_available_mb": 100819.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.06, "peak": 15.44, "min": 14.43}, "VDD_CPU_SOC_MSS": {"avg": 15.64, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 31.2, "peak": 40.95, "min": 24.04}, "VIN": {"avg": 71.53, "peak": 110.64, "min": 57.92}}, "power_watts_avg": 31.2, "energy_joules_est": 60.67, "sample_count": 15, "duration_seconds": 1.945}, "timestamp": "2026-01-17T15:17:27.769837"}
{"image_index": 258, "image_name": "000000026941.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000026941.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1385.269, "latencies_ms": [1385.269], "images_per_second": 0.722, "prompt_tokens": 9, "response_tokens_est": 29, "n_tiles": 6, "output_text": "A collection of old, worn suitcases are stacked on top of each other on a green cart, with a green door in the background.", "error": null, "sys_before": {"cpu_percent": 34.2, "ram_used_mb": 24953.0, "ram_available_mb": 100819.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 24954.0, "ram_available_mb": 100818.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.69, "peak": 15.14, "min": 13.9}, "VDD_CPU_SOC_MSS": {"avg": 15.43, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 33.76, "peak": 40.18, "min": 26.79}, "VIN": {"avg": 73.77, "peak": 96.86, "min": 61.77}}, "power_watts_avg": 33.76, "energy_joules_est": 46.78, "sample_count": 10, "duration_seconds": 1.386}, "timestamp": "2026-01-17T15:17:29.221393"}
{"image_index": 258, "image_name": "000000026941.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000026941.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2296.586, "latencies_ms": [2296.586], "images_per_second": 0.435, "prompt_tokens": 23, "response_tokens_est": 61, "n_tiles": 6, "output_text": "1. Green cart\n2. 2 green suitcases\n3. 2 brown suitcases\n4. 2 blue suitcases\n5. 2 green suitcases\n6. 2 brown suitcases\n7. 2 blue suitcases\n8. 2 green suitcases", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24954.0, "ram_available_mb": 100818.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 24954.2, "ram_available_mb": 100817.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.03, "peak": 15.44, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.63, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 30.63, "peak": 40.95, "min": 23.64}, "VIN": {"avg": 71.76, "peak": 119.87, "min": 58.79}}, "power_watts_avg": 30.63, "energy_joules_est": 70.36, "sample_count": 17, "duration_seconds": 2.297}, "timestamp": "2026-01-17T15:17:31.528050"}
{"image_index": 258, "image_name": "000000026941.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000026941.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3062.461, "latencies_ms": [3062.461], "images_per_second": 0.327, "prompt_tokens": 27, "response_tokens_est": 89, "n_tiles": 6, "output_text": "The main objects in the image are a collection of old suitcases stacked on top of each other. The suitcases are positioned in the foreground, with the largest one on the left and the smallest one on the right. The green cart is situated in the middle ground, slightly to the right of the image. The background features a green door and a poster with a child, while the left side of the image shows a green trash can.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24954.2, "ram_available_mb": 100817.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 24954.0, "ram_available_mb": 100818.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.1, "peak": 15.54, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.68, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 28.32, "peak": 40.18, "min": 23.25}, "VIN": {"avg": 69.96, "peak": 92.85, "min": 64.04}}, "power_watts_avg": 28.32, "energy_joules_est": 86.74, "sample_count": 24, "duration_seconds": 3.063}, "timestamp": "2026-01-17T15:17:34.596255"}
{"image_index": 258, "image_name": "000000026941.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000026941.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2370.725, "latencies_ms": [2370.725], "images_per_second": 0.422, "prompt_tokens": 21, "response_tokens_est": 64, "n_tiles": 6, "output_text": "The image depicts a collection of old, weathered suitcases stacked on top of each other on a green cart. The suitcases are placed against a green door, suggesting they are being transported or stored in a garage or storage area. The scene appears to be outdoors, with natural light illuminating the area.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24954.0, "ram_available_mb": 100818.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 24954.2, "ram_available_mb": 100818.0, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.1, "peak": 15.54, "min": 14.43}, "VDD_CPU_SOC_MSS": {"avg": 15.64, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 29.81, "peak": 40.16, "min": 23.64}, "VIN": {"avg": 73.11, "peak": 107.7, "min": 60.0}}, "power_watts_avg": 29.81, "energy_joules_est": 70.68, "sample_count": 18, "duration_seconds": 2.371}, "timestamp": "2026-01-17T15:17:36.973258"}
{"image_index": 258, "image_name": "000000026941.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000026941.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2402.734, "latencies_ms": [2402.734], "images_per_second": 0.416, "prompt_tokens": 19, "response_tokens_est": 65, "n_tiles": 6, "output_text": "The image features a collection of vintage suitcases stacked on top of each other, with a green cart in the foreground. The suitcases are made of various materials, including metal and fabric, and exhibit signs of wear and age. The lighting is natural, suggesting daytime, and the weather appears to be clear and sunny.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24954.2, "ram_available_mb": 100818.0, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 24954.0, "ram_available_mb": 100818.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.09, "peak": 15.54, "min": 14.43}, "VDD_CPU_SOC_MSS": {"avg": 15.66, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 29.81, "peak": 40.16, "min": 23.64}, "VIN": {"avg": 74.57, "peak": 117.55, "min": 63.75}}, "power_watts_avg": 29.81, "energy_joules_est": 71.64, "sample_count": 18, "duration_seconds": 2.403}, "timestamp": "2026-01-17T15:17:39.381969"}
{"image_index": 259, "image_name": "000000027186.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000027186.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1305.835, "latencies_ms": [1305.835], "images_per_second": 0.766, "prompt_tokens": 9, "response_tokens_est": 26, "n_tiles": 6, "output_text": "A young girl is sitting on a couch, wearing a pink dress with a floral pattern, and holding a white remote control.", "error": null, "sys_before": {"cpu_percent": 40.3, "ram_used_mb": 24954.0, "ram_available_mb": 100818.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 24954.5, "ram_available_mb": 100817.7, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.69, "peak": 15.24, "min": 13.9}, "VDD_CPU_SOC_MSS": {"avg": 15.4, "peak": 16.14, "min": 14.57}, "VDD_GPU": {"avg": 34.09, "peak": 40.16, "min": 27.57}, "VIN": {"avg": 73.53, "peak": 113.55, "min": 54.04}}, "power_watts_avg": 34.09, "energy_joules_est": 44.53, "sample_count": 9, "duration_seconds": 1.306}, "timestamp": "2026-01-17T15:17:40.748627"}
{"image_index": 259, "image_name": "000000027186.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000027186.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1769.967, "latencies_ms": [1769.967], "images_per_second": 0.565, "prompt_tokens": 23, "response_tokens_est": 40, "n_tiles": 6, "output_text": "object: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24954.5, "ram_available_mb": 100817.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 24954.4, "ram_available_mb": 100817.7, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.83, "peak": 15.24, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.47, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 32.72, "peak": 41.34, "min": 24.42}, "VIN": {"avg": 77.46, "peak": 123.31, "min": 58.53}}, "power_watts_avg": 32.72, "energy_joules_est": 57.93, "sample_count": 13, "duration_seconds": 1.77}, "timestamp": "2026-01-17T15:17:42.524524"}
{"image_index": 259, "image_name": "000000027186.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000027186.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2647.481, "latencies_ms": [2647.481], "images_per_second": 0.378, "prompt_tokens": 27, "response_tokens_est": 71, "n_tiles": 6, "output_text": "The main object in the foreground is a young girl wearing a pink dress with a floral pattern. She is holding a white remote control in her right hand. The background features a window with white blinds, partially drawn, allowing natural light to enter the room. The couch is positioned to the right of the girl, with a brown pillow on it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24954.4, "ram_available_mb": 100817.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 24955.2, "ram_available_mb": 100817.0, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.87, "peak": 15.24, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.57, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 29.13, "peak": 40.56, "min": 22.85}, "VIN": {"avg": 67.78, "peak": 89.81, "min": 54.58}}, "power_watts_avg": 29.13, "energy_joules_est": 77.13, "sample_count": 20, "duration_seconds": 2.648}, "timestamp": "2026-01-17T15:17:45.178092"}
{"image_index": 259, "image_name": "000000027186.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000027186.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2190.829, "latencies_ms": [2190.829], "images_per_second": 0.456, "prompt_tokens": 21, "response_tokens_est": 55, "n_tiles": 6, "output_text": "The image depicts a young girl sitting on a couch in a cozy living room. She is holding a white remote control and appears to be watching something on the screen. The room has a warm ambiance with blinds partially drawn, allowing natural light to filter in.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24955.2, "ram_available_mb": 100817.0, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 24955.2, "ram_available_mb": 100817.0, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.86, "peak": 15.24, "min": 14.3}, "VDD_CPU_SOC_MSS": {"avg": 15.42, "peak": 15.75, "min": 14.56}, "VDD_GPU": {"avg": 30.01, "peak": 40.95, "min": 23.24}, "VIN": {"avg": 68.39, "peak": 93.13, "min": 48.64}}, "power_watts_avg": 30.01, "energy_joules_est": 65.77, "sample_count": 17, "duration_seconds": 2.192}, "timestamp": "2026-01-17T15:17:47.375333"}
{"image_index": 259, "image_name": "000000027186.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000027186.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2048.402, "latencies_ms": [2048.402], "images_per_second": 0.488, "prompt_tokens": 19, "response_tokens_est": 50, "n_tiles": 6, "output_text": "The image features a young girl in a pink dress with a floral pattern, sitting on a beige couch. The room has a warm, cozy ambiance with natural light coming through partially closed white blinds, creating a soft and inviting atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24955.2, "ram_available_mb": 100817.0, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 24955.9, "ram_available_mb": 100816.3, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.86, "peak": 15.14, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.45, "peak": 15.75, "min": 14.56}, "VDD_GPU": {"avg": 30.6, "peak": 41.34, "min": 23.64}, "VIN": {"avg": 69.27, "peak": 90.7, "min": 62.26}}, "power_watts_avg": 30.6, "energy_joules_est": 62.73, "sample_count": 16, "duration_seconds": 2.05}, "timestamp": "2026-01-17T15:17:49.431207"}
{"image_index": 260, "image_name": "000000027620.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000027620.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2931.646, "latencies_ms": [2931.646], "images_per_second": 0.341, "prompt_tokens": 9, "response_tokens_est": 39, "n_tiles": 12, "output_text": "The image shows a cluttered office desk with a computer, a keyboard, a mouse, headphones, a trash can, and a chair, all arranged in a somewhat disorganized manner.", "error": null, "sys_before": {"cpu_percent": 43.8, "ram_used_mb": 24923.5, "ram_available_mb": 100848.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 24925.2, "ram_available_mb": 100847.0, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.2, "peak": 16.66, "min": 13.59}, "VDD_CPU_SOC_MSS": {"avg": 15.88, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 34.66, "peak": 43.73, "min": 26.39}, "VIN": {"avg": 75.75, "peak": 114.79, "min": 58.34}}, "power_watts_avg": 34.66, "energy_joules_est": 101.62, "sample_count": 23, "duration_seconds": 2.932}, "timestamp": "2026-01-17T15:17:52.445071"}
{"image_index": 260, "image_name": "000000027620.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000027620.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2824.525, "latencies_ms": [2824.525], "images_per_second": 0.354, "prompt_tokens": 23, "response_tokens_est": 34, "n_tiles": 12, "output_text": "1. Desk\n2. Computer\n3. Monitor\n4. Headphones\n5. Cable\n6. Keyboard\n7. Mouse\n8. Waste Bin", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24925.2, "ram_available_mb": 100847.0, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.6, "ram_used_mb": 24926.2, "ram_available_mb": 100846.0, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.25, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.94, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 36.17, "peak": 45.28, "min": 26.79}, "VIN": {"avg": 79.95, "peak": 127.39, "min": 61.47}}, "power_watts_avg": 36.17, "energy_joules_est": 102.18, "sample_count": 22, "duration_seconds": 2.825}, "timestamp": "2026-01-17T15:17:55.276051"}
{"image_index": 260, "image_name": "000000027620.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000027620.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 4804.835, "latencies_ms": [4804.835], "images_per_second": 0.208, "prompt_tokens": 27, "response_tokens_est": 92, "n_tiles": 12, "output_text": "The main objects in the image are a desk, a computer, a keyboard, a mouse, headphones, a trash can, and a chair. The desk is positioned in the foreground, with the computer and keyboard placed on it. The keyboard is near the computer, and the mouse is on the desk. The headphones are on the desk, and the trash can is to the left of the computer. The chair is in the background, near the desk.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24926.2, "ram_available_mb": 100846.0, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 24927.2, "ram_available_mb": 100845.0, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.16, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.68, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 31.88, "peak": 44.89, "min": 26.0}, "VIN": {"avg": 74.06, "peak": 124.91, "min": 58.16}}, "power_watts_avg": 31.88, "energy_joules_est": 153.19, "sample_count": 38, "duration_seconds": 4.805}, "timestamp": "2026-01-17T15:18:00.086810"}
{"image_index": 260, "image_name": "000000027620.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000027620.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 4074.031, "latencies_ms": [4074.031], "images_per_second": 0.245, "prompt_tokens": 21, "response_tokens_est": 70, "n_tiles": 12, "output_text": "The image depicts a cluttered office desk with various items scattered around. The desk is cluttered with a computer monitor, a keyboard, a mouse, a pair of headphones, a trash can, and other miscellaneous objects. The room appears to be a small office or workspace, with a window and a radiator visible in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24927.2, "ram_available_mb": 100845.0, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 24927.4, "ram_available_mb": 100844.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.91, "peak": 16.36, "min": 14.02}, "VDD_CPU_SOC_MSS": {"avg": 15.5, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 32.77, "peak": 45.69, "min": 25.6}, "VIN": {"avg": 72.14, "peak": 113.99, "min": 61.26}}, "power_watts_avg": 32.77, "energy_joules_est": 133.53, "sample_count": 32, "duration_seconds": 4.075}, "timestamp": "2026-01-17T15:18:04.167907"}
{"image_index": 260, "image_name": "000000027620.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000027620.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3754.978, "latencies_ms": [3754.978], "images_per_second": 0.266, "prompt_tokens": 19, "response_tokens_est": 61, "n_tiles": 12, "output_text": "The image shows a cluttered office desk with a cluttered computer setup. The desk is cluttered with various items such as a computer monitor, keyboard, mouse, headphones, and a trash can. The lighting in the room is dim, and the overall atmosphere appears to be somewhat disorganized.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24927.4, "ram_available_mb": 100844.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 24928.1, "ram_available_mb": 100844.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.91, "peak": 16.26, "min": 13.92}, "VDD_CPU_SOC_MSS": {"avg": 15.5, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 33.62, "peak": 45.3, "min": 25.6}, "VIN": {"avg": 74.89, "peak": 116.22, "min": 59.81}}, "power_watts_avg": 33.62, "energy_joules_est": 126.25, "sample_count": 29, "duration_seconds": 3.755}, "timestamp": "2026-01-17T15:18:07.929097"}
{"image_index": 261, "image_name": "000000027696.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000027696.jpg", "image_width": 640, "image_height": 410, "image_resolution": "640x410", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1815.231, "latencies_ms": [1815.231], "images_per_second": 0.551, "prompt_tokens": 9, "response_tokens_est": 45, "n_tiles": 6, "output_text": "The image shows a freshly baked pizza with a variety of toppings, including melted cheese, sliced green peppers, and chunks of mushrooms, all resting on a white plate with a red and white checkered tablecloth.", "error": null, "sys_before": {"cpu_percent": 38.3, "ram_used_mb": 24940.2, "ram_available_mb": 100831.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 24941.2, "ram_available_mb": 100831.0, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.72, "peak": 15.22, "min": 13.59}, "VDD_CPU_SOC_MSS": {"avg": 15.38, "peak": 15.75, "min": 14.17}, "VDD_GPU": {"avg": 31.57, "peak": 40.57, "min": 24.43}, "VIN": {"avg": 73.13, "peak": 116.39, "min": 60.21}}, "power_watts_avg": 31.57, "energy_joules_est": 57.32, "sample_count": 14, "duration_seconds": 1.816}, "timestamp": "2026-01-17T15:18:09.808187"}
{"image_index": 261, "image_name": "000000027696.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000027696.jpg", "image_width": 640, "image_height": 410, "image_resolution": "640x410", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1682.518, "latencies_ms": [1682.518], "images_per_second": 0.594, "prompt_tokens": 23, "response_tokens_est": 37, "n_tiles": 6, "output_text": "1. Pizza\n2. Pork\n3. Mushrooms\n4. Pepperoni\n5. Tomato\n6. Cheese\n7. Sauce\n8. Spices", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 24941.2, "ram_available_mb": 100831.0, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 24942.7, "ram_available_mb": 100829.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.88, "peak": 15.34, "min": 14.43}, "VDD_CPU_SOC_MSS": {"avg": 15.5, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 32.33, "peak": 40.97, "min": 24.43}, "VIN": {"avg": 73.19, "peak": 114.66, "min": 55.21}}, "power_watts_avg": 32.33, "energy_joules_est": 54.41, "sample_count": 13, "duration_seconds": 1.683}, "timestamp": "2026-01-17T15:18:11.496767"}
{"image_index": 261, "image_name": "000000027696.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000027696.jpg", "image_width": 640, "image_height": 410, "image_resolution": "640x410", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2645.158, "latencies_ms": [2645.158], "images_per_second": 0.378, "prompt_tokens": 27, "response_tokens_est": 71, "n_tiles": 6, "output_text": "The main object in the foreground is a freshly baked pizza with toppings such as pepperoni, mushrooms, and red bell peppers. The pizza is placed on a white plate, which is placed on a red and white checkered tablecloth. In the background, there is a fork partially visible, suggesting that someone is about to eat the pizza.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24942.7, "ram_available_mb": 100829.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 24942.9, "ram_available_mb": 100829.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.9, "peak": 15.24, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.49, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 29.01, "peak": 40.56, "min": 22.86}, "VIN": {"avg": 71.24, "peak": 112.19, "min": 61.88}}, "power_watts_avg": 29.01, "energy_joules_est": 76.74, "sample_count": 20, "duration_seconds": 2.645}, "timestamp": "2026-01-17T15:18:14.147767"}
{"image_index": 261, "image_name": "000000027696.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000027696.jpg", "image_width": 640, "image_height": 410, "image_resolution": "640x410", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2074.108, "latencies_ms": [2074.108], "images_per_second": 0.482, "prompt_tokens": 21, "response_tokens_est": 51, "n_tiles": 6, "output_text": "The image shows a freshly baked pizza with a variety of toppings, including green peppers, mushrooms, and possibly some cheese. The pizza is placed on a white plate, and there is a red and white checkered tablecloth in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24942.9, "ram_available_mb": 100829.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 24943.2, "ram_available_mb": 100829.0, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.88, "peak": 15.24, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.5, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 30.41, "peak": 40.18, "min": 23.64}, "VIN": {"avg": 70.51, "peak": 104.98, "min": 59.79}}, "power_watts_avg": 30.41, "energy_joules_est": 63.09, "sample_count": 16, "duration_seconds": 2.075}, "timestamp": "2026-01-17T15:18:16.228192"}
{"image_index": 261, "image_name": "000000027696.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000027696.jpg", "image_width": 640, "image_height": 410, "image_resolution": "640x410", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2226.52, "latencies_ms": [2226.52], "images_per_second": 0.449, "prompt_tokens": 19, "response_tokens_est": 56, "n_tiles": 6, "output_text": "The image showcases a freshly baked pizza with a golden-brown crust, topped with melted cheese, red tomato sauce, green bell peppers, and chunks of mushrooms. The lighting is bright, highlighting the vibrant colors of the pizza and the checkered tablecloth beneath it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24943.2, "ram_available_mb": 100829.0, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 24943.7, "ram_available_mb": 100828.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.88, "peak": 15.24, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.52, "peak": 15.75, "min": 14.56}, "VDD_GPU": {"avg": 29.84, "peak": 40.16, "min": 23.25}, "VIN": {"avg": 73.01, "peak": 118.74, "min": 59.84}}, "power_watts_avg": 29.84, "energy_joules_est": 66.45, "sample_count": 17, "duration_seconds": 2.227}, "timestamp": "2026-01-17T15:18:18.460704"}
{"image_index": 262, "image_name": "000000027768.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000027768.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1748.774, "latencies_ms": [1748.774], "images_per_second": 0.572, "prompt_tokens": 9, "response_tokens_est": 76, "n_tiles": 1, "output_text": "The image depicts a modern city bus, specifically the Metropolitan Transit System, with the route number 5, as indicated on the side of the bus. The bus is white with red accents and features large windows, allowing passengers to view the surroundings. A driver is visible inside the bus, and the bus is parked on a street with a backdrop of modern buildings and trees.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24943.7, "ram_available_mb": 100828.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24943.9, "ram_available_mb": 100828.3, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 4845.7, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.38, "peak": 15.72, "min": 14.51}, "VDD_CPU_SOC_MSS": {"avg": 16.42, "peak": 16.93, "min": 15.36}, "VDD_GPU": {"avg": 21.4, "peak": 25.6, "min": 20.09}, "VIN": {"avg": 63.08, "peak": 66.17, "min": 55.09}}, "power_watts_avg": 21.4, "energy_joules_est": 37.43, "sample_count": 13, "duration_seconds": 1.749}, "timestamp": "2026-01-17T15:18:20.233709"}
{"image_index": 262, "image_name": "000000027768.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000027768.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2896.853, "latencies_ms": [2896.853], "images_per_second": 0.345, "prompt_tokens": 23, "response_tokens_est": 129, "n_tiles": 1, "output_text": "- Bus: 1\n- Bus driver: 1\n- Bus stop sign: 1\n- Bus stop signboard: 1\n- Bus stop signboard (partially visible): 1\n- Bus stop signboard (partially visible): 1\n- Bus stop signboard (partially visible): 1\n- Bus stop signboard (partially visible): 1\n- Bus stop signboard (partially visible): 1\n- Bus stop signboard (partially visible): 1\n- Bus stop signboard (partially visible): 1\n- Bus stop signboard (part", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24943.9, "ram_available_mb": 100828.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24944.9, "ram_available_mb": 100827.3, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 4856.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.64, "peak": 15.93, "min": 15.22}, "VDD_CPU_SOC_MSS": {"avg": 16.68, "peak": 16.94, "min": 16.14}, "VDD_GPU": {"avg": 20.45, "peak": 23.64, "min": 19.69}, "VIN": {"avg": 62.62, "peak": 70.04, "min": 52.07}}, "power_watts_avg": 20.45, "energy_joules_est": 59.25, "sample_count": 22, "duration_seconds": 2.897}, "timestamp": "2026-01-17T15:18:23.136048"}
{"image_index": 262, "image_name": "000000027768.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000027768.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1501.713, "latencies_ms": [1501.713], "images_per_second": 0.666, "prompt_tokens": 27, "response_tokens_est": 65, "n_tiles": 1, "output_text": "The main object in the image is a bus, which is positioned in the foreground. The bus is white with red accents and has a destination sign on its front. The bus is parked on the street, and there are trees in the background. The bus has a driver inside, and there are other vehicles parked nearby.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24944.9, "ram_available_mb": 100827.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24945.6, "ram_available_mb": 100826.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 4859.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.56, "peak": 15.82, "min": 15.22}, "VDD_CPU_SOC_MSS": {"avg": 16.43, "peak": 16.54, "min": 16.14}, "VDD_GPU": {"avg": 21.1, "peak": 23.64, "min": 20.09}, "VIN": {"avg": 62.6, "peak": 64.76, "min": 59.8}}, "power_watts_avg": 21.1, "energy_joules_est": 31.69, "sample_count": 11, "duration_seconds": 1.502}, "timestamp": "2026-01-17T15:18:24.644021"}
{"image_index": 262, "image_name": "000000027768.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000027768.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2429.283, "latencies_ms": [2429.283], "images_per_second": 0.412, "prompt_tokens": 21, "response_tokens_est": 107, "n_tiles": 1, "output_text": "The image depicts a city bus in motion on a city street, with a clear blue sky overhead. The bus is part of the Metropolitan Transit System, as indicated by the text on its side. The bus is white with red accents and has a digital display showing the route number \"D 1\" and the destination \"DONATE.\" The bus is parked on the side of the street, and a person is visible inside the bus, possibly a driver. The surrounding area includes buildings and trees, suggesting an urban setting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24945.6, "ram_available_mb": 100826.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24946.6, "ram_available_mb": 100825.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 4854.7, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.56, "peak": 15.72, "min": 15.22}, "VDD_CPU_SOC_MSS": {"avg": 16.54, "peak": 16.93, "min": 16.14}, "VDD_GPU": {"avg": 20.59, "peak": 24.03, "min": 19.7}, "VIN": {"avg": 63.5, "peak": 69.53, "min": 57.78}}, "power_watts_avg": 20.59, "energy_joules_est": 50.02, "sample_count": 19, "duration_seconds": 2.429}, "timestamp": "2026-01-17T15:18:27.078833"}
{"image_index": 262, "image_name": "000000027768.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000027768.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1125.327, "latencies_ms": [1125.327], "images_per_second": 0.889, "prompt_tokens": 19, "response_tokens_est": 48, "n_tiles": 1, "output_text": "The bus is predominantly white with red accents, featuring a modern design with a large front windshield and a digital display. The bus is parked on a sunny day with clear blue skies and shadows cast on the ground, indicating bright sunlight.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 24946.6, "ram_available_mb": 100825.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24946.9, "ram_available_mb": 100825.3, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 4853.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.39, "peak": 15.62, "min": 15.01}, "VDD_CPU_SOC_MSS": {"avg": 16.44, "peak": 16.54, "min": 16.14}, "VDD_GPU": {"avg": 21.48, "peak": 23.64, "min": 20.1}, "VIN": {"avg": 62.84, "peak": 65.82, "min": 53.18}}, "power_watts_avg": 21.48, "energy_joules_est": 24.18, "sample_count": 8, "duration_seconds": 1.126}, "timestamp": "2026-01-17T15:18:28.209598"}
{"image_index": 263, "image_name": "000000027932.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000027932.jpg", "image_width": 288, "image_height": 307, "image_resolution": "288x307", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 667.84, "latencies_ms": [667.84], "images_per_second": 1.497, "prompt_tokens": 9, "response_tokens_est": 27, "n_tiles": 1, "output_text": "A baseball glove and a baseball cap are resting on a concrete surface, with the cap's logo prominently displayed on the front.", "error": null, "sys_before": {"cpu_percent": 38.9, "ram_used_mb": 24946.9, "ram_available_mb": 100825.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24946.8, "ram_available_mb": 100825.3, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 4845.7, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.34, "peak": 15.52, "min": 15.01}, "VDD_CPU_SOC_MSS": {"avg": 16.38, "peak": 16.54, "min": 16.14}, "VDD_GPU": {"avg": 21.98, "peak": 23.64, "min": 20.88}, "VIN": {"avg": 63.67, "peak": 65.15, "min": 62.02}}, "power_watts_avg": 21.98, "energy_joules_est": 14.69, "sample_count": 5, "duration_seconds": 0.668}, "timestamp": "2026-01-17T15:18:28.894393"}
{"image_index": 263, "image_name": "000000027932.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000027932.jpg", "image_width": 288, "image_height": 307, "image_resolution": "288x307", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 379.035, "latencies_ms": [379.035], "images_per_second": 2.638, "prompt_tokens": 23, "response_tokens_est": 14, "n_tiles": 1, "output_text": "baseball cap: 1\nbaseball glove: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24946.8, "ram_available_mb": 100825.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 24946.8, "ram_available_mb": 100825.3, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 4856.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.22, "peak": 15.32, "min": 15.11}, "VDD_CPU_SOC_MSS": {"avg": 16.14, "peak": 16.14, "min": 16.14}, "VDD_GPU": {"avg": 23.84, "peak": 24.43, "min": 23.25}, "VIN": {"avg": 63.45, "peak": 64.05, "min": 62.84}}, "power_watts_avg": 23.84, "energy_joules_est": 9.05, "sample_count": 2, "duration_seconds": 0.38}, "timestamp": "2026-01-17T15:18:29.279206"}
{"image_index": 263, "image_name": "000000027932.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000027932.jpg", "image_width": 288, "image_height": 307, "image_resolution": "288x307", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1707.657, "latencies_ms": [1707.657], "images_per_second": 0.586, "prompt_tokens": 27, "response_tokens_est": 74, "n_tiles": 1, "output_text": "The main object in the foreground is a baseball glove, which is placed on the ground. The glove is positioned near a baseball cap, which is also on the ground. The baseball cap is positioned near the edge of the image, suggesting it is slightly out of focus. The background is out of focus, indicating that the focus is on the glove and the cap.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24947.1, "ram_available_mb": 100825.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24947.1, "ram_available_mb": 100825.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 4859.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.5, "peak": 15.72, "min": 15.01}, "VDD_CPU_SOC_MSS": {"avg": 16.39, "peak": 16.54, "min": 15.74}, "VDD_GPU": {"avg": 21.37, "peak": 25.61, "min": 19.7}, "VIN": {"avg": 62.71, "peak": 67.1, "min": 60.01}}, "power_watts_avg": 21.37, "energy_joules_est": 36.5, "sample_count": 13, "duration_seconds": 1.708}, "timestamp": "2026-01-17T15:18:30.993014"}
{"image_index": 263, "image_name": "000000027932.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000027932.jpg", "image_width": 288, "image_height": 307, "image_resolution": "288x307", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1061.764, "latencies_ms": [1061.764], "images_per_second": 0.942, "prompt_tokens": 21, "response_tokens_est": 45, "n_tiles": 1, "output_text": "The image depicts a baseball glove and a baseball cap placed on a dirt surface, likely a baseball field. The glove is open, revealing a baseball, and the cap is worn with a white logo on the front.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24947.1, "ram_available_mb": 100825.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24948.1, "ram_available_mb": 100824.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 4854.7, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.49, "peak": 15.62, "min": 15.22}, "VDD_CPU_SOC_MSS": {"avg": 16.44, "peak": 16.54, "min": 16.14}, "VDD_GPU": {"avg": 21.38, "peak": 23.64, "min": 20.1}, "VIN": {"avg": 63.08, "peak": 65.81, "min": 61.77}}, "power_watts_avg": 21.38, "energy_joules_est": 22.71, "sample_count": 8, "duration_seconds": 1.062}, "timestamp": "2026-01-17T15:18:32.060431"}
{"image_index": 263, "image_name": "000000027932.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000027932.jpg", "image_width": 288, "image_height": 307, "image_resolution": "288x307", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1077.623, "latencies_ms": [1077.623], "images_per_second": 0.928, "prompt_tokens": 19, "response_tokens_est": 46, "n_tiles": 1, "output_text": "The image features a dark blue baseball cap with a white logo on the front, resting on a baseball glove. The glove is made of leather with metal components, and the cap is placed on a gravel surface under natural lighting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24948.1, "ram_available_mb": 100824.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24949.0, "ram_available_mb": 100823.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 4853.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.47, "peak": 15.72, "min": 15.11}, "VDD_CPU_SOC_MSS": {"avg": 16.54, "peak": 16.93, "min": 16.14}, "VDD_GPU": {"avg": 21.62, "peak": 24.04, "min": 20.1}, "VIN": {"avg": 63.39, "peak": 64.86, "min": 61.39}}, "power_watts_avg": 21.62, "energy_joules_est": 23.3, "sample_count": 8, "duration_seconds": 1.078}, "timestamp": "2026-01-17T15:18:33.143693"}
{"image_index": 264, "image_name": "000000027972.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000027972.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1142.391, "latencies_ms": [1142.391], "images_per_second": 0.875, "prompt_tokens": 9, "response_tokens_est": 20, "n_tiles": 6, "output_text": "A surfer is riding a wave in the ocean, wearing a red shirt and black shorts.", "error": null, "sys_before": {"cpu_percent": 37.3, "ram_used_mb": 24953.0, "ram_available_mb": 100819.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 24953.8, "ram_available_mb": 100818.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.76, "peak": 15.34, "min": 14.2}, "VDD_CPU_SOC_MSS": {"avg": 15.7, "peak": 16.53, "min": 14.96}, "VDD_GPU": {"avg": 34.12, "peak": 39.38, "min": 28.76}, "VIN": {"avg": 81.89, "peak": 110.69, "min": 62.61}}, "power_watts_avg": 34.12, "energy_joules_est": 38.99, "sample_count": 8, "duration_seconds": 1.143}, "timestamp": "2026-01-17T15:18:34.346984"}
{"image_index": 264, "image_name": "000000027972.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000027972.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1295.063, "latencies_ms": [1295.063], "images_per_second": 0.772, "prompt_tokens": 23, "response_tokens_est": 24, "n_tiles": 6, "output_text": "surfboard: 1\nsurfer: 1\nwater: 1\nocean: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24953.8, "ram_available_mb": 100818.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 24953.8, "ram_available_mb": 100818.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.92, "peak": 15.44, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.53, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 36.02, "peak": 40.97, "min": 28.36}, "VIN": {"avg": 75.57, "peak": 107.83, "min": 62.6}}, "power_watts_avg": 36.02, "energy_joules_est": 46.66, "sample_count": 9, "duration_seconds": 1.295}, "timestamp": "2026-01-17T15:18:35.648706"}
{"image_index": 264, "image_name": "000000027972.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000027972.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2219.438, "latencies_ms": [2219.438], "images_per_second": 0.451, "prompt_tokens": 27, "response_tokens_est": 56, "n_tiles": 6, "output_text": "The main object in the foreground is a surfer riding a wave. The surfer is positioned on a surfboard, which is partially submerged in the water. The background features a large, frothy wave, indicating the surfer's position in relation to the wave.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24953.8, "ram_available_mb": 100818.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 24954.0, "ram_available_mb": 100818.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.9, "peak": 15.34, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.33, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 30.49, "peak": 41.34, "min": 23.25}, "VIN": {"avg": 69.96, "peak": 103.57, "min": 61.29}}, "power_watts_avg": 30.49, "energy_joules_est": 67.68, "sample_count": 17, "duration_seconds": 2.22}, "timestamp": "2026-01-17T15:18:37.874184"}
{"image_index": 264, "image_name": "000000027972.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000027972.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2696.013, "latencies_ms": [2696.013], "images_per_second": 0.371, "prompt_tokens": 21, "response_tokens_est": 73, "n_tiles": 6, "output_text": "The image captures a surfer riding a wave in a vibrant blue ocean. The surfer, dressed in a red shirt and black wetsuit, is skillfully maneuvering a white surfboard with yellow and green accents. The scene is set in a dynamic and energetic environment, showcasing the thrill and excitement of surfing in a natural, aquatic setting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24954.0, "ram_available_mb": 100818.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 24954.8, "ram_available_mb": 100817.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.88, "peak": 15.24, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.45, "peak": 15.75, "min": 14.56}, "VDD_GPU": {"avg": 28.62, "peak": 40.16, "min": 22.85}, "VIN": {"avg": 70.69, "peak": 107.78, "min": 58.6}}, "power_watts_avg": 28.62, "energy_joules_est": 77.17, "sample_count": 21, "duration_seconds": 2.696}, "timestamp": "2026-01-17T15:18:40.576239"}
{"image_index": 264, "image_name": "000000027972.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000027972.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1936.254, "latencies_ms": [1936.254], "images_per_second": 0.516, "prompt_tokens": 19, "response_tokens_est": 46, "n_tiles": 6, "output_text": "The image depicts a surfer riding a wave in a vibrant turquoise ocean. The surfer is wearing a red wetsuit, and the wave is breaking with white foam, indicating a sunny and windy day.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24954.8, "ram_available_mb": 100817.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 24954.8, "ram_available_mb": 100817.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.83, "peak": 15.14, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.3, "peak": 15.74, "min": 14.57}, "VDD_GPU": {"avg": 30.96, "peak": 40.95, "min": 23.64}, "VIN": {"avg": 70.43, "peak": 115.21, "min": 55.02}}, "power_watts_avg": 30.96, "energy_joules_est": 59.96, "sample_count": 15, "duration_seconds": 1.937}, "timestamp": "2026-01-17T15:18:42.518679"}
{"image_index": 265, "image_name": "000000027982.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000027982.jpg", "image_width": 500, "image_height": 334, "image_resolution": "500x334", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1681.12, "latencies_ms": [1681.12], "images_per_second": 0.595, "prompt_tokens": 9, "response_tokens_est": 40, "n_tiles": 6, "output_text": "The image shows a black and white photograph of a bathroom, featuring a toilet with its lid open, a toilet paper roll on the floor, and a small trash can with a brush inside it.", "error": null, "sys_before": {"cpu_percent": 44.1, "ram_used_mb": 24954.8, "ram_available_mb": 100817.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 24955.0, "ram_available_mb": 100817.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.8, "peak": 15.14, "min": 13.9}, "VDD_CPU_SOC_MSS": {"avg": 15.47, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 31.79, "peak": 40.18, "min": 24.83}, "VIN": {"avg": 74.84, "peak": 122.72, "min": 60.43}}, "power_watts_avg": 31.79, "energy_joules_est": 53.46, "sample_count": 13, "duration_seconds": 1.682}, "timestamp": "2026-01-17T15:18:44.243533"}
{"image_index": 265, "image_name": "000000027982.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000027982.jpg", "image_width": 500, "image_height": 334, "image_resolution": "500x334", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1643.588, "latencies_ms": [1643.588], "images_per_second": 0.608, "prompt_tokens": 23, "response_tokens_est": 37, "n_tiles": 6, "output_text": "toilet paper: 8\ntoilet brush: 1\ntoilet paper roll: 1\ntoilet paper bag: 1\ntoilet paper: 8", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24955.0, "ram_available_mb": 100817.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 24954.7, "ram_available_mb": 100817.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.98, "peak": 15.44, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.58, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 33.08, "peak": 40.95, "min": 25.6}, "VIN": {"avg": 75.12, "peak": 105.38, "min": 63.55}}, "power_watts_avg": 33.08, "energy_joules_est": 54.38, "sample_count": 12, "duration_seconds": 1.644}, "timestamp": "2026-01-17T15:18:45.893853"}
{"image_index": 265, "image_name": "000000027982.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000027982.jpg", "image_width": 500, "image_height": 334, "image_resolution": "500x334", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2355.833, "latencies_ms": [2355.833], "images_per_second": 0.424, "prompt_tokens": 27, "response_tokens_est": 61, "n_tiles": 6, "output_text": "The toilet is positioned in the foreground, with its lid open and seat down. To the right of the toilet, there is a stack of toilet paper rolls. The background features a wall with a patterned tile design, and a small, dark object is visible on the right side of the image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24954.7, "ram_available_mb": 100817.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 24954.7, "ram_available_mb": 100817.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.92, "peak": 15.34, "min": 14.43}, "VDD_CPU_SOC_MSS": {"avg": 15.49, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 29.94, "peak": 40.56, "min": 23.25}, "VIN": {"avg": 71.58, "peak": 106.88, "min": 55.54}}, "power_watts_avg": 29.94, "energy_joules_est": 70.54, "sample_count": 18, "duration_seconds": 2.356}, "timestamp": "2026-01-17T15:18:48.255335"}
{"image_index": 265, "image_name": "000000027982.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000027982.jpg", "image_width": 500, "image_height": 334, "image_resolution": "500x334", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2470.928, "latencies_ms": [2470.928], "images_per_second": 0.405, "prompt_tokens": 21, "response_tokens_est": 65, "n_tiles": 6, "output_text": "The image depicts a bathroom setting with a white toilet positioned against a tiled wall. The toilet is accompanied by a toilet brush and a roll of toilet paper, indicating that the scene is likely in a bathroom. The overall setting appears to be a typical bathroom environment with a focus on the toilet and its associated items.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24954.7, "ram_available_mb": 100817.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 24955.0, "ram_available_mb": 100817.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.87, "peak": 15.24, "min": 14.3}, "VDD_CPU_SOC_MSS": {"avg": 15.38, "peak": 15.75, "min": 14.57}, "VDD_GPU": {"avg": 29.17, "peak": 40.56, "min": 22.86}, "VIN": {"avg": 71.75, "peak": 114.15, "min": 62.29}}, "power_watts_avg": 29.17, "energy_joules_est": 72.09, "sample_count": 19, "duration_seconds": 2.471}, "timestamp": "2026-01-17T15:18:50.732351"}
{"image_index": 265, "image_name": "000000027982.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000027982.jpg", "image_width": 500, "image_height": 334, "image_resolution": "500x334", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1879.88, "latencies_ms": [1879.88], "images_per_second": 0.532, "prompt_tokens": 19, "response_tokens_est": 44, "n_tiles": 6, "output_text": "The image is a black and white photograph of a bathroom. The lighting is dim, casting shadows on the floor and walls. The materials used include tiles for the walls and floor, a toilet, and toilet paper.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24955.0, "ram_available_mb": 100817.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 24955.2, "ram_available_mb": 100817.0, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.82, "peak": 15.24, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.41, "peak": 16.14, "min": 14.57}, "VDD_GPU": {"avg": 31.12, "peak": 40.16, "min": 24.03}, "VIN": {"avg": 72.98, "peak": 115.95, "min": 61.42}}, "power_watts_avg": 31.12, "energy_joules_est": 58.52, "sample_count": 14, "duration_seconds": 1.88}, "timestamp": "2026-01-17T15:18:52.618272"}
{"image_index": 266, "image_name": "000000028285.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000028285.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1598.77, "latencies_ms": [1598.77], "images_per_second": 0.625, "prompt_tokens": 9, "response_tokens_est": 37, "n_tiles": 6, "output_text": "The image features a white clock tower with a blue dome, set against a backdrop of a clear blue sky and lush green trees, with a tiled roof visible in the foreground.", "error": null, "sys_before": {"cpu_percent": 34.4, "ram_used_mb": 24955.2, "ram_available_mb": 100817.0, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 24955.2, "ram_available_mb": 100817.0, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.73, "peak": 15.14, "min": 13.79}, "VDD_CPU_SOC_MSS": {"avg": 15.48, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 32.27, "peak": 40.16, "min": 25.21}, "VIN": {"avg": 76.08, "peak": 109.18, "min": 56.55}}, "power_watts_avg": 32.27, "energy_joules_est": 51.61, "sample_count": 12, "duration_seconds": 1.599}, "timestamp": "2026-01-17T15:18:54.266784"}
{"image_index": 266, "image_name": "000000028285.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000028285.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1727.902, "latencies_ms": [1727.902], "images_per_second": 0.579, "prompt_tokens": 23, "response_tokens_est": 40, "n_tiles": 6, "output_text": "object: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 24955.2, "ram_available_mb": 100817.0, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 24955.5, "ram_available_mb": 100816.7, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.0, "peak": 15.44, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.66, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 32.39, "peak": 40.57, "min": 24.83}, "VIN": {"avg": 74.89, "peak": 101.19, "min": 57.78}}, "power_watts_avg": 32.39, "energy_joules_est": 55.98, "sample_count": 13, "duration_seconds": 1.728}, "timestamp": "2026-01-17T15:18:56.000603"}
{"image_index": 266, "image_name": "000000028285.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000028285.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2408.391, "latencies_ms": [2408.391], "images_per_second": 0.415, "prompt_tokens": 27, "response_tokens_est": 65, "n_tiles": 6, "output_text": "The main objects in the image are a white clock tower and a tiled roof. The clock tower is positioned in the background, while the tiled roof is in the foreground. The clock tower is near the top of the image, and the roof is at the bottom, creating a clear spatial relationship between the two.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24955.5, "ram_available_mb": 100816.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 24955.2, "ram_available_mb": 100817.0, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.06, "peak": 15.44, "min": 14.43}, "VDD_CPU_SOC_MSS": {"avg": 15.67, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 29.82, "peak": 40.57, "min": 23.64}, "VIN": {"avg": 72.14, "peak": 116.29, "min": 56.3}}, "power_watts_avg": 29.82, "energy_joules_est": 71.83, "sample_count": 19, "duration_seconds": 2.409}, "timestamp": "2026-01-17T15:18:58.414777"}
{"image_index": 266, "image_name": "000000028285.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000028285.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1904.931, "latencies_ms": [1904.931], "images_per_second": 0.525, "prompt_tokens": 21, "response_tokens_est": 46, "n_tiles": 6, "output_text": "The image depicts a quaint, historic building with a white tower and a clock, set against a clear blue sky. The building is surrounded by lush greenery, and the roof is tiled in a traditional style.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24955.2, "ram_available_mb": 100817.0, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 24955.2, "ram_available_mb": 100817.0, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.01, "peak": 15.44, "min": 14.43}, "VDD_CPU_SOC_MSS": {"avg": 15.63, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 31.57, "peak": 40.56, "min": 24.04}, "VIN": {"avg": 73.5, "peak": 114.37, "min": 48.19}}, "power_watts_avg": 31.57, "energy_joules_est": 60.15, "sample_count": 14, "duration_seconds": 1.905}, "timestamp": "2026-01-17T15:19:00.329920"}
{"image_index": 266, "image_name": "000000028285.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000028285.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1986.324, "latencies_ms": [1986.324], "images_per_second": 0.503, "prompt_tokens": 19, "response_tokens_est": 48, "n_tiles": 6, "output_text": "The image features a white clock tower with a blue dome, set against a clear blue sky. The tower is adorned with intricate metalwork and has a tiled roof. The lighting is bright and natural, suggesting it is daytime.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24955.2, "ram_available_mb": 100817.0, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 24955.4, "ram_available_mb": 100816.7, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.87, "peak": 15.24, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.46, "peak": 15.75, "min": 14.56}, "VDD_GPU": {"avg": 31.07, "peak": 40.56, "min": 23.64}, "VIN": {"avg": 75.45, "peak": 116.16, "min": 63.19}}, "power_watts_avg": 31.07, "energy_joules_est": 61.73, "sample_count": 15, "duration_seconds": 1.987}, "timestamp": "2026-01-17T15:19:02.322474"}
{"image_index": 267, "image_name": "000000028449.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000028449.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1437.292, "latencies_ms": [1437.292], "images_per_second": 0.696, "prompt_tokens": 9, "response_tokens_est": 31, "n_tiles": 6, "output_text": "The image depicts a group of elephants in a natural, outdoor setting, with one elephant in the foreground prominently displaying its trunk and tusks.", "error": null, "sys_before": {"cpu_percent": 46.2, "ram_used_mb": 24955.4, "ram_available_mb": 100816.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 24955.4, "ram_available_mb": 100816.7, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.69, "peak": 15.14, "min": 13.79}, "VDD_CPU_SOC_MSS": {"avg": 15.46, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 32.91, "peak": 40.16, "min": 26.01}, "VIN": {"avg": 79.91, "peak": 120.0, "min": 62.51}}, "power_watts_avg": 32.91, "energy_joules_est": 47.31, "sample_count": 11, "duration_seconds": 1.438}, "timestamp": "2026-01-17T15:19:03.820065"}
{"image_index": 267, "image_name": "000000028449.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000028449.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1147.308, "latencies_ms": [1147.308], "images_per_second": 0.872, "prompt_tokens": 23, "response_tokens_est": 18, "n_tiles": 6, "output_text": "elephant: 3\nbushes: 1\ntree: 1", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24955.4, "ram_available_mb": 100816.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 24955.7, "ram_available_mb": 100816.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.77, "peak": 15.24, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.3, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 36.54, "peak": 40.97, "min": 30.33}, "VIN": {"avg": 78.39, "peak": 111.07, "min": 54.4}}, "power_watts_avg": 36.54, "energy_joules_est": 41.93, "sample_count": 8, "duration_seconds": 1.148}, "timestamp": "2026-01-17T15:19:04.977298"}
{"image_index": 267, "image_name": "000000028449.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000028449.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2336.862, "latencies_ms": [2336.862], "images_per_second": 0.428, "prompt_tokens": 27, "response_tokens_est": 60, "n_tiles": 6, "output_text": "The main object in the foreground is an elephant, which is positioned near the center of the image. The background features several other elephants, with some partially obscured by the foreground elephant. The elephants are situated in a natural environment with greenery and dirt, suggesting a savannah or forest habitat.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24955.7, "ram_available_mb": 100816.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 24956.2, "ram_available_mb": 100816.0, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.84, "peak": 15.34, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.53, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 30.25, "peak": 41.76, "min": 23.24}, "VIN": {"avg": 70.12, "peak": 110.28, "min": 58.57}}, "power_watts_avg": 30.25, "energy_joules_est": 70.7, "sample_count": 18, "duration_seconds": 2.337}, "timestamp": "2026-01-17T15:19:07.320307"}
{"image_index": 267, "image_name": "000000028449.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000028449.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2446.157, "latencies_ms": [2446.157], "images_per_second": 0.409, "prompt_tokens": 21, "response_tokens_est": 65, "n_tiles": 6, "output_text": "The image depicts a group of elephants in a natural, outdoor setting. The elephants are standing on a dirt path surrounded by greenery, with some of them appearing to be in the process of moving or interacting with each other. The scene suggests a peaceful and natural environment, likely in a savanna or forest area.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24956.2, "ram_available_mb": 100816.0, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 24956.2, "ram_available_mb": 100816.0, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.94, "peak": 15.24, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.58, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 29.27, "peak": 40.16, "min": 23.24}, "VIN": {"avg": 69.45, "peak": 92.76, "min": 56.08}}, "power_watts_avg": 29.27, "energy_joules_est": 71.61, "sample_count": 19, "duration_seconds": 2.447}, "timestamp": "2026-01-17T15:19:09.772824"}
{"image_index": 267, "image_name": "000000028449.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000028449.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2029.282, "latencies_ms": [2029.282], "images_per_second": 0.493, "prompt_tokens": 19, "response_tokens_est": 50, "n_tiles": 6, "output_text": "The image depicts a group of elephants in a natural, outdoor setting. The elephants are predominantly gray in color, with some having darker patches. The lighting is natural, suggesting it is daytime, and the weather appears to be clear and sunny.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24956.2, "ram_available_mb": 100816.0, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 24956.4, "ram_available_mb": 100815.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.91, "peak": 15.24, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.56, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 30.96, "peak": 40.18, "min": 23.64}, "VIN": {"avg": 75.66, "peak": 111.77, "min": 62.09}}, "power_watts_avg": 30.96, "energy_joules_est": 62.84, "sample_count": 15, "duration_seconds": 2.03}, "timestamp": "2026-01-17T15:19:11.808407"}
{"image_index": 268, "image_name": "000000028452.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000028452.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2728.848, "latencies_ms": [2728.848], "images_per_second": 0.366, "prompt_tokens": 9, "response_tokens_est": 33, "n_tiles": 12, "output_text": "The image shows a refrigerator with a partially open door, revealing a small section of the interior, and a toilet with its lid open, revealing a toilet bowl.", "error": null, "sys_before": {"cpu_percent": 44.0, "ram_used_mb": 24917.9, "ram_available_mb": 100854.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 24919.4, "ram_available_mb": 100852.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.2, "peak": 16.66, "min": 13.59}, "VDD_CPU_SOC_MSS": {"avg": 15.95, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 35.4, "peak": 43.73, "min": 27.18}, "VIN": {"avg": 78.39, "peak": 108.64, "min": 60.89}}, "power_watts_avg": 35.4, "energy_joules_est": 96.61, "sample_count": 21, "duration_seconds": 2.729}, "timestamp": "2026-01-17T15:19:14.624350"}
{"image_index": 268, "image_name": "000000028452.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000028452.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 3567.933, "latencies_ms": [3567.933], "images_per_second": 0.28, "prompt_tokens": 23, "response_tokens_est": 56, "n_tiles": 12, "output_text": "- Refrigerator: 1\n- Water bottle: 1\n- Bowl: 1\n- Bowl holder: 1\n- Dish rack: 1\n- Dishware: 1\n- Dish: 1\n- Dish rack: 1", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24919.4, "ram_available_mb": 100852.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 24920.6, "ram_available_mb": 100851.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.21, "peak": 16.56, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.7, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 34.17, "peak": 45.68, "min": 26.0}, "VIN": {"avg": 74.85, "peak": 104.53, "min": 58.83}}, "power_watts_avg": 34.17, "energy_joules_est": 121.93, "sample_count": 28, "duration_seconds": 3.568}, "timestamp": "2026-01-17T15:19:18.198673"}
{"image_index": 268, "image_name": "000000028452.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000028452.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3523.892, "latencies_ms": [3523.892], "images_per_second": 0.284, "prompt_tokens": 27, "response_tokens_est": 55, "n_tiles": 12, "output_text": "The refrigerator is located in the foreground, with its door open, revealing its contents. The sink is situated to the right of the refrigerator, with a bottle of liquid placed on the countertop. The toilet is in the background, slightly to the right of the sink.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24920.6, "ram_available_mb": 100851.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 24921.4, "ram_available_mb": 100850.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.22, "peak": 16.56, "min": 14.12}, "VDD_CPU_SOC_MSS": {"avg": 15.8, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 34.14, "peak": 45.68, "min": 26.0}, "VIN": {"avg": 77.43, "peak": 110.05, "min": 60.66}}, "power_watts_avg": 34.14, "energy_joules_est": 120.33, "sample_count": 28, "duration_seconds": 3.525}, "timestamp": "2026-01-17T15:19:21.731742"}
{"image_index": 268, "image_name": "000000028452.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000028452.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3396.285, "latencies_ms": [3396.285], "images_per_second": 0.294, "prompt_tokens": 21, "response_tokens_est": 51, "n_tiles": 12, "output_text": "The image shows a small, well-lit kitchen area with a refrigerator and a toilet. The refrigerator is open, revealing its contents, while the toilet is closed. The setting appears to be a domestic environment, possibly a kitchen or a small bathroom.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24921.4, "ram_available_mb": 100850.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 24922.4, "ram_available_mb": 100849.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.22, "peak": 16.56, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.72, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 34.34, "peak": 45.28, "min": 26.0}, "VIN": {"avg": 76.75, "peak": 119.31, "min": 55.35}}, "power_watts_avg": 34.34, "energy_joules_est": 116.64, "sample_count": 27, "duration_seconds": 3.397}, "timestamp": "2026-01-17T15:19:25.134530"}
{"image_index": 268, "image_name": "000000028452.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000028452.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3297.378, "latencies_ms": [3297.378], "images_per_second": 0.303, "prompt_tokens": 19, "response_tokens_est": 48, "n_tiles": 12, "output_text": "The image shows a white refrigerator with a glass door, illuminated by a warm, soft light. The refrigerator is placed on a tiled floor, and the overall scene appears to be indoors, possibly in a kitchen or dining area.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24922.4, "ram_available_mb": 100849.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 24924.1, "ram_available_mb": 100848.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.21, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.82, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 34.65, "peak": 44.91, "min": 26.0}, "VIN": {"avg": 78.79, "peak": 108.43, "min": 61.53}}, "power_watts_avg": 34.65, "energy_joules_est": 114.27, "sample_count": 26, "duration_seconds": 3.298}, "timestamp": "2026-01-17T15:19:28.438242"}
{"image_index": 269, "image_name": "000000028809.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000028809.jpg", "image_width": 498, "image_height": 500, "image_resolution": "498x500", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 871.213, "latencies_ms": [871.213], "images_per_second": 1.148, "prompt_tokens": 9, "response_tokens_est": 36, "n_tiles": 1, "output_text": "The image shows a close-up view of a bunch of ripe, yellow bananas arranged on a surface, with a blurred background that suggests a bright, natural light environment.", "error": null, "sys_before": {"cpu_percent": 22.2, "ram_used_mb": 24924.1, "ram_available_mb": 100848.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 24924.1, "ram_available_mb": 100848.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 4845.7, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.03, "peak": 15.42, "min": 14.51}, "VDD_CPU_SOC_MSS": {"avg": 15.82, "peak": 16.54, "min": 14.96}, "VDD_GPU": {"avg": 23.7, "peak": 27.18, "min": 21.28}, "VIN": {"avg": 63.73, "peak": 69.17, "min": 56.88}}, "power_watts_avg": 23.7, "energy_joules_est": 20.67, "sample_count": 6, "duration_seconds": 0.872}, "timestamp": "2026-01-17T15:19:29.334821"}
{"image_index": 269, "image_name": "000000028809.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000028809.jpg", "image_width": 498, "image_height": 500, "image_resolution": "498x500", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 222.733, "latencies_ms": [222.733], "images_per_second": 4.49, "prompt_tokens": 23, "response_tokens_est": 7, "n_tiles": 1, "output_text": "- Bananas: 8", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24924.1, "ram_available_mb": 100848.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 24924.3, "ram_available_mb": 100847.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 4856.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.11, "peak": 15.11, "min": 15.11}, "VDD_CPU_SOC_MSS": {"avg": 16.14, "peak": 16.14, "min": 16.14}, "VDD_GPU": {"avg": 24.82, "peak": 24.82, "min": 24.82}, "VIN": {"avg": 60.25, "peak": 60.25, "min": 60.25}}, "power_watts_avg": 24.82, "energy_joules_est": 5.54, "sample_count": 1, "duration_seconds": 0.223}, "timestamp": "2026-01-17T15:19:29.563343"}
{"image_index": 269, "image_name": "000000028809.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000028809.jpg", "image_width": 498, "image_height": 500, "image_resolution": "498x500", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1237.419, "latencies_ms": [1237.419], "images_per_second": 0.808, "prompt_tokens": 27, "response_tokens_est": 54, "n_tiles": 1, "output_text": "The main objects in the image are bananas, which are located in the foreground. The bananas are arranged in a row, with the closest ones to the viewer. The background is blurred, indicating that the focus is on the bananas in the foreground.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24924.3, "ram_available_mb": 100847.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24924.3, "ram_available_mb": 100847.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 4859.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.42, "peak": 15.62, "min": 14.91}, "VDD_CPU_SOC_MSS": {"avg": 16.45, "peak": 16.93, "min": 16.14}, "VDD_GPU": {"avg": 22.8, "peak": 27.17, "min": 20.49}, "VIN": {"avg": 63.96, "peak": 67.95, "min": 60.85}}, "power_watts_avg": 22.8, "energy_joules_est": 28.22, "sample_count": 9, "duration_seconds": 1.238}, "timestamp": "2026-01-17T15:19:30.806542"}
{"image_index": 269, "image_name": "000000028809.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000028809.jpg", "image_width": 498, "image_height": 500, "image_resolution": "498x500", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1331.337, "latencies_ms": [1331.337], "images_per_second": 0.751, "prompt_tokens": 21, "response_tokens_est": 57, "n_tiles": 1, "output_text": "The image shows a close-up view of a bunch of ripe bananas on a shelf. The bananas are yellow and appear fresh, with some having visible labels attached to them. The setting is likely a grocery store or a market, as the bananas are displayed for sale.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24924.3, "ram_available_mb": 100847.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24924.5, "ram_available_mb": 100847.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 4854.7, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.56, "peak": 15.72, "min": 15.11}, "VDD_CPU_SOC_MSS": {"avg": 16.5, "peak": 16.54, "min": 16.14}, "VDD_GPU": {"avg": 21.43, "peak": 24.42, "min": 20.09}, "VIN": {"avg": 62.14, "peak": 66.24, "min": 51.06}}, "power_watts_avg": 21.43, "energy_joules_est": 28.54, "sample_count": 10, "duration_seconds": 1.332}, "timestamp": "2026-01-17T15:19:32.143918"}
{"image_index": 269, "image_name": "000000028809.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000028809.jpg", "image_width": 498, "image_height": 500, "image_resolution": "498x500", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1106.771, "latencies_ms": [1106.771], "images_per_second": 0.904, "prompt_tokens": 19, "response_tokens_est": 47, "n_tiles": 1, "output_text": "The image features a bunch of ripe, yellow bananas with a slightly blurred background, suggesting a shallow depth of field. The lighting is soft and diffused, casting gentle shadows and highlighting the smooth texture of the bananas.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24924.5, "ram_available_mb": 100847.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24925.0, "ram_available_mb": 100847.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 4853.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.51, "peak": 15.72, "min": 15.22}, "VDD_CPU_SOC_MSS": {"avg": 16.49, "peak": 16.93, "min": 16.13}, "VDD_GPU": {"avg": 21.53, "peak": 24.04, "min": 20.09}, "VIN": {"avg": 63.99, "peak": 69.71, "min": 57.6}}, "power_watts_avg": 21.53, "energy_joules_est": 23.83, "sample_count": 8, "duration_seconds": 1.107}, "timestamp": "2026-01-17T15:19:33.256491"}
{"image_index": 270, "image_name": "000000028993.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000028993.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1065.117, "latencies_ms": [1065.117], "images_per_second": 0.939, "prompt_tokens": 9, "response_tokens_est": 46, "n_tiles": 1, "output_text": "The image depicts a row of rusty, cylindrical fire hydrants standing on a paved surface, with a backdrop of a cityscape featuring modern high-rise buildings and a mix of urban and natural elements.", "error": null, "sys_before": {"cpu_percent": 30.8, "ram_used_mb": 24925.3, "ram_available_mb": 100846.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24925.5, "ram_available_mb": 100846.7, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 4845.7, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.62, "peak": 15.82, "min": 15.22}, "VDD_CPU_SOC_MSS": {"avg": 16.53, "peak": 16.93, "min": 16.14}, "VDD_GPU": {"avg": 21.62, "peak": 23.64, "min": 20.49}, "VIN": {"avg": 62.52, "peak": 65.02, "min": 61.32}}, "power_watts_avg": 21.62, "energy_joules_est": 23.03, "sample_count": 8, "duration_seconds": 1.065}, "timestamp": "2026-01-17T15:19:34.344601"}
{"image_index": 270, "image_name": "000000028993.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000028993.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1044.569, "latencies_ms": [1044.569], "images_per_second": 0.957, "prompt_tokens": 23, "response_tokens_est": 45, "n_tiles": 1, "output_text": "object: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 24925.5, "ram_available_mb": 100846.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24926.5, "ram_available_mb": 100845.7, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 4856.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.56, "peak": 15.72, "min": 15.32}, "VDD_CPU_SOC_MSS": {"avg": 16.59, "peak": 16.93, "min": 16.14}, "VDD_GPU": {"avg": 21.95, "peak": 24.42, "min": 20.48}, "VIN": {"avg": 63.5, "peak": 70.04, "min": 60.55}}, "power_watts_avg": 21.95, "energy_joules_est": 22.94, "sample_count": 7, "duration_seconds": 1.045}, "timestamp": "2026-01-17T15:19:35.394863"}
{"image_index": 270, "image_name": "000000028993.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000028993.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1290.386, "latencies_ms": [1290.386], "images_per_second": 0.775, "prompt_tokens": 27, "response_tokens_est": 56, "n_tiles": 1, "output_text": "The main objects in the image are three rusty, cylindrical objects placed in the foreground. They are positioned near a large, ornate building in the background. The building is situated on a paved area with a few trees and other buildings visible in the distance.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24926.5, "ram_available_mb": 100845.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 24926.5, "ram_available_mb": 100845.7, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 4859.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.65, "peak": 15.82, "min": 15.32}, "VDD_CPU_SOC_MSS": {"avg": 16.49, "peak": 16.54, "min": 16.14}, "VDD_GPU": {"avg": 21.52, "peak": 24.43, "min": 20.1}, "VIN": {"avg": 63.76, "peak": 68.66, "min": 60.14}}, "power_watts_avg": 21.52, "energy_joules_est": 27.78, "sample_count": 10, "duration_seconds": 1.291}, "timestamp": "2026-01-17T15:19:36.690408"}
{"image_index": 270, "image_name": "000000028993.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000028993.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1726.758, "latencies_ms": [1726.758], "images_per_second": 0.579, "prompt_tokens": 21, "response_tokens_est": 75, "n_tiles": 1, "output_text": "The image depicts a cityscape with a row of rusty, cylindrical fire hydrants standing on a paved area. In the background, there are tall buildings and a few trees, suggesting an urban environment. The scene appears to be a public space, possibly a plaza or a city square, with the fire hydrants serving as a focal point.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24926.5, "ram_available_mb": 100845.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24927.7, "ram_available_mb": 100844.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 4854.7, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.61, "peak": 15.82, "min": 15.22}, "VDD_CPU_SOC_MSS": {"avg": 16.48, "peak": 16.54, "min": 16.14}, "VDD_GPU": {"avg": 20.91, "peak": 24.03, "min": 19.7}, "VIN": {"avg": 63.89, "peak": 67.84, "min": 56.99}}, "power_watts_avg": 20.91, "energy_joules_est": 36.12, "sample_count": 13, "duration_seconds": 1.727}, "timestamp": "2026-01-17T15:19:38.423120"}
{"image_index": 270, "image_name": "000000028993.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000028993.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1300.443, "latencies_ms": [1300.443], "images_per_second": 0.769, "prompt_tokens": 19, "response_tokens_est": 56, "n_tiles": 1, "output_text": "The image features a row of rust-colored, cylindrical objects, likely fire hydrants, set against a backdrop of a cityscape with modern high-rise buildings. The scene is illuminated by natural daylight, casting soft shadows and highlighting the textures of the metal surfaces.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24927.7, "ram_available_mb": 100844.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24927.7, "ram_available_mb": 100844.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 4853.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.54, "peak": 15.72, "min": 15.32}, "VDD_CPU_SOC_MSS": {"avg": 16.46, "peak": 16.54, "min": 16.14}, "VDD_GPU": {"avg": 21.24, "peak": 24.04, "min": 20.09}, "VIN": {"avg": 62.24, "peak": 64.17, "min": 57.96}}, "power_watts_avg": 21.24, "energy_joules_est": 27.63, "sample_count": 10, "duration_seconds": 1.301}, "timestamp": "2026-01-17T15:19:39.729247"}
{"image_index": 271, "image_name": "000000029187.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000029187.jpg", "image_width": 640, "image_height": 491, "image_resolution": "640x491", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2323.041, "latencies_ms": [2323.041], "images_per_second": 0.43, "prompt_tokens": 9, "response_tokens_est": 21, "n_tiles": 12, "output_text": "A jockey is riding a horse in a race, with the number 8 on the saddle.", "error": null, "sys_before": {"cpu_percent": 49.2, "ram_used_mb": 24943.8, "ram_available_mb": 100828.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.4, "ram_used_mb": 24936.6, "ram_available_mb": 100835.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.27, "peak": 16.66, "min": 13.9}, "VDD_CPU_SOC_MSS": {"avg": 16.09, "peak": 17.71, "min": 14.96}, "VDD_GPU": {"avg": 36.17, "peak": 43.31, "min": 27.57}, "VIN": {"avg": 83.57, "peak": 124.96, "min": 61.07}}, "power_watts_avg": 36.17, "energy_joules_est": 84.04, "sample_count": 18, "duration_seconds": 2.324}, "timestamp": "2026-01-17T15:19:42.146449"}
{"image_index": 271, "image_name": "000000029187.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000029187.jpg", "image_width": 640, "image_height": 491, "image_resolution": "640x491", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 6011.637, "latencies_ms": [6011.637], "images_per_second": 0.166, "prompt_tokens": 23, "response_tokens_est": 128, "n_tiles": 12, "output_text": "object: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject:", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24936.6, "ram_available_mb": 100835.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 24931.2, "ram_available_mb": 100841.0, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.17, "peak": 16.56, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.56, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 30.99, "peak": 46.07, "min": 26.0}, "VIN": {"avg": 71.72, "peak": 110.39, "min": 45.54}}, "power_watts_avg": 30.99, "energy_joules_est": 186.32, "sample_count": 47, "duration_seconds": 6.012}, "timestamp": "2026-01-17T15:19:48.164536"}
{"image_index": 271, "image_name": "000000029187.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000029187.jpg", "image_width": 640, "image_height": 491, "image_resolution": "640x491", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 4490.283, "latencies_ms": [4490.283], "images_per_second": 0.223, "prompt_tokens": 27, "response_tokens_est": 83, "n_tiles": 12, "output_text": "The main object in the foreground is a brown horse with a white blaze on its face, wearing a bridle and a saddle. The number \"8\" is attached to the saddle. The background features a dirt track with a yellow and blue sign that reads \"Magnum\" and \"www.magnum.co.nz.\" There is also a white fence and a person wearing a helmet and a yellow jacket.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24931.2, "ram_available_mb": 100841.0, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 24931.9, "ram_available_mb": 100840.3, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.23, "peak": 16.66, "min": 14.12}, "VDD_CPU_SOC_MSS": {"avg": 15.79, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 32.48, "peak": 44.89, "min": 26.0}, "VIN": {"avg": 76.28, "peak": 117.15, "min": 61.14}}, "power_watts_avg": 32.48, "energy_joules_est": 145.85, "sample_count": 35, "duration_seconds": 4.491}, "timestamp": "2026-01-17T15:19:52.660779"}
{"image_index": 271, "image_name": "000000029187.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000029187.jpg", "image_width": 640, "image_height": 491, "image_resolution": "640x491", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3312.304, "latencies_ms": [3312.304], "images_per_second": 0.302, "prompt_tokens": 21, "response_tokens_est": 49, "n_tiles": 12, "output_text": "The image depicts a horse race track with a jockey in a yellow jacket and helmet riding a brown horse. The horse is equipped with a bridle and a number tag, and the track is surrounded by a fence and advertising boards.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24931.9, "ram_available_mb": 100840.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 24930.4, "ram_available_mb": 100841.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.26, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.85, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 34.79, "peak": 45.69, "min": 26.39}, "VIN": {"avg": 77.1, "peak": 117.38, "min": 57.81}}, "power_watts_avg": 34.79, "energy_joules_est": 115.25, "sample_count": 26, "duration_seconds": 3.313}, "timestamp": "2026-01-17T15:19:55.979667"}
{"image_index": 271, "image_name": "000000029187.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000029187.jpg", "image_width": 640, "image_height": 491, "image_resolution": "640x491", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3457.469, "latencies_ms": [3457.469], "images_per_second": 0.289, "prompt_tokens": 19, "response_tokens_est": 53, "n_tiles": 12, "output_text": "The horse in the image is brown with a white blaze on its face and a black bridle. The horse is wearing a red saddle and a white number tag. The lighting is bright, indicating it is daytime, and the weather appears to be clear.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24930.4, "ram_available_mb": 100841.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 24931.9, "ram_available_mb": 100840.3, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.23, "peak": 16.56, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.78, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 34.39, "peak": 45.69, "min": 26.0}, "VIN": {"avg": 74.97, "peak": 113.73, "min": 61.13}}, "power_watts_avg": 34.39, "energy_joules_est": 118.92, "sample_count": 27, "duration_seconds": 3.458}, "timestamp": "2026-01-17T15:19:59.443649"}
{"image_index": 272, "image_name": "000000029393.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000029393.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2206.801, "latencies_ms": [2206.801], "images_per_second": 0.453, "prompt_tokens": 9, "response_tokens_est": 18, "n_tiles": 12, "output_text": "A brown dog is standing on a blue ledge, looking directly at the camera.", "error": null, "sys_before": {"cpu_percent": 40.8, "ram_used_mb": 24927.8, "ram_available_mb": 100844.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.2, "ram_used_mb": 24929.5, "ram_available_mb": 100842.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.98, "peak": 16.46, "min": 13.41}, "VDD_CPU_SOC_MSS": {"avg": 15.79, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 37.79, "peak": 44.51, "min": 29.15}, "VIN": {"avg": 81.05, "peak": 115.83, "min": 65.79}}, "power_watts_avg": 37.79, "energy_joules_est": 83.41, "sample_count": 17, "duration_seconds": 2.207}, "timestamp": "2026-01-17T15:20:01.753174"}
{"image_index": 272, "image_name": "000000029393.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000029393.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2570.362, "latencies_ms": [2570.362], "images_per_second": 0.389, "prompt_tokens": 23, "response_tokens_est": 27, "n_tiles": 12, "output_text": "dog: 1\nlemon: 1\ntree: 1\nfence: 1\nbench: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24929.5, "ram_available_mb": 100842.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 24922.6, "ram_available_mb": 100849.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.08, "peak": 16.36, "min": 14.12}, "VDD_CPU_SOC_MSS": {"avg": 15.63, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 37.44, "peak": 45.69, "min": 27.18}, "VIN": {"avg": 83.53, "peak": 130.44, "min": 64.44}}, "power_watts_avg": 37.44, "energy_joules_est": 96.25, "sample_count": 20, "duration_seconds": 2.571}, "timestamp": "2026-01-17T15:20:04.330357"}
{"image_index": 272, "image_name": "000000029393.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000029393.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3926.391, "latencies_ms": [3926.391], "images_per_second": 0.255, "prompt_tokens": 27, "response_tokens_est": 66, "n_tiles": 12, "output_text": "The main object in the foreground is a brown dog standing on a blue ledge. The dog is positioned near the left side of the image. In the background, there is a wooden fence and a tree with green leaves. The fence is located behind the tree, and the tree is situated to the right of the fence.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24922.6, "ram_available_mb": 100849.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 24923.2, "ram_available_mb": 100849.0, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.92, "peak": 16.36, "min": 14.02}, "VDD_CPU_SOC_MSS": {"avg": 15.42, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 33.5, "peak": 45.3, "min": 25.61}, "VIN": {"avg": 71.71, "peak": 116.32, "min": 58.26}}, "power_watts_avg": 33.5, "energy_joules_est": 131.54, "sample_count": 30, "duration_seconds": 3.927}, "timestamp": "2026-01-17T15:20:08.263867"}
{"image_index": 272, "image_name": "000000029393.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000029393.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3644.719, "latencies_ms": [3644.719], "images_per_second": 0.274, "prompt_tokens": 21, "response_tokens_est": 58, "n_tiles": 12, "output_text": "The image depicts a serene outdoor scene with a brown dog standing on a blue ledge, surrounded by lush greenery and a wooden fence. The dog appears to be looking directly at the camera, while the sunlight casts shadows on the ground, creating a peaceful and natural atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24923.2, "ram_available_mb": 100849.0, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 24924.1, "ram_available_mb": 100848.0, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.95, "peak": 16.36, "min": 14.02}, "VDD_CPU_SOC_MSS": {"avg": 15.37, "peak": 16.92, "min": 14.17}, "VDD_GPU": {"avg": 33.91, "peak": 45.3, "min": 25.6}, "VIN": {"avg": 76.54, "peak": 121.98, "min": 56.81}}, "power_watts_avg": 33.91, "energy_joules_est": 123.6, "sample_count": 28, "duration_seconds": 3.645}, "timestamp": "2026-01-17T15:20:11.916051"}
{"image_index": 272, "image_name": "000000029393.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000029393.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3468.426, "latencies_ms": [3468.426], "images_per_second": 0.288, "prompt_tokens": 19, "response_tokens_est": 53, "n_tiles": 12, "output_text": "The image features a dog standing on a blue ledge, with its fur appearing light brown and its tail curled up. The dog is surrounded by lush green foliage, and the scene is bathed in bright sunlight, casting shadows on the ground.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24924.1, "ram_available_mb": 100848.0, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 24924.1, "ram_available_mb": 100848.0, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.93, "peak": 16.26, "min": 14.02}, "VDD_CPU_SOC_MSS": {"avg": 15.41, "peak": 17.32, "min": 14.17}, "VDD_GPU": {"avg": 34.13, "peak": 45.3, "min": 25.6}, "VIN": {"avg": 77.67, "peak": 119.42, "min": 61.84}}, "power_watts_avg": 34.13, "energy_joules_est": 118.4, "sample_count": 27, "duration_seconds": 3.469}, "timestamp": "2026-01-17T15:20:15.391413"}
{"image_index": 273, "image_name": "000000029397.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000029397.jpg", "image_width": 640, "image_height": 449, "image_resolution": "640x449", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1249.348, "latencies_ms": [1249.348], "images_per_second": 0.8, "prompt_tokens": 9, "response_tokens_est": 24, "n_tiles": 6, "output_text": "A person is standing on a wooden bench with a blue sign on it, wearing red pants and blue sneakers.", "error": null, "sys_before": {"cpu_percent": 47.1, "ram_used_mb": 24933.1, "ram_available_mb": 100839.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 24934.3, "ram_available_mb": 100837.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.55, "peak": 15.04, "min": 13.61}, "VDD_CPU_SOC_MSS": {"avg": 15.18, "peak": 15.75, "min": 14.17}, "VDD_GPU": {"avg": 35.01, "peak": 40.95, "min": 27.97}, "VIN": {"avg": 82.92, "peak": 115.34, "min": 64.13}}, "power_watts_avg": 35.01, "energy_joules_est": 43.75, "sample_count": 9, "duration_seconds": 1.25}, "timestamp": "2026-01-17T15:20:16.703483"}
{"image_index": 273, "image_name": "000000029397.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000029397.jpg", "image_width": 640, "image_height": 449, "image_resolution": "640x449", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1766.822, "latencies_ms": [1766.822], "images_per_second": 0.566, "prompt_tokens": 23, "response_tokens_est": 40, "n_tiles": 6, "output_text": "object: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24934.3, "ram_available_mb": 100837.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 24935.8, "ram_available_mb": 100836.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.85, "peak": 15.34, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.32, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 32.79, "peak": 41.74, "min": 24.43}, "VIN": {"avg": 71.25, "peak": 90.95, "min": 61.53}}, "power_watts_avg": 32.79, "energy_joules_est": 57.94, "sample_count": 13, "duration_seconds": 1.767}, "timestamp": "2026-01-17T15:20:18.476610"}
{"image_index": 273, "image_name": "000000029397.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000029397.jpg", "image_width": 640, "image_height": 449, "image_resolution": "640x449", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2196.442, "latencies_ms": [2196.442], "images_per_second": 0.455, "prompt_tokens": 27, "response_tokens_est": 55, "n_tiles": 6, "output_text": "The main object in the foreground is a wooden bench with a blue sign attached to its side. The bench is positioned on a red brick floor, and the sign is placed near the bench's base. The background features a concrete wall, which is slightly out of focus.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 24935.8, "ram_available_mb": 100836.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 24936.2, "ram_available_mb": 100835.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.86, "peak": 15.24, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.31, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 30.1, "peak": 40.56, "min": 23.25}, "VIN": {"avg": 70.68, "peak": 104.84, "min": 62.8}}, "power_watts_avg": 30.1, "energy_joules_est": 66.13, "sample_count": 17, "duration_seconds": 2.197}, "timestamp": "2026-01-17T15:20:20.679010"}
{"image_index": 273, "image_name": "000000029397.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000029397.jpg", "image_width": 640, "image_height": 449, "image_resolution": "640x449", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3125.775, "latencies_ms": [3125.775], "images_per_second": 0.32, "prompt_tokens": 21, "response_tokens_est": 88, "n_tiles": 6, "output_text": "The image depicts a wooden bench placed on a brick-paved surface, likely outdoors. The bench has a rustic appearance with visible wood grain and metal pegs for support. A blue sign with handwritten text is attached to the bench, although the content of the text is not entirely clear. The setting appears to be a public space, possibly a park or a street corner, given the bench's placement and the surrounding brickwork.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24936.2, "ram_available_mb": 100835.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 24937.5, "ram_available_mb": 100834.7, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.91, "peak": 15.24, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.5, "peak": 15.75, "min": 14.56}, "VDD_GPU": {"avg": 27.99, "peak": 40.97, "min": 22.85}, "VIN": {"avg": 69.66, "peak": 114.01, "min": 59.23}}, "power_watts_avg": 27.99, "energy_joules_est": 87.51, "sample_count": 24, "duration_seconds": 3.127}, "timestamp": "2026-01-17T15:20:23.811770"}
{"image_index": 273, "image_name": "000000029397.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000029397.jpg", "image_width": 640, "image_height": 449, "image_resolution": "640x449", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2158.479, "latencies_ms": [2158.479], "images_per_second": 0.463, "prompt_tokens": 19, "response_tokens_est": 54, "n_tiles": 6, "output_text": "The image features a wooden bench with a rustic, weathered appearance. The bench is placed on a red brick surface, and the bench itself has a natural wood finish. The lighting is soft and diffused, casting gentle shadows and highlighting the texture of the wood.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 24937.5, "ram_available_mb": 100834.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 24938.4, "ram_available_mb": 100833.7, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.88, "peak": 15.24, "min": 14.3}, "VDD_CPU_SOC_MSS": {"avg": 15.35, "peak": 15.75, "min": 14.57}, "VDD_GPU": {"avg": 30.01, "peak": 40.57, "min": 23.25}, "VIN": {"avg": 74.49, "peak": 115.9, "min": 59.38}}, "power_watts_avg": 30.01, "energy_joules_est": 64.79, "sample_count": 17, "duration_seconds": 2.159}, "timestamp": "2026-01-17T15:20:25.976452"}
{"image_index": 274, "image_name": "000000029596.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000029596.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1868.694, "latencies_ms": [1868.694], "images_per_second": 0.535, "prompt_tokens": 9, "response_tokens_est": 47, "n_tiles": 6, "output_text": "The image depicts a well-appointed hotel room with a cozy, inviting atmosphere, featuring a red sofa, a wooden dining table with a white tablecloth, a wooden cabinet, and a television mounted on the wall.", "error": null, "sys_before": {"cpu_percent": 42.0, "ram_used_mb": 24938.9, "ram_available_mb": 100833.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 24939.4, "ram_available_mb": 100832.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.79, "peak": 15.22, "min": 13.79}, "VDD_CPU_SOC_MSS": {"avg": 15.47, "peak": 16.14, "min": 14.17}, "VDD_GPU": {"avg": 31.03, "peak": 39.77, "min": 24.42}, "VIN": {"avg": 73.0, "peak": 108.04, "min": 61.58}}, "power_watts_avg": 31.03, "energy_joules_est": 58.0, "sample_count": 14, "duration_seconds": 1.869}, "timestamp": "2026-01-17T15:20:27.898703"}
{"image_index": 274, "image_name": "000000029596.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000029596.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1770.574, "latencies_ms": [1770.574], "images_per_second": 0.565, "prompt_tokens": 23, "response_tokens_est": 40, "n_tiles": 6, "output_text": "object: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24939.4, "ram_available_mb": 100832.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 24939.7, "ram_available_mb": 100832.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.87, "peak": 15.34, "min": 14.4}, "VDD_CPU_SOC_MSS": {"avg": 15.6, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 32.18, "peak": 40.57, "min": 24.42}, "VIN": {"avg": 72.44, "peak": 106.7, "min": 61.9}}, "power_watts_avg": 32.18, "energy_joules_est": 56.99, "sample_count": 13, "duration_seconds": 1.771}, "timestamp": "2026-01-17T15:20:29.675448"}
{"image_index": 274, "image_name": "000000029596.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000029596.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3558.855, "latencies_ms": [3558.855], "images_per_second": 0.281, "prompt_tokens": 27, "response_tokens_est": 103, "n_tiles": 6, "output_text": "The main objects in the image are arranged in a way that showcases a cozy and well-organized living space. The foreground features a red sofa with a beige pillow, positioned near a wooden side table with a lamp. In the background, there is a large window with curtains, allowing natural light to illuminate the room. To the right, a wooden cabinet with a television on top is visible. The overall arrangement creates a sense of comfort and functionality, making the space inviting and easy to navigate.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24939.7, "ram_available_mb": 100832.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 24940.1, "ram_available_mb": 100832.0, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.91, "peak": 15.24, "min": 14.3}, "VDD_CPU_SOC_MSS": {"avg": 15.58, "peak": 15.75, "min": 14.56}, "VDD_GPU": {"avg": 27.35, "peak": 40.95, "min": 22.85}, "VIN": {"avg": 68.27, "peak": 104.33, "min": 61.34}}, "power_watts_avg": 27.35, "energy_joules_est": 97.35, "sample_count": 28, "duration_seconds": 3.559}, "timestamp": "2026-01-17T15:20:33.241178"}
{"image_index": 274, "image_name": "000000029596.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000029596.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3381.414, "latencies_ms": [3381.414], "images_per_second": 0.296, "prompt_tokens": 21, "response_tokens_est": 97, "n_tiles": 6, "output_text": "The image depicts a well-appointed hotel room with a cozy and inviting atmosphere. The room features a red sofa, a wooden table with a white tablecloth, a wooden chair, a small side table with a lamp, and a television on the wall. The room is well-lit with natural light coming through the window, and there are decorative items on the table, including a vase with flowers. The overall setting suggests a comfortable and relaxing environment for guests.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24940.1, "ram_available_mb": 100832.0, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 24940.1, "ram_available_mb": 100832.0, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.9, "peak": 15.24, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.34, "peak": 15.75, "min": 14.57}, "VDD_GPU": {"avg": 27.51, "peak": 40.16, "min": 22.85}, "VIN": {"avg": 70.73, "peak": 106.28, "min": 59.12}}, "power_watts_avg": 27.51, "energy_joules_est": 93.04, "sample_count": 26, "duration_seconds": 3.382}, "timestamp": "2026-01-17T15:20:36.628922"}
{"image_index": 274, "image_name": "000000029596.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000029596.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1739.275, "latencies_ms": [1739.275], "images_per_second": 0.575, "prompt_tokens": 19, "response_tokens_est": 39, "n_tiles": 6, "output_text": "The room is warmly lit with natural light streaming through large windows covered with sheer curtains. The walls are painted in a soft yellow hue, complemented by a wooden dining table and chairs.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24940.1, "ram_available_mb": 100832.0, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 24940.6, "ram_available_mb": 100831.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.83, "peak": 15.24, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.29, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 31.79, "peak": 40.18, "min": 24.42}, "VIN": {"avg": 71.18, "peak": 103.51, "min": 57.02}}, "power_watts_avg": 31.79, "energy_joules_est": 55.3, "sample_count": 13, "duration_seconds": 1.74}, "timestamp": "2026-01-17T15:20:38.374864"}
{"image_index": 275, "image_name": "000000029640.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000029640.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2195.611, "latencies_ms": [2195.611], "images_per_second": 0.455, "prompt_tokens": 9, "response_tokens_est": 59, "n_tiles": 6, "output_text": "The image shows a well-composed and colorful vegetable and meat stir-fry in a black wok, featuring a mix of broccoli, carrots, zucchini, and possibly other vegetables, all cooked with pieces of ham or bacon, creating a vibrant and nutritious meal.", "error": null, "sys_before": {"cpu_percent": 44.6, "ram_used_mb": 24940.6, "ram_available_mb": 100831.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 24940.9, "ram_available_mb": 100831.3, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.89, "peak": 15.22, "min": 13.79}, "VDD_CPU_SOC_MSS": {"avg": 15.54, "peak": 16.14, "min": 14.17}, "VDD_GPU": {"avg": 29.92, "peak": 40.57, "min": 23.64}, "VIN": {"avg": 71.15, "peak": 116.47, "min": 60.4}}, "power_watts_avg": 29.92, "energy_joules_est": 65.7, "sample_count": 17, "duration_seconds": 2.196}, "timestamp": "2026-01-17T15:20:40.621239"}
{"image_index": 275, "image_name": "000000029640.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000029640.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1483.773, "latencies_ms": [1483.773], "images_per_second": 0.674, "prompt_tokens": 23, "response_tokens_est": 31, "n_tiles": 6, "output_text": "broccoli: 10\ncarrots: 3\nonions: 1\nsausage: 1\npotatoes: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24940.9, "ram_available_mb": 100831.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 24940.9, "ram_available_mb": 100831.3, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.99, "peak": 15.44, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.57, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 33.55, "peak": 40.18, "min": 26.0}, "VIN": {"avg": 76.22, "peak": 114.69, "min": 62.32}}, "power_watts_avg": 33.55, "energy_joules_est": 49.8, "sample_count": 11, "duration_seconds": 1.484}, "timestamp": "2026-01-17T15:20:42.111358"}
{"image_index": 275, "image_name": "000000029640.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000029640.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3677.45, "latencies_ms": [3677.45], "images_per_second": 0.272, "prompt_tokens": 27, "response_tokens_est": 112, "n_tiles": 6, "output_text": "The main object in the image is a black frying pan containing a colorful and well-arranged salad. The salad consists of various vegetables such as broccoli, carrots, zucchini, and possibly other greens, along with pieces of ham and possibly other meats. The frying pan is placed on a dark surface, likely a countertop, and there is a metal utensil, possibly a spatula, partially visible in the foreground. The background is mostly out of focus, emphasizing the salad and the utensil.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24940.9, "ram_available_mb": 100831.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 24941.4, "ram_available_mb": 100830.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.12, "peak": 15.44, "min": 14.43}, "VDD_CPU_SOC_MSS": {"avg": 15.68, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 27.9, "peak": 41.34, "min": 23.24}, "VIN": {"avg": 69.99, "peak": 109.39, "min": 52.61}}, "power_watts_avg": 27.9, "energy_joules_est": 102.61, "sample_count": 28, "duration_seconds": 3.678}, "timestamp": "2026-01-17T15:20:45.794554"}
{"image_index": 275, "image_name": "000000029640.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000029640.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2028.317, "latencies_ms": [2028.317], "images_per_second": 0.493, "prompt_tokens": 21, "response_tokens_est": 51, "n_tiles": 6, "output_text": "The image shows a close-up view of a black frying pan containing a colorful and healthy vegetable and meat mixture. The pan is on a stove, and a metal spatula is partially visible, suggesting that the dish is being prepared or cooked.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24941.4, "ram_available_mb": 100830.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 24941.4, "ram_available_mb": 100830.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.03, "peak": 15.54, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.64, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 30.93, "peak": 40.16, "min": 24.04}, "VIN": {"avg": 69.89, "peak": 92.94, "min": 58.91}}, "power_watts_avg": 30.93, "energy_joules_est": 62.75, "sample_count": 15, "duration_seconds": 2.029}, "timestamp": "2026-01-17T15:20:47.828835"}
{"image_index": 275, "image_name": "000000029640.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000029640.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1890.171, "latencies_ms": [1890.171], "images_per_second": 0.529, "prompt_tokens": 19, "response_tokens_est": 46, "n_tiles": 6, "output_text": "The image showcases a vibrant and colorful dish of broccoli, ham, and other vegetables in a black frying pan. The lighting is bright, highlighting the fresh colors of the ingredients and creating a warm, inviting atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24941.4, "ram_available_mb": 100830.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 24942.6, "ram_available_mb": 100829.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.01, "peak": 15.54, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.61, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 31.68, "peak": 40.56, "min": 24.43}, "VIN": {"avg": 71.56, "peak": 100.99, "min": 62.56}}, "power_watts_avg": 31.68, "energy_joules_est": 59.89, "sample_count": 14, "duration_seconds": 1.89}, "timestamp": "2026-01-17T15:20:49.725180"}
{"image_index": 276, "image_name": "000000029675.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000029675.jpg", "image_width": 478, "image_height": 640, "image_resolution": "478x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2391.222, "latencies_ms": [2391.222], "images_per_second": 0.418, "prompt_tokens": 9, "response_tokens_est": 23, "n_tiles": 12, "output_text": "The image shows a close-up of two hot dogs with mustard on them, placed on a dark plate.", "error": null, "sys_before": {"cpu_percent": 44.2, "ram_used_mb": 24914.0, "ram_available_mb": 100858.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.4, "ram_used_mb": 24916.7, "ram_available_mb": 100855.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.23, "peak": 16.66, "min": 13.69}, "VDD_CPU_SOC_MSS": {"avg": 15.99, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 36.72, "peak": 43.71, "min": 28.36}, "VIN": {"avg": 77.42, "peak": 95.24, "min": 66.02}}, "power_watts_avg": 36.72, "energy_joules_est": 87.82, "sample_count": 18, "duration_seconds": 2.392}, "timestamp": "2026-01-17T15:20:52.200732"}
{"image_index": 276, "image_name": "000000029675.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000029675.jpg", "image_width": 478, "image_height": 640, "image_resolution": "478x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2508.639, "latencies_ms": [2508.639], "images_per_second": 0.399, "prompt_tokens": 23, "response_tokens_est": 25, "n_tiles": 12, "output_text": "hot dog: 2\nbun: 2\nketchup: 2\nmustard: 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24916.9, "ram_available_mb": 100855.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 24918.1, "ram_available_mb": 100854.0, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.31, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.93, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 37.85, "peak": 45.28, "min": 28.36}, "VIN": {"avg": 80.09, "peak": 118.18, "min": 66.91}}, "power_watts_avg": 37.85, "energy_joules_est": 94.96, "sample_count": 19, "duration_seconds": 2.509}, "timestamp": "2026-01-17T15:20:54.715932"}
{"image_index": 276, "image_name": "000000029675.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000029675.jpg", "image_width": 478, "image_height": 640, "image_resolution": "478x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3608.541, "latencies_ms": [3608.541], "images_per_second": 0.277, "prompt_tokens": 27, "response_tokens_est": 57, "n_tiles": 12, "output_text": "The main objects in the image are two hot dogs placed on a dark plate. The hot dogs are positioned in the foreground, with the one on the left slightly overlapping the one on the right. The background is out of focus, emphasizing the hot dogs as the central subject.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24918.1, "ram_available_mb": 100854.0, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 24919.1, "ram_available_mb": 100853.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.2, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.7, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 34.16, "peak": 44.89, "min": 26.0}, "VIN": {"avg": 80.32, "peak": 117.92, "min": 63.11}}, "power_watts_avg": 34.16, "energy_joules_est": 123.28, "sample_count": 28, "duration_seconds": 3.609}, "timestamp": "2026-01-17T15:20:58.331448"}
{"image_index": 276, "image_name": "000000029675.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000029675.jpg", "image_width": 478, "image_height": 640, "image_resolution": "478x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3852.013, "latencies_ms": [3852.013], "images_per_second": 0.26, "prompt_tokens": 21, "response_tokens_est": 63, "n_tiles": 12, "output_text": "The image shows a close-up of a hot dog on a dark plate, with a yellow and red ketchup and mustard sauce generously applied to the hot dog. The hot dog is placed on a dark surface, possibly a table, and there is a glimpse of a blue object in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24919.1, "ram_available_mb": 100853.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 24920.6, "ram_available_mb": 100851.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.06, "peak": 16.56, "min": 14.12}, "VDD_CPU_SOC_MSS": {"avg": 15.59, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 33.13, "peak": 44.89, "min": 25.6}, "VIN": {"avg": 73.86, "peak": 109.6, "min": 58.07}}, "power_watts_avg": 33.13, "energy_joules_est": 127.63, "sample_count": 30, "duration_seconds": 3.852}, "timestamp": "2026-01-17T15:21:02.189793"}
{"image_index": 276, "image_name": "000000029675.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000029675.jpg", "image_width": 478, "image_height": 640, "image_resolution": "478x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3539.103, "latencies_ms": [3539.103], "images_per_second": 0.283, "prompt_tokens": 19, "response_tokens_est": 55, "n_tiles": 12, "output_text": "The image features a hot dog with a yellow mustard and ketchup topping, placed on a dark plate. The lighting is soft and natural, casting a warm glow on the food. The background is blurred, emphasizing the hot dog as the focal point.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24920.6, "ram_available_mb": 100851.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 24920.6, "ram_available_mb": 100851.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.94, "peak": 16.26, "min": 14.12}, "VDD_CPU_SOC_MSS": {"avg": 15.45, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 34.17, "peak": 44.91, "min": 25.6}, "VIN": {"avg": 74.64, "peak": 103.13, "min": 64.1}}, "power_watts_avg": 34.17, "energy_joules_est": 120.94, "sample_count": 27, "duration_seconds": 3.539}, "timestamp": "2026-01-17T15:21:05.735326"}
{"image_index": 277, "image_name": "000000029984.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000029984.jpg", "image_width": 640, "image_height": 492, "image_resolution": "640x492", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2557.865, "latencies_ms": [2557.865], "images_per_second": 0.391, "prompt_tokens": 9, "response_tokens_est": 28, "n_tiles": 12, "output_text": "A group of people are swimming in the ocean, with a green umbrella providing shade for a couple of chairs on the sandy shore.", "error": null, "sys_before": {"cpu_percent": 38.5, "ram_used_mb": 24925.0, "ram_available_mb": 100847.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.5, "ram_used_mb": 24926.0, "ram_available_mb": 100846.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.13, "peak": 16.66, "min": 13.19}, "VDD_CPU_SOC_MSS": {"avg": 15.89, "peak": 17.71, "min": 14.18}, "VDD_GPU": {"avg": 36.43, "peak": 44.12, "min": 27.57}, "VIN": {"avg": 81.08, "peak": 116.59, "min": 56.81}}, "power_watts_avg": 36.43, "energy_joules_est": 93.2, "sample_count": 19, "duration_seconds": 2.558}, "timestamp": "2026-01-17T15:21:08.402172"}
{"image_index": 277, "image_name": "000000029984.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000029984.jpg", "image_width": 640, "image_height": 492, "image_resolution": "640x492", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 3030.597, "latencies_ms": [3030.597], "images_per_second": 0.33, "prompt_tokens": 23, "response_tokens_est": 40, "n_tiles": 12, "output_text": "object: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24926.0, "ram_available_mb": 100846.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 24926.5, "ram_available_mb": 100845.7, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.24, "peak": 16.56, "min": 14.12}, "VDD_CPU_SOC_MSS": {"avg": 15.75, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 35.86, "peak": 45.68, "min": 26.39}, "VIN": {"avg": 79.66, "peak": 116.17, "min": 58.61}}, "power_watts_avg": 35.86, "energy_joules_est": 108.69, "sample_count": 23, "duration_seconds": 3.031}, "timestamp": "2026-01-17T15:21:11.444382"}
{"image_index": 277, "image_name": "000000029984.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000029984.jpg", "image_width": 640, "image_height": 492, "image_resolution": "640x492", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3775.831, "latencies_ms": [3775.831], "images_per_second": 0.265, "prompt_tokens": 27, "response_tokens_est": 62, "n_tiles": 12, "output_text": "The main objects in the image are a beach umbrella, a beach chair, and the ocean. The beach umbrella is located in the foreground, near the sandy shore. The beach chair is positioned in the middle ground, near the ocean. The ocean is in the background, stretching out to the horizon.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24926.5, "ram_available_mb": 100845.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 24926.7, "ram_available_mb": 100845.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.2, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.71, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 33.79, "peak": 45.28, "min": 26.0}, "VIN": {"avg": 78.19, "peak": 133.32, "min": 55.06}}, "power_watts_avg": 33.79, "energy_joules_est": 127.6, "sample_count": 29, "duration_seconds": 3.776}, "timestamp": "2026-01-17T15:21:15.227014"}
{"image_index": 277, "image_name": "000000029984.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000029984.jpg", "image_width": 640, "image_height": 492, "image_resolution": "640x492", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3673.318, "latencies_ms": [3673.318], "images_per_second": 0.272, "prompt_tokens": 21, "response_tokens_est": 59, "n_tiles": 12, "output_text": "The image depicts a serene beach scene with a calm ocean and a sandy shore. There are several people in the water, enjoying the sun and the sea breeze. In the foreground, there are colorful beach chairs and umbrellas, suggesting a leisurely day at the beach.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24926.7, "ram_available_mb": 100845.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 24926.4, "ram_available_mb": 100845.7, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.2, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.79, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 33.94, "peak": 44.91, "min": 26.01}, "VIN": {"avg": 78.73, "peak": 128.71, "min": 57.03}}, "power_watts_avg": 33.94, "energy_joules_est": 124.69, "sample_count": 28, "duration_seconds": 3.674}, "timestamp": "2026-01-17T15:21:18.907248"}
{"image_index": 277, "image_name": "000000029984.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000029984.jpg", "image_width": 640, "image_height": 492, "image_resolution": "640x492", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3535.161, "latencies_ms": [3535.161], "images_per_second": 0.283, "prompt_tokens": 19, "response_tokens_est": 55, "n_tiles": 12, "output_text": "The image depicts a serene beach scene with a calm ocean and a sandy shore. The sky is overcast, casting a soft, diffused light over the entire scene. The beach chairs are colorful, with one pink and one blue chair prominently visible.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24926.4, "ram_available_mb": 100845.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 24926.7, "ram_available_mb": 100845.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.19, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.76, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 34.2, "peak": 45.69, "min": 26.0}, "VIN": {"avg": 77.44, "peak": 123.22, "min": 63.93}}, "power_watts_avg": 34.2, "energy_joules_est": 120.92, "sample_count": 27, "duration_seconds": 3.536}, "timestamp": "2026-01-17T15:21:22.448975"}
{"image_index": 278, "image_name": "000000030213.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000030213.jpg", "image_width": 640, "image_height": 449, "image_resolution": "640x449", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2036.614, "latencies_ms": [2036.614], "images_per_second": 0.491, "prompt_tokens": 9, "response_tokens_est": 53, "n_tiles": 6, "output_text": "The image depicts a vintage kitchen with a retro design, featuring a green-painted wooden cabinet, a white sink with a silver faucet, a wooden table with various items, a wooden chair, and a basket of fruit on top of a white cabinet.", "error": null, "sys_before": {"cpu_percent": 40.0, "ram_used_mb": 24946.1, "ram_available_mb": 100826.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 24946.6, "ram_available_mb": 100825.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.79, "peak": 15.22, "min": 13.72}, "VDD_CPU_SOC_MSS": {"avg": 15.46, "peak": 16.14, "min": 14.17}, "VDD_GPU": {"avg": 30.99, "peak": 40.97, "min": 24.04}, "VIN": {"avg": 72.36, "peak": 120.72, "min": 62.02}}, "power_watts_avg": 30.99, "energy_joules_est": 63.12, "sample_count": 15, "duration_seconds": 2.037}, "timestamp": "2026-01-17T15:21:24.554428"}
{"image_index": 278, "image_name": "000000030213.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000030213.jpg", "image_width": 640, "image_height": 449, "image_resolution": "640x449", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 3934.957, "latencies_ms": [3934.957], "images_per_second": 0.254, "prompt_tokens": 23, "response_tokens_est": 121, "n_tiles": 6, "output_text": "- Kitchen\n  - cabinets: 2\n  - drawers: 4\n  - sink: 1\n  - table: 1\n  - chairs: 2\n- Kitchen utensils\n  - kettle: 1\n  - mixer: 1\n  - bowl: 1\n  - cup: 1\n  - spoon: 1\n  - juicer: 1\n- Decorative items\n  - calendar: 1\n  - basket: 1\n  - fan: 1\n  - picture: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24946.6, "ram_available_mb": 100825.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 24947.5, "ram_available_mb": 100824.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.13, "peak": 15.54, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.7, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 27.21, "peak": 40.16, "min": 23.24}, "VIN": {"avg": 68.86, "peak": 96.48, "min": 57.55}}, "power_watts_avg": 27.21, "energy_joules_est": 107.08, "sample_count": 30, "duration_seconds": 3.935}, "timestamp": "2026-01-17T15:21:28.496091"}
{"image_index": 278, "image_name": "000000030213.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000030213.jpg", "image_width": 640, "image_height": 449, "image_resolution": "640x449", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 4129.802, "latencies_ms": [4129.802], "images_per_second": 0.242, "prompt_tokens": 27, "response_tokens_est": 129, "n_tiles": 6, "output_text": "The main objects in the image are arranged in a way that showcases the spatial relationships between them. The left side of the image features a green dresser with drawers, a white sink, and a wooden table. The sink is positioned near the dresser. In the background, there is a wooden cabinet with a mirror above it. The right side of the image shows a white cabinet with a mirror on the door, a wooden chair, and a basket of fruit on top of it. The basket is placed near the cabinet. The floor is covered with a patterned rug, and there is a wooden basket on the floor near the chair", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24947.5, "ram_available_mb": 100824.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 24947.5, "ram_available_mb": 100824.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.14, "peak": 15.54, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.69, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 27.11, "peak": 40.16, "min": 23.24}, "VIN": {"avg": 69.79, "peak": 111.3, "min": 59.23}}, "power_watts_avg": 27.11, "energy_joules_est": 111.97, "sample_count": 31, "duration_seconds": 4.13}, "timestamp": "2026-01-17T15:21:32.631987"}
{"image_index": 278, "image_name": "000000030213.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000030213.jpg", "image_width": 640, "image_height": 449, "image_resolution": "640x449", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3166.213, "latencies_ms": [3166.213], "images_per_second": 0.316, "prompt_tokens": 21, "response_tokens_est": 93, "n_tiles": 6, "output_text": "The image depicts a vintage kitchen with a retro aesthetic. The room features a green-painted wooden cabinet, a white sink with a silver faucet, a wooden table with various items, a wooden chair, and a basket of fruit on top of a white cabinet. The walls are adorned with a patterned wallpaper, and there are several items on the table, including a mixer and a bowl. The overall setting suggests a nostalgic and charming atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24947.5, "ram_available_mb": 100824.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 24948.5, "ram_available_mb": 100823.7, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.11, "peak": 15.44, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.67, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 28.15, "peak": 40.16, "min": 23.24}, "VIN": {"avg": 71.0, "peak": 108.83, "min": 55.6}}, "power_watts_avg": 28.15, "energy_joules_est": 89.14, "sample_count": 24, "duration_seconds": 3.167}, "timestamp": "2026-01-17T15:21:35.804791"}
{"image_index": 278, "image_name": "000000030213.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000030213.jpg", "image_width": 640, "image_height": 449, "image_resolution": "640x449", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2289.707, "latencies_ms": [2289.707], "images_per_second": 0.437, "prompt_tokens": 19, "response_tokens_est": 61, "n_tiles": 6, "output_text": "The room is filled with vintage elements, including a green-painted wooden cabinet, a white sink with a silver faucet, and a wooden table with a white top. The walls are adorned with a tropical wallpaper pattern, and the floor is covered with a colorful, floral-patterned rug.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24948.5, "ram_available_mb": 100823.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 24948.5, "ram_available_mb": 100823.7, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.06, "peak": 15.54, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.63, "peak": 16.14, "min": 14.57}, "VDD_GPU": {"avg": 30.1, "peak": 40.16, "min": 23.64}, "VIN": {"avg": 73.48, "peak": 107.36, "min": 62.85}}, "power_watts_avg": 30.1, "energy_joules_est": 68.93, "sample_count": 17, "duration_seconds": 2.29}, "timestamp": "2026-01-17T15:21:38.100462"}
{"image_index": 279, "image_name": "000000030494.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000030494.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2320.046, "latencies_ms": [2320.046], "images_per_second": 0.431, "prompt_tokens": 9, "response_tokens_est": 21, "n_tiles": 12, "output_text": "A black and white dog is standing on a dirt path surrounded by fallen leaves and a tree trunk.", "error": null, "sys_before": {"cpu_percent": 43.2, "ram_used_mb": 24925.4, "ram_available_mb": 100846.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.3, "ram_used_mb": 24926.4, "ram_available_mb": 100845.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.26, "peak": 16.66, "min": 13.79}, "VDD_CPU_SOC_MSS": {"avg": 16.05, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 36.76, "peak": 43.71, "min": 28.36}, "VIN": {"avg": 79.48, "peak": 124.31, "min": 66.47}}, "power_watts_avg": 36.76, "energy_joules_est": 85.3, "sample_count": 18, "duration_seconds": 2.32}, "timestamp": "2026-01-17T15:21:40.502114"}
{"image_index": 279, "image_name": "000000030494.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000030494.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2384.332, "latencies_ms": [2384.332], "images_per_second": 0.419, "prompt_tokens": 23, "response_tokens_est": 21, "n_tiles": 12, "output_text": "dog: 1\nleaves: 1\ntree: 1\nground: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24926.4, "ram_available_mb": 100845.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.3, "ram_used_mb": 24927.6, "ram_available_mb": 100844.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.34, "peak": 16.56, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.91, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 38.6, "peak": 45.3, "min": 29.56}, "VIN": {"avg": 84.31, "peak": 116.45, "min": 65.77}}, "power_watts_avg": 38.6, "energy_joules_est": 92.06, "sample_count": 17, "duration_seconds": 2.385}, "timestamp": "2026-01-17T15:21:42.892987"}
{"image_index": 279, "image_name": "000000030494.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000030494.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 4270.154, "latencies_ms": [4270.154], "images_per_second": 0.234, "prompt_tokens": 27, "response_tokens_est": 77, "n_tiles": 12, "output_text": "The main object in the foreground is a dog, which is positioned near the center of the image. The dog is facing to the right, with its head turned slightly towards the camera. The background consists of a tree trunk and a leafy area, which are slightly out of focus. The dog is not directly near the tree trunk, but it is in close proximity to it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24927.6, "ram_available_mb": 100844.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 24928.3, "ram_available_mb": 100843.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.22, "peak": 16.66, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.79, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 33.06, "peak": 46.09, "min": 26.0}, "VIN": {"avg": 76.9, "peak": 129.52, "min": 56.24}}, "power_watts_avg": 33.06, "energy_joules_est": 141.19, "sample_count": 33, "duration_seconds": 4.271}, "timestamp": "2026-01-17T15:21:47.169929"}
{"image_index": 279, "image_name": "000000030494.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000030494.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3087.856, "latencies_ms": [3087.856], "images_per_second": 0.324, "prompt_tokens": 21, "response_tokens_est": 42, "n_tiles": 12, "output_text": "The image depicts a small dog in a lush, green forest setting. The dog is standing on a dirt path surrounded by fallen leaves and greenery, with its tail raised and ears perked up.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24928.3, "ram_available_mb": 100843.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 24928.8, "ram_available_mb": 100843.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.26, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.79, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 35.3, "peak": 45.28, "min": 26.39}, "VIN": {"avg": 78.21, "peak": 115.3, "min": 53.15}}, "power_watts_avg": 35.3, "energy_joules_est": 109.02, "sample_count": 24, "duration_seconds": 3.088}, "timestamp": "2026-01-17T15:21:50.264901"}
{"image_index": 279, "image_name": "000000030494.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000030494.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3910.928, "latencies_ms": [3910.928], "images_per_second": 0.256, "prompt_tokens": 19, "response_tokens_est": 67, "n_tiles": 12, "output_text": "The image depicts a dog standing on a forest floor covered with brown leaves and dirt. The dog has a black and white coat, and its fur appears to be slightly matted. The lighting is natural, suggesting it is daytime, and the weather seems to be mild, as there are no signs of rain or extreme heat.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24928.8, "ram_available_mb": 100843.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 24929.8, "ram_available_mb": 100842.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.25, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.84, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 33.6, "peak": 44.89, "min": 26.0}, "VIN": {"avg": 72.39, "peak": 91.04, "min": 62.72}}, "power_watts_avg": 33.6, "energy_joules_est": 131.42, "sample_count": 30, "duration_seconds": 3.911}, "timestamp": "2026-01-17T15:21:54.183353"}
{"image_index": 280, "image_name": "000000030504.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000030504.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2458.977, "latencies_ms": [2458.977], "images_per_second": 0.407, "prompt_tokens": 9, "response_tokens_est": 25, "n_tiles": 12, "output_text": "A skier is skiing down a snowy slope, wearing a helmet and goggles, and carrying a backpack.", "error": null, "sys_before": {"cpu_percent": 44.5, "ram_used_mb": 24916.9, "ram_available_mb": 100855.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.4, "ram_used_mb": 24918.8, "ram_available_mb": 100853.3, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.17, "peak": 16.66, "min": 13.39}, "VDD_CPU_SOC_MSS": {"avg": 15.95, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 36.3, "peak": 44.12, "min": 27.57}, "VIN": {"avg": 78.38, "peak": 116.33, "min": 56.15}}, "power_watts_avg": 36.3, "energy_joules_est": 89.27, "sample_count": 19, "duration_seconds": 2.459}, "timestamp": "2026-01-17T15:21:56.745107"}
{"image_index": 280, "image_name": "000000030504.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000030504.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2992.658, "latencies_ms": [2992.658], "images_per_second": 0.334, "prompt_tokens": 23, "response_tokens_est": 39, "n_tiles": 12, "output_text": "1. Skis\n2. Skier\n3. Snow\n4. Snowboard\n5. Snowboard\n6. Snowboard\n7. Snowboard\n8. Snowboard", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24918.8, "ram_available_mb": 100853.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 24919.6, "ram_available_mb": 100852.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.26, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.78, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 35.84, "peak": 45.68, "min": 26.39}, "VIN": {"avg": 79.93, "peak": 126.23, "min": 50.77}}, "power_watts_avg": 35.84, "energy_joules_est": 107.27, "sample_count": 23, "duration_seconds": 2.993}, "timestamp": "2026-01-17T15:21:59.744292"}
{"image_index": 280, "image_name": "000000030504.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000030504.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 4567.703, "latencies_ms": [4567.703], "images_per_second": 0.219, "prompt_tokens": 27, "response_tokens_est": 84, "n_tiles": 12, "output_text": "The main object in the foreground is a person standing on skis, wearing green boots and a black jacket. The person is holding ski poles and appears to be in motion, possibly skiing downhill. The background features a snowy landscape with trees and a clear blue sky. The person is positioned slightly to the right of the center, with the skis and poles extending towards the left side of the image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24919.8, "ram_available_mb": 100852.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 24920.8, "ram_available_mb": 100851.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.9, "peak": 16.36, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.38, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 32.33, "peak": 45.69, "min": 25.6}, "VIN": {"avg": 72.41, "peak": 115.02, "min": 51.94}}, "power_watts_avg": 32.33, "energy_joules_est": 147.69, "sample_count": 35, "duration_seconds": 4.568}, "timestamp": "2026-01-17T15:22:04.322372"}
{"image_index": 280, "image_name": "000000030504.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000030504.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 4037.087, "latencies_ms": [4037.087], "images_per_second": 0.248, "prompt_tokens": 21, "response_tokens_est": 69, "n_tiles": 12, "output_text": "The image depicts a person skiing down a snowy slope. The individual is wearing a helmet, goggles, and ski gear, including a jacket and pants, and is holding ski poles. The snowy landscape and clear blue sky suggest a winter setting, and the person appears to be in motion, possibly skiing downhill.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24920.8, "ram_available_mb": 100851.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 24921.8, "ram_available_mb": 100850.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.89, "peak": 16.36, "min": 13.82}, "VDD_CPU_SOC_MSS": {"avg": 15.37, "peak": 17.32, "min": 14.17}, "VDD_GPU": {"avg": 33.01, "peak": 45.69, "min": 25.6}, "VIN": {"avg": 74.02, "peak": 115.2, "min": 55.19}}, "power_watts_avg": 33.01, "energy_joules_est": 133.28, "sample_count": 31, "duration_seconds": 4.038}, "timestamp": "2026-01-17T15:22:08.366604"}
{"image_index": 280, "image_name": "000000030504.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000030504.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3550.63, "latencies_ms": [3550.63], "images_per_second": 0.282, "prompt_tokens": 19, "response_tokens_est": 55, "n_tiles": 12, "output_text": "The image depicts a person skiing in a snowy landscape. The individual is wearing a black jacket, black pants, and green ski boots, and is holding ski poles. The snow is bright white, and the sky is clear blue, indicating a sunny day.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24921.8, "ram_available_mb": 100850.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 24923.2, "ram_available_mb": 100848.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.92, "peak": 16.36, "min": 13.92}, "VDD_CPU_SOC_MSS": {"avg": 15.43, "peak": 17.32, "min": 14.17}, "VDD_GPU": {"avg": 34.21, "peak": 45.3, "min": 25.61}, "VIN": {"avg": 75.33, "peak": 115.45, "min": 58.54}}, "power_watts_avg": 34.21, "energy_joules_est": 121.48, "sample_count": 27, "duration_seconds": 3.551}, "timestamp": "2026-01-17T15:22:11.923850"}
{"image_index": 281, "image_name": "000000030675.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000030675.jpg", "image_width": 640, "image_height": 360, "image_resolution": "640x360", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1082.957, "latencies_ms": [1082.957], "images_per_second": 0.923, "prompt_tokens": 9, "response_tokens_est": 40, "n_tiles": 2, "output_text": "The image depicts a large, orange locomotive with the number 6309, labeled \"BNSF,\" traveling on railroad tracks surrounded by a barren landscape with leafless trees.", "error": null, "sys_before": {"cpu_percent": 35.6, "ram_used_mb": 24923.2, "ram_available_mb": 100848.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24923.7, "ram_available_mb": 100848.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5055.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.81, "peak": 15.52, "min": 13.9}, "VDD_CPU_SOC_MSS": {"avg": 15.26, "peak": 15.76, "min": 14.56}, "VDD_GPU": {"avg": 25.95, "peak": 32.3, "min": 22.06}, "VIN": {"avg": 68.88, "peak": 102.19, "min": 60.95}}, "power_watts_avg": 25.95, "energy_joules_est": 28.12, "sample_count": 8, "duration_seconds": 1.084}, "timestamp": "2026-01-17T15:22:13.040595"}
{"image_index": 281, "image_name": "000000030675.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000030675.jpg", "image_width": 640, "image_height": 360, "image_resolution": "640x360", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1090.093, "latencies_ms": [1090.093], "images_per_second": 0.917, "prompt_tokens": 23, "response_tokens_est": 40, "n_tiles": 2, "output_text": "object: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24923.7, "ram_available_mb": 100848.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 24923.7, "ram_available_mb": 100848.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5064.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.2, "peak": 15.72, "min": 14.71}, "VDD_CPU_SOC_MSS": {"avg": 15.7, "peak": 16.14, "min": 15.35}, "VDD_GPU": {"avg": 25.51, "peak": 31.11, "min": 21.67}, "VIN": {"avg": 65.96, "peak": 91.79, "min": 59.18}}, "power_watts_avg": 25.51, "energy_joules_est": 27.82, "sample_count": 8, "duration_seconds": 1.09}, "timestamp": "2026-01-17T15:22:14.136749"}
{"image_index": 281, "image_name": "000000030675.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000030675.jpg", "image_width": 640, "image_height": 360, "image_resolution": "640x360", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2498.64, "latencies_ms": [2498.64], "images_per_second": 0.4, "prompt_tokens": 27, "response_tokens_est": 102, "n_tiles": 2, "output_text": "The main object in the image is a train, which is positioned in the foreground on the right side of the frame. The train is painted in orange and yellow colors with the letters \"BNSF\" prominently displayed on its side. The train is traveling on a set of railroad tracks that run parallel to the ground. In the background, there is a line of leafless trees, indicating that the season is likely winter or early spring. The sky is clear and blue, suggesting good weather conditions.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24923.7, "ram_available_mb": 100848.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24924.2, "ram_available_mb": 100848.0, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5067.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.33, "peak": 15.72, "min": 14.91}, "VDD_CPU_SOC_MSS": {"avg": 16.12, "peak": 16.54, "min": 15.35}, "VDD_GPU": {"avg": 22.98, "peak": 31.12, "min": 20.88}, "VIN": {"avg": 65.59, "peak": 97.28, "min": 55.25}}, "power_watts_avg": 22.98, "energy_joules_est": 57.43, "sample_count": 19, "duration_seconds": 2.499}, "timestamp": "2026-01-17T15:22:16.641561"}
{"image_index": 281, "image_name": "000000030675.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000030675.jpg", "image_width": 640, "image_height": 360, "image_resolution": "640x360", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1343.877, "latencies_ms": [1343.877], "images_per_second": 0.744, "prompt_tokens": 21, "response_tokens_est": 51, "n_tiles": 2, "output_text": "The image depicts a train traveling on a railway track, with a clear blue sky above and leafless trees in the background. The train is painted in bright orange and black colors, and the number \"6309\" is visible on its side.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24924.2, "ram_available_mb": 100848.0, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24924.2, "ram_available_mb": 100848.0, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5063.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.07, "peak": 15.52, "min": 14.61}, "VDD_CPU_SOC_MSS": {"avg": 15.83, "peak": 16.54, "min": 14.96}, "VDD_GPU": {"avg": 24.46, "peak": 30.33, "min": 21.28}, "VIN": {"avg": 66.7, "peak": 103.68, "min": 57.31}}, "power_watts_avg": 24.46, "energy_joules_est": 32.88, "sample_count": 10, "duration_seconds": 1.344}, "timestamp": "2026-01-17T15:22:17.991384"}
{"image_index": 281, "image_name": "000000030675.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000030675.jpg", "image_width": 640, "image_height": 360, "image_resolution": "640x360", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1636.768, "latencies_ms": [1636.768], "images_per_second": 0.611, "prompt_tokens": 19, "response_tokens_est": 64, "n_tiles": 2, "output_text": "The image features a bright orange locomotive with the number 6309 prominently displayed on its side. The locomotive is painted in a striking orange color with black and yellow accents, and it is situated on a railway track surrounded by a barren landscape with leafless trees under a clear blue sky.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24924.2, "ram_available_mb": 100848.0, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 24924.4, "ram_available_mb": 100847.7, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5062.6, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.17, "peak": 15.72, "min": 14.51}, "VDD_CPU_SOC_MSS": {"avg": 15.74, "peak": 16.14, "min": 15.36}, "VDD_GPU": {"avg": 24.07, "peak": 30.74, "min": 20.89}, "VIN": {"avg": 64.43, "peak": 90.41, "min": 56.62}}, "power_watts_avg": 24.07, "energy_joules_est": 39.4, "sample_count": 12, "duration_seconds": 1.637}, "timestamp": "2026-01-17T15:22:19.633997"}
{"image_index": 282, "image_name": "000000030785.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000030785.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2863.234, "latencies_ms": [2863.234], "images_per_second": 0.349, "prompt_tokens": 9, "response_tokens_est": 37, "n_tiles": 12, "output_text": "The image shows a plate of food, which includes a piece of bread topped with a green spread, a small bowl of broccoli, and a blue bowl with a white rim.", "error": null, "sys_before": {"cpu_percent": 47.5, "ram_used_mb": 24924.3, "ram_available_mb": 100847.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 24926.2, "ram_available_mb": 100845.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.2, "peak": 16.76, "min": 13.69}, "VDD_CPU_SOC_MSS": {"avg": 15.91, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 34.67, "peak": 43.33, "min": 26.39}, "VIN": {"avg": 79.41, "peak": 112.62, "min": 56.61}}, "power_watts_avg": 34.67, "energy_joules_est": 99.28, "sample_count": 22, "duration_seconds": 2.864}, "timestamp": "2026-01-17T15:22:22.587463"}
{"image_index": 282, "image_name": "000000030785.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000030785.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2988.235, "latencies_ms": [2988.235], "images_per_second": 0.335, "prompt_tokens": 23, "response_tokens_est": 39, "n_tiles": 12, "output_text": "- plate: 1\n- bowl: 2\n- broccoli: 2\n- avocado: 1\n- bread: 1\n- lettuce: 0", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24926.2, "ram_available_mb": 100845.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 24927.2, "ram_available_mb": 100845.0, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.25, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.83, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 35.69, "peak": 45.28, "min": 26.39}, "VIN": {"avg": 79.56, "peak": 112.56, "min": 58.02}}, "power_watts_avg": 35.69, "energy_joules_est": 106.67, "sample_count": 23, "duration_seconds": 2.989}, "timestamp": "2026-01-17T15:22:25.582393"}
{"image_index": 282, "image_name": "000000030785.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000030785.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3846.312, "latencies_ms": [3846.312], "images_per_second": 0.26, "prompt_tokens": 27, "response_tokens_est": 64, "n_tiles": 12, "output_text": "The main objects in the image are a plate of food and a bowl of broccoli. The plate of food is positioned in the foreground, with the bowl of broccoli placed in the background. The plate of food is placed on a wooden surface, while the bowl of broccoli is on a different wooden surface.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24927.2, "ram_available_mb": 100845.0, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 24928.0, "ram_available_mb": 100844.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.21, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.71, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 33.57, "peak": 44.89, "min": 26.0}, "VIN": {"avg": 74.91, "peak": 111.92, "min": 65.02}}, "power_watts_avg": 33.57, "energy_joules_est": 129.13, "sample_count": 29, "duration_seconds": 3.847}, "timestamp": "2026-01-17T15:22:29.439055"}
{"image_index": 282, "image_name": "000000030785.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000030785.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3711.464, "latencies_ms": [3711.464], "images_per_second": 0.269, "prompt_tokens": 21, "response_tokens_est": 60, "n_tiles": 12, "output_text": "The image depicts a plate of food arranged on a wooden table. The plate contains a portion of avocado, a serving of broccoli, and a small bowl of what appears to be a sauce or dressing. The setting suggests a casual dining environment, possibly a home or a casual restaurant.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24928.0, "ram_available_mb": 100844.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 24929.2, "ram_available_mb": 100843.0, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.17, "peak": 16.66, "min": 14.02}, "VDD_CPU_SOC_MSS": {"avg": 15.73, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 33.65, "peak": 45.69, "min": 26.0}, "VIN": {"avg": 78.1, "peak": 117.0, "min": 54.66}}, "power_watts_avg": 33.65, "energy_joules_est": 124.91, "sample_count": 29, "duration_seconds": 3.712}, "timestamp": "2026-01-17T15:22:33.158769"}
{"image_index": 282, "image_name": "000000030785.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000030785.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 4179.805, "latencies_ms": [4179.805], "images_per_second": 0.239, "prompt_tokens": 19, "response_tokens_est": 74, "n_tiles": 12, "output_text": "The image features a plate of food with a light blue rim and a white interior. The food includes a piece of bread topped with a green spread, broccoli, and a small bowl of what appears to be a sauce or dressing. The lighting is warm and soft, casting a gentle glow on the food, and the wooden surface provides a natural and rustic backdrop.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24929.2, "ram_available_mb": 100843.0, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 24930.4, "ram_available_mb": 100841.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.16, "peak": 16.56, "min": 14.12}, "VDD_CPU_SOC_MSS": {"avg": 15.73, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 32.77, "peak": 45.68, "min": 26.0}, "VIN": {"avg": 73.56, "peak": 111.04, "min": 56.62}}, "power_watts_avg": 32.77, "energy_joules_est": 136.99, "sample_count": 33, "duration_seconds": 4.18}, "timestamp": "2026-01-17T15:22:37.346416"}
{"image_index": 283, "image_name": "000000030828.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000030828.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1143.134, "latencies_ms": [1143.134], "images_per_second": 0.875, "prompt_tokens": 9, "response_tokens_est": 20, "n_tiles": 6, "output_text": "A person is lying on a bench in a park, covered with a quilted orange blanket.", "error": null, "sys_before": {"cpu_percent": 37.9, "ram_used_mb": 24950.6, "ram_available_mb": 100821.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 24951.8, "ram_available_mb": 100820.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.51, "peak": 15.04, "min": 13.72}, "VDD_CPU_SOC_MSS": {"avg": 15.16, "peak": 16.14, "min": 14.18}, "VDD_GPU": {"avg": 35.6, "peak": 40.56, "min": 29.54}, "VIN": {"avg": 76.35, "peak": 100.55, "min": 60.9}}, "power_watts_avg": 35.6, "energy_joules_est": 40.71, "sample_count": 8, "duration_seconds": 1.143}, "timestamp": "2026-01-17T15:22:38.558580"}
{"image_index": 283, "image_name": "000000030828.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000030828.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1726.793, "latencies_ms": [1726.793], "images_per_second": 0.579, "prompt_tokens": 23, "response_tokens_est": 40, "n_tiles": 6, "output_text": "object: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24951.8, "ram_available_mb": 100820.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 24952.8, "ram_available_mb": 100819.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.0, "peak": 15.44, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.6, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 33.18, "peak": 41.74, "min": 24.82}, "VIN": {"avg": 72.09, "peak": 109.67, "min": 57.4}}, "power_watts_avg": 33.18, "energy_joules_est": 57.31, "sample_count": 13, "duration_seconds": 1.727}, "timestamp": "2026-01-17T15:22:40.291460"}
{"image_index": 283, "image_name": "000000030828.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000030828.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2619.295, "latencies_ms": [2619.295], "images_per_second": 0.382, "prompt_tokens": 27, "response_tokens_est": 73, "n_tiles": 6, "output_text": "The main objects in the image are a bench and a person lying on it. The bench is located in the foreground, while the person is lying on it. The person is near the bench, and the bench is near the person. The background features a fence and some greenery, indicating that the scene is set in a park or a similar outdoor area.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24952.8, "ram_available_mb": 100819.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 24953.8, "ram_available_mb": 100818.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.11, "peak": 15.44, "min": 14.43}, "VDD_CPU_SOC_MSS": {"avg": 15.77, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 29.43, "peak": 40.57, "min": 23.64}, "VIN": {"avg": 70.29, "peak": 105.79, "min": 58.8}}, "power_watts_avg": 29.43, "energy_joules_est": 77.09, "sample_count": 20, "duration_seconds": 2.62}, "timestamp": "2026-01-17T15:22:42.916697"}
{"image_index": 283, "image_name": "000000030828.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000030828.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2485.711, "latencies_ms": [2485.711], "images_per_second": 0.402, "prompt_tokens": 21, "response_tokens_est": 68, "n_tiles": 6, "output_text": "The image depicts a park bench with a person lying down on it, covered with a quilted orange blanket. The bench is situated on a grassy area with a metal fence in the background, and there are two parking meters visible. The scene suggests a quiet, possibly cold day, as the person is dressed in warm clothing.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24953.8, "ram_available_mb": 100818.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 24955.0, "ram_available_mb": 100817.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.1, "peak": 15.44, "min": 14.43}, "VDD_CPU_SOC_MSS": {"avg": 15.66, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 29.59, "peak": 40.18, "min": 23.64}, "VIN": {"avg": 72.73, "peak": 112.5, "min": 61.38}}, "power_watts_avg": 29.59, "energy_joules_est": 73.57, "sample_count": 19, "duration_seconds": 2.486}, "timestamp": "2026-01-17T15:22:45.408598"}
{"image_index": 283, "image_name": "000000030828.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000030828.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2104.529, "latencies_ms": [2104.529], "images_per_second": 0.475, "prompt_tokens": 19, "response_tokens_est": 54, "n_tiles": 6, "output_text": "The image depicts a bench in a park with a blue and red blanket draped over it. The bench is made of metal, and the blanket is orange. The lighting is natural, suggesting it is daytime, and the weather appears to be clear and sunny.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24955.0, "ram_available_mb": 100817.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 24955.2, "ram_available_mb": 100816.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.03, "peak": 15.44, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.65, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 30.85, "peak": 40.97, "min": 24.03}, "VIN": {"avg": 70.19, "peak": 113.23, "min": 50.55}}, "power_watts_avg": 30.85, "energy_joules_est": 64.93, "sample_count": 16, "duration_seconds": 2.105}, "timestamp": "2026-01-17T15:22:47.519052"}
{"image_index": 284, "image_name": "000000031050.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031050.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1981.616, "latencies_ms": [1981.616], "images_per_second": 0.505, "prompt_tokens": 9, "response_tokens_est": 51, "n_tiles": 6, "output_text": "The image shows a collection of decorative items displayed on a wall, including a large, colorful vase with a blue and yellow pattern, a small, decorative plant, and various other objects that appear to be part of an art exhibit or museum display.", "error": null, "sys_before": {"cpu_percent": 41.3, "ram_used_mb": 24955.2, "ram_available_mb": 100816.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 24956.5, "ram_available_mb": 100815.7, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.83, "peak": 15.14, "min": 13.9}, "VDD_CPU_SOC_MSS": {"avg": 15.5, "peak": 16.14, "min": 14.17}, "VDD_GPU": {"avg": 30.89, "peak": 40.57, "min": 24.03}, "VIN": {"avg": 72.26, "peak": 112.39, "min": 55.37}}, "power_watts_avg": 30.89, "energy_joules_est": 61.23, "sample_count": 14, "duration_seconds": 1.982}, "timestamp": "2026-01-17T15:22:49.565299"}
{"image_index": 284, "image_name": "000000031050.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031050.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1728.84, "latencies_ms": [1728.84], "images_per_second": 0.578, "prompt_tokens": 23, "response_tokens_est": 40, "n_tiles": 6, "output_text": "object: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24956.5, "ram_available_mb": 100815.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 24957.5, "ram_available_mb": 100814.7, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.0, "peak": 15.44, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.6, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 32.33, "peak": 40.18, "min": 24.83}, "VIN": {"avg": 73.66, "peak": 106.87, "min": 62.37}}, "power_watts_avg": 32.33, "energy_joules_est": 55.91, "sample_count": 13, "duration_seconds": 1.729}, "timestamp": "2026-01-17T15:22:51.300257"}
{"image_index": 284, "image_name": "000000031050.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031050.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2538.525, "latencies_ms": [2538.525], "images_per_second": 0.394, "prompt_tokens": 27, "response_tokens_est": 70, "n_tiles": 6, "output_text": "The main object in the foreground is a large, colorful vase with a blue and yellow pattern. It is placed on a white pedestal. To the left of the vase, there is a small, dark-colored vase. In the background, there is a brown wall with various decorative items, including a small sculpture and a plant.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24957.5, "ram_available_mb": 100814.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 24957.2, "ram_available_mb": 100815.0, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.11, "peak": 15.54, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.69, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 29.49, "peak": 40.56, "min": 23.64}, "VIN": {"avg": 68.29, "peak": 93.4, "min": 58.17}}, "power_watts_avg": 29.49, "energy_joules_est": 74.87, "sample_count": 20, "duration_seconds": 2.539}, "timestamp": "2026-01-17T15:22:53.845007"}
{"image_index": 284, "image_name": "000000031050.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031050.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3426.979, "latencies_ms": [3426.979], "images_per_second": 0.292, "prompt_tokens": 21, "response_tokens_est": 103, "n_tiles": 6, "output_text": "The image depicts an indoor setting, likely a museum or gallery, showcasing various decorative items. The focal point is a large, ornate vase with a blue and yellow pattern, placed on a white pedestal. To the left, there is a smaller, dark-colored vase with a similar design. The background features a wall with a brownish hue and a few small, white informational plaques. The overall atmosphere suggests a display of art and artifacts, possibly related to cultural or historical themes.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24957.2, "ram_available_mb": 100815.0, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 24957.7, "ram_available_mb": 100814.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.12, "peak": 15.44, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.79, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 27.88, "peak": 40.56, "min": 23.25}, "VIN": {"avg": 69.07, "peak": 108.11, "min": 53.91}}, "power_watts_avg": 27.88, "energy_joules_est": 95.56, "sample_count": 27, "duration_seconds": 3.427}, "timestamp": "2026-01-17T15:22:57.278151"}
{"image_index": 284, "image_name": "000000031050.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031050.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2185.716, "latencies_ms": [2185.716], "images_per_second": 0.458, "prompt_tokens": 19, "response_tokens_est": 57, "n_tiles": 6, "output_text": "The image showcases a collection of art pieces, including a strikingly colorful vase with blue and yellow hues, a rustic wooden sculpture, and a dark, metallic sculpture. The lighting is soft and warm, casting gentle shadows that enhance the textures and colors of the objects.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24957.7, "ram_available_mb": 100814.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 24957.7, "ram_available_mb": 100814.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.05, "peak": 15.44, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.7, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 30.33, "peak": 40.56, "min": 23.64}, "VIN": {"avg": 71.33, "peak": 105.17, "min": 62.5}}, "power_watts_avg": 30.33, "energy_joules_est": 66.3, "sample_count": 17, "duration_seconds": 2.186}, "timestamp": "2026-01-17T15:22:59.474030"}
{"image_index": 285, "image_name": "000000031093.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031093.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1558.602, "latencies_ms": [1558.602], "images_per_second": 0.642, "prompt_tokens": 9, "response_tokens_est": 34, "n_tiles": 6, "output_text": "A skateboarder is performing a trick on a curved ramp at a skate park, wearing a white t-shirt, blue jeans, and a helmet with protective gear.", "error": null, "sys_before": {"cpu_percent": 42.3, "ram_used_mb": 24957.7, "ram_available_mb": 100814.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 24957.7, "ram_available_mb": 100814.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.67, "peak": 15.14, "min": 13.79}, "VDD_CPU_SOC_MSS": {"avg": 15.39, "peak": 16.14, "min": 14.57}, "VDD_GPU": {"avg": 31.94, "peak": 40.18, "min": 24.82}, "VIN": {"avg": 76.96, "peak": 107.02, "min": 61.48}}, "power_watts_avg": 31.94, "energy_joules_est": 49.8, "sample_count": 12, "duration_seconds": 1.559}, "timestamp": "2026-01-17T15:23:01.095596"}
{"image_index": 285, "image_name": "000000031093.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031093.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1800.296, "latencies_ms": [1800.296], "images_per_second": 0.555, "prompt_tokens": 23, "response_tokens_est": 41, "n_tiles": 6, "output_text": "1. Skateboarder\n2. Helmet\n3. T-shirt\n4. Gloves\n5. Pads\n6. Leggings\n7. Shoes\n8. Ramp", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24957.7, "ram_available_mb": 100814.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 24958.4, "ram_available_mb": 100813.7, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.8, "peak": 15.24, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.32, "peak": 15.75, "min": 14.56}, "VDD_GPU": {"avg": 32.17, "peak": 40.56, "min": 24.42}, "VIN": {"avg": 70.83, "peak": 92.83, "min": 63.34}}, "power_watts_avg": 32.17, "energy_joules_est": 57.93, "sample_count": 13, "duration_seconds": 1.801}, "timestamp": "2026-01-17T15:23:02.902186"}
{"image_index": 285, "image_name": "000000031093.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031093.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3591.641, "latencies_ms": [3591.641], "images_per_second": 0.278, "prompt_tokens": 27, "response_tokens_est": 104, "n_tiles": 6, "output_text": "The main object in the foreground is a skateboarder performing a trick on a concrete ramp. The skateboarder is wearing a white t-shirt, blue jeans, and a helmet. The skateboard is in motion, with the skateboarder's feet on the board. In the background, there is another person standing on the ramp, and a person is working on a machine near the ramp. The skateboarder is positioned near the center of the image, while the background elements are slightly out of focus.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24958.4, "ram_available_mb": 100813.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 24958.2, "ram_available_mb": 100814.0, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.88, "peak": 15.24, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.45, "peak": 15.75, "min": 14.56}, "VDD_GPU": {"avg": 27.33, "peak": 40.95, "min": 22.85}, "VIN": {"avg": 67.92, "peak": 108.27, "min": 52.98}}, "power_watts_avg": 27.33, "energy_joules_est": 98.17, "sample_count": 28, "duration_seconds": 3.592}, "timestamp": "2026-01-17T15:23:06.499518"}
{"image_index": 285, "image_name": "000000031093.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031093.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3068.678, "latencies_ms": [3068.678], "images_per_second": 0.326, "prompt_tokens": 21, "response_tokens_est": 86, "n_tiles": 6, "output_text": "The image captures a skateboarder performing a trick in a concrete bowl at a skate park during twilight. The skateboarder is wearing a white t-shirt, blue jeans, and a helmet, and is in mid-air, showcasing a dynamic and athletic posture. The setting is a well-maintained skate park with a smooth, curved concrete bowl, and the lighting suggests it is either early morning or late afternoon.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24958.4, "ram_available_mb": 100813.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 24958.7, "ram_available_mb": 100813.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.9, "peak": 15.24, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.53, "peak": 15.75, "min": 14.56}, "VDD_GPU": {"avg": 28.07, "peak": 40.16, "min": 22.85}, "VIN": {"avg": 67.7, "peak": 95.95, "min": 57.17}}, "power_watts_avg": 28.07, "energy_joules_est": 86.15, "sample_count": 23, "duration_seconds": 3.069}, "timestamp": "2026-01-17T15:23:09.574885"}
{"image_index": 285, "image_name": "000000031093.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031093.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1995.92, "latencies_ms": [1995.92], "images_per_second": 0.501, "prompt_tokens": 19, "response_tokens_est": 48, "n_tiles": 6, "output_text": "The skateboarder is wearing a white t-shirt, blue jeans, and white sneakers. The skateboard is black with white wheels. The setting is outdoors during the evening, with the sun casting a warm glow on the scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24958.7, "ram_available_mb": 100813.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 24958.9, "ram_available_mb": 100813.3, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.84, "peak": 15.24, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.38, "peak": 15.75, "min": 14.57}, "VDD_GPU": {"avg": 30.78, "peak": 40.18, "min": 23.64}, "VIN": {"avg": 70.63, "peak": 106.96, "min": 61.66}}, "power_watts_avg": 30.78, "energy_joules_est": 61.44, "sample_count": 15, "duration_seconds": 1.996}, "timestamp": "2026-01-17T15:23:11.576825"}
{"image_index": 286, "image_name": "000000031118.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031118.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1818.445, "latencies_ms": [1818.445], "images_per_second": 0.55, "prompt_tokens": 9, "response_tokens_est": 45, "n_tiles": 6, "output_text": "The image depicts a bustling city street at night, illuminated by a variety of colorful lights, including a large Christmas tree adorned with white and blue lights, and a historic clock tower with a clock face.", "error": null, "sys_before": {"cpu_percent": 39.2, "ram_used_mb": 24958.9, "ram_available_mb": 100813.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 24959.1, "ram_available_mb": 100813.0, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.75, "peak": 15.11, "min": 13.69}, "VDD_CPU_SOC_MSS": {"avg": 15.38, "peak": 15.75, "min": 14.17}, "VDD_GPU": {"avg": 31.09, "peak": 40.16, "min": 24.42}, "VIN": {"avg": 73.82, "peak": 108.3, "min": 57.03}}, "power_watts_avg": 31.09, "energy_joules_est": 56.55, "sample_count": 14, "duration_seconds": 1.819}, "timestamp": "2026-01-17T15:23:13.457353"}
{"image_index": 286, "image_name": "000000031118.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031118.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1730.766, "latencies_ms": [1730.766], "images_per_second": 0.578, "prompt_tokens": 23, "response_tokens_est": 40, "n_tiles": 6, "output_text": "object: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24959.1, "ram_available_mb": 100813.0, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 24959.6, "ram_available_mb": 100812.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.01, "peak": 15.54, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.6, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 32.09, "peak": 40.16, "min": 24.82}, "VIN": {"avg": 72.65, "peak": 101.02, "min": 62.26}}, "power_watts_avg": 32.09, "energy_joules_est": 55.55, "sample_count": 13, "duration_seconds": 1.731}, "timestamp": "2026-01-17T15:23:15.194311"}
{"image_index": 286, "image_name": "000000031118.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031118.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2779.539, "latencies_ms": [2779.539], "images_per_second": 0.36, "prompt_tokens": 27, "response_tokens_est": 79, "n_tiles": 6, "output_text": "The main objects in the image are a large Christmas tree and a clock tower. The Christmas tree is situated in the foreground, while the clock tower is in the background. The tree is decorated with white lights and blue ornaments, and it is positioned near the clock tower. The clock tower is illuminated with white lights, and it is situated on the left side of the image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24959.6, "ram_available_mb": 100812.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 24959.4, "ram_available_mb": 100812.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.09, "peak": 15.44, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.68, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 28.91, "peak": 40.95, "min": 23.64}, "VIN": {"avg": 68.17, "peak": 85.98, "min": 55.52}}, "power_watts_avg": 28.91, "energy_joules_est": 80.38, "sample_count": 22, "duration_seconds": 2.78}, "timestamp": "2026-01-17T15:23:17.980445"}
{"image_index": 286, "image_name": "000000031118.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031118.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2509.878, "latencies_ms": [2509.878], "images_per_second": 0.398, "prompt_tokens": 21, "response_tokens_est": 69, "n_tiles": 6, "output_text": "The image depicts a bustling city street at night, illuminated by a variety of lights, including string lights and street lamps. A large Christmas tree adorned with white lights stands prominently in the center, surrounded by people and shops. The scene is lively and festive, with the tree and lights creating a warm and inviting atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24959.4, "ram_available_mb": 100812.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 24959.4, "ram_available_mb": 100812.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.1, "peak": 15.44, "min": 14.43}, "VDD_CPU_SOC_MSS": {"avg": 15.67, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 29.54, "peak": 40.18, "min": 23.64}, "VIN": {"avg": 74.27, "peak": 121.1, "min": 62.62}}, "power_watts_avg": 29.54, "energy_joules_est": 74.16, "sample_count": 19, "duration_seconds": 2.51}, "timestamp": "2026-01-17T15:23:20.496756"}
{"image_index": 286, "image_name": "000000031118.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031118.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2700.686, "latencies_ms": [2700.686], "images_per_second": 0.37, "prompt_tokens": 19, "response_tokens_est": 76, "n_tiles": 6, "output_text": "The image depicts a festive scene at night, featuring a large Christmas tree adorned with white lights and blue lights. The tree is situated in front of a historic building with a clock tower, illuminated by warm, yellow lights. The ground appears wet, suggesting recent rain, and the overall atmosphere is bright and cheerful, with a mix of traditional and modern elements.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24959.4, "ram_available_mb": 100812.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 24959.4, "ram_available_mb": 100812.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.08, "peak": 15.44, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.65, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 28.93, "peak": 40.57, "min": 23.64}, "VIN": {"avg": 69.8, "peak": 105.19, "min": 58.15}}, "power_watts_avg": 28.93, "energy_joules_est": 78.14, "sample_count": 21, "duration_seconds": 2.701}, "timestamp": "2026-01-17T15:23:23.203611"}
{"image_index": 287, "image_name": "000000031217.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031217.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1304.385, "latencies_ms": [1304.385], "images_per_second": 0.767, "prompt_tokens": 9, "response_tokens_est": 26, "n_tiles": 6, "output_text": "A young man is playing tennis on a court, with a net separating him from the background of trees and foliage.", "error": null, "sys_before": {"cpu_percent": 39.7, "ram_used_mb": 24959.4, "ram_available_mb": 100812.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 24959.6, "ram_available_mb": 100812.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.66, "peak": 15.14, "min": 13.79}, "VDD_CPU_SOC_MSS": {"avg": 15.35, "peak": 16.14, "min": 14.57}, "VDD_GPU": {"avg": 33.45, "peak": 40.18, "min": 26.79}, "VIN": {"avg": 79.16, "peak": 117.61, "min": 62.76}}, "power_watts_avg": 33.45, "energy_joules_est": 43.65, "sample_count": 10, "duration_seconds": 1.305}, "timestamp": "2026-01-17T15:23:24.569998"}
{"image_index": 287, "image_name": "000000031217.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031217.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1675.527, "latencies_ms": [1675.527], "images_per_second": 0.597, "prompt_tokens": 23, "response_tokens_est": 38, "n_tiles": 6, "output_text": "1. Tennis player\n2. Tennis racket\n3. Tennis ball\n4. Tennis court\n5. Fence\n6. Trees\n7. Net\n8. Person", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24959.6, "ram_available_mb": 100812.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 24959.4, "ram_available_mb": 100812.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.98, "peak": 15.44, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.55, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 33.58, "peak": 41.74, "min": 25.61}, "VIN": {"avg": 72.95, "peak": 104.21, "min": 63.69}}, "power_watts_avg": 33.58, "energy_joules_est": 56.29, "sample_count": 12, "duration_seconds": 1.676}, "timestamp": "2026-01-17T15:23:26.253470"}
{"image_index": 287, "image_name": "000000031217.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031217.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2758.643, "latencies_ms": [2758.643], "images_per_second": 0.362, "prompt_tokens": 27, "response_tokens_est": 78, "n_tiles": 6, "output_text": "The main object in the foreground is a young male tennis player, dressed in a blue shirt and black shorts, holding a tennis racket. He is positioned on a tennis court, with a chain-link fence and trees in the background. The tennis ball is in the air, slightly to the right of the player, indicating that he is in the middle of a serve or a return.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24959.4, "ram_available_mb": 100812.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 24959.6, "ram_available_mb": 100812.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.08, "peak": 15.54, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.65, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 29.06, "peak": 40.56, "min": 23.64}, "VIN": {"avg": 68.72, "peak": 97.65, "min": 57.37}}, "power_watts_avg": 29.06, "energy_joules_est": 80.17, "sample_count": 21, "duration_seconds": 2.759}, "timestamp": "2026-01-17T15:23:29.018028"}
{"image_index": 287, "image_name": "000000031217.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031217.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2073.352, "latencies_ms": [2073.352], "images_per_second": 0.482, "prompt_tokens": 21, "response_tokens_est": 53, "n_tiles": 6, "output_text": "The image depicts a young male tennis player in a blue shirt and black shorts, actively engaged in a game of tennis on a court. The court is surrounded by a chain-link fence and a dense forest in the background, indicating a secluded outdoor setting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24959.6, "ram_available_mb": 100812.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 24959.9, "ram_available_mb": 100812.3, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.06, "peak": 15.44, "min": 14.43}, "VDD_CPU_SOC_MSS": {"avg": 15.62, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 30.88, "peak": 40.97, "min": 24.04}, "VIN": {"avg": 71.52, "peak": 112.33, "min": 55.61}}, "power_watts_avg": 30.88, "energy_joules_est": 64.04, "sample_count": 16, "duration_seconds": 2.074}, "timestamp": "2026-01-17T15:23:31.097501"}
{"image_index": 287, "image_name": "000000031217.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031217.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2569.475, "latencies_ms": [2569.475], "images_per_second": 0.389, "prompt_tokens": 19, "response_tokens_est": 71, "n_tiles": 6, "output_text": "The image depicts a young tennis player in a blue shirt and black shorts, actively engaged in a game on a gray tennis court. The court is surrounded by a chain-link fence, and the background features dense greenery, indicating a park or recreational area. The lighting is natural, suggesting daytime, and the weather appears to be clear and sunny.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24959.9, "ram_available_mb": 100812.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 24959.8, "ram_available_mb": 100812.3, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.08, "peak": 15.44, "min": 14.43}, "VDD_CPU_SOC_MSS": {"avg": 15.69, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 29.37, "peak": 40.56, "min": 23.64}, "VIN": {"avg": 70.1, "peak": 97.03, "min": 55.66}}, "power_watts_avg": 29.37, "energy_joules_est": 75.48, "sample_count": 20, "duration_seconds": 2.57}, "timestamp": "2026-01-17T15:23:33.673067"}
{"image_index": 288, "image_name": "000000031248.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031248.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1816.107, "latencies_ms": [1816.107], "images_per_second": 0.551, "prompt_tokens": 9, "response_tokens_est": 45, "n_tiles": 6, "output_text": "The image depicts a well-appointed living room with a classic fireplace, a plush armchair, and a small side table with a lamp, all set against a backdrop of bookshelves filled with books.", "error": null, "sys_before": {"cpu_percent": 45.0, "ram_used_mb": 24960.3, "ram_available_mb": 100811.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 24960.3, "ram_available_mb": 100811.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.83, "peak": 15.14, "min": 14.1}, "VDD_CPU_SOC_MSS": {"avg": 15.55, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 31.12, "peak": 39.77, "min": 24.42}, "VIN": {"avg": 76.77, "peak": 114.82, "min": 57.71}}, "power_watts_avg": 31.12, "energy_joules_est": 56.54, "sample_count": 14, "duration_seconds": 1.817}, "timestamp": "2026-01-17T15:23:35.534247"}
{"image_index": 288, "image_name": "000000031248.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031248.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1725.871, "latencies_ms": [1725.871], "images_per_second": 0.579, "prompt_tokens": 23, "response_tokens_est": 40, "n_tiles": 6, "output_text": "object: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24960.3, "ram_available_mb": 100811.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 24960.8, "ram_available_mb": 100811.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.03, "peak": 15.34, "min": 14.43}, "VDD_CPU_SOC_MSS": {"avg": 15.63, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 32.36, "peak": 40.97, "min": 24.82}, "VIN": {"avg": 75.95, "peak": 109.78, "min": 63.49}}, "power_watts_avg": 32.36, "energy_joules_est": 55.87, "sample_count": 13, "duration_seconds": 1.727}, "timestamp": "2026-01-17T15:23:37.266469"}
{"image_index": 288, "image_name": "000000031248.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031248.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2328.771, "latencies_ms": [2328.771], "images_per_second": 0.429, "prompt_tokens": 27, "response_tokens_est": 62, "n_tiles": 6, "output_text": "The main objects in the image are a bookshelf filled with books, a fireplace, a chair, and a lamp. The bookshelf is located in the background, while the chair and lamp are in the foreground. The bookshelf is near the fireplace, and the chair is positioned close to the lamp.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24960.8, "ram_available_mb": 100811.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 24961.8, "ram_available_mb": 100810.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.08, "peak": 15.44, "min": 14.43}, "VDD_CPU_SOC_MSS": {"avg": 15.64, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 30.16, "peak": 40.57, "min": 23.64}, "VIN": {"avg": 70.98, "peak": 107.87, "min": 62.12}}, "power_watts_avg": 30.16, "energy_joules_est": 70.26, "sample_count": 18, "duration_seconds": 2.329}, "timestamp": "2026-01-17T15:23:39.601644"}
{"image_index": 288, "image_name": "000000031248.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031248.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2424.926, "latencies_ms": [2424.926], "images_per_second": 0.412, "prompt_tokens": 21, "response_tokens_est": 66, "n_tiles": 6, "output_text": "The image depicts a cozy, well-appointed living room with a classic and elegant decor. The room features a white fireplace with a green marble surround, a plush armchair, a small side table with a lamp, and a potted plant. The setting suggests a comfortable and inviting space for relaxation and conversation.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24961.8, "ram_available_mb": 100810.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 24962.1, "ram_available_mb": 100810.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.08, "peak": 15.44, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.66, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 29.94, "peak": 40.18, "min": 23.64}, "VIN": {"avg": 71.52, "peak": 122.05, "min": 56.9}}, "power_watts_avg": 29.94, "energy_joules_est": 72.62, "sample_count": 18, "duration_seconds": 2.425}, "timestamp": "2026-01-17T15:23:42.036772"}
{"image_index": 288, "image_name": "000000031248.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031248.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2133.568, "latencies_ms": [2133.568], "images_per_second": 0.469, "prompt_tokens": 19, "response_tokens_est": 55, "n_tiles": 6, "output_text": "The room features a classic and elegant design with a white fireplace, marble mantel, and a white mantelpiece. The room is well-lit with natural light, and the furniture and decor are made of high-quality materials, including wood, marble, and fabric.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24962.1, "ram_available_mb": 100810.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 24962.0, "ram_available_mb": 100810.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.06, "peak": 15.44, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.62, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 30.6, "peak": 40.56, "min": 24.03}, "VIN": {"avg": 71.71, "peak": 111.0, "min": 47.92}}, "power_watts_avg": 30.6, "energy_joules_est": 65.3, "sample_count": 16, "duration_seconds": 2.134}, "timestamp": "2026-01-17T15:23:44.176680"}
{"image_index": 289, "image_name": "000000031269.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031269.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2725.874, "latencies_ms": [2725.874], "images_per_second": 0.367, "prompt_tokens": 9, "response_tokens_est": 33, "n_tiles": 12, "output_text": "The image depicts a group of zebras grazing in a grassy field, with their distinctive black and white striped patterns visible on their bodies.", "error": null, "sys_before": {"cpu_percent": 40.8, "ram_used_mb": 24921.6, "ram_available_mb": 100850.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 24922.8, "ram_available_mb": 100849.3, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.23, "peak": 16.66, "min": 13.79}, "VDD_CPU_SOC_MSS": {"avg": 15.99, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 35.49, "peak": 44.1, "min": 26.79}, "VIN": {"avg": 78.63, "peak": 112.18, "min": 65.12}}, "power_watts_avg": 35.49, "energy_joules_est": 96.75, "sample_count": 21, "duration_seconds": 2.726}, "timestamp": "2026-01-17T15:23:46.981580"}
{"image_index": 289, "image_name": "000000031269.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031269.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2248.98, "latencies_ms": [2248.98], "images_per_second": 0.445, "prompt_tokens": 23, "response_tokens_est": 17, "n_tiles": 12, "output_text": "zebra: 2\ngrass: 1\ntrees: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24922.8, "ram_available_mb": 100849.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.3, "ram_used_mb": 24923.3, "ram_available_mb": 100848.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.3, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.88, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 38.6, "peak": 45.3, "min": 30.33}, "VIN": {"avg": 85.82, "peak": 128.13, "min": 66.21}}, "power_watts_avg": 38.6, "energy_joules_est": 86.84, "sample_count": 17, "duration_seconds": 2.25}, "timestamp": "2026-01-17T15:23:49.238599"}
{"image_index": 289, "image_name": "000000031269.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031269.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 4584.395, "latencies_ms": [4584.395], "images_per_second": 0.218, "prompt_tokens": 27, "response_tokens_est": 86, "n_tiles": 12, "output_text": "The main objects in the image are two zebras. The foreground features a zebra with a prominent black and white striped pattern, standing in a muddy patch. The background shows another zebra, slightly out of focus, grazing in the same area. The zebra in the foreground is closer to the viewer, while the one in the background is further away, creating a sense of depth in the image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24923.3, "ram_available_mb": 100848.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 24923.8, "ram_available_mb": 100848.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.19, "peak": 16.66, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.64, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 32.54, "peak": 45.28, "min": 26.0}, "VIN": {"avg": 74.89, "peak": 117.31, "min": 59.18}}, "power_watts_avg": 32.54, "energy_joules_est": 149.19, "sample_count": 36, "duration_seconds": 4.585}, "timestamp": "2026-01-17T15:23:53.829589"}
{"image_index": 289, "image_name": "000000031269.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031269.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3125.3, "latencies_ms": [3125.3], "images_per_second": 0.32, "prompt_tokens": 21, "response_tokens_est": 43, "n_tiles": 12, "output_text": "The image depicts a serene savanna landscape with two zebras grazing in a field. The zebras are surrounded by tall grasses and scattered trees, creating a natural and peaceful environment.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24923.8, "ram_available_mb": 100848.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 24924.6, "ram_available_mb": 100847.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.23, "peak": 16.66, "min": 14.12}, "VDD_CPU_SOC_MSS": {"avg": 15.79, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 35.24, "peak": 44.91, "min": 26.39}, "VIN": {"avg": 78.31, "peak": 111.76, "min": 55.51}}, "power_watts_avg": 35.24, "energy_joules_est": 110.15, "sample_count": 24, "duration_seconds": 3.126}, "timestamp": "2026-01-17T15:23:56.961368"}
{"image_index": 289, "image_name": "000000031269.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031269.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3641.339, "latencies_ms": [3641.339], "images_per_second": 0.275, "prompt_tokens": 19, "response_tokens_est": 58, "n_tiles": 12, "output_text": "The image depicts a scene of two zebras grazing in a savanna. The zebras have distinctive black and white stripes, and the lighting is bright, indicating a sunny day. The grass is tall and dry, and the ground is covered in patches of dirt.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24924.6, "ram_available_mb": 100847.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 24924.6, "ram_available_mb": 100847.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.2, "peak": 16.66, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.77, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 33.9, "peak": 44.91, "min": 25.6}, "VIN": {"avg": 76.38, "peak": 118.31, "min": 54.22}}, "power_watts_avg": 33.9, "energy_joules_est": 123.46, "sample_count": 28, "duration_seconds": 3.642}, "timestamp": "2026-01-17T15:24:00.609193"}
{"image_index": 290, "image_name": "000000031296.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031296.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1548.155, "latencies_ms": [1548.155], "images_per_second": 0.646, "prompt_tokens": 9, "response_tokens_est": 35, "n_tiles": 6, "output_text": "The image depicts a group of people seated around a long table in a restaurant, with various items on the table, including drinks, condiments, and a clock.", "error": null, "sys_before": {"cpu_percent": 37.5, "ram_used_mb": 24951.2, "ram_available_mb": 100821.0, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 24951.4, "ram_available_mb": 100820.7, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.67, "peak": 15.11, "min": 13.72}, "VDD_CPU_SOC_MSS": {"avg": 15.32, "peak": 15.75, "min": 14.17}, "VDD_GPU": {"avg": 32.59, "peak": 40.56, "min": 25.21}, "VIN": {"avg": 76.47, "peak": 114.9, "min": 48.96}}, "power_watts_avg": 32.59, "energy_joules_est": 50.47, "sample_count": 12, "duration_seconds": 1.549}, "timestamp": "2026-01-17T15:24:02.212974"}
{"image_index": 290, "image_name": "000000031296.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031296.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 4113.917, "latencies_ms": [4113.917], "images_per_second": 0.243, "prompt_tokens": 23, "response_tokens_est": 129, "n_tiles": 6, "output_text": "- table: 8\n- chairs: 8\n- people: 10\n- tablecloth: 1\n- napkins: 2\n- cups: 2\n- plates: 2\n- condiments: 2\n- cups of coffee: 2\n- cups of tea: 2\n- napkins: 2\n- plates: 2\n- cups: 2\n- condiments: 2\n- napkins: 2\n- cups: 2\n- condiments: 2\n- napkins: 2\n- cups: 2\n-", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24951.4, "ram_available_mb": 100820.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 24951.9, "ram_available_mb": 100820.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.14, "peak": 15.44, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.7, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 27.33, "peak": 40.97, "min": 23.64}, "VIN": {"avg": 68.96, "peak": 95.43, "min": 64.08}}, "power_watts_avg": 27.33, "energy_joules_est": 112.45, "sample_count": 32, "duration_seconds": 4.114}, "timestamp": "2026-01-17T15:24:06.333284"}
{"image_index": 290, "image_name": "000000031296.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031296.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2948.165, "latencies_ms": [2948.165], "images_per_second": 0.339, "prompt_tokens": 27, "response_tokens_est": 85, "n_tiles": 6, "output_text": "In the image, the main objects are a group of people seated around a long wooden table in a restaurant. The table is positioned in the foreground, with the people seated around it. The background shows other tables and chairs, as well as a counter with various items. The people are positioned near the counter, with some standing and others seated. The overall scene is set in a restaurant with a warm and inviting atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24951.9, "ram_available_mb": 100820.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 24952.4, "ram_available_mb": 100819.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.11, "peak": 15.44, "min": 14.43}, "VDD_CPU_SOC_MSS": {"avg": 15.63, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 28.6, "peak": 40.95, "min": 23.64}, "VIN": {"avg": 70.41, "peak": 113.87, "min": 63.19}}, "power_watts_avg": 28.6, "energy_joules_est": 84.33, "sample_count": 23, "duration_seconds": 2.949}, "timestamp": "2026-01-17T15:24:09.287217"}
{"image_index": 290, "image_name": "000000031296.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031296.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2456.044, "latencies_ms": [2456.044], "images_per_second": 0.407, "prompt_tokens": 21, "response_tokens_est": 67, "n_tiles": 6, "output_text": "The image depicts a cozy, dimly lit restaurant with a group of people gathered around a long table. The setting appears to be a casual dining area, with various individuals seated and engaged in conversation. The atmosphere is relaxed, and the room is decorated with warm lighting and rustic elements, creating a welcoming environment for the patrons.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24952.4, "ram_available_mb": 100819.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 24952.9, "ram_available_mb": 100819.3, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.08, "peak": 15.44, "min": 14.43}, "VDD_CPU_SOC_MSS": {"avg": 15.64, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 29.48, "peak": 40.16, "min": 23.64}, "VIN": {"avg": 69.69, "peak": 108.46, "min": 61.63}}, "power_watts_avg": 29.48, "energy_joules_est": 72.42, "sample_count": 19, "duration_seconds": 2.457}, "timestamp": "2026-01-17T15:24:11.749524"}
{"image_index": 290, "image_name": "000000031296.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031296.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2108.622, "latencies_ms": [2108.622], "images_per_second": 0.474, "prompt_tokens": 19, "response_tokens_est": 54, "n_tiles": 6, "output_text": "The image depicts a warmly lit, rustic restaurant with wooden floors and brick walls. The lighting is soft and warm, creating a cozy atmosphere. The wooden tables and chairs, along with the red and black chairs, add a touch of color to the scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24952.9, "ram_available_mb": 100819.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 24953.2, "ram_available_mb": 100819.0, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.04, "peak": 15.44, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.6, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 30.62, "peak": 40.16, "min": 24.03}, "VIN": {"avg": 70.69, "peak": 113.82, "min": 54.93}}, "power_watts_avg": 30.62, "energy_joules_est": 64.58, "sample_count": 16, "duration_seconds": 2.109}, "timestamp": "2026-01-17T15:24:13.864347"}
{"image_index": 291, "image_name": "000000031322.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031322.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1356.161, "latencies_ms": [1356.161], "images_per_second": 0.737, "prompt_tokens": 9, "response_tokens_est": 28, "n_tiles": 6, "output_text": "A group of white swans is swimming in a calm body of water, with a few of them standing on the water's surface.", "error": null, "sys_before": {"cpu_percent": 41.7, "ram_used_mb": 24953.2, "ram_available_mb": 100819.0, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 24952.9, "ram_available_mb": 100819.3, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.7, "peak": 15.14, "min": 14.0}, "VDD_CPU_SOC_MSS": {"avg": 15.39, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 33.83, "peak": 40.16, "min": 27.18}, "VIN": {"avg": 76.57, "peak": 103.47, "min": 58.52}}, "power_watts_avg": 33.83, "energy_joules_est": 45.9, "sample_count": 10, "duration_seconds": 1.357}, "timestamp": "2026-01-17T15:24:15.275039"}
{"image_index": 291, "image_name": "000000031322.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031322.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 809.357, "latencies_ms": [809.357], "images_per_second": 1.236, "prompt_tokens": 23, "response_tokens_est": 6, "n_tiles": 6, "output_text": "swan: 8", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24952.9, "ram_available_mb": 100819.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.3, "ram_used_mb": 24953.6, "ram_available_mb": 100818.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.83, "peak": 15.44, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.35, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 38.44, "peak": 40.56, "min": 33.09}, "VIN": {"avg": 76.56, "peak": 95.74, "min": 62.35}}, "power_watts_avg": 38.44, "energy_joules_est": 31.12, "sample_count": 5, "duration_seconds": 0.81}, "timestamp": "2026-01-17T15:24:16.090444"}
{"image_index": 291, "image_name": "000000031322.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031322.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2353.333, "latencies_ms": [2353.333], "images_per_second": 0.425, "prompt_tokens": 27, "response_tokens_est": 63, "n_tiles": 6, "output_text": "The main objects in the image are a group of white swans swimming in a body of water. The foreground features a single white swan, while the background is filled with more swans and boats. The water is calm, and the scene is set against a backdrop of a dock and a clear sky.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24953.6, "ram_available_mb": 100818.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 24953.9, "ram_available_mb": 100818.3, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.05, "peak": 15.34, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.66, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 31.66, "peak": 43.71, "min": 23.64}, "VIN": {"avg": 71.3, "peak": 121.28, "min": 63.35}}, "power_watts_avg": 31.66, "energy_joules_est": 74.53, "sample_count": 18, "duration_seconds": 2.354}, "timestamp": "2026-01-17T15:24:18.450626"}
{"image_index": 291, "image_name": "000000031322.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031322.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1808.488, "latencies_ms": [1808.488], "images_per_second": 0.553, "prompt_tokens": 21, "response_tokens_est": 43, "n_tiles": 6, "output_text": "The image depicts a serene marina scene with several white swans swimming in a calm body of water. The setting is peaceful, with the swans peacefully floating and enjoying the tranquil environment.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24953.9, "ram_available_mb": 100818.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 24953.9, "ram_available_mb": 100818.3, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.04, "peak": 15.44, "min": 14.43}, "VDD_CPU_SOC_MSS": {"avg": 15.66, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 31.79, "peak": 40.97, "min": 24.43}, "VIN": {"avg": 72.98, "peak": 113.09, "min": 60.08}}, "power_watts_avg": 31.79, "energy_joules_est": 57.51, "sample_count": 14, "duration_seconds": 1.809}, "timestamp": "2026-01-17T15:24:20.265690"}
{"image_index": 291, "image_name": "000000031322.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031322.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2728.953, "latencies_ms": [2728.953], "images_per_second": 0.366, "prompt_tokens": 19, "response_tokens_est": 77, "n_tiles": 6, "output_text": "The image depicts a serene scene of a group of white swans swimming in a calm body of water. The lighting is soft and warm, suggesting either early morning or late afternoon, with the sun casting a gentle glow on the water and the swans. The overall atmosphere is peaceful and tranquil, with the swans appearing to be at ease in their natural habitat.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24953.9, "ram_available_mb": 100818.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 24954.1, "ram_available_mb": 100818.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.07, "peak": 15.54, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.9, "peak": 16.53, "min": 14.96}, "VDD_GPU": {"avg": 28.96, "peak": 40.56, "min": 23.64}, "VIN": {"avg": 70.49, "peak": 113.84, "min": 60.92}}, "power_watts_avg": 28.96, "energy_joules_est": 79.04, "sample_count": 21, "duration_seconds": 2.729}, "timestamp": "2026-01-17T15:24:23.000777"}
{"image_index": 292, "image_name": "000000031620.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031620.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2323.885, "latencies_ms": [2323.885], "images_per_second": 0.43, "prompt_tokens": 9, "response_tokens_est": 21, "n_tiles": 12, "output_text": "A bride and groom are cutting a cake together in a decorated room with a white canopy overhead.", "error": null, "sys_before": {"cpu_percent": 43.4, "ram_used_mb": 24922.8, "ram_available_mb": 100849.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.4, "ram_used_mb": 24924.3, "ram_available_mb": 100847.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.2, "peak": 16.66, "min": 13.39}, "VDD_CPU_SOC_MSS": {"avg": 15.96, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 36.54, "peak": 43.73, "min": 28.36}, "VIN": {"avg": 80.48, "peak": 122.39, "min": 66.05}}, "power_watts_avg": 36.54, "energy_joules_est": 84.93, "sample_count": 18, "duration_seconds": 2.324}, "timestamp": "2026-01-17T15:24:25.424245"}
{"image_index": 292, "image_name": "000000031620.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031620.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 6035.132, "latencies_ms": [6035.132], "images_per_second": 0.166, "prompt_tokens": 23, "response_tokens_est": 128, "n_tiles": 12, "output_text": "- tablecloth: 1\n- table: 1\n- chairs: 1\n- microphone: 1\n- speaker: 1\n- cake: 1\n- cake stand: 1\n- tablecloth: 1\n- table: 1\n- chairs: 1\n- microphone: 1\n- speaker: 1\n- cake: 1\n- cake stand: 1\n- tablecloth: 1\n- table: 1\n- chairs: 1\n- microphone: 1\n- speaker: 1\n- cake: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24924.3, "ram_available_mb": 100847.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 24925.0, "ram_available_mb": 100847.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.13, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.55, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 30.71, "peak": 45.28, "min": 26.0}, "VIN": {"avg": 75.13, "peak": 121.53, "min": 64.0}}, "power_watts_avg": 30.71, "energy_joules_est": 185.35, "sample_count": 48, "duration_seconds": 6.036}, "timestamp": "2026-01-17T15:24:31.466177"}
{"image_index": 292, "image_name": "000000031620.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031620.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3781.287, "latencies_ms": [3781.287], "images_per_second": 0.264, "prompt_tokens": 27, "response_tokens_est": 62, "n_tiles": 12, "output_text": "The main objects in the image are the bride and groom, who are standing near a table. The table is located in the foreground, with the bride and groom positioned on it. The background features a stage with a microphone stand and a guitar, indicating that the event is likely a wedding reception or ceremony.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24925.0, "ram_available_mb": 100847.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 24925.3, "ram_available_mb": 100846.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.19, "peak": 16.66, "min": 14.12}, "VDD_CPU_SOC_MSS": {"avg": 15.64, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 33.63, "peak": 44.49, "min": 26.0}, "VIN": {"avg": 80.29, "peak": 139.21, "min": 62.11}}, "power_watts_avg": 33.63, "energy_joules_est": 127.18, "sample_count": 29, "duration_seconds": 3.782}, "timestamp": "2026-01-17T15:24:35.253830"}
{"image_index": 292, "image_name": "000000031620.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031620.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3667.524, "latencies_ms": [3667.524], "images_per_second": 0.273, "prompt_tokens": 21, "response_tokens_est": 59, "n_tiles": 12, "output_text": "The image depicts a wedding reception taking place in a tented venue. Guests are seated around tables covered with white tablecloths, while a couple stands at the center, cutting a cake together. The setting is intimate and festive, with string lights and decorative elements enhancing the ambiance.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24925.3, "ram_available_mb": 100846.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 24926.0, "ram_available_mb": 100846.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.21, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.68, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 33.93, "peak": 45.3, "min": 26.0}, "VIN": {"avg": 79.25, "peak": 137.78, "min": 58.57}}, "power_watts_avg": 33.93, "energy_joules_est": 124.45, "sample_count": 28, "duration_seconds": 3.668}, "timestamp": "2026-01-17T15:24:38.927715"}
{"image_index": 292, "image_name": "000000031620.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031620.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3731.83, "latencies_ms": [3731.83], "images_per_second": 0.268, "prompt_tokens": 19, "response_tokens_est": 61, "n_tiles": 12, "output_text": "The image depicts a wedding reception with a warm and inviting atmosphere. The venue features a ceiling adorned with white and pastel-colored flags, creating a festive ambiance. The lighting is soft and warm, with a combination of natural and artificial light sources, enhancing the overall mood of the event.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24926.0, "ram_available_mb": 100846.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 24926.3, "ram_available_mb": 100845.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.2, "peak": 16.66, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.64, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 33.73, "peak": 45.68, "min": 26.0}, "VIN": {"avg": 77.59, "peak": 141.29, "min": 58.31}}, "power_watts_avg": 33.73, "energy_joules_est": 125.89, "sample_count": 29, "duration_seconds": 3.732}, "timestamp": "2026-01-17T15:24:42.666402"}
{"image_index": 293, "image_name": "000000031735.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031735.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2998.078, "latencies_ms": [2998.078], "images_per_second": 0.334, "prompt_tokens": 9, "response_tokens_est": 41, "n_tiles": 12, "output_text": "The image depicts a cozy living room with a red wall, a dark blue sofa, a small round wooden table with a lamp, a potted plant, and a framed picture on the wall.", "error": null, "sys_before": {"cpu_percent": 42.2, "ram_used_mb": 24940.1, "ram_available_mb": 100832.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 24934.4, "ram_available_mb": 100837.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.15, "peak": 16.66, "min": 13.39}, "VDD_CPU_SOC_MSS": {"avg": 15.81, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 34.76, "peak": 44.1, "min": 26.4}, "VIN": {"avg": 81.07, "peak": 125.93, "min": 63.72}}, "power_watts_avg": 34.76, "energy_joules_est": 104.23, "sample_count": 23, "duration_seconds": 2.999}, "timestamp": "2026-01-17T15:24:45.772553"}
{"image_index": 293, "image_name": "000000031735.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031735.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 3328.175, "latencies_ms": [3328.175], "images_per_second": 0.3, "prompt_tokens": 23, "response_tokens_est": 49, "n_tiles": 12, "output_text": "- table: 2\n- sofa: 1\n- lamp: 1\n- vase: 1\n- plant: 1\n- window: 1\n- curtain: 1\n- rug: 1", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24934.4, "ram_available_mb": 100837.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 24931.2, "ram_available_mb": 100841.0, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.23, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.76, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 35.02, "peak": 44.89, "min": 26.39}, "VIN": {"avg": 77.69, "peak": 112.1, "min": 66.11}}, "power_watts_avg": 35.02, "energy_joules_est": 116.57, "sample_count": 25, "duration_seconds": 3.329}, "timestamp": "2026-01-17T15:24:49.107483"}
{"image_index": 293, "image_name": "000000031735.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031735.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 4660.303, "latencies_ms": [4660.303], "images_per_second": 0.215, "prompt_tokens": 27, "response_tokens_est": 88, "n_tiles": 12, "output_text": "The main objects in the image are a sofa, a coffee table, and a small round table. The sofa is positioned in the foreground, with a blue throw blanket draped over it. The coffee table is located to the left of the sofa, and the small round table is positioned to the right of the sofa. The sofa is near the center of the image, while the coffee table and small round table are positioned further back.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24931.2, "ram_available_mb": 100841.0, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 24930.1, "ram_available_mb": 100842.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.19, "peak": 16.66, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.67, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 32.32, "peak": 44.91, "min": 26.0}, "VIN": {"avg": 73.46, "peak": 104.7, "min": 61.09}}, "power_watts_avg": 32.32, "energy_joules_est": 150.64, "sample_count": 36, "duration_seconds": 4.661}, "timestamp": "2026-01-17T15:24:53.775507"}
{"image_index": 293, "image_name": "000000031735.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031735.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 4218.981, "latencies_ms": [4218.981], "images_per_second": 0.237, "prompt_tokens": 21, "response_tokens_est": 75, "n_tiles": 12, "output_text": "The image depicts a cozy living room with a rich red wall and a large window allowing natural light to filter in. The room features a dark blue sofa with a few pillows, a small round wooden table with a lamp, a glass coffee table, and a potted plant. The overall atmosphere is warm and inviting, with a mix of traditional and modern elements.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24930.1, "ram_available_mb": 100842.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 24930.8, "ram_available_mb": 100841.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.15, "peak": 16.66, "min": 14.12}, "VDD_CPU_SOC_MSS": {"avg": 15.62, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 32.7, "peak": 45.69, "min": 26.0}, "VIN": {"avg": 73.57, "peak": 116.24, "min": 56.21}}, "power_watts_avg": 32.7, "energy_joules_est": 137.98, "sample_count": 33, "duration_seconds": 4.219}, "timestamp": "2026-01-17T15:24:58.002127"}
{"image_index": 293, "image_name": "000000031735.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031735.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2902.706, "latencies_ms": [2902.706], "images_per_second": 0.345, "prompt_tokens": 19, "response_tokens_est": 36, "n_tiles": 12, "output_text": "The room is warmly lit with natural light streaming through a window adorned with white lace curtains. The walls are painted a deep red, creating a cozy and inviting atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24930.8, "ram_available_mb": 100841.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 24930.6, "ram_available_mb": 100841.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.2, "peak": 16.56, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.76, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 35.72, "peak": 44.91, "min": 26.39}, "VIN": {"avg": 76.45, "peak": 112.27, "min": 64.04}}, "power_watts_avg": 35.72, "energy_joules_est": 103.7, "sample_count": 23, "duration_seconds": 2.903}, "timestamp": "2026-01-17T15:25:00.913598"}
{"image_index": 294, "image_name": "000000031749.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031749.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1336.513, "latencies_ms": [1336.513], "images_per_second": 0.748, "prompt_tokens": 9, "response_tokens_est": 39, "n_tiles": 4, "output_text": "The image depicts a surreal and whimsical scene featuring a humanoid figure with a clock face on its head, creating a juxtaposition between the human and the mechanical.", "error": null, "sys_before": {"cpu_percent": 31.9, "ram_used_mb": 24932.1, "ram_available_mb": 100840.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 24932.8, "ram_available_mb": 100839.3, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4606.7, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5444.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.57, "peak": 15.22, "min": 13.72}, "VDD_CPU_SOC_MSS": {"avg": 15.28, "peak": 15.75, "min": 14.18}, "VDD_GPU": {"avg": 30.02, "peak": 37.42, "min": 24.03}, "VIN": {"avg": 69.94, "peak": 102.03, "min": 57.12}}, "power_watts_avg": 30.02, "energy_joules_est": 40.13, "sample_count": 10, "duration_seconds": 1.337}, "timestamp": "2026-01-17T15:25:02.306743"}
{"image_index": 294, "image_name": "000000031749.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031749.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1366.612, "latencies_ms": [1366.612], "images_per_second": 0.732, "prompt_tokens": 23, "response_tokens_est": 40, "n_tiles": 4, "output_text": "object: 1\nobject: 2\nobject: 3\nobject: 4\nobject: 5\nobject: 6\nobject: 7\nobject: 8", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24932.8, "ram_available_mb": 100839.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 6.5, "ram_used_mb": 24933.8, "ram_available_mb": 100838.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4606.7, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5455.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.95, "peak": 15.32, "min": 14.51}, "VDD_CPU_SOC_MSS": {"avg": 15.75, "peak": 16.14, "min": 15.35}, "VDD_GPU": {"avg": 29.98, "peak": 37.03, "min": 24.03}, "VIN": {"avg": 70.46, "peak": 108.83, "min": 53.89}}, "power_watts_avg": 29.98, "energy_joules_est": 40.99, "sample_count": 10, "duration_seconds": 1.367}, "timestamp": "2026-01-17T15:25:03.680247"}
{"image_index": 294, "image_name": "000000031749.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031749.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3340.995, "latencies_ms": [3340.995], "images_per_second": 0.299, "prompt_tokens": 27, "response_tokens_est": 114, "n_tiles": 4, "output_text": "The main object in the image is a clock with a face that is partially obscured by a humanoid figure. The clock is positioned on the left side of the image, with its face facing towards the right. The figure, which appears to be a doll or a mannequin, is placed in the foreground, slightly to the right of the clock. The background is a plain, light-colored wall, which helps to highlight the clock and the figure. The overall composition places the clock and the figure in the foreground, with the background being a neutral space.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24933.8, "ram_available_mb": 100838.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 24934.1, "ram_available_mb": 100838.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4606.7, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5458.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.07, "peak": 15.32, "min": 14.51}, "VDD_CPU_SOC_MSS": {"avg": 15.89, "peak": 16.14, "min": 15.35}, "VDD_GPU": {"avg": 24.96, "peak": 38.6, "min": 22.06}, "VIN": {"avg": 65.56, "peak": 83.89, "min": 60.95}}, "power_watts_avg": 24.96, "energy_joules_est": 83.4, "sample_count": 25, "duration_seconds": 3.341}, "timestamp": "2026-01-17T15:25:07.027252"}
{"image_index": 294, "image_name": "000000031749.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031749.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2481.948, "latencies_ms": [2481.948], "images_per_second": 0.403, "prompt_tokens": 21, "response_tokens_est": 82, "n_tiles": 4, "output_text": "The image depicts a surreal and whimsical scene featuring a clock with a human face. The clock is placed on a wooden surface, and the face of the clock is a realistic human face with a red and white striped pattern. The clock's hands are pointing to approximately 10:10. The overall setting appears to be indoors, possibly in a room with a warm, ambient light.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24934.1, "ram_available_mb": 100838.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 24934.3, "ram_available_mb": 100837.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4606.7, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5453.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.07, "peak": 15.32, "min": 14.51}, "VDD_CPU_SOC_MSS": {"avg": 15.65, "peak": 15.75, "min": 15.35}, "VDD_GPU": {"avg": 26.11, "peak": 37.42, "min": 22.06}, "VIN": {"avg": 66.86, "peak": 100.73, "min": 59.1}}, "power_watts_avg": 26.11, "energy_joules_est": 64.81, "sample_count": 19, "duration_seconds": 2.482}, "timestamp": "2026-01-17T15:25:09.515484"}
{"image_index": 294, "image_name": "000000031749.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031749.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2016.939, "latencies_ms": [2016.939], "images_per_second": 0.496, "prompt_tokens": 19, "response_tokens_est": 64, "n_tiles": 4, "output_text": "The image features a clock with a striking red face and hands, set against a warm, yellowish background. The clock appears to be made of a glossy, reflective material, giving it a slightly metallic sheen. The lighting is soft and diffused, casting gentle shadows and highlighting the clock's intricate details.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24934.3, "ram_available_mb": 100837.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 24935.0, "ram_available_mb": 100837.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4606.7, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5451.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.02, "peak": 15.32, "min": 14.53}, "VDD_CPU_SOC_MSS": {"avg": 15.67, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 27.18, "peak": 37.03, "min": 22.45}, "VIN": {"avg": 68.6, "peak": 106.05, "min": 59.39}}, "power_watts_avg": 27.18, "energy_joules_est": 54.83, "sample_count": 15, "duration_seconds": 2.017}, "timestamp": "2026-01-17T15:25:11.538432"}
{"image_index": 295, "image_name": "000000031817.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031817.jpg", "image_width": 334, "image_height": 640, "image_resolution": "334x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1018.76, "latencies_ms": [1018.76], "images_per_second": 0.982, "prompt_tokens": 9, "response_tokens_est": 38, "n_tiles": 2, "output_text": "A person is sitting on a motorcycle, wearing a beige jacket, beige pants, and black shoes, with a helmet on their head, and appears to be looking downwards.", "error": null, "sys_before": {"cpu_percent": 40.6, "ram_used_mb": 24935.0, "ram_available_mb": 100837.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 24935.5, "ram_available_mb": 100836.7, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5055.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.82, "peak": 15.42, "min": 14.3}, "VDD_CPU_SOC_MSS": {"avg": 15.58, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 25.95, "peak": 31.12, "min": 22.46}, "VIN": {"avg": 66.24, "peak": 79.49, "min": 59.12}}, "power_watts_avg": 25.95, "energy_joules_est": 26.45, "sample_count": 7, "duration_seconds": 1.019}, "timestamp": "2026-01-17T15:25:12.582485"}
{"image_index": 295, "image_name": "000000031817.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031817.jpg", "image_width": 334, "image_height": 640, "image_resolution": "334x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1234.175, "latencies_ms": [1234.175], "images_per_second": 0.81, "prompt_tokens": 23, "response_tokens_est": 47, "n_tiles": 2, "output_text": "helmet: 1\nshirt: 1\npants: 1\nsocks: 1\nfootwear: 1\nmotorcycle: 1\nseat: 1\nseatbelt: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24935.5, "ram_available_mb": 100836.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 24935.5, "ram_available_mb": 100836.7, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5064.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.22, "peak": 15.42, "min": 14.71}, "VDD_CPU_SOC_MSS": {"avg": 15.57, "peak": 15.75, "min": 14.96}, "VDD_GPU": {"avg": 25.13, "peak": 31.12, "min": 21.68}, "VIN": {"avg": 68.43, "peak": 109.68, "min": 61.02}}, "power_watts_avg": 25.13, "energy_joules_est": 31.02, "sample_count": 9, "duration_seconds": 1.235}, "timestamp": "2026-01-17T15:25:13.822755"}
{"image_index": 295, "image_name": "000000031817.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031817.jpg", "image_width": 334, "image_height": 640, "image_resolution": "334x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1482.097, "latencies_ms": [1482.097], "images_per_second": 0.675, "prompt_tokens": 27, "response_tokens_est": 58, "n_tiles": 2, "output_text": "The main object in the foreground is a person wearing a beige jacket and beige pants, seated on a motorcycle. The motorcycle is positioned to the right of the person. In the background, there are other people and a red vehicle, which is further away from the main subject.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24935.5, "ram_available_mb": 100836.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24935.8, "ram_available_mb": 100836.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5067.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.26, "peak": 15.72, "min": 14.71}, "VDD_CPU_SOC_MSS": {"avg": 15.74, "peak": 16.14, "min": 15.34}, "VDD_GPU": {"avg": 24.37, "peak": 30.74, "min": 21.29}, "VIN": {"avg": 67.09, "peak": 97.33, "min": 61.25}}, "power_watts_avg": 24.37, "energy_joules_est": 36.12, "sample_count": 11, "duration_seconds": 1.482}, "timestamp": "2026-01-17T15:25:15.309890"}
{"image_index": 295, "image_name": "000000031817.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031817.jpg", "image_width": 334, "image_height": 640, "image_resolution": "334x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1611.729, "latencies_ms": [1611.729], "images_per_second": 0.62, "prompt_tokens": 21, "response_tokens_est": 63, "n_tiles": 2, "output_text": "The image depicts a person riding a motorcycle, wearing a beige jacket and beige pants, with a helmet on their head. The individual is seated on the motorcycle, with their hands resting on the handlebars. In the background, there are other people and a red vehicle, suggesting an outdoor setting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24935.8, "ram_available_mb": 100836.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 24935.7, "ram_available_mb": 100836.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5063.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.24, "peak": 15.62, "min": 14.71}, "VDD_CPU_SOC_MSS": {"avg": 15.81, "peak": 16.14, "min": 14.95}, "VDD_GPU": {"avg": 23.87, "peak": 30.34, "min": 20.89}, "VIN": {"avg": 66.43, "peak": 102.14, "min": 56.51}}, "power_watts_avg": 23.87, "energy_joules_est": 38.48, "sample_count": 12, "duration_seconds": 1.612}, "timestamp": "2026-01-17T15:25:16.927654"}
{"image_index": 295, "image_name": "000000031817.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031817.jpg", "image_width": 334, "image_height": 640, "image_resolution": "334x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1489.957, "latencies_ms": [1489.957], "images_per_second": 0.671, "prompt_tokens": 19, "response_tokens_est": 58, "n_tiles": 2, "output_text": "The rider is wearing a beige jacket and beige pants, with a black helmet and black shoes. The helmet has a clear visor, and the jacket appears to be made of a light, possibly cotton or cotton blend material. The lighting is bright, suggesting it is daytime.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24935.7, "ram_available_mb": 100836.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 24936.0, "ram_available_mb": 100836.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5062.6, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.1, "peak": 15.52, "min": 14.71}, "VDD_CPU_SOC_MSS": {"avg": 16.04, "peak": 16.54, "min": 15.36}, "VDD_GPU": {"avg": 24.15, "peak": 29.95, "min": 21.27}, "VIN": {"avg": 67.29, "peak": 106.48, "min": 57.26}}, "power_watts_avg": 24.15, "energy_joules_est": 35.99, "sample_count": 11, "duration_seconds": 1.49}, "timestamp": "2026-01-17T15:25:18.423363"}
{"image_index": 296, "image_name": "000000032038.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032038.jpg", "image_width": 333, "image_height": 500, "image_resolution": "333x500", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1432.819, "latencies_ms": [1432.819], "images_per_second": 0.698, "prompt_tokens": 9, "response_tokens_est": 31, "n_tiles": 6, "output_text": "A large, freshly baked pizza with melted cheese and green herbs is placed on a wooden table, with a person's hand partially visible in the background.", "error": null, "sys_before": {"cpu_percent": 50.0, "ram_used_mb": 24958.0, "ram_available_mb": 100814.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 24958.5, "ram_available_mb": 100813.7, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.88, "peak": 15.34, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.64, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 32.44, "peak": 39.77, "min": 25.6}, "VIN": {"avg": 72.69, "peak": 112.99, "min": 61.14}}, "power_watts_avg": 32.44, "energy_joules_est": 46.5, "sample_count": 11, "duration_seconds": 1.433}, "timestamp": "2026-01-17T15:25:19.898034"}
{"image_index": 296, "image_name": "000000032038.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032038.jpg", "image_width": 333, "image_height": 500, "image_resolution": "333x500", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 4271.117, "latencies_ms": [4271.117], "images_per_second": 0.234, "prompt_tokens": 23, "response_tokens_est": 129, "n_tiles": 6, "output_text": "pizza: 1\nbutter: 1\nbutter knife: 1\nbutter paper: 1\nbutter paper tray: 1\nbutter tray: 1\nbutter tray: 1\nbutter tray: 1\nbutter tray: 1\nbutter tray: 1\nbutter tray: 1\nbutter tray: 1\nbutter tray: 1\nbutter tray: 1\nbutter tray: 1\nbutter tray: 1\nbutter tray: 1\nbutter tray: 1\nbutter tray", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24958.5, "ram_available_mb": 100813.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 24958.7, "ram_available_mb": 100813.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.92, "peak": 15.34, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.34, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 26.66, "peak": 40.95, "min": 22.85}, "VIN": {"avg": 68.72, "peak": 112.52, "min": 55.8}}, "power_watts_avg": 26.66, "energy_joules_est": 113.88, "sample_count": 33, "duration_seconds": 4.272}, "timestamp": "2026-01-17T15:25:24.175460"}
{"image_index": 296, "image_name": "000000032038.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032038.jpg", "image_width": 333, "image_height": 500, "image_resolution": "333x500", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2290.925, "latencies_ms": [2290.925], "images_per_second": 0.437, "prompt_tokens": 27, "response_tokens_est": 59, "n_tiles": 6, "output_text": "The main object in the foreground is a freshly baked pizza with melted cheese and green herbs on top. The pizza is placed on a wooden surface, likely a table or countertop. In the background, there are other kitchen utensils and containers, suggesting that this is a kitchen setting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24958.7, "ram_available_mb": 100813.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 24958.7, "ram_available_mb": 100813.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.9, "peak": 15.24, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.52, "peak": 15.75, "min": 14.56}, "VDD_GPU": {"avg": 29.91, "peak": 40.16, "min": 23.25}, "VIN": {"avg": 69.7, "peak": 114.81, "min": 53.23}}, "power_watts_avg": 29.91, "energy_joules_est": 68.53, "sample_count": 17, "duration_seconds": 2.291}, "timestamp": "2026-01-17T15:25:26.472798"}
{"image_index": 296, "image_name": "000000032038.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032038.jpg", "image_width": 333, "image_height": 500, "image_resolution": "333x500", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2180.388, "latencies_ms": [2180.388], "images_per_second": 0.459, "prompt_tokens": 21, "response_tokens_est": 55, "n_tiles": 6, "output_text": "The image depicts a close-up view of a freshly baked pizza on a wooden surface, likely in a kitchen or dining area. The pizza is topped with melted cheese and green herbs, and there are other pizzas and kitchen utensils visible in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24958.7, "ram_available_mb": 100813.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 24958.7, "ram_available_mb": 100813.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.89, "peak": 15.24, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.49, "peak": 15.75, "min": 14.56}, "VDD_GPU": {"avg": 29.98, "peak": 40.56, "min": 23.24}, "VIN": {"avg": 68.95, "peak": 92.69, "min": 54.47}}, "power_watts_avg": 29.98, "energy_joules_est": 65.37, "sample_count": 17, "duration_seconds": 2.181}, "timestamp": "2026-01-17T15:25:28.659178"}
{"image_index": 296, "image_name": "000000032038.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032038.jpg", "image_width": 333, "image_height": 500, "image_resolution": "333x500", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1782.577, "latencies_ms": [1782.577], "images_per_second": 0.561, "prompt_tokens": 19, "response_tokens_est": 41, "n_tiles": 6, "output_text": "The image features a freshly baked pizza with a golden-brown crust and a melted, gooey cheese topping. The lighting is dim, casting shadows and highlighting the texture of the crust and cheese.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24958.7, "ram_available_mb": 100813.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 24959.9, "ram_available_mb": 100812.3, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.88, "peak": 15.24, "min": 14.3}, "VDD_CPU_SOC_MSS": {"avg": 15.38, "peak": 15.75, "min": 14.56}, "VDD_GPU": {"avg": 32.0, "peak": 40.18, "min": 24.42}, "VIN": {"avg": 71.16, "peak": 95.13, "min": 61.53}}, "power_watts_avg": 32.0, "energy_joules_est": 57.06, "sample_count": 13, "duration_seconds": 1.783}, "timestamp": "2026-01-17T15:25:30.448328"}
{"image_index": 297, "image_name": "000000032081.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032081.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2599.331, "latencies_ms": [2599.331], "images_per_second": 0.385, "prompt_tokens": 9, "response_tokens_est": 29, "n_tiles": 12, "output_text": "A tennis player is in the middle of a match, wearing a white outfit and a headband, and is holding a tennis racket.", "error": null, "sys_before": {"cpu_percent": 49.6, "ram_used_mb": 24924.4, "ram_available_mb": 100847.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 24926.1, "ram_available_mb": 100846.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.14, "peak": 16.66, "min": 13.39}, "VDD_CPU_SOC_MSS": {"avg": 15.83, "peak": 17.71, "min": 14.17}, "VDD_GPU": {"avg": 35.67, "peak": 43.73, "min": 27.18}, "VIN": {"avg": 75.55, "peak": 116.36, "min": 60.75}}, "power_watts_avg": 35.67, "energy_joules_est": 92.73, "sample_count": 20, "duration_seconds": 2.6}, "timestamp": "2026-01-17T15:25:33.147077"}
{"image_index": 297, "image_name": "000000032081.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032081.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2827.718, "latencies_ms": [2827.718], "images_per_second": 0.354, "prompt_tokens": 23, "response_tokens_est": 34, "n_tiles": 12, "output_text": "woman: 1\ntennis racket: 1\ntennis ball: 1\ntennis court: 1\ntennis net: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24926.1, "ram_available_mb": 100846.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 24927.3, "ram_available_mb": 100844.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.26, "peak": 16.56, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.84, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 36.55, "peak": 45.28, "min": 27.18}, "VIN": {"avg": 77.61, "peak": 136.05, "min": 58.12}}, "power_watts_avg": 36.55, "energy_joules_est": 103.37, "sample_count": 21, "duration_seconds": 2.828}, "timestamp": "2026-01-17T15:25:35.985462"}
{"image_index": 297, "image_name": "000000032081.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032081.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3341.266, "latencies_ms": [3341.266], "images_per_second": 0.299, "prompt_tokens": 27, "response_tokens_est": 49, "n_tiles": 12, "output_text": "The main object in the foreground is a tennis player, who is positioned near the net. The player is wearing a white outfit and white shoes. The background features a tennis court with a net, and the grass is well-maintained.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24927.3, "ram_available_mb": 100844.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 24927.1, "ram_available_mb": 100845.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.18, "peak": 16.56, "min": 14.12}, "VDD_CPU_SOC_MSS": {"avg": 15.82, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 34.53, "peak": 45.28, "min": 26.01}, "VIN": {"avg": 71.56, "peak": 88.57, "min": 62.34}}, "power_watts_avg": 34.53, "energy_joules_est": 115.39, "sample_count": 26, "duration_seconds": 3.342}, "timestamp": "2026-01-17T15:25:39.333553"}
{"image_index": 297, "image_name": "000000032081.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032081.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3878.337, "latencies_ms": [3878.337], "images_per_second": 0.258, "prompt_tokens": 21, "response_tokens_est": 65, "n_tiles": 12, "output_text": "The image depicts a tennis player in a white outfit, including a skirt and shoes, preparing to hit a tennis ball on a well-maintained grass court. The player is captured in a dynamic pose, with one arm raised and the other holding the racket, indicating the moment of action during a tennis match.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24927.1, "ram_available_mb": 100845.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 24928.3, "ram_available_mb": 100843.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.19, "peak": 16.56, "min": 14.12}, "VDD_CPU_SOC_MSS": {"avg": 15.85, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 33.23, "peak": 44.51, "min": 26.01}, "VIN": {"avg": 74.56, "peak": 102.78, "min": 58.25}}, "power_watts_avg": 33.23, "energy_joules_est": 128.89, "sample_count": 30, "duration_seconds": 3.879}, "timestamp": "2026-01-17T15:25:43.218516"}
{"image_index": 297, "image_name": "000000032081.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032081.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3197.314, "latencies_ms": [3197.314], "images_per_second": 0.313, "prompt_tokens": 19, "response_tokens_est": 45, "n_tiles": 12, "output_text": "The tennis player is wearing a white outfit, including a skirt and a sleeveless top, paired with white shoes. The court is well-maintained with green grass, and the lighting suggests it is a sunny day.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24928.3, "ram_available_mb": 100843.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 24928.3, "ram_available_mb": 100843.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.21, "peak": 16.66, "min": 14.12}, "VDD_CPU_SOC_MSS": {"avg": 15.76, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 35.07, "peak": 45.3, "min": 26.39}, "VIN": {"avg": 79.65, "peak": 120.06, "min": 64.09}}, "power_watts_avg": 35.07, "energy_joules_est": 112.15, "sample_count": 24, "duration_seconds": 3.198}, "timestamp": "2026-01-17T15:25:46.422630"}
{"image_index": 298, "image_name": "000000032285.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032285.jpg", "image_width": 640, "image_height": 423, "image_resolution": "640x423", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1357.663, "latencies_ms": [1357.663], "images_per_second": 0.737, "prompt_tokens": 9, "response_tokens_est": 28, "n_tiles": 6, "output_text": "The image shows a small bathroom with a white toilet, a towel hanging on the wall, a shower curtain, and a towel rack.", "error": null, "sys_before": {"cpu_percent": 45.0, "ram_used_mb": 24944.7, "ram_available_mb": 100827.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 24945.0, "ram_available_mb": 100827.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.65, "peak": 15.14, "min": 13.82}, "VDD_CPU_SOC_MSS": {"avg": 15.31, "peak": 16.14, "min": 14.17}, "VDD_GPU": {"avg": 34.35, "peak": 40.97, "min": 26.79}, "VIN": {"avg": 78.2, "peak": 109.13, "min": 61.66}}, "power_watts_avg": 34.35, "energy_joules_est": 46.65, "sample_count": 10, "duration_seconds": 1.358}, "timestamp": "2026-01-17T15:25:47.840886"}
{"image_index": 298, "image_name": "000000032285.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032285.jpg", "image_width": 640, "image_height": 423, "image_resolution": "640x423", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1823.554, "latencies_ms": [1823.554], "images_per_second": 0.548, "prompt_tokens": 23, "response_tokens_est": 42, "n_tiles": 6, "output_text": "1. Toilet\n2. Shower curtain\n3. Towel\n4. Shelf\n5. Floor tiles\n6. Wall tiles\n7. Wall outlet\n8. Recessed light", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 24945.0, "ram_available_mb": 100827.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 24946.4, "ram_available_mb": 100825.7, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.85, "peak": 15.34, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.38, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 32.45, "peak": 41.34, "min": 24.43}, "VIN": {"avg": 70.49, "peak": 96.15, "min": 61.34}}, "power_watts_avg": 32.45, "energy_joules_est": 59.19, "sample_count": 13, "duration_seconds": 1.824}, "timestamp": "2026-01-17T15:25:49.670812"}
{"image_index": 298, "image_name": "000000032285.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032285.jpg", "image_width": 640, "image_height": 423, "image_resolution": "640x423", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3222.938, "latencies_ms": [3222.938], "images_per_second": 0.31, "prompt_tokens": 27, "response_tokens_est": 91, "n_tiles": 6, "output_text": "The toilet is positioned on the left side of the image, with its lid closed. The toilet paper roll is placed near the toilet. The shower curtain is hanging on the right side of the toilet, partially covering the bathtub. The bathtub is located in the background, and the shower curtain is near the bathtub. The shelves are located to the right of the bathtub, and the towels are placed on the shelves.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24946.4, "ram_available_mb": 100825.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 24946.9, "ram_available_mb": 100825.3, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.88, "peak": 15.24, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.54, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 27.78, "peak": 40.57, "min": 22.85}, "VIN": {"avg": 68.76, "peak": 121.17, "min": 54.83}}, "power_watts_avg": 27.78, "energy_joules_est": 89.55, "sample_count": 25, "duration_seconds": 3.223}, "timestamp": "2026-01-17T15:25:52.900905"}
{"image_index": 298, "image_name": "000000032285.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032285.jpg", "image_width": 640, "image_height": 423, "image_resolution": "640x423", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2953.578, "latencies_ms": [2953.578], "images_per_second": 0.339, "prompt_tokens": 21, "response_tokens_est": 82, "n_tiles": 6, "output_text": "The image depicts a small, well-organized bathroom with a white toilet and a bathtub. The walls are painted in a light yellow color, and there is a towel rack with a black towel hanging on it. The floor is covered with brown tiles, and there are white shelves with folded towels on them. The overall setting appears clean and tidy, with a focus on functionality and simplicity.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24946.9, "ram_available_mb": 100825.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 24947.2, "ram_available_mb": 100825.0, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.9, "peak": 15.24, "min": 14.2}, "VDD_CPU_SOC_MSS": {"avg": 15.56, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 28.07, "peak": 40.16, "min": 22.85}, "VIN": {"avg": 68.44, "peak": 98.25, "min": 61.16}}, "power_watts_avg": 28.07, "energy_joules_est": 82.92, "sample_count": 23, "duration_seconds": 2.954}, "timestamp": "2026-01-17T15:25:55.860945"}
{"image_index": 298, "image_name": "000000032285.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032285.jpg", "image_width": 640, "image_height": 423, "image_resolution": "640x423", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1934.999, "latencies_ms": [1934.999], "images_per_second": 0.517, "prompt_tokens": 19, "response_tokens_est": 46, "n_tiles": 6, "output_text": "The bathroom features a light yellow wall, a white toilet, and a white bathtub. The lighting is soft and natural, likely from a nearby window, and the materials include ceramic tiles for the floor and white fixtures.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24947.2, "ram_available_mb": 100825.0, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 24947.2, "ram_available_mb": 100825.0, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.85, "peak": 15.24, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.3, "peak": 15.74, "min": 14.56}, "VDD_GPU": {"avg": 30.99, "peak": 40.95, "min": 23.64}, "VIN": {"avg": 69.93, "peak": 93.41, "min": 51.66}}, "power_watts_avg": 30.99, "energy_joules_est": 59.98, "sample_count": 15, "duration_seconds": 1.935}, "timestamp": "2026-01-17T15:25:57.802268"}
{"image_index": 299, "image_name": "000000032334.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032334.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2607.256, "latencies_ms": [2607.256], "images_per_second": 0.384, "prompt_tokens": 9, "response_tokens_est": 29, "n_tiles": 12, "output_text": "The image shows a group of people sitting at a table in a restaurant, with two of them holding wine glasses and smiling at the camera.", "error": null, "sys_before": {"cpu_percent": 42.9, "ram_used_mb": 24924.3, "ram_available_mb": 100847.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.6, "ram_used_mb": 24926.5, "ram_available_mb": 100845.7, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.13, "peak": 16.76, "min": 13.39}, "VDD_CPU_SOC_MSS": {"avg": 15.88, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 35.83, "peak": 43.73, "min": 27.18}, "VIN": {"avg": 78.66, "peak": 115.9, "min": 61.1}}, "power_watts_avg": 35.83, "energy_joules_est": 93.43, "sample_count": 20, "duration_seconds": 2.608}, "timestamp": "2026-01-17T15:26:00.507993"}
{"image_index": 299, "image_name": "000000032334.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032334.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2792.618, "latencies_ms": [2792.618], "images_per_second": 0.358, "prompt_tokens": 23, "response_tokens_est": 33, "n_tiles": 12, "output_text": "1. Glasses\n2. Wine\n3. Woman\n4. Man\n5. Table\n6. Menu\n7. Person\n8. Room", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24926.5, "ram_available_mb": 100845.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 24928.0, "ram_available_mb": 100844.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.98, "peak": 16.36, "min": 14.02}, "VDD_CPU_SOC_MSS": {"avg": 15.5, "peak": 17.32, "min": 14.17}, "VDD_GPU": {"avg": 36.8, "peak": 46.09, "min": 26.79}, "VIN": {"avg": 77.1, "peak": 117.68, "min": 59.51}}, "power_watts_avg": 36.8, "energy_joules_est": 102.79, "sample_count": 21, "duration_seconds": 2.793}, "timestamp": "2026-01-17T15:26:03.311306"}
{"image_index": 299, "image_name": "000000032334.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032334.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 4080.172, "latencies_ms": [4080.172], "images_per_second": 0.245, "prompt_tokens": 27, "response_tokens_est": 70, "n_tiles": 12, "output_text": "The main objects in the image are two people, one in the foreground and one in the background. The person in the foreground is holding a wine glass, while the person in the background is partially visible and appears to be standing. The table in the foreground has a menu and a glass, while the background features a window and a person standing.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24928.0, "ram_available_mb": 100844.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 24927.7, "ram_available_mb": 100844.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.86, "peak": 16.26, "min": 13.79}, "VDD_CPU_SOC_MSS": {"avg": 15.4, "peak": 17.32, "min": 14.17}, "VDD_GPU": {"avg": 32.94, "peak": 45.69, "min": 25.6}, "VIN": {"avg": 74.19, "peak": 124.1, "min": 52.32}}, "power_watts_avg": 32.94, "energy_joules_est": 134.42, "sample_count": 32, "duration_seconds": 4.081}, "timestamp": "2026-01-17T15:26:07.398678"}
{"image_index": 299, "image_name": "000000032334.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032334.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3822.457, "latencies_ms": [3822.457], "images_per_second": 0.262, "prompt_tokens": 21, "response_tokens_est": 63, "n_tiles": 12, "output_text": "The image depicts a casual indoor setting, likely a restaurant or bar, where a group of people is gathered. The individuals are engaged in conversation, with some holding wine glasses, suggesting a social or celebratory occasion. The atmosphere appears relaxed and convivial, with the focus on the interaction between the people.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24927.7, "ram_available_mb": 100844.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 24929.2, "ram_available_mb": 100843.0, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.91, "peak": 16.36, "min": 13.79}, "VDD_CPU_SOC_MSS": {"avg": 15.46, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 33.27, "peak": 45.3, "min": 25.6}, "VIN": {"avg": 74.33, "peak": 120.11, "min": 65.82}}, "power_watts_avg": 33.27, "energy_joules_est": 127.2, "sample_count": 30, "duration_seconds": 3.823}, "timestamp": "2026-01-17T15:26:11.228701"}
{"image_index": 299, "image_name": "000000032334.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032334.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2841.392, "latencies_ms": [2841.392], "images_per_second": 0.352, "prompt_tokens": 19, "response_tokens_est": 35, "n_tiles": 12, "output_text": "The image features a brightly lit indoor setting with natural light streaming in through a window. The individuals are wearing casual clothing, and the overall atmosphere is warm and inviting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24929.2, "ram_available_mb": 100843.0, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 24929.7, "ram_available_mb": 100842.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.0, "peak": 16.36, "min": 13.9}, "VDD_CPU_SOC_MSS": {"avg": 15.62, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 35.99, "peak": 44.91, "min": 26.39}, "VIN": {"avg": 77.6, "peak": 112.5, "min": 60.03}}, "power_watts_avg": 35.99, "energy_joules_est": 102.28, "sample_count": 22, "duration_seconds": 2.842}, "timestamp": "2026-01-17T15:26:14.076730"}
{"image_index": 300, "image_name": "000000032570.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032570.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1376.876, "latencies_ms": [1376.876], "images_per_second": 0.726, "prompt_tokens": 9, "response_tokens_est": 29, "n_tiles": 6, "output_text": "A surfer is riding a wave in the ocean, skillfully maneuvering the surfboard amidst the frothy, blue-green waters.", "error": null, "sys_before": {"cpu_percent": 40.0, "ram_used_mb": 24951.6, "ram_available_mb": 100820.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 24952.3, "ram_available_mb": 100819.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.66, "peak": 15.14, "min": 13.72}, "VDD_CPU_SOC_MSS": {"avg": 15.27, "peak": 16.14, "min": 14.17}, "VDD_GPU": {"avg": 34.31, "peak": 40.95, "min": 27.19}, "VIN": {"avg": 75.79, "peak": 122.21, "min": 58.8}}, "power_watts_avg": 34.31, "energy_joules_est": 47.25, "sample_count": 10, "duration_seconds": 1.377}, "timestamp": "2026-01-17T15:26:15.511316"}
{"image_index": 300, "image_name": "000000032570.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032570.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1297.305, "latencies_ms": [1297.305], "images_per_second": 0.771, "prompt_tokens": 23, "response_tokens_est": 24, "n_tiles": 6, "output_text": "surfboard: 1\nsurfer: 1\nocean: 1\nwater: 1", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24952.3, "ram_available_mb": 100819.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 24953.3, "ram_available_mb": 100818.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.97, "peak": 15.44, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.59, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 35.09, "peak": 41.74, "min": 27.57}, "VIN": {"avg": 76.64, "peak": 92.69, "min": 61.39}}, "power_watts_avg": 35.09, "energy_joules_est": 45.54, "sample_count": 10, "duration_seconds": 1.298}, "timestamp": "2026-01-17T15:26:16.815214"}
{"image_index": 300, "image_name": "000000032570.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032570.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1569.221, "latencies_ms": [1569.221], "images_per_second": 0.637, "prompt_tokens": 27, "response_tokens_est": 33, "n_tiles": 6, "output_text": "The main object in the foreground is a surfer riding a wave. The wave is in the background, and the surfer is near the water's surface.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24953.3, "ram_available_mb": 100818.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 24953.8, "ram_available_mb": 100818.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.85, "peak": 15.24, "min": 14.43}, "VDD_CPU_SOC_MSS": {"avg": 15.35, "peak": 15.75, "min": 14.56}, "VDD_GPU": {"avg": 34.16, "peak": 41.34, "min": 25.6}, "VIN": {"avg": 75.86, "peak": 114.95, "min": 61.36}}, "power_watts_avg": 34.16, "energy_joules_est": 53.63, "sample_count": 11, "duration_seconds": 1.57}, "timestamp": "2026-01-17T15:26:18.394393"}
{"image_index": 300, "image_name": "000000032570.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032570.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3584.827, "latencies_ms": [3584.827], "images_per_second": 0.279, "prompt_tokens": 21, "response_tokens_est": 104, "n_tiles": 6, "output_text": "The image captures a dynamic scene of a surfer riding a wave in the ocean. The surfer, dressed in a black wetsuit, is skillfully maneuvering a surfboard amidst the frothy white crest of the wave. The ocean water is a mix of turquoise and blue hues, with the surfer's white surfboard contrasting against the vibrant blue of the water. The setting is a sunny day, with the sun shining brightly, creating a visually striking and energetic atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24953.8, "ram_available_mb": 100818.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 24954.3, "ram_available_mb": 100817.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.88, "peak": 15.14, "min": 14.2}, "VDD_CPU_SOC_MSS": {"avg": 15.47, "peak": 15.75, "min": 14.56}, "VDD_GPU": {"avg": 27.45, "peak": 41.34, "min": 22.85}, "VIN": {"avg": 68.82, "peak": 101.57, "min": 57.1}}, "power_watts_avg": 27.45, "energy_joules_est": 98.43, "sample_count": 28, "duration_seconds": 3.586}, "timestamp": "2026-01-17T15:26:21.986175"}
{"image_index": 300, "image_name": "000000032570.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032570.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2245.944, "latencies_ms": [2245.944], "images_per_second": 0.445, "prompt_tokens": 19, "response_tokens_est": 57, "n_tiles": 6, "output_text": "The image captures a surfer riding a wave in clear, blue-green water, with the sun shining brightly, creating a vivid and dynamic scene. The surfer is wearing a black wetsuit, and the water is splashing around, indicating a strong and powerful wave.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24954.3, "ram_available_mb": 100817.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 24954.3, "ram_available_mb": 100817.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.85, "peak": 15.24, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.42, "peak": 15.75, "min": 14.56}, "VDD_GPU": {"avg": 29.94, "peak": 40.56, "min": 23.25}, "VIN": {"avg": 68.05, "peak": 93.29, "min": 62.08}}, "power_watts_avg": 29.94, "energy_joules_est": 67.26, "sample_count": 17, "duration_seconds": 2.247}, "timestamp": "2026-01-17T15:26:24.238421"}
{"image_index": 301, "image_name": "000000032610.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032610.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1439.828, "latencies_ms": [1439.828], "images_per_second": 0.695, "prompt_tokens": 9, "response_tokens_est": 31, "n_tiles": 6, "output_text": "The image shows a cluttered desk with various electronic devices, including laptops, a computer monitor, and a router, all connected to a power source.", "error": null, "sys_before": {"cpu_percent": 35.8, "ram_used_mb": 24954.3, "ram_available_mb": 100817.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 24954.5, "ram_available_mb": 100817.7, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.69, "peak": 15.11, "min": 13.79}, "VDD_CPU_SOC_MSS": {"avg": 15.39, "peak": 16.14, "min": 14.18}, "VDD_GPU": {"avg": 33.17, "peak": 40.56, "min": 26.39}, "VIN": {"avg": 84.42, "peak": 126.22, "min": 63.57}}, "power_watts_avg": 33.17, "energy_joules_est": 47.77, "sample_count": 10, "duration_seconds": 1.44}, "timestamp": "2026-01-17T15:26:25.734060"}
{"image_index": 301, "image_name": "000000032610.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032610.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1736.226, "latencies_ms": [1736.226], "images_per_second": 0.576, "prompt_tokens": 23, "response_tokens_est": 39, "n_tiles": 6, "output_text": "1. Laptop bag\n2. Laptop\n3. Laptop bag\n4. Laptop bag\n5. Laptop bag\n6. Laptop bag\n7. Laptop bag\n8. Laptop bag", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24954.5, "ram_available_mb": 100817.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 24954.5, "ram_available_mb": 100817.7, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.86, "peak": 15.34, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.32, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 32.33, "peak": 40.95, "min": 24.43}, "VIN": {"avg": 68.88, "peak": 91.46, "min": 59.71}}, "power_watts_avg": 32.33, "energy_joules_est": 56.14, "sample_count": 13, "duration_seconds": 1.736}, "timestamp": "2026-01-17T15:26:27.476475"}
{"image_index": 301, "image_name": "000000032610.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032610.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3504.264, "latencies_ms": [3504.264], "images_per_second": 0.285, "prompt_tokens": 27, "response_tokens_est": 101, "n_tiles": 6, "output_text": "The main objects in the image are a laptop, a black bag, and a black computer tower. The laptop is positioned in the foreground on the right side of the image, while the black bag is on the left side, closer to the foreground. The black computer tower is in the background, slightly to the right. The laptop screen is visible, displaying a webpage with various icons and text. The laptop and the computer tower are connected by a black cable, which is also visible on the table.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 24954.5, "ram_available_mb": 100817.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 24954.5, "ram_available_mb": 100817.7, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.91, "peak": 15.24, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.38, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 27.33, "peak": 40.57, "min": 22.85}, "VIN": {"avg": 68.18, "peak": 114.67, "min": 55.11}}, "power_watts_avg": 27.33, "energy_joules_est": 95.78, "sample_count": 27, "duration_seconds": 3.505}, "timestamp": "2026-01-17T15:26:30.987195"}
{"image_index": 301, "image_name": "000000032610.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032610.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2077.728, "latencies_ms": [2077.728], "images_per_second": 0.481, "prompt_tokens": 21, "response_tokens_est": 51, "n_tiles": 6, "output_text": "The image depicts a cluttered workspace with various electronic devices and cables spread across a wooden surface. The setting appears to be a home or office environment, with a focus on the messiness of the workspace, indicating a busy or active work area.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24954.5, "ram_available_mb": 100817.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 24955.2, "ram_available_mb": 100816.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.84, "peak": 15.24, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.43, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 30.28, "peak": 40.16, "min": 23.25}, "VIN": {"avg": 72.92, "peak": 113.54, "min": 61.73}}, "power_watts_avg": 30.28, "energy_joules_est": 62.92, "sample_count": 16, "duration_seconds": 2.078}, "timestamp": "2026-01-17T15:26:33.071696"}
{"image_index": 301, "image_name": "000000032610.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032610.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1992.521, "latencies_ms": [1992.521], "images_per_second": 0.502, "prompt_tokens": 19, "response_tokens_est": 48, "n_tiles": 6, "output_text": "The image depicts a cluttered workspace with a variety of laptops and a black bag on a wooden surface. The lighting is dim, and the overall atmosphere appears to be somewhat disorganized, with cables and devices strewn about.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24955.2, "ram_available_mb": 100816.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 24955.0, "ram_available_mb": 100817.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.83, "peak": 15.24, "min": 14.2}, "VDD_CPU_SOC_MSS": {"avg": 15.35, "peak": 15.75, "min": 14.56}, "VDD_GPU": {"avg": 30.88, "peak": 40.57, "min": 23.64}, "VIN": {"avg": 71.01, "peak": 111.61, "min": 56.89}}, "power_watts_avg": 30.88, "energy_joules_est": 61.54, "sample_count": 15, "duration_seconds": 1.993}, "timestamp": "2026-01-17T15:26:35.069900"}
{"image_index": 302, "image_name": "000000032735.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032735.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1334.142, "latencies_ms": [1334.142], "images_per_second": 0.75, "prompt_tokens": 9, "response_tokens_est": 27, "n_tiles": 6, "output_text": "A skier is captured mid-air, performing a jump over a snowy slope, with a clear blue sky in the background.", "error": null, "sys_before": {"cpu_percent": 40.3, "ram_used_mb": 24955.0, "ram_available_mb": 100817.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 24955.2, "ram_available_mb": 100816.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.62, "peak": 15.11, "min": 13.69}, "VDD_CPU_SOC_MSS": {"avg": 15.27, "peak": 16.14, "min": 14.18}, "VDD_GPU": {"avg": 33.48, "peak": 40.18, "min": 26.79}, "VIN": {"avg": 76.3, "peak": 117.66, "min": 60.95}}, "power_watts_avg": 33.48, "energy_joules_est": 44.68, "sample_count": 10, "duration_seconds": 1.335}, "timestamp": "2026-01-17T15:26:36.463162"}
{"image_index": 302, "image_name": "000000032735.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032735.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1698.753, "latencies_ms": [1698.753], "images_per_second": 0.589, "prompt_tokens": 23, "response_tokens_est": 39, "n_tiles": 6, "output_text": "1. Skier\n2. Ski poles\n3. Ski\n4. Ski poles\n5. Ski jacket\n6. Ski pants\n7. Ski boots\n8. Ski helmet", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24955.2, "ram_available_mb": 100816.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 24955.5, "ram_available_mb": 100816.7, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.0, "peak": 15.54, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.58, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 33.22, "peak": 40.95, "min": 25.21}, "VIN": {"avg": 73.29, "peak": 103.32, "min": 60.08}}, "power_watts_avg": 33.22, "energy_joules_est": 56.45, "sample_count": 12, "duration_seconds": 1.699}, "timestamp": "2026-01-17T15:26:38.168859"}
{"image_index": 302, "image_name": "000000032735.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032735.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2685.975, "latencies_ms": [2685.975], "images_per_second": 0.372, "prompt_tokens": 27, "response_tokens_est": 75, "n_tiles": 6, "output_text": "The main object in the foreground is a skier in mid-air, performing a jump. The skier is wearing a bright red and green suit, and is holding ski poles. The background features a snowy mountain slope, with another skier visible further away. The skier in the foreground is closer to the camera, while the other skier is further away.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24955.5, "ram_available_mb": 100816.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 24955.2, "ram_available_mb": 100816.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.05, "peak": 15.44, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.65, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 29.12, "peak": 40.56, "min": 23.25}, "VIN": {"avg": 71.67, "peak": 102.6, "min": 57.47}}, "power_watts_avg": 29.12, "energy_joules_est": 78.23, "sample_count": 21, "duration_seconds": 2.686}, "timestamp": "2026-01-17T15:26:40.861207"}
{"image_index": 302, "image_name": "000000032735.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032735.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3082.031, "latencies_ms": [3082.031], "images_per_second": 0.324, "prompt_tokens": 21, "response_tokens_est": 91, "n_tiles": 6, "output_text": "The image captures a dynamic scene of a skier in mid-air, performing a jump over a snowy slope. The skier is dressed in a vibrant red and green jacket, black pants, and a green helmet, with ski poles extended. In the background, another skier is visible, also dressed in ski gear, standing on the snowy slope. The sky is clear, and the overall setting suggests a sunny day at a ski resort.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24955.2, "ram_available_mb": 100816.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 24955.7, "ram_available_mb": 100816.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.17, "peak": 15.54, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.82, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 28.43, "peak": 40.18, "min": 23.24}, "VIN": {"avg": 70.2, "peak": 110.84, "min": 56.2}}, "power_watts_avg": 28.43, "energy_joules_est": 87.63, "sample_count": 23, "duration_seconds": 3.082}, "timestamp": "2026-01-17T15:26:43.953539"}
{"image_index": 302, "image_name": "000000032735.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032735.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2634.719, "latencies_ms": [2634.719], "images_per_second": 0.38, "prompt_tokens": 19, "response_tokens_est": 74, "n_tiles": 6, "output_text": "The image captures a skier in mid-air, dressed in a vibrant red and green suit, with a green helmet and goggles. The skier is performing a jump over a snowy slope, with a clear blue sky and a few clouds in the background. The lighting is bright and natural, indicating daytime, and the snow appears pristine and untouched.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24955.7, "ram_available_mb": 100816.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 24956.7, "ram_available_mb": 100815.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.1, "peak": 15.44, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.75, "peak": 16.53, "min": 14.96}, "VDD_GPU": {"avg": 29.15, "peak": 39.78, "min": 23.64}, "VIN": {"avg": 72.13, "peak": 111.06, "min": 54.09}}, "power_watts_avg": 29.15, "energy_joules_est": 76.82, "sample_count": 20, "duration_seconds": 2.635}, "timestamp": "2026-01-17T15:26:46.594127"}
{"image_index": 303, "image_name": "000000032811.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032811.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2443.012, "latencies_ms": [2443.012], "images_per_second": 0.409, "prompt_tokens": 9, "response_tokens_est": 25, "n_tiles": 12, "output_text": "A small bird is perched on the edge of a rusty window, looking out at a calm body of water.", "error": null, "sys_before": {"cpu_percent": 46.3, "ram_used_mb": 24915.1, "ram_available_mb": 100857.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.5, "ram_used_mb": 24917.3, "ram_available_mb": 100854.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.24, "peak": 16.66, "min": 13.59}, "VDD_CPU_SOC_MSS": {"avg": 15.92, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 36.59, "peak": 43.73, "min": 28.36}, "VIN": {"avg": 80.4, "peak": 110.04, "min": 64.34}}, "power_watts_avg": 36.59, "energy_joules_est": 89.41, "sample_count": 18, "duration_seconds": 2.443}, "timestamp": "2026-01-17T15:26:49.128720"}
{"image_index": 303, "image_name": "000000032811.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032811.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2944.925, "latencies_ms": [2944.925], "images_per_second": 0.34, "prompt_tokens": 23, "response_tokens_est": 38, "n_tiles": 12, "output_text": "bird: 1\nwindow: 1\ndoor: 1\nframe: 1\ndoor handle: 1\ndoor knob: 1\ndoor lock: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24917.3, "ram_available_mb": 100854.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 24918.5, "ram_available_mb": 100853.7, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.31, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.91, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 36.06, "peak": 44.89, "min": 26.79}, "VIN": {"avg": 75.67, "peak": 110.14, "min": 66.25}}, "power_watts_avg": 36.06, "energy_joules_est": 106.21, "sample_count": 22, "duration_seconds": 2.945}, "timestamp": "2026-01-17T15:26:52.080350"}
{"image_index": 303, "image_name": "000000032811.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032811.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 4048.006, "latencies_ms": [4048.006], "images_per_second": 0.247, "prompt_tokens": 27, "response_tokens_est": 70, "n_tiles": 12, "output_text": "The main object in the foreground is a bird perched on a rusty metal railing near the window. The window is situated in the middle ground, slightly to the left, and is framed by a dark wall. The background features a large body of water, likely a lake or sea, with a distant landmass visible on the horizon.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24918.5, "ram_available_mb": 100853.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 24919.5, "ram_available_mb": 100852.7, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.21, "peak": 16.56, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.86, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 33.1, "peak": 44.89, "min": 26.0}, "VIN": {"avg": 77.85, "peak": 132.7, "min": 63.01}}, "power_watts_avg": 33.1, "energy_joules_est": 134.0, "sample_count": 31, "duration_seconds": 4.048}, "timestamp": "2026-01-17T15:26:56.136278"}
{"image_index": 303, "image_name": "000000032811.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032811.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3095.93, "latencies_ms": [3095.93], "images_per_second": 0.323, "prompt_tokens": 21, "response_tokens_est": 42, "n_tiles": 12, "output_text": "The image depicts a dimly lit room with a large, old-fashioned television set on a stand. A bird is perched on the edge of the television screen, seemingly observing the scene outside.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 24919.5, "ram_available_mb": 100852.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 24920.5, "ram_available_mb": 100851.7, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.25, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.86, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 35.14, "peak": 44.91, "min": 26.39}, "VIN": {"avg": 79.01, "peak": 113.76, "min": 53.36}}, "power_watts_avg": 35.14, "energy_joules_est": 108.8, "sample_count": 24, "duration_seconds": 3.096}, "timestamp": "2026-01-17T15:26:59.238419"}
{"image_index": 303, "image_name": "000000032811.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032811.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3347.449, "latencies_ms": [3347.449], "images_per_second": 0.299, "prompt_tokens": 19, "response_tokens_est": 49, "n_tiles": 12, "output_text": "The image shows a dark, dimly lit room with a rusty, weathered window frame. The window is reflecting a view of a calm, blue sea, and a small, dark bird is perched on the window sill.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24920.5, "ram_available_mb": 100851.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 24921.5, "ram_available_mb": 100850.7, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.99, "peak": 16.36, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.52, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 34.48, "peak": 45.28, "min": 25.61}, "VIN": {"avg": 74.31, "peak": 116.98, "min": 61.31}}, "power_watts_avg": 34.48, "energy_joules_est": 115.45, "sample_count": 26, "duration_seconds": 3.348}, "timestamp": "2026-01-17T15:27:02.593002"}
{"image_index": 304, "image_name": "000000032817.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032817.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2194.353, "latencies_ms": [2194.353], "images_per_second": 0.456, "prompt_tokens": 9, "response_tokens_est": 17, "n_tiles": 12, "output_text": "A man is bending over a toilet, picking up a white toilet paper roll.", "error": null, "sys_before": {"cpu_percent": 46.3, "ram_used_mb": 24927.5, "ram_available_mb": 100844.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.3, "ram_used_mb": 24925.3, "ram_available_mb": 100846.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.12, "peak": 16.66, "min": 13.09}, "VDD_CPU_SOC_MSS": {"avg": 15.88, "peak": 17.71, "min": 13.78}, "VDD_GPU": {"avg": 37.31, "peak": 44.12, "min": 29.56}, "VIN": {"avg": 78.72, "peak": 116.45, "min": 65.81}}, "power_watts_avg": 37.31, "energy_joules_est": 81.88, "sample_count": 17, "duration_seconds": 2.195}, "timestamp": "2026-01-17T15:27:04.895290"}
{"image_index": 304, "image_name": "000000032817.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032817.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 3189.875, "latencies_ms": [3189.875], "images_per_second": 0.313, "prompt_tokens": 23, "response_tokens_est": 45, "n_tiles": 12, "output_text": "1. Toilet\n2. Trash can\n3. Shelf\n4. Trash bag\n5. Shelf\n6. Trash bag\n7. Trash bag\n8. Trash bag", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24925.3, "ram_available_mb": 100846.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 24918.6, "ram_available_mb": 100853.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.26, "peak": 16.66, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.89, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 35.57, "peak": 45.69, "min": 26.39}, "VIN": {"avg": 75.06, "peak": 110.03, "min": 57.51}}, "power_watts_avg": 35.57, "energy_joules_est": 113.48, "sample_count": 24, "duration_seconds": 3.19}, "timestamp": "2026-01-17T15:27:08.095807"}
{"image_index": 304, "image_name": "000000032817.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032817.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3466.86, "latencies_ms": [3466.86], "images_per_second": 0.288, "prompt_tokens": 27, "response_tokens_est": 53, "n_tiles": 12, "output_text": "The main object in the foreground is a black trash can filled with various items, including a white towel. The trash can is positioned near the toilet, which is in the background. The person is bending over the toilet, possibly picking something up or cleaning it.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 24918.6, "ram_available_mb": 100853.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 24919.1, "ram_available_mb": 100853.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.23, "peak": 16.66, "min": 14.12}, "VDD_CPU_SOC_MSS": {"avg": 15.79, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 34.3, "peak": 45.28, "min": 26.39}, "VIN": {"avg": 80.25, "peak": 128.93, "min": 59.59}}, "power_watts_avg": 34.3, "energy_joules_est": 118.92, "sample_count": 27, "duration_seconds": 3.467}, "timestamp": "2026-01-17T15:27:11.572203"}
{"image_index": 304, "image_name": "000000032817.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032817.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2773.911, "latencies_ms": [2773.911], "images_per_second": 0.361, "prompt_tokens": 21, "response_tokens_est": 33, "n_tiles": 12, "output_text": "The image depicts a cluttered bathroom with a toilet and a trash can. A person is seen bending over the toilet, possibly cleaning or organizing the area.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 24919.1, "ram_available_mb": 100853.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 24919.3, "ram_available_mb": 100852.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.29, "peak": 16.66, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.93, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 36.55, "peak": 44.49, "min": 27.19}, "VIN": {"avg": 78.71, "peak": 116.14, "min": 65.28}}, "power_watts_avg": 36.55, "energy_joules_est": 101.4, "sample_count": 21, "duration_seconds": 2.774}, "timestamp": "2026-01-17T15:27:14.352691"}
{"image_index": 304, "image_name": "000000032817.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032817.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2881.878, "latencies_ms": [2881.878], "images_per_second": 0.347, "prompt_tokens": 19, "response_tokens_est": 36, "n_tiles": 12, "output_text": "The image shows a bathroom with a beige wall and a white toilet. The lighting is dim, and the scene is slightly blurred, indicating movement or a quick snapshot.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24919.3, "ram_available_mb": 100852.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 24919.1, "ram_available_mb": 100853.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.31, "peak": 16.66, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 16.0, "peak": 17.71, "min": 14.96}, "VDD_GPU": {"avg": 36.08, "peak": 44.91, "min": 26.8}, "VIN": {"avg": 83.26, "peak": 129.77, "min": 66.15}}, "power_watts_avg": 36.08, "energy_joules_est": 103.99, "sample_count": 22, "duration_seconds": 2.882}, "timestamp": "2026-01-17T15:27:17.241091"}
{"image_index": 305, "image_name": "000000032861.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032861.jpg", "image_width": 426, "image_height": 640, "image_resolution": "426x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1279.947, "latencies_ms": [1279.947], "images_per_second": 0.781, "prompt_tokens": 9, "response_tokens_est": 25, "n_tiles": 6, "output_text": "A man is standing in a rainy room, holding an umbrella, and looking at his reflection in the doorway.", "error": null, "sys_before": {"cpu_percent": 46.3, "ram_used_mb": 24941.0, "ram_available_mb": 100831.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 24943.0, "ram_available_mb": 100829.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.6, "peak": 15.14, "min": 13.82}, "VDD_CPU_SOC_MSS": {"avg": 15.35, "peak": 16.14, "min": 14.17}, "VDD_GPU": {"avg": 35.14, "peak": 40.95, "min": 28.36}, "VIN": {"avg": 82.43, "peak": 119.51, "min": 60.52}}, "power_watts_avg": 35.14, "energy_joules_est": 44.99, "sample_count": 9, "duration_seconds": 1.28}, "timestamp": "2026-01-17T15:27:18.581850"}
{"image_index": 305, "image_name": "000000032861.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032861.jpg", "image_width": 426, "image_height": 640, "image_resolution": "426x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1566.383, "latencies_ms": [1566.383], "images_per_second": 0.638, "prompt_tokens": 23, "response_tokens_est": 34, "n_tiles": 6, "output_text": "1. Person\n2. Rain\n3. Door\n4. Wall\n5. Mirror\n6. Umbrella\n7. Window\n8. Light", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24943.0, "ram_available_mb": 100829.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 24943.7, "ram_available_mb": 100828.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.98, "peak": 15.44, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.62, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 33.81, "peak": 41.74, "min": 25.6}, "VIN": {"avg": 75.4, "peak": 116.68, "min": 58.18}}, "power_watts_avg": 33.81, "energy_joules_est": 52.97, "sample_count": 12, "duration_seconds": 1.567}, "timestamp": "2026-01-17T15:27:20.154410"}
{"image_index": 305, "image_name": "000000032861.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032861.jpg", "image_width": 426, "image_height": 640, "image_resolution": "426x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2648.269, "latencies_ms": [2648.269], "images_per_second": 0.378, "prompt_tokens": 27, "response_tokens_est": 74, "n_tiles": 6, "output_text": "The main object in the foreground is a person holding an umbrella, standing in a corridor with red walls. The person is positioned near the center of the image, slightly to the left. The background features a door and a framed picture on the wall, which are slightly out of focus. The person's reflection is visible in the door, adding depth to the scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24943.7, "ram_available_mb": 100828.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 24944.7, "ram_available_mb": 100827.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.11, "peak": 15.44, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.79, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 29.54, "peak": 40.95, "min": 23.64}, "VIN": {"avg": 69.36, "peak": 92.64, "min": 58.77}}, "power_watts_avg": 29.54, "energy_joules_est": 78.24, "sample_count": 20, "duration_seconds": 2.649}, "timestamp": "2026-01-17T15:27:22.813004"}
{"image_index": 305, "image_name": "000000032861.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032861.jpg", "image_width": 426, "image_height": 640, "image_resolution": "426x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2426.157, "latencies_ms": [2426.157], "images_per_second": 0.412, "prompt_tokens": 21, "response_tokens_est": 66, "n_tiles": 6, "output_text": "The scene depicts a man standing in a corridor, holding an umbrella to shield himself from the rain. The rain is visibly falling, creating a dynamic and somewhat dramatic atmosphere. The man is dressed in a blue shirt and dark pants, and the corridor is painted in a striking red color, adding to the visual contrast.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24944.7, "ram_available_mb": 100827.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 24944.7, "ram_available_mb": 100827.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.09, "peak": 15.44, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.69, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 29.54, "peak": 40.18, "min": 23.64}, "VIN": {"avg": 71.36, "peak": 103.48, "min": 62.57}}, "power_watts_avg": 29.54, "energy_joules_est": 71.68, "sample_count": 19, "duration_seconds": 2.426}, "timestamp": "2026-01-17T15:27:25.245459"}
{"image_index": 305, "image_name": "000000032861.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032861.jpg", "image_width": 426, "image_height": 640, "image_resolution": "426x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2669.912, "latencies_ms": [2669.912], "images_per_second": 0.375, "prompt_tokens": 19, "response_tokens_est": 75, "n_tiles": 6, "output_text": "The image depicts a person standing in a corridor under a dark, rainy sky. The rain is visibly falling, creating a dynamic and somewhat dramatic atmosphere. The person is wearing a blue shirt and dark pants, and the corridor is painted in a striking red color. The lighting is dim, with the rain adding a cool, moody tone to the scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24944.7, "ram_available_mb": 100827.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 24945.6, "ram_available_mb": 100826.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.07, "peak": 15.44, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.67, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 29.13, "peak": 40.95, "min": 23.64}, "VIN": {"avg": 69.33, "peak": 86.95, "min": 61.05}}, "power_watts_avg": 29.13, "energy_joules_est": 77.79, "sample_count": 21, "duration_seconds": 2.67}, "timestamp": "2026-01-17T15:27:27.921657"}
{"image_index": 306, "image_name": "000000032887.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032887.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2560.916, "latencies_ms": [2560.916], "images_per_second": 0.39, "prompt_tokens": 9, "response_tokens_est": 28, "n_tiles": 12, "output_text": "A man in hiking gear is standing on a rocky path surrounded by lush greenery, with a signpost in the background indicating directions.", "error": null, "sys_before": {"cpu_percent": 47.5, "ram_used_mb": 24926.8, "ram_available_mb": 100845.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.6, "ram_used_mb": 24928.5, "ram_available_mb": 100843.7, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.23, "peak": 16.66, "min": 13.69}, "VDD_CPU_SOC_MSS": {"avg": 15.94, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 35.9, "peak": 44.1, "min": 27.18}, "VIN": {"avg": 83.34, "peak": 135.46, "min": 60.67}}, "power_watts_avg": 35.9, "energy_joules_est": 91.95, "sample_count": 20, "duration_seconds": 2.561}, "timestamp": "2026-01-17T15:27:30.571914"}
{"image_index": 306, "image_name": "000000032887.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032887.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 3737.908, "latencies_ms": [3737.908], "images_per_second": 0.268, "prompt_tokens": 23, "response_tokens_est": 61, "n_tiles": 12, "output_text": "1. Man: 1\n2. Backpack: 1\n3. Hiking stick: 1\n4. Rock: 1\n5. Path: 1\n6. Waterfall: 1\n7. Tree: 1\n8. Stone steps: 1", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24928.5, "ram_available_mb": 100843.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 24929.0, "ram_available_mb": 100843.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.21, "peak": 16.56, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.83, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 33.94, "peak": 45.69, "min": 26.0}, "VIN": {"avg": 78.62, "peak": 125.37, "min": 66.42}}, "power_watts_avg": 33.94, "energy_joules_est": 126.88, "sample_count": 29, "duration_seconds": 3.738}, "timestamp": "2026-01-17T15:27:34.316647"}
{"image_index": 306, "image_name": "000000032887.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032887.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3851.946, "latencies_ms": [3851.946], "images_per_second": 0.26, "prompt_tokens": 27, "response_tokens_est": 64, "n_tiles": 12, "output_text": "The main object in the foreground is a man wearing a red long-sleeve shirt and blue jeans, equipped with a backpack and walking stick. He is standing on a rocky path surrounded by greenery and rocks. In the background, there is a signpost with directional arrows, indicating the path's direction.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24929.0, "ram_available_mb": 100843.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 24930.2, "ram_available_mb": 100842.0, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.19, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.8, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 33.47, "peak": 45.28, "min": 26.0}, "VIN": {"avg": 76.72, "peak": 110.44, "min": 66.74}}, "power_watts_avg": 33.47, "energy_joules_est": 128.94, "sample_count": 30, "duration_seconds": 3.852}, "timestamp": "2026-01-17T15:27:38.175227"}
{"image_index": 306, "image_name": "000000032887.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032887.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3473.007, "latencies_ms": [3473.007], "images_per_second": 0.288, "prompt_tokens": 21, "response_tokens_est": 53, "n_tiles": 12, "output_text": "The image depicts a hiker in a forested area, equipped with a backpack and trekking poles, navigating a rocky path. The hiker is surrounded by lush greenery and appears to be in a natural, possibly mountainous or forested region.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24930.2, "ram_available_mb": 100842.0, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 24930.4, "ram_available_mb": 100841.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.2, "peak": 16.66, "min": 14.12}, "VDD_CPU_SOC_MSS": {"avg": 15.79, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 34.15, "peak": 45.69, "min": 26.01}, "VIN": {"avg": 74.3, "peak": 110.57, "min": 66.67}}, "power_watts_avg": 34.15, "energy_joules_est": 118.63, "sample_count": 27, "duration_seconds": 3.474}, "timestamp": "2026-01-17T15:27:41.655413"}
{"image_index": 306, "image_name": "000000032887.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032887.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3876.156, "latencies_ms": [3876.156], "images_per_second": 0.258, "prompt_tokens": 19, "response_tokens_est": 65, "n_tiles": 12, "output_text": "The image features a person in a red long-sleeve shirt and blue jeans, equipped with a backpack and walking stick, standing on a rocky path surrounded by lush greenery. The lighting is natural, suggesting it is daytime, and the weather appears to be mild, with no visible signs of rain or extreme heat.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24930.4, "ram_available_mb": 100841.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 24930.4, "ram_available_mb": 100841.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.19, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.77, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 33.34, "peak": 44.91, "min": 26.0}, "VIN": {"avg": 73.35, "peak": 91.62, "min": 54.92}}, "power_watts_avg": 33.34, "energy_joules_est": 129.25, "sample_count": 30, "duration_seconds": 3.877}, "timestamp": "2026-01-17T15:27:45.538026"}
{"image_index": 307, "image_name": "000000032901.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032901.jpg", "image_width": 640, "image_height": 546, "image_resolution": "640x546", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2768.094, "latencies_ms": [2768.094], "images_per_second": 0.361, "prompt_tokens": 9, "response_tokens_est": 34, "n_tiles": 12, "output_text": "The image shows three men standing together in a room, with one man in the center wearing a suit and tie, and the other two men in casual attire.", "error": null, "sys_before": {"cpu_percent": 41.4, "ram_used_mb": 24936.3, "ram_available_mb": 100835.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 24925.8, "ram_available_mb": 100846.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.13, "peak": 16.66, "min": 13.39}, "VDD_CPU_SOC_MSS": {"avg": 15.86, "peak": 17.71, "min": 14.17}, "VDD_GPU": {"avg": 35.51, "peak": 44.1, "min": 26.79}, "VIN": {"avg": 78.19, "peak": 120.8, "min": 63.11}}, "power_watts_avg": 35.51, "energy_joules_est": 98.31, "sample_count": 21, "duration_seconds": 2.768}, "timestamp": "2026-01-17T15:27:48.408595"}
{"image_index": 307, "image_name": "000000032901.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032901.jpg", "image_width": 640, "image_height": 546, "image_resolution": "640x546", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2621.295, "latencies_ms": [2621.295], "images_per_second": 0.381, "prompt_tokens": 23, "response_tokens_est": 28, "n_tiles": 12, "output_text": "- Two men\n- Two men\n- Two men\n- Two men\n- Two men\n- Two men\n- Two men", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24925.8, "ram_available_mb": 100846.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 24926.3, "ram_available_mb": 100845.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.27, "peak": 16.56, "min": 14.12}, "VDD_CPU_SOC_MSS": {"avg": 15.96, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 37.16, "peak": 45.3, "min": 27.57}, "VIN": {"avg": 80.62, "peak": 124.12, "min": 67.26}}, "power_watts_avg": 37.16, "energy_joules_est": 97.43, "sample_count": 20, "duration_seconds": 2.622}, "timestamp": "2026-01-17T15:27:51.036446"}
{"image_index": 307, "image_name": "000000032901.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032901.jpg", "image_width": 640, "image_height": 546, "image_resolution": "640x546", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 5278.123, "latencies_ms": [5278.123], "images_per_second": 0.189, "prompt_tokens": 27, "response_tokens_est": 106, "n_tiles": 12, "output_text": "The main objects in the image are three men standing in a room. The man on the left is wearing a checkered shirt and has his left hand on the shoulder of the man next to him. The man in the middle is wearing a striped shirt and has his right hand on the shoulder of the man on the right. The man on the right is wearing a light blue shirt and has his left hand on the back of the man next to him. The background includes a red chair and a shelf with various bottles.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24926.3, "ram_available_mb": 100845.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 24924.6, "ram_available_mb": 100847.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.17, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.72, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 31.59, "peak": 46.46, "min": 26.0}, "VIN": {"avg": 73.64, "peak": 116.82, "min": 58.5}}, "power_watts_avg": 31.59, "energy_joules_est": 166.75, "sample_count": 42, "duration_seconds": 5.279}, "timestamp": "2026-01-17T15:27:56.321152"}
{"image_index": 307, "image_name": "000000032901.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032901.jpg", "image_width": 640, "image_height": 546, "image_resolution": "640x546", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 4496.23, "latencies_ms": [4496.23], "images_per_second": 0.222, "prompt_tokens": 21, "response_tokens_est": 83, "n_tiles": 12, "output_text": "The image depicts a group of three men standing in an indoor setting, likely a room or hall. They are smiling and appear to be posing for a photo. The men are dressed in casual to semi-formal attire, with one wearing a checkered shirt and the other two in button-up shirts. The background shows a red wall and a glimpse of a bar or counter with various items on it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24924.6, "ram_available_mb": 100847.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 24925.0, "ram_available_mb": 100847.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.17, "peak": 16.56, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.66, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 32.4, "peak": 45.28, "min": 25.6}, "VIN": {"avg": 74.45, "peak": 115.11, "min": 57.09}}, "power_watts_avg": 32.4, "energy_joules_est": 145.69, "sample_count": 35, "duration_seconds": 4.497}, "timestamp": "2026-01-17T15:28:00.824034"}
{"image_index": 307, "image_name": "000000032901.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032901.jpg", "image_width": 640, "image_height": 546, "image_resolution": "640x546", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 4935.453, "latencies_ms": [4935.453], "images_per_second": 0.203, "prompt_tokens": 19, "response_tokens_est": 95, "n_tiles": 12, "output_text": "The image features three men standing in an indoor setting. The man on the left is wearing a light blue checkered shirt and dark pants, the man in the middle is dressed in a white shirt and light-colored pants, and the man on the right is wearing a light blue shirt and dark pants. The lighting is warm and ambient, with a soft glow illuminating the scene. The colors are muted, with the men's clothing and the background being primarily neutral tones.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24924.7, "ram_available_mb": 100847.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 24924.9, "ram_available_mb": 100847.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.86, "peak": 16.26, "min": 13.9}, "VDD_CPU_SOC_MSS": {"avg": 15.25, "peak": 17.32, "min": 14.17}, "VDD_GPU": {"avg": 31.73, "peak": 45.69, "min": 25.6}, "VIN": {"avg": 71.6, "peak": 120.65, "min": 56.57}}, "power_watts_avg": 31.73, "energy_joules_est": 156.62, "sample_count": 39, "duration_seconds": 4.936}, "timestamp": "2026-01-17T15:28:05.766296"}
{"image_index": 308, "image_name": "000000032941.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032941.jpg", "image_width": 458, "image_height": 640, "image_resolution": "458x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2222.432, "latencies_ms": [2222.432], "images_per_second": 0.45, "prompt_tokens": 9, "response_tokens_est": 18, "n_tiles": 12, "output_text": "A yellow traffic sign is mounted on a black pole, indicating a one-way street.", "error": null, "sys_before": {"cpu_percent": 39.2, "ram_used_mb": 24944.9, "ram_available_mb": 100827.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.4, "ram_used_mb": 24942.8, "ram_available_mb": 100829.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.19, "peak": 16.66, "min": 13.29}, "VDD_CPU_SOC_MSS": {"avg": 16.0, "peak": 17.71, "min": 14.18}, "VDD_GPU": {"avg": 37.26, "peak": 43.71, "min": 29.16}, "VIN": {"avg": 83.61, "peak": 119.82, "min": 67.4}}, "power_watts_avg": 37.26, "energy_joules_est": 82.82, "sample_count": 17, "duration_seconds": 2.223}, "timestamp": "2026-01-17T15:28:08.083643"}
{"image_index": 308, "image_name": "000000032941.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032941.jpg", "image_width": 458, "image_height": 640, "image_resolution": "458x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 3017.766, "latencies_ms": [3017.766], "images_per_second": 0.331, "prompt_tokens": 23, "response_tokens_est": 40, "n_tiles": 12, "output_text": "object: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24942.8, "ram_available_mb": 100829.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 24940.2, "ram_available_mb": 100832.0, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.26, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.83, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 36.03, "peak": 46.09, "min": 26.79}, "VIN": {"avg": 75.63, "peak": 110.59, "min": 62.21}}, "power_watts_avg": 36.03, "energy_joules_est": 108.75, "sample_count": 23, "duration_seconds": 3.018}, "timestamp": "2026-01-17T15:28:11.108303"}
{"image_index": 308, "image_name": "000000032941.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032941.jpg", "image_width": 458, "image_height": 640, "image_resolution": "458x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3503.871, "latencies_ms": [3503.871], "images_per_second": 0.285, "prompt_tokens": 27, "response_tokens_est": 54, "n_tiles": 12, "output_text": "The main objects in the image are a traffic light, a yellow sign, and a black pole. The traffic light is positioned in the background, while the yellow sign is placed near the pole. The black pole is situated in the foreground, close to the curb.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24940.2, "ram_available_mb": 100832.0, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 24936.9, "ram_available_mb": 100835.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.24, "peak": 16.56, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.84, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 34.43, "peak": 44.89, "min": 26.01}, "VIN": {"avg": 74.51, "peak": 101.51, "min": 58.31}}, "power_watts_avg": 34.43, "energy_joules_est": 120.65, "sample_count": 26, "duration_seconds": 3.504}, "timestamp": "2026-01-17T15:28:14.618816"}
{"image_index": 308, "image_name": "000000032941.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032941.jpg", "image_width": 458, "image_height": 640, "image_resolution": "458x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 4955.585, "latencies_ms": [4955.585], "images_per_second": 0.202, "prompt_tokens": 21, "response_tokens_est": 97, "n_tiles": 12, "output_text": "The image depicts a street scene in an urban area during the daytime. The street is lined with buildings, and there is a traffic light visible in the foreground. A yellow sign with a black border is placed on the sidewalk, and a black metal structure is situated on the sidewalk. A few cars are visible on the road, and a red double-decker bus is parked on the side. The sky is overcast, and there are a few pedestrians walking on the sidewalk.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24936.9, "ram_available_mb": 100835.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 24937.4, "ram_available_mb": 100834.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.18, "peak": 16.66, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.61, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 31.75, "peak": 45.68, "min": 26.0}, "VIN": {"avg": 73.4, "peak": 115.59, "min": 55.85}}, "power_watts_avg": 31.75, "energy_joules_est": 157.36, "sample_count": 39, "duration_seconds": 4.956}, "timestamp": "2026-01-17T15:28:19.581639"}
{"image_index": 308, "image_name": "000000032941.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032941.jpg", "image_width": 458, "image_height": 640, "image_resolution": "458x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3777.575, "latencies_ms": [3777.575], "images_per_second": 0.265, "prompt_tokens": 19, "response_tokens_est": 62, "n_tiles": 12, "output_text": "The image depicts a street scene with a yellow traffic sign mounted on a black pole. The sign is illuminated by a red light, indicating a stop or caution. The surrounding area is wet, suggesting recent rain, and the buildings are made of brick and stone, with a cloudy sky overhead.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24937.4, "ram_available_mb": 100834.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 24937.6, "ram_available_mb": 100834.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.19, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.73, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 33.6, "peak": 44.89, "min": 26.0}, "VIN": {"avg": 75.09, "peak": 108.87, "min": 60.11}}, "power_watts_avg": 33.6, "energy_joules_est": 126.94, "sample_count": 29, "duration_seconds": 3.778}, "timestamp": "2026-01-17T15:28:23.365640"}
{"image_index": 309, "image_name": "000000033005.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033005.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1195.449, "latencies_ms": [1195.449], "images_per_second": 0.837, "prompt_tokens": 9, "response_tokens_est": 22, "n_tiles": 6, "output_text": "A young man is standing on a tennis court, preparing to hit a tennis ball with his racket.", "error": null, "sys_before": {"cpu_percent": 42.0, "ram_used_mb": 24959.3, "ram_available_mb": 100812.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 24961.0, "ram_available_mb": 100811.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.61, "peak": 15.14, "min": 13.72}, "VDD_CPU_SOC_MSS": {"avg": 15.22, "peak": 16.14, "min": 14.18}, "VDD_GPU": {"avg": 34.66, "peak": 40.97, "min": 27.97}, "VIN": {"avg": 76.04, "peak": 102.35, "min": 62.15}}, "power_watts_avg": 34.66, "energy_joules_est": 41.45, "sample_count": 9, "duration_seconds": 1.196}, "timestamp": "2026-01-17T15:28:24.626620"}
{"image_index": 309, "image_name": "000000033005.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033005.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1797.84, "latencies_ms": [1797.84], "images_per_second": 0.556, "prompt_tokens": 23, "response_tokens_est": 41, "n_tiles": 6, "output_text": "1. Tennis court\n2. Tennis player\n3. Tennis racket\n4. Tennis ball\n5. Fence\n6. Green net\n7. Green fence\n8. Red sign", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24961.0, "ram_available_mb": 100811.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 24961.8, "ram_available_mb": 100810.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.84, "peak": 15.24, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.5, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 32.84, "peak": 41.34, "min": 24.42}, "VIN": {"avg": 69.56, "peak": 105.83, "min": 60.32}}, "power_watts_avg": 32.84, "energy_joules_est": 59.06, "sample_count": 13, "duration_seconds": 1.798}, "timestamp": "2026-01-17T15:28:26.434929"}
{"image_index": 309, "image_name": "000000033005.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033005.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2593.862, "latencies_ms": [2593.862], "images_per_second": 0.386, "prompt_tokens": 27, "response_tokens_est": 69, "n_tiles": 6, "output_text": "The main object in the foreground is a young man standing on a tennis court, holding a tennis racket. The background features a green fence and a sign with the text \"VOX SPORTS\" and \"prince rule the court.\" The tennis court is surrounded by a green fence, and the sign is placed near the fence.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24961.8, "ram_available_mb": 100810.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 24962.5, "ram_available_mb": 100809.7, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.88, "peak": 15.24, "min": 14.2}, "VDD_CPU_SOC_MSS": {"avg": 15.33, "peak": 15.75, "min": 14.56}, "VDD_GPU": {"avg": 28.96, "peak": 40.56, "min": 23.25}, "VIN": {"avg": 68.28, "peak": 104.21, "min": 62.53}}, "power_watts_avg": 28.96, "energy_joules_est": 75.14, "sample_count": 20, "duration_seconds": 2.594}, "timestamp": "2026-01-17T15:28:29.035525"}
{"image_index": 309, "image_name": "000000033005.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033005.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2760.83, "latencies_ms": [2760.83], "images_per_second": 0.362, "prompt_tokens": 21, "response_tokens_est": 75, "n_tiles": 6, "output_text": "The image depicts a young male tennis player on a tennis court during nighttime. He is dressed in a white shirt and black shorts, holding a tennis racket, and appears to be preparing to serve or return a shot. The court is enclosed by a green fence, and there is a sign with the text \"VOX SPORTS\" in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24962.5, "ram_available_mb": 100809.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 24963.2, "ram_available_mb": 100808.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.91, "peak": 15.24, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.52, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 28.55, "peak": 40.18, "min": 22.85}, "VIN": {"avg": 69.4, "peak": 114.17, "min": 55.79}}, "power_watts_avg": 28.55, "energy_joules_est": 78.83, "sample_count": 21, "duration_seconds": 2.761}, "timestamp": "2026-01-17T15:28:31.802991"}
{"image_index": 309, "image_name": "000000033005.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033005.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1515.638, "latencies_ms": [1515.638], "images_per_second": 0.66, "prompt_tokens": 19, "response_tokens_est": 31, "n_tiles": 6, "output_text": "The image depicts a tennis court at night, illuminated by artificial lighting. The court surface is green, and the net is made of metal.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24963.2, "ram_available_mb": 100808.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 24963.2, "ram_available_mb": 100808.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.8, "peak": 15.24, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.35, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 33.27, "peak": 40.57, "min": 25.6}, "VIN": {"avg": 71.78, "peak": 89.5, "min": 59.93}}, "power_watts_avg": 33.27, "energy_joules_est": 50.44, "sample_count": 11, "duration_seconds": 1.516}, "timestamp": "2026-01-17T15:28:33.324750"}
{"image_index": 310, "image_name": "000000033104.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033104.jpg", "image_width": 428, "image_height": 500, "image_resolution": "428x500", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 3133.844, "latencies_ms": [3133.844], "images_per_second": 0.319, "prompt_tokens": 9, "response_tokens_est": 45, "n_tiles": 12, "output_text": "A group of skiers is gathered around a net, with one skier in a white helmet and another in a black helmet, both wearing protective gear, and a third skier in a red helmet is also present.", "error": null, "sys_before": {"cpu_percent": 43.8, "ram_used_mb": 24917.0, "ram_available_mb": 100855.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 24917.5, "ram_available_mb": 100854.7, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.15, "peak": 16.76, "min": 13.39}, "VDD_CPU_SOC_MSS": {"avg": 15.85, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 34.22, "peak": 44.12, "min": 26.01}, "VIN": {"avg": 79.24, "peak": 126.62, "min": 57.4}}, "power_watts_avg": 34.22, "energy_joules_est": 107.25, "sample_count": 24, "duration_seconds": 3.134}, "timestamp": "2026-01-17T15:28:36.556157"}
{"image_index": 310, "image_name": "000000033104.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033104.jpg", "image_width": 428, "image_height": 500, "image_resolution": "428x500", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 6031.027, "latencies_ms": [6031.027], "images_per_second": 0.166, "prompt_tokens": 23, "response_tokens_est": 129, "n_tiles": 12, "output_text": "object: helmet\n1\nobject: helmet\n1\nobject: helmet\n1\nobject: helmet\n1\nobject: helmet\n1\nobject: helmet\n1\nobject: helmet\n1\nobject: helmet\n1\nobject: helmet\n1\nobject: helmet\n1\nobject: helmet\n1\nobject: helmet\n1\nobject: helmet\n1\nobject: helmet\n1\nobject: helmet\n1\nobject: helmet\n1\nobject: helmet\n1\nobject: helmet\n1\nobject: helmet\n1\nobject: helmet\n1\nobject: helmet\n1\nobject:", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 24917.5, "ram_available_mb": 100854.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 24917.7, "ram_available_mb": 100854.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.13, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.6, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 30.73, "peak": 45.3, "min": 26.0}, "VIN": {"avg": 73.33, "peak": 117.68, "min": 61.14}}, "power_watts_avg": 30.73, "energy_joules_est": 185.34, "sample_count": 47, "duration_seconds": 6.031}, "timestamp": "2026-01-17T15:28:42.593505"}
{"image_index": 310, "image_name": "000000033104.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033104.jpg", "image_width": 428, "image_height": 500, "image_resolution": "428x500", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 4149.845, "latencies_ms": [4149.845], "images_per_second": 0.241, "prompt_tokens": 27, "response_tokens_est": 73, "n_tiles": 12, "output_text": "The main objects in the image are skiers and a net. The skiers are positioned in the foreground, with one skier in the center foreground and another skier on the right side. The net is situated between the skiers, with the skiers standing near it. The background features a snowy landscape with other skiers and a blue fence.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24917.7, "ram_available_mb": 100854.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 24918.2, "ram_available_mb": 100854.0, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.2, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.7, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 32.84, "peak": 44.89, "min": 26.0}, "VIN": {"avg": 74.28, "peak": 112.47, "min": 57.65}}, "power_watts_avg": 32.84, "energy_joules_est": 136.29, "sample_count": 32, "duration_seconds": 4.15}, "timestamp": "2026-01-17T15:28:46.751230"}
{"image_index": 310, "image_name": "000000033104.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033104.jpg", "image_width": 428, "image_height": 500, "image_resolution": "428x500", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 4150.143, "latencies_ms": [4150.143], "images_per_second": 0.241, "prompt_tokens": 21, "response_tokens_est": 73, "n_tiles": 12, "output_text": "The image depicts a group of skiers in a snowy environment, likely at a ski resort. They are gathered around a blue netting, possibly for safety or to mark a specific area. The skiers are dressed in winter gear, including helmets, goggles, and ski jackets, and are engaged in conversation or preparing for their next run.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24918.2, "ram_available_mb": 100854.0, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 24918.4, "ram_available_mb": 100853.7, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.18, "peak": 16.56, "min": 14.12}, "VDD_CPU_SOC_MSS": {"avg": 15.78, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 32.94, "peak": 44.91, "min": 26.0}, "VIN": {"avg": 75.35, "peak": 117.0, "min": 58.22}}, "power_watts_avg": 32.94, "energy_joules_est": 136.71, "sample_count": 31, "duration_seconds": 4.15}, "timestamp": "2026-01-17T15:28:50.908099"}
{"image_index": 310, "image_name": "000000033104.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033104.jpg", "image_width": 428, "image_height": 500, "image_resolution": "428x500", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 4288.352, "latencies_ms": [4288.352], "images_per_second": 0.233, "prompt_tokens": 19, "response_tokens_est": 77, "n_tiles": 12, "output_text": "The image depicts a snowy scene with a blue netting fence in the foreground, holding skis and poles. The skiers are wearing helmets and jackets, and the snow appears to be freshly fallen. The lighting is bright, indicating it is daytime, and the overall weather seems to be cold, as evidenced by the snow and the skiers' attire.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24918.4, "ram_available_mb": 100853.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 24919.2, "ram_available_mb": 100853.0, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.16, "peak": 16.56, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.63, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 32.75, "peak": 45.68, "min": 26.0}, "VIN": {"avg": 74.09, "peak": 112.73, "min": 59.32}}, "power_watts_avg": 32.75, "energy_joules_est": 140.46, "sample_count": 33, "duration_seconds": 4.289}, "timestamp": "2026-01-17T15:28:55.203130"}
{"image_index": 311, "image_name": "000000033109.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033109.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2399.493, "latencies_ms": [2399.493], "images_per_second": 0.417, "prompt_tokens": 9, "response_tokens_est": 23, "n_tiles": 12, "output_text": "A large blue truck is driving down a wet street, with other vehicles and a residential area in the background.", "error": null, "sys_before": {"cpu_percent": 42.4, "ram_used_mb": 24939.4, "ram_available_mb": 100832.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.4, "ram_used_mb": 24938.6, "ram_available_mb": 100833.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.14, "peak": 16.66, "min": 13.19}, "VDD_CPU_SOC_MSS": {"avg": 15.88, "peak": 17.71, "min": 14.17}, "VDD_GPU": {"avg": 36.72, "peak": 44.1, "min": 28.36}, "VIN": {"avg": 78.82, "peak": 113.28, "min": 59.23}}, "power_watts_avg": 36.72, "energy_joules_est": 88.12, "sample_count": 18, "duration_seconds": 2.4}, "timestamp": "2026-01-17T15:28:57.719991"}
{"image_index": 311, "image_name": "000000033109.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033109.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2979.691, "latencies_ms": [2979.691], "images_per_second": 0.336, "prompt_tokens": 23, "response_tokens_est": 38, "n_tiles": 12, "output_text": "1. Blue truck\n2. Blue car\n3. Bus\n4. House\n5. Street light\n6. Tree\n7. Sidewalk\n8. Green grass", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24938.6, "ram_available_mb": 100833.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 24934.1, "ram_available_mb": 100838.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.21, "peak": 16.66, "min": 14.12}, "VDD_CPU_SOC_MSS": {"avg": 15.82, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 36.04, "peak": 44.89, "min": 26.39}, "VIN": {"avg": 76.5, "peak": 114.82, "min": 59.1}}, "power_watts_avg": 36.04, "energy_joules_est": 107.41, "sample_count": 22, "duration_seconds": 2.98}, "timestamp": "2026-01-17T15:29:00.706209"}
{"image_index": 311, "image_name": "000000033109.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033109.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 4250.127, "latencies_ms": [4250.127], "images_per_second": 0.235, "prompt_tokens": 27, "response_tokens_est": 75, "n_tiles": 12, "output_text": "The main object in the foreground is a blue truck with the text \"HUISMANGROUP.COM\" on its side. The truck is positioned on the right side of the road, near the curb. In the background, there are residential houses, a street lamp, and a tree. The truck is relatively close to the sidewalk, indicating it is near the road.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24934.1, "ram_available_mb": 100838.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 24932.5, "ram_available_mb": 100839.7, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.89, "peak": 16.36, "min": 14.02}, "VDD_CPU_SOC_MSS": {"avg": 15.32, "peak": 17.32, "min": 14.17}, "VDD_GPU": {"avg": 32.69, "peak": 46.09, "min": 25.6}, "VIN": {"avg": 72.37, "peak": 112.98, "min": 58.41}}, "power_watts_avg": 32.69, "energy_joules_est": 138.96, "sample_count": 33, "duration_seconds": 4.251}, "timestamp": "2026-01-17T15:29:04.963482"}
{"image_index": 311, "image_name": "000000033109.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033109.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3713.51, "latencies_ms": [3713.51], "images_per_second": 0.269, "prompt_tokens": 21, "response_tokens_est": 60, "n_tiles": 12, "output_text": "The image depicts a wet street scene with a blue truck driving on the road. The truck is followed by a bus and several cars, all navigating the wet road. The setting appears to be a residential area with houses and trees in the background, and the weather seems to be overcast.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24932.5, "ram_available_mb": 100839.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 24933.7, "ram_available_mb": 100838.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.93, "peak": 16.36, "min": 13.92}, "VDD_CPU_SOC_MSS": {"avg": 15.45, "peak": 17.32, "min": 14.17}, "VDD_GPU": {"avg": 33.54, "peak": 44.91, "min": 25.6}, "VIN": {"avg": 73.89, "peak": 114.4, "min": 63.1}}, "power_watts_avg": 33.54, "energy_joules_est": 124.57, "sample_count": 29, "duration_seconds": 3.714}, "timestamp": "2026-01-17T15:29:08.684734"}
{"image_index": 311, "image_name": "000000033109.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033109.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3757.98, "latencies_ms": [3757.98], "images_per_second": 0.266, "prompt_tokens": 19, "response_tokens_est": 61, "n_tiles": 12, "output_text": "The image depicts a blue truck with a white stripe running along its side, parked on a wet street. The truck's design features a large windshield and a prominent logo on the front. The surrounding environment includes a residential area with houses, a wet street, and a partly cloudy sky.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24933.7, "ram_available_mb": 100838.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 24933.7, "ram_available_mb": 100838.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.91, "peak": 16.36, "min": 14.02}, "VDD_CPU_SOC_MSS": {"avg": 15.49, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 33.62, "peak": 45.69, "min": 25.6}, "VIN": {"avg": 75.05, "peak": 120.6, "min": 58.23}}, "power_watts_avg": 33.62, "energy_joules_est": 126.35, "sample_count": 29, "duration_seconds": 3.758}, "timestamp": "2026-01-17T15:29:12.449491"}
{"image_index": 312, "image_name": "000000033114.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033114.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2864.71, "latencies_ms": [2864.71], "images_per_second": 0.349, "prompt_tokens": 9, "response_tokens_est": 37, "n_tiles": 12, "output_text": "The image shows a view of an airport runway with a plane on the tarmac, surrounded by various airport equipment and signs, including traffic lights and a red fire hydrant.", "error": null, "sys_before": {"cpu_percent": 40.2, "ram_used_mb": 24936.2, "ram_available_mb": 100835.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 24933.1, "ram_available_mb": 100839.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.14, "peak": 16.76, "min": 13.29}, "VDD_CPU_SOC_MSS": {"avg": 15.83, "peak": 17.71, "min": 14.17}, "VDD_GPU": {"avg": 35.11, "peak": 44.1, "min": 26.8}, "VIN": {"avg": 78.03, "peak": 110.38, "min": 62.47}}, "power_watts_avg": 35.11, "energy_joules_est": 100.6, "sample_count": 22, "duration_seconds": 2.865}, "timestamp": "2026-01-17T15:29:15.410315"}
{"image_index": 312, "image_name": "000000033114.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033114.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 3002.12, "latencies_ms": [3002.12], "images_per_second": 0.333, "prompt_tokens": 23, "response_tokens_est": 40, "n_tiles": 12, "output_text": "object: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24933.1, "ram_available_mb": 100839.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 24932.3, "ram_available_mb": 100839.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.29, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.85, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 35.74, "peak": 45.69, "min": 26.79}, "VIN": {"avg": 75.44, "peak": 111.54, "min": 66.51}}, "power_watts_avg": 35.74, "energy_joules_est": 107.32, "sample_count": 23, "duration_seconds": 3.003}, "timestamp": "2026-01-17T15:29:18.419369"}
{"image_index": 312, "image_name": "000000033114.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033114.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 4465.489, "latencies_ms": [4465.489], "images_per_second": 0.224, "prompt_tokens": 27, "response_tokens_est": 83, "n_tiles": 12, "output_text": "The main objects in the image are a series of traffic lights positioned in the foreground, with a row of red and white traffic lights in the middle ground, and a large airplane in the background. The traffic lights are situated on a concrete surface, while the airplane is on the tarmac. The background features a hazy landscape with mountains and a body of water, creating a sense of depth and distance.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24932.3, "ram_available_mb": 100839.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 24933.5, "ram_available_mb": 100838.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.25, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.83, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 32.56, "peak": 44.89, "min": 26.0}, "VIN": {"avg": 75.91, "peak": 115.6, "min": 63.35}}, "power_watts_avg": 32.56, "energy_joules_est": 145.41, "sample_count": 35, "duration_seconds": 4.466}, "timestamp": "2026-01-17T15:29:22.891493"}
{"image_index": 312, "image_name": "000000033114.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033114.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3521.648, "latencies_ms": [3521.648], "images_per_second": 0.284, "prompt_tokens": 21, "response_tokens_est": 55, "n_tiles": 12, "output_text": "The image depicts a scene at an airport with a large airplane on the tarmac, preparing to take off or having just landed. The background shows a hazy, overcast sky and a distant mountain range, indicating a possibly humid or foggy day.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24933.5, "ram_available_mb": 100838.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 24936.5, "ram_available_mb": 100835.7, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.25, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.77, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 34.41, "peak": 45.69, "min": 26.0}, "VIN": {"avg": 75.69, "peak": 113.81, "min": 53.44}}, "power_watts_avg": 34.41, "energy_joules_est": 121.19, "sample_count": 27, "duration_seconds": 3.522}, "timestamp": "2026-01-17T15:29:26.419892"}
{"image_index": 312, "image_name": "000000033114.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033114.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 4113.311, "latencies_ms": [4113.311], "images_per_second": 0.243, "prompt_tokens": 19, "response_tokens_est": 72, "n_tiles": 12, "output_text": "The image features a scene with a large airplane on the tarmac, surrounded by a hazy, foggy atmosphere. The airplane is predominantly white with blue accents, and the tarmac is wet, reflecting the surrounding environment. The lighting is soft and diffused, likely due to the fog, and the overall weather appears to be overcast.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24936.5, "ram_available_mb": 100835.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 24937.0, "ram_available_mb": 100835.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.18, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.66, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 33.05, "peak": 44.89, "min": 26.0}, "VIN": {"avg": 74.25, "peak": 111.9, "min": 64.71}}, "power_watts_avg": 33.05, "energy_joules_est": 135.96, "sample_count": 32, "duration_seconds": 4.114}, "timestamp": "2026-01-17T15:29:30.540020"}
{"image_index": 313, "image_name": "000000033221.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033221.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1224.108, "latencies_ms": [1224.108], "images_per_second": 0.817, "prompt_tokens": 9, "response_tokens_est": 23, "n_tiles": 6, "output_text": "A group of people is walking on a sandy beach, with one person carrying a baby in a carrier.", "error": null, "sys_before": {"cpu_percent": 46.9, "ram_used_mb": 24952.9, "ram_available_mb": 100819.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 24954.9, "ram_available_mb": 100817.3, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.58, "peak": 15.04, "min": 13.82}, "VDD_CPU_SOC_MSS": {"avg": 15.18, "peak": 15.74, "min": 14.18}, "VDD_GPU": {"avg": 34.92, "peak": 40.57, "min": 28.36}, "VIN": {"avg": 69.37, "peak": 78.31, "min": 60.86}}, "power_watts_avg": 34.92, "energy_joules_est": 42.77, "sample_count": 9, "duration_seconds": 1.225}, "timestamp": "2026-01-17T15:29:31.825660"}
{"image_index": 313, "image_name": "000000033221.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033221.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2210.413, "latencies_ms": [2210.413], "images_per_second": 0.452, "prompt_tokens": 23, "response_tokens_est": 56, "n_tiles": 6, "output_text": "1. Woman: 2\n2. Woman: 2\n3. Woman: 2\n4. Woman: 2\n5. Woman: 2\n6. Woman: 2\n7. Woman: 2\n8. Woman: 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24954.9, "ram_available_mb": 100817.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 24955.4, "ram_available_mb": 100816.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.92, "peak": 15.24, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.58, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 30.93, "peak": 42.13, "min": 23.24}, "VIN": {"avg": 72.71, "peak": 108.13, "min": 63.74}}, "power_watts_avg": 30.93, "energy_joules_est": 68.38, "sample_count": 17, "duration_seconds": 2.211}, "timestamp": "2026-01-17T15:29:34.042398"}
{"image_index": 313, "image_name": "000000033221.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033221.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2644.911, "latencies_ms": [2644.911], "images_per_second": 0.378, "prompt_tokens": 27, "response_tokens_est": 71, "n_tiles": 6, "output_text": "In the image, the main objects are a group of people walking on the sandy beach. The person in the foreground is holding a blue towel and wearing an orange shirt. The person in the background is holding a black baton. The group is walking towards the left side of the image, with the beach and the ocean visible in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24955.4, "ram_available_mb": 100816.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 24956.9, "ram_available_mb": 100815.3, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.89, "peak": 15.24, "min": 14.3}, "VDD_CPU_SOC_MSS": {"avg": 15.55, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 29.03, "peak": 40.56, "min": 22.86}, "VIN": {"avg": 70.76, "peak": 109.29, "min": 59.2}}, "power_watts_avg": 29.03, "energy_joules_est": 76.8, "sample_count": 20, "duration_seconds": 2.646}, "timestamp": "2026-01-17T15:29:36.698061"}
{"image_index": 313, "image_name": "000000033221.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033221.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2357.504, "latencies_ms": [2357.504], "images_per_second": 0.424, "prompt_tokens": 21, "response_tokens_est": 61, "n_tiles": 6, "output_text": "The scene is set on a sandy beach with a group of people walking along the shore. The individuals are dressed in casual beach attire, and some are carrying bags. The setting appears to be a sunny day with clear skies, and there are some trees and a mountain range in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24956.9, "ram_available_mb": 100815.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 24956.6, "ram_available_mb": 100815.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.86, "peak": 15.14, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.42, "peak": 15.75, "min": 14.56}, "VDD_GPU": {"avg": 29.65, "peak": 40.95, "min": 23.24}, "VIN": {"avg": 71.36, "peak": 114.11, "min": 61.94}}, "power_watts_avg": 29.65, "energy_joules_est": 69.92, "sample_count": 18, "duration_seconds": 2.358}, "timestamp": "2026-01-17T15:29:39.062252"}
{"image_index": 313, "image_name": "000000033221.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033221.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1821.422, "latencies_ms": [1821.422], "images_per_second": 0.549, "prompt_tokens": 19, "response_tokens_est": 42, "n_tiles": 6, "output_text": "The image depicts a sunny beach scene with clear blue skies and a few scattered clouds. The sandy beach is dotted with orange traffic cones, and a few people are walking along the shoreline.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24956.6, "ram_available_mb": 100815.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 24957.6, "ram_available_mb": 100814.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.86, "peak": 15.24, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.33, "peak": 15.74, "min": 14.57}, "VDD_GPU": {"avg": 31.46, "peak": 40.97, "min": 24.04}, "VIN": {"avg": 76.53, "peak": 112.55, "min": 57.92}}, "power_watts_avg": 31.46, "energy_joules_est": 57.33, "sample_count": 14, "duration_seconds": 1.822}, "timestamp": "2026-01-17T15:29:40.890174"}
{"image_index": 314, "image_name": "000000033368.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033368.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1363.743, "latencies_ms": [1363.743], "images_per_second": 0.733, "prompt_tokens": 9, "response_tokens_est": 27, "n_tiles": 6, "output_text": "A tennis player is standing on a blue court, holding a tennis racket, and appears to be contemplating his next move.", "error": null, "sys_before": {"cpu_percent": 42.9, "ram_used_mb": 24957.6, "ram_available_mb": 100814.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 24958.6, "ram_available_mb": 100813.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.57, "peak": 15.04, "min": 13.9}, "VDD_CPU_SOC_MSS": {"avg": 15.2, "peak": 15.74, "min": 14.56}, "VDD_GPU": {"avg": 33.56, "peak": 40.18, "min": 26.39}, "VIN": {"avg": 78.83, "peak": 112.24, "min": 59.73}}, "power_watts_avg": 33.56, "energy_joules_est": 45.78, "sample_count": 10, "duration_seconds": 1.364}, "timestamp": "2026-01-17T15:29:42.308462"}
{"image_index": 314, "image_name": "000000033368.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033368.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1796.253, "latencies_ms": [1796.253], "images_per_second": 0.557, "prompt_tokens": 23, "response_tokens_est": 41, "n_tiles": 6, "output_text": "1. Tennis player\n2. Tennis racket\n3. Tennis ball\n4. Tennis court\n5. Blue surface\n6. White shorts\n7. White socks\n8. White shoes", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24958.6, "ram_available_mb": 100813.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 24958.8, "ram_available_mb": 100813.3, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.84, "peak": 15.24, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.55, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 31.88, "peak": 40.95, "min": 24.03}, "VIN": {"avg": 74.17, "peak": 119.5, "min": 62.98}}, "power_watts_avg": 31.88, "energy_joules_est": 57.28, "sample_count": 14, "duration_seconds": 1.797}, "timestamp": "2026-01-17T15:29:44.110902"}
{"image_index": 314, "image_name": "000000033368.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033368.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2879.371, "latencies_ms": [2879.371], "images_per_second": 0.347, "prompt_tokens": 27, "response_tokens_est": 79, "n_tiles": 6, "output_text": "The main object in the image is a tennis player standing on a blue tennis court. The player is positioned in the foreground, slightly to the left, and is holding a tennis racket in his right hand. The court is marked with white lines and has a visible logo near the bottom right corner. The background is mostly dark, focusing the viewer's attention on the player and the court.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24958.8, "ram_available_mb": 100813.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 24959.1, "ram_available_mb": 100813.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.87, "peak": 15.24, "min": 14.3}, "VDD_CPU_SOC_MSS": {"avg": 15.59, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 28.63, "peak": 40.97, "min": 22.86}, "VIN": {"avg": 71.32, "peak": 119.76, "min": 55.48}}, "power_watts_avg": 28.63, "energy_joules_est": 82.45, "sample_count": 22, "duration_seconds": 2.88}, "timestamp": "2026-01-17T15:29:46.997697"}
{"image_index": 314, "image_name": "000000033368.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033368.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3245.672, "latencies_ms": [3245.672], "images_per_second": 0.308, "prompt_tokens": 21, "response_tokens_est": 92, "n_tiles": 6, "output_text": "The image depicts a tennis player on a blue court, likely during a match or practice session. The player is dressed in a light blue shirt, white shorts, and white socks, holding a tennis racket in his right hand. The court is marked with the word \"TENNIS\" in white letters, and the player appears to be in a moment of rest or contemplation, possibly after a point or during a break in the game.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24959.1, "ram_available_mb": 100813.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 24959.1, "ram_available_mb": 100813.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.9, "peak": 15.24, "min": 14.3}, "VDD_CPU_SOC_MSS": {"avg": 15.57, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 27.73, "peak": 40.57, "min": 22.85}, "VIN": {"avg": 71.6, "peak": 115.59, "min": 59.07}}, "power_watts_avg": 27.73, "energy_joules_est": 90.01, "sample_count": 25, "duration_seconds": 3.246}, "timestamp": "2026-01-17T15:29:50.249832"}
{"image_index": 314, "image_name": "000000033368.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033368.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2443.22, "latencies_ms": [2443.22], "images_per_second": 0.409, "prompt_tokens": 19, "response_tokens_est": 64, "n_tiles": 6, "output_text": "The image depicts a tennis player on a blue court, illuminated by bright sunlight. The player is wearing a light blue shirt, white shorts, and white socks, with a tennis racket in hand. The bright lighting casts a shadow of the player on the court, highlighting the texture of the blue surface.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24959.1, "ram_available_mb": 100813.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 24960.1, "ram_available_mb": 100812.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.86, "peak": 15.14, "min": 14.3}, "VDD_CPU_SOC_MSS": {"avg": 15.5, "peak": 15.75, "min": 14.57}, "VDD_GPU": {"avg": 29.4, "peak": 40.95, "min": 23.24}, "VIN": {"avg": 69.86, "peak": 108.81, "min": 62.44}}, "power_watts_avg": 29.4, "energy_joules_est": 71.84, "sample_count": 19, "duration_seconds": 2.444}, "timestamp": "2026-01-17T15:29:52.699204"}
{"image_index": 315, "image_name": "000000033638.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033638.jpg", "image_width": 425, "image_height": 640, "image_resolution": "425x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1082.63, "latencies_ms": [1082.63], "images_per_second": 0.924, "prompt_tokens": 9, "response_tokens_est": 18, "n_tiles": 6, "output_text": "A woman is standing by a black stove, looking at a bowl on the counter.", "error": null, "sys_before": {"cpu_percent": 41.5, "ram_used_mb": 24960.1, "ram_available_mb": 100812.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 24961.0, "ram_available_mb": 100811.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.61, "peak": 15.14, "min": 13.79}, "VDD_CPU_SOC_MSS": {"avg": 15.35, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 35.2, "peak": 40.57, "min": 29.54}, "VIN": {"avg": 83.23, "peak": 120.67, "min": 58.75}}, "power_watts_avg": 35.2, "energy_joules_est": 38.12, "sample_count": 8, "duration_seconds": 1.083}, "timestamp": "2026-01-17T15:29:53.831557"}
{"image_index": 315, "image_name": "000000033638.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033638.jpg", "image_width": 425, "image_height": 640, "image_resolution": "425x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2026.747, "latencies_ms": [2026.747], "images_per_second": 0.493, "prompt_tokens": 23, "response_tokens_est": 50, "n_tiles": 6, "output_text": "1. Kitchen counter\n2. Kitchen sink\n3. Kitchen cabinet\n4. Kitchen utensils\n5. Kitchen utensils\n6. Kitchen utensils\n7. Kitchen utensils\n8. Kitchen utensils", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24961.0, "ram_available_mb": 100811.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 24961.0, "ram_available_mb": 100811.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.95, "peak": 15.34, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.51, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 32.06, "peak": 42.15, "min": 24.03}, "VIN": {"avg": 74.53, "peak": 112.26, "min": 61.74}}, "power_watts_avg": 32.06, "energy_joules_est": 64.99, "sample_count": 15, "duration_seconds": 2.027}, "timestamp": "2026-01-17T15:29:55.864638"}
{"image_index": 315, "image_name": "000000033638.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033638.jpg", "image_width": 425, "image_height": 640, "image_resolution": "425x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3597.104, "latencies_ms": [3597.104], "images_per_second": 0.278, "prompt_tokens": 27, "response_tokens_est": 106, "n_tiles": 6, "output_text": "In the image, the main object is a black, vintage-style stove with a brick base. To the left of the stove, there is a woman standing, bending over slightly, and near the stove, there are various kitchen items such as a blue bowl, a white bowl, a blue can, and a metal pot. The background features a white cabinet, a window with a white curtain, and a hanging light fixture. The stove is positioned in the foreground, while the woman and kitchen items are in the midground.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 24961.0, "ram_available_mb": 100811.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 24961.1, "ram_available_mb": 100811.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.01, "peak": 15.24, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.61, "peak": 15.75, "min": 14.56}, "VDD_GPU": {"avg": 27.49, "peak": 40.57, "min": 23.25}, "VIN": {"avg": 68.35, "peak": 107.1, "min": 60.24}}, "power_watts_avg": 27.49, "energy_joules_est": 98.89, "sample_count": 28, "duration_seconds": 3.597}, "timestamp": "2026-01-17T15:29:59.468138"}
{"image_index": 315, "image_name": "000000033638.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033638.jpg", "image_width": 425, "image_height": 640, "image_resolution": "425x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2112.081, "latencies_ms": [2112.081], "images_per_second": 0.473, "prompt_tokens": 21, "response_tokens_est": 53, "n_tiles": 6, "output_text": "The image depicts a kitchen scene with a woman standing by a black stove, seemingly engaged in cooking or preparing food. The kitchen is cluttered with various kitchen items, including plates, bowls, and utensils, indicating a busy and lived-in space.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24961.1, "ram_available_mb": 100811.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 24961.3, "ram_available_mb": 100810.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.94, "peak": 15.24, "min": 14.3}, "VDD_CPU_SOC_MSS": {"avg": 15.5, "peak": 15.75, "min": 14.56}, "VDD_GPU": {"avg": 30.4, "peak": 40.18, "min": 23.64}, "VIN": {"avg": 71.83, "peak": 110.62, "min": 62.86}}, "power_watts_avg": 30.4, "energy_joules_est": 64.22, "sample_count": 16, "duration_seconds": 2.112}, "timestamp": "2026-01-17T15:30:01.587248"}
{"image_index": 315, "image_name": "000000033638.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033638.jpg", "image_width": 425, "image_height": 640, "image_resolution": "425x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2310.041, "latencies_ms": [2310.041], "images_per_second": 0.433, "prompt_tokens": 19, "response_tokens_est": 59, "n_tiles": 6, "output_text": "The image depicts a kitchen with a rustic, vintage aesthetic. The room is dimly lit, with a focus on the black stove and the woman standing near it. The woman is wearing a floral top and beige shorts, and she appears to be engaged in cooking or preparing food.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24961.3, "ram_available_mb": 100810.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 24962.3, "ram_available_mb": 100809.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.88, "peak": 15.34, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.57, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 29.72, "peak": 40.57, "min": 23.25}, "VIN": {"avg": 71.16, "peak": 113.15, "min": 62.94}}, "power_watts_avg": 29.72, "energy_joules_est": 68.66, "sample_count": 18, "duration_seconds": 2.31}, "timestamp": "2026-01-17T15:30:03.903487"}
{"image_index": 316, "image_name": "000000033707.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033707.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1708.983, "latencies_ms": [1708.983], "images_per_second": 0.585, "prompt_tokens": 9, "response_tokens_est": 74, "n_tiles": 1, "output_text": "The image depicts a serene scene of two giraffes standing close to each other, with one giraffe slightly taller than the other, both displaying their characteristic long necks and spotted coats. They are positioned in front of a beige building with a wooden door, surrounded by a rocky landscape and lush greenery, creating a tranquil and natural environment.", "error": null, "sys_before": {"cpu_percent": 5.3, "ram_used_mb": 24962.3, "ram_available_mb": 100809.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24963.0, "ram_available_mb": 100809.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 4845.7, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.31, "peak": 15.62, "min": 14.4}, "VDD_CPU_SOC_MSS": {"avg": 16.57, "peak": 16.93, "min": 15.36}, "VDD_GPU": {"avg": 21.34, "peak": 25.61, "min": 19.7}, "VIN": {"avg": 65.16, "peak": 73.07, "min": 62.4}}, "power_watts_avg": 21.34, "energy_joules_est": 36.48, "sample_count": 13, "duration_seconds": 1.709}, "timestamp": "2026-01-17T15:30:05.633344"}
{"image_index": 316, "image_name": "000000033707.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033707.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1038.271, "latencies_ms": [1038.271], "images_per_second": 0.963, "prompt_tokens": 23, "response_tokens_est": 44, "n_tiles": 1, "output_text": "- giraffe: 2\n- building: 1\n- tree: 1\n- fence: 1\n- rocks: 1\n- dirt path: 1\n- grass: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24963.0, "ram_available_mb": 100809.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24963.2, "ram_available_mb": 100808.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 4856.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.43, "peak": 15.62, "min": 15.11}, "VDD_CPU_SOC_MSS": {"avg": 16.68, "peak": 16.93, "min": 16.54}, "VDD_GPU": {"avg": 21.57, "peak": 24.03, "min": 20.09}, "VIN": {"avg": 64.03, "peak": 66.28, "min": 60.59}}, "power_watts_avg": 21.57, "energy_joules_est": 22.4, "sample_count": 8, "duration_seconds": 1.039}, "timestamp": "2026-01-17T15:30:06.677437"}
{"image_index": 316, "image_name": "000000033707.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033707.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1351.275, "latencies_ms": [1351.275], "images_per_second": 0.74, "prompt_tokens": 27, "response_tokens_est": 58, "n_tiles": 1, "output_text": "The main objects in the image are two giraffes standing in front of a building. The giraffe on the left is closer to the camera, while the one on the right is slightly behind it. The building is in the background, and the rocky ground is in the foreground.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24963.2, "ram_available_mb": 100808.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24963.5, "ram_available_mb": 100808.7, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 4859.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.5, "peak": 15.72, "min": 15.11}, "VDD_CPU_SOC_MSS": {"avg": 16.66, "peak": 16.94, "min": 16.15}, "VDD_GPU": {"avg": 21.23, "peak": 24.02, "min": 20.09}, "VIN": {"avg": 63.94, "peak": 72.85, "min": 60.91}}, "power_watts_avg": 21.23, "energy_joules_est": 28.69, "sample_count": 10, "duration_seconds": 1.352}, "timestamp": "2026-01-17T15:30:08.035123"}
{"image_index": 316, "image_name": "000000033707.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033707.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1482.357, "latencies_ms": [1482.357], "images_per_second": 0.675, "prompt_tokens": 21, "response_tokens_est": 64, "n_tiles": 1, "output_text": "The image depicts a serene scene at a zoo, featuring two giraffes standing close to each other in front of a beige building. The giraffes are surrounded by a rocky terrain, and the background includes lush green trees and a cloudy sky, creating a tranquil and naturalistic environment.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24963.5, "ram_available_mb": 100808.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24964.4, "ram_available_mb": 100807.7, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 4854.7, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.45, "peak": 15.62, "min": 15.11}, "VDD_CPU_SOC_MSS": {"avg": 16.65, "peak": 16.94, "min": 16.14}, "VDD_GPU": {"avg": 21.14, "peak": 24.04, "min": 20.1}, "VIN": {"avg": 62.83, "peak": 68.6, "min": 54.62}}, "power_watts_avg": 21.14, "energy_joules_est": 31.35, "sample_count": 11, "duration_seconds": 1.483}, "timestamp": "2026-01-17T15:30:09.523522"}
{"image_index": 316, "image_name": "000000033707.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033707.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1172.257, "latencies_ms": [1172.257], "images_per_second": 0.853, "prompt_tokens": 19, "response_tokens_est": 50, "n_tiles": 1, "output_text": "The image depicts a giraffe standing next to a building, with a backdrop of lush green trees and a cloudy sky. The giraffe's coat is a mix of brown and tan, and the building is made of light-colored stone.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24964.4, "ram_available_mb": 100807.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24964.7, "ram_available_mb": 100807.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 4853.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.54, "peak": 15.72, "min": 15.22}, "VDD_CPU_SOC_MSS": {"avg": 16.54, "peak": 16.93, "min": 16.14}, "VDD_GPU": {"avg": 21.37, "peak": 24.04, "min": 20.09}, "VIN": {"avg": 63.23, "peak": 67.5, "min": 61.5}}, "power_watts_avg": 21.37, "energy_joules_est": 25.06, "sample_count": 9, "duration_seconds": 1.173}, "timestamp": "2026-01-17T15:30:10.701525"}
{"image_index": 317, "image_name": "000000033759.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033759.jpg", "image_width": 640, "image_height": 457, "image_resolution": "640x457", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2795.163, "latencies_ms": [2795.163], "images_per_second": 0.358, "prompt_tokens": 9, "response_tokens_est": 35, "n_tiles": 12, "output_text": "A young baseball player is in the middle of a game, swinging a bat at a flying ball, while wearing a green jersey and white pants, and a black helmet.", "error": null, "sys_before": {"cpu_percent": 43.8, "ram_used_mb": 24941.3, "ram_available_mb": 100830.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 24940.7, "ram_available_mb": 100831.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.3, "peak": 16.66, "min": 14.2}, "VDD_CPU_SOC_MSS": {"avg": 16.1, "peak": 17.71, "min": 14.96}, "VDD_GPU": {"avg": 35.0, "peak": 43.73, "min": 26.79}, "VIN": {"avg": 84.57, "peak": 141.38, "min": 59.09}}, "power_watts_avg": 35.0, "energy_joules_est": 97.85, "sample_count": 21, "duration_seconds": 2.796}, "timestamp": "2026-01-17T15:30:13.566802"}
{"image_index": 317, "image_name": "000000033759.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033759.jpg", "image_width": 640, "image_height": 457, "image_resolution": "640x457", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 3099.022, "latencies_ms": [3099.022], "images_per_second": 0.323, "prompt_tokens": 23, "response_tokens_est": 42, "n_tiles": 12, "output_text": "baseball bat: 1\nbaseball: 1\nhelmet: 1\nuniform: 1\nsocks: 2\nglove: 1\nfootwear: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24940.7, "ram_available_mb": 100831.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 24941.2, "ram_available_mb": 100831.0, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.22, "peak": 16.56, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.75, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 35.32, "peak": 45.28, "min": 26.39}, "VIN": {"avg": 76.69, "peak": 113.05, "min": 65.52}}, "power_watts_avg": 35.32, "energy_joules_est": 109.47, "sample_count": 24, "duration_seconds": 3.1}, "timestamp": "2026-01-17T15:30:16.673377"}
{"image_index": 317, "image_name": "000000033759.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033759.jpg", "image_width": 640, "image_height": 457, "image_resolution": "640x457", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 4640.292, "latencies_ms": [4640.292], "images_per_second": 0.216, "prompt_tokens": 27, "response_tokens_est": 87, "n_tiles": 12, "output_text": "The main object in the foreground is a young baseball player wearing a green jersey and white pants, holding a blue bat. The player is positioned near the center of the image, slightly to the left. In the background, there is a chain-link fence, which separates the field from the surrounding area. The fence is tall and extends across the entire width of the image, providing a clear boundary between the field and the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24941.2, "ram_available_mb": 100831.0, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 24941.4, "ram_available_mb": 100830.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.16, "peak": 16.56, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.61, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 32.14, "peak": 44.91, "min": 25.6}, "VIN": {"avg": 74.37, "peak": 113.78, "min": 57.12}}, "power_watts_avg": 32.14, "energy_joules_est": 149.15, "sample_count": 36, "duration_seconds": 4.641}, "timestamp": "2026-01-17T15:30:21.320417"}
{"image_index": 317, "image_name": "000000033759.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033759.jpg", "image_width": 640, "image_height": 457, "image_resolution": "640x457", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3878.707, "latencies_ms": [3878.707], "images_per_second": 0.258, "prompt_tokens": 21, "response_tokens_est": 65, "n_tiles": 12, "output_text": "The image depicts a young baseball player in a green jersey and white pants, equipped with a black helmet and a blue bat, preparing to swing at a baseball. The scene takes place in a baseball field enclosed by a chain-link fence, with a lush green field and trees in the background, indicating a sunny day.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24941.4, "ram_available_mb": 100830.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 24941.9, "ram_available_mb": 100830.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.17, "peak": 16.56, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.69, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 33.5, "peak": 45.69, "min": 26.01}, "VIN": {"avg": 74.2, "peak": 111.99, "min": 56.99}}, "power_watts_avg": 33.5, "energy_joules_est": 129.95, "sample_count": 30, "duration_seconds": 3.879}, "timestamp": "2026-01-17T15:30:25.210015"}
{"image_index": 317, "image_name": "000000033759.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033759.jpg", "image_width": 640, "image_height": 457, "image_resolution": "640x457", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3099.283, "latencies_ms": [3099.283], "images_per_second": 0.323, "prompt_tokens": 19, "response_tokens_est": 42, "n_tiles": 12, "output_text": "The baseball player is wearing a green jersey with yellow accents, white pants, and black shoes. The scene is bright and sunny, with the player's shadow visible on the ground, indicating a clear day.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24941.9, "ram_available_mb": 100830.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 24941.7, "ram_available_mb": 100830.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.22, "peak": 16.56, "min": 14.12}, "VDD_CPU_SOC_MSS": {"avg": 15.73, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 35.34, "peak": 45.3, "min": 26.4}, "VIN": {"avg": 78.22, "peak": 123.28, "min": 55.22}}, "power_watts_avg": 35.34, "energy_joules_est": 109.54, "sample_count": 24, "duration_seconds": 3.1}, "timestamp": "2026-01-17T15:30:28.315875"}
{"image_index": 318, "image_name": "000000033854.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033854.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1282.955, "latencies_ms": [1282.955], "images_per_second": 0.779, "prompt_tokens": 9, "response_tokens_est": 25, "n_tiles": 6, "output_text": "A vintage car and motorcycle are parked on a cobblestone street, with a crowd of people gathered in the background.", "error": null, "sys_before": {"cpu_percent": 44.2, "ram_used_mb": 24961.3, "ram_available_mb": 100810.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 24962.8, "ram_available_mb": 100809.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.6, "peak": 15.04, "min": 13.72}, "VDD_CPU_SOC_MSS": {"avg": 15.24, "peak": 15.75, "min": 14.18}, "VDD_GPU": {"avg": 34.15, "peak": 40.57, "min": 27.18}, "VIN": {"avg": 75.3, "peak": 109.6, "min": 63.37}}, "power_watts_avg": 34.15, "energy_joules_est": 43.83, "sample_count": 10, "duration_seconds": 1.283}, "timestamp": "2026-01-17T15:30:29.662010"}
{"image_index": 318, "image_name": "000000033854.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033854.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1544.484, "latencies_ms": [1544.484], "images_per_second": 0.647, "prompt_tokens": 23, "response_tokens_est": 32, "n_tiles": 6, "output_text": "car: 2\nmotorcycle: 3\nbus: 1\nstreet light: 1\nbuilding: 1\nflag: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24962.8, "ram_available_mb": 100809.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 24963.0, "ram_available_mb": 100809.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.81, "peak": 15.24, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.48, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 33.55, "peak": 41.36, "min": 25.22}, "VIN": {"avg": 74.41, "peak": 118.04, "min": 61.79}}, "power_watts_avg": 33.55, "energy_joules_est": 51.83, "sample_count": 12, "duration_seconds": 1.545}, "timestamp": "2026-01-17T15:30:31.217756"}
{"image_index": 318, "image_name": "000000033854.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033854.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2972.327, "latencies_ms": [2972.327], "images_per_second": 0.336, "prompt_tokens": 27, "response_tokens_est": 82, "n_tiles": 6, "output_text": "The main objects in the image are parked cars and motorcycles. The closest car is parked on the left side, while the motorcycle is parked on the right side. The background features a crowd of people, a red bus, and a building under construction. The scene is set on a cobblestone street, and the overall setting appears to be a public area with a mix of vintage and modern elements.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24963.0, "ram_available_mb": 100809.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 24964.0, "ram_available_mb": 100808.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.85, "peak": 15.24, "min": 14.2}, "VDD_CPU_SOC_MSS": {"avg": 15.42, "peak": 15.75, "min": 14.57}, "VDD_GPU": {"avg": 28.29, "peak": 40.56, "min": 22.86}, "VIN": {"avg": 69.26, "peak": 111.69, "min": 53.92}}, "power_watts_avg": 28.29, "energy_joules_est": 84.1, "sample_count": 23, "duration_seconds": 2.973}, "timestamp": "2026-01-17T15:30:34.196450"}
{"image_index": 318, "image_name": "000000033854.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033854.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2084.31, "latencies_ms": [2084.31], "images_per_second": 0.48, "prompt_tokens": 21, "response_tokens_est": 51, "n_tiles": 6, "output_text": "The image depicts a bustling outdoor scene with vintage cars parked on a cobblestone street. The setting appears to be a public area, possibly a fair or a car show, with various people milling about and a red bus in the background.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 24964.2, "ram_available_mb": 100807.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 24965.5, "ram_available_mb": 100806.7, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.82, "peak": 15.14, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.5, "peak": 15.75, "min": 14.56}, "VDD_GPU": {"avg": 30.43, "peak": 40.95, "min": 23.24}, "VIN": {"avg": 70.17, "peak": 111.59, "min": 52.79}}, "power_watts_avg": 30.43, "energy_joules_est": 63.43, "sample_count": 16, "duration_seconds": 2.085}, "timestamp": "2026-01-17T15:30:36.286995"}
{"image_index": 318, "image_name": "000000033854.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033854.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2452.261, "latencies_ms": [2452.261], "images_per_second": 0.408, "prompt_tokens": 19, "response_tokens_est": 64, "n_tiles": 6, "output_text": "The image features a cobblestone street lined with vintage cars, including a black car and a blue car, parked along the side. The cobblestones are arranged in a pattern, and the street is surrounded by green trees and a clear blue sky. The lighting is bright and natural, indicating a sunny day.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24965.5, "ram_available_mb": 100806.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 24965.5, "ram_available_mb": 100806.7, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.85, "peak": 15.24, "min": 14.3}, "VDD_CPU_SOC_MSS": {"avg": 15.5, "peak": 15.75, "min": 14.56}, "VDD_GPU": {"avg": 29.38, "peak": 40.95, "min": 22.85}, "VIN": {"avg": 71.58, "peak": 106.67, "min": 60.08}}, "power_watts_avg": 29.38, "energy_joules_est": 72.06, "sample_count": 19, "duration_seconds": 2.453}, "timestamp": "2026-01-17T15:30:38.745157"}
{"image_index": 319, "image_name": "000000034071.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034071.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1492.588, "latencies_ms": [1492.588], "images_per_second": 0.67, "prompt_tokens": 9, "response_tokens_est": 33, "n_tiles": 6, "output_text": "The image depicts a close-up view of two parking meters, with the focus on the details of their circular, transparent tops and the visible text on them.", "error": null, "sys_before": {"cpu_percent": 41.4, "ram_used_mb": 24965.5, "ram_available_mb": 100806.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 24966.2, "ram_available_mb": 100806.0, "ram_percent": 19.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.71, "peak": 15.14, "min": 13.9}, "VDD_CPU_SOC_MSS": {"avg": 15.46, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 32.95, "peak": 40.16, "min": 26.01}, "VIN": {"avg": 71.66, "peak": 92.64, "min": 55.81}}, "power_watts_avg": 32.95, "energy_joules_est": 49.2, "sample_count": 11, "duration_seconds": 1.493}, "timestamp": "2026-01-17T15:30:40.282321"}
{"image_index": 319, "image_name": "000000034071.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034071.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1027.7, "latencies_ms": [1027.7], "images_per_second": 0.973, "prompt_tokens": 23, "response_tokens_est": 14, "n_tiles": 6, "output_text": "parking meter: 2\nparking meter: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24966.2, "ram_available_mb": 100806.0, "ram_percent": 19.9}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 24966.2, "ram_available_mb": 100806.0, "ram_percent": 19.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.91, "peak": 15.44, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.41, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 37.08, "peak": 40.56, "min": 32.3}, "VIN": {"avg": 82.99, "peak": 112.3, "min": 64.17}}, "power_watts_avg": 37.08, "energy_joules_est": 38.12, "sample_count": 7, "duration_seconds": 1.028}, "timestamp": "2026-01-17T15:30:41.315861"}
{"image_index": 319, "image_name": "000000034071.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034071.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2441.823, "latencies_ms": [2441.823], "images_per_second": 0.41, "prompt_tokens": 27, "response_tokens_est": 66, "n_tiles": 6, "output_text": "The main objects in the image are two parking meters positioned in the foreground. The foreground is the closest and most detailed part of the image, while the background is blurred and out of focus. The parking meters are positioned near each other, with the closest one slightly to the left and the one further back to the right.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24966.5, "ram_available_mb": 100805.7, "ram_percent": 19.9}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 24967.7, "ram_available_mb": 100804.5, "ram_percent": 19.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.04, "peak": 15.54, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.64, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 30.66, "peak": 41.74, "min": 23.64}, "VIN": {"avg": 72.09, "peak": 112.69, "min": 54.91}}, "power_watts_avg": 30.66, "energy_joules_est": 74.88, "sample_count": 18, "duration_seconds": 2.442}, "timestamp": "2026-01-17T15:30:43.763511"}
{"image_index": 319, "image_name": "000000034071.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034071.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3201.638, "latencies_ms": [3201.638], "images_per_second": 0.312, "prompt_tokens": 21, "response_tokens_est": 94, "n_tiles": 6, "output_text": "The image depicts a dimly lit scene with a focus on two parking meters, which are situated in an urban environment. The setting appears to be during twilight or early evening, as the sky is dim and the sun is low on the horizon, casting a warm glow over the scene. The lighting creates a bokeh effect, with the focus on the parking meters and the blurred background, emphasizing the stillness and quietness of the moment.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24967.7, "ram_available_mb": 100804.5, "ram_percent": 19.9}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 24968.4, "ram_available_mb": 100803.8, "ram_percent": 19.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.06, "peak": 15.44, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.7, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 28.07, "peak": 40.56, "min": 23.25}, "VIN": {"avg": 67.2, "peak": 81.32, "min": 56.13}}, "power_watts_avg": 28.07, "energy_joules_est": 89.88, "sample_count": 25, "duration_seconds": 3.202}, "timestamp": "2026-01-17T15:30:46.971299"}
{"image_index": 319, "image_name": "000000034071.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034071.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2488.073, "latencies_ms": [2488.073], "images_per_second": 0.402, "prompt_tokens": 19, "response_tokens_est": 68, "n_tiles": 6, "output_text": "The image features two parking meters with a warm, golden hue, likely due to the lighting conditions. The lighting creates a soft, blurred effect, with bokeh lights visible in the background, suggesting a cityscape at sunset. The colors are dominated by warm tones, with the metallic surfaces of the meters reflecting the ambient light.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24968.4, "ram_available_mb": 100803.8, "ram_percent": 19.9}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 24968.7, "ram_available_mb": 100803.5, "ram_percent": 19.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.07, "peak": 15.44, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.67, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 29.52, "peak": 40.16, "min": 23.64}, "VIN": {"avg": 73.09, "peak": 115.71, "min": 63.16}}, "power_watts_avg": 29.52, "energy_joules_est": 73.46, "sample_count": 19, "duration_seconds": 2.489}, "timestamp": "2026-01-17T15:30:49.465630"}
{"image_index": 320, "image_name": "000000034139.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034139.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2596.515, "latencies_ms": [2596.515], "images_per_second": 0.385, "prompt_tokens": 9, "response_tokens_est": 29, "n_tiles": 12, "output_text": "The image shows a brown suitcase with various stickers and a plaque on it, placed on a platform, with three people standing behind it.", "error": null, "sys_before": {"cpu_percent": 46.3, "ram_used_mb": 24924.9, "ram_available_mb": 100847.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.6, "ram_used_mb": 24926.6, "ram_available_mb": 100845.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.2, "peak": 16.76, "min": 13.59}, "VDD_CPU_SOC_MSS": {"avg": 15.96, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 35.88, "peak": 43.71, "min": 27.18}, "VIN": {"avg": 75.34, "peak": 103.85, "min": 60.48}}, "power_watts_avg": 35.88, "energy_joules_est": 93.17, "sample_count": 20, "duration_seconds": 2.597}, "timestamp": "2026-01-17T15:30:52.144736"}
{"image_index": 320, "image_name": "000000034139.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034139.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2791.396, "latencies_ms": [2791.396], "images_per_second": 0.358, "prompt_tokens": 23, "response_tokens_est": 33, "n_tiles": 12, "output_text": "1. Suitcase\n2. Man\n3. Woman\n4. Jacket\n5. Bag\n6. Sign\n7. Sign\n8. Sign", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24926.6, "ram_available_mb": 100845.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 24927.6, "ram_available_mb": 100844.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.24, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.76, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 36.83, "peak": 45.3, "min": 27.18}, "VIN": {"avg": 78.14, "peak": 118.28, "min": 60.51}}, "power_watts_avg": 36.83, "energy_joules_est": 102.82, "sample_count": 21, "duration_seconds": 2.792}, "timestamp": "2026-01-17T15:30:54.943334"}
{"image_index": 320, "image_name": "000000034139.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034139.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 4260.201, "latencies_ms": [4260.201], "images_per_second": 0.235, "prompt_tokens": 27, "response_tokens_est": 76, "n_tiles": 12, "output_text": "The main object in the foreground is a brown suitcase with various stickers and a sign on it. The suitcase is placed on a platform. In the background, there is a man and a woman standing next to each other. The man is wearing a black jacket and the woman is wearing a blue jacket. The sign on the suitcase is near the man and woman.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24927.6, "ram_available_mb": 100844.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 24928.8, "ram_available_mb": 100843.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.18, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.65, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 32.8, "peak": 44.89, "min": 26.0}, "VIN": {"avg": 76.63, "peak": 126.91, "min": 57.04}}, "power_watts_avg": 32.8, "energy_joules_est": 139.75, "sample_count": 33, "duration_seconds": 4.261}, "timestamp": "2026-01-17T15:30:59.209968"}
{"image_index": 320, "image_name": "000000034139.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034139.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 4256.068, "latencies_ms": [4256.068], "images_per_second": 0.235, "prompt_tokens": 21, "response_tokens_est": 75, "n_tiles": 12, "output_text": "The image depicts a group of people standing in front of a building with a sign that reads \"Fidelity Investments.\" The setting appears to be an outdoor area, possibly a plaza or a public square, with a large, brown suitcase prominently displayed on a platform. The individuals are dressed in casual attire, and the atmosphere seems relaxed and informal.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24928.8, "ram_available_mb": 100843.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 24929.3, "ram_available_mb": 100842.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.93, "peak": 16.36, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.38, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 32.72, "peak": 45.3, "min": 25.61}, "VIN": {"avg": 74.88, "peak": 127.2, "min": 60.28}}, "power_watts_avg": 32.72, "energy_joules_est": 139.27, "sample_count": 33, "duration_seconds": 4.256}, "timestamp": "2026-01-17T15:31:03.472408"}
{"image_index": 320, "image_name": "000000034139.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034139.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 4678.966, "latencies_ms": [4678.966], "images_per_second": 0.214, "prompt_tokens": 19, "response_tokens_est": 88, "n_tiles": 12, "output_text": "The image features a brown leather suitcase with various stickers and a sign on it. The suitcase is placed on a platform with a sign that reads \"GOD BLESS AMERICA\" by J. Seward Johnson. The background includes a building with a sign for \"Fidelity Investments\" and a couple of people standing nearby. The lighting is natural, suggesting it is daytime, and the weather appears to be clear.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24929.3, "ram_available_mb": 100842.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 24930.5, "ram_available_mb": 100841.7, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.88, "peak": 16.36, "min": 13.92}, "VDD_CPU_SOC_MSS": {"avg": 15.45, "peak": 17.32, "min": 14.17}, "VDD_GPU": {"avg": 32.11, "peak": 45.69, "min": 25.6}, "VIN": {"avg": 73.56, "peak": 121.74, "min": 58.03}}, "power_watts_avg": 32.11, "energy_joules_est": 150.26, "sample_count": 36, "duration_seconds": 4.68}, "timestamp": "2026-01-17T15:31:08.158610"}
{"image_index": 321, "image_name": "000000034205.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034205.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1710.308, "latencies_ms": [1710.308], "images_per_second": 0.585, "prompt_tokens": 9, "response_tokens_est": 41, "n_tiles": 6, "output_text": "The image showcases a plate of seared scallops, garnished with fresh green parsley, and accompanied by a side of broccoli, all presented in a visually appealing and appetizing manner.", "error": null, "sys_before": {"cpu_percent": 43.2, "ram_used_mb": 24945.1, "ram_available_mb": 100827.0, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 24946.4, "ram_available_mb": 100825.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.71, "peak": 15.11, "min": 13.51}, "VDD_CPU_SOC_MSS": {"avg": 15.38, "peak": 16.14, "min": 14.17}, "VDD_GPU": {"avg": 32.0, "peak": 40.56, "min": 24.82}, "VIN": {"avg": 74.51, "peak": 105.51, "min": 59.09}}, "power_watts_avg": 32.0, "energy_joules_est": 54.74, "sample_count": 13, "duration_seconds": 1.711}, "timestamp": "2026-01-17T15:31:09.929963"}
{"image_index": 321, "image_name": "000000034205.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034205.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 4121.13, "latencies_ms": [4121.13], "images_per_second": 0.243, "prompt_tokens": 23, "response_tokens_est": 128, "n_tiles": 6, "output_text": "object: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject:", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24946.4, "ram_available_mb": 100825.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 24947.3, "ram_available_mb": 100824.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.13, "peak": 15.34, "min": 14.43}, "VDD_CPU_SOC_MSS": {"avg": 15.76, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 27.14, "peak": 41.34, "min": 23.24}, "VIN": {"avg": 70.77, "peak": 116.41, "min": 63.46}}, "power_watts_avg": 27.14, "energy_joules_est": 111.86, "sample_count": 33, "duration_seconds": 4.122}, "timestamp": "2026-01-17T15:31:14.056955"}
{"image_index": 321, "image_name": "000000034205.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034205.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2163.484, "latencies_ms": [2163.484], "images_per_second": 0.462, "prompt_tokens": 27, "response_tokens_est": 56, "n_tiles": 6, "output_text": "The main objects in the image are a plate of food, which includes a piece of fish, mushrooms, and broccoli. The fish is positioned in the foreground, with the mushrooms and broccoli surrounding it. The broccoli is in the background, slightly out of focus.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24947.3, "ram_available_mb": 100824.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 24947.3, "ram_available_mb": 100824.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.05, "peak": 15.44, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.77, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 30.38, "peak": 40.57, "min": 23.64}, "VIN": {"avg": 73.02, "peak": 108.78, "min": 56.33}}, "power_watts_avg": 30.38, "energy_joules_est": 65.74, "sample_count": 17, "duration_seconds": 2.164}, "timestamp": "2026-01-17T15:31:16.226650"}
{"image_index": 321, "image_name": "000000034205.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034205.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2944.415, "latencies_ms": [2944.415], "images_per_second": 0.34, "prompt_tokens": 21, "response_tokens_est": 85, "n_tiles": 6, "output_text": "The image depicts a close-up view of a dish featuring a piece of grilled fish, likely a salmon or similar type, garnished with fresh green herbs. The fish is placed on a bed of saut\u00e9ed mushrooms, which appear to be cooked to a tender consistency. The setting suggests a culinary presentation, possibly in a restaurant or a home kitchen, where the fish and mushrooms are being prepared and served together.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24947.3, "ram_available_mb": 100824.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 24948.1, "ram_available_mb": 100824.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.1, "peak": 15.44, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.68, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 28.55, "peak": 40.57, "min": 23.25}, "VIN": {"avg": 71.34, "peak": 108.16, "min": 59.94}}, "power_watts_avg": 28.55, "energy_joules_est": 84.08, "sample_count": 23, "duration_seconds": 2.945}, "timestamp": "2026-01-17T15:31:19.177003"}
{"image_index": 321, "image_name": "000000034205.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034205.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1939.762, "latencies_ms": [1939.762], "images_per_second": 0.516, "prompt_tokens": 19, "response_tokens_est": 48, "n_tiles": 6, "output_text": "The image showcases a dish featuring a grilled or seared piece of fish, garnished with fresh green herbs. The lighting highlights the golden-brown crust of the fish, while the vibrant green of the herbs adds a fresh contrast.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24948.1, "ram_available_mb": 100824.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 24949.8, "ram_available_mb": 100822.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.06, "peak": 15.44, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.69, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 31.15, "peak": 40.16, "min": 24.04}, "VIN": {"avg": 69.13, "peak": 90.61, "min": 60.15}}, "power_watts_avg": 31.15, "energy_joules_est": 60.44, "sample_count": 15, "duration_seconds": 1.94}, "timestamp": "2026-01-17T15:31:21.122332"}
{"image_index": 322, "image_name": "000000034257.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034257.jpg", "image_width": 500, "image_height": 309, "image_resolution": "500x309", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1631.438, "latencies_ms": [1631.438], "images_per_second": 0.613, "prompt_tokens": 9, "response_tokens_est": 38, "n_tiles": 6, "output_text": "The image shows a variety of fresh vegetables, including carrots, cabbage, and leafy greens, arranged in baskets on a wooden surface, likely at a market or grocery store.", "error": null, "sys_before": {"cpu_percent": 44.0, "ram_used_mb": 24949.8, "ram_available_mb": 100822.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 24949.5, "ram_available_mb": 100822.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.76, "peak": 15.14, "min": 13.9}, "VDD_CPU_SOC_MSS": {"avg": 15.48, "peak": 16.14, "min": 14.57}, "VDD_GPU": {"avg": 32.23, "peak": 40.16, "min": 25.21}, "VIN": {"avg": 73.81, "peak": 102.0, "min": 61.25}}, "power_watts_avg": 32.23, "energy_joules_est": 52.59, "sample_count": 12, "duration_seconds": 1.632}, "timestamp": "2026-01-17T15:31:22.810513"}
{"image_index": 322, "image_name": "000000034257.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034257.jpg", "image_width": 500, "image_height": 309, "image_resolution": "500x309", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1340.873, "latencies_ms": [1340.873], "images_per_second": 0.746, "prompt_tokens": 23, "response_tokens_est": 25, "n_tiles": 6, "output_text": "carrots: 20\nonions: 10\nlettuce: 10\ncabbage: 10", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24949.5, "ram_available_mb": 100822.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 24949.5, "ram_available_mb": 100822.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.82, "peak": 15.24, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.59, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 34.66, "peak": 41.34, "min": 26.79}, "VIN": {"avg": 70.79, "peak": 91.4, "min": 51.11}}, "power_watts_avg": 34.66, "energy_joules_est": 46.49, "sample_count": 10, "duration_seconds": 1.341}, "timestamp": "2026-01-17T15:31:24.157413"}
{"image_index": 322, "image_name": "000000034257.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034257.jpg", "image_width": 500, "image_height": 309, "image_resolution": "500x309", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2025.273, "latencies_ms": [2025.273], "images_per_second": 0.494, "prompt_tokens": 27, "response_tokens_est": 49, "n_tiles": 6, "output_text": "The main objects in the image are a basket filled with carrots and a basket filled with green vegetables. The carrots are in the foreground, while the green vegetables are in the background. The basket containing the carrots is placed near the green vegetables.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24949.5, "ram_available_mb": 100822.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 24949.5, "ram_available_mb": 100822.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.85, "peak": 15.24, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.35, "peak": 15.75, "min": 14.56}, "VDD_GPU": {"avg": 31.48, "peak": 41.34, "min": 23.64}, "VIN": {"avg": 70.52, "peak": 115.18, "min": 57.4}}, "power_watts_avg": 31.48, "energy_joules_est": 63.76, "sample_count": 15, "duration_seconds": 2.026}, "timestamp": "2026-01-17T15:31:26.192523"}
{"image_index": 322, "image_name": "000000034257.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034257.jpg", "image_width": 500, "image_height": 309, "image_resolution": "500x309", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2733.257, "latencies_ms": [2733.257], "images_per_second": 0.366, "prompt_tokens": 21, "response_tokens_est": 74, "n_tiles": 6, "output_text": "The image depicts a vibrant and colorful display of fresh vegetables at a market stall. The stall is filled with a variety of leafy greens, including lettuce, cabbage, and kale, alongside a basket of carrots and a bunch of onions. The scene is set in a bustling market environment, where customers can easily spot and select the fresh produce.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24949.5, "ram_available_mb": 100822.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 24950.3, "ram_available_mb": 100821.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.87, "peak": 15.14, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.47, "peak": 15.75, "min": 14.56}, "VDD_GPU": {"avg": 28.68, "peak": 40.56, "min": 22.85}, "VIN": {"avg": 69.56, "peak": 114.35, "min": 58.3}}, "power_watts_avg": 28.68, "energy_joules_est": 78.4, "sample_count": 21, "duration_seconds": 2.734}, "timestamp": "2026-01-17T15:31:28.931462"}
{"image_index": 322, "image_name": "000000034257.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034257.jpg", "image_width": 500, "image_height": 309, "image_resolution": "500x309", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2813.102, "latencies_ms": [2813.102], "images_per_second": 0.355, "prompt_tokens": 19, "response_tokens_est": 77, "n_tiles": 6, "output_text": "The image showcases a vibrant and colorful display of fresh vegetables, including leafy greens, carrots, and onions, all arranged in a basket. The lighting is bright and natural, highlighting the freshness and vibrancy of the produce. The colors range from the deep green of the leafy greens to the bright orange of the carrots, creating a visually appealing and appetizing scene.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24950.3, "ram_available_mb": 100821.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 24952.5, "ram_available_mb": 100819.7, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.86, "peak": 15.14, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.61, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 28.47, "peak": 40.95, "min": 22.85}, "VIN": {"avg": 67.41, "peak": 94.31, "min": 58.33}}, "power_watts_avg": 28.47, "energy_joules_est": 80.1, "sample_count": 22, "duration_seconds": 2.813}, "timestamp": "2026-01-17T15:31:31.751466"}
{"image_index": 323, "image_name": "000000034417.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034417.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2592.542, "latencies_ms": [2592.542], "images_per_second": 0.386, "prompt_tokens": 9, "response_tokens_est": 29, "n_tiles": 12, "output_text": "The image depicts a bustling bakery with multiple doughnut machines in operation, producing a large number of freshly baked doughnuts.", "error": null, "sys_before": {"cpu_percent": 44.2, "ram_used_mb": 24919.7, "ram_available_mb": 100852.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.6, "ram_used_mb": 24921.9, "ram_available_mb": 100850.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.18, "peak": 16.66, "min": 13.49}, "VDD_CPU_SOC_MSS": {"avg": 15.98, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 35.57, "peak": 43.73, "min": 27.18}, "VIN": {"avg": 79.97, "peak": 113.73, "min": 66.13}}, "power_watts_avg": 35.57, "energy_joules_est": 92.23, "sample_count": 20, "duration_seconds": 2.593}, "timestamp": "2026-01-17T15:31:34.436453"}
{"image_index": 323, "image_name": "000000034417.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034417.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 6033.204, "latencies_ms": [6033.204], "images_per_second": 0.166, "prompt_tokens": 23, "response_tokens_est": 128, "n_tiles": 12, "output_text": "object: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject:", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24921.9, "ram_available_mb": 100850.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 24923.4, "ram_available_mb": 100848.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.14, "peak": 16.56, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.55, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 30.73, "peak": 45.28, "min": 26.0}, "VIN": {"avg": 72.52, "peak": 111.65, "min": 57.62}}, "power_watts_avg": 30.73, "energy_joules_est": 185.41, "sample_count": 48, "duration_seconds": 6.034}, "timestamp": "2026-01-17T15:31:40.476105"}
{"image_index": 323, "image_name": "000000034417.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034417.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3671.886, "latencies_ms": [3671.886], "images_per_second": 0.272, "prompt_tokens": 27, "response_tokens_est": 59, "n_tiles": 12, "output_text": "The main objects in the image are a doughnut machine and a doughnut stand. The doughnut machine is located in the foreground, with doughnuts being processed on the conveyor belt. The doughnut stand is situated in the background, with doughnuts being displayed on the shelves.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24923.4, "ram_available_mb": 100848.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 24923.4, "ram_available_mb": 100848.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.21, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.68, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 33.71, "peak": 45.28, "min": 26.0}, "VIN": {"avg": 75.75, "peak": 116.33, "min": 60.37}}, "power_watts_avg": 33.71, "energy_joules_est": 123.8, "sample_count": 29, "duration_seconds": 3.672}, "timestamp": "2026-01-17T15:31:44.154393"}
{"image_index": 323, "image_name": "000000034417.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034417.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 4936.523, "latencies_ms": [4936.523], "images_per_second": 0.203, "prompt_tokens": 21, "response_tokens_est": 96, "n_tiles": 12, "output_text": "The image depicts a bustling bakery or doughnut shop with a focus on the production line for donuts. The setting is indoors, likely in a commercial kitchen or bakery, with various donuts being prepared and packaged. The workers are actively engaged in their tasks, with one person in a white hat handling dough and another in a red shirt preparing the donuts. The overall atmosphere is busy and industrious, with the donuts being the central activity.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24923.4, "ram_available_mb": 100848.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 24923.4, "ram_available_mb": 100848.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.15, "peak": 16.66, "min": 14.12}, "VDD_CPU_SOC_MSS": {"avg": 15.68, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 31.73, "peak": 45.69, "min": 25.6}, "VIN": {"avg": 73.22, "peak": 111.46, "min": 57.28}}, "power_watts_avg": 31.73, "energy_joules_est": 156.66, "sample_count": 39, "duration_seconds": 4.937}, "timestamp": "2026-01-17T15:31:49.097663"}
{"image_index": 323, "image_name": "000000034417.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034417.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2688.082, "latencies_ms": [2688.082], "images_per_second": 0.372, "prompt_tokens": 19, "response_tokens_est": 30, "n_tiles": 12, "output_text": "The image depicts a bakery with a bright and clean environment. The lighting is artificial, and the materials used are primarily metal and glass.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24923.4, "ram_available_mb": 100848.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.5, "ram_used_mb": 24924.4, "ram_available_mb": 100847.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.26, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.86, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 36.67, "peak": 45.68, "min": 27.57}, "VIN": {"avg": 78.73, "peak": 117.15, "min": 64.46}}, "power_watts_avg": 36.67, "energy_joules_est": 98.59, "sample_count": 21, "duration_seconds": 2.689}, "timestamp": "2026-01-17T15:31:51.792201"}
{"image_index": 324, "image_name": "000000034452.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034452.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2325.719, "latencies_ms": [2325.719], "images_per_second": 0.43, "prompt_tokens": 9, "response_tokens_est": 21, "n_tiles": 12, "output_text": "A man is playing frisbee in a forest, wearing a green jacket and khaki shorts.", "error": null, "sys_before": {"cpu_percent": 43.2, "ram_used_mb": 24925.7, "ram_available_mb": 100846.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.3, "ram_used_mb": 24924.9, "ram_available_mb": 100847.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.15, "peak": 16.66, "min": 13.29}, "VDD_CPU_SOC_MSS": {"avg": 15.9, "peak": 17.71, "min": 14.17}, "VDD_GPU": {"avg": 37.09, "peak": 44.51, "min": 28.76}, "VIN": {"avg": 79.72, "peak": 102.02, "min": 67.61}}, "power_watts_avg": 37.09, "energy_joules_est": 86.27, "sample_count": 18, "duration_seconds": 2.326}, "timestamp": "2026-01-17T15:31:54.226360"}
{"image_index": 324, "image_name": "000000034452.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034452.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 3020.03, "latencies_ms": [3020.03], "images_per_second": 0.331, "prompt_tokens": 23, "response_tokens_est": 40, "n_tiles": 12, "output_text": "frisbee: 1\nman: 1\nhat: 1\nshorts: 1\nsocks: 1\nfootwear: 1\ntree: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24924.9, "ram_available_mb": 100847.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 24925.8, "ram_available_mb": 100846.3, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.27, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.9, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 36.06, "peak": 45.68, "min": 26.8}, "VIN": {"avg": 80.34, "peak": 113.71, "min": 59.07}}, "power_watts_avg": 36.06, "energy_joules_est": 108.92, "sample_count": 23, "duration_seconds": 3.02}, "timestamp": "2026-01-17T15:31:57.254460"}
{"image_index": 324, "image_name": "000000034452.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034452.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 4175.181, "latencies_ms": [4175.181], "images_per_second": 0.24, "prompt_tokens": 27, "response_tokens_est": 73, "n_tiles": 12, "output_text": "The main object in the foreground is a man wearing a green jacket and khaki shorts, who is bending over to pick up an orange frisbee. The background consists of a dense forest with tall trees and a mix of green and brown leaves on the ground. The man is positioned near the center of the image, with the trees forming a natural backdrop.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24925.8, "ram_available_mb": 100846.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 24927.2, "ram_available_mb": 100845.0, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.11, "peak": 16.56, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.63, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 32.98, "peak": 45.69, "min": 25.6}, "VIN": {"avg": 73.56, "peak": 123.64, "min": 54.56}}, "power_watts_avg": 32.98, "energy_joules_est": 137.71, "sample_count": 33, "duration_seconds": 4.175}, "timestamp": "2026-01-17T15:32:01.437908"}
{"image_index": 324, "image_name": "000000034452.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034452.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3445.601, "latencies_ms": [3445.601], "images_per_second": 0.29, "prompt_tokens": 21, "response_tokens_est": 52, "n_tiles": 12, "output_text": "The image depicts a serene forest scene with a man in a green jacket and cap playing with a frisbee. The ground is covered with fallen leaves and twigs, and the trees are bare, indicating it might be late fall or winter.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24927.2, "ram_available_mb": 100845.0, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 24928.1, "ram_available_mb": 100844.0, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.93, "peak": 16.36, "min": 13.92}, "VDD_CPU_SOC_MSS": {"avg": 15.45, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 34.31, "peak": 45.3, "min": 25.6}, "VIN": {"avg": 74.95, "peak": 115.54, "min": 61.02}}, "power_watts_avg": 34.31, "energy_joules_est": 118.23, "sample_count": 27, "duration_seconds": 3.446}, "timestamp": "2026-01-17T15:32:04.890091"}
{"image_index": 324, "image_name": "000000034452.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034452.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3727.905, "latencies_ms": [3727.905], "images_per_second": 0.268, "prompt_tokens": 19, "response_tokens_est": 60, "n_tiles": 12, "output_text": "The image depicts a forest scene with a man wearing a green jacket and khaki shorts. The ground is covered with fallen leaves and pine needles, and the trees have thin, bare branches. The lighting is natural, suggesting it is daytime, and the overall atmosphere is serene and peaceful.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24928.1, "ram_available_mb": 100844.0, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 24929.4, "ram_available_mb": 100842.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.91, "peak": 16.26, "min": 13.92}, "VDD_CPU_SOC_MSS": {"avg": 15.37, "peak": 17.32, "min": 14.17}, "VDD_GPU": {"avg": 33.63, "peak": 44.91, "min": 25.6}, "VIN": {"avg": 75.37, "peak": 117.56, "min": 60.31}}, "power_watts_avg": 33.63, "energy_joules_est": 125.38, "sample_count": 29, "duration_seconds": 3.728}, "timestamp": "2026-01-17T15:32:08.626534"}
{"image_index": 325, "image_name": "000000034760.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034760.jpg", "image_width": 426, "image_height": 640, "image_resolution": "426x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1436.039, "latencies_ms": [1436.039], "images_per_second": 0.696, "prompt_tokens": 9, "response_tokens_est": 31, "n_tiles": 6, "output_text": "The image shows a bathroom with a white sink, a toilet, and a shower curtain, all set against a beige tiled floor and walls.", "error": null, "sys_before": {"cpu_percent": 38.4, "ram_used_mb": 24950.3, "ram_available_mb": 100821.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 24951.3, "ram_available_mb": 100820.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.62, "peak": 15.11, "min": 13.49}, "VDD_CPU_SOC_MSS": {"avg": 15.28, "peak": 15.75, "min": 14.17}, "VDD_GPU": {"avg": 33.23, "peak": 40.97, "min": 26.0}, "VIN": {"avg": 74.97, "peak": 102.21, "min": 58.34}}, "power_watts_avg": 33.23, "energy_joules_est": 47.73, "sample_count": 11, "duration_seconds": 1.436}, "timestamp": "2026-01-17T15:32:10.128365"}
{"image_index": 325, "image_name": "000000034760.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034760.jpg", "image_width": 426, "image_height": 640, "image_resolution": "426x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1721.014, "latencies_ms": [1721.014], "images_per_second": 0.581, "prompt_tokens": 23, "response_tokens_est": 40, "n_tiles": 6, "output_text": "object: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24951.3, "ram_available_mb": 100820.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 24952.2, "ram_available_mb": 100819.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.05, "peak": 15.54, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.69, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 32.54, "peak": 40.97, "min": 24.82}, "VIN": {"avg": 71.78, "peak": 84.87, "min": 63.92}}, "power_watts_avg": 32.54, "energy_joules_est": 56.02, "sample_count": 13, "duration_seconds": 1.721}, "timestamp": "2026-01-17T15:32:11.856256"}
{"image_index": 325, "image_name": "000000034760.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034760.jpg", "image_width": 426, "image_height": 640, "image_resolution": "426x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3752.64, "latencies_ms": [3752.64], "images_per_second": 0.266, "prompt_tokens": 27, "response_tokens_est": 110, "n_tiles": 6, "output_text": "The bathroom features a white sink with a faucet on the left side of the image. The sink is placed on a beige countertop. To the right of the sink, there is a toilet with a white lid and a silver faucet. The toilet is positioned next to a shower curtain with blue and white stripes. The shower curtain is hanging on a shower rod. The floor is covered with beige tiles, and there is a towel rack above the sink. The bathroom appears to be well-lit, with a neutral color scheme.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24952.2, "ram_available_mb": 100819.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 24952.5, "ram_available_mb": 100819.7, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.95, "peak": 15.34, "min": 14.43}, "VDD_CPU_SOC_MSS": {"avg": 15.63, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 27.28, "peak": 40.57, "min": 22.86}, "VIN": {"avg": 70.69, "peak": 122.46, "min": 54.86}}, "power_watts_avg": 27.28, "energy_joules_est": 102.38, "sample_count": 29, "duration_seconds": 3.753}, "timestamp": "2026-01-17T15:32:15.614745"}
{"image_index": 325, "image_name": "000000034760.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034760.jpg", "image_width": 426, "image_height": 640, "image_resolution": "426x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2611.515, "latencies_ms": [2611.515], "images_per_second": 0.383, "prompt_tokens": 21, "response_tokens_est": 70, "n_tiles": 6, "output_text": "The image depicts a bathroom with a beige tiled floor and walls. The room features a white sink, a toilet, and a shower with a curtain. The setting appears to be a well-maintained and clean bathroom, with various bathroom items such as a can of shampoo and a bottle of lotion on the countertop.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24952.5, "ram_available_mb": 100819.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 24953.5, "ram_available_mb": 100818.7, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.89, "peak": 15.24, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.51, "peak": 15.75, "min": 14.56}, "VDD_GPU": {"avg": 28.82, "peak": 40.18, "min": 22.86}, "VIN": {"avg": 71.82, "peak": 109.34, "min": 62.38}}, "power_watts_avg": 28.82, "energy_joules_est": 75.28, "sample_count": 20, "duration_seconds": 2.612}, "timestamp": "2026-01-17T15:32:18.232650"}
{"image_index": 325, "image_name": "000000034760.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034760.jpg", "image_width": 426, "image_height": 640, "image_resolution": "426x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1883.216, "latencies_ms": [1883.216], "images_per_second": 0.531, "prompt_tokens": 19, "response_tokens_est": 44, "n_tiles": 6, "output_text": "The bathroom features a beige tiled floor and walls, with a white sink and faucet. The shower curtain is striped in blue and white. The lighting is soft and warm, creating a cozy atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24953.5, "ram_available_mb": 100818.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 24954.7, "ram_available_mb": 100817.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.84, "peak": 15.34, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.35, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 31.12, "peak": 40.57, "min": 23.64}, "VIN": {"avg": 71.98, "peak": 110.58, "min": 59.49}}, "power_watts_avg": 31.12, "energy_joules_est": 58.62, "sample_count": 14, "duration_seconds": 1.884}, "timestamp": "2026-01-17T15:32:20.122096"}
{"image_index": 326, "image_name": "000000034873.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034873.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2670.456, "latencies_ms": [2670.456], "images_per_second": 0.374, "prompt_tokens": 9, "response_tokens_est": 31, "n_tiles": 12, "output_text": "The image shows a kitchen with a wooden dining table and chairs, a black countertop, a white sink, and a window with greenery outside.", "error": null, "sys_before": {"cpu_percent": 43.1, "ram_used_mb": 24925.9, "ram_available_mb": 100846.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 24928.4, "ram_available_mb": 100843.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.12, "peak": 16.66, "min": 13.29}, "VDD_CPU_SOC_MSS": {"avg": 15.88, "peak": 17.71, "min": 14.17}, "VDD_GPU": {"avg": 35.55, "peak": 43.71, "min": 27.18}, "VIN": {"avg": 75.95, "peak": 111.52, "min": 58.2}}, "power_watts_avg": 35.55, "energy_joules_est": 94.95, "sample_count": 20, "duration_seconds": 2.671}, "timestamp": "2026-01-17T15:32:22.892181"}
{"image_index": 326, "image_name": "000000034873.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034873.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 4208.137, "latencies_ms": [4208.137], "images_per_second": 0.238, "prompt_tokens": 23, "response_tokens_est": 75, "n_tiles": 12, "output_text": "kitchen sink: 1\nkitchen counter: 1\nkitchen cabinets: 1\nkitchen faucet: 1\nkitchen towel: 1\nbowl: 1\ndishware: 1\ndish: 1\ndish rack: 1\ndinner table: 1\nchairs: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24928.4, "ram_available_mb": 100843.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 24929.8, "ram_available_mb": 100842.3, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.23, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.82, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 32.9, "peak": 44.91, "min": 26.0}, "VIN": {"avg": 76.06, "peak": 114.43, "min": 56.55}}, "power_watts_avg": 32.9, "energy_joules_est": 138.46, "sample_count": 32, "duration_seconds": 4.209}, "timestamp": "2026-01-17T15:32:27.111114"}
{"image_index": 326, "image_name": "000000034873.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034873.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3986.616, "latencies_ms": [3986.616], "images_per_second": 0.251, "prompt_tokens": 27, "response_tokens_est": 68, "n_tiles": 12, "output_text": "The main objects in the image are a kitchen sink, a wooden dining table, and a wooden cabinet. The sink is located in the foreground, near the cabinet, while the dining table is in the background, slightly to the right. The cabinet is situated to the left of the sink, and the table is positioned behind the cabinet.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24929.8, "ram_available_mb": 100842.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 24931.1, "ram_available_mb": 100841.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.18, "peak": 16.56, "min": 14.12}, "VDD_CPU_SOC_MSS": {"avg": 15.68, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 33.16, "peak": 45.28, "min": 26.0}, "VIN": {"avg": 71.14, "peak": 90.32, "min": 57.24}}, "power_watts_avg": 33.16, "energy_joules_est": 132.22, "sample_count": 31, "duration_seconds": 3.987}, "timestamp": "2026-01-17T15:32:31.104694"}
{"image_index": 326, "image_name": "000000034873.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034873.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3676.989, "latencies_ms": [3676.989], "images_per_second": 0.272, "prompt_tokens": 21, "response_tokens_est": 59, "n_tiles": 12, "output_text": "The image depicts a well-organized kitchen with a modern design. The kitchen features a sleek black countertop, a white sink, and a wooden dining table with six chairs. The room is brightly lit with natural light coming through a window, creating a clean and inviting atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24931.1, "ram_available_mb": 100841.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 24932.1, "ram_available_mb": 100840.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.2, "peak": 16.66, "min": 14.12}, "VDD_CPU_SOC_MSS": {"avg": 15.75, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 33.72, "peak": 44.51, "min": 26.0}, "VIN": {"avg": 72.89, "peak": 111.07, "min": 59.79}}, "power_watts_avg": 33.72, "energy_joules_est": 124.0, "sample_count": 28, "duration_seconds": 3.677}, "timestamp": "2026-01-17T15:32:34.787993"}
{"image_index": 326, "image_name": "000000034873.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034873.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3061.168, "latencies_ms": [3061.168], "images_per_second": 0.327, "prompt_tokens": 19, "response_tokens_est": 41, "n_tiles": 12, "output_text": "The kitchen features a modern design with a sleek black countertop and a wooden dining table. The room is well-lit with natural light streaming in through large windows, creating a bright and inviting atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24932.1, "ram_available_mb": 100840.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 24933.2, "ram_available_mb": 100838.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.25, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.87, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 35.45, "peak": 44.89, "min": 26.39}, "VIN": {"avg": 77.88, "peak": 113.8, "min": 57.98}}, "power_watts_avg": 35.45, "energy_joules_est": 108.53, "sample_count": 23, "duration_seconds": 3.061}, "timestamp": "2026-01-17T15:32:37.855560"}
{"image_index": 327, "image_name": "000000035062.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000035062.jpg", "image_width": 425, "image_height": 640, "image_resolution": "425x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1285.462, "latencies_ms": [1285.462], "images_per_second": 0.778, "prompt_tokens": 9, "response_tokens_est": 25, "n_tiles": 6, "output_text": "A baby is lying in a bed covered with a blanket that has a pattern of white daisies with orange centers.", "error": null, "sys_before": {"cpu_percent": 43.2, "ram_used_mb": 24953.4, "ram_available_mb": 100818.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 24955.2, "ram_available_mb": 100817.0, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.57, "peak": 15.04, "min": 13.72}, "VDD_CPU_SOC_MSS": {"avg": 15.18, "peak": 15.75, "min": 14.18}, "VDD_GPU": {"avg": 35.1, "peak": 40.95, "min": 27.97}, "VIN": {"avg": 85.33, "peak": 122.01, "min": 56.54}}, "power_watts_avg": 35.1, "energy_joules_est": 45.13, "sample_count": 9, "duration_seconds": 1.286}, "timestamp": "2026-01-17T15:32:39.206455"}
{"image_index": 327, "image_name": "000000035062.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000035062.jpg", "image_width": 425, "image_height": 640, "image_resolution": "425x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1302.561, "latencies_ms": [1302.561], "images_per_second": 0.768, "prompt_tokens": 23, "response_tokens_est": 24, "n_tiles": 6, "output_text": "bed: 1\npillow: 1\nblanket: 1\ndresser: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24955.2, "ram_available_mb": 100817.0, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 24955.9, "ram_available_mb": 100816.3, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.9, "peak": 15.44, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.55, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 35.05, "peak": 41.34, "min": 27.18}, "VIN": {"avg": 80.23, "peak": 117.83, "min": 61.07}}, "power_watts_avg": 35.05, "energy_joules_est": 45.67, "sample_count": 10, "duration_seconds": 1.303}, "timestamp": "2026-01-17T15:32:40.515349"}
{"image_index": 327, "image_name": "000000035062.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000035062.jpg", "image_width": 425, "image_height": 640, "image_resolution": "425x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3184.058, "latencies_ms": [3184.058], "images_per_second": 0.314, "prompt_tokens": 27, "response_tokens_est": 93, "n_tiles": 6, "output_text": "The main object in the image is a bed with a patterned blanket. The blanket is spread out on the bed, covering the entire surface. The pattern on the blanket consists of white daisies with orange centers. The bed is positioned against a wall, and there is a small piece of furniture, possibly a nightstand, to the right of the bed. The overall scene is dimly lit, with the primary focus on the bed and its blanket.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24955.9, "ram_available_mb": 100816.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 24956.9, "ram_available_mb": 100815.3, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.07, "peak": 15.44, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.67, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 28.69, "peak": 41.34, "min": 23.24}, "VIN": {"avg": 70.06, "peak": 111.46, "min": 52.83}}, "power_watts_avg": 28.69, "energy_joules_est": 91.36, "sample_count": 24, "duration_seconds": 3.185}, "timestamp": "2026-01-17T15:32:43.705849"}
{"image_index": 327, "image_name": "000000035062.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000035062.jpg", "image_width": 425, "image_height": 640, "image_resolution": "425x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2300.746, "latencies_ms": [2300.746], "images_per_second": 0.435, "prompt_tokens": 21, "response_tokens_est": 61, "n_tiles": 6, "output_text": "The image depicts a cozy, dimly lit bedroom scene where a baby is lying on a bed covered with a dark blue blanket adorned with white daisy patterns. The baby appears to be sleeping peacefully, surrounded by the soft glow of a lamp, creating a serene and intimate atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24956.9, "ram_available_mb": 100815.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 24957.6, "ram_available_mb": 100814.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.03, "peak": 15.54, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.68, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 30.17, "peak": 40.16, "min": 23.64}, "VIN": {"avg": 74.6, "peak": 116.28, "min": 63.34}}, "power_watts_avg": 30.17, "energy_joules_est": 69.43, "sample_count": 17, "duration_seconds": 2.301}, "timestamp": "2026-01-17T15:32:46.013798"}
{"image_index": 327, "image_name": "000000035062.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000035062.jpg", "image_width": 425, "image_height": 640, "image_resolution": "425x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1488.583, "latencies_ms": [1488.583], "images_per_second": 0.672, "prompt_tokens": 19, "response_tokens_est": 31, "n_tiles": 6, "output_text": "The image features a bed with a dark blue blanket adorned with white daisy patterns. The lighting is dim, creating a cozy and intimate atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24957.6, "ram_available_mb": 100814.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 24958.6, "ram_available_mb": 100813.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.96, "peak": 15.54, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.53, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 33.31, "peak": 40.57, "min": 26.01}, "VIN": {"avg": 73.08, "peak": 110.64, "min": 50.17}}, "power_watts_avg": 33.31, "energy_joules_est": 49.6, "sample_count": 11, "duration_seconds": 1.489}, "timestamp": "2026-01-17T15:32:47.508444"}
{"image_index": 328, "image_name": "000000035197.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000035197.jpg", "image_width": 429, "image_height": 640, "image_resolution": "429x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1484.073, "latencies_ms": [1484.073], "images_per_second": 0.674, "prompt_tokens": 9, "response_tokens_est": 33, "n_tiles": 6, "output_text": "A skateboarder is captured mid-action on a skateboard, with a black and white photograph emphasizing the contrast between the skateboard and the urban environment.", "error": null, "sys_before": {"cpu_percent": 47.4, "ram_used_mb": 24958.3, "ram_available_mb": 100813.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 24959.1, "ram_available_mb": 100813.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.79, "peak": 15.24, "min": 13.9}, "VDD_CPU_SOC_MSS": {"avg": 15.46, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 33.23, "peak": 40.97, "min": 26.0}, "VIN": {"avg": 76.23, "peak": 111.59, "min": 59.39}}, "power_watts_avg": 33.23, "energy_joules_est": 49.33, "sample_count": 11, "duration_seconds": 1.485}, "timestamp": "2026-01-17T15:32:49.050268"}
{"image_index": 328, "image_name": "000000035197.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000035197.jpg", "image_width": 429, "image_height": 640, "image_resolution": "429x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 4118.132, "latencies_ms": [4118.132], "images_per_second": 0.243, "prompt_tokens": 23, "response_tokens_est": 128, "n_tiles": 6, "output_text": "skateboard: 1\nskateboarder: 1\nskateboard: 1\nskateboarder: 1\nskateboard: 1\nskateboard: 1\nskateboard: 1\nskateboard: 1\nskateboard: 1\nskateboard: 1\nskateboard: 1\nskateboard: 1\nskateboard: 1\nskateboard: 1\nskateboard: 1\nskateboard: 1\nskateboard: 1\nskateboard: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24959.1, "ram_available_mb": 100813.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 24959.8, "ram_available_mb": 100812.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.14, "peak": 15.54, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.7, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 27.28, "peak": 40.56, "min": 23.24}, "VIN": {"avg": 67.43, "peak": 83.58, "min": 58.81}}, "power_watts_avg": 27.28, "energy_joules_est": 112.35, "sample_count": 31, "duration_seconds": 4.119}, "timestamp": "2026-01-17T15:32:53.174529"}
{"image_index": 328, "image_name": "000000035197.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000035197.jpg", "image_width": 429, "image_height": 640, "image_resolution": "429x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2868.847, "latencies_ms": [2868.847], "images_per_second": 0.349, "prompt_tokens": 27, "response_tokens_est": 82, "n_tiles": 6, "output_text": "The main object in the foreground is a skateboard, which is positioned near the bottom left corner of the image. The skateboard has a visible wheel and a deck. In the background, there is a person standing on the right side of the image, near the skateboard. The person is wearing shorts and sneakers. The skateboard is in the foreground, while the person is in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24959.8, "ram_available_mb": 100812.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 24960.5, "ram_available_mb": 100811.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.1, "peak": 15.54, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.69, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 28.92, "peak": 40.16, "min": 23.64}, "VIN": {"avg": 70.16, "peak": 92.17, "min": 57.8}}, "power_watts_avg": 28.92, "energy_joules_est": 82.98, "sample_count": 21, "duration_seconds": 2.869}, "timestamp": "2026-01-17T15:32:56.049255"}
{"image_index": 328, "image_name": "000000035197.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000035197.jpg", "image_width": 429, "image_height": 640, "image_resolution": "429x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2239.483, "latencies_ms": [2239.483], "images_per_second": 0.447, "prompt_tokens": 21, "response_tokens_est": 59, "n_tiles": 6, "output_text": "The image depicts a skateboarder in motion, captured in black and white, on a concrete surface with a large, angular concrete structure in the background. The skateboarder is wearing casual attire, including shorts and sneakers, and is performing a trick on the skateboard.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24960.5, "ram_available_mb": 100811.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 24960.5, "ram_available_mb": 100811.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.06, "peak": 15.54, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.62, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 30.53, "peak": 40.16, "min": 24.03}, "VIN": {"avg": 74.16, "peak": 111.25, "min": 62.38}}, "power_watts_avg": 30.53, "energy_joules_est": 68.38, "sample_count": 16, "duration_seconds": 2.24}, "timestamp": "2026-01-17T15:32:58.298899"}
{"image_index": 328, "image_name": "000000035197.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000035197.jpg", "image_width": 429, "image_height": 640, "image_resolution": "429x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2863.846, "latencies_ms": [2863.846], "images_per_second": 0.349, "prompt_tokens": 19, "response_tokens_est": 80, "n_tiles": 6, "output_text": "The image is a black and white photograph featuring a skateboarder in motion, captured with a high-contrast, sharp focus. The skateboarder is wearing a white t-shirt, plaid shorts, and sneakers, with a shadow cast on the ground. The lighting is bright, likely from a clear sky, and the skateboarder's shadow is clearly visible on the ground.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24960.5, "ram_available_mb": 100811.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 24960.8, "ram_available_mb": 100811.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.02, "peak": 15.44, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.66, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 28.47, "peak": 40.16, "min": 22.85}, "VIN": {"avg": 68.54, "peak": 108.74, "min": 55.88}}, "power_watts_avg": 28.47, "energy_joules_est": 81.54, "sample_count": 22, "duration_seconds": 2.864}, "timestamp": "2026-01-17T15:33:01.168937"}
{"image_index": 329, "image_name": "000000035279.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000035279.jpg", "image_width": 640, "image_height": 463, "image_resolution": "640x463", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2827.111, "latencies_ms": [2827.111], "images_per_second": 0.354, "prompt_tokens": 9, "response_tokens_est": 36, "n_tiles": 12, "output_text": "The image shows a man lying on the floor with various items scattered around him, including a laptop, a camera, a smartphone, a book, and a tennis racket.", "error": null, "sys_before": {"cpu_percent": 44.6, "ram_used_mb": 24920.5, "ram_available_mb": 100851.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 24922.7, "ram_available_mb": 100849.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.2, "peak": 16.66, "min": 13.59}, "VDD_CPU_SOC_MSS": {"avg": 15.91, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 34.95, "peak": 43.71, "min": 26.39}, "VIN": {"avg": 76.93, "peak": 105.56, "min": 61.76}}, "power_watts_avg": 34.95, "energy_joules_est": 98.82, "sample_count": 22, "duration_seconds": 2.827}, "timestamp": "2026-01-17T15:33:04.078397"}
{"image_index": 329, "image_name": "000000035279.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000035279.jpg", "image_width": 640, "image_height": 463, "image_resolution": "640x463", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2895.59, "latencies_ms": [2895.59], "images_per_second": 0.345, "prompt_tokens": 23, "response_tokens_est": 36, "n_tiles": 12, "output_text": "1. Laptop\n2. Phone\n3. Camera\n4. Book\n5. Tennis racket\n6. Tennis ball\n7. Water bottle\n8. Key", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24922.7, "ram_available_mb": 100849.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 24923.5, "ram_available_mb": 100848.7, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.26, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.89, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 35.92, "peak": 44.91, "min": 26.8}, "VIN": {"avg": 76.58, "peak": 104.75, "min": 58.13}}, "power_watts_avg": 35.92, "energy_joules_est": 104.02, "sample_count": 22, "duration_seconds": 2.896}, "timestamp": "2026-01-17T15:33:06.980665"}
{"image_index": 329, "image_name": "000000035279.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000035279.jpg", "image_width": 640, "image_height": 463, "image_resolution": "640x463", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 4121.709, "latencies_ms": [4121.709], "images_per_second": 0.243, "prompt_tokens": 27, "response_tokens_est": 72, "n_tiles": 12, "output_text": "The main objects in the image are scattered across the carpeted floor. The laptop is positioned in the background, slightly to the left, with a bottle and a phone nearby. The book, with its title \"The Men Who Ruled India,\" is in the foreground, lying on the carpet. The book's title and author's name are clearly visible.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24923.5, "ram_available_mb": 100848.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 24924.7, "ram_available_mb": 100847.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.18, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.8, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 32.94, "peak": 44.89, "min": 25.6}, "VIN": {"avg": 75.2, "peak": 126.05, "min": 62.41}}, "power_watts_avg": 32.94, "energy_joules_est": 135.78, "sample_count": 32, "duration_seconds": 4.122}, "timestamp": "2026-01-17T15:33:11.109060"}
{"image_index": 329, "image_name": "000000035279.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000035279.jpg", "image_width": 640, "image_height": 463, "image_resolution": "640x463", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3567.373, "latencies_ms": [3567.373], "images_per_second": 0.28, "prompt_tokens": 21, "response_tokens_est": 56, "n_tiles": 12, "output_text": "The image depicts a cluttered indoor setting with a person lying on the floor, surrounded by various items. The person is lying on a carpeted floor, and there is a laptop, a smartphone, a camera, a book, and other miscellaneous objects scattered around.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24924.7, "ram_available_mb": 100847.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 24925.2, "ram_available_mb": 100847.0, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.23, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.92, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 34.0, "peak": 44.49, "min": 26.0}, "VIN": {"avg": 77.04, "peak": 109.63, "min": 65.82}}, "power_watts_avg": 34.0, "energy_joules_est": 121.31, "sample_count": 27, "duration_seconds": 3.568}, "timestamp": "2026-01-17T15:33:14.682903"}
{"image_index": 329, "image_name": "000000035279.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000035279.jpg", "image_width": 640, "image_height": 463, "image_resolution": "640x463", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3910.538, "latencies_ms": [3910.538], "images_per_second": 0.256, "prompt_tokens": 19, "response_tokens_est": 66, "n_tiles": 12, "output_text": "The image features a laptop with a green snake graphic on its screen, a black camera, a black smartphone, a red and black tennis racket, and a book titled \"The Men Who Ruled India\" by Philip Mason. The lighting is bright, and the overall setting appears to be indoors with a carpeted floor.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24925.2, "ram_available_mb": 100847.0, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 24925.2, "ram_available_mb": 100847.0, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.18, "peak": 16.66, "min": 14.12}, "VDD_CPU_SOC_MSS": {"avg": 15.87, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 33.28, "peak": 44.51, "min": 26.0}, "VIN": {"avg": 72.26, "peak": 91.16, "min": 62.65}}, "power_watts_avg": 33.28, "energy_joules_est": 130.16, "sample_count": 30, "duration_seconds": 3.911}, "timestamp": "2026-01-17T15:33:18.601101"}
{"image_index": 330, "image_name": "000000035326.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000035326.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2697.341, "latencies_ms": [2697.341], "images_per_second": 0.371, "prompt_tokens": 9, "response_tokens_est": 32, "n_tiles": 12, "output_text": "The image shows a kitchen countertop with a black stove top, a black oven door, and various kitchen utensils and containers on the countertop.", "error": null, "sys_before": {"cpu_percent": 44.7, "ram_used_mb": 24929.5, "ram_available_mb": 100842.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 24931.0, "ram_available_mb": 100841.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.17, "peak": 16.66, "min": 13.39}, "VDD_CPU_SOC_MSS": {"avg": 15.92, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 35.79, "peak": 44.12, "min": 27.19}, "VIN": {"avg": 81.73, "peak": 115.59, "min": 65.85}}, "power_watts_avg": 35.79, "energy_joules_est": 96.55, "sample_count": 20, "duration_seconds": 2.698}, "timestamp": "2026-01-17T15:33:21.400139"}
{"image_index": 330, "image_name": "000000035326.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000035326.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 3980.705, "latencies_ms": [3980.705], "images_per_second": 0.251, "prompt_tokens": 23, "response_tokens_est": 68, "n_tiles": 12, "output_text": "- stove: 1\n- oven: 1\n- oven mitt: 1\n- oven rack: 1\n- oven door: 1\n- oven handle: 1\n- oven knob: 1\n- oven pan: 1\n- oven tray: 1\n- oven drawer: 1", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24928.8, "ram_available_mb": 100843.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 24926.9, "ram_available_mb": 100845.3, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.2, "peak": 16.66, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.71, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 33.16, "peak": 45.28, "min": 26.0}, "VIN": {"avg": 76.66, "peak": 122.18, "min": 55.72}}, "power_watts_avg": 33.16, "energy_joules_est": 132.02, "sample_count": 31, "duration_seconds": 3.981}, "timestamp": "2026-01-17T15:33:25.387160"}
{"image_index": 330, "image_name": "000000035326.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000035326.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3564.71, "latencies_ms": [3564.71], "images_per_second": 0.281, "prompt_tokens": 27, "response_tokens_est": 56, "n_tiles": 12, "output_text": "The main objects in the image are a kitchen countertop, a stove, and a kitchen towel. The countertop is located in the foreground, with the stove positioned in the middle ground. The kitchen towel is hanging on the right side of the image, near the stove.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24926.9, "ram_available_mb": 100845.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 24922.9, "ram_available_mb": 100849.3, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.23, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.76, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 34.01, "peak": 44.89, "min": 26.0}, "VIN": {"avg": 79.03, "peak": 124.29, "min": 60.32}}, "power_watts_avg": 34.01, "energy_joules_est": 121.25, "sample_count": 28, "duration_seconds": 3.565}, "timestamp": "2026-01-17T15:33:28.958998"}
{"image_index": 330, "image_name": "000000035326.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000035326.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3869.944, "latencies_ms": [3869.944], "images_per_second": 0.258, "prompt_tokens": 21, "response_tokens_est": 66, "n_tiles": 12, "output_text": "The image depicts a kitchen countertop with a black stove and a black oven. The countertop is adorned with various kitchen items, including a black trash can, a black oven mitt, and a black oven rack. The setting appears to be a well-organized kitchen with a focus on functionality and cleanliness.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24922.9, "ram_available_mb": 100849.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 24923.1, "ram_available_mb": 100849.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.29, "peak": 16.66, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.82, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 33.6, "peak": 44.89, "min": 26.0}, "VIN": {"avg": 75.93, "peak": 115.49, "min": 66.35}}, "power_watts_avg": 33.6, "energy_joules_est": 130.04, "sample_count": 30, "duration_seconds": 3.87}, "timestamp": "2026-01-17T15:33:32.835711"}
{"image_index": 330, "image_name": "000000035326.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000035326.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3850.013, "latencies_ms": [3850.013], "images_per_second": 0.26, "prompt_tokens": 19, "response_tokens_est": 65, "n_tiles": 12, "output_text": "The image shows a kitchen counter with a dark countertop and a black stove. The counter is adorned with various kitchen items, including a black trash can, a black oven, and a black stove with white cooking rings. The lighting in the kitchen is dim, and the overall atmosphere appears to be warm and cozy.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24923.1, "ram_available_mb": 100849.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 24923.1, "ram_available_mb": 100849.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.26, "peak": 16.66, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.88, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 33.5, "peak": 44.89, "min": 26.0}, "VIN": {"avg": 78.46, "peak": 128.74, "min": 64.83}}, "power_watts_avg": 33.5, "energy_joules_est": 128.99, "sample_count": 30, "duration_seconds": 3.85}, "timestamp": "2026-01-17T15:33:36.692404"}
{"image_index": 331, "image_name": "000000035682.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000035682.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2994.941, "latencies_ms": [2994.941], "images_per_second": 0.334, "prompt_tokens": 9, "response_tokens_est": 41, "n_tiles": 12, "output_text": "A young man is sitting at a table in a restaurant, holding a donut with pink icing and colorful sprinkles, while a tray of donuts and pastries is visible in the background.", "error": null, "sys_before": {"cpu_percent": 44.2, "ram_used_mb": 24917.7, "ram_available_mb": 100854.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 24919.4, "ram_available_mb": 100852.7, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.16, "peak": 16.66, "min": 13.39}, "VDD_CPU_SOC_MSS": {"avg": 15.87, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 34.84, "peak": 44.49, "min": 26.4}, "VIN": {"avg": 78.37, "peak": 125.98, "min": 66.04}}, "power_watts_avg": 34.84, "energy_joules_est": 104.36, "sample_count": 23, "duration_seconds": 2.995}, "timestamp": "2026-01-17T15:33:39.790583"}
{"image_index": 331, "image_name": "000000035682.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000035682.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 6040.548, "latencies_ms": [6040.548], "images_per_second": 0.166, "prompt_tokens": 23, "response_tokens_est": 129, "n_tiles": 12, "output_text": "- donut: 3\n- cup: 1\n- plate: 1\n- tray: 1\n- napkin: 1\n- donut with frosting: 1\n- donut with sprinkles: 1\n- donut with chocolate: 1\n- donut with lemon: 1\n- donut with red sprinkles: 1\n- donut with red and yellow sprinkles: 1\n- donut with red and white sprinkles: 1\n- donut with red and yellow sprinkles: 1\n- donut with red and white sprink", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24919.4, "ram_available_mb": 100852.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 24919.9, "ram_available_mb": 100852.3, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.14, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.79, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 30.76, "peak": 44.89, "min": 26.0}, "VIN": {"avg": 72.06, "peak": 101.22, "min": 56.95}}, "power_watts_avg": 30.76, "energy_joules_est": 185.82, "sample_count": 47, "duration_seconds": 6.041}, "timestamp": "2026-01-17T15:33:45.837953"}
{"image_index": 331, "image_name": "000000035682.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000035682.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 4881.25, "latencies_ms": [4881.25], "images_per_second": 0.205, "prompt_tokens": 27, "response_tokens_est": 94, "n_tiles": 12, "output_text": "The main object in the foreground is a plate of donuts, with a pink glazed donut in the center and a chocolate glazed donut to its right. The plate is placed on a tray, which is in the foreground. In the background, there is a person standing behind a counter, and a table with a yellow tablecloth. The donuts are placed near the person and the table, while the person is further away in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24919.9, "ram_available_mb": 100852.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 24920.9, "ram_available_mb": 100851.3, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.15, "peak": 16.66, "min": 14.12}, "VDD_CPU_SOC_MSS": {"avg": 15.76, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 31.88, "peak": 44.89, "min": 26.0}, "VIN": {"avg": 74.91, "peak": 113.87, "min": 57.3}}, "power_watts_avg": 31.88, "energy_joules_est": 155.63, "sample_count": 38, "duration_seconds": 4.882}, "timestamp": "2026-01-17T15:33:50.727824"}
{"image_index": 331, "image_name": "000000035682.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000035682.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 4014.597, "latencies_ms": [4014.597], "images_per_second": 0.249, "prompt_tokens": 21, "response_tokens_est": 69, "n_tiles": 12, "output_text": "The image depicts a cozy and inviting setting, likely inside a caf\u00e9 or restaurant. The scene is filled with various items on a table, including a plate of donuts, a cup of coffee, and a tray with a tray of donuts. The background shows a person sitting at a table, possibly enjoying their meal or coffee.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24920.9, "ram_available_mb": 100851.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 24921.6, "ram_available_mb": 100850.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.17, "peak": 16.66, "min": 14.12}, "VDD_CPU_SOC_MSS": {"avg": 15.7, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 33.07, "peak": 45.69, "min": 26.0}, "VIN": {"avg": 75.53, "peak": 108.99, "min": 56.63}}, "power_watts_avg": 33.07, "energy_joules_est": 132.78, "sample_count": 31, "duration_seconds": 4.015}, "timestamp": "2026-01-17T15:33:54.748827"}
{"image_index": 331, "image_name": "000000035682.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000035682.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3877.394, "latencies_ms": [3877.394], "images_per_second": 0.258, "prompt_tokens": 19, "response_tokens_est": 65, "n_tiles": 12, "output_text": "The image features a warmly lit indoor setting with a focus on a tray of donuts. The donuts are adorned with various colorful sprinkles and glazed in shades of pink and red. The tray is placed on a table, and the background includes a blurred counter with a person and a yellow bag.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24921.6, "ram_available_mb": 100850.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 24922.1, "ram_available_mb": 100850.0, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.2, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.65, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 33.38, "peak": 44.89, "min": 26.0}, "VIN": {"avg": 77.1, "peak": 117.82, "min": 54.91}}, "power_watts_avg": 33.38, "energy_joules_est": 129.44, "sample_count": 30, "duration_seconds": 3.878}, "timestamp": "2026-01-17T15:33:58.632423"}
{"image_index": 332, "image_name": "000000035770.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000035770.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2404.951, "latencies_ms": [2404.951], "images_per_second": 0.416, "prompt_tokens": 9, "response_tokens_est": 23, "n_tiles": 12, "output_text": "The image shows a bathroom sink with a decorative plate on the side, featuring a black and white cat design.", "error": null, "sys_before": {"cpu_percent": 42.2, "ram_used_mb": 24919.5, "ram_available_mb": 100852.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.4, "ram_used_mb": 24920.3, "ram_available_mb": 100851.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.06, "peak": 16.56, "min": 13.39}, "VDD_CPU_SOC_MSS": {"avg": 15.77, "peak": 17.71, "min": 14.17}, "VDD_GPU": {"avg": 36.83, "peak": 44.12, "min": 27.97}, "VIN": {"avg": 75.14, "peak": 109.27, "min": 63.88}}, "power_watts_avg": 36.83, "energy_joules_est": 88.58, "sample_count": 18, "duration_seconds": 2.405}, "timestamp": "2026-01-17T15:34:01.136525"}
{"image_index": 332, "image_name": "000000035770.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000035770.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 3202.653, "latencies_ms": [3202.653], "images_per_second": 0.312, "prompt_tokens": 23, "response_tokens_est": 45, "n_tiles": 12, "output_text": "- sink: 1\n- toilet: 1\n- tiled wall: 1\n- table: 1\n- decorative plate: 1\n- fish: 1\n- cat: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24920.3, "ram_available_mb": 100851.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 24917.8, "ram_available_mb": 100854.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.97, "peak": 16.36, "min": 14.02}, "VDD_CPU_SOC_MSS": {"avg": 15.52, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 35.26, "peak": 46.48, "min": 26.0}, "VIN": {"avg": 75.82, "peak": 117.05, "min": 59.57}}, "power_watts_avg": 35.26, "energy_joules_est": 112.94, "sample_count": 25, "duration_seconds": 3.203}, "timestamp": "2026-01-17T15:34:04.345661"}
{"image_index": 332, "image_name": "000000035770.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000035770.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3821.171, "latencies_ms": [3821.171], "images_per_second": 0.262, "prompt_tokens": 27, "response_tokens_est": 63, "n_tiles": 12, "output_text": "The main object in the foreground is a white ceramic sink with a clear glass basin. The sink is placed on a wooden countertop. In the background, there is a white toilet with a blue-tiled wall. The sink is positioned near the toilet, creating a functional and visually appealing layout in the bathroom.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24917.8, "ram_available_mb": 100854.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 24918.8, "ram_available_mb": 100853.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.95, "peak": 16.46, "min": 14.12}, "VDD_CPU_SOC_MSS": {"avg": 15.54, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 33.66, "peak": 44.89, "min": 25.6}, "VIN": {"avg": 77.28, "peak": 117.51, "min": 63.75}}, "power_watts_avg": 33.66, "energy_joules_est": 128.64, "sample_count": 29, "duration_seconds": 3.822}, "timestamp": "2026-01-17T15:34:08.174574"}
{"image_index": 332, "image_name": "000000035770.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000035770.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3966.454, "latencies_ms": [3966.454], "images_per_second": 0.252, "prompt_tokens": 21, "response_tokens_est": 67, "n_tiles": 12, "output_text": "The image depicts a bathroom setting with a white bathtub and a glass shower enclosure. The bathtub is partially filled with water, and there is a decorative plate with a turtle design on the edge of the tub. The shower area is tiled with blue tiles, and the floor appears to be a dark color.", "error": null, "sys_before": {"cpu_percent": 16.7, "ram_used_mb": 24918.8, "ram_available_mb": 100853.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 24918.8, "ram_available_mb": 100853.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.9, "peak": 16.36, "min": 13.92}, "VDD_CPU_SOC_MSS": {"avg": 15.4, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 33.1, "peak": 45.3, "min": 25.6}, "VIN": {"avg": 74.15, "peak": 111.47, "min": 59.8}}, "power_watts_avg": 33.1, "energy_joules_est": 131.3, "sample_count": 31, "duration_seconds": 3.967}, "timestamp": "2026-01-17T15:34:12.148034"}
{"image_index": 332, "image_name": "000000035770.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000035770.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3653.802, "latencies_ms": [3653.802], "images_per_second": 0.274, "prompt_tokens": 19, "response_tokens_est": 58, "n_tiles": 12, "output_text": "The image features a bathroom with a white bathtub and a white sink. The bathtub has a blue tiled surround, and the sink has a clear glass top. The lighting in the bathroom is bright, and the overall color scheme is neutral with white and blue tones.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24918.8, "ram_available_mb": 100853.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 24919.3, "ram_available_mb": 100852.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.91, "peak": 16.26, "min": 13.92}, "VDD_CPU_SOC_MSS": {"avg": 15.42, "peak": 17.32, "min": 14.17}, "VDD_GPU": {"avg": 33.75, "peak": 44.91, "min": 25.6}, "VIN": {"avg": 73.36, "peak": 108.64, "min": 61.74}}, "power_watts_avg": 33.75, "energy_joules_est": 123.33, "sample_count": 28, "duration_seconds": 3.654}, "timestamp": "2026-01-17T15:34:15.808044"}
{"image_index": 333, "image_name": "000000035963.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000035963.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 3515.842, "latencies_ms": [3515.842], "images_per_second": 0.284, "prompt_tokens": 9, "response_tokens_est": 57, "n_tiles": 12, "output_text": "A small, white teddy bear is sitting on a wooden sign that reads \"Joseph Panis\" and \"Marilyn,\" with a sign that says \"Died 1-1-53\" and \"Died 4-11-98\" in the background.", "error": null, "sys_before": {"cpu_percent": 40.0, "ram_used_mb": 24932.0, "ram_available_mb": 100840.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 24934.5, "ram_available_mb": 100837.7, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.15, "peak": 16.76, "min": 13.19}, "VDD_CPU_SOC_MSS": {"avg": 15.82, "peak": 17.71, "min": 14.17}, "VDD_GPU": {"avg": 33.69, "peak": 44.12, "min": 26.39}, "VIN": {"avg": 77.26, "peak": 122.31, "min": 61.99}}, "power_watts_avg": 33.69, "energy_joules_est": 118.46, "sample_count": 27, "duration_seconds": 3.516}, "timestamp": "2026-01-17T15:34:19.427144"}
{"image_index": 333, "image_name": "000000035963.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000035963.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 3030.859, "latencies_ms": [3030.859], "images_per_second": 0.33, "prompt_tokens": 23, "response_tokens_est": 40, "n_tiles": 12, "output_text": "object: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24934.5, "ram_available_mb": 100837.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 24935.7, "ram_available_mb": 100836.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.25, "peak": 16.56, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.87, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 35.81, "peak": 45.69, "min": 26.39}, "VIN": {"avg": 83.1, "peak": 135.43, "min": 58.71}}, "power_watts_avg": 35.81, "energy_joules_est": 108.55, "sample_count": 23, "duration_seconds": 3.031}, "timestamp": "2026-01-17T15:34:22.464670"}
{"image_index": 333, "image_name": "000000035963.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000035963.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 5911.354, "latencies_ms": [5911.354], "images_per_second": 0.169, "prompt_tokens": 27, "response_tokens_est": 124, "n_tiles": 12, "output_text": "The main object in the foreground is a large, white teddy bear with a wooden sign that reads \"Joseph Panis\" and \"Mystie Malulani.\" The bear is positioned on a patch of grass, with a small pile of red bricks nearby. In the background, there is a white pillar and a sign that says \"DIED 1-1-53\" and \"DIED 4-1-98.\" The scene appears to be set in a garden or park, with the teddy bear and sign placed in the foreground, while the background features a white pillar and a sign.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 24935.7, "ram_available_mb": 100836.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 24935.7, "ram_available_mb": 100836.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.13, "peak": 16.66, "min": 14.12}, "VDD_CPU_SOC_MSS": {"avg": 15.69, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 30.83, "peak": 45.28, "min": 25.6}, "VIN": {"avg": 70.41, "peak": 108.63, "min": 54.98}}, "power_watts_avg": 30.83, "energy_joules_est": 182.26, "sample_count": 46, "duration_seconds": 5.912}, "timestamp": "2026-01-17T15:34:28.383353"}
{"image_index": 333, "image_name": "000000035963.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000035963.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 4387.43, "latencies_ms": [4387.43], "images_per_second": 0.228, "prompt_tokens": 21, "response_tokens_est": 80, "n_tiles": 12, "output_text": "The image depicts a serene outdoor scene with a small, weathered wooden sign reading \"Joseph Panis\" and a white teddy bear sitting on top of it. The bear is positioned on a patch of grass, with a few small red bricks and a few green plants in the background. The setting appears to be a garden or park, and the overall atmosphere is peaceful and quiet.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24935.7, "ram_available_mb": 100836.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 24935.7, "ram_available_mb": 100836.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.17, "peak": 16.66, "min": 14.12}, "VDD_CPU_SOC_MSS": {"avg": 15.73, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 32.59, "peak": 46.09, "min": 26.0}, "VIN": {"avg": 75.73, "peak": 127.88, "min": 66.58}}, "power_watts_avg": 32.59, "energy_joules_est": 143.0, "sample_count": 34, "duration_seconds": 4.388}, "timestamp": "2026-01-17T15:34:32.777150"}
{"image_index": 333, "image_name": "000000035963.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000035963.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3400.753, "latencies_ms": [3400.753], "images_per_second": 0.294, "prompt_tokens": 19, "response_tokens_est": 51, "n_tiles": 12, "output_text": "The notable visual attributes of the image include a white teddy bear with a distressed appearance, placed on a weathered wooden sign. The setting is outdoors with a mix of greenery and brown dirt, and the lighting is natural, suggesting daytime.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24935.7, "ram_available_mb": 100836.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 24935.9, "ram_available_mb": 100836.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.2, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.76, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 34.54, "peak": 45.28, "min": 26.01}, "VIN": {"avg": 77.52, "peak": 122.15, "min": 64.61}}, "power_watts_avg": 34.54, "energy_joules_est": 117.48, "sample_count": 26, "duration_seconds": 3.401}, "timestamp": "2026-01-17T15:34:36.184081"}
{"image_index": 334, "image_name": "000000036494.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000036494.jpg", "image_width": 640, "image_height": 434, "image_resolution": "640x434", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1789.244, "latencies_ms": [1789.244], "images_per_second": 0.559, "prompt_tokens": 9, "response_tokens_est": 44, "n_tiles": 6, "output_text": "The image depicts a bustling restaurant with numerous patrons seated at tables, engaged in conversation and dining. The interior features a large clock prominently displayed on the wall, adding a unique architectural element to the space.", "error": null, "sys_before": {"cpu_percent": 41.1, "ram_used_mb": 24957.3, "ram_available_mb": 100814.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 24958.3, "ram_available_mb": 100813.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.77, "peak": 15.22, "min": 13.72}, "VDD_CPU_SOC_MSS": {"avg": 15.41, "peak": 16.14, "min": 14.17}, "VDD_GPU": {"avg": 31.54, "peak": 40.56, "min": 24.42}, "VIN": {"avg": 71.94, "peak": 96.27, "min": 60.68}}, "power_watts_avg": 31.54, "energy_joules_est": 56.45, "sample_count": 14, "duration_seconds": 1.79}, "timestamp": "2026-01-17T15:34:38.041552"}
{"image_index": 334, "image_name": "000000036494.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000036494.jpg", "image_width": 640, "image_height": 434, "image_resolution": "640x434", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 4072.4, "latencies_ms": [4072.4], "images_per_second": 0.246, "prompt_tokens": 23, "response_tokens_est": 126, "n_tiles": 6, "output_text": "- People: 20\n- Tables: 10\n- Chairs: 20\n- Cups: 10\n- Glasses: 10\n- Napkins: 10\n- Utensils: 10\n- Food: 10\n- Dishes: 10\n- Cutlery: 10\n- Cutting board: 10\n- Cutting knife: 10\n- Spoons: 10\n- Plates: 10\n- Bowls: 10\n- Cups: 10\n- Candles: 1\n- Clock: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24958.3, "ram_available_mb": 100813.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 24958.7, "ram_available_mb": 100813.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.14, "peak": 15.44, "min": 14.43}, "VDD_CPU_SOC_MSS": {"avg": 15.7, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 27.17, "peak": 40.56, "min": 23.24}, "VIN": {"avg": 68.37, "peak": 108.89, "min": 60.13}}, "power_watts_avg": 27.17, "energy_joules_est": 110.65, "sample_count": 32, "duration_seconds": 4.073}, "timestamp": "2026-01-17T15:34:42.119703"}
{"image_index": 334, "image_name": "000000036494.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000036494.jpg", "image_width": 640, "image_height": 434, "image_resolution": "640x434", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2516.088, "latencies_ms": [2516.088], "images_per_second": 0.397, "prompt_tokens": 27, "response_tokens_est": 69, "n_tiles": 6, "output_text": "The main objects in the image are the large clock and the dining area. The clock is positioned in the background, with its hands pointing towards the center of the image. The dining area is in the foreground, with people seated at tables around the clock. The clock is near the ceiling, while the dining area is on the ground floor.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24958.7, "ram_available_mb": 100813.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 24959.5, "ram_available_mb": 100812.7, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.08, "peak": 15.54, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.71, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 29.54, "peak": 40.16, "min": 23.64}, "VIN": {"avg": 71.2, "peak": 108.91, "min": 57.44}}, "power_watts_avg": 29.54, "energy_joules_est": 74.33, "sample_count": 19, "duration_seconds": 2.516}, "timestamp": "2026-01-17T15:34:44.642571"}
{"image_index": 334, "image_name": "000000036494.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000036494.jpg", "image_width": 640, "image_height": 434, "image_resolution": "640x434", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2296.983, "latencies_ms": [2296.983], "images_per_second": 0.435, "prompt_tokens": 21, "response_tokens_est": 61, "n_tiles": 6, "output_text": "The image depicts a bustling restaurant interior with a large clock prominently displayed on the wall. The scene is filled with patrons seated at tables, engaged in conversations and dining. The warm lighting and rustic decor create a cozy atmosphere, while the clock adds a touch of elegance to the setting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24959.5, "ram_available_mb": 100812.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 24959.7, "ram_available_mb": 100812.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.06, "peak": 15.44, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.66, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 30.07, "peak": 40.95, "min": 23.64}, "VIN": {"avg": 71.25, "peak": 95.73, "min": 55.33}}, "power_watts_avg": 30.07, "energy_joules_est": 69.08, "sample_count": 18, "duration_seconds": 2.297}, "timestamp": "2026-01-17T15:34:46.945731"}
{"image_index": 334, "image_name": "000000036494.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000036494.jpg", "image_width": 640, "image_height": 434, "image_resolution": "640x434", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2020.512, "latencies_ms": [2020.512], "images_per_second": 0.495, "prompt_tokens": 19, "response_tokens_est": 51, "n_tiles": 6, "output_text": "The image depicts a bustling restaurant with a warm, inviting ambiance. The lighting is soft and warm, creating a cozy atmosphere. The color palette includes earthy tones, with wooden furniture and a large clock adding a touch of vintage charm.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24959.7, "ram_available_mb": 100812.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 24959.7, "ram_available_mb": 100812.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.08, "peak": 15.54, "min": 14.43}, "VDD_CPU_SOC_MSS": {"avg": 15.67, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 30.94, "peak": 40.18, "min": 24.04}, "VIN": {"avg": 69.08, "peak": 92.3, "min": 63.36}}, "power_watts_avg": 30.94, "energy_joules_est": 62.53, "sample_count": 15, "duration_seconds": 2.021}, "timestamp": "2026-01-17T15:34:48.972125"}
{"image_index": 335, "image_name": "000000036539.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000036539.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1061.125, "latencies_ms": [1061.125], "images_per_second": 0.942, "prompt_tokens": 9, "response_tokens_est": 17, "n_tiles": 6, "output_text": "A person is skiing down a snowy slope with a child following behind.", "error": null, "sys_before": {"cpu_percent": 45.5, "ram_used_mb": 24959.7, "ram_available_mb": 100812.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 24960.7, "ram_available_mb": 100811.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.62, "peak": 15.14, "min": 13.9}, "VDD_CPU_SOC_MSS": {"avg": 15.35, "peak": 16.14, "min": 14.57}, "VDD_GPU": {"avg": 35.35, "peak": 40.18, "min": 29.94}, "VIN": {"avg": 79.61, "peak": 115.0, "min": 59.25}}, "power_watts_avg": 35.35, "energy_joules_est": 37.53, "sample_count": 8, "duration_seconds": 1.062}, "timestamp": "2026-01-17T15:34:50.090512"}
{"image_index": 335, "image_name": "000000036539.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000036539.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2106.689, "latencies_ms": [2106.689], "images_per_second": 0.475, "prompt_tokens": 23, "response_tokens_est": 54, "n_tiles": 6, "output_text": "- Person: 1\n- Ski poles: 2\n- Ski: 1\n- Snowboard: 1\n- Snowsuit: 1\n- Gloves: 2\n- Hat: 1\n- Sunglasses: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24960.7, "ram_available_mb": 100811.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 24960.9, "ram_available_mb": 100811.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.02, "peak": 15.44, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.62, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 31.61, "peak": 41.74, "min": 24.03}, "VIN": {"avg": 71.99, "peak": 109.21, "min": 59.11}}, "power_watts_avg": 31.61, "energy_joules_est": 66.61, "sample_count": 16, "duration_seconds": 2.107}, "timestamp": "2026-01-17T15:34:52.202998"}
{"image_index": 335, "image_name": "000000036539.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000036539.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2188.733, "latencies_ms": [2188.733], "images_per_second": 0.457, "prompt_tokens": 27, "response_tokens_est": 57, "n_tiles": 6, "output_text": "In the image, the main objects are the two individuals and the snowy landscape. The person in the foreground is standing on a snowboard, while the other person is walking in the background. The snowy landscape is the backdrop, with trees and rocks visible in the distance.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24960.9, "ram_available_mb": 100811.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 24961.9, "ram_available_mb": 100810.3, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.05, "peak": 15.54, "min": 14.43}, "VDD_CPU_SOC_MSS": {"avg": 15.65, "peak": 16.14, "min": 14.57}, "VDD_GPU": {"avg": 30.55, "peak": 40.16, "min": 24.03}, "VIN": {"avg": 69.85, "peak": 91.43, "min": 61.57}}, "power_watts_avg": 30.55, "energy_joules_est": 66.88, "sample_count": 16, "duration_seconds": 2.189}, "timestamp": "2026-01-17T15:34:54.398088"}
{"image_index": 335, "image_name": "000000036539.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000036539.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2513.164, "latencies_ms": [2513.164], "images_per_second": 0.398, "prompt_tokens": 21, "response_tokens_est": 69, "n_tiles": 6, "output_text": "The image depicts a snowy mountain slope with a family of three enjoying a winter day. The family is dressed in winter gear, including jackets, pants, and snow boots, and are engaged in skiing activities. The setting is a snowy mountain, and the family appears to be having a fun and active day on the slopes.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24961.9, "ram_available_mb": 100810.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 24962.7, "ram_available_mb": 100809.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.08, "peak": 15.54, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.69, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 29.54, "peak": 40.56, "min": 23.64}, "VIN": {"avg": 71.53, "peak": 108.25, "min": 62.82}}, "power_watts_avg": 29.54, "energy_joules_est": 74.25, "sample_count": 19, "duration_seconds": 2.513}, "timestamp": "2026-01-17T15:34:56.917154"}
{"image_index": 335, "image_name": "000000036539.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000036539.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2208.783, "latencies_ms": [2208.783], "images_per_second": 0.453, "prompt_tokens": 19, "response_tokens_est": 58, "n_tiles": 6, "output_text": "The image depicts a snowy landscape with a bright, sunny day. The snow is pristine and untouched, with a few scattered rocks and trees in the background. The lighting is clear and bright, casting sharp shadows on the snow, indicating a sunny day with strong sunlight.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24962.7, "ram_available_mb": 100809.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 24963.9, "ram_available_mb": 100808.3, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.08, "peak": 15.54, "min": 14.43}, "VDD_CPU_SOC_MSS": {"avg": 15.62, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 30.62, "peak": 40.16, "min": 24.03}, "VIN": {"avg": 70.84, "peak": 113.12, "min": 55.34}}, "power_watts_avg": 30.62, "energy_joules_est": 67.64, "sample_count": 16, "duration_seconds": 2.209}, "timestamp": "2026-01-17T15:34:59.135782"}
{"image_index": 336, "image_name": "000000036660.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000036660.jpg", "image_width": 500, "image_height": 323, "image_resolution": "500x323", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1356.77, "latencies_ms": [1356.77], "images_per_second": 0.737, "prompt_tokens": 9, "response_tokens_est": 27, "n_tiles": 6, "output_text": "The image shows a pair of feet with white painted toenails, wearing black flip-flops, lying on a wooden surface.", "error": null, "sys_before": {"cpu_percent": 44.9, "ram_used_mb": 24963.6, "ram_available_mb": 100808.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 24963.9, "ram_available_mb": 100808.3, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.64, "peak": 15.14, "min": 13.79}, "VDD_CPU_SOC_MSS": {"avg": 15.27, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 33.44, "peak": 40.18, "min": 26.39}, "VIN": {"avg": 75.9, "peak": 109.52, "min": 55.23}}, "power_watts_avg": 33.44, "energy_joules_est": 45.39, "sample_count": 10, "duration_seconds": 1.357}, "timestamp": "2026-01-17T15:35:00.553533"}
{"image_index": 336, "image_name": "000000036660.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000036660.jpg", "image_width": 500, "image_height": 323, "image_resolution": "500x323", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1569.882, "latencies_ms": [1569.882], "images_per_second": 0.637, "prompt_tokens": 23, "response_tokens_est": 33, "n_tiles": 6, "output_text": "1. Flip phone\n2. Battery\n3. Phone\n4. Battery\n5. Phone\n6. Battery\n7. Phone\n8. Battery", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24963.9, "ram_available_mb": 100808.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 24963.6, "ram_available_mb": 100808.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.81, "peak": 15.24, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.29, "peak": 15.74, "min": 14.56}, "VDD_GPU": {"avg": 33.28, "peak": 40.95, "min": 25.21}, "VIN": {"avg": 75.05, "peak": 115.24, "min": 61.29}}, "power_watts_avg": 33.28, "energy_joules_est": 52.26, "sample_count": 12, "duration_seconds": 1.57}, "timestamp": "2026-01-17T15:35:02.129453"}
{"image_index": 336, "image_name": "000000036660.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000036660.jpg", "image_width": 500, "image_height": 323, "image_resolution": "500x323", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2476.152, "latencies_ms": [2476.152], "images_per_second": 0.404, "prompt_tokens": 27, "response_tokens_est": 65, "n_tiles": 6, "output_text": "The main objects in the image are a black flip phone, a black flip phone, and a black flip phone. The flip phones are positioned in the foreground, with the one on the left slightly closer to the viewer. The black flip phone on the right is further back, creating a sense of depth in the image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24963.6, "ram_available_mb": 100808.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 24963.6, "ram_available_mb": 100808.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.87, "peak": 15.24, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.5, "peak": 15.75, "min": 14.56}, "VDD_GPU": {"avg": 29.6, "peak": 41.34, "min": 23.24}, "VIN": {"avg": 69.53, "peak": 95.73, "min": 54.22}}, "power_watts_avg": 29.6, "energy_joules_est": 73.3, "sample_count": 19, "duration_seconds": 2.476}, "timestamp": "2026-01-17T15:35:04.611509"}
{"image_index": 336, "image_name": "000000036660.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000036660.jpg", "image_width": 500, "image_height": 323, "image_resolution": "500x323", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1627.327, "latencies_ms": [1627.327], "images_per_second": 0.615, "prompt_tokens": 21, "response_tokens_est": 35, "n_tiles": 6, "output_text": "The image depicts a wooden surface with a pair of black flip-flops placed on it. The flip-flops are adorned with white nail polish on the toes.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24963.6, "ram_available_mb": 100808.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 24964.3, "ram_available_mb": 100807.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.82, "peak": 15.24, "min": 14.3}, "VDD_CPU_SOC_MSS": {"avg": 15.29, "peak": 15.74, "min": 14.56}, "VDD_GPU": {"avg": 32.53, "peak": 40.56, "min": 24.82}, "VIN": {"avg": 73.19, "peak": 120.47, "min": 52.15}}, "power_watts_avg": 32.53, "energy_joules_est": 52.95, "sample_count": 12, "duration_seconds": 1.628}, "timestamp": "2026-01-17T15:35:06.244950"}
{"image_index": 336, "image_name": "000000036660.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000036660.jpg", "image_width": 500, "image_height": 323, "image_resolution": "500x323", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2108.216, "latencies_ms": [2108.216], "images_per_second": 0.474, "prompt_tokens": 19, "response_tokens_est": 52, "n_tiles": 6, "output_text": "The image shows a pair of black flip-flops with white painted toenails. The wooden surface on which the flip-flops are placed has a warm, reddish-brown hue, and the lighting is soft and natural, casting gentle shadows.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24964.3, "ram_available_mb": 100807.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 24964.3, "ram_available_mb": 100807.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.84, "peak": 15.24, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.43, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 30.58, "peak": 40.56, "min": 23.25}, "VIN": {"avg": 71.26, "peak": 110.69, "min": 61.91}}, "power_watts_avg": 30.58, "energy_joules_est": 64.48, "sample_count": 16, "duration_seconds": 2.109}, "timestamp": "2026-01-17T15:35:08.359310"}
{"image_index": 337, "image_name": "000000036678.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000036678.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1003.864, "latencies_ms": [1003.864], "images_per_second": 0.996, "prompt_tokens": 9, "response_tokens_est": 36, "n_tiles": 2, "output_text": "The image depicts the iconic Big Ben and the Houses of Parliament, illuminated against a cloudy sky, with a river in the foreground and a boat in the distance.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 24964.3, "ram_available_mb": 100807.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 24964.6, "ram_available_mb": 100807.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5055.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.62, "peak": 15.22, "min": 14.0}, "VDD_CPU_SOC_MSS": {"avg": 15.41, "peak": 16.14, "min": 14.57}, "VDD_GPU": {"avg": 26.05, "peak": 31.51, "min": 22.45}, "VIN": {"avg": 67.86, "peak": 108.39, "min": 55.38}}, "power_watts_avg": 26.05, "energy_joules_est": 26.17, "sample_count": 7, "duration_seconds": 1.005}, "timestamp": "2026-01-17T15:35:09.397658"}
{"image_index": 337, "image_name": "000000036678.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000036678.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1095.79, "latencies_ms": [1095.79], "images_per_second": 0.913, "prompt_tokens": 23, "response_tokens_est": 40, "n_tiles": 2, "output_text": "object: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24964.6, "ram_available_mb": 100807.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24964.6, "ram_available_mb": 100807.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5064.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.13, "peak": 15.52, "min": 14.61}, "VDD_CPU_SOC_MSS": {"avg": 15.9, "peak": 16.15, "min": 15.36}, "VDD_GPU": {"avg": 25.6, "peak": 31.11, "min": 22.06}, "VIN": {"avg": 68.07, "peak": 99.55, "min": 58.28}}, "power_watts_avg": 25.6, "energy_joules_est": 28.06, "sample_count": 8, "duration_seconds": 1.096}, "timestamp": "2026-01-17T15:35:10.499396"}
{"image_index": 337, "image_name": "000000036678.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000036678.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1411.703, "latencies_ms": [1411.703], "images_per_second": 0.708, "prompt_tokens": 27, "response_tokens_est": 52, "n_tiles": 2, "output_text": "The main objects in the image are a large building with a clock tower and a river with several boats. The building is in the background, while the river and boats are in the foreground. The clock tower is prominent and stands out against the skyline.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24964.6, "ram_available_mb": 100807.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 24964.8, "ram_available_mb": 100807.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5067.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.19, "peak": 15.52, "min": 14.71}, "VDD_CPU_SOC_MSS": {"avg": 15.89, "peak": 16.14, "min": 15.35}, "VDD_GPU": {"avg": 24.21, "peak": 30.73, "min": 20.88}, "VIN": {"avg": 67.07, "peak": 99.6, "min": 60.2}}, "power_watts_avg": 24.21, "energy_joules_est": 34.18, "sample_count": 11, "duration_seconds": 1.412}, "timestamp": "2026-01-17T15:35:11.917266"}
{"image_index": 337, "image_name": "000000036678.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000036678.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2085.582, "latencies_ms": [2085.582], "images_per_second": 0.479, "prompt_tokens": 21, "response_tokens_est": 81, "n_tiles": 2, "output_text": "The image depicts a serene riverside scene with a large, illuminated building, likely the Houses of Parliament, situated on the right side. The sky is overcast, and the water reflects the soft glow of the building's lights. In the foreground, there is a boat with the word \"PAGE\" visible on its side, suggesting it might be a tour or sightseeing vessel.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24964.8, "ram_available_mb": 100807.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 24965.1, "ram_available_mb": 100807.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5063.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.23, "peak": 15.42, "min": 14.81}, "VDD_CPU_SOC_MSS": {"avg": 16.17, "peak": 16.54, "min": 15.36}, "VDD_GPU": {"avg": 22.9, "peak": 29.94, "min": 20.48}, "VIN": {"avg": 66.23, "peak": 95.38, "min": 61.19}}, "power_watts_avg": 22.9, "energy_joules_est": 47.77, "sample_count": 16, "duration_seconds": 2.086}, "timestamp": "2026-01-17T15:35:14.008952"}
{"image_index": 337, "image_name": "000000036678.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000036678.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1225.739, "latencies_ms": [1225.739], "images_per_second": 0.816, "prompt_tokens": 19, "response_tokens_est": 44, "n_tiles": 2, "output_text": "The image showcases a grand, illuminated building with a clock tower, likely the Houses of Parliament, situated on a river. The sky is overcast, casting a soft, diffused light over the scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24965.1, "ram_available_mb": 100807.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 24964.8, "ram_available_mb": 100807.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5062.6, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.01, "peak": 15.42, "min": 14.51}, "VDD_CPU_SOC_MSS": {"avg": 15.88, "peak": 16.15, "min": 15.36}, "VDD_GPU": {"avg": 24.55, "peak": 29.94, "min": 21.27}, "VIN": {"avg": 66.84, "peak": 92.07, "min": 59.92}}, "power_watts_avg": 24.55, "energy_joules_est": 30.1, "sample_count": 9, "duration_seconds": 1.226}, "timestamp": "2026-01-17T15:35:15.240546"}
{"image_index": 338, "image_name": "000000036844.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000036844.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 3434.107, "latencies_ms": [3434.107], "images_per_second": 0.291, "prompt_tokens": 9, "response_tokens_est": 54, "n_tiles": 12, "output_text": "The image depicts a cozy living room with a mix of modern and vintage furniture, including a green couch, a red armchair, a black leather sofa, a small table with a television, a red rug, and various decorative items on the walls and shelves.", "error": null, "sys_before": {"cpu_percent": 45.5, "ram_used_mb": 24932.6, "ram_available_mb": 100839.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 24933.1, "ram_available_mb": 100839.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.22, "peak": 16.66, "min": 14.0}, "VDD_CPU_SOC_MSS": {"avg": 15.88, "peak": 17.71, "min": 14.96}, "VDD_GPU": {"avg": 33.53, "peak": 43.73, "min": 26.01}, "VIN": {"avg": 79.01, "peak": 118.17, "min": 57.58}}, "power_watts_avg": 33.53, "energy_joules_est": 115.15, "sample_count": 26, "duration_seconds": 3.434}, "timestamp": "2026-01-17T15:35:18.747777"}
{"image_index": 338, "image_name": "000000036844.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000036844.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 3327.977, "latencies_ms": [3327.977], "images_per_second": 0.3, "prompt_tokens": 23, "response_tokens_est": 49, "n_tiles": 12, "output_text": "1. Living room\n2. Sofa\n3. Coffee table\n4. TV stand\n5. TV\n6. Floor lamp\n7. Potted plants\n8. Chairs\n9. Floor\n10. Ceiling fan", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24933.1, "ram_available_mb": 100839.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 24934.3, "ram_available_mb": 100837.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.23, "peak": 16.56, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.72, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 34.74, "peak": 45.3, "min": 26.0}, "VIN": {"avg": 79.07, "peak": 120.78, "min": 58.27}}, "power_watts_avg": 34.74, "energy_joules_est": 115.62, "sample_count": 26, "duration_seconds": 3.328}, "timestamp": "2026-01-17T15:35:22.082690"}
{"image_index": 338, "image_name": "000000036844.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000036844.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 6035.822, "latencies_ms": [6035.822], "images_per_second": 0.166, "prompt_tokens": 27, "response_tokens_est": 129, "n_tiles": 12, "output_text": "The main objects in the image are arranged in a way that showcases a mix of furniture and decor. The left side of the image features a red leather chair, a small round table, and a bicycle leaning against the wall. The center of the image includes a green couch, a small table with a television, and a red rug. The right side of the image displays a black leather couch, a white coffee table, and a white refrigerator. The background includes a window with a view of trees outside, a white wall, and a ceiling fan. The foreground is dominated by the green couch and the red rug, while the background is", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24934.3, "ram_available_mb": 100837.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 24935.8, "ram_available_mb": 100836.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.16, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.63, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 30.76, "peak": 45.68, "min": 25.6}, "VIN": {"avg": 73.0, "peak": 124.75, "min": 62.0}}, "power_watts_avg": 30.76, "energy_joules_est": 185.67, "sample_count": 48, "duration_seconds": 6.036}, "timestamp": "2026-01-17T15:35:28.124433"}
{"image_index": 338, "image_name": "000000036844.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000036844.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 4252.237, "latencies_ms": [4252.237], "images_per_second": 0.235, "prompt_tokens": 21, "response_tokens_est": 76, "n_tiles": 12, "output_text": "The image depicts a cozy living room with a wooden floor and a red rug. The room is well-lit with natural light coming through large windows, and it contains various furniture pieces such as a green couch, a black leather chair, a small table, and a television. The space appears to be lived-in and well-organized, with personal items scattered around.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24935.8, "ram_available_mb": 100836.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 24936.1, "ram_available_mb": 100836.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.17, "peak": 16.66, "min": 14.12}, "VDD_CPU_SOC_MSS": {"avg": 15.68, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 32.66, "peak": 44.91, "min": 26.0}, "VIN": {"avg": 72.79, "peak": 91.72, "min": 57.86}}, "power_watts_avg": 32.66, "energy_joules_est": 138.89, "sample_count": 33, "duration_seconds": 4.253}, "timestamp": "2026-01-17T15:35:32.383133"}
{"image_index": 338, "image_name": "000000036844.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000036844.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3024.094, "latencies_ms": [3024.094], "images_per_second": 0.331, "prompt_tokens": 19, "response_tokens_est": 40, "n_tiles": 12, "output_text": "The room is well-lit with natural light streaming in through large windows, creating a bright and airy atmosphere. The wooden floor reflects the sunlight, adding to the overall brightness of the space.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 24936.1, "ram_available_mb": 100836.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 24936.3, "ram_available_mb": 100835.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.23, "peak": 16.56, "min": 14.12}, "VDD_CPU_SOC_MSS": {"avg": 15.78, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 35.48, "peak": 45.69, "min": 26.39}, "VIN": {"avg": 81.1, "peak": 137.48, "min": 63.0}}, "power_watts_avg": 35.48, "energy_joules_est": 107.32, "sample_count": 24, "duration_seconds": 3.025}, "timestamp": "2026-01-17T15:35:35.414304"}
{"image_index": 339, "image_name": "000000036861.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000036861.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2695.933, "latencies_ms": [2695.933], "images_per_second": 0.371, "prompt_tokens": 9, "response_tokens_est": 32, "n_tiles": 12, "output_text": "The image shows a parking meter on a sidewalk, with a large advertisement for \"YEAR OF SAVING\" displayed on a building in the background.", "error": null, "sys_before": {"cpu_percent": 42.6, "ram_used_mb": 24947.9, "ram_available_mb": 100824.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 24945.3, "ram_available_mb": 100826.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.18, "peak": 16.66, "min": 13.49}, "VDD_CPU_SOC_MSS": {"avg": 15.92, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 36.02, "peak": 44.49, "min": 27.18}, "VIN": {"avg": 80.67, "peak": 119.12, "min": 60.59}}, "power_watts_avg": 36.02, "energy_joules_est": 97.12, "sample_count": 20, "duration_seconds": 2.696}, "timestamp": "2026-01-17T15:35:38.210349"}
{"image_index": 339, "image_name": "000000036861.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000036861.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 6040.76, "latencies_ms": [6040.76], "images_per_second": 0.166, "prompt_tokens": 23, "response_tokens_est": 129, "n_tiles": 12, "output_text": "object: 1 parking meter\nobject: 1 parking meter\nobject: 1 parking meter\nobject: 1 parking meter\nobject: 1 parking meter\nobject: 1 parking meter\nobject: 1 parking meter\nobject: 1 parking meter\nobject: 1 parking meter\nobject: 1 parking meter\nobject: 1 parking meter\nobject: 1 parking meter\nobject: 1 parking meter\nobject: 1 parking meter\nobject: 1 parking meter\nobject: 1 parking meter\nobject: 1 parking meter\nobject: 1 parking meter\nobject:", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24945.3, "ram_available_mb": 100826.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 24940.5, "ram_available_mb": 100831.7, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.14, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.63, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 30.64, "peak": 45.28, "min": 26.0}, "VIN": {"avg": 71.91, "peak": 114.22, "min": 56.62}}, "power_watts_avg": 30.64, "energy_joules_est": 185.11, "sample_count": 47, "duration_seconds": 6.042}, "timestamp": "2026-01-17T15:35:44.257932"}
{"image_index": 339, "image_name": "000000036861.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000036861.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3780.029, "latencies_ms": [3780.029], "images_per_second": 0.265, "prompt_tokens": 27, "response_tokens_est": 62, "n_tiles": 12, "output_text": "The parking meter is positioned in the foreground, slightly to the left, and is the most prominent object in the image. The building in the background is slightly to the right and behind the parking meter. The street is in the middle ground, with a sidewalk and a curb separating it from the parking meter.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24940.5, "ram_available_mb": 100831.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 24939.9, "ram_available_mb": 100832.3, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.2, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.76, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 33.59, "peak": 44.89, "min": 26.0}, "VIN": {"avg": 78.63, "peak": 132.57, "min": 56.31}}, "power_watts_avg": 33.59, "energy_joules_est": 126.99, "sample_count": 29, "duration_seconds": 3.781}, "timestamp": "2026-01-17T15:35:48.044352"}
{"image_index": 339, "image_name": "000000036861.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000036861.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 4219.671, "latencies_ms": [4219.671], "images_per_second": 0.237, "prompt_tokens": 21, "response_tokens_est": 75, "n_tiles": 12, "output_text": "The image depicts a parking meter situated on a sidewalk in front of a building. The scene is set in an urban environment, likely in a city or town, with a clear sky and a building with large windows in the background. The parking meter is red and black, and it is positioned on the sidewalk, indicating that it is a common sight in the area.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24939.9, "ram_available_mb": 100832.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 24939.9, "ram_available_mb": 100832.3, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.17, "peak": 16.56, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.74, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 32.86, "peak": 45.68, "min": 26.0}, "VIN": {"avg": 74.28, "peak": 113.46, "min": 54.61}}, "power_watts_avg": 32.86, "energy_joules_est": 138.67, "sample_count": 33, "duration_seconds": 4.22}, "timestamp": "2026-01-17T15:35:52.270351"}
{"image_index": 339, "image_name": "000000036861.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000036861.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3024.595, "latencies_ms": [3024.595], "images_per_second": 0.331, "prompt_tokens": 19, "response_tokens_est": 40, "n_tiles": 12, "output_text": "The parking meter in the image is predominantly black with a digital display. It is mounted on a red post. The surrounding environment includes a clear blue sky, a brick building, and a sidewalk.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24939.9, "ram_available_mb": 100832.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 24939.9, "ram_available_mb": 100832.3, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.22, "peak": 16.56, "min": 14.12}, "VDD_CPU_SOC_MSS": {"avg": 15.86, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 35.37, "peak": 45.68, "min": 26.39}, "VIN": {"avg": 75.89, "peak": 108.72, "min": 60.28}}, "power_watts_avg": 35.37, "energy_joules_est": 107.01, "sample_count": 24, "duration_seconds": 3.025}, "timestamp": "2026-01-17T15:35:55.301712"}
{"image_index": 340, "image_name": "000000036936.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000036936.jpg", "image_width": 640, "image_height": 404, "image_resolution": "640x404", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1224.262, "latencies_ms": [1224.262], "images_per_second": 0.817, "prompt_tokens": 9, "response_tokens_est": 23, "n_tiles": 6, "output_text": "A group of people is gathered around a table, with a festive atmosphere, as they enjoy a holiday celebration.", "error": null, "sys_before": {"cpu_percent": 42.7, "ram_used_mb": 24948.1, "ram_available_mb": 100824.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 24949.6, "ram_available_mb": 100822.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.6, "peak": 15.04, "min": 13.82}, "VDD_CPU_SOC_MSS": {"avg": 15.35, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 35.19, "peak": 40.97, "min": 28.36}, "VIN": {"avg": 84.38, "peak": 118.26, "min": 55.28}}, "power_watts_avg": 35.19, "energy_joules_est": 43.1, "sample_count": 9, "duration_seconds": 1.225}, "timestamp": "2026-01-17T15:35:56.583859"}
{"image_index": 340, "image_name": "000000036936.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000036936.jpg", "image_width": 640, "image_height": 404, "image_resolution": "640x404", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 4151.732, "latencies_ms": [4151.732], "images_per_second": 0.241, "prompt_tokens": 23, "response_tokens_est": 129, "n_tiles": 6, "output_text": "- TV: 1\n- Candles: 2\n- Candle: 1\n- Candle holder: 1\n- Candle: 1\n- Candle holder: 1\n- Candle: 1\n- Candle holder: 1\n- Candle: 1\n- Candle holder: 1\n- Candle: 1\n- Candle holder: 1\n- Candle: 1\n- Candle holder: 1\n- Candle: 1\n- Candle holder: 1\n- Candle: 1\n- Candle", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24949.6, "ram_available_mb": 100822.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 24950.5, "ram_available_mb": 100821.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.11, "peak": 15.44, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.75, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 27.32, "peak": 40.95, "min": 23.24}, "VIN": {"avg": 69.1, "peak": 112.4, "min": 56.76}}, "power_watts_avg": 27.32, "energy_joules_est": 113.44, "sample_count": 32, "duration_seconds": 4.152}, "timestamp": "2026-01-17T15:36:00.742178"}
{"image_index": 340, "image_name": "000000036936.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000036936.jpg", "image_width": 640, "image_height": 404, "image_resolution": "640x404", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2677.291, "latencies_ms": [2677.291], "images_per_second": 0.374, "prompt_tokens": 27, "response_tokens_est": 72, "n_tiles": 6, "output_text": "The main objects in the image are a television, a couch, and a table. The television is positioned on the left side of the image, with a small shelf above it. The couch is in the background, and the table is in the foreground. The table holds various items such as a red gift, a candle, a phone, and snacks.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24950.5, "ram_available_mb": 100821.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 24950.3, "ram_available_mb": 100821.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.86, "peak": 15.14, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.52, "peak": 15.75, "min": 14.56}, "VDD_GPU": {"avg": 28.79, "peak": 40.97, "min": 22.86}, "VIN": {"avg": 68.8, "peak": 93.77, "min": 61.56}}, "power_watts_avg": 28.79, "energy_joules_est": 77.09, "sample_count": 21, "duration_seconds": 2.678}, "timestamp": "2026-01-17T15:36:03.425607"}
{"image_index": 340, "image_name": "000000036936.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000036936.jpg", "image_width": 640, "image_height": 404, "image_resolution": "640x404", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2613.098, "latencies_ms": [2613.098], "images_per_second": 0.383, "prompt_tokens": 21, "response_tokens_est": 70, "n_tiles": 6, "output_text": "The image depicts a cozy living room scene where a group of people is gathered around a table. The room is decorated with festive elements, including a Christmas tree, a red and gold Christmas wreath, and a lit candle on the table. The individuals appear to be engaged in a casual, social gathering, possibly celebrating the holiday season.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24950.3, "ram_available_mb": 100821.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 24950.5, "ram_available_mb": 100821.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.9, "peak": 15.24, "min": 14.3}, "VDD_CPU_SOC_MSS": {"avg": 15.43, "peak": 15.75, "min": 14.56}, "VDD_GPU": {"avg": 29.01, "peak": 40.95, "min": 22.85}, "VIN": {"avg": 73.71, "peak": 114.02, "min": 61.0}}, "power_watts_avg": 29.01, "energy_joules_est": 75.82, "sample_count": 20, "duration_seconds": 2.614}, "timestamp": "2026-01-17T15:36:06.044895"}
{"image_index": 340, "image_name": "000000036936.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000036936.jpg", "image_width": 640, "image_height": 404, "image_resolution": "640x404", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1599.19, "latencies_ms": [1599.19], "images_per_second": 0.625, "prompt_tokens": 19, "response_tokens_est": 34, "n_tiles": 6, "output_text": "The room is warmly lit with a soft glow, creating a cozy atmosphere. The furniture and decorations are made of wood, adding a rustic touch to the space.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24950.5, "ram_available_mb": 100821.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 24950.8, "ram_available_mb": 100821.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.81, "peak": 15.24, "min": 14.3}, "VDD_CPU_SOC_MSS": {"avg": 15.35, "peak": 15.75, "min": 14.56}, "VDD_GPU": {"avg": 32.62, "peak": 40.56, "min": 24.82}, "VIN": {"avg": 77.58, "peak": 107.3, "min": 62.57}}, "power_watts_avg": 32.62, "energy_joules_est": 52.18, "sample_count": 12, "duration_seconds": 1.6}, "timestamp": "2026-01-17T15:36:07.650166"}
{"image_index": 341, "image_name": "000000037670.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000037670.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2658.637, "latencies_ms": [2658.637], "images_per_second": 0.376, "prompt_tokens": 9, "response_tokens_est": 31, "n_tiles": 12, "output_text": "A person is holding a remote control in their hand, which is connected to a modern, sleek, white appliance with multiple buttons and a digital display.", "error": null, "sys_before": {"cpu_percent": 49.2, "ram_used_mb": 24920.9, "ram_available_mb": 100851.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 24922.1, "ram_available_mb": 100850.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.97, "peak": 16.46, "min": 13.49}, "VDD_CPU_SOC_MSS": {"avg": 15.71, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 35.9, "peak": 44.1, "min": 26.79}, "VIN": {"avg": 83.27, "peak": 139.09, "min": 55.29}}, "power_watts_avg": 35.9, "energy_joules_est": 95.46, "sample_count": 20, "duration_seconds": 2.659}, "timestamp": "2026-01-17T15:36:10.404574"}
{"image_index": 341, "image_name": "000000037670.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000037670.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 3035.416, "latencies_ms": [3035.416], "images_per_second": 0.329, "prompt_tokens": 23, "response_tokens_est": 40, "n_tiles": 12, "output_text": "1. Remote control\n2. Wall outlet\n3. Wall switch\n4. Wall switch\n5. Wall switch\n6. Wall switch\n7. Wall switch\n8. Wall switch", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24922.1, "ram_available_mb": 100850.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 24923.1, "ram_available_mb": 100849.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.95, "peak": 16.36, "min": 13.92}, "VDD_CPU_SOC_MSS": {"avg": 15.54, "peak": 17.32, "min": 14.17}, "VDD_GPU": {"avg": 35.86, "peak": 45.3, "min": 26.39}, "VIN": {"avg": 77.17, "peak": 119.65, "min": 56.43}}, "power_watts_avg": 35.86, "energy_joules_est": 108.87, "sample_count": 23, "duration_seconds": 3.036}, "timestamp": "2026-01-17T15:36:13.446418"}
{"image_index": 341, "image_name": "000000037670.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000037670.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 4460.387, "latencies_ms": [4460.387], "images_per_second": 0.224, "prompt_tokens": 27, "response_tokens_est": 81, "n_tiles": 12, "output_text": "The main object in the foreground is a white appliance, likely a washing machine, with a control panel on its front. The control panel has several buttons and a display screen. The appliance is placed on a wooden floor, and there is a white door or cabinet to the left of the appliance. The background is mostly white, with a plain wall and a small section of a door or cabinet visible.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 24923.1, "ram_available_mb": 100849.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 24924.1, "ram_available_mb": 100848.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.86, "peak": 16.36, "min": 13.9}, "VDD_CPU_SOC_MSS": {"avg": 15.31, "peak": 17.32, "min": 14.17}, "VDD_GPU": {"avg": 32.37, "peak": 46.09, "min": 25.6}, "VIN": {"avg": 73.63, "peak": 127.97, "min": 59.63}}, "power_watts_avg": 32.37, "energy_joules_est": 144.41, "sample_count": 35, "duration_seconds": 4.461}, "timestamp": "2026-01-17T15:36:17.914801"}
{"image_index": 341, "image_name": "000000037670.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000037670.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 4214.584, "latencies_ms": [4214.584], "images_per_second": 0.237, "prompt_tokens": 21, "response_tokens_est": 74, "n_tiles": 12, "output_text": "The image depicts a modern, minimalist bathroom setting with a focus on a sleek, white wall-mounted toilet. A person's hand is seen holding a small, rectangular object, possibly a remote control or a small device, which is being used to operate the toilet. The overall scene is clean, uncluttered, and designed with a contemporary aesthetic.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24924.1, "ram_available_mb": 100848.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 24924.6, "ram_available_mb": 100847.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.88, "peak": 16.36, "min": 13.92}, "VDD_CPU_SOC_MSS": {"avg": 15.32, "peak": 17.32, "min": 14.17}, "VDD_GPU": {"avg": 32.5, "peak": 45.69, "min": 25.6}, "VIN": {"avg": 73.15, "peak": 101.73, "min": 54.12}}, "power_watts_avg": 32.5, "energy_joules_est": 136.98, "sample_count": 33, "duration_seconds": 4.215}, "timestamp": "2026-01-17T15:36:22.135660"}
{"image_index": 341, "image_name": "000000037670.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000037670.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3549.593, "latencies_ms": [3549.593], "images_per_second": 0.282, "prompt_tokens": 19, "response_tokens_est": 55, "n_tiles": 12, "output_text": "The image features a modern, minimalist bathroom with a sleek, white wall-mounted toilet. The toilet has a sleek, metallic design with a circular base and a small, rectangular control panel. The lighting is soft and ambient, creating a clean and serene atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24924.6, "ram_available_mb": 100847.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 24924.6, "ram_available_mb": 100847.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.92, "peak": 16.36, "min": 13.92}, "VDD_CPU_SOC_MSS": {"avg": 15.5, "peak": 17.32, "min": 14.17}, "VDD_GPU": {"avg": 34.12, "peak": 44.91, "min": 25.6}, "VIN": {"avg": 77.17, "peak": 120.84, "min": 56.4}}, "power_watts_avg": 34.12, "energy_joules_est": 121.13, "sample_count": 27, "duration_seconds": 3.55}, "timestamp": "2026-01-17T15:36:25.695651"}
{"image_index": 342, "image_name": "000000037689.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000037689.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1199.103, "latencies_ms": [1199.103], "images_per_second": 0.834, "prompt_tokens": 9, "response_tokens_est": 22, "n_tiles": 6, "output_text": "A snowboarder is performing a jump over a snow wall, with spectators watching from the side.", "error": null, "sys_before": {"cpu_percent": 38.2, "ram_used_mb": 24941.0, "ram_available_mb": 100831.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 24943.0, "ram_available_mb": 100829.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.53, "peak": 15.04, "min": 13.61}, "VDD_CPU_SOC_MSS": {"avg": 15.22, "peak": 16.14, "min": 14.17}, "VDD_GPU": {"avg": 34.75, "peak": 40.97, "min": 27.97}, "VIN": {"avg": 74.17, "peak": 107.12, "min": 63.54}}, "power_watts_avg": 34.75, "energy_joules_est": 41.69, "sample_count": 9, "duration_seconds": 1.2}, "timestamp": "2026-01-17T15:36:26.956894"}
{"image_index": 342, "image_name": "000000037689.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000037689.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1732.32, "latencies_ms": [1732.32], "images_per_second": 0.577, "prompt_tokens": 23, "response_tokens_est": 40, "n_tiles": 6, "output_text": "object: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24943.0, "ram_available_mb": 100829.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 24943.7, "ram_available_mb": 100828.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.97, "peak": 15.44, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.59, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 32.81, "peak": 41.34, "min": 24.82}, "VIN": {"avg": 72.07, "peak": 111.76, "min": 55.21}}, "power_watts_avg": 32.81, "energy_joules_est": 56.85, "sample_count": 13, "duration_seconds": 1.733}, "timestamp": "2026-01-17T15:36:28.696037"}
{"image_index": 342, "image_name": "000000037689.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000037689.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2475.24, "latencies_ms": [2475.24], "images_per_second": 0.404, "prompt_tokens": 27, "response_tokens_est": 65, "n_tiles": 6, "output_text": "The main objects in the image are a snowboarder performing a jump and a snowy mountain slope. The snowboarder is in the foreground, while the snowy mountain slope is in the background. The snowboarder is near the snowy slope, and the snowboard is also near the snowy slope.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24943.7, "ram_available_mb": 100828.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 24944.7, "ram_available_mb": 100827.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.91, "peak": 15.24, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.44, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 29.67, "peak": 41.34, "min": 23.24}, "VIN": {"avg": 71.54, "peak": 113.07, "min": 61.88}}, "power_watts_avg": 29.67, "energy_joules_est": 73.45, "sample_count": 19, "duration_seconds": 2.476}, "timestamp": "2026-01-17T15:36:31.177061"}
{"image_index": 342, "image_name": "000000037689.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000037689.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2276.418, "latencies_ms": [2276.418], "images_per_second": 0.439, "prompt_tokens": 21, "response_tokens_est": 58, "n_tiles": 6, "output_text": "The image depicts a snowboarding competition taking place on a snowy mountain slope. Spectators are gathered on the side of the slope, watching the snowboarders perform tricks. The snowboarders are airborne, executing impressive maneuvers, while the crowd cheers them on.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24944.7, "ram_available_mb": 100827.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 24945.4, "ram_available_mb": 100826.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.88, "peak": 15.34, "min": 14.3}, "VDD_CPU_SOC_MSS": {"avg": 15.45, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 29.82, "peak": 40.16, "min": 23.25}, "VIN": {"avg": 70.0, "peak": 118.24, "min": 61.54}}, "power_watts_avg": 29.82, "energy_joules_est": 67.9, "sample_count": 17, "duration_seconds": 2.277}, "timestamp": "2026-01-17T15:36:33.459760"}
{"image_index": 342, "image_name": "000000037689.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000037689.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1681.06, "latencies_ms": [1681.06], "images_per_second": 0.595, "prompt_tokens": 19, "response_tokens_est": 37, "n_tiles": 6, "output_text": "The image depicts a snowy landscape with a clear blue sky, indicating a sunny day. The snow appears to be freshly fallen, with a few tracks visible on the ground.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24945.4, "ram_available_mb": 100826.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 24945.7, "ram_available_mb": 100826.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.84, "peak": 15.24, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.41, "peak": 15.75, "min": 14.56}, "VDD_GPU": {"avg": 32.03, "peak": 40.56, "min": 24.42}, "VIN": {"avg": 74.07, "peak": 111.59, "min": 59.66}}, "power_watts_avg": 32.03, "energy_joules_est": 53.85, "sample_count": 13, "duration_seconds": 1.681}, "timestamp": "2026-01-17T15:36:35.146779"}
{"image_index": 343, "image_name": "000000037740.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000037740.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2794.436, "latencies_ms": [2794.436], "images_per_second": 0.358, "prompt_tokens": 9, "response_tokens_est": 35, "n_tiles": 12, "output_text": "The image shows a well-organized home office with a desk, computer, and a chair, along with a potted plant and a white couch in the background.", "error": null, "sys_before": {"cpu_percent": 46.7, "ram_used_mb": 24925.3, "ram_available_mb": 100846.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 24927.0, "ram_available_mb": 100845.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.16, "peak": 16.66, "min": 13.39}, "VDD_CPU_SOC_MSS": {"avg": 15.95, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 35.24, "peak": 43.71, "min": 26.79}, "VIN": {"avg": 76.05, "peak": 100.01, "min": 61.13}}, "power_watts_avg": 35.24, "energy_joules_est": 98.49, "sample_count": 21, "duration_seconds": 2.795}, "timestamp": "2026-01-17T15:36:38.040685"}
{"image_index": 343, "image_name": "000000037740.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000037740.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2822.804, "latencies_ms": [2822.804], "images_per_second": 0.354, "prompt_tokens": 23, "response_tokens_est": 34, "n_tiles": 12, "output_text": "1. Desk\n2. Chair\n3. Computer monitor\n4. Laptop\n5. Keyboard\n6. Mouse\n7. Wall outlet\n8. Floor", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24927.0, "ram_available_mb": 100845.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 24928.3, "ram_available_mb": 100843.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.24, "peak": 16.56, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.89, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 36.25, "peak": 45.28, "min": 26.79}, "VIN": {"avg": 78.65, "peak": 117.32, "min": 62.54}}, "power_watts_avg": 36.25, "energy_joules_est": 102.34, "sample_count": 22, "duration_seconds": 2.823}, "timestamp": "2026-01-17T15:36:40.870643"}
{"image_index": 343, "image_name": "000000037740.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000037740.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 4524.103, "latencies_ms": [4524.103], "images_per_second": 0.221, "prompt_tokens": 27, "response_tokens_est": 84, "n_tiles": 12, "output_text": "The main objects in the image are a desk, a chair, and a potted plant. The desk is positioned in the foreground, with a computer monitor, a keyboard, and a mouse on it. The chair is positioned to the left of the desk, and the potted plant is situated to the right of the desk. The plant is placed on a small white tray, which is placed on the floor.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24928.3, "ram_available_mb": 100843.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 24928.3, "ram_available_mb": 100843.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.17, "peak": 16.56, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.65, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 32.45, "peak": 45.68, "min": 26.0}, "VIN": {"avg": 74.79, "peak": 127.12, "min": 55.94}}, "power_watts_avg": 32.45, "energy_joules_est": 146.82, "sample_count": 35, "duration_seconds": 4.524}, "timestamp": "2026-01-17T15:36:45.400956"}
{"image_index": 343, "image_name": "000000037740.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000037740.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3939.252, "latencies_ms": [3939.252], "images_per_second": 0.254, "prompt_tokens": 21, "response_tokens_est": 67, "n_tiles": 12, "output_text": "The image depicts a cozy, well-lit room with a modern, minimalist aesthetic. The room features a small desk with a computer setup, a comfortable white chair, and a potted plant adding a touch of greenery. The overall atmosphere is serene and inviting, with soft lighting and a clean, organized space.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24928.3, "ram_available_mb": 100843.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 24928.5, "ram_available_mb": 100843.7, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.21, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.72, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 33.4, "peak": 45.28, "min": 26.0}, "VIN": {"avg": 78.57, "peak": 126.75, "min": 58.1}}, "power_watts_avg": 33.4, "energy_joules_est": 131.58, "sample_count": 30, "duration_seconds": 3.94}, "timestamp": "2026-01-17T15:36:49.346663"}
{"image_index": 343, "image_name": "000000037740.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000037740.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3158.688, "latencies_ms": [3158.688], "images_per_second": 0.317, "prompt_tokens": 19, "response_tokens_est": 44, "n_tiles": 12, "output_text": "The room is well-lit with a warm, yellowish glow from a lamp on the desk, creating a cozy atmosphere. The carpet is light beige, complementing the overall neutral color scheme of the room.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24928.5, "ram_available_mb": 100843.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 24929.5, "ram_available_mb": 100842.7, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.22, "peak": 16.56, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.76, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 35.1, "peak": 45.68, "min": 26.39}, "VIN": {"avg": 76.8, "peak": 111.97, "min": 50.47}}, "power_watts_avg": 35.1, "energy_joules_est": 110.88, "sample_count": 25, "duration_seconds": 3.159}, "timestamp": "2026-01-17T15:36:52.511954"}
{"image_index": 344, "image_name": "000000037751.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000037751.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2695.834, "latencies_ms": [2695.834], "images_per_second": 0.371, "prompt_tokens": 9, "response_tokens_est": 32, "n_tiles": 12, "output_text": "A person is riding a motorcycle on a rocky, mountainous road, with a scenic view of green hills and a partly cloudy sky in the background.", "error": null, "sys_before": {"cpu_percent": 39.6, "ram_used_mb": 24932.9, "ram_available_mb": 100839.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 24932.6, "ram_available_mb": 100839.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.16, "peak": 16.76, "min": 13.39}, "VDD_CPU_SOC_MSS": {"avg": 15.96, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 35.88, "peak": 44.12, "min": 27.18}, "VIN": {"avg": 80.67, "peak": 114.02, "min": 65.82}}, "power_watts_avg": 35.88, "energy_joules_est": 96.74, "sample_count": 20, "duration_seconds": 2.696}, "timestamp": "2026-01-17T15:36:55.314892"}
{"image_index": 344, "image_name": "000000037751.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000037751.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2957.733, "latencies_ms": [2957.733], "images_per_second": 0.338, "prompt_tokens": 23, "response_tokens_est": 38, "n_tiles": 12, "output_text": "1. Motorcycle\n2. Helmet\n3. Rider\n4. Gloves\n5. Backpack\n6. Motorcycle seat\n7. Road\n8. Hills", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24932.6, "ram_available_mb": 100839.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 24930.5, "ram_available_mb": 100841.7, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.26, "peak": 16.66, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.87, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 35.76, "peak": 45.68, "min": 26.39}, "VIN": {"avg": 79.65, "peak": 115.9, "min": 62.55}}, "power_watts_avg": 35.76, "energy_joules_est": 105.79, "sample_count": 23, "duration_seconds": 2.958}, "timestamp": "2026-01-17T15:36:58.281651"}
{"image_index": 344, "image_name": "000000037751.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000037751.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3576.848, "latencies_ms": [3576.848], "images_per_second": 0.28, "prompt_tokens": 27, "response_tokens_est": 55, "n_tiles": 12, "output_text": "The main objects in the image are a person on a motorcycle and a mountainous landscape. The person is positioned in the foreground, riding the motorcycle on a rocky, uneven terrain. The mountainous landscape is in the background, stretching out with rolling hills and dense forests.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24930.5, "ram_available_mb": 100841.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 24930.4, "ram_available_mb": 100841.7, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.08, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.59, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 33.87, "peak": 44.89, "min": 25.6}, "VIN": {"avg": 75.28, "peak": 118.57, "min": 56.22}}, "power_watts_avg": 33.87, "energy_joules_est": 121.16, "sample_count": 27, "duration_seconds": 3.577}, "timestamp": "2026-01-17T15:37:01.864756"}
{"image_index": 344, "image_name": "000000037751.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000037751.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 4069.586, "latencies_ms": [4069.586], "images_per_second": 0.246, "prompt_tokens": 21, "response_tokens_est": 70, "n_tiles": 12, "output_text": "The image depicts a rugged, mountainous landscape with a dirt road winding through the terrain. A person is riding a motorcycle, wearing a helmet and a jacket, and appears to be enjoying the scenic ride. The setting is characterized by rocky paths, greenery, and a partly cloudy sky, creating a picturesque and adventurous atmosphere.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24930.4, "ram_available_mb": 100841.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 24930.7, "ram_available_mb": 100841.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.9, "peak": 16.36, "min": 13.92}, "VDD_CPU_SOC_MSS": {"avg": 15.4, "peak": 17.32, "min": 14.17}, "VDD_GPU": {"avg": 33.15, "peak": 45.69, "min": 25.6}, "VIN": {"avg": 72.64, "peak": 115.23, "min": 64.07}}, "power_watts_avg": 33.15, "energy_joules_est": 134.92, "sample_count": 31, "duration_seconds": 4.07}, "timestamp": "2026-01-17T15:37:05.940954"}
{"image_index": 344, "image_name": "000000037751.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000037751.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3509.176, "latencies_ms": [3509.176], "images_per_second": 0.285, "prompt_tokens": 19, "response_tokens_est": 54, "n_tiles": 12, "output_text": "The image depicts a rugged, rocky terrain with a clear blue sky overhead. The lighting is bright and natural, suggesting it is daytime. The scene is characterized by a mix of greenery and rocky surfaces, with a few scattered clouds adding to the overall atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24930.7, "ram_available_mb": 100841.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 24930.7, "ram_available_mb": 100841.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.94, "peak": 16.36, "min": 13.92}, "VDD_CPU_SOC_MSS": {"avg": 15.48, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 34.06, "peak": 44.91, "min": 25.61}, "VIN": {"avg": 76.12, "peak": 125.14, "min": 59.11}}, "power_watts_avg": 34.06, "energy_joules_est": 119.53, "sample_count": 27, "duration_seconds": 3.509}, "timestamp": "2026-01-17T15:37:09.456617"}
{"image_index": 345, "image_name": "000000037777.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000037777.jpg", "image_width": 352, "image_height": 230, "image_resolution": "352x230", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1490.414, "latencies_ms": [1490.414], "images_per_second": 0.671, "prompt_tokens": 9, "response_tokens_est": 33, "n_tiles": 6, "output_text": "The image depicts a modern kitchen with wooden cabinets, a white refrigerator, a stove, and a countertop with a bowl of oranges on a table.", "error": null, "sys_before": {"cpu_percent": 42.7, "ram_used_mb": 24942.4, "ram_available_mb": 100829.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 24944.3, "ram_available_mb": 100827.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.65, "peak": 15.11, "min": 13.61}, "VDD_CPU_SOC_MSS": {"avg": 15.35, "peak": 16.14, "min": 14.17}, "VDD_GPU": {"avg": 33.3, "peak": 40.95, "min": 26.0}, "VIN": {"avg": 81.06, "peak": 113.48, "min": 64.31}}, "power_watts_avg": 33.3, "energy_joules_est": 49.65, "sample_count": 11, "duration_seconds": 1.491}, "timestamp": "2026-01-17T15:37:11.006238"}
{"image_index": 345, "image_name": "000000037777.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000037777.jpg", "image_width": 352, "image_height": 230, "image_resolution": "352x230", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1782.795, "latencies_ms": [1782.795], "images_per_second": 0.561, "prompt_tokens": 23, "response_tokens_est": 42, "n_tiles": 6, "output_text": "kitchen: 2\ncabinets: 2\ndishwasher: 1\noven: 1\nfridge: 1\ntable: 1\nfruit bowl: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24944.3, "ram_available_mb": 100827.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 24944.3, "ram_available_mb": 100827.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.03, "peak": 15.44, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.6, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 32.51, "peak": 40.57, "min": 24.82}, "VIN": {"avg": 73.76, "peak": 103.67, "min": 57.52}}, "power_watts_avg": 32.51, "energy_joules_est": 57.97, "sample_count": 13, "duration_seconds": 1.783}, "timestamp": "2026-01-17T15:37:12.795414"}
{"image_index": 345, "image_name": "000000037777.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000037777.jpg", "image_width": 352, "image_height": 230, "image_resolution": "352x230", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2110.052, "latencies_ms": [2110.052], "images_per_second": 0.474, "prompt_tokens": 27, "response_tokens_est": 52, "n_tiles": 6, "output_text": "The main objects in the image are a kitchen and a dining area. The kitchen is located in the background, while the dining area is in the foreground. The dining area has a table with fruit on it, and there is a refrigerator in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24944.3, "ram_available_mb": 100827.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 24945.3, "ram_available_mb": 100826.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.89, "peak": 15.34, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.48, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 30.61, "peak": 40.57, "min": 23.25}, "VIN": {"avg": 70.84, "peak": 92.82, "min": 59.11}}, "power_watts_avg": 30.61, "energy_joules_est": 64.6, "sample_count": 16, "duration_seconds": 2.11}, "timestamp": "2026-01-17T15:37:14.913508"}
{"image_index": 345, "image_name": "000000037777.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000037777.jpg", "image_width": 352, "image_height": 230, "image_resolution": "352x230", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2307.515, "latencies_ms": [2307.515], "images_per_second": 0.433, "prompt_tokens": 21, "response_tokens_est": 59, "n_tiles": 6, "output_text": "The image depicts a modern kitchen with a clean and bright ambiance. The kitchen features wooden cabinets, a white refrigerator, and a countertop with various kitchen appliances and utensils. There is a dining table with a bowl of oranges on it, suggesting a casual dining area.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24945.3, "ram_available_mb": 100826.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 24945.3, "ram_available_mb": 100826.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.86, "peak": 15.34, "min": 14.2}, "VDD_CPU_SOC_MSS": {"avg": 15.49, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 29.82, "peak": 40.16, "min": 23.24}, "VIN": {"avg": 69.53, "peak": 114.08, "min": 53.39}}, "power_watts_avg": 29.82, "energy_joules_est": 68.82, "sample_count": 17, "duration_seconds": 2.308}, "timestamp": "2026-01-17T15:37:17.227367"}
{"image_index": 345, "image_name": "000000037777.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000037777.jpg", "image_width": 352, "image_height": 230, "image_resolution": "352x230", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2250.771, "latencies_ms": [2250.771], "images_per_second": 0.444, "prompt_tokens": 19, "response_tokens_est": 57, "n_tiles": 6, "output_text": "The kitchen features a warm, wooden cabinetry with a light beige wall color. The lighting is soft and natural, coming from a ceiling light and a window. The table is set with a bowl of oranges and bananas, adding a pop of color to the scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24945.3, "ram_available_mb": 100826.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 24945.8, "ram_available_mb": 100826.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.89, "peak": 15.34, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.45, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 29.84, "peak": 40.16, "min": 23.24}, "VIN": {"avg": 71.62, "peak": 116.07, "min": 58.81}}, "power_watts_avg": 29.84, "energy_joules_est": 67.17, "sample_count": 17, "duration_seconds": 2.251}, "timestamp": "2026-01-17T15:37:19.483982"}
{"image_index": 346, "image_name": "000000037988.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000037988.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2389.257, "latencies_ms": [2389.257], "images_per_second": 0.419, "prompt_tokens": 9, "response_tokens_est": 23, "n_tiles": 12, "output_text": "A female tennis player is in the middle of a match, preparing to hit the ball with her racket.", "error": null, "sys_before": {"cpu_percent": 44.4, "ram_used_mb": 24926.7, "ram_available_mb": 100845.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.4, "ram_used_mb": 24927.9, "ram_available_mb": 100844.3, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.22, "peak": 16.66, "min": 13.59}, "VDD_CPU_SOC_MSS": {"avg": 15.98, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 36.65, "peak": 43.71, "min": 28.36}, "VIN": {"avg": 84.38, "peak": 125.34, "min": 65.47}}, "power_watts_avg": 36.65, "energy_joules_est": 87.58, "sample_count": 18, "duration_seconds": 2.39}, "timestamp": "2026-01-17T15:37:21.957241"}
{"image_index": 346, "image_name": "000000037988.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000037988.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 3254.875, "latencies_ms": [3254.875], "images_per_second": 0.307, "prompt_tokens": 23, "response_tokens_est": 47, "n_tiles": 12, "output_text": "1. Tennis player\n2. Tennis racket\n3. Tennis ball\n4. Tennis court\n5. Blue sign with \"POLO\" text\n6. Blue wall\n7. Green floor\n8. White cap", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 24927.9, "ram_available_mb": 100844.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 24929.1, "ram_available_mb": 100843.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.27, "peak": 16.66, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.78, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 35.08, "peak": 45.28, "min": 26.39}, "VIN": {"avg": 81.42, "peak": 120.33, "min": 56.64}}, "power_watts_avg": 35.08, "energy_joules_est": 114.19, "sample_count": 25, "duration_seconds": 3.255}, "timestamp": "2026-01-17T15:37:25.218587"}
{"image_index": 346, "image_name": "000000037988.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000037988.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 4493.557, "latencies_ms": [4493.557], "images_per_second": 0.223, "prompt_tokens": 27, "response_tokens_est": 83, "n_tiles": 12, "output_text": "The main object in the foreground is a tennis player, standing on the court. The player is holding a tennis racket and appears to be preparing to hit a tennis ball. The background features a blue barrier with the word \"POLO\" written on it, and a microphone is visible on the left side. The player is positioned near the center of the image, with the barrier and microphone in the background.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24929.1, "ram_available_mb": 100843.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 24929.1, "ram_available_mb": 100843.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.17, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.65, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 32.39, "peak": 45.28, "min": 26.0}, "VIN": {"avg": 76.8, "peak": 125.47, "min": 65.08}}, "power_watts_avg": 32.39, "energy_joules_est": 145.56, "sample_count": 35, "duration_seconds": 4.494}, "timestamp": "2026-01-17T15:37:29.718624"}
{"image_index": 346, "image_name": "000000037988.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000037988.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 5403.989, "latencies_ms": [5403.989], "images_per_second": 0.185, "prompt_tokens": 21, "response_tokens_est": 110, "n_tiles": 12, "output_text": "The image depicts a scene on a tennis court, where a female tennis player is in the midst of a serve. The player is dressed in a white outfit and is positioned on the right side of the frame, with her racket raised above her head, ready to strike the tennis ball. The court is marked with white lines, and there is a blue sign with the word \"POLO\" on it, indicating the brand of the court. The setting appears to be a professional tennis match, with the player focused on executing her serve.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 24929.1, "ram_available_mb": 100843.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 24929.6, "ram_available_mb": 100842.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.18, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.65, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 31.26, "peak": 44.91, "min": 26.0}, "VIN": {"avg": 74.16, "peak": 113.44, "min": 56.38}}, "power_watts_avg": 31.26, "energy_joules_est": 168.94, "sample_count": 42, "duration_seconds": 5.404}, "timestamp": "2026-01-17T15:37:35.129731"}
{"image_index": 346, "image_name": "000000037988.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000037988.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3772.041, "latencies_ms": [3772.041], "images_per_second": 0.265, "prompt_tokens": 19, "response_tokens_est": 62, "n_tiles": 12, "output_text": "The image depicts a tennis player on a green court, wearing a white cap and a white sleeveless top. The court surface is marked with white lines, and there is a blue sign with the word \"POLO\" on it. The lighting is bright, and the weather appears to be clear.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24929.6, "ram_available_mb": 100842.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 24930.3, "ram_available_mb": 100841.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.2, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.76, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 33.63, "peak": 45.28, "min": 26.0}, "VIN": {"avg": 79.2, "peak": 124.63, "min": 57.24}}, "power_watts_avg": 33.63, "energy_joules_est": 126.87, "sample_count": 29, "duration_seconds": 3.772}, "timestamp": "2026-01-17T15:37:38.907676"}
{"image_index": 347, "image_name": "000000038048.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038048.jpg", "image_width": 299, "image_height": 500, "image_resolution": "299x500", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1093.91, "latencies_ms": [1093.91], "images_per_second": 0.914, "prompt_tokens": 9, "response_tokens_est": 18, "n_tiles": 6, "output_text": "A red fire hydrant is placed on the sidewalk, with a person standing nearby.", "error": null, "sys_before": {"cpu_percent": 43.5, "ram_used_mb": 24942.5, "ram_available_mb": 100829.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 24942.7, "ram_available_mb": 100829.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.54, "peak": 15.14, "min": 13.72}, "VDD_CPU_SOC_MSS": {"avg": 15.2, "peak": 16.14, "min": 14.17}, "VDD_GPU": {"avg": 35.5, "peak": 40.56, "min": 29.54}, "VIN": {"avg": 78.25, "peak": 114.39, "min": 60.36}}, "power_watts_avg": 35.5, "energy_joules_est": 38.85, "sample_count": 8, "duration_seconds": 1.094}, "timestamp": "2026-01-17T15:37:40.064683"}
{"image_index": 347, "image_name": "000000038048.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038048.jpg", "image_width": 299, "image_height": 500, "image_resolution": "299x500", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1395.422, "latencies_ms": [1395.422], "images_per_second": 0.717, "prompt_tokens": 23, "response_tokens_est": 27, "n_tiles": 6, "output_text": "fire hydrant: 1\nsign: 1\ntree: 1\nhouse: 1\nman: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24942.7, "ram_available_mb": 100829.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 24943.4, "ram_available_mb": 100828.7, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.83, "peak": 15.24, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.47, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 35.53, "peak": 41.74, "min": 27.18}, "VIN": {"avg": 79.14, "peak": 122.17, "min": 60.61}}, "power_watts_avg": 35.53, "energy_joules_est": 49.59, "sample_count": 10, "duration_seconds": 1.396}, "timestamp": "2026-01-17T15:37:41.466050"}
{"image_index": 347, "image_name": "000000038048.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038048.jpg", "image_width": 299, "image_height": 500, "image_resolution": "299x500", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2036.702, "latencies_ms": [2036.702], "images_per_second": 0.491, "prompt_tokens": 27, "response_tokens_est": 50, "n_tiles": 6, "output_text": "The main object in the foreground is a red fire hydrant, which is positioned on the sidewalk. In the background, there is a person standing near the sidewalk. The fire hydrant is near the sidewalk, while the person is further away.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24943.4, "ram_available_mb": 100828.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 24944.4, "ram_available_mb": 100827.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.92, "peak": 15.34, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.62, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 31.46, "peak": 40.95, "min": 23.64}, "VIN": {"avg": 71.29, "peak": 106.2, "min": 60.99}}, "power_watts_avg": 31.46, "energy_joules_est": 64.1, "sample_count": 15, "duration_seconds": 2.037}, "timestamp": "2026-01-17T15:37:43.513433"}
{"image_index": 347, "image_name": "000000038048.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038048.jpg", "image_width": 299, "image_height": 500, "image_resolution": "299x500", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2071.179, "latencies_ms": [2071.179], "images_per_second": 0.483, "prompt_tokens": 21, "response_tokens_est": 51, "n_tiles": 6, "output_text": "The image depicts a scene on a sunny day with a red fire hydrant situated on a sidewalk. In the background, there is a person standing near a building, and the area appears to be a quiet urban street with green trees and buildings.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24944.4, "ram_available_mb": 100827.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 24945.4, "ram_available_mb": 100826.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.88, "peak": 15.24, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.49, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 30.86, "peak": 40.18, "min": 23.64}, "VIN": {"avg": 71.28, "peak": 113.61, "min": 52.04}}, "power_watts_avg": 30.86, "energy_joules_est": 63.93, "sample_count": 15, "duration_seconds": 2.072}, "timestamp": "2026-01-17T15:37:45.590849"}
{"image_index": 347, "image_name": "000000038048.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038048.jpg", "image_width": 299, "image_height": 500, "image_resolution": "299x500", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1732.36, "latencies_ms": [1732.36], "images_per_second": 0.577, "prompt_tokens": 19, "response_tokens_est": 39, "n_tiles": 6, "output_text": "The image features a bright red fire hydrant on a concrete sidewalk, with a clear blue sky overhead. The hydrant is illuminated by natural sunlight, casting a shadow on the ground.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24945.4, "ram_available_mb": 100826.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 24945.4, "ram_available_mb": 100826.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.88, "peak": 15.34, "min": 14.3}, "VDD_CPU_SOC_MSS": {"avg": 15.5, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 31.85, "peak": 40.56, "min": 24.42}, "VIN": {"avg": 72.17, "peak": 100.84, "min": 59.3}}, "power_watts_avg": 31.85, "energy_joules_est": 55.19, "sample_count": 13, "duration_seconds": 1.733}, "timestamp": "2026-01-17T15:37:47.329390"}
{"image_index": 348, "image_name": "000000038070.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038070.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2222.463, "latencies_ms": [2222.463], "images_per_second": 0.45, "prompt_tokens": 9, "response_tokens_est": 18, "n_tiles": 12, "output_text": "The image shows a toilet with a white seat and lid, situated in a bathroom.", "error": null, "sys_before": {"cpu_percent": 42.2, "ram_used_mb": 24925.6, "ram_available_mb": 100846.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.3, "ram_used_mb": 24927.6, "ram_available_mb": 100844.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.18, "peak": 16.66, "min": 13.39}, "VDD_CPU_SOC_MSS": {"avg": 15.93, "peak": 17.71, "min": 14.18}, "VDD_GPU": {"avg": 37.12, "peak": 44.1, "min": 29.15}, "VIN": {"avg": 79.01, "peak": 113.67, "min": 62.18}}, "power_watts_avg": 37.12, "energy_joules_est": 82.52, "sample_count": 17, "duration_seconds": 2.223}, "timestamp": "2026-01-17T15:37:49.639503"}
{"image_index": 348, "image_name": "000000038070.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038070.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 5999.467, "latencies_ms": [5999.467], "images_per_second": 0.167, "prompt_tokens": 23, "response_tokens_est": 129, "n_tiles": 12, "output_text": "toilet: 1\ntoilet paper: 1\ntoilet brush: 1\ntoilet paper roll: 1\ntoilet paper holder: 1\ntoilet paper: 1\ntoilet brush holder: 1\ntoilet brush: 1\ntoilet brush holder: 1\ntoilet paper holder: 1\ntoilet paper: 1\ntoilet brush holder: 1\ntoilet paper holder: 1\ntoilet paper: 1\ntoilet paper holder: 1\ntoilet paper: 1\ntoilet paper holder: 1\nto", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24927.6, "ram_available_mb": 100844.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 24927.6, "ram_available_mb": 100844.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.2, "peak": 16.66, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.71, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 30.97, "peak": 46.46, "min": 26.0}, "VIN": {"avg": 73.48, "peak": 121.57, "min": 53.6}}, "power_watts_avg": 30.97, "energy_joules_est": 185.82, "sample_count": 47, "duration_seconds": 6.0}, "timestamp": "2026-01-17T15:37:55.645699"}
{"image_index": 348, "image_name": "000000038070.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038070.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 4048.34, "latencies_ms": [4048.34], "images_per_second": 0.247, "prompt_tokens": 27, "response_tokens_est": 70, "n_tiles": 12, "output_text": "The main object in the foreground is a toilet, which is positioned near the left side of the image. The toilet is in the foreground, making it the most prominent object in the scene. The background features a wall, which is slightly to the right of the toilet. The wall is in the background, providing a neutral backdrop to the toilet.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24927.6, "ram_available_mb": 100844.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 24928.3, "ram_available_mb": 100843.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.19, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.7, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 33.16, "peak": 44.49, "min": 26.0}, "VIN": {"avg": 75.8, "peak": 109.21, "min": 57.86}}, "power_watts_avg": 33.16, "energy_joules_est": 134.26, "sample_count": 31, "duration_seconds": 4.049}, "timestamp": "2026-01-17T15:37:59.700279"}
{"image_index": 348, "image_name": "000000038070.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038070.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3201.121, "latencies_ms": [3201.121], "images_per_second": 0.312, "prompt_tokens": 21, "response_tokens_est": 45, "n_tiles": 12, "output_text": "The image depicts a bathroom setting with a toilet and a toilet paper holder. The toilet is white and placed on a beige carpeted floor. There is a person standing next to the toilet, wearing black shoes.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24928.3, "ram_available_mb": 100843.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 24928.3, "ram_available_mb": 100843.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.98, "peak": 16.36, "min": 14.12}, "VDD_CPU_SOC_MSS": {"avg": 15.48, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 34.85, "peak": 45.69, "min": 26.0}, "VIN": {"avg": 74.83, "peak": 110.16, "min": 55.82}}, "power_watts_avg": 34.85, "energy_joules_est": 111.57, "sample_count": 25, "duration_seconds": 3.201}, "timestamp": "2026-01-17T15:38:02.907593"}
{"image_index": 348, "image_name": "000000038070.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038070.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2994.705, "latencies_ms": [2994.705], "images_per_second": 0.334, "prompt_tokens": 19, "response_tokens_est": 39, "n_tiles": 12, "output_text": "The toilet in the image is white, with a glossy finish, and is situated on a tiled floor. The lighting is bright, casting a clear reflection on the toilet's surface.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24928.3, "ram_available_mb": 100843.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 24929.3, "ram_available_mb": 100842.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.97, "peak": 16.36, "min": 13.92}, "VDD_CPU_SOC_MSS": {"avg": 15.49, "peak": 17.32, "min": 14.17}, "VDD_GPU": {"avg": 35.73, "peak": 45.3, "min": 26.39}, "VIN": {"avg": 74.91, "peak": 96.53, "min": 65.5}}, "power_watts_avg": 35.73, "energy_joules_est": 107.02, "sample_count": 23, "duration_seconds": 2.995}, "timestamp": "2026-01-17T15:38:05.908785"}
{"image_index": 349, "image_name": "000000038118.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038118.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1601.543, "latencies_ms": [1601.543], "images_per_second": 0.624, "prompt_tokens": 9, "response_tokens_est": 37, "n_tiles": 6, "output_text": "A skier in a red jacket and black pants is skiing down a snowy slope, with ski poles in hand, and the snow is visibly disturbed by their movement.", "error": null, "sys_before": {"cpu_percent": 38.4, "ram_used_mb": 24944.9, "ram_available_mb": 100827.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 24945.9, "ram_available_mb": 100826.3, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.64, "peak": 15.11, "min": 13.51}, "VDD_CPU_SOC_MSS": {"avg": 15.32, "peak": 15.75, "min": 14.18}, "VDD_GPU": {"avg": 32.73, "peak": 40.57, "min": 25.21}, "VIN": {"avg": 73.96, "peak": 93.73, "min": 64.39}}, "power_watts_avg": 32.73, "energy_joules_est": 52.44, "sample_count": 12, "duration_seconds": 1.602}, "timestamp": "2026-01-17T15:38:07.578183"}
{"image_index": 349, "image_name": "000000038118.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038118.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1712.245, "latencies_ms": [1712.245], "images_per_second": 0.584, "prompt_tokens": 23, "response_tokens_est": 38, "n_tiles": 6, "output_text": "1. Skier\n2. Ski poles\n3. Ski\n4. Snow\n5. Snowboard\n6. Snowboarder\n7. Snow\n8. Snowboard", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24945.9, "ram_available_mb": 100826.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 24947.6, "ram_available_mb": 100824.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.9, "peak": 15.34, "min": 14.43}, "VDD_CPU_SOC_MSS": {"avg": 15.5, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 32.15, "peak": 40.56, "min": 24.43}, "VIN": {"avg": 73.73, "peak": 107.74, "min": 60.01}}, "power_watts_avg": 32.15, "energy_joules_est": 55.07, "sample_count": 13, "duration_seconds": 1.713}, "timestamp": "2026-01-17T15:38:09.296745"}
{"image_index": 349, "image_name": "000000038118.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038118.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2364.066, "latencies_ms": [2364.066], "images_per_second": 0.423, "prompt_tokens": 27, "response_tokens_est": 61, "n_tiles": 6, "output_text": "The main objects in the image are a person skiing and a snow-covered mountain. The person is positioned in the foreground, closer to the viewer, while the mountain is in the background, further away. The ski tracks on the snow indicate that the person has been skiing on the mountain.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24947.6, "ram_available_mb": 100824.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 24948.6, "ram_available_mb": 100823.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.88, "peak": 15.24, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.53, "peak": 15.75, "min": 14.57}, "VDD_GPU": {"avg": 29.85, "peak": 40.95, "min": 23.24}, "VIN": {"avg": 68.83, "peak": 98.24, "min": 56.39}}, "power_watts_avg": 29.85, "energy_joules_est": 70.58, "sample_count": 18, "duration_seconds": 2.365}, "timestamp": "2026-01-17T15:38:11.666972"}
{"image_index": 349, "image_name": "000000038118.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038118.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2251.152, "latencies_ms": [2251.152], "images_per_second": 0.444, "prompt_tokens": 21, "response_tokens_est": 57, "n_tiles": 6, "output_text": "The image depicts a snowy mountainous landscape with a clear blue sky overhead. A skier dressed in red gear is navigating the snowy terrain, using ski poles to maintain balance and direction. The scene is set in a winter environment, likely during a skiing activity.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24948.6, "ram_available_mb": 100823.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 24948.9, "ram_available_mb": 100823.3, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.87, "peak": 15.24, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.52, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 29.96, "peak": 40.16, "min": 23.24}, "VIN": {"avg": 71.04, "peak": 111.15, "min": 55.69}}, "power_watts_avg": 29.96, "energy_joules_est": 67.46, "sample_count": 17, "duration_seconds": 2.252}, "timestamp": "2026-01-17T15:38:13.924904"}
{"image_index": 349, "image_name": "000000038118.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038118.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2387.517, "latencies_ms": [2387.517], "images_per_second": 0.419, "prompt_tokens": 19, "response_tokens_est": 62, "n_tiles": 6, "output_text": "The image depicts a snowy mountain landscape under a clear blue sky. The snow-covered ground is illuminated by the bright sunlight, casting shadows and highlighting the texture of the snow. The man in the red jacket and black pants is equipped with ski poles and is actively skiing down the slope.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24948.9, "ram_available_mb": 100823.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 24948.6, "ram_available_mb": 100823.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.86, "peak": 15.24, "min": 14.3}, "VDD_CPU_SOC_MSS": {"avg": 15.53, "peak": 15.75, "min": 14.56}, "VDD_GPU": {"avg": 29.76, "peak": 40.95, "min": 23.24}, "VIN": {"avg": 68.62, "peak": 117.2, "min": 52.97}}, "power_watts_avg": 29.76, "energy_joules_est": 71.07, "sample_count": 18, "duration_seconds": 2.388}, "timestamp": "2026-01-17T15:38:16.322598"}
{"image_index": 350, "image_name": "000000038210.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038210.jpg", "image_width": 428, "image_height": 640, "image_resolution": "428x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1631.206, "latencies_ms": [1631.206], "images_per_second": 0.613, "prompt_tokens": 9, "response_tokens_est": 38, "n_tiles": 6, "output_text": "A man is skiing down a snowy trail, wearing a red and white jacket, black pants, and a beanie, with a bib on his back that reads \"30.\"", "error": null, "sys_before": {"cpu_percent": 41.0, "ram_used_mb": 24948.6, "ram_available_mb": 100823.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 24948.6, "ram_available_mb": 100823.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.66, "peak": 15.11, "min": 13.59}, "VDD_CPU_SOC_MSS": {"avg": 15.35, "peak": 16.14, "min": 14.18}, "VDD_GPU": {"avg": 32.07, "peak": 40.18, "min": 25.22}, "VIN": {"avg": 74.64, "peak": 111.79, "min": 56.02}}, "power_watts_avg": 32.07, "energy_joules_est": 52.32, "sample_count": 12, "duration_seconds": 1.632}, "timestamp": "2026-01-17T15:38:18.017295"}
{"image_index": 350, "image_name": "000000038210.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038210.jpg", "image_width": 428, "image_height": 640, "image_resolution": "428x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1755.405, "latencies_ms": [1755.405], "images_per_second": 0.57, "prompt_tokens": 23, "response_tokens_est": 41, "n_tiles": 6, "output_text": "1. Ski poles\n2. Ski\n3. Ski skis\n4. Ski skier\n5. Skiing\n6. Snow\n7. Snowboarder\n8. Snowboard", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24948.6, "ram_available_mb": 100823.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 24948.8, "ram_available_mb": 100823.3, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.04, "peak": 15.54, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.63, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 32.36, "peak": 40.18, "min": 24.82}, "VIN": {"avg": 70.8, "peak": 103.95, "min": 55.16}}, "power_watts_avg": 32.36, "energy_joules_est": 56.82, "sample_count": 13, "duration_seconds": 1.756}, "timestamp": "2026-01-17T15:38:19.778638"}
{"image_index": 350, "image_name": "000000038210.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038210.jpg", "image_width": 428, "image_height": 640, "image_resolution": "428x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2871.084, "latencies_ms": [2871.084], "images_per_second": 0.348, "prompt_tokens": 27, "response_tokens_est": 79, "n_tiles": 6, "output_text": "The main object in the foreground is a skier wearing a red and white jacket, black pants, and a beanie. The skier is holding ski poles and appears to be in motion, skiing on a snowy trail. In the background, there are other skiers and a snowy mountain landscape. The skier in the background is further away and slightly out of focus.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24948.8, "ram_available_mb": 100823.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 24949.1, "ram_available_mb": 100823.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.91, "peak": 15.24, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.6, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 28.72, "peak": 41.34, "min": 22.85}, "VIN": {"avg": 67.37, "peak": 103.43, "min": 51.02}}, "power_watts_avg": 28.72, "energy_joules_est": 82.46, "sample_count": 22, "duration_seconds": 2.871}, "timestamp": "2026-01-17T15:38:22.656620"}
{"image_index": 350, "image_name": "000000038210.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038210.jpg", "image_width": 428, "image_height": 640, "image_resolution": "428x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2444.084, "latencies_ms": [2444.084], "images_per_second": 0.409, "prompt_tokens": 21, "response_tokens_est": 64, "n_tiles": 6, "output_text": "The image depicts a snowy landscape with a person dressed in winter attire, including a beanie and ski gear, actively skiing down a snow-covered path. The scene is set in a forested area, with snow-covered trees and a clear sky, suggesting a cold, wintry environment.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24949.1, "ram_available_mb": 100823.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 24949.1, "ram_available_mb": 100823.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.89, "peak": 15.24, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.46, "peak": 15.75, "min": 14.56}, "VDD_GPU": {"avg": 29.23, "peak": 40.16, "min": 23.25}, "VIN": {"avg": 71.03, "peak": 111.09, "min": 61.53}}, "power_watts_avg": 29.23, "energy_joules_est": 71.45, "sample_count": 19, "duration_seconds": 2.445}, "timestamp": "2026-01-17T15:38:25.107188"}
{"image_index": 350, "image_name": "000000038210.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038210.jpg", "image_width": 428, "image_height": 640, "image_resolution": "428x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2444.464, "latencies_ms": [2444.464], "images_per_second": 0.409, "prompt_tokens": 19, "response_tokens_est": 64, "n_tiles": 6, "output_text": "The image depicts a snowy landscape with a man dressed in a red and white jacket, black pants, and a gray beanie, holding ski poles. The scene is illuminated by soft, diffused light, likely from an overcast sky, and the snow-covered ground and trees are clearly visible.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24949.1, "ram_available_mb": 100823.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 24948.8, "ram_available_mb": 100823.3, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.87, "peak": 15.14, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.38, "peak": 15.75, "min": 14.57}, "VDD_GPU": {"avg": 29.42, "peak": 41.36, "min": 23.25}, "VIN": {"avg": 67.5, "peak": 78.63, "min": 56.41}}, "power_watts_avg": 29.42, "energy_joules_est": 71.93, "sample_count": 19, "duration_seconds": 2.445}, "timestamp": "2026-01-17T15:38:27.558368"}
{"image_index": 351, "image_name": "000000038576.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038576.jpg", "image_width": 438, "image_height": 640, "image_resolution": "438x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1656.24, "latencies_ms": [1656.24], "images_per_second": 0.604, "prompt_tokens": 9, "response_tokens_est": 39, "n_tiles": 6, "output_text": "The image shows a black and white photo of a computer setup with a monitor, keyboard, and mouse on a desk, with a wall and a partial view of a window in the background.", "error": null, "sys_before": {"cpu_percent": 38.3, "ram_used_mb": 24948.8, "ram_available_mb": 100823.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 24949.8, "ram_available_mb": 100822.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.75, "peak": 15.14, "min": 13.9}, "VDD_CPU_SOC_MSS": {"avg": 15.45, "peak": 16.14, "min": 14.57}, "VDD_GPU": {"avg": 32.17, "peak": 40.18, "min": 25.21}, "VIN": {"avg": 76.46, "peak": 107.94, "min": 61.19}}, "power_watts_avg": 32.17, "energy_joules_est": 53.31, "sample_count": 12, "duration_seconds": 1.657}, "timestamp": "2026-01-17T15:38:29.263844"}
{"image_index": 351, "image_name": "000000038576.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038576.jpg", "image_width": 438, "image_height": 640, "image_resolution": "438x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1159.844, "latencies_ms": [1159.844], "images_per_second": 0.862, "prompt_tokens": 23, "response_tokens_est": 19, "n_tiles": 6, "output_text": "- computer monitor: 1\n- keyboard: 1\n- mouse: 2", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 24949.8, "ram_available_mb": 100822.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 24949.6, "ram_available_mb": 100822.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.93, "peak": 15.44, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.5, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 36.29, "peak": 40.57, "min": 30.33}, "VIN": {"avg": 81.87, "peak": 124.61, "min": 61.32}}, "power_watts_avg": 36.29, "energy_joules_est": 42.11, "sample_count": 8, "duration_seconds": 1.16}, "timestamp": "2026-01-17T15:38:30.433830"}
{"image_index": 351, "image_name": "000000038576.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038576.jpg", "image_width": 438, "image_height": 640, "image_resolution": "438x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3108.972, "latencies_ms": [3108.972], "images_per_second": 0.322, "prompt_tokens": 27, "response_tokens_est": 91, "n_tiles": 6, "output_text": "The main objects in the image are a computer monitor, a keyboard, and a mouse. The monitor is positioned in the foreground, slightly to the left, with the keyboard and mouse placed in front of it. The keyboard is on the right side of the monitor, and the mouse is positioned to the left of the keyboard. The background is minimal, with a plain wall and a light-colored surface, which helps to focus attention on the main objects.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24949.6, "ram_available_mb": 100822.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 24950.3, "ram_available_mb": 100821.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.08, "peak": 15.34, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.67, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 28.99, "peak": 42.13, "min": 23.64}, "VIN": {"avg": 70.51, "peak": 98.12, "min": 62.82}}, "power_watts_avg": 28.99, "energy_joules_est": 90.14, "sample_count": 24, "duration_seconds": 3.109}, "timestamp": "2026-01-17T15:38:33.548632"}
{"image_index": 351, "image_name": "000000038576.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038576.jpg", "image_width": 438, "image_height": 640, "image_resolution": "438x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2155.359, "latencies_ms": [2155.359], "images_per_second": 0.464, "prompt_tokens": 21, "response_tokens_est": 56, "n_tiles": 6, "output_text": "The image depicts a modern office workspace with a computer setup. The desk features a large monitor displaying a desktop wallpaper, a keyboard, and a mouse. The workspace is well-lit, and the image is in black and white, giving it a classic and professional appearance.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24950.3, "ram_available_mb": 100821.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 24950.5, "ram_available_mb": 100821.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.07, "peak": 15.44, "min": 14.43}, "VDD_CPU_SOC_MSS": {"avg": 15.7, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 30.33, "peak": 40.56, "min": 24.03}, "VIN": {"avg": 70.81, "peak": 109.07, "min": 56.92}}, "power_watts_avg": 30.33, "energy_joules_est": 65.39, "sample_count": 17, "duration_seconds": 2.156}, "timestamp": "2026-01-17T15:38:35.709993"}
{"image_index": 351, "image_name": "000000038576.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038576.jpg", "image_width": 438, "image_height": 640, "image_resolution": "438x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1942.941, "latencies_ms": [1942.941], "images_per_second": 0.515, "prompt_tokens": 19, "response_tokens_est": 48, "n_tiles": 6, "output_text": "The image is a black and white photograph of a computer setup. The computer monitor displays a desktop wallpaper, and the keyboard and mouse are placed on a wooden desk. The lighting is soft and even, creating a calm and professional atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24950.5, "ram_available_mb": 100821.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 24950.5, "ram_available_mb": 100821.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.06, "peak": 15.44, "min": 14.43}, "VDD_CPU_SOC_MSS": {"avg": 15.64, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 31.12, "peak": 40.18, "min": 24.03}, "VIN": {"avg": 75.01, "peak": 105.51, "min": 57.78}}, "power_watts_avg": 31.12, "energy_joules_est": 60.47, "sample_count": 15, "duration_seconds": 1.943}, "timestamp": "2026-01-17T15:38:37.658786"}
{"image_index": 352, "image_name": "000000038678.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038678.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2566.948, "latencies_ms": [2566.948], "images_per_second": 0.39, "prompt_tokens": 9, "response_tokens_est": 28, "n_tiles": 12, "output_text": "A woman is sitting in a train carriage, holding a doughnut and a coffee cup, with a blurry background of other passengers.", "error": null, "sys_before": {"cpu_percent": 46.1, "ram_used_mb": 24927.6, "ram_available_mb": 100844.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 24929.3, "ram_available_mb": 100842.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.17, "peak": 16.66, "min": 13.49}, "VDD_CPU_SOC_MSS": {"avg": 15.86, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 35.85, "peak": 43.73, "min": 27.18}, "VIN": {"avg": 79.07, "peak": 118.91, "min": 60.23}}, "power_watts_avg": 35.85, "energy_joules_est": 92.04, "sample_count": 20, "duration_seconds": 2.567}, "timestamp": "2026-01-17T15:38:40.323288"}
{"image_index": 352, "image_name": "000000038678.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038678.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2758.399, "latencies_ms": [2758.399], "images_per_second": 0.363, "prompt_tokens": 23, "response_tokens_est": 32, "n_tiles": 12, "output_text": "1. Woman\n2. Woman\n3. Woman\n4. Woman\n5. Woman\n6. Woman\n7. Woman\n8. Woman", "error": null, "sys_before": {"cpu_percent": 40.0, "ram_used_mb": 24929.3, "ram_available_mb": 100842.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 24930.5, "ram_available_mb": 100841.7, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.26, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.77, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 36.58, "peak": 45.28, "min": 27.18}, "VIN": {"avg": 79.48, "peak": 108.16, "min": 59.35}}, "power_watts_avg": 36.58, "energy_joules_est": 100.91, "sample_count": 21, "duration_seconds": 2.759}, "timestamp": "2026-01-17T15:38:43.087956"}
{"image_index": 352, "image_name": "000000038678.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038678.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3363.819, "latencies_ms": [3363.819], "images_per_second": 0.297, "prompt_tokens": 27, "response_tokens_est": 50, "n_tiles": 12, "output_text": "The main objects in the image are a woman and a man. The woman is holding a doughnut in the foreground, while the man is partially visible in the background. The doughnut is near the woman, and the man is further away.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24930.5, "ram_available_mb": 100841.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 24930.7, "ram_available_mb": 100841.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.22, "peak": 16.56, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.77, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 34.69, "peak": 45.68, "min": 26.01}, "VIN": {"avg": 74.67, "peak": 122.93, "min": 57.6}}, "power_watts_avg": 34.69, "energy_joules_est": 116.71, "sample_count": 26, "duration_seconds": 3.364}, "timestamp": "2026-01-17T15:38:46.459455"}
{"image_index": 352, "image_name": "000000038678.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038678.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3598.627, "latencies_ms": [3598.627], "images_per_second": 0.278, "prompt_tokens": 21, "response_tokens_est": 57, "n_tiles": 12, "output_text": "The image depicts a scene inside a train carriage, where a woman is seated and holding a doughnut. The woman is wearing a striped top and has short hair. The train carriage is filled with other passengers, and there is a window with blurred greenery outside.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24930.7, "ram_available_mb": 100841.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 24930.7, "ram_available_mb": 100841.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.22, "peak": 16.56, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.7, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 34.09, "peak": 45.3, "min": 26.01}, "VIN": {"avg": 77.2, "peak": 127.31, "min": 59.71}}, "power_watts_avg": 34.09, "energy_joules_est": 122.69, "sample_count": 28, "duration_seconds": 3.599}, "timestamp": "2026-01-17T15:38:50.064229"}
{"image_index": 352, "image_name": "000000038678.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038678.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3432.404, "latencies_ms": [3432.404], "images_per_second": 0.291, "prompt_tokens": 19, "response_tokens_est": 52, "n_tiles": 12, "output_text": "The image features a woman with short, light-colored hair, wearing a maroon top. She is holding a white plate with a golden-brown donut, sitting in a dimly lit train carriage. The lighting is warm, creating a cozy atmosphere.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24930.7, "ram_available_mb": 100841.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 24932.0, "ram_available_mb": 100840.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.21, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.69, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 34.33, "peak": 44.89, "min": 26.0}, "VIN": {"avg": 76.84, "peak": 114.92, "min": 59.95}}, "power_watts_avg": 34.33, "energy_joules_est": 117.85, "sample_count": 27, "duration_seconds": 3.433}, "timestamp": "2026-01-17T15:38:53.503147"}
{"image_index": 353, "image_name": "000000038825.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038825.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1303.355, "latencies_ms": [1303.355], "images_per_second": 0.767, "prompt_tokens": 9, "response_tokens_est": 26, "n_tiles": 6, "output_text": "The image depicts a zebra grazing on a lush green field, with its distinctive black and white stripes clearly visible.", "error": null, "sys_before": {"cpu_percent": 40.7, "ram_used_mb": 24947.1, "ram_available_mb": 100825.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 24947.6, "ram_available_mb": 100824.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.64, "peak": 15.14, "min": 13.82}, "VDD_CPU_SOC_MSS": {"avg": 15.31, "peak": 16.14, "min": 14.17}, "VDD_GPU": {"avg": 34.84, "peak": 40.95, "min": 27.97}, "VIN": {"avg": 74.81, "peak": 103.68, "min": 64.57}}, "power_watts_avg": 34.84, "energy_joules_est": 45.42, "sample_count": 9, "duration_seconds": 1.304}, "timestamp": "2026-01-17T15:38:54.865717"}
{"image_index": 353, "image_name": "000000038825.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038825.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1086.877, "latencies_ms": [1086.877], "images_per_second": 0.92, "prompt_tokens": 23, "response_tokens_est": 16, "n_tiles": 6, "output_text": "zebra: 2\ngrass: 1\nground: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24947.6, "ram_available_mb": 100824.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 24947.6, "ram_available_mb": 100824.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.79, "peak": 15.24, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.3, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 36.92, "peak": 41.34, "min": 30.33}, "VIN": {"avg": 75.6, "peak": 97.34, "min": 62.13}}, "power_watts_avg": 36.92, "energy_joules_est": 40.14, "sample_count": 8, "duration_seconds": 1.087}, "timestamp": "2026-01-17T15:38:55.959586"}
{"image_index": 353, "image_name": "000000038825.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038825.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2357.835, "latencies_ms": [2357.835], "images_per_second": 0.424, "prompt_tokens": 27, "response_tokens_est": 61, "n_tiles": 6, "output_text": "The main objects in the image are two zebras. The foreground features a zebra with a black and white striped pattern, while the background shows another zebra with similar stripes. The zebra in the foreground is closer to the camera, while the one in the background is further away.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24947.6, "ram_available_mb": 100824.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 24948.1, "ram_available_mb": 100824.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.89, "peak": 15.24, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.59, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 30.72, "peak": 42.13, "min": 23.25}, "VIN": {"avg": 71.38, "peak": 97.76, "min": 62.66}}, "power_watts_avg": 30.72, "energy_joules_est": 72.45, "sample_count": 18, "duration_seconds": 2.358}, "timestamp": "2026-01-17T15:38:58.323019"}
{"image_index": 353, "image_name": "000000038825.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038825.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2048.54, "latencies_ms": [2048.54], "images_per_second": 0.488, "prompt_tokens": 21, "response_tokens_est": 50, "n_tiles": 6, "output_text": "The image depicts a zebra grazing on a lush green field. The zebra is the main focus, with its distinctive black and white stripes clearly visible. The background is blurred, emphasizing the zebra's presence and activity.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24948.1, "ram_available_mb": 100824.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 24948.3, "ram_available_mb": 100823.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.88, "peak": 15.24, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.5, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 30.55, "peak": 40.56, "min": 23.64}, "VIN": {"avg": 72.22, "peak": 111.91, "min": 63.23}}, "power_watts_avg": 30.55, "energy_joules_est": 62.6, "sample_count": 16, "duration_seconds": 2.049}, "timestamp": "2026-01-17T15:39:00.377844"}
{"image_index": 353, "image_name": "000000038825.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038825.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1850.656, "latencies_ms": [1850.656], "images_per_second": 0.54, "prompt_tokens": 19, "response_tokens_est": 43, "n_tiles": 6, "output_text": "The image features a zebra with a striking black and white striped pattern, standing on a lush green grassy field. The lighting is natural, casting soft shadows and highlighting the vivid contrast between the stripes.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24948.3, "ram_available_mb": 100823.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 24949.0, "ram_available_mb": 100823.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.86, "peak": 15.24, "min": 14.3}, "VDD_CPU_SOC_MSS": {"avg": 15.41, "peak": 15.75, "min": 14.56}, "VDD_GPU": {"avg": 31.57, "peak": 40.56, "min": 24.04}, "VIN": {"avg": 75.91, "peak": 113.12, "min": 63.91}}, "power_watts_avg": 31.57, "energy_joules_est": 58.43, "sample_count": 14, "duration_seconds": 1.851}, "timestamp": "2026-01-17T15:39:02.234697"}
{"image_index": 354, "image_name": "000000038829.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038829.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1569.569, "latencies_ms": [1569.569], "images_per_second": 0.637, "prompt_tokens": 9, "response_tokens_est": 36, "n_tiles": 6, "output_text": "A group of young students are riding bicycles in a city, with a woman in a green jacket and a man in a white shirt and blue pants also riding a bicycle.", "error": null, "sys_before": {"cpu_percent": 39.0, "ram_used_mb": 24949.0, "ram_available_mb": 100823.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 24949.3, "ram_available_mb": 100822.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.79, "peak": 15.14, "min": 13.9}, "VDD_CPU_SOC_MSS": {"avg": 15.48, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 32.4, "peak": 40.16, "min": 25.21}, "VIN": {"avg": 72.72, "peak": 93.56, "min": 61.93}}, "power_watts_avg": 32.4, "energy_joules_est": 50.86, "sample_count": 12, "duration_seconds": 1.57}, "timestamp": "2026-01-17T15:39:03.849893"}
{"image_index": 354, "image_name": "000000038829.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038829.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1739.645, "latencies_ms": [1739.645], "images_per_second": 0.575, "prompt_tokens": 23, "response_tokens_est": 39, "n_tiles": 6, "output_text": "1. Bicycle\n2. Motorcycle\n3. People\n4. Motorcycle\n5. Bicycle\n6. Motorcycle\n7. Bicycle\n8. Motorcycle", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24949.3, "ram_available_mb": 100822.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 24949.0, "ram_available_mb": 100823.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.84, "peak": 15.24, "min": 14.43}, "VDD_CPU_SOC_MSS": {"avg": 15.41, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 32.45, "peak": 40.95, "min": 24.42}, "VIN": {"avg": 76.52, "peak": 116.29, "min": 63.35}}, "power_watts_avg": 32.45, "energy_joules_est": 56.47, "sample_count": 13, "duration_seconds": 1.74}, "timestamp": "2026-01-17T15:39:05.596426"}
{"image_index": 354, "image_name": "000000038829.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038829.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 4132.716, "latencies_ms": [4132.716], "images_per_second": 0.242, "prompt_tokens": 27, "response_tokens_est": 123, "n_tiles": 6, "output_text": "In the image, there is a green bicycle parked on the left side of the frame, with a person wearing a white shirt and dark pants seated on it. The bicycle is positioned near the curb, with a white plastic basket attached to the front. In the background, there is a black motorcycle parked on the right side of the frame, with a person wearing a white helmet and a light-colored jacket seated on it. The motorcycle is parked further back on the street, and there is a person walking in the background. The overall scene depicts a casual street setting with people riding bicycles and motorcycles.", "error": null, "sys_before": {"cpu_percent": 16.7, "ram_used_mb": 24949.0, "ram_available_mb": 100823.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 24949.8, "ram_available_mb": 100822.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.92, "peak": 15.24, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.6, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 26.73, "peak": 40.56, "min": 22.85}, "VIN": {"avg": 67.0, "peak": 115.24, "min": 56.0}}, "power_watts_avg": 26.73, "energy_joules_est": 110.48, "sample_count": 32, "duration_seconds": 4.133}, "timestamp": "2026-01-17T15:39:09.735562"}
{"image_index": 354, "image_name": "000000038829.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038829.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2873.999, "latencies_ms": [2873.999], "images_per_second": 0.348, "prompt_tokens": 21, "response_tokens_est": 79, "n_tiles": 6, "output_text": "The image depicts a bustling street scene in a city, likely in Southeast Asia, where a group of young individuals are riding bicycles. The setting is urban, with buildings and shops in the background, and the weather appears to be sunny. The individuals are dressed in casual attire, with some wearing white shirts and others in blue, suggesting they might be students or young professionals.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24949.8, "ram_available_mb": 100822.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 24950.8, "ram_available_mb": 100821.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.88, "peak": 15.24, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.53, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 28.27, "peak": 40.16, "min": 22.86}, "VIN": {"avg": 70.99, "peak": 106.24, "min": 61.46}}, "power_watts_avg": 28.27, "energy_joules_est": 81.26, "sample_count": 22, "duration_seconds": 2.874}, "timestamp": "2026-01-17T15:39:12.616237"}
{"image_index": 354, "image_name": "000000038829.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038829.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2191.797, "latencies_ms": [2191.797], "images_per_second": 0.456, "prompt_tokens": 19, "response_tokens_est": 55, "n_tiles": 6, "output_text": "The image depicts a street scene with a mix of modern and traditional elements. Notable visual attributes include a green bicycle with a basket, a black scooter, and a white motorcycle. The lighting is natural, suggesting daytime, and the weather appears to be clear.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24950.8, "ram_available_mb": 100821.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 24951.0, "ram_available_mb": 100821.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.88, "peak": 15.34, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.33, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 30.21, "peak": 40.56, "min": 23.24}, "VIN": {"avg": 71.06, "peak": 107.68, "min": 56.91}}, "power_watts_avg": 30.21, "energy_joules_est": 66.23, "sample_count": 16, "duration_seconds": 2.192}, "timestamp": "2026-01-17T15:39:14.815166"}
{"image_index": 355, "image_name": "000000039405.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039405.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2567.768, "latencies_ms": [2567.768], "images_per_second": 0.389, "prompt_tokens": 9, "response_tokens_est": 28, "n_tiles": 12, "output_text": "A tennis player is preparing to hit a tennis ball on a well-maintained grass court, with spectators seated in the background.", "error": null, "sys_before": {"cpu_percent": 27.6, "ram_used_mb": 24918.0, "ram_available_mb": 100854.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.6, "ram_used_mb": 24920.2, "ram_available_mb": 100852.0, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.25, "peak": 16.66, "min": 13.79}, "VDD_CPU_SOC_MSS": {"avg": 15.95, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 36.32, "peak": 44.1, "min": 27.57}, "VIN": {"avg": 83.27, "peak": 121.48, "min": 59.85}}, "power_watts_avg": 36.32, "energy_joules_est": 93.28, "sample_count": 19, "duration_seconds": 2.568}, "timestamp": "2026-01-17T15:39:17.442014"}
{"image_index": 355, "image_name": "000000039405.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039405.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 3134.12, "latencies_ms": [3134.12], "images_per_second": 0.319, "prompt_tokens": 23, "response_tokens_est": 43, "n_tiles": 12, "output_text": "1. Tennis player\n2. Tennis racket\n3. Tennis ball\n4. Tennis court\n5. Stadium seating\n6. Spectators\n7. Green grass\n8. Person sitting on bench", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24920.2, "ram_available_mb": 100852.0, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 24920.2, "ram_available_mb": 100852.0, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.21, "peak": 16.56, "min": 14.12}, "VDD_CPU_SOC_MSS": {"avg": 15.81, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 35.38, "peak": 45.68, "min": 26.39}, "VIN": {"avg": 72.18, "peak": 109.44, "min": 56.98}}, "power_watts_avg": 35.38, "energy_joules_est": 110.9, "sample_count": 24, "duration_seconds": 3.135}, "timestamp": "2026-01-17T15:39:20.586540"}
{"image_index": 355, "image_name": "000000039405.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039405.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 4610.68, "latencies_ms": [4610.68], "images_per_second": 0.217, "prompt_tokens": 27, "response_tokens_est": 86, "n_tiles": 12, "output_text": "The main objects in the image are a tennis court, a tennis player, and a crowd of spectators. The tennis player is positioned on the left side of the image, near the baseline, preparing to hit the tennis ball. The crowd of spectators is located in the background, seated on benches, watching the match. The tennis court is the central focus, with the player and the spectators forming the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24920.2, "ram_available_mb": 100852.0, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 24919.9, "ram_available_mb": 100852.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.15, "peak": 16.66, "min": 14.12}, "VDD_CPU_SOC_MSS": {"avg": 15.65, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 32.11, "peak": 44.91, "min": 25.6}, "VIN": {"avg": 73.3, "peak": 107.26, "min": 58.19}}, "power_watts_avg": 32.11, "energy_joules_est": 148.06, "sample_count": 36, "duration_seconds": 4.611}, "timestamp": "2026-01-17T15:39:25.203929"}
{"image_index": 355, "image_name": "000000039405.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039405.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3711.495, "latencies_ms": [3711.495], "images_per_second": 0.269, "prompt_tokens": 21, "response_tokens_est": 60, "n_tiles": 12, "output_text": "The image depicts a tennis match taking place on a well-maintained grass court, with spectators seated in the background. The players are actively engaged in the game, with one player in a white outfit preparing to hit the ball, while the other player is positioned to receive the serve.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24919.9, "ram_available_mb": 100852.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 24919.9, "ram_available_mb": 100852.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.18, "peak": 16.56, "min": 14.12}, "VDD_CPU_SOC_MSS": {"avg": 15.66, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 33.7, "peak": 44.91, "min": 26.0}, "VIN": {"avg": 75.36, "peak": 106.06, "min": 60.34}}, "power_watts_avg": 33.7, "energy_joules_est": 125.09, "sample_count": 29, "duration_seconds": 3.712}, "timestamp": "2026-01-17T15:39:28.921935"}
{"image_index": 355, "image_name": "000000039405.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039405.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3301.944, "latencies_ms": [3301.944], "images_per_second": 0.303, "prompt_tokens": 19, "response_tokens_est": 48, "n_tiles": 12, "output_text": "The image depicts a tennis match taking place on a well-maintained grass court. The lighting is bright, likely from stadium lights, and the weather appears to be clear, with no visible signs of rain or adverse weather conditions.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24919.9, "ram_available_mb": 100852.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 24920.7, "ram_available_mb": 100851.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.19, "peak": 16.66, "min": 14.12}, "VDD_CPU_SOC_MSS": {"avg": 15.72, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 34.64, "peak": 45.69, "min": 26.01}, "VIN": {"avg": 80.42, "peak": 129.07, "min": 61.67}}, "power_watts_avg": 34.64, "energy_joules_est": 114.4, "sample_count": 26, "duration_seconds": 3.303}, "timestamp": "2026-01-17T15:39:32.230588"}
{"image_index": 356, "image_name": "000000039477.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039477.jpg", "image_width": 640, "image_height": 421, "image_resolution": "640x421", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1715.986, "latencies_ms": [1715.986], "images_per_second": 0.583, "prompt_tokens": 9, "response_tokens_est": 41, "n_tiles": 6, "output_text": "The image shows a cozy living room with a brown couch, a white cushion, a small plant on a table, a white sewing machine, a green plastic bag, and a window with white curtains.", "error": null, "sys_before": {"cpu_percent": 39.2, "ram_used_mb": 24943.5, "ram_available_mb": 100828.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 24944.3, "ram_available_mb": 100827.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.75, "peak": 15.14, "min": 13.72}, "VDD_CPU_SOC_MSS": {"avg": 15.41, "peak": 16.14, "min": 14.17}, "VDD_GPU": {"avg": 31.94, "peak": 40.56, "min": 24.83}, "VIN": {"avg": 73.53, "peak": 122.55, "min": 54.1}}, "power_watts_avg": 31.94, "energy_joules_est": 54.82, "sample_count": 13, "duration_seconds": 1.716}, "timestamp": "2026-01-17T15:39:34.008252"}
{"image_index": 356, "image_name": "000000039477.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039477.jpg", "image_width": 640, "image_height": 421, "image_resolution": "640x421", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 4143.755, "latencies_ms": [4143.755], "images_per_second": 0.241, "prompt_tokens": 23, "response_tokens_est": 129, "n_tiles": 6, "output_text": "- TV stand: 1\n- TV: 1\n- Curtains: 1\n- Window: 1\n- Rug: 1\n- Rug pad: 1\n- Pillow: 1\n- Pillow cover: 1\n- Potted plants: 2\n- Potted plant: 2\n- Scissors: 1\n- Scissors: 1\n- Small plant: 1\n- Small plant: 1\n- Small plant: 1\n- Small plant: 1\n- Small plant: 1\n- Small plant: 1\n- Small", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24944.3, "ram_available_mb": 100827.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 24945.0, "ram_available_mb": 100827.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.07, "peak": 15.44, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.74, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 26.99, "peak": 40.95, "min": 23.25}, "VIN": {"avg": 69.51, "peak": 111.61, "min": 50.13}}, "power_watts_avg": 26.99, "energy_joules_est": 111.85, "sample_count": 33, "duration_seconds": 4.144}, "timestamp": "2026-01-17T15:39:38.158587"}
{"image_index": 356, "image_name": "000000039477.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039477.jpg", "image_width": 640, "image_height": 421, "image_resolution": "640x421", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2884.162, "latencies_ms": [2884.162], "images_per_second": 0.347, "prompt_tokens": 27, "response_tokens_est": 82, "n_tiles": 6, "output_text": "The main objects in the image are a sewing machine, a small plant, a cushion, and a television stand. The sewing machine is positioned on the left side of the image, near the television stand. The small plant is placed on the right side of the sewing machine, near the cushion. The television stand is in the background, and the cushion is placed on the right side of the television stand.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24945.0, "ram_available_mb": 100827.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 24945.7, "ram_available_mb": 100826.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.04, "peak": 15.44, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.66, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 28.68, "peak": 40.56, "min": 23.24}, "VIN": {"avg": 69.24, "peak": 94.74, "min": 57.66}}, "power_watts_avg": 28.68, "energy_joules_est": 82.73, "sample_count": 22, "duration_seconds": 2.885}, "timestamp": "2026-01-17T15:39:41.049234"}
{"image_index": 356, "image_name": "000000039477.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039477.jpg", "image_width": 640, "image_height": 421, "image_resolution": "640x421", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2603.727, "latencies_ms": [2603.727], "images_per_second": 0.384, "prompt_tokens": 21, "response_tokens_est": 72, "n_tiles": 6, "output_text": "The image depicts a cozy living room with a small television on a wooden stand, a brown couch, and a white cushion on a small table. The room is well-lit with natural light coming through a window covered by sheer white curtains. The overall setting appears to be a comfortable and lived-in space, possibly in a home or a small apartment.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24945.7, "ram_available_mb": 100826.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 24945.7, "ram_available_mb": 100826.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.03, "peak": 15.44, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.65, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 29.29, "peak": 40.16, "min": 23.64}, "VIN": {"avg": 70.62, "peak": 96.39, "min": 63.64}}, "power_watts_avg": 29.29, "energy_joules_est": 76.27, "sample_count": 20, "duration_seconds": 2.604}, "timestamp": "2026-01-17T15:39:43.658963"}
{"image_index": 356, "image_name": "000000039477.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039477.jpg", "image_width": 640, "image_height": 421, "image_resolution": "640x421", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1917.475, "latencies_ms": [1917.475], "images_per_second": 0.522, "prompt_tokens": 19, "response_tokens_est": 47, "n_tiles": 6, "output_text": "The room features a cozy and well-lit atmosphere with natural light streaming through the partially drawn white curtains. The walls are painted in a light, neutral color, complementing the warm brown and white tones of the furniture and decor.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24945.7, "ram_available_mb": 100826.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 24946.2, "ram_available_mb": 100826.0, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.0, "peak": 15.44, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.61, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 31.6, "peak": 40.57, "min": 24.42}, "VIN": {"avg": 72.79, "peak": 105.16, "min": 52.28}}, "power_watts_avg": 31.6, "energy_joules_est": 60.61, "sample_count": 14, "duration_seconds": 1.918}, "timestamp": "2026-01-17T15:39:45.586960"}
{"image_index": 357, "image_name": "000000039480.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039480.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1553.149, "latencies_ms": [1553.149], "images_per_second": 0.644, "prompt_tokens": 9, "response_tokens_est": 35, "n_tiles": 6, "output_text": "A tennis player in a bright red outfit is holding a tennis racket and appears to be in a moment of celebration or reaction after a successful play on a clay court.", "error": null, "sys_before": {"cpu_percent": 41.1, "ram_used_mb": 24946.2, "ram_available_mb": 100826.0, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 24946.5, "ram_available_mb": 100825.7, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.76, "peak": 15.24, "min": 14.0}, "VDD_CPU_SOC_MSS": {"avg": 15.46, "peak": 16.14, "min": 14.57}, "VDD_GPU": {"avg": 32.8, "peak": 40.57, "min": 25.6}, "VIN": {"avg": 72.39, "peak": 102.58, "min": 50.17}}, "power_watts_avg": 32.8, "energy_joules_est": 50.95, "sample_count": 11, "duration_seconds": 1.553}, "timestamp": "2026-01-17T15:39:47.185886"}
{"image_index": 357, "image_name": "000000039480.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039480.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1868.711, "latencies_ms": [1868.711], "images_per_second": 0.535, "prompt_tokens": 23, "response_tokens_est": 45, "n_tiles": 6, "output_text": "1. Tennis player\n2. Red clay court\n3. Tennis racket\n4. White visor\n5. Red dress\n6. White headband\n7. White tennis shoes\n8. Green padding", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24946.5, "ram_available_mb": 100825.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 24947.2, "ram_available_mb": 100825.0, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.0, "peak": 15.44, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.61, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 32.08, "peak": 41.36, "min": 24.42}, "VIN": {"avg": 71.36, "peak": 109.27, "min": 50.24}}, "power_watts_avg": 32.08, "energy_joules_est": 59.96, "sample_count": 14, "duration_seconds": 1.869}, "timestamp": "2026-01-17T15:39:49.061204"}
{"image_index": 357, "image_name": "000000039480.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039480.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2375.894, "latencies_ms": [2375.894], "images_per_second": 0.421, "prompt_tokens": 27, "response_tokens_est": 63, "n_tiles": 6, "output_text": "The main object in the foreground is a tennis player dressed in a bright orange dress, holding a tennis racket. The player is standing on a clay court, which is the main object in the background. The clay court is surrounded by a green fence and a green mat, which are also in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24947.2, "ram_available_mb": 100825.0, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 24952.1, "ram_available_mb": 100820.0, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.04, "peak": 15.44, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.66, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 29.85, "peak": 40.57, "min": 23.64}, "VIN": {"avg": 73.53, "peak": 111.36, "min": 60.2}}, "power_watts_avg": 29.85, "energy_joules_est": 70.93, "sample_count": 18, "duration_seconds": 2.376}, "timestamp": "2026-01-17T15:39:51.443286"}
{"image_index": 357, "image_name": "000000039480.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039480.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2758.384, "latencies_ms": [2758.384], "images_per_second": 0.363, "prompt_tokens": 21, "response_tokens_est": 77, "n_tiles": 6, "output_text": "The image captures a moment on a clay tennis court, where a female tennis player is in the midst of a match. She is dressed in a vibrant red outfit and is holding a tennis racket, poised to hit the ball. The court is marked with white lines, and the background features a green barrier and a crocodile logo, indicating the event's location and sponsors.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24952.1, "ram_available_mb": 100820.0, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 24953.1, "ram_available_mb": 100819.0, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.06, "peak": 15.44, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.63, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 28.94, "peak": 40.56, "min": 23.24}, "VIN": {"avg": 72.07, "peak": 115.32, "min": 52.81}}, "power_watts_avg": 28.94, "energy_joules_est": 79.84, "sample_count": 21, "duration_seconds": 2.759}, "timestamp": "2026-01-17T15:39:54.208588"}
{"image_index": 357, "image_name": "000000039480.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039480.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2070.893, "latencies_ms": [2070.893], "images_per_second": 0.483, "prompt_tokens": 19, "response_tokens_est": 52, "n_tiles": 6, "output_text": "The image features a red clay tennis court with a bright, natural light illuminating the scene. The surface of the court is a reddish-brown color, and the lighting is even and clear, highlighting the details of the court and the athlete.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24953.4, "ram_available_mb": 100818.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 24954.6, "ram_available_mb": 100817.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.01, "peak": 15.44, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.56, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 30.98, "peak": 40.56, "min": 24.03}, "VIN": {"avg": 67.46, "peak": 85.83, "min": 50.9}}, "power_watts_avg": 30.98, "energy_joules_est": 64.16, "sample_count": 15, "duration_seconds": 2.071}, "timestamp": "2026-01-17T15:39:56.287715"}
{"image_index": 358, "image_name": "000000039484.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039484.jpg", "image_width": 640, "image_height": 437, "image_resolution": "640x437", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1602.386, "latencies_ms": [1602.386], "images_per_second": 0.624, "prompt_tokens": 9, "response_tokens_est": 37, "n_tiles": 6, "output_text": "The image depicts a bustling urban street scene with a variety of parked cars, a busy intersection, and a mix of commercial establishments, including a restaurant and a theater.", "error": null, "sys_before": {"cpu_percent": 32.7, "ram_used_mb": 24954.6, "ram_available_mb": 100817.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 24954.6, "ram_available_mb": 100817.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.81, "peak": 15.24, "min": 14.0}, "VDD_CPU_SOC_MSS": {"avg": 15.48, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 32.2, "peak": 40.16, "min": 25.21}, "VIN": {"avg": 76.87, "peak": 117.24, "min": 58.9}}, "power_watts_avg": 32.2, "energy_joules_est": 51.61, "sample_count": 12, "duration_seconds": 1.603}, "timestamp": "2026-01-17T15:39:57.938193"}
{"image_index": 358, "image_name": "000000039484.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039484.jpg", "image_width": 640, "image_height": 437, "image_resolution": "640x437", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 4234.507, "latencies_ms": [4234.507], "images_per_second": 0.236, "prompt_tokens": 23, "response_tokens_est": 128, "n_tiles": 6, "output_text": "object: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject:", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24954.6, "ram_available_mb": 100817.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 24956.1, "ram_available_mb": 100816.0, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.99, "peak": 15.44, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.59, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 26.72, "peak": 40.56, "min": 22.85}, "VIN": {"avg": 69.27, "peak": 111.95, "min": 59.22}}, "power_watts_avg": 26.72, "energy_joules_est": 113.16, "sample_count": 34, "duration_seconds": 4.235}, "timestamp": "2026-01-17T15:40:02.178713"}
{"image_index": 358, "image_name": "000000039484.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039484.jpg", "image_width": 640, "image_height": 437, "image_resolution": "640x437", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3001.716, "latencies_ms": [3001.716], "images_per_second": 0.333, "prompt_tokens": 27, "response_tokens_est": 83, "n_tiles": 6, "output_text": "The main objects in the image are a street scene with various buildings, cars, and people. The foreground features a parked silver car, while the background shows a row of buildings with signs and a restaurant named \"Herman's.\" The street is busy with cars, and there are people sitting at outdoor tables near the restaurant. The scene is set in an urban environment with a mix of commercial and residential buildings.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24956.1, "ram_available_mb": 100816.0, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 24956.4, "ram_available_mb": 100815.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.86, "peak": 15.24, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.34, "peak": 15.75, "min": 14.56}, "VDD_GPU": {"avg": 27.97, "peak": 40.18, "min": 22.85}, "VIN": {"avg": 68.4, "peak": 106.27, "min": 54.84}}, "power_watts_avg": 27.97, "energy_joules_est": 83.97, "sample_count": 23, "duration_seconds": 3.002}, "timestamp": "2026-01-17T15:40:05.186094"}
{"image_index": 358, "image_name": "000000039484.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039484.jpg", "image_width": 640, "image_height": 437, "image_resolution": "640x437", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2798.457, "latencies_ms": [2798.457], "images_per_second": 0.357, "prompt_tokens": 21, "response_tokens_est": 76, "n_tiles": 6, "output_text": "The image depicts a bustling urban street scene, likely in a city with a mix of commercial and residential buildings. The street is lined with parked cars, and there are several pedestrians visible. The buildings have various signs and advertisements, including a prominent \"Omnifest\" billboard. The sky is overcast, and the overall atmosphere appears to be busy and lively.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24956.4, "ram_available_mb": 100815.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 24956.6, "ram_available_mb": 100815.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.84, "peak": 15.24, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.36, "peak": 15.75, "min": 14.56}, "VDD_GPU": {"avg": 28.29, "peak": 40.16, "min": 22.86}, "VIN": {"avg": 69.28, "peak": 108.6, "min": 60.32}}, "power_watts_avg": 28.29, "energy_joules_est": 79.18, "sample_count": 22, "duration_seconds": 2.799}, "timestamp": "2026-01-17T15:40:07.990607"}
{"image_index": 358, "image_name": "000000039484.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039484.jpg", "image_width": 640, "image_height": 437, "image_resolution": "640x437", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2547.061, "latencies_ms": [2547.061], "images_per_second": 0.393, "prompt_tokens": 19, "response_tokens_est": 67, "n_tiles": 6, "output_text": "The image depicts a bustling urban street scene with a variety of notable visual attributes. The buildings are primarily brick and red, with some featuring green awnings. The street is lined with parked cars, and there are several streetlights and traffic signs visible. The sky is overcast, providing a subdued lighting condition.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24956.6, "ram_available_mb": 100815.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 24956.6, "ram_available_mb": 100815.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.82, "peak": 15.24, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.52, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 28.96, "peak": 40.16, "min": 22.85}, "VIN": {"avg": 70.41, "peak": 113.73, "min": 53.75}}, "power_watts_avg": 28.96, "energy_joules_est": 73.77, "sample_count": 19, "duration_seconds": 2.547}, "timestamp": "2026-01-17T15:40:10.543934"}
{"image_index": 359, "image_name": "000000039551.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039551.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1413.652, "latencies_ms": [1413.652], "images_per_second": 0.707, "prompt_tokens": 9, "response_tokens_est": 30, "n_tiles": 6, "output_text": "A female tennis player is captured mid-action on a brightly lit tennis court, with her racket in hand, poised to strike the ball.", "error": null, "sys_before": {"cpu_percent": 39.0, "ram_used_mb": 24956.6, "ram_available_mb": 100815.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 24957.9, "ram_available_mb": 100814.3, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.67, "peak": 15.14, "min": 13.9}, "VDD_CPU_SOC_MSS": {"avg": 15.39, "peak": 16.14, "min": 14.57}, "VDD_GPU": {"avg": 33.48, "peak": 40.16, "min": 26.79}, "VIN": {"avg": 76.65, "peak": 108.41, "min": 60.4}}, "power_watts_avg": 33.48, "energy_joules_est": 47.34, "sample_count": 10, "duration_seconds": 1.414}, "timestamp": "2026-01-17T15:40:12.004773"}
{"image_index": 359, "image_name": "000000039551.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039551.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1788.151, "latencies_ms": [1788.151], "images_per_second": 0.559, "prompt_tokens": 23, "response_tokens_est": 42, "n_tiles": 6, "output_text": "1. Tennis player\n2. Tennis racket\n3. Tennis ball\n4. Tennis court\n5. Tennis net\n6. Tennis shoes\n7. Tennis outfit\n8. Tennis visor", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24957.9, "ram_available_mb": 100814.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 24957.6, "ram_available_mb": 100814.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.0, "peak": 15.54, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.6, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 32.3, "peak": 40.97, "min": 24.82}, "VIN": {"avg": 69.17, "peak": 89.07, "min": 60.45}}, "power_watts_avg": 32.3, "energy_joules_est": 57.77, "sample_count": 13, "duration_seconds": 1.788}, "timestamp": "2026-01-17T15:40:13.799375"}
{"image_index": 359, "image_name": "000000039551.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039551.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2738.229, "latencies_ms": [2738.229], "images_per_second": 0.365, "prompt_tokens": 27, "response_tokens_est": 74, "n_tiles": 6, "output_text": "The main object in the foreground is a tennis player, who is positioned on the left side of the image. The tennis player is holding a tennis racket and appears to be preparing for a serve. The background features a tennis court with a green surface and a blue boundary line. The tennis ball is visible in the distance, slightly to the right of the player.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24957.6, "ram_available_mb": 100814.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 24957.6, "ram_available_mb": 100814.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.88, "peak": 15.34, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.34, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 28.85, "peak": 40.57, "min": 22.85}, "VIN": {"avg": 67.0, "peak": 89.63, "min": 55.28}}, "power_watts_avg": 28.85, "energy_joules_est": 79.01, "sample_count": 21, "duration_seconds": 2.739}, "timestamp": "2026-01-17T15:40:16.545896"}
{"image_index": 359, "image_name": "000000039551.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039551.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2710.048, "latencies_ms": [2710.048], "images_per_second": 0.369, "prompt_tokens": 21, "response_tokens_est": 73, "n_tiles": 6, "output_text": "The image captures a moment on a tennis court during a match. The player, dressed in a red outfit, is in the midst of a serve, with her racket in motion and her body leaning forward. The court is marked with white lines, and the green surface is visible. The player's focused expression and athletic stance indicate the intensity of the game.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24957.6, "ram_available_mb": 100814.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 24957.8, "ram_available_mb": 100814.3, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.85, "peak": 15.24, "min": 14.2}, "VDD_CPU_SOC_MSS": {"avg": 15.35, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 28.47, "peak": 40.16, "min": 22.85}, "VIN": {"avg": 71.09, "peak": 111.18, "min": 53.01}}, "power_watts_avg": 28.47, "energy_joules_est": 77.16, "sample_count": 21, "duration_seconds": 2.71}, "timestamp": "2026-01-17T15:40:19.261857"}
{"image_index": 359, "image_name": "000000039551.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039551.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1745.104, "latencies_ms": [1745.104], "images_per_second": 0.573, "prompt_tokens": 19, "response_tokens_est": 39, "n_tiles": 6, "output_text": "The tennis player is wearing a red outfit, which stands out against the green and blue court. The lighting is bright, indicating it is a sunny day, casting clear shadows on the court.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24957.8, "ram_available_mb": 100814.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 24958.6, "ram_available_mb": 100813.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.8, "peak": 15.14, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.44, "peak": 15.75, "min": 14.56}, "VDD_GPU": {"avg": 31.81, "peak": 40.95, "min": 24.03}, "VIN": {"avg": 69.17, "peak": 98.54, "min": 49.54}}, "power_watts_avg": 31.81, "energy_joules_est": 55.52, "sample_count": 13, "duration_seconds": 1.745}, "timestamp": "2026-01-17T15:40:21.012934"}
{"image_index": 360, "image_name": "000000039670.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039670.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 729.412, "latencies_ms": [729.412], "images_per_second": 1.371, "prompt_tokens": 9, "response_tokens_est": 14, "n_tiles": 4, "output_text": "A train is traveling on a track surrounded by lush greenery.", "error": null, "sys_before": {"cpu_percent": 38.8, "ram_used_mb": 24958.6, "ram_available_mb": 100813.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 24958.8, "ram_available_mb": 100813.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4606.7, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5444.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.21, "peak": 14.51, "min": 13.79}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.35, "min": 14.57}, "VDD_GPU": {"avg": 33.4, "peak": 37.03, "min": 29.94}, "VIN": {"avg": 75.59, "peak": 105.42, "min": 57.77}}, "power_watts_avg": 33.4, "energy_joules_est": 24.38, "sample_count": 5, "duration_seconds": 0.73}, "timestamp": "2026-01-17T15:40:21.793698"}
{"image_index": 360, "image_name": "000000039670.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039670.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 3590.131, "latencies_ms": [3590.131], "images_per_second": 0.279, "prompt_tokens": 23, "response_tokens_est": 129, "n_tiles": 4, "output_text": "object: train\nobject: train car\nobject: train tracks\nobject: greenery\nobject: bushes\nobject: trees\nobject: train engine\nobject: train car doors\nobject: train car windows\nobject: train car doors\nobject: train car windows\nobject: train car doors\nobject: train car windows\nobject: train car doors\nobject: train car windows\nobject: train car doors\nobject: train car windows\nobject: train car doors\nobject: train car windows\nobject: train car doors\nobject: train car windows\nobject: train car doors\nobject: train car windows", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24958.8, "ram_available_mb": 100813.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 24959.6, "ram_available_mb": 100812.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4606.7, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5455.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.04, "peak": 15.22, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.74, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 25.55, "peak": 39.77, "min": 22.07}, "VIN": {"avg": 67.14, "peak": 107.01, "min": 58.23}}, "power_watts_avg": 25.55, "energy_joules_est": 91.73, "sample_count": 28, "duration_seconds": 3.59}, "timestamp": "2026-01-17T15:40:25.393821"}
{"image_index": 360, "image_name": "000000039670.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039670.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1806.811, "latencies_ms": [1806.811], "images_per_second": 0.553, "prompt_tokens": 27, "response_tokens_est": 57, "n_tiles": 4, "output_text": "The main object in the image is a train traveling on a curved railway track. The train is positioned in the foreground, with the railway track curving to the right. The surrounding environment includes lush greenery and tall grasses, indicating that the train is in a natural setting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24959.6, "ram_available_mb": 100812.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 24959.8, "ram_available_mb": 100812.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4606.7, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5458.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.95, "peak": 15.22, "min": 14.51}, "VDD_CPU_SOC_MSS": {"avg": 15.69, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 27.6, "peak": 37.03, "min": 22.45}, "VIN": {"avg": 69.54, "peak": 104.64, "min": 59.03}}, "power_watts_avg": 27.6, "energy_joules_est": 49.88, "sample_count": 14, "duration_seconds": 1.807}, "timestamp": "2026-01-17T15:40:27.208811"}
{"image_index": 360, "image_name": "000000039670.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039670.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1428.316, "latencies_ms": [1428.316], "images_per_second": 0.7, "prompt_tokens": 21, "response_tokens_est": 42, "n_tiles": 4, "output_text": "The image depicts a train traveling on a curving railway track surrounded by lush greenery. The train is moving through a natural environment, with the tracks cutting through a grassy area with some rocks.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24959.8, "ram_available_mb": 100812.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 24960.0, "ram_available_mb": 100812.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4606.7, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5453.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.9, "peak": 15.22, "min": 14.51}, "VDD_CPU_SOC_MSS": {"avg": 15.64, "peak": 16.14, "min": 15.35}, "VDD_GPU": {"avg": 29.04, "peak": 37.42, "min": 23.25}, "VIN": {"avg": 66.61, "peak": 96.72, "min": 54.4}}, "power_watts_avg": 29.04, "energy_joules_est": 41.49, "sample_count": 11, "duration_seconds": 1.429}, "timestamp": "2026-01-17T15:40:28.643183"}
{"image_index": 360, "image_name": "000000039670.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039670.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1352.263, "latencies_ms": [1352.263], "images_per_second": 0.74, "prompt_tokens": 19, "response_tokens_est": 39, "n_tiles": 4, "output_text": "The train in the image is painted in a combination of blue and orange, with a white roof. The lighting is natural, suggesting it is daytime, and the weather appears to be clear.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24960.0, "ram_available_mb": 100812.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 24959.8, "ram_available_mb": 100812.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4606.7, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5451.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.86, "peak": 15.22, "min": 14.61}, "VDD_CPU_SOC_MSS": {"avg": 15.55, "peak": 15.75, "min": 15.35}, "VDD_GPU": {"avg": 29.94, "peak": 37.82, "min": 24.03}, "VIN": {"avg": 70.7, "peak": 105.03, "min": 60.1}}, "power_watts_avg": 29.94, "energy_joules_est": 40.49, "sample_count": 10, "duration_seconds": 1.352}, "timestamp": "2026-01-17T15:40:30.005745"}
{"image_index": 361, "image_name": "000000039769.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039769.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2258.548, "latencies_ms": [2258.548], "images_per_second": 0.443, "prompt_tokens": 9, "response_tokens_est": 19, "n_tiles": 12, "output_text": "A tabby cat is lying on a pink blanket, appearing to be sleeping peacefully.", "error": null, "sys_before": {"cpu_percent": 43.2, "ram_used_mb": 24933.8, "ram_available_mb": 100838.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.2, "ram_used_mb": 24932.5, "ram_available_mb": 100839.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.19, "peak": 16.66, "min": 13.49}, "VDD_CPU_SOC_MSS": {"avg": 16.02, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 37.19, "peak": 43.71, "min": 29.15}, "VIN": {"avg": 79.04, "peak": 105.85, "min": 64.92}}, "power_watts_avg": 37.19, "energy_joules_est": 84.01, "sample_count": 17, "duration_seconds": 2.259}, "timestamp": "2026-01-17T15:40:32.358211"}
{"image_index": 361, "image_name": "000000039769.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039769.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2239.729, "latencies_ms": [2239.729], "images_per_second": 0.446, "prompt_tokens": 23, "response_tokens_est": 17, "n_tiles": 12, "output_text": "cat: 1\nremote control: 2\nblanket: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24932.5, "ram_available_mb": 100839.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.2, "ram_used_mb": 24933.8, "ram_available_mb": 100838.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.37, "peak": 16.56, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.93, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 38.8, "peak": 45.68, "min": 29.54}, "VIN": {"avg": 82.84, "peak": 110.96, "min": 66.53}}, "power_watts_avg": 38.8, "energy_joules_est": 86.92, "sample_count": 17, "duration_seconds": 2.24}, "timestamp": "2026-01-17T15:40:34.605004"}
{"image_index": 361, "image_name": "000000039769.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039769.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 4017.313, "latencies_ms": [4017.313], "images_per_second": 0.249, "prompt_tokens": 27, "response_tokens_est": 69, "n_tiles": 12, "output_text": "The main object in the foreground is a cat lying on a pink blanket. The cat is positioned near the center of the image, with its body facing the left side of the frame. The background includes a red couch, which is partially visible behind the cat. The remote control is placed to the left of the cat, near the couch.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24933.8, "ram_available_mb": 100838.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 24934.5, "ram_available_mb": 100837.7, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.19, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.7, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 33.54, "peak": 45.68, "min": 26.0}, "VIN": {"avg": 75.63, "peak": 102.93, "min": 58.06}}, "power_watts_avg": 33.54, "energy_joules_est": 134.75, "sample_count": 31, "duration_seconds": 4.018}, "timestamp": "2026-01-17T15:40:38.630184"}
{"image_index": 361, "image_name": "000000039769.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039769.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 4180.426, "latencies_ms": [4180.426], "images_per_second": 0.239, "prompt_tokens": 21, "response_tokens_est": 74, "n_tiles": 12, "output_text": "The image depicts a serene scene of a tabby cat lying on a pink blanket, seemingly asleep. The cat's body is stretched out, with its head resting on the blanket, while its paws are visible and relaxed. The setting appears to be a cozy, indoor environment, possibly a living room or bedroom, with a soft, warm ambiance.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24934.5, "ram_available_mb": 100837.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 24936.4, "ram_available_mb": 100835.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.17, "peak": 16.66, "min": 14.12}, "VDD_CPU_SOC_MSS": {"avg": 15.69, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 32.91, "peak": 45.28, "min": 26.0}, "VIN": {"avg": 76.07, "peak": 114.57, "min": 62.95}}, "power_watts_avg": 32.91, "energy_joules_est": 137.59, "sample_count": 32, "duration_seconds": 4.181}, "timestamp": "2026-01-17T15:40:42.816882"}
{"image_index": 361, "image_name": "000000039769.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039769.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3598.482, "latencies_ms": [3598.482], "images_per_second": 0.278, "prompt_tokens": 19, "response_tokens_est": 57, "n_tiles": 12, "output_text": "The image features a tabby cat with a striped pattern, lying on a pink blanket. The lighting is soft and natural, casting gentle shadows on the cat's fur. The colors are vibrant, with the pink blanket providing a striking contrast to the cat's striped fur.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24936.4, "ram_available_mb": 100835.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 24936.6, "ram_available_mb": 100835.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.19, "peak": 16.56, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.76, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 33.96, "peak": 45.28, "min": 26.0}, "VIN": {"avg": 74.33, "peak": 102.92, "min": 62.5}}, "power_watts_avg": 33.96, "energy_joules_est": 122.22, "sample_count": 28, "duration_seconds": 3.599}, "timestamp": "2026-01-17T15:40:46.422230"}
{"image_index": 362, "image_name": "000000039785.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039785.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2366.338, "latencies_ms": [2366.338], "images_per_second": 0.423, "prompt_tokens": 9, "response_tokens_est": 22, "n_tiles": 12, "output_text": "A man is surfing in a river, wearing a black wetsuit and holding a blue surfboard.", "error": null, "sys_before": {"cpu_percent": 44.1, "ram_used_mb": 24925.2, "ram_available_mb": 100847.0, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.5, "ram_used_mb": 24926.6, "ram_available_mb": 100845.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.11, "peak": 16.76, "min": 13.19}, "VDD_CPU_SOC_MSS": {"avg": 15.81, "peak": 17.71, "min": 14.17}, "VDD_GPU": {"avg": 36.8, "peak": 44.49, "min": 28.36}, "VIN": {"avg": 79.65, "peak": 116.89, "min": 66.51}}, "power_watts_avg": 36.8, "energy_joules_est": 87.09, "sample_count": 18, "duration_seconds": 2.367}, "timestamp": "2026-01-17T15:40:48.899047"}
{"image_index": 362, "image_name": "000000039785.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039785.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2821.383, "latencies_ms": [2821.383], "images_per_second": 0.354, "prompt_tokens": 23, "response_tokens_est": 34, "n_tiles": 12, "output_text": "surfer: 1\nwetsuit: 1\nboard: 1\nocean: 1\nriver: 1\nbench: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24926.6, "ram_available_mb": 100845.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 24927.6, "ram_available_mb": 100844.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.29, "peak": 16.66, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.91, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 36.76, "peak": 46.07, "min": 27.18}, "VIN": {"avg": 81.11, "peak": 117.55, "min": 63.68}}, "power_watts_avg": 36.76, "energy_joules_est": 103.73, "sample_count": 21, "duration_seconds": 2.822}, "timestamp": "2026-01-17T15:40:51.727777"}
{"image_index": 362, "image_name": "000000039785.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039785.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3576.799, "latencies_ms": [3576.799], "images_per_second": 0.28, "prompt_tokens": 27, "response_tokens_est": 56, "n_tiles": 12, "output_text": "The main objects in the image are a river, a man, and a bench. The river is in the foreground, with the man standing on a surfboard near the water's edge. The bench is located further back in the scene, providing a resting spot for people.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24927.6, "ram_available_mb": 100844.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 24927.4, "ram_available_mb": 100844.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.23, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.73, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 34.19, "peak": 44.91, "min": 26.0}, "VIN": {"avg": 74.17, "peak": 114.48, "min": 60.12}}, "power_watts_avg": 34.19, "energy_joules_est": 122.3, "sample_count": 27, "duration_seconds": 3.577}, "timestamp": "2026-01-17T15:40:55.312115"}
{"image_index": 362, "image_name": "000000039785.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039785.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3190.803, "latencies_ms": [3190.803], "images_per_second": 0.313, "prompt_tokens": 21, "response_tokens_est": 45, "n_tiles": 12, "output_text": "The image depicts a scene at a river with a man in a black wetsuit surfing on a white wave. The setting is outdoors, likely in a natural environment with trees and a bench visible in the background.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 24927.4, "ram_available_mb": 100844.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 24928.3, "ram_available_mb": 100843.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.24, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.78, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 34.88, "peak": 44.49, "min": 26.39}, "VIN": {"avg": 76.18, "peak": 102.19, "min": 66.5}}, "power_watts_avg": 34.88, "energy_joules_est": 111.31, "sample_count": 25, "duration_seconds": 3.191}, "timestamp": "2026-01-17T15:40:58.509358"}
{"image_index": 362, "image_name": "000000039785.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039785.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3948.243, "latencies_ms": [3948.243], "images_per_second": 0.253, "prompt_tokens": 19, "response_tokens_est": 66, "n_tiles": 12, "output_text": "The image depicts a scene with a man in a black wetsuit surfing on a turbulent river. The river is brownish, and the man is standing on a surfboard. The lighting is natural, suggesting it is daytime, and the weather appears to be windy, as indicated by the choppy water.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24928.3, "ram_available_mb": 100843.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 24929.1, "ram_available_mb": 100843.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.03, "peak": 16.56, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.47, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 33.39, "peak": 45.28, "min": 25.6}, "VIN": {"avg": 75.46, "peak": 114.15, "min": 56.37}}, "power_watts_avg": 33.39, "energy_joules_est": 131.85, "sample_count": 30, "duration_seconds": 3.949}, "timestamp": "2026-01-17T15:41:02.464297"}
{"image_index": 363, "image_name": "000000039914.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039914.jpg", "image_width": 487, "image_height": 640, "image_resolution": "487x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2594.211, "latencies_ms": [2594.211], "images_per_second": 0.385, "prompt_tokens": 9, "response_tokens_est": 29, "n_tiles": 12, "output_text": "A woman and a child are standing on a grassy field, with the woman holding a colorful kite that is flying in the air.", "error": null, "sys_before": {"cpu_percent": 44.8, "ram_used_mb": 24928.7, "ram_available_mb": 100843.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.5, "ram_used_mb": 24928.8, "ram_available_mb": 100843.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.12, "peak": 16.66, "min": 13.19}, "VDD_CPU_SOC_MSS": {"avg": 15.81, "peak": 17.71, "min": 13.78}, "VDD_GPU": {"avg": 36.24, "peak": 44.1, "min": 27.57}, "VIN": {"avg": 77.72, "peak": 103.1, "min": 62.94}}, "power_watts_avg": 36.24, "energy_joules_est": 94.02, "sample_count": 19, "duration_seconds": 2.594}, "timestamp": "2026-01-17T15:41:05.160746"}
{"image_index": 363, "image_name": "000000039914.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039914.jpg", "image_width": 487, "image_height": 640, "image_resolution": "487x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2754.925, "latencies_ms": [2754.925], "images_per_second": 0.363, "prompt_tokens": 23, "response_tokens_est": 32, "n_tiles": 12, "output_text": "1. Person\n2. Person\n3. Person\n4. Person\n5. Person\n6. Person\n7. Person\n8. Person", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24928.8, "ram_available_mb": 100843.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 24926.1, "ram_available_mb": 100846.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.29, "peak": 16.56, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.86, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 36.76, "peak": 45.28, "min": 27.18}, "VIN": {"avg": 82.42, "peak": 139.12, "min": 65.74}}, "power_watts_avg": 36.76, "energy_joules_est": 101.28, "sample_count": 21, "duration_seconds": 2.755}, "timestamp": "2026-01-17T15:41:07.922002"}
{"image_index": 363, "image_name": "000000039914.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039914.jpg", "image_width": 487, "image_height": 640, "image_resolution": "487x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 4083.479, "latencies_ms": [4083.479], "images_per_second": 0.245, "prompt_tokens": 27, "response_tokens_est": 71, "n_tiles": 12, "output_text": "The main objects in the image are a woman and a child, both standing on the grass. The woman is wearing a black jacket and blue jeans, while the child is wearing a black top and pink pants. The child is holding a colorful kite, which is near the woman. The background features a park with trees and a clear blue sky.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24926.1, "ram_available_mb": 100846.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 24926.8, "ram_available_mb": 100845.3, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.2, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.82, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 33.19, "peak": 46.09, "min": 26.0}, "VIN": {"avg": 74.81, "peak": 118.13, "min": 60.71}}, "power_watts_avg": 33.19, "energy_joules_est": 135.54, "sample_count": 32, "duration_seconds": 4.084}, "timestamp": "2026-01-17T15:41:12.011451"}
{"image_index": 363, "image_name": "000000039914.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039914.jpg", "image_width": 487, "image_height": 640, "image_resolution": "487x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3605.307, "latencies_ms": [3605.307], "images_per_second": 0.277, "prompt_tokens": 21, "response_tokens_est": 57, "n_tiles": 12, "output_text": "The image depicts a serene park scene with a woman and a child standing on a grassy field. The woman is holding a colorful kite, while the child is watching it fly. The setting is a peaceful park with trees and a clear blue sky in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24926.8, "ram_available_mb": 100845.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 24927.3, "ram_available_mb": 100844.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.21, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.69, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 33.99, "peak": 44.51, "min": 26.0}, "VIN": {"avg": 76.48, "peak": 102.84, "min": 56.51}}, "power_watts_avg": 33.99, "energy_joules_est": 122.55, "sample_count": 28, "duration_seconds": 3.606}, "timestamp": "2026-01-17T15:41:15.623256"}
{"image_index": 363, "image_name": "000000039914.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039914.jpg", "image_width": 487, "image_height": 640, "image_resolution": "487x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 5036.475, "latencies_ms": [5036.475], "images_per_second": 0.199, "prompt_tokens": 19, "response_tokens_est": 99, "n_tiles": 12, "output_text": "The image depicts a vibrant outdoor scene with a clear blue sky, a lush green lawn, and a few bare trees in the background. The sunlight casts soft shadows, creating a warm and inviting atmosphere. The woman and child are dressed in casual attire, with the woman wearing a black leather jacket and blue jeans, and the child in a pink top and blue jeans. The scene is bright and cheerful, with the colorful kite adding a playful touch to the overall ambiance.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24927.3, "ram_available_mb": 100844.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 24927.8, "ram_available_mb": 100844.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.16, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.67, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 31.51, "peak": 44.89, "min": 26.0}, "VIN": {"avg": 71.98, "peak": 112.73, "min": 58.14}}, "power_watts_avg": 31.51, "energy_joules_est": 158.71, "sample_count": 40, "duration_seconds": 5.037}, "timestamp": "2026-01-17T15:41:20.666188"}
{"image_index": 364, "image_name": "000000039951.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039951.jpg", "image_width": 640, "image_height": 445, "image_resolution": "640x445", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1310.05, "latencies_ms": [1310.05], "images_per_second": 0.763, "prompt_tokens": 9, "response_tokens_est": 26, "n_tiles": 6, "output_text": "A young tennis player is captured mid-action on a court, with a tennis ball in the air, ready to be hit.", "error": null, "sys_before": {"cpu_percent": 47.5, "ram_used_mb": 24952.4, "ram_available_mb": 100819.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 24953.6, "ram_available_mb": 100818.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.59, "peak": 15.14, "min": 13.72}, "VDD_CPU_SOC_MSS": {"avg": 15.26, "peak": 16.14, "min": 14.17}, "VDD_GPU": {"avg": 34.53, "peak": 40.56, "min": 27.97}, "VIN": {"avg": 79.26, "peak": 105.08, "min": 61.14}}, "power_watts_avg": 34.53, "energy_joules_est": 45.25, "sample_count": 9, "duration_seconds": 1.31}, "timestamp": "2026-01-17T15:41:22.038939"}
{"image_index": 364, "image_name": "000000039951.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039951.jpg", "image_width": 640, "image_height": 445, "image_resolution": "640x445", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1888.763, "latencies_ms": [1888.763], "images_per_second": 0.529, "prompt_tokens": 23, "response_tokens_est": 46, "n_tiles": 6, "output_text": "1. Tennis player\n2. Tennis racket\n3. Tennis ball\n4. Green net\n5. Green tarp\n6. Blue tennis court\n7. White net\n8. Green tarpaulin", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24953.6, "ram_available_mb": 100818.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 24954.1, "ram_available_mb": 100818.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.03, "peak": 15.44, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.58, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 32.07, "peak": 40.95, "min": 24.42}, "VIN": {"avg": 69.27, "peak": 81.42, "min": 59.73}}, "power_watts_avg": 32.07, "energy_joules_est": 60.59, "sample_count": 14, "duration_seconds": 1.889}, "timestamp": "2026-01-17T15:41:23.933833"}
{"image_index": 364, "image_name": "000000039951.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039951.jpg", "image_width": 640, "image_height": 445, "image_resolution": "640x445", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2787.045, "latencies_ms": [2787.045], "images_per_second": 0.359, "prompt_tokens": 27, "response_tokens_est": 79, "n_tiles": 6, "output_text": "The main object in the foreground is a young tennis player, who is in the process of hitting a tennis ball. The player is positioned near the net, with his racket in his right hand, preparing to strike the ball. The background features a green tarpaulin, which is likely covering the court. The net is visible in the foreground, separating the court from the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24954.1, "ram_available_mb": 100818.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 24954.4, "ram_available_mb": 100817.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.11, "peak": 15.44, "min": 14.43}, "VDD_CPU_SOC_MSS": {"avg": 15.71, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 29.13, "peak": 40.95, "min": 23.64}, "VIN": {"avg": 69.36, "peak": 103.22, "min": 51.88}}, "power_watts_avg": 29.13, "energy_joules_est": 81.19, "sample_count": 21, "duration_seconds": 2.787}, "timestamp": "2026-01-17T15:41:26.727046"}
{"image_index": 364, "image_name": "000000039951.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039951.jpg", "image_width": 640, "image_height": 445, "image_resolution": "640x445", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2669.181, "latencies_ms": [2669.181], "images_per_second": 0.375, "prompt_tokens": 21, "response_tokens_est": 75, "n_tiles": 6, "output_text": "The image captures a young tennis player in mid-action on a tennis court, preparing to hit a tennis ball. The court is marked with white lines, and the player is wearing a white shirt, black shorts, and a red cap. The background shows a green tarpaulin, and there is a sign with the text \"Are you next?\" above the player.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24954.4, "ram_available_mb": 100817.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 24955.4, "ram_available_mb": 100816.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.12, "peak": 15.54, "min": 14.43}, "VDD_CPU_SOC_MSS": {"avg": 15.67, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 28.93, "peak": 40.16, "min": 23.64}, "VIN": {"avg": 69.76, "peak": 104.6, "min": 60.49}}, "power_watts_avg": 28.93, "energy_joules_est": 77.23, "sample_count": 21, "duration_seconds": 2.67}, "timestamp": "2026-01-17T15:41:29.402423"}
{"image_index": 364, "image_name": "000000039951.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039951.jpg", "image_width": 640, "image_height": 445, "image_resolution": "640x445", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2344.684, "latencies_ms": [2344.684], "images_per_second": 0.426, "prompt_tokens": 19, "response_tokens_est": 63, "n_tiles": 6, "output_text": "The image depicts a young tennis player in a white shirt and black shorts, wearing a red cap, actively engaged in a game on a blue and green tennis court. The lighting is bright, casting sharp shadows, indicating a sunny day. The net is white, and the background features a green tarp.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24955.4, "ram_available_mb": 100816.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 24956.3, "ram_available_mb": 100815.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.1, "peak": 15.54, "min": 14.43}, "VDD_CPU_SOC_MSS": {"avg": 15.64, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 29.81, "peak": 40.16, "min": 23.64}, "VIN": {"avg": 70.58, "peak": 112.87, "min": 55.19}}, "power_watts_avg": 29.81, "energy_joules_est": 69.92, "sample_count": 18, "duration_seconds": 2.345}, "timestamp": "2026-01-17T15:41:31.753571"}
{"image_index": 365, "image_name": "000000039956.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039956.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2244.4, "latencies_ms": [2244.4], "images_per_second": 0.446, "prompt_tokens": 9, "response_tokens_est": 61, "n_tiles": 6, "output_text": "The image depicts a cluttered room with various items scattered around, including a bed with a brown blanket, a wooden nightstand, a wooden chair, and a pile of boxes and bags, suggesting that the room is in the process of being unpacked or is in a state of disarray.", "error": null, "sys_before": {"cpu_percent": 41.7, "ram_used_mb": 24956.3, "ram_available_mb": 100815.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 24957.8, "ram_available_mb": 100814.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.93, "peak": 15.24, "min": 13.9}, "VDD_CPU_SOC_MSS": {"avg": 15.56, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 29.82, "peak": 40.16, "min": 23.64}, "VIN": {"avg": 74.15, "peak": 117.02, "min": 64.36}}, "power_watts_avg": 29.82, "energy_joules_est": 66.94, "sample_count": 17, "duration_seconds": 2.245}, "timestamp": "2026-01-17T15:41:34.057128"}
{"image_index": 365, "image_name": "000000039956.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039956.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1885.343, "latencies_ms": [1885.343], "images_per_second": 0.53, "prompt_tokens": 23, "response_tokens_est": 46, "n_tiles": 6, "output_text": "bed: 1\nclothes: 1\nbox: 1\ntrash can: 1\nchair: 1\ndrawer: 1\ncabinet: 1\ntable: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24957.8, "ram_available_mb": 100814.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 24958.8, "ram_available_mb": 100813.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.04, "peak": 15.44, "min": 14.43}, "VDD_CPU_SOC_MSS": {"avg": 15.63, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 31.74, "peak": 40.56, "min": 24.43}, "VIN": {"avg": 70.85, "peak": 91.68, "min": 55.58}}, "power_watts_avg": 31.74, "energy_joules_est": 59.85, "sample_count": 14, "duration_seconds": 1.886}, "timestamp": "2026-01-17T15:41:35.948978"}
{"image_index": 365, "image_name": "000000039956.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039956.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2214.3, "latencies_ms": [2214.3], "images_per_second": 0.452, "prompt_tokens": 27, "response_tokens_est": 58, "n_tiles": 6, "output_text": "The main objects in the image are scattered throughout the room, with the bed in the foreground and various items in the background. The bed is positioned near the left side of the image, while the items in the background are located further back, creating a sense of depth in the room.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24958.8, "ram_available_mb": 100813.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 24958.8, "ram_available_mb": 100813.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.07, "peak": 15.44, "min": 14.43}, "VDD_CPU_SOC_MSS": {"avg": 15.65, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 30.54, "peak": 40.95, "min": 24.04}, "VIN": {"avg": 72.1, "peak": 109.69, "min": 57.53}}, "power_watts_avg": 30.54, "energy_joules_est": 67.63, "sample_count": 17, "duration_seconds": 2.215}, "timestamp": "2026-01-17T15:41:38.169594"}
{"image_index": 365, "image_name": "000000039956.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039956.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2344.404, "latencies_ms": [2344.404], "images_per_second": 0.427, "prompt_tokens": 21, "response_tokens_est": 63, "n_tiles": 6, "output_text": "The image depicts a cluttered room with various items scattered around, including a bed with a brown blanket, a wooden nightstand, and a chair. The room appears to be in the process of being unpacked or tidied up, with boxes and other items visible on the floor and around the room.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24958.8, "ram_available_mb": 100813.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 24959.0, "ram_available_mb": 100813.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.1, "peak": 15.44, "min": 14.43}, "VDD_CPU_SOC_MSS": {"avg": 15.64, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 29.94, "peak": 40.18, "min": 23.64}, "VIN": {"avg": 69.85, "peak": 92.06, "min": 59.45}}, "power_watts_avg": 29.94, "energy_joules_est": 70.2, "sample_count": 18, "duration_seconds": 2.345}, "timestamp": "2026-01-17T15:41:40.520069"}
{"image_index": 365, "image_name": "000000039956.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039956.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1647.244, "latencies_ms": [1647.244], "images_per_second": 0.607, "prompt_tokens": 19, "response_tokens_est": 37, "n_tiles": 6, "output_text": "The room is dimly lit with warm, soft lighting, creating a cozy and intimate atmosphere. The walls are adorned with exposed brick, adding a rustic charm to the space.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24959.0, "ram_available_mb": 100813.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 24959.0, "ram_available_mb": 100813.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.03, "peak": 15.54, "min": 14.43}, "VDD_CPU_SOC_MSS": {"avg": 15.55, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 32.76, "peak": 40.16, "min": 25.21}, "VIN": {"avg": 74.03, "peak": 100.72, "min": 60.02}}, "power_watts_avg": 32.76, "energy_joules_est": 53.97, "sample_count": 12, "duration_seconds": 1.648}, "timestamp": "2026-01-17T15:41:42.173380"}
{"image_index": 366, "image_name": "000000040036.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000040036.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1486.399, "latencies_ms": [1486.399], "images_per_second": 0.673, "prompt_tokens": 9, "response_tokens_est": 33, "n_tiles": 6, "output_text": "A jockey is riding a horse in a race, with the horse's hooves off the ground, showcasing a dynamic and exciting moment in the race.", "error": null, "sys_before": {"cpu_percent": 45.9, "ram_used_mb": 24959.0, "ram_available_mb": 100813.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 24959.0, "ram_available_mb": 100813.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.75, "peak": 15.14, "min": 13.9}, "VDD_CPU_SOC_MSS": {"avg": 15.46, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 33.38, "peak": 40.57, "min": 26.0}, "VIN": {"avg": 76.68, "peak": 113.61, "min": 61.17}}, "power_watts_avg": 33.38, "energy_joules_est": 49.63, "sample_count": 11, "duration_seconds": 1.487}, "timestamp": "2026-01-17T15:41:43.721399"}
{"image_index": 366, "image_name": "000000040036.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000040036.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1934.357, "latencies_ms": [1934.357], "images_per_second": 0.517, "prompt_tokens": 23, "response_tokens_est": 46, "n_tiles": 6, "output_text": "1. Horse\n2. Rider\n3. Jumping fence\n4. Jumping bar\n5. Jumping pad\n6. Jumping rope\n7. Jumping rope holder\n8. Jumping pad holder", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24959.0, "ram_available_mb": 100813.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 24959.2, "ram_available_mb": 100812.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.89, "peak": 15.34, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.33, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 31.79, "peak": 40.95, "min": 24.03}, "VIN": {"avg": 70.46, "peak": 123.22, "min": 57.68}}, "power_watts_avg": 31.79, "energy_joules_est": 61.51, "sample_count": 14, "duration_seconds": 1.935}, "timestamp": "2026-01-17T15:41:45.662739"}
{"image_index": 366, "image_name": "000000040036.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000040036.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2367.529, "latencies_ms": [2367.529], "images_per_second": 0.422, "prompt_tokens": 27, "response_tokens_est": 61, "n_tiles": 6, "output_text": "The main object in the foreground is a brown horse with a rider on its back. The rider is wearing a red and green jacket and a helmet. The horse is equipped with a saddle and is in mid-jump over a wooden fence. The background features lush green trees, indicating an outdoor setting.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 24959.2, "ram_available_mb": 100812.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 24960.5, "ram_available_mb": 100811.7, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.85, "peak": 15.24, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.33, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 29.68, "peak": 40.57, "min": 23.24}, "VIN": {"avg": 72.15, "peak": 113.2, "min": 58.53}}, "power_watts_avg": 29.68, "energy_joules_est": 70.28, "sample_count": 18, "duration_seconds": 2.368}, "timestamp": "2026-01-17T15:41:48.036461"}
{"image_index": 366, "image_name": "000000040036.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000040036.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2615.385, "latencies_ms": [2615.385], "images_per_second": 0.382, "prompt_tokens": 21, "response_tokens_est": 70, "n_tiles": 6, "output_text": "The image captures a dynamic moment during a horse jumping event, where a jockey is riding a brown horse. The horse is mid-jump over a wooden fence, with the jockey wearing a red and green racing outfit and a helmet. The background features lush green trees, indicating an outdoor setting, likely a stable or a designated jumping area.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24960.5, "ram_available_mb": 100811.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 24960.7, "ram_available_mb": 100811.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.87, "peak": 15.34, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.34, "peak": 16.14, "min": 14.57}, "VDD_GPU": {"avg": 28.8, "peak": 40.16, "min": 22.86}, "VIN": {"avg": 70.69, "peak": 112.16, "min": 56.54}}, "power_watts_avg": 28.8, "energy_joules_est": 75.33, "sample_count": 20, "duration_seconds": 2.616}, "timestamp": "2026-01-17T15:41:50.657812"}
{"image_index": 366, "image_name": "000000040036.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000040036.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2132.553, "latencies_ms": [2132.553], "images_per_second": 0.469, "prompt_tokens": 19, "response_tokens_est": 53, "n_tiles": 6, "output_text": "The horse in the image is a rich brown color with a sleek, glossy coat. The rider is wearing a red and green riding jacket, a white helmet, and black riding boots. The lighting is bright and natural, suggesting it is a sunny day.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24960.7, "ram_available_mb": 100811.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 24960.5, "ram_available_mb": 100811.7, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.87, "peak": 15.24, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.3, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 30.28, "peak": 40.16, "min": 23.25}, "VIN": {"avg": 71.86, "peak": 106.33, "min": 61.17}}, "power_watts_avg": 30.28, "energy_joules_est": 64.58, "sample_count": 16, "duration_seconds": 2.133}, "timestamp": "2026-01-17T15:41:52.796599"}
{"image_index": 367, "image_name": "000000040083.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000040083.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1497.498, "latencies_ms": [1497.498], "images_per_second": 0.668, "prompt_tokens": 9, "response_tokens_est": 33, "n_tiles": 6, "output_text": "A man is sitting on a folding chair, leaning against a box filled with various items, while another man stands nearby, both seemingly engaged in their respective activities.", "error": null, "sys_before": {"cpu_percent": 47.4, "ram_used_mb": 24960.5, "ram_available_mb": 100811.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 24961.0, "ram_available_mb": 100811.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.7, "peak": 15.11, "min": 13.79}, "VDD_CPU_SOC_MSS": {"avg": 15.43, "peak": 16.14, "min": 14.17}, "VDD_GPU": {"avg": 32.37, "peak": 40.57, "min": 25.21}, "VIN": {"avg": 73.76, "peak": 101.39, "min": 60.58}}, "power_watts_avg": 32.37, "energy_joules_est": 48.48, "sample_count": 11, "duration_seconds": 1.498}, "timestamp": "2026-01-17T15:41:54.351339"}
{"image_index": 367, "image_name": "000000040083.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000040083.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 4149.523, "latencies_ms": [4149.523], "images_per_second": 0.241, "prompt_tokens": 23, "response_tokens_est": 128, "n_tiles": 6, "output_text": "object: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject:", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24961.0, "ram_available_mb": 100811.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 24961.2, "ram_available_mb": 100811.0, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.06, "peak": 15.44, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.68, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 27.08, "peak": 40.95, "min": 23.24}, "VIN": {"avg": 67.61, "peak": 95.51, "min": 50.54}}, "power_watts_avg": 27.08, "energy_joules_est": 112.38, "sample_count": 32, "duration_seconds": 4.15}, "timestamp": "2026-01-17T15:41:58.507093"}
{"image_index": 367, "image_name": "000000040083.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000040083.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3551.33, "latencies_ms": [3551.33], "images_per_second": 0.282, "prompt_tokens": 27, "response_tokens_est": 103, "n_tiles": 6, "output_text": "In the image, the man is positioned in the foreground, sitting on a folding chair near a small table with various items on it. The table is situated on the sidewalk, close to a bicycle parked on the side. The background features a street with parked cars and a building with a sign that reads \"\u5730\u5740\uff1a\u5229\u4e30\u8857\u4e94\u53f7\u826f\u53cb\u5e7f\u573a\u8d1f\u4e00\u697c.\" The overall scene is set in an urban environment, with the man and his surroundings providing a sense of everyday life in a bustling city.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24961.2, "ram_available_mb": 100811.0, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 24961.4, "ram_available_mb": 100810.7, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.95, "peak": 15.44, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.56, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 27.38, "peak": 40.16, "min": 22.85}, "VIN": {"avg": 67.39, "peak": 90.35, "min": 54.6}}, "power_watts_avg": 27.38, "energy_joules_est": 97.24, "sample_count": 27, "duration_seconds": 3.552}, "timestamp": "2026-01-17T15:42:02.069075"}
{"image_index": 367, "image_name": "000000040083.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000040083.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2826.422, "latencies_ms": [2826.422], "images_per_second": 0.354, "prompt_tokens": 21, "response_tokens_est": 77, "n_tiles": 6, "output_text": "The image depicts a street scene in an urban area, likely in a city with East Asian characters on signs. A man is sitting on a folding chair, surrounded by various items on a table, including a bicycle. Another man is standing nearby, and there are cars parked along the street. The setting appears to be a busy, pedestrian-friendly area with shops and street vendors.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24961.4, "ram_available_mb": 100810.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 24961.7, "ram_available_mb": 100810.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.83, "peak": 15.14, "min": 14.12}, "VDD_CPU_SOC_MSS": {"avg": 15.35, "peak": 15.75, "min": 14.56}, "VDD_GPU": {"avg": 28.33, "peak": 40.56, "min": 22.85}, "VIN": {"avg": 66.85, "peak": 88.55, "min": 58.82}}, "power_watts_avg": 28.33, "energy_joules_est": 80.08, "sample_count": 22, "duration_seconds": 2.827}, "timestamp": "2026-01-17T15:42:04.902654"}
{"image_index": 367, "image_name": "000000040083.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000040083.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2626.762, "latencies_ms": [2626.762], "images_per_second": 0.381, "prompt_tokens": 19, "response_tokens_est": 70, "n_tiles": 6, "output_text": "The image is a black and white photograph depicting a street scene with notable visual attributes. The scene is characterized by a mix of materials, including a wooden bench, a metal folding chair, and a wooden crate filled with various items. The lighting is natural, likely from the sun, casting shadows and highlighting the textures of the objects in the scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24961.7, "ram_available_mb": 100810.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 24962.4, "ram_available_mb": 100809.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.88, "peak": 15.34, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.34, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 28.72, "peak": 40.16, "min": 22.86}, "VIN": {"avg": 68.69, "peak": 90.77, "min": 57.74}}, "power_watts_avg": 28.72, "energy_joules_est": 75.45, "sample_count": 20, "duration_seconds": 2.627}, "timestamp": "2026-01-17T15:42:07.535751"}
{"image_index": 368, "image_name": "000000040471.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000040471.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1196.686, "latencies_ms": [1196.686], "images_per_second": 0.836, "prompt_tokens": 9, "response_tokens_est": 22, "n_tiles": 6, "output_text": "The image shows a kitchen with white cabinets, a black ceiling fan, and a window with blinds.", "error": null, "sys_before": {"cpu_percent": 39.7, "ram_used_mb": 24962.4, "ram_available_mb": 100809.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 24962.4, "ram_available_mb": 100809.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.54, "peak": 15.04, "min": 13.69}, "VDD_CPU_SOC_MSS": {"avg": 15.3, "peak": 16.14, "min": 14.17}, "VDD_GPU": {"avg": 34.61, "peak": 40.16, "min": 28.76}, "VIN": {"avg": 71.41, "peak": 81.4, "min": 60.76}}, "power_watts_avg": 34.61, "energy_joules_est": 41.43, "sample_count": 8, "duration_seconds": 1.197}, "timestamp": "2026-01-17T15:42:08.794415"}
{"image_index": 368, "image_name": "000000040471.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000040471.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 3031.847, "latencies_ms": [3031.847], "images_per_second": 0.33, "prompt_tokens": 23, "response_tokens_est": 84, "n_tiles": 6, "output_text": "- Kitchen\n- Count: 8\n- Cabinets\n- Count: 8\n- Refrigerator\n- Count: 1\n- Stove\n- Count: 1\n- Sink\n- Count: 1\n- Dishwasher\n- Count: 1\n- Count: 1\n- Count: 1\n- Count: 1\n- Count: 1", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24962.4, "ram_available_mb": 100809.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 24962.4, "ram_available_mb": 100809.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.85, "peak": 15.24, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.32, "peak": 15.74, "min": 14.56}, "VDD_GPU": {"avg": 28.39, "peak": 41.74, "min": 22.85}, "VIN": {"avg": 68.43, "peak": 112.8, "min": 52.66}}, "power_watts_avg": 28.39, "energy_joules_est": 86.09, "sample_count": 24, "duration_seconds": 3.032}, "timestamp": "2026-01-17T15:42:11.832639"}
{"image_index": 368, "image_name": "000000040471.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000040471.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2886.115, "latencies_ms": [2886.115], "images_per_second": 0.346, "prompt_tokens": 27, "response_tokens_est": 79, "n_tiles": 6, "output_text": "The main objects in the image are the kitchen cabinets, the refrigerator, and the countertop. The cabinets are located on the left side of the image, while the refrigerator is on the right. The countertop is situated in the middle, between the cabinets and the refrigerator. The countertop is also the focal point of the image, as it is where the various kitchen items are placed.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24962.4, "ram_available_mb": 100809.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 24962.9, "ram_available_mb": 100809.3, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.82, "peak": 15.24, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.44, "peak": 15.75, "min": 14.56}, "VDD_GPU": {"avg": 28.18, "peak": 40.16, "min": 22.85}, "VIN": {"avg": 66.53, "peak": 97.1, "min": 51.49}}, "power_watts_avg": 28.18, "energy_joules_est": 81.34, "sample_count": 22, "duration_seconds": 2.886}, "timestamp": "2026-01-17T15:42:14.724427"}
{"image_index": 368, "image_name": "000000040471.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000040471.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2458.375, "latencies_ms": [2458.375], "images_per_second": 0.407, "prompt_tokens": 21, "response_tokens_est": 64, "n_tiles": 6, "output_text": "The image depicts a kitchen with a white color scheme, featuring a white refrigerator, white cabinets, and a white countertop. The kitchen has a dark floor and a ceiling fan with a light fixture. The scene appears to be in a residential setting, possibly a home, with no people visible in the image.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24962.9, "ram_available_mb": 100809.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 24963.1, "ram_available_mb": 100809.0, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.81, "peak": 15.24, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.31, "peak": 15.74, "min": 14.56}, "VDD_GPU": {"avg": 29.09, "peak": 40.18, "min": 22.85}, "VIN": {"avg": 70.34, "peak": 111.04, "min": 62.34}}, "power_watts_avg": 29.09, "energy_joules_est": 71.52, "sample_count": 19, "duration_seconds": 2.459}, "timestamp": "2026-01-17T15:42:17.188943"}
{"image_index": 368, "image_name": "000000040471.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000040471.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1517.361, "latencies_ms": [1517.361], "images_per_second": 0.659, "prompt_tokens": 19, "response_tokens_est": 31, "n_tiles": 6, "output_text": "The kitchen is well-lit with a combination of natural and artificial light. The walls are painted white, and the ceiling features a classic light fixture.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24963.1, "ram_available_mb": 100809.0, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 24962.9, "ram_available_mb": 100809.3, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.78, "peak": 15.24, "min": 14.2}, "VDD_CPU_SOC_MSS": {"avg": 15.28, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 33.01, "peak": 40.16, "min": 25.6}, "VIN": {"avg": 75.21, "peak": 108.97, "min": 59.39}}, "power_watts_avg": 33.01, "energy_joules_est": 50.1, "sample_count": 11, "duration_seconds": 1.518}, "timestamp": "2026-01-17T15:42:18.713706"}
{"image_index": 369, "image_name": "000000040757.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000040757.jpg", "image_width": 425, "image_height": 640, "image_resolution": "425x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1199.599, "latencies_ms": [1199.599], "images_per_second": 0.834, "prompt_tokens": 9, "response_tokens_est": 22, "n_tiles": 6, "output_text": "A young child is lying on a bed, smiling and laughing, with a pacifier in their mouth.", "error": null, "sys_before": {"cpu_percent": 37.3, "ram_used_mb": 24962.9, "ram_available_mb": 100809.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 24963.1, "ram_available_mb": 100809.0, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.63, "peak": 15.14, "min": 13.9}, "VDD_CPU_SOC_MSS": {"avg": 15.4, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 34.75, "peak": 40.56, "min": 27.97}, "VIN": {"avg": 73.17, "peak": 101.21, "min": 59.1}}, "power_watts_avg": 34.75, "energy_joules_est": 41.7, "sample_count": 9, "duration_seconds": 1.2}, "timestamp": "2026-01-17T15:42:19.959340"}
{"image_index": 369, "image_name": "000000040757.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000040757.jpg", "image_width": 425, "image_height": 640, "image_resolution": "425x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1625.475, "latencies_ms": [1625.475], "images_per_second": 0.615, "prompt_tokens": 23, "response_tokens_est": 36, "n_tiles": 6, "output_text": "1. Baby\n2. Crib\n3. Pillow\n4. Bed\n5. Child\n6. Baby\n7. Crib\n8. Pillow", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24963.1, "ram_available_mb": 100809.0, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 24963.1, "ram_available_mb": 100809.0, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.95, "peak": 15.44, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.55, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 33.41, "peak": 40.95, "min": 25.21}, "VIN": {"avg": 73.27, "peak": 106.96, "min": 57.96}}, "power_watts_avg": 33.41, "energy_joules_est": 54.32, "sample_count": 12, "duration_seconds": 1.626}, "timestamp": "2026-01-17T15:42:21.591411"}
{"image_index": 369, "image_name": "000000040757.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000040757.jpg", "image_width": 425, "image_height": 640, "image_resolution": "425x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3018.534, "latencies_ms": [3018.534], "images_per_second": 0.331, "prompt_tokens": 27, "response_tokens_est": 87, "n_tiles": 6, "output_text": "The main object in the image is a young child lying on a bed. The child is positioned in the foreground, with a focus on their face and upper body. The bed has a patterned cover, and there is a blue object, possibly a toy or a blanket, near the child's head. The background is mostly dark, with a dark wall and a small portion of a television screen visible at the top left corner.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24963.1, "ram_available_mb": 100809.0, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 24963.4, "ram_available_mb": 100808.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.05, "peak": 15.44, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.65, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 28.65, "peak": 41.34, "min": 23.24}, "VIN": {"avg": 71.73, "peak": 115.39, "min": 59.76}}, "power_watts_avg": 28.65, "energy_joules_est": 86.49, "sample_count": 23, "duration_seconds": 3.019}, "timestamp": "2026-01-17T15:42:24.616364"}
{"image_index": 369, "image_name": "000000040757.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000040757.jpg", "image_width": 425, "image_height": 640, "image_resolution": "425x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2385.375, "latencies_ms": [2385.375], "images_per_second": 0.419, "prompt_tokens": 21, "response_tokens_est": 64, "n_tiles": 6, "output_text": "The image depicts a young child lying on a bed, seemingly asleep. The child is wearing a light-colored tank top and is surrounded by a patterned bedspread with various cartoon-like motifs. The room appears to be dimly lit, with a focus on the child, creating a cozy and intimate atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24963.4, "ram_available_mb": 100808.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 24963.6, "ram_available_mb": 100808.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.03, "peak": 15.44, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.62, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 29.78, "peak": 40.16, "min": 23.64}, "VIN": {"avg": 70.31, "peak": 115.68, "min": 59.51}}, "power_watts_avg": 29.78, "energy_joules_est": 71.05, "sample_count": 18, "duration_seconds": 2.386}, "timestamp": "2026-01-17T15:42:27.009175"}
{"image_index": 369, "image_name": "000000040757.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000040757.jpg", "image_width": 425, "image_height": 640, "image_resolution": "425x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1870.678, "latencies_ms": [1870.678], "images_per_second": 0.535, "prompt_tokens": 19, "response_tokens_est": 45, "n_tiles": 6, "output_text": "The image depicts a young child lying on a bed with a light-colored, patterned blanket. The child is wearing a white tank top and is surrounded by soft, ambient lighting, creating a cozy and warm atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24963.6, "ram_available_mb": 100808.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 24963.9, "ram_available_mb": 100808.3, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.98, "peak": 15.54, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.58, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 31.43, "peak": 40.16, "min": 24.42}, "VIN": {"avg": 74.4, "peak": 116.84, "min": 57.07}}, "power_watts_avg": 31.43, "energy_joules_est": 58.8, "sample_count": 14, "duration_seconds": 1.871}, "timestamp": "2026-01-17T15:42:28.885812"}
{"image_index": 370, "image_name": "000000041488.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000041488.jpg", "image_width": 640, "image_height": 369, "image_resolution": "640x369", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1794.812, "latencies_ms": [1794.812], "images_per_second": 0.557, "prompt_tokens": 9, "response_tokens_est": 42, "n_tiles": 6, "output_text": "The image shows a green highway sign with white lettering, indicating the direction to Queens Bronx, and a black and white sign with the Interstate 278 logo, both mounted on a metal structure.", "error": null, "sys_before": {"cpu_percent": 44.0, "ram_used_mb": 24963.9, "ram_available_mb": 100808.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 24963.9, "ram_available_mb": 100808.3, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.63, "peak": 15.04, "min": 13.79}, "VDD_CPU_SOC_MSS": {"avg": 15.33, "peak": 15.75, "min": 14.57}, "VDD_GPU": {"avg": 30.84, "peak": 40.16, "min": 23.64}, "VIN": {"avg": 73.21, "peak": 118.34, "min": 58.96}}, "power_watts_avg": 30.84, "energy_joules_est": 55.37, "sample_count": 14, "duration_seconds": 1.795}, "timestamp": "2026-01-17T15:42:30.738260"}
{"image_index": 370, "image_name": "000000041488.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000041488.jpg", "image_width": 640, "image_height": 369, "image_resolution": "640x369", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1918.949, "latencies_ms": [1918.949], "images_per_second": 0.521, "prompt_tokens": 23, "response_tokens_est": 45, "n_tiles": 6, "output_text": "1. Green highway sign\n2. Black metal structure\n3. Green highway sign\n4. White sign\n5. Green highway sign\n6. White sign\n7. Green highway sign\n8. White sign", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24963.9, "ram_available_mb": 100808.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 24964.1, "ram_available_mb": 100808.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.81, "peak": 15.24, "min": 14.2}, "VDD_CPU_SOC_MSS": {"avg": 15.33, "peak": 16.14, "min": 14.57}, "VDD_GPU": {"avg": 31.23, "peak": 40.16, "min": 23.64}, "VIN": {"avg": 75.09, "peak": 107.68, "min": 62.06}}, "power_watts_avg": 31.23, "energy_joules_est": 59.94, "sample_count": 14, "duration_seconds": 1.919}, "timestamp": "2026-01-17T15:42:32.667693"}
{"image_index": 370, "image_name": "000000041488.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000041488.jpg", "image_width": 640, "image_height": 369, "image_resolution": "640x369", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2347.278, "latencies_ms": [2347.278], "images_per_second": 0.426, "prompt_tokens": 27, "response_tokens_est": 60, "n_tiles": 6, "output_text": "The main object in the foreground is a green highway sign with white text. The sign is mounted on a metal structure, likely part of a larger highway signage system. In the background, there is a faint outline of a bridge structure, suggesting the sign is situated near a highway overpass.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24964.1, "ram_available_mb": 100808.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 24964.1, "ram_available_mb": 100808.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.81, "peak": 15.14, "min": 14.2}, "VDD_CPU_SOC_MSS": {"avg": 15.27, "peak": 15.74, "min": 14.56}, "VDD_GPU": {"avg": 29.63, "peak": 40.56, "min": 23.24}, "VIN": {"avg": 72.7, "peak": 115.24, "min": 61.51}}, "power_watts_avg": 29.63, "energy_joules_est": 69.56, "sample_count": 18, "duration_seconds": 2.348}, "timestamp": "2026-01-17T15:42:35.023551"}
{"image_index": 370, "image_name": "000000041488.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000041488.jpg", "image_width": 640, "image_height": 369, "image_resolution": "640x369", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2856.943, "latencies_ms": [2856.943], "images_per_second": 0.35, "prompt_tokens": 21, "response_tokens_est": 78, "n_tiles": 6, "output_text": "The image depicts a green highway sign with white lettering, indicating \"Queens Bronx\" and the Interstate 278 highway number. The sign is mounted on a metal structure, likely part of a highway or freeway, with a blurred background featuring industrial or construction elements. The scene suggests an urban or suburban setting, possibly near a highway interchange or a city area.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24964.1, "ram_available_mb": 100808.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 24964.8, "ram_available_mb": 100807.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.84, "peak": 15.24, "min": 14.2}, "VDD_CPU_SOC_MSS": {"avg": 15.35, "peak": 15.75, "min": 14.56}, "VDD_GPU": {"avg": 28.22, "peak": 40.16, "min": 22.85}, "VIN": {"avg": 71.58, "peak": 114.27, "min": 55.59}}, "power_watts_avg": 28.22, "energy_joules_est": 80.64, "sample_count": 22, "duration_seconds": 2.857}, "timestamp": "2026-01-17T15:42:37.887740"}
{"image_index": 370, "image_name": "000000041488.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000041488.jpg", "image_width": 640, "image_height": 369, "image_resolution": "640x369", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2428.567, "latencies_ms": [2428.567], "images_per_second": 0.412, "prompt_tokens": 19, "response_tokens_est": 63, "n_tiles": 6, "output_text": "The image features a green highway sign with white lettering, mounted on a metal structure. The sign is illuminated by artificial lighting, casting a bright glow on the sign and creating a stark contrast with the surrounding environment. The weather appears to be overcast, with a muted, gray sky in the background.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24964.8, "ram_available_mb": 100807.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 24965.1, "ram_available_mb": 100807.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.81, "peak": 15.24, "min": 14.2}, "VDD_CPU_SOC_MSS": {"avg": 15.29, "peak": 15.74, "min": 14.56}, "VDD_GPU": {"avg": 29.35, "peak": 40.16, "min": 22.85}, "VIN": {"avg": 70.02, "peak": 115.8, "min": 57.09}}, "power_watts_avg": 29.35, "energy_joules_est": 71.29, "sample_count": 18, "duration_seconds": 2.429}, "timestamp": "2026-01-17T15:42:40.326779"}
{"image_index": 371, "image_name": "000000041633.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000041633.jpg", "image_width": 640, "image_height": 429, "image_resolution": "640x429", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1280.727, "latencies_ms": [1280.727], "images_per_second": 0.781, "prompt_tokens": 9, "response_tokens_est": 25, "n_tiles": 6, "output_text": "The image features a vintage red Chevrolet car parked outdoors, with a clear sky and a few other cars in the background.", "error": null, "sys_before": {"cpu_percent": 41.8, "ram_used_mb": 24965.1, "ram_available_mb": 100807.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 24965.3, "ram_available_mb": 100806.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.55, "peak": 15.04, "min": 13.69}, "VDD_CPU_SOC_MSS": {"avg": 15.31, "peak": 16.14, "min": 14.57}, "VDD_GPU": {"avg": 33.96, "peak": 39.78, "min": 27.57}, "VIN": {"avg": 77.97, "peak": 110.31, "min": 61.99}}, "power_watts_avg": 33.96, "energy_joules_est": 43.5, "sample_count": 9, "duration_seconds": 1.281}, "timestamp": "2026-01-17T15:42:41.666685"}
{"image_index": 371, "image_name": "000000041633.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000041633.jpg", "image_width": 640, "image_height": 429, "image_resolution": "640x429", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1944.023, "latencies_ms": [1944.023], "images_per_second": 0.514, "prompt_tokens": 23, "response_tokens_est": 46, "n_tiles": 6, "output_text": "1. Red car\n2. Red truck\n3. White license plate\n4. White license plate\n5. White license plate\n6. White license plate\n7. White license plate\n8. White license plate", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24965.3, "ram_available_mb": 100806.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 24965.1, "ram_available_mb": 100807.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.81, "peak": 15.24, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.3, "peak": 15.74, "min": 14.56}, "VDD_GPU": {"avg": 31.61, "peak": 41.74, "min": 23.64}, "VIN": {"avg": 75.8, "peak": 112.35, "min": 62.17}}, "power_watts_avg": 31.61, "energy_joules_est": 61.46, "sample_count": 15, "duration_seconds": 1.944}, "timestamp": "2026-01-17T15:42:43.617444"}
{"image_index": 371, "image_name": "000000041633.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000041633.jpg", "image_width": 640, "image_height": 429, "image_resolution": "640x429", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2202.366, "latencies_ms": [2202.366], "images_per_second": 0.454, "prompt_tokens": 27, "response_tokens_est": 55, "n_tiles": 6, "output_text": "The main object in the foreground is a vintage red Chevrolet truck. The truck is parked on a concrete surface, with its front end facing the camera. The background features a green tent and other parked cars, indicating that the scene is likely at a car show or gathering.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24965.1, "ram_available_mb": 100807.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 24965.3, "ram_available_mb": 100806.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.84, "peak": 15.34, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.38, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 29.82, "peak": 40.56, "min": 23.25}, "VIN": {"avg": 69.44, "peak": 99.11, "min": 58.94}}, "power_watts_avg": 29.82, "energy_joules_est": 65.69, "sample_count": 17, "duration_seconds": 2.203}, "timestamp": "2026-01-17T15:42:45.826340"}
{"image_index": 371, "image_name": "000000041633.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000041633.jpg", "image_width": 640, "image_height": 429, "image_resolution": "640x429", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1975.628, "latencies_ms": [1975.628], "images_per_second": 0.506, "prompt_tokens": 21, "response_tokens_est": 47, "n_tiles": 6, "output_text": "The image depicts a classic red Chevrolet truck parked outdoors, likely at a car show or gathering. The setting is a paved area with other vintage cars and a green tent in the background, suggesting a car enthusiast's event.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24965.3, "ram_available_mb": 100806.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 24965.3, "ram_available_mb": 100806.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.8, "peak": 15.24, "min": 14.2}, "VDD_CPU_SOC_MSS": {"avg": 15.28, "peak": 15.74, "min": 14.56}, "VDD_GPU": {"avg": 30.62, "peak": 40.18, "min": 23.64}, "VIN": {"avg": 69.31, "peak": 89.45, "min": 58.17}}, "power_watts_avg": 30.62, "energy_joules_est": 60.5, "sample_count": 15, "duration_seconds": 1.976}, "timestamp": "2026-01-17T15:42:47.808469"}
{"image_index": 371, "image_name": "000000041633.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000041633.jpg", "image_width": 640, "image_height": 429, "image_resolution": "640x429", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2286.7, "latencies_ms": [2286.7], "images_per_second": 0.437, "prompt_tokens": 19, "response_tokens_est": 58, "n_tiles": 6, "output_text": "The notable visual attributes of the red car in the image include its glossy finish, which reflects the surrounding environment, and its classic design reminiscent of mid-20th century automobiles. The lighting is natural, with a partly cloudy sky casting soft shadows on the car's surface.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24965.3, "ram_available_mb": 100806.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 24965.3, "ram_available_mb": 100806.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.83, "peak": 15.24, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.35, "peak": 15.75, "min": 14.56}, "VDD_GPU": {"avg": 29.85, "peak": 40.18, "min": 23.24}, "VIN": {"avg": 67.8, "peak": 102.57, "min": 53.5}}, "power_watts_avg": 29.85, "energy_joules_est": 68.27, "sample_count": 17, "duration_seconds": 2.287}, "timestamp": "2026-01-17T15:42:50.105650"}
{"image_index": 372, "image_name": "000000041635.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000041635.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2699.247, "latencies_ms": [2699.247], "images_per_second": 0.37, "prompt_tokens": 9, "response_tokens_est": 32, "n_tiles": 12, "output_text": "The image depicts a black and white photograph of a group of cows standing in a field, with a barbed wire fence running horizontally in the background.", "error": null, "sys_before": {"cpu_percent": 42.0, "ram_used_mb": 24934.9, "ram_available_mb": 100837.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 24936.7, "ram_available_mb": 100835.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.17, "peak": 16.66, "min": 13.49}, "VDD_CPU_SOC_MSS": {"avg": 15.9, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 35.54, "peak": 44.1, "min": 27.18}, "VIN": {"avg": 84.67, "peak": 130.06, "min": 65.97}}, "power_watts_avg": 35.54, "energy_joules_est": 95.94, "sample_count": 20, "duration_seconds": 2.7}, "timestamp": "2026-01-17T15:42:52.888257"}
{"image_index": 372, "image_name": "000000041635.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000041635.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2929.019, "latencies_ms": [2929.019], "images_per_second": 0.341, "prompt_tokens": 23, "response_tokens_est": 37, "n_tiles": 12, "output_text": "1. Cows\n2. Barbed wire\n3. Grass\n4. Trees\n5. Sky\n6. Farmland\n7. House\n8. Animals", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24936.7, "ram_available_mb": 100835.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 24937.9, "ram_available_mb": 100834.3, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.23, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.73, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 36.07, "peak": 45.28, "min": 26.79}, "VIN": {"avg": 82.4, "peak": 130.64, "min": 57.71}}, "power_watts_avg": 36.07, "energy_joules_est": 105.66, "sample_count": 22, "duration_seconds": 2.929}, "timestamp": "2026-01-17T15:42:55.823891"}
{"image_index": 372, "image_name": "000000041635.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000041635.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3992.783, "latencies_ms": [3992.783], "images_per_second": 0.25, "prompt_tokens": 27, "response_tokens_est": 68, "n_tiles": 12, "output_text": "The main objects in the image are a group of cows, with the foreground and background cows being the most prominent. The cows are positioned in a way that the foreground cow is closer to the camera, while the background cow is further away. The cows are standing in a field with a barbed wire fence running horizontally in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24937.9, "ram_available_mb": 100834.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 24938.4, "ram_available_mb": 100833.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.18, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.64, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 33.19, "peak": 44.89, "min": 25.6}, "VIN": {"avg": 72.81, "peak": 113.48, "min": 55.61}}, "power_watts_avg": 33.19, "energy_joules_est": 132.54, "sample_count": 31, "duration_seconds": 3.993}, "timestamp": "2026-01-17T15:42:59.824477"}
{"image_index": 372, "image_name": "000000041635.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000041635.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3582.494, "latencies_ms": [3582.494], "images_per_second": 0.279, "prompt_tokens": 21, "response_tokens_est": 56, "n_tiles": 12, "output_text": "The image depicts a rural scene with a barbed wire fence stretching across the frame. In the foreground, there are two cows, one white and one brown, standing close to the fence. The background shows a vast, open field with sparse vegetation and a few trees.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24938.4, "ram_available_mb": 100833.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 24939.3, "ram_available_mb": 100832.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.93, "peak": 16.36, "min": 14.12}, "VDD_CPU_SOC_MSS": {"avg": 15.47, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 33.8, "peak": 45.69, "min": 25.6}, "VIN": {"avg": 76.3, "peak": 117.69, "min": 57.16}}, "power_watts_avg": 33.8, "energy_joules_est": 121.1, "sample_count": 28, "duration_seconds": 3.583}, "timestamp": "2026-01-17T15:43:03.414311"}
{"image_index": 372, "image_name": "000000041635.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000041635.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 4136.645, "latencies_ms": [4136.645], "images_per_second": 0.242, "prompt_tokens": 19, "response_tokens_est": 72, "n_tiles": 12, "output_text": "The image is a black and white photograph featuring a rustic, rural scene. The notable visual attributes include a barbed wire fence stretching across the image, a few cows grazing in the foreground, and a partly cloudy sky. The lighting is natural, with shadows cast by the cows and the fence, and the overall mood is serene and peaceful.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24939.3, "ram_available_mb": 100832.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 24939.3, "ram_available_mb": 100832.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.9, "peak": 16.26, "min": 13.92}, "VDD_CPU_SOC_MSS": {"avg": 15.35, "peak": 17.32, "min": 14.17}, "VDD_GPU": {"avg": 32.85, "peak": 44.91, "min": 25.6}, "VIN": {"avg": 74.96, "peak": 117.13, "min": 59.67}}, "power_watts_avg": 32.85, "energy_joules_est": 135.9, "sample_count": 32, "duration_seconds": 4.137}, "timestamp": "2026-01-17T15:43:07.557935"}
{"image_index": 373, "image_name": "000000041872.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000041872.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2147.586, "latencies_ms": [2147.586], "images_per_second": 0.466, "prompt_tokens": 9, "response_tokens_est": 57, "n_tiles": 6, "output_text": "The image depicts a cozy, warmly lit bedroom with a wooden bed frame, a beige bedspread, and a wooden nightstand with a lamp. The room features a fireplace with a mantel, a beige armchair, and a window with closed blinds.", "error": null, "sys_before": {"cpu_percent": 41.2, "ram_used_mb": 24956.0, "ram_available_mb": 100816.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 24956.3, "ram_available_mb": 100815.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.77, "peak": 15.11, "min": 13.51}, "VDD_CPU_SOC_MSS": {"avg": 15.43, "peak": 15.75, "min": 14.17}, "VDD_GPU": {"avg": 30.35, "peak": 40.56, "min": 23.64}, "VIN": {"avg": 71.4, "peak": 100.04, "min": 64.25}}, "power_watts_avg": 30.35, "energy_joules_est": 65.19, "sample_count": 16, "duration_seconds": 2.148}, "timestamp": "2026-01-17T15:43:09.770719"}
{"image_index": 373, "image_name": "000000041872.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000041872.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2023.858, "latencies_ms": [2023.858], "images_per_second": 0.494, "prompt_tokens": 23, "response_tokens_est": 51, "n_tiles": 6, "output_text": "- bed: 1\n- nightstand: 1\n- lamp: 1\n- chair: 1\n- fireplace: 1\n- TV: 1\n- blinds: 1\n- wall clock: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24956.3, "ram_available_mb": 100815.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 24957.0, "ram_available_mb": 100815.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.04, "peak": 15.44, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.64, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 31.17, "peak": 40.16, "min": 24.03}, "VIN": {"avg": 72.45, "peak": 117.64, "min": 61.47}}, "power_watts_avg": 31.17, "energy_joules_est": 63.09, "sample_count": 15, "duration_seconds": 2.024}, "timestamp": "2026-01-17T15:43:11.800662"}
{"image_index": 373, "image_name": "000000041872.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000041872.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2732.867, "latencies_ms": [2732.867], "images_per_second": 0.366, "prompt_tokens": 27, "response_tokens_est": 77, "n_tiles": 6, "output_text": "The main objects in the image are a bed, a chair, and a fireplace. The bed is positioned in the foreground, with a cushion on the left side. The chair is placed near the bed, and the fireplace is situated to the right of the bed. The fireplace is the most prominent object in the image, with a dark stone surround and a visible fire burning inside.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24957.0, "ram_available_mb": 100815.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 24957.5, "ram_available_mb": 100814.7, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.09, "peak": 15.54, "min": 14.43}, "VDD_CPU_SOC_MSS": {"avg": 15.67, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 28.96, "peak": 39.77, "min": 23.64}, "VIN": {"avg": 71.64, "peak": 108.98, "min": 63.6}}, "power_watts_avg": 28.96, "energy_joules_est": 79.15, "sample_count": 21, "duration_seconds": 2.733}, "timestamp": "2026-01-17T15:43:14.539174"}
{"image_index": 373, "image_name": "000000041872.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000041872.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2778.637, "latencies_ms": [2778.637], "images_per_second": 0.36, "prompt_tokens": 21, "response_tokens_est": 79, "n_tiles": 6, "output_text": "The image depicts a cozy, warmly lit bedroom with a wooden ceiling and walls. The room features a large bed with a brown bedspread, a wooden nightstand with a lamp, and a beige armchair. There is a fireplace with a stone surround and a small television mounted on the wall above it. The room is well-lit, creating a comfortable and inviting atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24957.5, "ram_available_mb": 100814.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 24958.2, "ram_available_mb": 100814.0, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.09, "peak": 15.44, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.65, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 29.04, "peak": 40.56, "min": 23.64}, "VIN": {"avg": 71.86, "peak": 112.2, "min": 61.92}}, "power_watts_avg": 29.04, "energy_joules_est": 80.7, "sample_count": 21, "duration_seconds": 2.779}, "timestamp": "2026-01-17T15:43:17.323950"}
{"image_index": 373, "image_name": "000000041872.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000041872.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1801.02, "latencies_ms": [1801.02], "images_per_second": 0.555, "prompt_tokens": 19, "response_tokens_est": 43, "n_tiles": 6, "output_text": "The room is warmly lit with a soft, yellowish glow, creating a cozy and inviting atmosphere. The wooden ceiling and walls, along with the wooden furniture, contribute to the rustic charm of the space.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24958.2, "ram_available_mb": 100814.0, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 24959.4, "ram_available_mb": 100812.7, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.06, "peak": 15.44, "min": 14.43}, "VDD_CPU_SOC_MSS": {"avg": 15.61, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 31.54, "peak": 40.16, "min": 24.42}, "VIN": {"avg": 75.85, "peak": 122.54, "min": 58.84}}, "power_watts_avg": 31.54, "energy_joules_est": 56.82, "sample_count": 14, "duration_seconds": 1.802}, "timestamp": "2026-01-17T15:43:19.131027"}
{"image_index": 374, "image_name": "000000041888.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000041888.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2961.236, "latencies_ms": [2961.236], "images_per_second": 0.338, "prompt_tokens": 9, "response_tokens_est": 40, "n_tiles": 12, "output_text": "The image depicts a group of turkeys in a dry, grassy field, with one of the turkeys standing and another lying down, both displaying their characteristic speckled plumage.", "error": null, "sys_before": {"cpu_percent": 41.4, "ram_used_mb": 24931.9, "ram_available_mb": 100840.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 24932.9, "ram_available_mb": 100839.3, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.17, "peak": 16.66, "min": 13.59}, "VDD_CPU_SOC_MSS": {"avg": 15.93, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 34.49, "peak": 43.71, "min": 26.39}, "VIN": {"avg": 75.06, "peak": 105.66, "min": 60.76}}, "power_watts_avg": 34.49, "energy_joules_est": 102.14, "sample_count": 23, "duration_seconds": 2.962}, "timestamp": "2026-01-17T15:43:22.183605"}
{"image_index": 374, "image_name": "000000041888.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000041888.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2749.872, "latencies_ms": [2749.872], "images_per_second": 0.364, "prompt_tokens": 23, "response_tokens_est": 32, "n_tiles": 12, "output_text": "1. Turkey\n2. Turkey\n3. Turkey\n4. Turkey\n5. Turkey\n6. Turkey\n7. Turkey\n8. Turkey", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24932.9, "ram_available_mb": 100839.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 24934.1, "ram_available_mb": 100838.0, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.27, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.91, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 36.57, "peak": 45.28, "min": 27.19}, "VIN": {"avg": 83.75, "peak": 130.32, "min": 61.61}}, "power_watts_avg": 36.57, "energy_joules_est": 100.58, "sample_count": 21, "duration_seconds": 2.75}, "timestamp": "2026-01-17T15:43:24.939918"}
{"image_index": 374, "image_name": "000000041888.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000041888.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3234.343, "latencies_ms": [3234.343], "images_per_second": 0.309, "prompt_tokens": 27, "response_tokens_est": 46, "n_tiles": 12, "output_text": "The main objects in the image are a group of turkeys. The turkeys are positioned in the foreground, with one standing and the other two lying down. The background features a dry, grassy field with sparse vegetation.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24934.1, "ram_available_mb": 100838.0, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 24935.9, "ram_available_mb": 100836.3, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.22, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.81, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 34.88, "peak": 45.28, "min": 26.39}, "VIN": {"avg": 78.36, "peak": 126.1, "min": 58.67}}, "power_watts_avg": 34.88, "energy_joules_est": 112.83, "sample_count": 25, "duration_seconds": 3.235}, "timestamp": "2026-01-17T15:43:28.181847"}
{"image_index": 374, "image_name": "000000041888.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000041888.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3732.732, "latencies_ms": [3732.732], "images_per_second": 0.268, "prompt_tokens": 21, "response_tokens_est": 61, "n_tiles": 12, "output_text": "The image depicts a serene natural setting with dry, brown grass covering the ground. In the foreground, there are two turkeys standing and one of them is partially buried in the grass. The scene suggests a tranquil, possibly arid environment where the turkeys are foraging or resting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24935.9, "ram_available_mb": 100836.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 24936.1, "ram_available_mb": 100836.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.19, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.75, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 33.75, "peak": 45.3, "min": 26.0}, "VIN": {"avg": 75.51, "peak": 121.78, "min": 61.96}}, "power_watts_avg": 33.75, "energy_joules_est": 126.0, "sample_count": 29, "duration_seconds": 3.733}, "timestamp": "2026-01-17T15:43:31.922315"}
{"image_index": 374, "image_name": "000000041888.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000041888.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2925.888, "latencies_ms": [2925.888], "images_per_second": 0.342, "prompt_tokens": 19, "response_tokens_est": 37, "n_tiles": 12, "output_text": "The image depicts a scene with a dry, brown grassy field under a cloudy sky. The lighting is soft and diffused, casting a gentle glow over the landscape.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24936.1, "ram_available_mb": 100836.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 24936.1, "ram_available_mb": 100836.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.23, "peak": 16.56, "min": 14.12}, "VDD_CPU_SOC_MSS": {"avg": 15.83, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 35.69, "peak": 44.89, "min": 26.79}, "VIN": {"avg": 81.14, "peak": 118.82, "min": 66.93}}, "power_watts_avg": 35.69, "energy_joules_est": 104.44, "sample_count": 23, "duration_seconds": 2.926}, "timestamp": "2026-01-17T15:43:34.854855"}
{"image_index": 375, "image_name": "000000041990.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000041990.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 3072.658, "latencies_ms": [3072.658], "images_per_second": 0.325, "prompt_tokens": 9, "response_tokens_est": 43, "n_tiles": 12, "output_text": "Two skiers are standing on a snowy slope, each holding a ski pole and wearing appropriate winter gear, including helmets and goggles, as they enjoy their time skiing in the snowy environment.", "error": null, "sys_before": {"cpu_percent": 47.3, "ram_used_mb": 24931.9, "ram_available_mb": 100840.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 24930.6, "ram_available_mb": 100841.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.13, "peak": 16.66, "min": 13.29}, "VDD_CPU_SOC_MSS": {"avg": 15.85, "peak": 17.71, "min": 14.17}, "VDD_GPU": {"avg": 34.6, "peak": 44.12, "min": 26.39}, "VIN": {"avg": 80.04, "peak": 119.43, "min": 58.04}}, "power_watts_avg": 34.6, "energy_joules_est": 106.32, "sample_count": 23, "duration_seconds": 3.073}, "timestamp": "2026-01-17T15:43:38.033726"}
{"image_index": 375, "image_name": "000000041990.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000041990.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 3061.476, "latencies_ms": [3061.476], "images_per_second": 0.327, "prompt_tokens": 23, "response_tokens_est": 41, "n_tiles": 12, "output_text": "1. Ski poles\n2. Ski boots\n3. Ski skis\n4. Ski poles\n5. Ski poles\n6. Ski poles\n7. Ski poles\n8. Ski poles", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24930.6, "ram_available_mb": 100841.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 24930.4, "ram_available_mb": 100841.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.23, "peak": 16.56, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.81, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 35.41, "peak": 45.68, "min": 26.39}, "VIN": {"avg": 76.57, "peak": 110.51, "min": 49.0}}, "power_watts_avg": 35.41, "energy_joules_est": 108.42, "sample_count": 24, "duration_seconds": 3.062}, "timestamp": "2026-01-17T15:43:41.102825"}
{"image_index": 375, "image_name": "000000041990.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000041990.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 5927.137, "latencies_ms": [5927.137], "images_per_second": 0.169, "prompt_tokens": 27, "response_tokens_est": 125, "n_tiles": 12, "output_text": "The main objects in the image are two skiers, one on the left and one on the right. The skier on the left is closer to the foreground, while the skier on the right is further back. The skier on the left is standing on a snow-covered slope, holding ski poles, and wearing a red jacket. The skier on the right is also on a snow-covered slope, but is closer to the background, wearing a black jacket and holding ski poles. Both skiers are in the snowy environment, and the snow-covered slope is the central focus of the image.", "error": null, "sys_before": {"cpu_percent": 16.7, "ram_used_mb": 24930.4, "ram_available_mb": 100841.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 24930.9, "ram_available_mb": 100841.3, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.14, "peak": 16.56, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.58, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 30.82, "peak": 45.28, "min": 26.0}, "VIN": {"avg": 72.68, "peak": 114.64, "min": 56.27}}, "power_watts_avg": 30.82, "energy_joules_est": 182.69, "sample_count": 47, "duration_seconds": 5.928}, "timestamp": "2026-01-17T15:43:47.036782"}
{"image_index": 375, "image_name": "000000041990.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000041990.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 4046.18, "latencies_ms": [4046.18], "images_per_second": 0.247, "prompt_tokens": 21, "response_tokens_est": 70, "n_tiles": 12, "output_text": "The scene depicts a snowy landscape with a group of people engaged in skiing. The individuals are dressed in winter gear, including jackets, pants, and goggles, and are using ski poles to navigate the snowy terrain. The setting is a snowy mountainous area, and the individuals appear to be enjoying their time skiing.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24930.9, "ram_available_mb": 100841.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 24932.0, "ram_available_mb": 100840.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.18, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.86, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 33.15, "peak": 44.91, "min": 26.0}, "VIN": {"avg": 74.15, "peak": 109.64, "min": 60.23}}, "power_watts_avg": 33.15, "energy_joules_est": 134.15, "sample_count": 31, "duration_seconds": 4.047}, "timestamp": "2026-01-17T15:43:51.093915"}
{"image_index": 375, "image_name": "000000041990.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000041990.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 4561.914, "latencies_ms": [4561.914], "images_per_second": 0.219, "prompt_tokens": 19, "response_tokens_est": 85, "n_tiles": 12, "output_text": "The image depicts a snowy landscape with two skiers, one in a white jacket and the other in a dark jacket, both wearing ski goggles and holding ski poles. The snow is white and appears to be freshly fallen, with trees heavily laden with snow in the background. The lighting is soft and diffused, likely due to the overcast sky, and the overall atmosphere is serene and peaceful.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24932.0, "ram_available_mb": 100840.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 24932.0, "ram_available_mb": 100840.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.16, "peak": 16.66, "min": 14.12}, "VDD_CPU_SOC_MSS": {"avg": 15.59, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 32.1, "peak": 45.28, "min": 25.6}, "VIN": {"avg": 76.22, "peak": 127.54, "min": 57.78}}, "power_watts_avg": 32.1, "energy_joules_est": 146.45, "sample_count": 36, "duration_seconds": 4.562}, "timestamp": "2026-01-17T15:43:55.662718"}
{"image_index": 376, "image_name": "000000042070.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042070.jpg", "image_width": 640, "image_height": 512, "image_resolution": "640x512", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 3073.82, "latencies_ms": [3073.82], "images_per_second": 0.325, "prompt_tokens": 9, "response_tokens_est": 43, "n_tiles": 12, "output_text": "The image shows a white bus with a digital display showing the number 51 and the word \"CROSSTOWN\" on its front, parked on a street with a partly cloudy sky in the background.", "error": null, "sys_before": {"cpu_percent": 39.6, "ram_used_mb": 24936.1, "ram_available_mb": 100836.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 24937.8, "ram_available_mb": 100834.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.13, "peak": 16.66, "min": 13.29}, "VDD_CPU_SOC_MSS": {"avg": 15.79, "peak": 17.71, "min": 14.18}, "VDD_GPU": {"avg": 34.25, "peak": 44.1, "min": 26.39}, "VIN": {"avg": 76.23, "peak": 104.1, "min": 61.52}}, "power_watts_avg": 34.25, "energy_joules_est": 105.29, "sample_count": 24, "duration_seconds": 3.074}, "timestamp": "2026-01-17T15:43:58.843939"}
{"image_index": 376, "image_name": "000000042070.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042070.jpg", "image_width": 640, "image_height": 512, "image_resolution": "640x512", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 3559.626, "latencies_ms": [3559.626], "images_per_second": 0.281, "prompt_tokens": 23, "response_tokens_est": 55, "n_tiles": 12, "output_text": "- Bus: 1\n- Bus number display: 1\n- Building: 1\n- Street light: 1\n- Street sign: 1\n- Car: 1\n- License plate: 1\n- License plate number: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24937.8, "ram_available_mb": 100834.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 24938.3, "ram_available_mb": 100833.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.04, "peak": 16.46, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.51, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 34.09, "peak": 44.89, "min": 25.6}, "VIN": {"avg": 73.18, "peak": 107.71, "min": 61.12}}, "power_watts_avg": 34.09, "energy_joules_est": 121.36, "sample_count": 27, "duration_seconds": 3.56}, "timestamp": "2026-01-17T15:44:02.409939"}
{"image_index": 376, "image_name": "000000042070.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042070.jpg", "image_width": 640, "image_height": 512, "image_resolution": "640x512", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3036.642, "latencies_ms": [3036.642], "images_per_second": 0.329, "prompt_tokens": 27, "response_tokens_est": 40, "n_tiles": 12, "output_text": "The bus is positioned in the foreground, with its front facing the camera. The license plate is visible on the front of the bus. The background features a building and a partly cloudy sky.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24938.3, "ram_available_mb": 100833.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 24938.3, "ram_available_mb": 100833.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.96, "peak": 16.36, "min": 13.92}, "VDD_CPU_SOC_MSS": {"avg": 15.56, "peak": 17.32, "min": 14.17}, "VDD_GPU": {"avg": 35.59, "peak": 45.3, "min": 26.0}, "VIN": {"avg": 78.81, "peak": 130.82, "min": 62.88}}, "power_watts_avg": 35.59, "energy_joules_est": 108.09, "sample_count": 23, "duration_seconds": 3.037}, "timestamp": "2026-01-17T15:44:05.453063"}
{"image_index": 376, "image_name": "000000042070.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042070.jpg", "image_width": 640, "image_height": 512, "image_resolution": "640x512", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3163.683, "latencies_ms": [3163.683], "images_per_second": 0.316, "prompt_tokens": 21, "response_tokens_est": 44, "n_tiles": 12, "output_text": "The image depicts a white bus with a digital display showing the number \"51\" and \"CROSSTOWN\" in red letters. The bus is parked on a street with a clear sky in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24938.3, "ram_available_mb": 100833.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 24938.5, "ram_available_mb": 100833.7, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.97, "peak": 16.36, "min": 14.02}, "VDD_CPU_SOC_MSS": {"avg": 15.57, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 35.37, "peak": 45.3, "min": 26.0}, "VIN": {"avg": 74.41, "peak": 102.09, "min": 59.56}}, "power_watts_avg": 35.37, "energy_joules_est": 111.92, "sample_count": 24, "duration_seconds": 3.164}, "timestamp": "2026-01-17T15:44:08.623373"}
{"image_index": 376, "image_name": "000000042070.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042070.jpg", "image_width": 640, "image_height": 512, "image_resolution": "640x512", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3552.164, "latencies_ms": [3552.164], "images_per_second": 0.282, "prompt_tokens": 19, "response_tokens_est": 55, "n_tiles": 12, "output_text": "The bus in the image is predominantly white with blue and green accents. The digital display shows the number \"51\" and \"CROSSTOWN\" in red. The lighting is bright and natural, indicating daytime. The weather appears to be clear with a blue sky.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24938.5, "ram_available_mb": 100833.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 24937.6, "ram_available_mb": 100834.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.91, "peak": 16.36, "min": 13.9}, "VDD_CPU_SOC_MSS": {"avg": 15.54, "peak": 17.32, "min": 14.17}, "VDD_GPU": {"avg": 34.22, "peak": 45.69, "min": 25.61}, "VIN": {"avg": 75.06, "peak": 113.02, "min": 63.44}}, "power_watts_avg": 34.22, "energy_joules_est": 121.57, "sample_count": 27, "duration_seconds": 3.553}, "timestamp": "2026-01-17T15:44:12.183235"}
{"image_index": 377, "image_name": "000000042102.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042102.jpg", "image_width": 246, "image_height": 640, "image_resolution": "246x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2077.891, "latencies_ms": [2077.891], "images_per_second": 0.481, "prompt_tokens": 9, "response_tokens_est": 27, "n_tiles": 10, "output_text": "The image shows a man dressed in a formal blue suit, white shirt, and striped tie, standing against a white wall.", "error": null, "sys_before": {"cpu_percent": 40.9, "ram_used_mb": 24941.1, "ram_available_mb": 100831.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 24930.3, "ram_available_mb": 100841.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4613.5, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 6643.4, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.75, "peak": 15.85, "min": 13.19}, "VDD_CPU_SOC_MSS": {"avg": 15.45, "peak": 16.92, "min": 14.17}, "VDD_GPU": {"avg": 35.77, "peak": 42.15, "min": 27.18}, "VIN": {"avg": 84.1, "peak": 131.92, "min": 66.9}}, "power_watts_avg": 35.77, "energy_joules_est": 74.34, "sample_count": 16, "duration_seconds": 2.078}, "timestamp": "2026-01-17T15:44:14.358004"}
{"image_index": 377, "image_name": "000000042102.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042102.jpg", "image_width": 246, "image_height": 640, "image_resolution": "246x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2298.298, "latencies_ms": [2298.298], "images_per_second": 0.435, "prompt_tokens": 23, "response_tokens_est": 33, "n_tiles": 10, "output_text": "1. man\n2. suit\n3. tie\n4. skirt\n5. tights\n6. shoes\n7. bag\n8. wall", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24930.3, "ram_available_mb": 100841.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 24930.8, "ram_available_mb": 100841.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4613.5, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 6654.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.95, "peak": 16.05, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.54, "peak": 16.92, "min": 14.56}, "VDD_GPU": {"avg": 36.49, "peak": 44.1, "min": 27.18}, "VIN": {"avg": 75.1, "peak": 129.53, "min": 51.19}}, "power_watts_avg": 36.49, "energy_joules_est": 83.88, "sample_count": 17, "duration_seconds": 2.299}, "timestamp": "2026-01-17T15:44:16.667015"}
{"image_index": 377, "image_name": "000000042102.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042102.jpg", "image_width": 246, "image_height": 640, "image_resolution": "246x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3593.717, "latencies_ms": [3593.717], "images_per_second": 0.278, "prompt_tokens": 27, "response_tokens_est": 74, "n_tiles": 10, "output_text": "The main object in the image is a young man standing in front of a plain white wall. He is dressed in a dark blue suit, white shirt, and striped tie. The man is holding a black handbag in his left hand and has his right hand slightly extended. The background is a plain white wall, and the man is positioned in the foreground.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24930.8, "ram_available_mb": 100841.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 24931.5, "ram_available_mb": 100840.7, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4613.5, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 6657.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.94, "peak": 16.05, "min": 14.12}, "VDD_CPU_SOC_MSS": {"avg": 15.42, "peak": 16.92, "min": 14.56}, "VDD_GPU": {"avg": 32.11, "peak": 43.71, "min": 25.21}, "VIN": {"avg": 74.17, "peak": 111.86, "min": 63.87}}, "power_watts_avg": 32.11, "energy_joules_est": 115.4, "sample_count": 28, "duration_seconds": 3.594}, "timestamp": "2026-01-17T15:44:20.267129"}
{"image_index": 377, "image_name": "000000042102.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042102.jpg", "image_width": 246, "image_height": 640, "image_resolution": "246x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3019.005, "latencies_ms": [3019.005], "images_per_second": 0.331, "prompt_tokens": 21, "response_tokens_est": 56, "n_tiles": 10, "output_text": "The image depicts a man dressed in a formal blue suit and tie standing against a plain white wall. He is holding a black handbag in his left hand and has his right hand slightly extended. The setting appears to be indoors, possibly in a professional or formal environment.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24931.5, "ram_available_mb": 100840.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 24932.2, "ram_available_mb": 100840.0, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4613.5, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 6652.6, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.94, "peak": 15.95, "min": 14.1}, "VDD_CPU_SOC_MSS": {"avg": 15.51, "peak": 16.92, "min": 14.56}, "VDD_GPU": {"avg": 33.67, "peak": 43.71, "min": 25.6}, "VIN": {"avg": 74.39, "peak": 124.44, "min": 54.23}}, "power_watts_avg": 33.67, "energy_joules_est": 101.67, "sample_count": 23, "duration_seconds": 3.019}, "timestamp": "2026-01-17T15:44:23.292684"}
{"image_index": 377, "image_name": "000000042102.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042102.jpg", "image_width": 246, "image_height": 640, "image_resolution": "246x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2737.716, "latencies_ms": [2737.716], "images_per_second": 0.365, "prompt_tokens": 19, "response_tokens_est": 47, "n_tiles": 10, "output_text": "The man is dressed in a dark blue suit with a white shirt and a striped tie. The lighting is bright, casting a clear shadow of the man on the wall. The man is wearing black tights and black shoes.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24932.2, "ram_available_mb": 100840.0, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 24933.2, "ram_available_mb": 100839.0, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4613.5, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 6651.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.95, "peak": 15.95, "min": 14.2}, "VDD_CPU_SOC_MSS": {"avg": 15.46, "peak": 16.92, "min": 14.56}, "VDD_GPU": {"avg": 34.25, "peak": 43.71, "min": 25.6}, "VIN": {"avg": 76.92, "peak": 130.78, "min": 60.17}}, "power_watts_avg": 34.25, "energy_joules_est": 93.78, "sample_count": 21, "duration_seconds": 2.738}, "timestamp": "2026-01-17T15:44:26.036897"}
{"image_index": 378, "image_name": "000000042178.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042178.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2903.069, "latencies_ms": [2903.069], "images_per_second": 0.344, "prompt_tokens": 9, "response_tokens_est": 38, "n_tiles": 12, "output_text": "The image depicts a large, industrial facility with numerous tall, metallic structures, possibly part of a power plant or a large warehouse, surrounded by a hazy, foggy atmosphere.", "error": null, "sys_before": {"cpu_percent": 44.8, "ram_used_mb": 24921.1, "ram_available_mb": 100851.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 24919.3, "ram_available_mb": 100852.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.14, "peak": 16.76, "min": 13.19}, "VDD_CPU_SOC_MSS": {"avg": 15.78, "peak": 17.71, "min": 14.17}, "VDD_GPU": {"avg": 35.2, "peak": 44.12, "min": 26.79}, "VIN": {"avg": 76.91, "peak": 104.96, "min": 66.04}}, "power_watts_avg": 35.2, "energy_joules_est": 102.2, "sample_count": 22, "duration_seconds": 2.903}, "timestamp": "2026-01-17T15:44:29.052111"}
{"image_index": 378, "image_name": "000000042178.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042178.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 3025.711, "latencies_ms": [3025.711], "images_per_second": 0.331, "prompt_tokens": 23, "response_tokens_est": 40, "n_tiles": 12, "output_text": "1. Power lines\n2. Power lines\n3. Power lines\n4. Power lines\n5. Power lines\n6. Power lines\n7. Power lines\n8. Power lines", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24919.3, "ram_available_mb": 100852.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 24918.4, "ram_available_mb": 100853.7, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.24, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.83, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 35.65, "peak": 45.28, "min": 26.8}, "VIN": {"avg": 77.6, "peak": 117.77, "min": 67.08}}, "power_watts_avg": 35.65, "energy_joules_est": 107.88, "sample_count": 23, "duration_seconds": 3.026}, "timestamp": "2026-01-17T15:44:32.088440"}
{"image_index": 378, "image_name": "000000042178.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042178.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3404.051, "latencies_ms": [3404.051], "images_per_second": 0.294, "prompt_tokens": 27, "response_tokens_est": 51, "n_tiles": 12, "output_text": "The main objects in the image are a series of train tracks and overhead electric lines. The tracks are located in the foreground, while the electric lines stretch across the background. The train tracks are near the foreground, and the electric lines are further back.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24918.4, "ram_available_mb": 100853.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 24914.3, "ram_available_mb": 100857.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.2, "peak": 16.66, "min": 14.12}, "VDD_CPU_SOC_MSS": {"avg": 15.88, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 34.63, "peak": 45.28, "min": 26.0}, "VIN": {"avg": 82.09, "peak": 123.55, "min": 63.71}}, "power_watts_avg": 34.63, "energy_joules_est": 117.9, "sample_count": 26, "duration_seconds": 3.405}, "timestamp": "2026-01-17T15:44:35.498714"}
{"image_index": 378, "image_name": "000000042178.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042178.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 4110.695, "latencies_ms": [4110.695], "images_per_second": 0.243, "prompt_tokens": 21, "response_tokens_est": 72, "n_tiles": 12, "output_text": "The image depicts a large, industrial setting with a series of metallic structures, possibly part of a factory or a warehouse. The scene is bathed in a warm, yellowish light, suggesting either early morning or late afternoon. The structures are filled with numerous wires and cables, and there are signs with numbers indicating specific locations or sections within the facility.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24914.3, "ram_available_mb": 100857.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 24915.3, "ram_available_mb": 100856.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.18, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.73, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 32.93, "peak": 44.91, "min": 26.0}, "VIN": {"avg": 76.78, "peak": 129.81, "min": 57.05}}, "power_watts_avg": 32.93, "energy_joules_est": 135.38, "sample_count": 32, "duration_seconds": 4.111}, "timestamp": "2026-01-17T15:44:39.616262"}
{"image_index": 378, "image_name": "000000042178.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042178.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 4897.537, "latencies_ms": [4897.537], "images_per_second": 0.204, "prompt_tokens": 19, "response_tokens_est": 95, "n_tiles": 12, "output_text": "The image depicts a scene with a hazy, overcast sky, casting a soft, diffused light over the entire scene. The lighting is muted, with a warm, yellowish tint, creating a serene and somewhat melancholic atmosphere. The materials in the scene include a series of metallic structures, possibly part of a train or industrial setup, and the overall color palette is dominated by shades of gray and brown, with occasional hints of metallic sheen.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24915.3, "ram_available_mb": 100856.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 24916.5, "ram_available_mb": 100855.7, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.15, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.77, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 31.82, "peak": 44.89, "min": 26.0}, "VIN": {"avg": 73.59, "peak": 117.09, "min": 61.25}}, "power_watts_avg": 31.82, "energy_joules_est": 155.85, "sample_count": 38, "duration_seconds": 4.898}, "timestamp": "2026-01-17T15:44:44.520230"}
{"image_index": 379, "image_name": "000000042276.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042276.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2490.205, "latencies_ms": [2490.205], "images_per_second": 0.402, "prompt_tokens": 9, "response_tokens_est": 26, "n_tiles": 12, "output_text": "The image shows a bathroom with a toilet, a shower curtain, a towel rack, and various shoes scattered on the floor.", "error": null, "sys_before": {"cpu_percent": 44.7, "ram_used_mb": 24930.3, "ram_available_mb": 100841.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.6, "ram_used_mb": 24925.8, "ram_available_mb": 100846.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.17, "peak": 16.66, "min": 13.39}, "VDD_CPU_SOC_MSS": {"avg": 15.95, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 36.2, "peak": 44.12, "min": 27.57}, "VIN": {"avg": 77.19, "peak": 110.01, "min": 65.34}}, "power_watts_avg": 36.2, "energy_joules_est": 90.16, "sample_count": 19, "duration_seconds": 2.49}, "timestamp": "2026-01-17T15:44:47.115317"}
{"image_index": 379, "image_name": "000000042276.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042276.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 3053.404, "latencies_ms": [3053.404], "images_per_second": 0.328, "prompt_tokens": 23, "response_tokens_est": 41, "n_tiles": 12, "output_text": "1. Toilet\n2. Shoebox\n3. Gloves\n4. Gloves\n5. Gloves\n6. Gloves\n7. Gloves\n8. Gloves", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24925.8, "ram_available_mb": 100846.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 24919.6, "ram_available_mb": 100852.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.29, "peak": 16.66, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.96, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 35.61, "peak": 45.28, "min": 26.39}, "VIN": {"avg": 78.94, "peak": 114.91, "min": 62.02}}, "power_watts_avg": 35.61, "energy_joules_est": 108.75, "sample_count": 24, "duration_seconds": 3.054}, "timestamp": "2026-01-17T15:44:50.175321"}
{"image_index": 379, "image_name": "000000042276.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042276.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3641.742, "latencies_ms": [3641.742], "images_per_second": 0.275, "prompt_tokens": 27, "response_tokens_est": 58, "n_tiles": 12, "output_text": "The main objects in the image are a toilet, a pair of shoes, and a helmet. The toilet is located in the background, while the shoes and helmet are in the foreground. The shoes are near the toilet, and the helmet is on the floor in front of the toilet.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24919.6, "ram_available_mb": 100852.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 24918.1, "ram_available_mb": 100854.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.21, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.84, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 33.89, "peak": 45.68, "min": 26.0}, "VIN": {"avg": 76.93, "peak": 118.4, "min": 60.96}}, "power_watts_avg": 33.89, "energy_joules_est": 123.43, "sample_count": 29, "duration_seconds": 3.642}, "timestamp": "2026-01-17T15:44:53.823481"}
{"image_index": 379, "image_name": "000000042276.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042276.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3459.327, "latencies_ms": [3459.327], "images_per_second": 0.289, "prompt_tokens": 21, "response_tokens_est": 53, "n_tiles": 12, "output_text": "The image depicts a cluttered bathroom floor with various items scattered around. The bathroom has a white toilet, a green stool with a plant on it, a pair of shoes, a black and white helmet, and a white towel hanging on the wall.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24918.1, "ram_available_mb": 100854.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 24919.8, "ram_available_mb": 100852.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.23, "peak": 16.66, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.79, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 34.34, "peak": 44.91, "min": 26.0}, "VIN": {"avg": 77.05, "peak": 112.11, "min": 66.18}}, "power_watts_avg": 34.34, "energy_joules_est": 118.81, "sample_count": 27, "duration_seconds": 3.46}, "timestamp": "2026-01-17T15:44:57.290723"}
{"image_index": 379, "image_name": "000000042276.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042276.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3026.394, "latencies_ms": [3026.394], "images_per_second": 0.33, "prompt_tokens": 19, "response_tokens_est": 40, "n_tiles": 12, "output_text": "The bathroom is well-lit with natural light, creating a bright and airy atmosphere. The walls are painted in a light color, complementing the overall clean and tidy look of the space.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24919.8, "ram_available_mb": 100852.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 24920.2, "ram_available_mb": 100851.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.22, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.91, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 35.48, "peak": 45.68, "min": 26.39}, "VIN": {"avg": 79.43, "peak": 129.23, "min": 59.11}}, "power_watts_avg": 35.48, "energy_joules_est": 107.39, "sample_count": 24, "duration_seconds": 3.027}, "timestamp": "2026-01-17T15:45:00.323933"}
{"image_index": 380, "image_name": "000000042296.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042296.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1355.927, "latencies_ms": [1355.927], "images_per_second": 0.738, "prompt_tokens": 9, "response_tokens_est": 28, "n_tiles": 6, "output_text": "A polar bear is seen playfully splashing in a pool of water, with one of its paws gripping a green ball.", "error": null, "sys_before": {"cpu_percent": 37.5, "ram_used_mb": 24956.8, "ram_available_mb": 100815.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 24957.3, "ram_available_mb": 100814.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.6, "peak": 15.11, "min": 13.61}, "VDD_CPU_SOC_MSS": {"avg": 15.27, "peak": 16.14, "min": 14.17}, "VDD_GPU": {"avg": 34.03, "peak": 40.56, "min": 26.79}, "VIN": {"avg": 79.07, "peak": 107.3, "min": 61.8}}, "power_watts_avg": 34.03, "energy_joules_est": 46.15, "sample_count": 10, "duration_seconds": 1.356}, "timestamp": "2026-01-17T15:45:01.748621"}
{"image_index": 380, "image_name": "000000042296.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042296.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 974.42, "latencies_ms": [974.42], "images_per_second": 1.026, "prompt_tokens": 23, "response_tokens_est": 12, "n_tiles": 6, "output_text": "ball: 3\npolar bear: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24957.3, "ram_available_mb": 100814.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 24958.5, "ram_available_mb": 100813.7, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.82, "peak": 15.34, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.29, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 37.7, "peak": 41.36, "min": 32.7}, "VIN": {"avg": 78.26, "peak": 97.68, "min": 54.03}}, "power_watts_avg": 37.7, "energy_joules_est": 36.75, "sample_count": 7, "duration_seconds": 0.975}, "timestamp": "2026-01-17T15:45:02.729376"}
{"image_index": 380, "image_name": "000000042296.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042296.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2367.748, "latencies_ms": [2367.748], "images_per_second": 0.422, "prompt_tokens": 27, "response_tokens_est": 62, "n_tiles": 6, "output_text": "In the image, the main objects are a polar bear and a ball. The polar bear is in the foreground, swimming near the green and yellow balls. The green ball is near the polar bear, while the yellow ball is further away. The background features a sandy area with rocks and some vegetation.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24958.5, "ram_available_mb": 100813.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 24958.5, "ram_available_mb": 100813.7, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.95, "peak": 15.24, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.51, "peak": 15.75, "min": 14.56}, "VDD_GPU": {"avg": 30.79, "peak": 42.53, "min": 23.24}, "VIN": {"avg": 68.3, "peak": 89.69, "min": 61.38}}, "power_watts_avg": 30.79, "energy_joules_est": 72.92, "sample_count": 18, "duration_seconds": 2.368}, "timestamp": "2026-01-17T15:45:05.103569"}
{"image_index": 380, "image_name": "000000042296.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042296.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3035.024, "latencies_ms": [3035.024], "images_per_second": 0.329, "prompt_tokens": 21, "response_tokens_est": 85, "n_tiles": 6, "output_text": "The image depicts a polar bear in a shallow body of water, likely a pool or a shallow river, engaging in playful behavior. The bear is seen holding a ball in its mouth, with its paws submerged in the water, creating a dynamic and lively scene. The setting appears to be an outdoor environment, possibly a zoo or a wildlife sanctuary, given the presence of rocks and natural elements in the background.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 24958.8, "ram_available_mb": 100813.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 24959.0, "ram_available_mb": 100813.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.95, "peak": 15.24, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.63, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 28.22, "peak": 40.57, "min": 22.85}, "VIN": {"avg": 70.04, "peak": 100.71, "min": 62.15}}, "power_watts_avg": 28.22, "energy_joules_est": 85.66, "sample_count": 23, "duration_seconds": 3.035}, "timestamp": "2026-01-17T15:45:08.145797"}
{"image_index": 380, "image_name": "000000042296.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042296.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2281.916, "latencies_ms": [2281.916], "images_per_second": 0.438, "prompt_tokens": 19, "response_tokens_est": 58, "n_tiles": 6, "output_text": "The image depicts a polar bear swimming in a body of water, with its fur appearing white and its paws and head partially submerged. The lighting is natural, suggesting it is daytime, and the water is clear, allowing visibility of the bear's fur and the surrounding environment.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24959.0, "ram_available_mb": 100813.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 24959.2, "ram_available_mb": 100812.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.87, "peak": 15.34, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.4, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 29.62, "peak": 40.16, "min": 23.25}, "VIN": {"avg": 73.57, "peak": 117.1, "min": 57.05}}, "power_watts_avg": 29.62, "energy_joules_est": 67.6, "sample_count": 17, "duration_seconds": 2.282}, "timestamp": "2026-01-17T15:45:10.434025"}
{"image_index": 381, "image_name": "000000042528.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042528.jpg", "image_width": 640, "image_height": 495, "image_resolution": "640x495", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2422.525, "latencies_ms": [2422.525], "images_per_second": 0.413, "prompt_tokens": 9, "response_tokens_est": 24, "n_tiles": 12, "output_text": "A person is holding a Samsung flip phone in their hand, with a pair of blue jeans visible in the foreground.", "error": null, "sys_before": {"cpu_percent": 43.2, "ram_used_mb": 24926.6, "ram_available_mb": 100845.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.4, "ram_used_mb": 24928.6, "ram_available_mb": 100843.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.19, "peak": 16.66, "min": 13.39}, "VDD_CPU_SOC_MSS": {"avg": 16.01, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 36.7, "peak": 44.12, "min": 28.37}, "VIN": {"avg": 84.0, "peak": 113.48, "min": 56.29}}, "power_watts_avg": 36.7, "energy_joules_est": 88.92, "sample_count": 18, "duration_seconds": 2.423}, "timestamp": "2026-01-17T15:45:12.953866"}
{"image_index": 381, "image_name": "000000042528.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042528.jpg", "image_width": 640, "image_height": 495, "image_resolution": "640x495", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2414.377, "latencies_ms": [2414.377], "images_per_second": 0.414, "prompt_tokens": 23, "response_tokens_est": 22, "n_tiles": 12, "output_text": "cell phone: 1\nclothes: 1\nfloor: 1\ntable: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24928.6, "ram_available_mb": 100843.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.3, "ram_used_mb": 24928.6, "ram_available_mb": 100843.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.32, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.99, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 38.12, "peak": 45.68, "min": 28.76}, "VIN": {"avg": 87.0, "peak": 135.84, "min": 56.17}}, "power_watts_avg": 38.12, "energy_joules_est": 92.05, "sample_count": 18, "duration_seconds": 2.415}, "timestamp": "2026-01-17T15:45:15.375057"}
{"image_index": 381, "image_name": "000000042528.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042528.jpg", "image_width": 640, "image_height": 495, "image_resolution": "640x495", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3132.11, "latencies_ms": [3132.11], "images_per_second": 0.319, "prompt_tokens": 27, "response_tokens_est": 43, "n_tiles": 12, "output_text": "The main object in the foreground is a person's hand holding a mobile phone. The phone is positioned near the person's hand. The background features a wooden floor and a window with a view of the sky.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24928.6, "ram_available_mb": 100843.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 24929.3, "ram_available_mb": 100842.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.25, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.81, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 35.45, "peak": 45.28, "min": 26.39}, "VIN": {"avg": 73.99, "peak": 106.11, "min": 61.94}}, "power_watts_avg": 35.45, "energy_joules_est": 111.04, "sample_count": 24, "duration_seconds": 3.132}, "timestamp": "2026-01-17T15:45:18.513653"}
{"image_index": 381, "image_name": "000000042528.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042528.jpg", "image_width": 640, "image_height": 495, "image_resolution": "640x495", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3293.359, "latencies_ms": [3293.359], "images_per_second": 0.304, "prompt_tokens": 21, "response_tokens_est": 48, "n_tiles": 12, "output_text": "The image depicts a person holding a Samsung flip phone, which is placed on a wooden surface. The background shows a wooden floor and a window with a view of the sky. The person's hand is visible, holding the phone.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24929.3, "ram_available_mb": 100842.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 24930.3, "ram_available_mb": 100841.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.25, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.78, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 34.77, "peak": 44.89, "min": 26.0}, "VIN": {"avg": 76.26, "peak": 106.55, "min": 58.54}}, "power_watts_avg": 34.77, "energy_joules_est": 114.52, "sample_count": 25, "duration_seconds": 3.294}, "timestamp": "2026-01-17T15:45:21.814377"}
{"image_index": 381, "image_name": "000000042528.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042528.jpg", "image_width": 640, "image_height": 495, "image_resolution": "640x495", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3128.618, "latencies_ms": [3128.618], "images_per_second": 0.32, "prompt_tokens": 19, "response_tokens_est": 43, "n_tiles": 12, "output_text": "The image features a wooden floor with a clear, bright sky visible through a skylight above. The lighting is natural, casting soft shadows and highlighting the textures of the wooden planks and the metallic phone.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24930.3, "ram_available_mb": 100841.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 24930.3, "ram_available_mb": 100841.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.23, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.75, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 35.13, "peak": 45.68, "min": 26.39}, "VIN": {"avg": 76.26, "peak": 129.84, "min": 55.97}}, "power_watts_avg": 35.13, "energy_joules_est": 109.94, "sample_count": 24, "duration_seconds": 3.129}, "timestamp": "2026-01-17T15:45:24.949963"}
{"image_index": 382, "image_name": "000000042563.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042563.jpg", "image_width": 624, "image_height": 640, "image_resolution": "624x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 564.803, "latencies_ms": [564.803], "images_per_second": 1.771, "prompt_tokens": 9, "response_tokens_est": 22, "n_tiles": 1, "output_text": "A snowy train is traveling on tracks through a snowy landscape with trees and a cloudy sky.", "error": null, "sys_before": {"cpu_percent": 12.1, "ram_used_mb": 24930.8, "ram_available_mb": 100841.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 24931.0, "ram_available_mb": 100841.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 4845.7, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.86, "peak": 15.22, "min": 14.4}, "VDD_CPU_SOC_MSS": {"avg": 15.55, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 24.82, "peak": 27.18, "min": 22.85}, "VIN": {"avg": 63.3, "peak": 65.84, "min": 61.13}}, "power_watts_avg": 24.82, "energy_joules_est": 14.03, "sample_count": 4, "duration_seconds": 0.565}, "timestamp": "2026-01-17T15:45:25.544753"}
{"image_index": 382, "image_name": "000000042563.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042563.jpg", "image_width": 624, "image_height": 640, "image_resolution": "624x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2839.892, "latencies_ms": [2839.892], "images_per_second": 0.352, "prompt_tokens": 23, "response_tokens_est": 129, "n_tiles": 1, "output_text": "train: 1\ntrees: 3\ntrees: 3\ntrees: 3\ntrees: 3\ntrees: 3\ntrees: 3\ntrees: 3\ntrees: 3\ntrees: 3\ntrees: 3\ntrees: 3\ntrees: 3\ntrees: 3\ntrees: 3\ntrees: 3\ntrees: 3\ntrees: 3\ntrees: 3\ntrees: 3\ntrees: 3\ntrees:", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24931.0, "ram_available_mb": 100841.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24931.2, "ram_available_mb": 100840.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 4856.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.6, "peak": 15.82, "min": 15.01}, "VDD_CPU_SOC_MSS": {"avg": 16.59, "peak": 16.94, "min": 15.76}, "VDD_GPU": {"avg": 20.97, "peak": 25.59, "min": 20.09}, "VIN": {"avg": 63.81, "peak": 69.79, "min": 58.42}}, "power_watts_avg": 20.97, "energy_joules_est": 59.57, "sample_count": 21, "duration_seconds": 2.841}, "timestamp": "2026-01-17T15:45:28.391208"}
{"image_index": 382, "image_name": "000000042563.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042563.jpg", "image_width": 624, "image_height": 640, "image_resolution": "624x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1601.484, "latencies_ms": [1601.484], "images_per_second": 0.624, "prompt_tokens": 27, "response_tokens_est": 71, "n_tiles": 1, "output_text": "The main object in the foreground is a yellow train moving on the tracks. The train is positioned near the center of the image, slightly to the left. The background features a snowy landscape with trees and a snow-covered ground. The train is in the middle of the scene, with the trees and snow-covered ground extending towards the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24931.2, "ram_available_mb": 100840.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24932.5, "ram_available_mb": 100839.7, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 4859.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.55, "peak": 15.72, "min": 15.22}, "VDD_CPU_SOC_MSS": {"avg": 16.54, "peak": 16.93, "min": 16.14}, "VDD_GPU": {"avg": 21.21, "peak": 24.03, "min": 20.09}, "VIN": {"avg": 63.47, "peak": 67.88, "min": 60.35}}, "power_watts_avg": 21.21, "energy_joules_est": 33.98, "sample_count": 12, "duration_seconds": 1.602}, "timestamp": "2026-01-17T15:45:29.998780"}
{"image_index": 382, "image_name": "000000042563.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042563.jpg", "image_width": 624, "image_height": 640, "image_resolution": "624x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1492.806, "latencies_ms": [1492.806], "images_per_second": 0.67, "prompt_tokens": 21, "response_tokens_est": 66, "n_tiles": 1, "output_text": "The image depicts a snowy scene on a railway track, with a train traveling through a snowy landscape. The train is moving away from the viewer, leaving a trail of snow behind it. The surrounding area is covered in snow, and there are trees with snow-covered branches visible on both sides of the track.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24932.5, "ram_available_mb": 100839.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24932.5, "ram_available_mb": 100839.7, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 4854.7, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.59, "peak": 15.72, "min": 15.32}, "VDD_CPU_SOC_MSS": {"avg": 16.47, "peak": 16.54, "min": 16.14}, "VDD_GPU": {"avg": 21.24, "peak": 23.62, "min": 20.1}, "VIN": {"avg": 62.43, "peak": 69.2, "min": 56.19}}, "power_watts_avg": 21.24, "energy_joules_est": 31.72, "sample_count": 11, "duration_seconds": 1.493}, "timestamp": "2026-01-17T15:45:31.497810"}
{"image_index": 382, "image_name": "000000042563.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042563.jpg", "image_width": 624, "image_height": 640, "image_resolution": "624x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1515.989, "latencies_ms": [1515.989], "images_per_second": 0.66, "prompt_tokens": 19, "response_tokens_est": 67, "n_tiles": 1, "output_text": "The image depicts a snowy scene with a train traveling on a snow-covered railway track. The train is painted in a bright yellow color, contrasting sharply against the white snow. The sky is overcast, with a uniform gray hue, and the lighting is subdued, suggesting a cold, wintry day.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24932.5, "ram_available_mb": 100839.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24933.0, "ram_available_mb": 100839.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 4853.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.52, "peak": 15.72, "min": 15.11}, "VDD_CPU_SOC_MSS": {"avg": 16.46, "peak": 16.54, "min": 16.14}, "VDD_GPU": {"avg": 21.17, "peak": 23.64, "min": 20.1}, "VIN": {"avg": 63.25, "peak": 66.56, "min": 61.74}}, "power_watts_avg": 21.17, "energy_joules_est": 32.1, "sample_count": 11, "duration_seconds": 1.516}, "timestamp": "2026-01-17T15:45:33.019631"}
{"image_index": 383, "image_name": "000000042628.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042628.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1220.659, "latencies_ms": [1220.659], "images_per_second": 0.819, "prompt_tokens": 9, "response_tokens_est": 23, "n_tiles": 6, "output_text": "A group of people are walking through a snowy area, with a large pile of snow blocking the path.", "error": null, "sys_before": {"cpu_percent": 44.1, "ram_used_mb": 24946.1, "ram_available_mb": 100826.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 24946.4, "ram_available_mb": 100825.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.86, "peak": 15.34, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.7, "peak": 16.53, "min": 14.96}, "VDD_GPU": {"avg": 33.7, "peak": 39.78, "min": 27.57}, "VIN": {"avg": 79.32, "peak": 109.12, "min": 60.72}}, "power_watts_avg": 33.7, "energy_joules_est": 41.15, "sample_count": 9, "duration_seconds": 1.221}, "timestamp": "2026-01-17T15:45:34.284814"}
{"image_index": 383, "image_name": "000000042628.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042628.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1721.412, "latencies_ms": [1721.412], "images_per_second": 0.581, "prompt_tokens": 23, "response_tokens_est": 40, "n_tiles": 6, "output_text": "object: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24946.4, "ram_available_mb": 100825.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 24946.9, "ram_available_mb": 100825.3, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.04, "peak": 15.44, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.59, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 33.09, "peak": 41.34, "min": 24.82}, "VIN": {"avg": 73.25, "peak": 109.99, "min": 60.78}}, "power_watts_avg": 33.09, "energy_joules_est": 56.97, "sample_count": 13, "duration_seconds": 1.722}, "timestamp": "2026-01-17T15:45:36.012508"}
{"image_index": 383, "image_name": "000000042628.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042628.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2239.408, "latencies_ms": [2239.408], "images_per_second": 0.447, "prompt_tokens": 27, "response_tokens_est": 59, "n_tiles": 6, "output_text": "The main objects in the image are a snow-covered area with a fire hydrant in the foreground and a group of people in the background. The fire hydrant is located near the center of the image, while the people are further back, indicating a clear spatial relationship between the two.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 24946.9, "ram_available_mb": 100825.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 24947.6, "ram_available_mb": 100824.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.09, "peak": 15.54, "min": 14.43}, "VDD_CPU_SOC_MSS": {"avg": 15.63, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 30.42, "peak": 40.16, "min": 23.64}, "VIN": {"avg": 68.68, "peak": 85.89, "min": 56.66}}, "power_watts_avg": 30.42, "energy_joules_est": 68.13, "sample_count": 17, "duration_seconds": 2.24}, "timestamp": "2026-01-17T15:45:38.257252"}
{"image_index": 383, "image_name": "000000042628.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042628.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2317.88, "latencies_ms": [2317.88], "images_per_second": 0.431, "prompt_tokens": 21, "response_tokens_est": 62, "n_tiles": 6, "output_text": "The image depicts a snowy urban scene with a large pile of snow blocking a street. Several people are walking through the snow, some carrying bags and others with snowboards. The setting appears to be a city street during winter, with vehicles parked on the side and a brick wall in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24947.6, "ram_available_mb": 100824.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 24947.8, "ram_available_mb": 100824.3, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.09, "peak": 15.54, "min": 14.43}, "VDD_CPU_SOC_MSS": {"avg": 15.64, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 29.92, "peak": 40.18, "min": 23.64}, "VIN": {"avg": 71.78, "peak": 120.69, "min": 63.68}}, "power_watts_avg": 29.92, "energy_joules_est": 69.36, "sample_count": 18, "duration_seconds": 2.318}, "timestamp": "2026-01-17T15:45:40.583414"}
{"image_index": 383, "image_name": "000000042628.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042628.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2373.357, "latencies_ms": [2373.357], "images_per_second": 0.421, "prompt_tokens": 19, "response_tokens_est": 64, "n_tiles": 6, "output_text": "The image depicts a snow-covered scene with a large pile of snow in the foreground, a few people walking through the snow, and a fire hydrant partially buried in the snow. The lighting is dim, suggesting it is either early morning or late afternoon, and the overall atmosphere is cold and wintry.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24947.8, "ram_available_mb": 100824.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 24948.1, "ram_available_mb": 100824.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.08, "peak": 15.54, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.66, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 30.1, "peak": 40.16, "min": 23.64}, "VIN": {"avg": 75.31, "peak": 123.73, "min": 60.65}}, "power_watts_avg": 30.1, "energy_joules_est": 71.45, "sample_count": 17, "duration_seconds": 2.374}, "timestamp": "2026-01-17T15:45:42.963321"}
{"image_index": 384, "image_name": "000000042888.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042888.jpg", "image_width": 500, "image_height": 374, "image_resolution": "500x374", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 3583.241, "latencies_ms": [3583.241], "images_per_second": 0.279, "prompt_tokens": 9, "response_tokens_est": 58, "n_tiles": 12, "output_text": "The image shows a street sign indicating a \"TOW ZONE\" with a red arrow pointing to the right, and a yellow sign with a cartoonish face on the left side of the signpost. The signpost is surrounded by green trees and a brick building in the background.", "error": null, "sys_before": {"cpu_percent": 50.0, "ram_used_mb": 24905.7, "ram_available_mb": 100866.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 24907.2, "ram_available_mb": 100865.0, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.12, "peak": 16.66, "min": 13.39}, "VDD_CPU_SOC_MSS": {"avg": 15.72, "peak": 17.71, "min": 14.18}, "VDD_GPU": {"avg": 33.13, "peak": 43.71, "min": 26.01}, "VIN": {"avg": 73.61, "peak": 102.08, "min": 59.33}}, "power_watts_avg": 33.13, "energy_joules_est": 118.72, "sample_count": 27, "duration_seconds": 3.584}, "timestamp": "2026-01-17T15:45:46.643820"}
{"image_index": 384, "image_name": "000000042888.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042888.jpg", "image_width": 500, "image_height": 374, "image_resolution": "500x374", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 3023.446, "latencies_ms": [3023.446], "images_per_second": 0.331, "prompt_tokens": 23, "response_tokens_est": 40, "n_tiles": 12, "output_text": "object: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24907.2, "ram_available_mb": 100865.0, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 24908.4, "ram_available_mb": 100863.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.27, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.85, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 35.64, "peak": 45.3, "min": 26.4}, "VIN": {"avg": 81.78, "peak": 136.11, "min": 61.44}}, "power_watts_avg": 35.64, "energy_joules_est": 107.77, "sample_count": 23, "duration_seconds": 3.024}, "timestamp": "2026-01-17T15:45:49.673729"}
{"image_index": 384, "image_name": "000000042888.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042888.jpg", "image_width": 500, "image_height": 374, "image_resolution": "500x374", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 4729.374, "latencies_ms": [4729.374], "images_per_second": 0.211, "prompt_tokens": 27, "response_tokens_est": 90, "n_tiles": 12, "output_text": "The main objects in the image are a tree, a street sign, and a yellow object. The tree is in the foreground, with its branches and leaves partially obscuring the view of the street sign. The street sign is positioned in the background, slightly to the right, and is attached to a metal pole. The yellow object, which appears to be a sign or a piece of equipment, is also in the foreground, near the tree.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24908.4, "ram_available_mb": 100863.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 24909.4, "ram_available_mb": 100862.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.19, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.68, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 32.17, "peak": 45.28, "min": 26.0}, "VIN": {"avg": 75.73, "peak": 119.02, "min": 65.77}}, "power_watts_avg": 32.17, "energy_joules_est": 152.15, "sample_count": 36, "duration_seconds": 4.73}, "timestamp": "2026-01-17T15:45:54.409912"}
{"image_index": 384, "image_name": "000000042888.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042888.jpg", "image_width": 500, "image_height": 374, "image_resolution": "500x374", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3568.608, "latencies_ms": [3568.608], "images_per_second": 0.28, "prompt_tokens": 21, "response_tokens_est": 56, "n_tiles": 12, "output_text": "The image depicts a street scene with a yellow traffic sign attached to a metal pole. The sign reads \"TOW ZONE\" and features a red prohibition symbol. The surrounding area is lush with green foliage, indicating a park or a tree-lined street.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24909.4, "ram_available_mb": 100862.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 24910.3, "ram_available_mb": 100861.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.23, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.78, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 34.14, "peak": 44.91, "min": 26.0}, "VIN": {"avg": 76.21, "peak": 121.92, "min": 56.76}}, "power_watts_avg": 34.14, "energy_joules_est": 121.85, "sample_count": 27, "duration_seconds": 3.569}, "timestamp": "2026-01-17T15:45:57.985389"}
{"image_index": 384, "image_name": "000000042888.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042888.jpg", "image_width": 500, "image_height": 374, "image_resolution": "500x374", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3880.909, "latencies_ms": [3880.909], "images_per_second": 0.258, "prompt_tokens": 19, "response_tokens_est": 64, "n_tiles": 12, "output_text": "The image features a street sign with a red border and a white background, prominently displaying a red prohibition sign with a white arrow pointing to the right. The sign is mounted on a metal pole. The surrounding area is lush with green foliage, indicating a sunny day with ample sunlight illuminating the scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24910.3, "ram_available_mb": 100861.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 24911.6, "ram_available_mb": 100860.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.07, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.6, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 33.36, "peak": 44.91, "min": 25.6}, "VIN": {"avg": 73.62, "peak": 112.3, "min": 56.15}}, "power_watts_avg": 33.36, "energy_joules_est": 129.48, "sample_count": 29, "duration_seconds": 3.881}, "timestamp": "2026-01-17T15:46:01.876880"}
{"image_index": 385, "image_name": "000000042889.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042889.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2668.836, "latencies_ms": [2668.836], "images_per_second": 0.375, "prompt_tokens": 9, "response_tokens_est": 31, "n_tiles": 12, "output_text": "The image shows a collection of vintage electronic devices, including a keyboard, a mouse, and a small white speaker, all resting on a red surface.", "error": null, "sys_before": {"cpu_percent": 33.0, "ram_used_mb": 24922.9, "ram_available_mb": 100849.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 24917.1, "ram_available_mb": 100855.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.13, "peak": 16.66, "min": 13.29}, "VDD_CPU_SOC_MSS": {"avg": 15.84, "peak": 17.71, "min": 13.78}, "VDD_GPU": {"avg": 35.9, "peak": 44.12, "min": 27.18}, "VIN": {"avg": 75.62, "peak": 105.18, "min": 58.62}}, "power_watts_avg": 35.9, "energy_joules_est": 95.82, "sample_count": 20, "duration_seconds": 2.669}, "timestamp": "2026-01-17T15:46:04.627708"}
{"image_index": 385, "image_name": "000000042889.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042889.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2923.317, "latencies_ms": [2923.317], "images_per_second": 0.342, "prompt_tokens": 23, "response_tokens_est": 37, "n_tiles": 12, "output_text": "1. teddy bear\n2. glasses\n3. earphones\n4. cell phone\n5. keyboard\n6. mouse\n7. cord\n8. computer mouse", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24917.1, "ram_available_mb": 100855.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 24912.5, "ram_available_mb": 100859.7, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.27, "peak": 16.66, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.87, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 36.22, "peak": 45.28, "min": 26.8}, "VIN": {"avg": 79.44, "peak": 120.32, "min": 62.5}}, "power_watts_avg": 36.22, "energy_joules_est": 105.89, "sample_count": 22, "duration_seconds": 2.924}, "timestamp": "2026-01-17T15:46:07.559495"}
{"image_index": 385, "image_name": "000000042889.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042889.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3708.41, "latencies_ms": [3708.41], "images_per_second": 0.27, "prompt_tokens": 27, "response_tokens_est": 60, "n_tiles": 12, "output_text": "The main objects in the image are a teddy bear, a keyboard, and a mouse. The teddy bear is positioned in the foreground, while the keyboard and mouse are in the background. The keyboard is on the left side of the image, and the mouse is on the right side.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24912.5, "ram_available_mb": 100859.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 24912.5, "ram_available_mb": 100859.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.2, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.72, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 34.08, "peak": 45.28, "min": 26.0}, "VIN": {"avg": 74.3, "peak": 107.04, "min": 56.93}}, "power_watts_avg": 34.08, "energy_joules_est": 126.4, "sample_count": 28, "duration_seconds": 3.709}, "timestamp": "2026-01-17T15:46:11.279794"}
{"image_index": 385, "image_name": "000000042889.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042889.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 4317.818, "latencies_ms": [4317.818], "images_per_second": 0.232, "prompt_tokens": 21, "response_tokens_est": 78, "n_tiles": 12, "output_text": "The image depicts a cozy, dimly lit room with a red tabletop. On the table, there is a collection of vintage electronic devices, including a white and silver portable radio, a black and white keyboard, a black and gray mouse, and a pair of glasses. The scene suggests a personal, nostalgic space where someone might have been working or playing with these devices.", "error": null, "sys_before": {"cpu_percent": 40.0, "ram_used_mb": 24912.5, "ram_available_mb": 100859.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 24913.5, "ram_available_mb": 100858.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.2, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.66, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 32.76, "peak": 44.91, "min": 26.01}, "VIN": {"avg": 77.08, "peak": 133.19, "min": 63.44}}, "power_watts_avg": 32.76, "energy_joules_est": 141.47, "sample_count": 32, "duration_seconds": 4.318}, "timestamp": "2026-01-17T15:46:15.604232"}
{"image_index": 385, "image_name": "000000042889.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042889.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3091.365, "latencies_ms": [3091.365], "images_per_second": 0.323, "prompt_tokens": 19, "response_tokens_est": 42, "n_tiles": 12, "output_text": "The image features a teddy bear with a light brown fur coat, wearing glasses with a brown frame. The bear is resting on a red surface, which is illuminated by a soft, ambient light.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24913.5, "ram_available_mb": 100858.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 24914.0, "ram_available_mb": 100858.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.26, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.9, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 35.47, "peak": 44.89, "min": 26.4}, "VIN": {"avg": 82.15, "peak": 136.9, "min": 57.26}}, "power_watts_avg": 35.47, "energy_joules_est": 109.66, "sample_count": 23, "duration_seconds": 3.092}, "timestamp": "2026-01-17T15:46:18.703104"}
{"image_index": 386, "image_name": "000000043314.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000043314.jpg", "image_width": 500, "image_height": 376, "image_resolution": "500x376", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 3008.435, "latencies_ms": [3008.435], "images_per_second": 0.332, "prompt_tokens": 9, "response_tokens_est": 41, "n_tiles": 12, "output_text": "The image captures a dynamic moment of a skier in mid-air, performing a jump over a snowy mountain slope, with the skier wearing a white and orange outfit and a helmet for safety.", "error": null, "sys_before": {"cpu_percent": 23.9, "ram_used_mb": 24910.3, "ram_available_mb": 100861.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 24905.2, "ram_available_mb": 100867.0, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.18, "peak": 16.66, "min": 13.61}, "VDD_CPU_SOC_MSS": {"avg": 15.85, "peak": 17.71, "min": 14.17}, "VDD_GPU": {"avg": 34.85, "peak": 44.12, "min": 26.4}, "VIN": {"avg": 75.37, "peak": 110.09, "min": 64.83}}, "power_watts_avg": 34.85, "energy_joules_est": 104.85, "sample_count": 23, "duration_seconds": 3.009}, "timestamp": "2026-01-17T15:46:21.782579"}
{"image_index": 386, "image_name": "000000043314.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000043314.jpg", "image_width": 500, "image_height": 376, "image_resolution": "500x376", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 3027.133, "latencies_ms": [3027.133], "images_per_second": 0.33, "prompt_tokens": 23, "response_tokens_est": 40, "n_tiles": 12, "output_text": "object: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24905.2, "ram_available_mb": 100867.0, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 24916.3, "ram_available_mb": 100855.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.27, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.85, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 35.53, "peak": 45.3, "min": 26.39}, "VIN": {"avg": 81.4, "peak": 113.89, "min": 66.78}}, "power_watts_avg": 35.53, "energy_joules_est": 107.57, "sample_count": 23, "duration_seconds": 3.028}, "timestamp": "2026-01-17T15:46:24.817479"}
{"image_index": 386, "image_name": "000000043314.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000043314.jpg", "image_width": 500, "image_height": 376, "image_resolution": "500x376", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 4011.932, "latencies_ms": [4011.932], "images_per_second": 0.249, "prompt_tokens": 27, "response_tokens_est": 69, "n_tiles": 12, "output_text": "The main objects in the image are a snowboarder and a snowy mountain landscape. The snowboarder is positioned in the foreground, slightly off-center to the left, while the snowy mountain landscape forms the background. The snowboarder is near the snow-covered slope, and the snowy mountain landscape extends into the distance.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 24916.3, "ram_available_mb": 100855.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 24916.7, "ram_available_mb": 100855.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.19, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.71, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 33.19, "peak": 44.89, "min": 26.0}, "VIN": {"avg": 74.36, "peak": 105.58, "min": 61.27}}, "power_watts_avg": 33.19, "energy_joules_est": 133.17, "sample_count": 31, "duration_seconds": 4.012}, "timestamp": "2026-01-17T15:46:28.836077"}
{"image_index": 386, "image_name": "000000043314.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000043314.jpg", "image_width": 500, "image_height": 376, "image_resolution": "500x376", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3317.726, "latencies_ms": [3317.726], "images_per_second": 0.301, "prompt_tokens": 21, "response_tokens_est": 49, "n_tiles": 12, "output_text": "The image depicts a snowy mountainous landscape with a skier in mid-air, performing a jump. The skier is dressed in a white and orange outfit, and the snowy terrain is covered in a thick layer of snow.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24916.7, "ram_available_mb": 100855.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 24916.7, "ram_available_mb": 100855.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.24, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.75, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 34.75, "peak": 45.28, "min": 26.39}, "VIN": {"avg": 77.53, "peak": 128.36, "min": 66.33}}, "power_watts_avg": 34.75, "energy_joules_est": 115.31, "sample_count": 26, "duration_seconds": 3.318}, "timestamp": "2026-01-17T15:46:32.160390"}
{"image_index": 386, "image_name": "000000043314.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000043314.jpg", "image_width": 500, "image_height": 376, "image_resolution": "500x376", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2948.235, "latencies_ms": [2948.235], "images_per_second": 0.339, "prompt_tokens": 19, "response_tokens_est": 38, "n_tiles": 12, "output_text": "The image depicts a snowy mountainous landscape with a clear blue sky. The snow-covered ground and trees are illuminated by the sunlight, creating a bright and vibrant scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24916.7, "ram_available_mb": 100855.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 24918.0, "ram_available_mb": 100854.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.26, "peak": 16.56, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.75, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 35.86, "peak": 45.28, "min": 26.79}, "VIN": {"avg": 78.19, "peak": 118.03, "min": 63.54}}, "power_watts_avg": 35.86, "energy_joules_est": 105.74, "sample_count": 23, "duration_seconds": 2.949}, "timestamp": "2026-01-17T15:46:35.115590"}
{"image_index": 387, "image_name": "000000043435.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000043435.jpg", "image_width": 638, "image_height": 640, "image_resolution": "638x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1191.357, "latencies_ms": [1191.357], "images_per_second": 0.839, "prompt_tokens": 9, "response_tokens_est": 33, "n_tiles": 4, "output_text": "A surfer is skillfully riding a wave on a surfboard, with the water splashing around them and the sun casting a warm glow on the scene.", "error": null, "sys_before": {"cpu_percent": 22.6, "ram_used_mb": 24930.4, "ram_available_mb": 100841.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 24931.9, "ram_available_mb": 100840.3, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4606.7, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5444.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.7, "peak": 15.22, "min": 14.02}, "VDD_CPU_SOC_MSS": {"avg": 15.27, "peak": 15.75, "min": 14.57}, "VDD_GPU": {"avg": 30.9, "peak": 38.21, "min": 24.82}, "VIN": {"avg": 73.32, "peak": 108.9, "min": 58.84}}, "power_watts_avg": 30.9, "energy_joules_est": 36.83, "sample_count": 9, "duration_seconds": 1.192}, "timestamp": "2026-01-17T15:46:36.348826"}
{"image_index": 387, "image_name": "000000043435.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000043435.jpg", "image_width": 638, "image_height": 640, "image_resolution": "638x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1370.875, "latencies_ms": [1370.875], "images_per_second": 0.729, "prompt_tokens": 23, "response_tokens_est": 40, "n_tiles": 4, "output_text": "object: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24931.9, "ram_available_mb": 100840.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 24931.9, "ram_available_mb": 100840.3, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4606.7, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5455.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.95, "peak": 15.32, "min": 14.61}, "VDD_CPU_SOC_MSS": {"avg": 15.59, "peak": 16.14, "min": 15.35}, "VDD_GPU": {"avg": 30.25, "peak": 38.19, "min": 24.04}, "VIN": {"avg": 69.52, "peak": 102.29, "min": 60.43}}, "power_watts_avg": 30.25, "energy_joules_est": 41.48, "sample_count": 10, "duration_seconds": 1.371}, "timestamp": "2026-01-17T15:46:37.728528"}
{"image_index": 387, "image_name": "000000043435.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000043435.jpg", "image_width": 638, "image_height": 640, "image_resolution": "638x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2071.712, "latencies_ms": [2071.712], "images_per_second": 0.483, "prompt_tokens": 27, "response_tokens_est": 65, "n_tiles": 4, "output_text": "The main object in the image is a surfer riding a wave. The surfer is positioned in the foreground, slightly to the right, and is riding the wave that is in the background. The wave is large and occupies the majority of the frame, with the surfer positioned on the wave's crest.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24931.9, "ram_available_mb": 100840.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 24932.6, "ram_available_mb": 100839.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4606.7, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5458.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.96, "peak": 15.22, "min": 14.61}, "VDD_CPU_SOC_MSS": {"avg": 15.62, "peak": 15.75, "min": 15.35}, "VDD_GPU": {"avg": 27.34, "peak": 37.82, "min": 22.06}, "VIN": {"avg": 68.1, "peak": 99.49, "min": 58.4}}, "power_watts_avg": 27.34, "energy_joules_est": 56.65, "sample_count": 15, "duration_seconds": 2.072}, "timestamp": "2026-01-17T15:46:39.806289"}
{"image_index": 387, "image_name": "000000043435.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000043435.jpg", "image_width": 638, "image_height": 640, "image_resolution": "638x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2433.478, "latencies_ms": [2433.478], "images_per_second": 0.411, "prompt_tokens": 21, "response_tokens_est": 79, "n_tiles": 4, "output_text": "The image captures a dynamic scene of a surfer riding a wave in the ocean. The surfer is skillfully maneuvering on a surfboard, with the wave curling around them, creating a frothy white crest. The setting is a vast ocean with a clear sky, and the surfer is the central focus of the image, showcasing the thrill and beauty of surfing.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24932.6, "ram_available_mb": 100839.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 24933.6, "ram_available_mb": 100838.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4606.7, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5453.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.01, "peak": 15.22, "min": 14.51}, "VDD_CPU_SOC_MSS": {"avg": 15.62, "peak": 15.75, "min": 14.96}, "VDD_GPU": {"avg": 26.05, "peak": 36.63, "min": 21.67}, "VIN": {"avg": 66.25, "peak": 96.63, "min": 57.65}}, "power_watts_avg": 26.05, "energy_joules_est": 63.4, "sample_count": 18, "duration_seconds": 2.434}, "timestamp": "2026-01-17T15:46:42.246033"}
{"image_index": 387, "image_name": "000000043435.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000043435.jpg", "image_width": 638, "image_height": 640, "image_resolution": "638x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2491.85, "latencies_ms": [2491.85], "images_per_second": 0.401, "prompt_tokens": 19, "response_tokens_est": 81, "n_tiles": 4, "output_text": "The image captures a dynamic scene of a surfer riding a wave, with the surfer and the wave in sharp focus against a backdrop of a vast ocean. The lighting is natural, with sunlight reflecting off the water, creating a shimmering effect. The colors are primarily shades of blue and white, with the surfer's dark wetsuit providing a contrast against the lighter water and sky.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24933.6, "ram_available_mb": 100838.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 24933.8, "ram_available_mb": 100838.3, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4606.7, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5451.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.97, "peak": 15.22, "min": 14.51}, "VDD_CPU_SOC_MSS": {"avg": 15.62, "peak": 15.75, "min": 15.35}, "VDD_GPU": {"avg": 26.0, "peak": 37.03, "min": 21.66}, "VIN": {"avg": 66.95, "peak": 107.21, "min": 60.76}}, "power_watts_avg": 26.0, "energy_joules_est": 64.81, "sample_count": 18, "duration_seconds": 2.493}, "timestamp": "2026-01-17T15:46:44.744237"}
{"image_index": 388, "image_name": "000000043581.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000043581.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1711.707, "latencies_ms": [1711.707], "images_per_second": 0.584, "prompt_tokens": 9, "response_tokens_est": 41, "n_tiles": 6, "output_text": "The image shows a pizza with various toppings, including cheese, vegetables, and possibly meat, all placed on a metal pizza tray, which is placed on a table with a blue tablecloth.", "error": null, "sys_before": {"cpu_percent": 32.6, "ram_used_mb": 24937.8, "ram_available_mb": 100834.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 24939.3, "ram_available_mb": 100832.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.87, "peak": 15.24, "min": 14.1}, "VDD_CPU_SOC_MSS": {"avg": 15.5, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 31.3, "peak": 40.18, "min": 24.42}, "VIN": {"avg": 78.97, "peak": 122.19, "min": 62.79}}, "power_watts_avg": 31.3, "energy_joules_est": 53.59, "sample_count": 13, "duration_seconds": 1.712}, "timestamp": "2026-01-17T15:46:46.492479"}
{"image_index": 388, "image_name": "000000043581.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000043581.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1513.289, "latencies_ms": [1513.289], "images_per_second": 0.661, "prompt_tokens": 23, "response_tokens_est": 32, "n_tiles": 6, "output_text": "1. Pizza\n2. Plate\n3. Pizza\n4. Pizza\n5. Pizza\n6. Pizza\n7. Pizza\n8. Pizza", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24939.3, "ram_available_mb": 100832.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 24940.0, "ram_available_mb": 100832.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.02, "peak": 15.54, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.57, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 33.34, "peak": 40.56, "min": 25.6}, "VIN": {"avg": 74.78, "peak": 109.56, "min": 55.53}}, "power_watts_avg": 33.34, "energy_joules_est": 50.47, "sample_count": 11, "duration_seconds": 1.514}, "timestamp": "2026-01-17T15:46:48.011915"}
{"image_index": 388, "image_name": "000000043581.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000043581.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2517.042, "latencies_ms": [2517.042], "images_per_second": 0.397, "prompt_tokens": 27, "response_tokens_est": 69, "n_tiles": 6, "output_text": "The main object in the foreground is a pizza with a variety of toppings, including cheese, vegetables, and possibly meat. The pizza is placed on a metal tray. In the background, there is a jar of sauce and a napkin or paper towel. The pizza and the tray are on a table with a blue tablecloth.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24940.0, "ram_available_mb": 100832.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 24941.0, "ram_available_mb": 100831.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.09, "peak": 15.44, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.64, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 29.67, "peak": 40.56, "min": 23.64}, "VIN": {"avg": 69.14, "peak": 93.37, "min": 61.64}}, "power_watts_avg": 29.67, "energy_joules_est": 74.69, "sample_count": 19, "duration_seconds": 2.517}, "timestamp": "2026-01-17T15:46:50.534591"}
{"image_index": 388, "image_name": "000000043581.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000043581.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2787.821, "latencies_ms": [2787.821], "images_per_second": 0.359, "prompt_tokens": 21, "response_tokens_est": 79, "n_tiles": 6, "output_text": "The image depicts a pizza on a blue plate, placed on a table with a red surface. The pizza appears to be freshly baked, with a golden-brown crust and a variety of toppings, including melted cheese, pieces of meat, and green herbs. The setting suggests a casual dining environment, possibly a restaurant or a home kitchen, with dim lighting and a blurred background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24941.0, "ram_available_mb": 100831.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 24941.0, "ram_available_mb": 100831.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.13, "peak": 15.54, "min": 14.43}, "VDD_CPU_SOC_MSS": {"avg": 15.67, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 28.87, "peak": 40.18, "min": 23.25}, "VIN": {"avg": 71.62, "peak": 115.48, "min": 61.56}}, "power_watts_avg": 28.87, "energy_joules_est": 80.5, "sample_count": 21, "duration_seconds": 2.788}, "timestamp": "2026-01-17T15:46:53.328776"}
{"image_index": 388, "image_name": "000000043581.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000043581.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2215.016, "latencies_ms": [2215.016], "images_per_second": 0.451, "prompt_tokens": 19, "response_tokens_est": 58, "n_tiles": 6, "output_text": "The image features a pizza with a golden-brown crust, topped with melted cheese and various toppings, including pieces of meat and vegetables. The pizza is placed on a blue plate, and the background is dimly lit with a bluish hue, creating a cozy and inviting atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24941.0, "ram_available_mb": 100831.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 24941.3, "ram_available_mb": 100830.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.07, "peak": 15.44, "min": 14.43}, "VDD_CPU_SOC_MSS": {"avg": 15.62, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 30.48, "peak": 40.16, "min": 23.64}, "VIN": {"avg": 72.19, "peak": 123.32, "min": 56.33}}, "power_watts_avg": 30.48, "energy_joules_est": 67.52, "sample_count": 16, "duration_seconds": 2.215}, "timestamp": "2026-01-17T15:46:55.549912"}
{"image_index": 389, "image_name": "000000043737.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000043737.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2868.295, "latencies_ms": [2868.295], "images_per_second": 0.349, "prompt_tokens": 9, "response_tokens_est": 37, "n_tiles": 12, "output_text": "The image depicts a street scene during winter, with a prominent black clock mounted on a pole, snow-covered sidewalks, and a row of buildings with lit-up windows.", "error": null, "sys_before": {"cpu_percent": 26.4, "ram_used_mb": 24909.0, "ram_available_mb": 100863.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 24910.2, "ram_available_mb": 100861.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.25, "peak": 16.66, "min": 14.0}, "VDD_CPU_SOC_MSS": {"avg": 15.89, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 35.13, "peak": 44.1, "min": 26.79}, "VIN": {"avg": 80.21, "peak": 120.93, "min": 65.4}}, "power_watts_avg": 35.13, "energy_joules_est": 100.77, "sample_count": 22, "duration_seconds": 2.869}, "timestamp": "2026-01-17T15:46:58.476594"}
{"image_index": 389, "image_name": "000000043737.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000043737.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 3328.854, "latencies_ms": [3328.854], "images_per_second": 0.3, "prompt_tokens": 23, "response_tokens_est": 48, "n_tiles": 12, "output_text": "- clock: 1\n- building: 1\n- street: 1\n- car: 1\n- sign: 1\n- tree: 1\n- sidewalk: 1\n- building: 1", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 24910.2, "ram_available_mb": 100861.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 24910.0, "ram_available_mb": 100862.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.08, "peak": 16.56, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.53, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 34.56, "peak": 45.28, "min": 25.61}, "VIN": {"avg": 78.87, "peak": 130.45, "min": 64.9}}, "power_watts_avg": 34.56, "energy_joules_est": 115.06, "sample_count": 26, "duration_seconds": 3.329}, "timestamp": "2026-01-17T15:47:01.812282"}
{"image_index": 389, "image_name": "000000043737.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000043737.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3482.541, "latencies_ms": [3482.541], "images_per_second": 0.287, "prompt_tokens": 27, "response_tokens_est": 53, "n_tiles": 12, "output_text": "The main objects in the image are a street clock and a row of buildings. The street clock is positioned on the left side of the image, near the foreground. The row of buildings is located in the background, extending towards the right side of the image.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24910.0, "ram_available_mb": 100862.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 24911.2, "ram_available_mb": 100861.0, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.93, "peak": 16.26, "min": 14.02}, "VDD_CPU_SOC_MSS": {"avg": 15.35, "peak": 16.92, "min": 14.17}, "VDD_GPU": {"avg": 34.36, "peak": 45.69, "min": 25.61}, "VIN": {"avg": 77.19, "peak": 122.14, "min": 63.22}}, "power_watts_avg": 34.36, "energy_joules_est": 119.67, "sample_count": 26, "duration_seconds": 3.483}, "timestamp": "2026-01-17T15:47:05.301639"}
{"image_index": 389, "image_name": "000000043737.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000043737.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3863.403, "latencies_ms": [3863.403], "images_per_second": 0.259, "prompt_tokens": 21, "response_tokens_est": 64, "n_tiles": 12, "output_text": "The image depicts a snowy urban street scene during twilight. The street is lined with buildings, and a large, ornate clock is prominently displayed on a pole. The clock face shows the time as approximately 7:00, and the surrounding area is covered in snow, indicating a winter setting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24911.2, "ram_available_mb": 100861.0, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 24912.2, "ram_available_mb": 100860.0, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.93, "peak": 16.26, "min": 13.92}, "VDD_CPU_SOC_MSS": {"avg": 15.39, "peak": 17.32, "min": 14.17}, "VDD_GPU": {"avg": 33.41, "peak": 44.91, "min": 25.6}, "VIN": {"avg": 73.59, "peak": 111.01, "min": 59.3}}, "power_watts_avg": 33.41, "energy_joules_est": 129.09, "sample_count": 29, "duration_seconds": 3.864}, "timestamp": "2026-01-17T15:47:09.171712"}
{"image_index": 389, "image_name": "000000043737.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000043737.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3926.186, "latencies_ms": [3926.186], "images_per_second": 0.255, "prompt_tokens": 19, "response_tokens_est": 66, "n_tiles": 12, "output_text": "The image depicts a street scene during winter, with a prominent black clock mounted on a pole. The clock face is white with black Roman numerals, and the hands indicate a time of approximately 10:10. The surrounding area is covered in snow, and the sky is clear, suggesting a cold but sunny day.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24912.2, "ram_available_mb": 100860.0, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 24913.7, "ram_available_mb": 100858.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.91, "peak": 16.26, "min": 14.02}, "VDD_CPU_SOC_MSS": {"avg": 15.38, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 33.18, "peak": 45.3, "min": 25.6}, "VIN": {"avg": 77.32, "peak": 124.15, "min": 64.52}}, "power_watts_avg": 33.18, "energy_joules_est": 130.28, "sample_count": 30, "duration_seconds": 3.927}, "timestamp": "2026-01-17T15:47:13.104773"}
{"image_index": 390, "image_name": "000000043816.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000043816.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1656.203, "latencies_ms": [1656.203], "images_per_second": 0.604, "prompt_tokens": 9, "response_tokens_est": 39, "n_tiles": 6, "output_text": "A baseball player in a white and blue uniform is swinging a bat at a baseball, while a catcher in red gear is crouched behind home plate, ready to catch the ball.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24950.0, "ram_available_mb": 100822.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 24951.4, "ram_available_mb": 100820.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.71, "peak": 15.11, "min": 13.61}, "VDD_CPU_SOC_MSS": {"avg": 15.32, "peak": 15.75, "min": 14.18}, "VDD_GPU": {"avg": 32.24, "peak": 40.57, "min": 24.82}, "VIN": {"avg": 75.04, "peak": 110.55, "min": 62.54}}, "power_watts_avg": 32.24, "energy_joules_est": 53.41, "sample_count": 13, "duration_seconds": 1.657}, "timestamp": "2026-01-17T15:47:14.807786"}
{"image_index": 390, "image_name": "000000043816.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000043816.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2049.049, "latencies_ms": [2049.049], "images_per_second": 0.488, "prompt_tokens": 23, "response_tokens_est": 50, "n_tiles": 6, "output_text": "baseball player: 1\ncatcher: 1\numpire: 1\nhome plate: 1\nglove: 1\nbat: 1\npitcher: 1\ncatcher's mask: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24951.4, "ram_available_mb": 100820.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 24952.7, "ram_available_mb": 100819.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.87, "peak": 15.34, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.49, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 30.99, "peak": 40.97, "min": 23.64}, "VIN": {"avg": 75.36, "peak": 113.01, "min": 63.63}}, "power_watts_avg": 30.99, "energy_joules_est": 63.51, "sample_count": 15, "duration_seconds": 2.049}, "timestamp": "2026-01-17T15:47:16.863193"}
{"image_index": 390, "image_name": "000000043816.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000043816.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3028.788, "latencies_ms": [3028.788], "images_per_second": 0.33, "prompt_tokens": 27, "response_tokens_est": 84, "n_tiles": 6, "output_text": "The baseball player is positioned in the foreground, wearing a white and blue uniform, holding a bat, and preparing to swing at the incoming ball. The catcher is in the background, crouched behind home plate, wearing a red uniform and a catcher's mask, ready to catch the ball. The baseball is in the air, slightly to the left of the player, indicating the action of the swing.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24952.7, "ram_available_mb": 100819.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 24949.9, "ram_available_mb": 100822.3, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.86, "peak": 15.24, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.46, "peak": 15.75, "min": 14.56}, "VDD_GPU": {"avg": 28.14, "peak": 40.18, "min": 22.85}, "VIN": {"avg": 69.33, "peak": 113.39, "min": 59.14}}, "power_watts_avg": 28.14, "energy_joules_est": 85.24, "sample_count": 23, "duration_seconds": 3.029}, "timestamp": "2026-01-17T15:47:19.898621"}
{"image_index": 390, "image_name": "000000043816.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000043816.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2341.827, "latencies_ms": [2341.827], "images_per_second": 0.427, "prompt_tokens": 21, "response_tokens_est": 60, "n_tiles": 6, "output_text": "The image captures a dynamic moment during a baseball game, featuring a batter in mid-swing, a catcher in position, and a umpire ready to make a call. The scene takes place on a well-maintained baseball field with a dirt infield and green grass surrounding it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24949.9, "ram_available_mb": 100822.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 24949.2, "ram_available_mb": 100823.0, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.84, "peak": 15.24, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.42, "peak": 15.75, "min": 14.56}, "VDD_GPU": {"avg": 29.46, "peak": 40.18, "min": 23.24}, "VIN": {"avg": 72.82, "peak": 116.23, "min": 61.0}}, "power_watts_avg": 29.46, "energy_joules_est": 69.0, "sample_count": 18, "duration_seconds": 2.342}, "timestamp": "2026-01-17T15:47:22.247090"}
{"image_index": 390, "image_name": "000000043816.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000043816.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1967.586, "latencies_ms": [1967.586], "images_per_second": 0.508, "prompt_tokens": 19, "response_tokens_est": 47, "n_tiles": 6, "output_text": "The baseball player is wearing a blue helmet and white pinstripe uniform, which stands out against the green grass and brown dirt of the field. The lighting is bright, indicating a sunny day, casting sharp shadows on the field.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24949.2, "ram_available_mb": 100823.0, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 24949.9, "ram_available_mb": 100822.3, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.84, "peak": 15.24, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.4, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 30.8, "peak": 40.16, "min": 23.64}, "VIN": {"avg": 72.38, "peak": 97.95, "min": 57.33}}, "power_watts_avg": 30.8, "energy_joules_est": 60.62, "sample_count": 15, "duration_seconds": 1.968}, "timestamp": "2026-01-17T15:47:24.220854"}
{"image_index": 391, "image_name": "000000044068.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044068.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2492.779, "latencies_ms": [2492.779], "images_per_second": 0.401, "prompt_tokens": 9, "response_tokens_est": 26, "n_tiles": 12, "output_text": "A plush teddy bear with a red and white checkered bow around its neck is resting on a red leather chair.", "error": null, "sys_before": {"cpu_percent": 45.8, "ram_used_mb": 24918.0, "ram_available_mb": 100854.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.5, "ram_used_mb": 24919.4, "ram_available_mb": 100852.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.2, "peak": 16.66, "min": 13.49}, "VDD_CPU_SOC_MSS": {"avg": 15.95, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 36.2, "peak": 43.73, "min": 27.57}, "VIN": {"avg": 83.35, "peak": 134.12, "min": 64.53}}, "power_watts_avg": 36.2, "energy_joules_est": 90.25, "sample_count": 19, "duration_seconds": 2.493}, "timestamp": "2026-01-17T15:47:26.799908"}
{"image_index": 391, "image_name": "000000044068.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044068.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 3231.581, "latencies_ms": [3231.581], "images_per_second": 0.309, "prompt_tokens": 23, "response_tokens_est": 46, "n_tiles": 12, "output_text": "1. teddy bear\n2. chair\n3. cushion\n4. wicker basket\n5. red bow tie\n6. red pillow\n7. blue and white striped fabric\n8. wicker chair", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24919.4, "ram_available_mb": 100852.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 24920.4, "ram_available_mb": 100851.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.24, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.78, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 35.07, "peak": 45.28, "min": 26.39}, "VIN": {"avg": 81.14, "peak": 135.54, "min": 57.57}}, "power_watts_avg": 35.07, "energy_joules_est": 113.35, "sample_count": 25, "duration_seconds": 3.232}, "timestamp": "2026-01-17T15:47:30.037896"}
{"image_index": 391, "image_name": "000000044068.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044068.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 4116.485, "latencies_ms": [4116.485], "images_per_second": 0.243, "prompt_tokens": 27, "response_tokens_est": 72, "n_tiles": 12, "output_text": "The main object in the foreground is a brown teddy bear with a red and white plaid bow around its neck. The bear is resting on a red leather cushion. In the background, there is a wicker chair with a brown cushion. The wicker chair is partially visible, and the brown cushion is placed on top of the wicker chair.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24920.4, "ram_available_mb": 100851.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 24920.6, "ram_available_mb": 100851.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.17, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.66, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 32.96, "peak": 45.28, "min": 26.0}, "VIN": {"avg": 73.78, "peak": 102.61, "min": 55.31}}, "power_watts_avg": 32.96, "energy_joules_est": 135.69, "sample_count": 32, "duration_seconds": 4.117}, "timestamp": "2026-01-17T15:47:34.160681"}
{"image_index": 391, "image_name": "000000044068.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044068.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 4248.818, "latencies_ms": [4248.818], "images_per_second": 0.235, "prompt_tokens": 21, "response_tokens_est": 76, "n_tiles": 12, "output_text": "The image depicts a cozy indoor scene featuring a plush teddy bear with a red and white checkered bow around its neck. The bear is resting on a red leather chair, which is placed on a woven wicker surface. The setting appears to be a living room or a similar indoor space, with a striped blue and white fabric partially visible in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24920.6, "ram_available_mb": 100851.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 24921.9, "ram_available_mb": 100850.3, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.17, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.71, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 32.68, "peak": 45.68, "min": 26.0}, "VIN": {"avg": 74.87, "peak": 124.02, "min": 57.08}}, "power_watts_avg": 32.68, "energy_joules_est": 138.86, "sample_count": 34, "duration_seconds": 4.249}, "timestamp": "2026-01-17T15:47:38.416184"}
{"image_index": 391, "image_name": "000000044068.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044068.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3569.173, "latencies_ms": [3569.173], "images_per_second": 0.28, "prompt_tokens": 19, "response_tokens_est": 56, "n_tiles": 12, "output_text": "The image features a brown teddy bear with a red and white checkered bow around its neck. The bear is resting on a red leather chair, which is placed on a woven wicker surface. The lighting in the room is soft and warm, creating a cozy atmosphere.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24921.9, "ram_available_mb": 100850.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 24921.9, "ram_available_mb": 100850.3, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.19, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.75, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 33.9, "peak": 45.28, "min": 26.0}, "VIN": {"avg": 74.52, "peak": 126.89, "min": 57.64}}, "power_watts_avg": 33.9, "energy_joules_est": 121.01, "sample_count": 28, "duration_seconds": 3.57}, "timestamp": "2026-01-17T15:47:41.992112"}
{"image_index": 392, "image_name": "000000044195.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044195.jpg", "image_width": 361, "image_height": 500, "image_resolution": "361x500", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2193.547, "latencies_ms": [2193.547], "images_per_second": 0.456, "prompt_tokens": 9, "response_tokens_est": 17, "n_tiles": 12, "output_text": "Two people are standing on a snowy mountain slope, holding a snowboard.", "error": null, "sys_before": {"cpu_percent": 37.5, "ram_used_mb": 24931.7, "ram_available_mb": 100840.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.2, "ram_used_mb": 24927.9, "ram_available_mb": 100844.3, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.21, "peak": 16.76, "min": 13.49}, "VDD_CPU_SOC_MSS": {"avg": 15.86, "peak": 17.71, "min": 14.17}, "VDD_GPU": {"avg": 37.63, "peak": 44.51, "min": 29.94}, "VIN": {"avg": 78.87, "peak": 117.77, "min": 66.58}}, "power_watts_avg": 37.63, "energy_joules_est": 82.55, "sample_count": 17, "duration_seconds": 2.194}, "timestamp": "2026-01-17T15:47:44.270827"}
{"image_index": 392, "image_name": "000000044195.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044195.jpg", "image_width": 361, "image_height": 500, "image_resolution": "361x500", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2924.021, "latencies_ms": [2924.021], "images_per_second": 0.342, "prompt_tokens": 23, "response_tokens_est": 37, "n_tiles": 12, "output_text": "1. Snowboard\n2. Person\n3. Snow\n4. Ski\n5. Ski poles\n6. Ski boots\n7. Ski jacket\n8. Ski pants", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24927.9, "ram_available_mb": 100844.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 24927.4, "ram_available_mb": 100844.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.27, "peak": 16.56, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.83, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 36.18, "peak": 45.68, "min": 26.8}, "VIN": {"avg": 80.01, "peak": 140.05, "min": 64.75}}, "power_watts_avg": 36.18, "energy_joules_est": 105.81, "sample_count": 23, "duration_seconds": 2.924}, "timestamp": "2026-01-17T15:47:47.201656"}
{"image_index": 392, "image_name": "000000044195.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044195.jpg", "image_width": 361, "image_height": 500, "image_resolution": "361x500", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 4285.984, "latencies_ms": [4285.984], "images_per_second": 0.233, "prompt_tokens": 27, "response_tokens_est": 77, "n_tiles": 12, "output_text": "The main objects in the image are two individuals standing on a snowy slope. The person on the left is wearing a red jacket and black pants, while the person on the right is wearing a red jacket and black pants. The snowy slope is in the foreground, with the individuals standing near the center of the image. The background features a clear blue sky and distant mountains.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24927.4, "ram_available_mb": 100844.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 24928.1, "ram_available_mb": 100844.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.2, "peak": 16.56, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.8, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 32.79, "peak": 45.68, "min": 26.0}, "VIN": {"avg": 76.16, "peak": 119.23, "min": 65.39}}, "power_watts_avg": 32.79, "energy_joules_est": 140.55, "sample_count": 34, "duration_seconds": 4.286}, "timestamp": "2026-01-17T15:47:51.494261"}
{"image_index": 392, "image_name": "000000044195.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044195.jpg", "image_width": 361, "image_height": 500, "image_resolution": "361x500", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3872.625, "latencies_ms": [3872.625], "images_per_second": 0.258, "prompt_tokens": 21, "response_tokens_est": 65, "n_tiles": 12, "output_text": "The image depicts a snowy mountainous landscape under a clear blue sky. Two individuals are standing on the snow, one holding a snowboard. The scene suggests they are preparing for a snowboarding activity, with the snowboarder's posture indicating readiness and the snowboarder's grip on the board indicating anticipation.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24928.1, "ram_available_mb": 100844.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 24928.4, "ram_available_mb": 100843.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.19, "peak": 16.56, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.8, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 33.53, "peak": 45.68, "min": 26.01}, "VIN": {"avg": 74.2, "peak": 128.0, "min": 54.07}}, "power_watts_avg": 33.53, "energy_joules_est": 129.87, "sample_count": 30, "duration_seconds": 3.873}, "timestamp": "2026-01-17T15:47:55.375353"}
{"image_index": 392, "image_name": "000000044195.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044195.jpg", "image_width": 361, "image_height": 500, "image_resolution": "361x500", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 4249.826, "latencies_ms": [4249.826], "images_per_second": 0.235, "prompt_tokens": 19, "response_tokens_est": 76, "n_tiles": 12, "output_text": "The image depicts a bright, sunny day with clear blue skies. The snowy landscape is illuminated by the sun, casting a warm glow on the snow and creating a vivid contrast between the bright sunlight and the darker shadows. The snow appears to be freshly fallen, with a smooth, powdery texture, and the ground is covered in a light layer of snow.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24928.4, "ram_available_mb": 100843.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 24929.4, "ram_available_mb": 100842.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.18, "peak": 16.66, "min": 14.12}, "VDD_CPU_SOC_MSS": {"avg": 15.68, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 32.74, "peak": 44.49, "min": 26.0}, "VIN": {"avg": 76.91, "peak": 115.19, "min": 62.36}}, "power_watts_avg": 32.74, "energy_joules_est": 139.15, "sample_count": 33, "duration_seconds": 4.25}, "timestamp": "2026-01-17T15:47:59.632361"}
{"image_index": 393, "image_name": "000000044260.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044260.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1676.063, "latencies_ms": [1676.063], "images_per_second": 0.597, "prompt_tokens": 9, "response_tokens_est": 38, "n_tiles": 6, "output_text": "The image depicts a close-up view of a tree branch adorned with several red apples, some of which are hanging from the branch, while the rest are resting on the ground.", "error": null, "sys_before": {"cpu_percent": 23.7, "ram_used_mb": 24951.5, "ram_available_mb": 100820.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 24954.2, "ram_available_mb": 100818.0, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.66, "peak": 15.14, "min": 13.92}, "VDD_CPU_SOC_MSS": {"avg": 15.29, "peak": 16.14, "min": 14.17}, "VDD_GPU": {"avg": 32.37, "peak": 40.95, "min": 24.82}, "VIN": {"avg": 71.73, "peak": 98.57, "min": 55.55}}, "power_watts_avg": 32.37, "energy_joules_est": 54.27, "sample_count": 12, "duration_seconds": 1.677}, "timestamp": "2026-01-17T15:48:01.354552"}
{"image_index": 393, "image_name": "000000044260.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044260.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 926.332, "latencies_ms": [926.332], "images_per_second": 1.08, "prompt_tokens": 23, "response_tokens_est": 10, "n_tiles": 6, "output_text": "apple: 3\ntree: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24954.2, "ram_available_mb": 100818.0, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 24954.9, "ram_available_mb": 100817.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.69, "peak": 15.24, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.29, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 37.55, "peak": 40.57, "min": 32.3}, "VIN": {"avg": 86.4, "peak": 112.21, "min": 61.78}}, "power_watts_avg": 37.55, "energy_joules_est": 34.8, "sample_count": 6, "duration_seconds": 0.927}, "timestamp": "2026-01-17T15:48:02.291301"}
{"image_index": 393, "image_name": "000000044260.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044260.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2481.621, "latencies_ms": [2481.621], "images_per_second": 0.403, "prompt_tokens": 27, "response_tokens_est": 65, "n_tiles": 6, "output_text": "The main objects in the image are a tree branch with red apples hanging from it. The apples are positioned near the center of the image, with the branch extending towards the right side. The background features a blurred natural setting with more trees and foliage, creating a sense of depth and context for the scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24954.9, "ram_available_mb": 100817.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 24956.2, "ram_available_mb": 100816.0, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.85, "peak": 15.14, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.46, "peak": 15.75, "min": 14.56}, "VDD_GPU": {"avg": 30.62, "peak": 42.94, "min": 23.24}, "VIN": {"avg": 70.0, "peak": 105.86, "min": 60.28}}, "power_watts_avg": 30.62, "energy_joules_est": 76.01, "sample_count": 19, "duration_seconds": 2.482}, "timestamp": "2026-01-17T15:48:04.779214"}
{"image_index": 393, "image_name": "000000044260.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044260.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2052.026, "latencies_ms": [2052.026], "images_per_second": 0.487, "prompt_tokens": 21, "response_tokens_est": 50, "n_tiles": 6, "output_text": "The image depicts a rustic scene of an old, gnarled tree with a few red apples hanging from its branches. The background is blurred, but hints of a forested area can be seen, suggesting a natural, outdoor setting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24956.2, "ram_available_mb": 100816.0, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 24957.2, "ram_available_mb": 100815.0, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.88, "peak": 15.24, "min": 14.3}, "VDD_CPU_SOC_MSS": {"avg": 15.43, "peak": 15.75, "min": 14.56}, "VDD_GPU": {"avg": 30.48, "peak": 40.95, "min": 23.64}, "VIN": {"avg": 67.86, "peak": 86.47, "min": 60.6}}, "power_watts_avg": 30.48, "energy_joules_est": 62.57, "sample_count": 16, "duration_seconds": 2.053}, "timestamp": "2026-01-17T15:48:06.839257"}
{"image_index": 393, "image_name": "000000044260.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044260.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2501.812, "latencies_ms": [2501.812], "images_per_second": 0.4, "prompt_tokens": 19, "response_tokens_est": 66, "n_tiles": 6, "output_text": "The image features a tree with a brown trunk and branches, some of which are bare. The leaves are mostly brown and dry, indicating a season of autumn or late summer. The lighting is soft and warm, with a golden hue that suggests the sun is low in the sky, possibly during the early morning or late afternoon.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24957.2, "ram_available_mb": 100815.0, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 24958.4, "ram_available_mb": 100813.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.89, "peak": 15.24, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.5, "peak": 15.75, "min": 14.56}, "VDD_GPU": {"avg": 29.44, "peak": 40.56, "min": 23.24}, "VIN": {"avg": 70.29, "peak": 104.57, "min": 62.67}}, "power_watts_avg": 29.44, "energy_joules_est": 73.66, "sample_count": 19, "duration_seconds": 2.502}, "timestamp": "2026-01-17T15:48:09.351529"}
{"image_index": 394, "image_name": "000000044279.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044279.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2385.763, "latencies_ms": [2385.763], "images_per_second": 0.419, "prompt_tokens": 9, "response_tokens_est": 66, "n_tiles": 6, "output_text": "The image depicts a busy kitchen scene with two men wearing white uniforms, engaged in food preparation. One man is seen stirring ingredients in a large metal pot, while the other is handling a tray of food. The kitchen is equipped with various cooking utensils and appliances, and the overall atmosphere suggests a professional culinary environment.", "error": null, "sys_before": {"cpu_percent": 18.2, "ram_used_mb": 24958.1, "ram_available_mb": 100814.0, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 24958.4, "ram_available_mb": 100813.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.91, "peak": 15.22, "min": 13.9}, "VDD_CPU_SOC_MSS": {"avg": 15.53, "peak": 16.14, "min": 14.57}, "VDD_GPU": {"avg": 29.46, "peak": 40.18, "min": 23.64}, "VIN": {"avg": 73.81, "peak": 112.74, "min": 61.49}}, "power_watts_avg": 29.46, "energy_joules_est": 70.31, "sample_count": 18, "duration_seconds": 2.387}, "timestamp": "2026-01-17T15:48:11.774020"}
{"image_index": 394, "image_name": "000000044279.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044279.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1513.087, "latencies_ms": [1513.087], "images_per_second": 0.661, "prompt_tokens": 23, "response_tokens_est": 32, "n_tiles": 6, "output_text": "1. Chef\n2. Chef\n3. Chef\n4. Chef\n5. Chef\n6. Chef\n7. Chef\n8. Chef", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24958.4, "ram_available_mb": 100813.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 24958.1, "ram_available_mb": 100814.0, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.01, "peak": 15.44, "min": 14.43}, "VDD_CPU_SOC_MSS": {"avg": 15.62, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 32.99, "peak": 40.95, "min": 25.6}, "VIN": {"avg": 72.43, "peak": 94.3, "min": 57.46}}, "power_watts_avg": 32.99, "energy_joules_est": 49.94, "sample_count": 12, "duration_seconds": 1.514}, "timestamp": "2026-01-17T15:48:13.293856"}
{"image_index": 394, "image_name": "000000044279.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044279.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3352.903, "latencies_ms": [3352.903], "images_per_second": 0.298, "prompt_tokens": 27, "response_tokens_est": 100, "n_tiles": 6, "output_text": "In the image, the main objects are the two men working in the kitchen. The man in the foreground is focused on pouring a substance from a container into a bowl, while the man in the background is handling a tray of food. The kitchen is well-equipped with various utensils and appliances, and the countertops are cluttered with various items. The man in the foreground is positioned near the center of the image, while the man in the background is slightly to the left.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24958.1, "ram_available_mb": 100814.0, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 24959.6, "ram_available_mb": 100812.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.13, "peak": 15.44, "min": 14.43}, "VDD_CPU_SOC_MSS": {"avg": 15.73, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 28.14, "peak": 40.57, "min": 23.25}, "VIN": {"avg": 69.63, "peak": 97.38, "min": 62.01}}, "power_watts_avg": 28.14, "energy_joules_est": 94.36, "sample_count": 26, "duration_seconds": 3.353}, "timestamp": "2026-01-17T15:48:16.652970"}
{"image_index": 394, "image_name": "000000044279.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044279.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2485.582, "latencies_ms": [2485.582], "images_per_second": 0.402, "prompt_tokens": 21, "response_tokens_est": 68, "n_tiles": 6, "output_text": "The image depicts a bustling kitchen scene with two men wearing white uniforms, engaged in food preparation. They are surrounded by various kitchen tools and equipment, including a large metal pot, a cutting board, and a stainless steel countertop. The setting appears to be a professional kitchen, likely in a restaurant or a food service establishment.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24959.6, "ram_available_mb": 100812.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 24959.4, "ram_available_mb": 100812.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.08, "peak": 15.54, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.64, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 29.48, "peak": 40.16, "min": 23.64}, "VIN": {"avg": 70.37, "peak": 107.64, "min": 50.64}}, "power_watts_avg": 29.48, "energy_joules_est": 73.29, "sample_count": 19, "duration_seconds": 2.486}, "timestamp": "2026-01-17T15:48:19.145047"}
{"image_index": 394, "image_name": "000000044279.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044279.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1944.028, "latencies_ms": [1944.028], "images_per_second": 0.514, "prompt_tokens": 19, "response_tokens_est": 48, "n_tiles": 6, "output_text": "The image depicts a bustling kitchen scene with a metallic and industrial aesthetic. The lighting is bright, casting a warm glow over the workspace. The materials used include stainless steel surfaces, metal utensils, and various kitchen tools.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24959.4, "ram_available_mb": 100812.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 24959.6, "ram_available_mb": 100812.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.03, "peak": 15.54, "min": 14.43}, "VDD_CPU_SOC_MSS": {"avg": 15.61, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 31.43, "peak": 40.18, "min": 24.42}, "VIN": {"avg": 70.38, "peak": 104.69, "min": 59.16}}, "power_watts_avg": 31.43, "energy_joules_est": 61.11, "sample_count": 14, "duration_seconds": 1.944}, "timestamp": "2026-01-17T15:48:21.095354"}
{"image_index": 395, "image_name": "000000044590.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044590.jpg", "image_width": 640, "image_height": 246, "image_resolution": "640x246", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2194.632, "latencies_ms": [2194.632], "images_per_second": 0.456, "prompt_tokens": 9, "response_tokens_est": 31, "n_tiles": 10, "output_text": "A group of people is gathered around a row of motorcycles parked on the side of a road, with some standing and others sitting on the bikes.", "error": null, "sys_before": {"cpu_percent": 30.3, "ram_used_mb": 24915.1, "ram_available_mb": 100857.0, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 24916.4, "ram_available_mb": 100855.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4613.5, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 6643.4, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.98, "peak": 16.15, "min": 14.0}, "VDD_CPU_SOC_MSS": {"avg": 15.61, "peak": 16.92, "min": 14.56}, "VDD_GPU": {"avg": 35.47, "peak": 42.92, "min": 26.79}, "VIN": {"avg": 76.38, "peak": 116.81, "min": 64.88}}, "power_watts_avg": 35.47, "energy_joules_est": 77.85, "sample_count": 17, "duration_seconds": 2.195}, "timestamp": "2026-01-17T15:48:23.340894"}
{"image_index": 395, "image_name": "000000044590.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044590.jpg", "image_width": 640, "image_height": 246, "image_resolution": "640x246", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1605.349, "latencies_ms": [1605.349], "images_per_second": 0.623, "prompt_tokens": 23, "response_tokens_est": 11, "n_tiles": 10, "output_text": "motorcycle: 5\npeople: 3", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24916.4, "ram_available_mb": 100855.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.2, "ram_used_mb": 24917.1, "ram_available_mb": 100855.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4613.5, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 6654.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.99, "peak": 15.95, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.51, "peak": 16.92, "min": 14.56}, "VDD_GPU": {"avg": 39.77, "peak": 44.1, "min": 32.7}, "VIN": {"avg": 89.79, "peak": 132.13, "min": 67.49}}, "power_watts_avg": 39.77, "energy_joules_est": 63.86, "sample_count": 12, "duration_seconds": 1.606}, "timestamp": "2026-01-17T15:48:24.952564"}
{"image_index": 395, "image_name": "000000044590.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044590.jpg", "image_width": 640, "image_height": 246, "image_resolution": "640x246", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3147.953, "latencies_ms": [3147.953], "images_per_second": 0.318, "prompt_tokens": 27, "response_tokens_est": 60, "n_tiles": 10, "output_text": "The group of men is standing near the parked motorcycles, with the motorcycles positioned on the right side of the road. The motorcycles are in the foreground, while the men are in the background. The motorcycles are near the road, and the men are standing on the roadside.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24917.1, "ram_available_mb": 100855.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 24917.1, "ram_available_mb": 100855.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4613.5, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 6657.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.96, "peak": 15.95, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.53, "peak": 16.92, "min": 14.56}, "VDD_GPU": {"avg": 34.2, "peak": 45.68, "min": 25.21}, "VIN": {"avg": 80.56, "peak": 132.7, "min": 66.98}}, "power_watts_avg": 34.2, "energy_joules_est": 107.67, "sample_count": 24, "duration_seconds": 3.148}, "timestamp": "2026-01-17T15:48:28.108024"}
{"image_index": 395, "image_name": "000000044590.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044590.jpg", "image_width": 640, "image_height": 246, "image_resolution": "640x246", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3138.56, "latencies_ms": [3138.56], "images_per_second": 0.319, "prompt_tokens": 21, "response_tokens_est": 60, "n_tiles": 10, "output_text": "The image depicts a group of people gathered around a row of motorcycles parked on the side of a road. The setting appears to be outdoors during the daytime, with a cloudy sky overhead. The group seems to be engaged in a conversation, possibly discussing the motorcycles or their plans.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24917.1, "ram_available_mb": 100855.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 24917.8, "ram_available_mb": 100854.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4613.5, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 6652.6, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.94, "peak": 15.95, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.46, "peak": 16.92, "min": 14.56}, "VDD_GPU": {"avg": 33.18, "peak": 43.71, "min": 25.6}, "VIN": {"avg": 76.6, "peak": 111.43, "min": 61.3}}, "power_watts_avg": 33.18, "energy_joules_est": 104.15, "sample_count": 25, "duration_seconds": 3.139}, "timestamp": "2026-01-17T15:48:31.255252"}
{"image_index": 395, "image_name": "000000044590.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044590.jpg", "image_width": 640, "image_height": 246, "image_resolution": "640x246", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3300.198, "latencies_ms": [3300.198], "images_per_second": 0.303, "prompt_tokens": 19, "response_tokens_est": 65, "n_tiles": 10, "output_text": "The image depicts a group of people standing next to a line of motorcycles on a roadside. The sky is overcast, casting a soft, diffused light over the scene. The motorcycles are parked on the side of the road, and the people are dressed in casual attire suitable for cool weather.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24917.8, "ram_available_mb": 100854.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 24918.3, "ram_available_mb": 100853.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4613.5, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 6651.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.92, "peak": 15.95, "min": 14.12}, "VDD_CPU_SOC_MSS": {"avg": 15.5, "peak": 16.92, "min": 14.56}, "VDD_GPU": {"avg": 32.77, "peak": 43.31, "min": 25.21}, "VIN": {"avg": 73.92, "peak": 122.25, "min": 60.89}}, "power_watts_avg": 32.77, "energy_joules_est": 108.16, "sample_count": 26, "duration_seconds": 3.301}, "timestamp": "2026-01-17T15:48:34.561740"}
{"image_index": 396, "image_name": "000000044652.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044652.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1657.701, "latencies_ms": [1657.701], "images_per_second": 0.603, "prompt_tokens": 9, "response_tokens_est": 39, "n_tiles": 6, "output_text": "A black and white photograph captures a single-engine airplane in mid-flight, with its propellers spinning and the aircraft's tail and wings clearly visible against a backdrop of a cloudy sky.", "error": null, "sys_before": {"cpu_percent": 43.2, "ram_used_mb": 24953.8, "ram_available_mb": 100818.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 24956.2, "ram_available_mb": 100816.0, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.73, "peak": 15.11, "min": 13.69}, "VDD_CPU_SOC_MSS": {"avg": 15.47, "peak": 16.14, "min": 14.17}, "VDD_GPU": {"avg": 32.06, "peak": 40.56, "min": 24.82}, "VIN": {"avg": 73.86, "peak": 115.86, "min": 60.65}}, "power_watts_avg": 32.06, "energy_joules_est": 53.16, "sample_count": 13, "duration_seconds": 1.658}, "timestamp": "2026-01-17T15:48:36.280428"}
{"image_index": 396, "image_name": "000000044652.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044652.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1733.252, "latencies_ms": [1733.252], "images_per_second": 0.577, "prompt_tokens": 23, "response_tokens_est": 40, "n_tiles": 6, "output_text": "object: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24956.5, "ram_available_mb": 100815.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 24956.5, "ram_available_mb": 100815.7, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.0, "peak": 15.54, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.66, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 32.3, "peak": 40.56, "min": 24.82}, "VIN": {"avg": 76.11, "peak": 112.52, "min": 62.7}}, "power_watts_avg": 32.3, "energy_joules_est": 56.0, "sample_count": 13, "duration_seconds": 1.734}, "timestamp": "2026-01-17T15:48:38.020015"}
{"image_index": 396, "image_name": "000000044652.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044652.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2083.879, "latencies_ms": [2083.879], "images_per_second": 0.48, "prompt_tokens": 27, "response_tokens_est": 53, "n_tiles": 6, "output_text": "The main object in the image is a small airplane, which is positioned in the foreground. The airplane is flying towards the right side of the frame. The background consists of a cloudy sky, which is slightly blurred, indicating the airplane is moving quickly.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24956.5, "ram_available_mb": 100815.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 24957.7, "ram_available_mb": 100814.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.04, "peak": 15.44, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.64, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 31.22, "peak": 40.56, "min": 24.03}, "VIN": {"avg": 72.73, "peak": 101.23, "min": 61.84}}, "power_watts_avg": 31.22, "energy_joules_est": 65.07, "sample_count": 15, "duration_seconds": 2.084}, "timestamp": "2026-01-17T15:48:40.114941"}
{"image_index": 396, "image_name": "000000044652.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044652.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1860.267, "latencies_ms": [1860.267], "images_per_second": 0.538, "prompt_tokens": 21, "response_tokens_est": 45, "n_tiles": 6, "output_text": "The image depicts a black and white scene of a small airplane in flight, with its propellers spinning. The airplane is flying against a backdrop of a cloudy sky, creating a dramatic and somewhat moody atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24957.7, "ram_available_mb": 100814.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 24957.7, "ram_available_mb": 100814.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.01, "peak": 15.54, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.63, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 31.75, "peak": 40.16, "min": 24.42}, "VIN": {"avg": 74.59, "peak": 111.01, "min": 55.18}}, "power_watts_avg": 31.75, "energy_joules_est": 59.08, "sample_count": 13, "duration_seconds": 1.861}, "timestamp": "2026-01-17T15:48:41.985984"}
{"image_index": 396, "image_name": "000000044652.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044652.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2405.819, "latencies_ms": [2405.819], "images_per_second": 0.416, "prompt_tokens": 19, "response_tokens_est": 65, "n_tiles": 6, "output_text": "The image features a black and white photograph of a small airplane in flight, with a cloudy sky in the background. The airplane is predominantly white with black markings and has a propeller at the front. The lighting is natural, likely from the sun, casting shadows and highlighting the contours of the airplane and clouds.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24957.7, "ram_available_mb": 100814.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 24957.3, "ram_available_mb": 100814.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.07, "peak": 15.44, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.64, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 29.94, "peak": 40.56, "min": 23.64}, "VIN": {"avg": 73.33, "peak": 124.78, "min": 61.06}}, "power_watts_avg": 29.94, "energy_joules_est": 72.04, "sample_count": 18, "duration_seconds": 2.406}, "timestamp": "2026-01-17T15:48:44.397835"}
{"image_index": 397, "image_name": "000000044699.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044699.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1412.576, "latencies_ms": [1412.576], "images_per_second": 0.708, "prompt_tokens": 9, "response_tokens_est": 30, "n_tiles": 6, "output_text": "A group of sheep stands on a rocky hillside with a serene lake in the background, surrounded by green hills and a clear blue sky.", "error": null, "sys_before": {"cpu_percent": 40.5, "ram_used_mb": 24957.3, "ram_available_mb": 100814.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 24958.6, "ram_available_mb": 100813.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.72, "peak": 15.14, "min": 13.9}, "VDD_CPU_SOC_MSS": {"avg": 15.43, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 33.52, "peak": 40.57, "min": 26.4}, "VIN": {"avg": 76.08, "peak": 104.53, "min": 54.68}}, "power_watts_avg": 33.52, "energy_joules_est": 47.36, "sample_count": 10, "duration_seconds": 1.413}, "timestamp": "2026-01-17T15:48:45.868357"}
{"image_index": 397, "image_name": "000000044699.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044699.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1764.753, "latencies_ms": [1764.753], "images_per_second": 0.567, "prompt_tokens": 23, "response_tokens_est": 40, "n_tiles": 6, "output_text": "object: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24958.6, "ram_available_mb": 100813.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 24957.9, "ram_available_mb": 100814.3, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.87, "peak": 15.34, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.47, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 32.51, "peak": 41.34, "min": 24.43}, "VIN": {"avg": 73.81, "peak": 104.67, "min": 62.6}}, "power_watts_avg": 32.51, "energy_joules_est": 57.39, "sample_count": 13, "duration_seconds": 1.765}, "timestamp": "2026-01-17T15:48:47.639656"}
{"image_index": 397, "image_name": "000000044699.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044699.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3506.568, "latencies_ms": [3506.568], "images_per_second": 0.285, "prompt_tokens": 27, "response_tokens_est": 101, "n_tiles": 6, "output_text": "In the image, the main objects are a group of sheep located in the foreground and a body of water in the background. The sheep are positioned on a rocky terrain, with the largest sheep standing in the center, slightly to the right. The background features a serene body of water, likely a lake or a calm sea, surrounded by rolling hills and mountains. The sheep appear to be grazing or resting on the rocky outcrop, while the water body provides a peaceful and tranquil setting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24957.7, "ram_available_mb": 100814.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 24957.7, "ram_available_mb": 100814.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.9, "peak": 15.24, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.37, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 27.5, "peak": 40.95, "min": 22.86}, "VIN": {"avg": 70.45, "peak": 114.24, "min": 61.31}}, "power_watts_avg": 27.5, "energy_joules_est": 96.44, "sample_count": 27, "duration_seconds": 3.507}, "timestamp": "2026-01-17T15:48:51.152677"}
{"image_index": 397, "image_name": "000000044699.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044699.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1963.977, "latencies_ms": [1963.977], "images_per_second": 0.509, "prompt_tokens": 21, "response_tokens_est": 47, "n_tiles": 6, "output_text": "The image depicts a serene landscape featuring a group of sheep grazing on a grassy hillside. The sheep are surrounded by a picturesque view of a calm, turquoise lake nestled between rolling hills and mountains.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 24957.7, "ram_available_mb": 100814.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 24958.1, "ram_available_mb": 100814.0, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.83, "peak": 15.24, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.27, "peak": 15.74, "min": 14.56}, "VDD_GPU": {"avg": 30.75, "peak": 40.16, "min": 23.64}, "VIN": {"avg": 74.36, "peak": 115.12, "min": 58.59}}, "power_watts_avg": 30.75, "energy_joules_est": 60.41, "sample_count": 15, "duration_seconds": 1.964}, "timestamp": "2026-01-17T15:48:53.123223"}
{"image_index": 397, "image_name": "000000044699.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044699.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2049.409, "latencies_ms": [2049.409], "images_per_second": 0.488, "prompt_tokens": 19, "response_tokens_est": 50, "n_tiles": 6, "output_text": "The image depicts a serene landscape with a group of sheep standing on a rocky hillside. The sheep are predominantly white, with some having darker patches. The scene is bathed in natural sunlight, casting soft shadows on the rocky terrain.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24958.1, "ram_available_mb": 100814.0, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 24957.6, "ram_available_mb": 100814.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.86, "peak": 15.24, "min": 14.3}, "VDD_CPU_SOC_MSS": {"avg": 15.33, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 30.96, "peak": 40.56, "min": 23.64}, "VIN": {"avg": 74.72, "peak": 115.47, "min": 60.44}}, "power_watts_avg": 30.96, "energy_joules_est": 63.46, "sample_count": 15, "duration_seconds": 2.05}, "timestamp": "2026-01-17T15:48:55.182946"}
{"image_index": 398, "image_name": "000000044877.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044877.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1201.375, "latencies_ms": [1201.375], "images_per_second": 0.832, "prompt_tokens": 9, "response_tokens_est": 22, "n_tiles": 6, "output_text": "The image shows a woman in a wheelchair holding a tennis racket, with another person in the background.", "error": null, "sys_before": {"cpu_percent": 40.8, "ram_used_mb": 24957.6, "ram_available_mb": 100814.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 24957.6, "ram_available_mb": 100814.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.57, "peak": 15.04, "min": 13.69}, "VDD_CPU_SOC_MSS": {"avg": 15.26, "peak": 16.14, "min": 14.17}, "VDD_GPU": {"avg": 34.18, "peak": 40.16, "min": 27.97}, "VIN": {"avg": 78.92, "peak": 117.53, "min": 58.28}}, "power_watts_avg": 34.18, "energy_joules_est": 41.07, "sample_count": 9, "duration_seconds": 1.202}, "timestamp": "2026-01-17T15:48:56.444628"}
{"image_index": 398, "image_name": "000000044877.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044877.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1885.25, "latencies_ms": [1885.25], "images_per_second": 0.53, "prompt_tokens": 23, "response_tokens_est": 46, "n_tiles": 6, "output_text": "1. Woman in wheelchair\n2. Tennis racket\n3. Wheelchair\n4. Tennis ball\n5. Tennis court\n6. Tennis player\n7. Tennis player's shirt\n8. Tennis player's shorts", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24957.6, "ram_available_mb": 100814.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 24957.6, "ram_available_mb": 100814.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.0, "peak": 15.34, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.61, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 32.55, "peak": 41.34, "min": 24.83}, "VIN": {"avg": 73.45, "peak": 112.03, "min": 49.61}}, "power_watts_avg": 32.55, "energy_joules_est": 61.38, "sample_count": 14, "duration_seconds": 1.886}, "timestamp": "2026-01-17T15:48:58.336540"}
{"image_index": 398, "image_name": "000000044877.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044877.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2667.902, "latencies_ms": [2667.902], "images_per_second": 0.375, "prompt_tokens": 27, "response_tokens_est": 73, "n_tiles": 6, "output_text": "The main objects in the image are a woman in a wheelchair and a tennis racket. The woman is positioned in the foreground, with the racket held in her right hand. The background features another person, slightly out of focus, and a wall or structure. The wheelchair is positioned near the woman, and the racket is held near her right hand.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24957.6, "ram_available_mb": 100814.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 24958.6, "ram_available_mb": 100813.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.04, "peak": 15.44, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.65, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 29.27, "peak": 40.18, "min": 22.86}, "VIN": {"avg": 72.46, "peak": 105.82, "min": 62.18}}, "power_watts_avg": 29.27, "energy_joules_est": 78.1, "sample_count": 20, "duration_seconds": 2.668}, "timestamp": "2026-01-17T15:49:01.010534"}
{"image_index": 398, "image_name": "000000044877.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044877.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2453.195, "latencies_ms": [2453.195], "images_per_second": 0.408, "prompt_tokens": 21, "response_tokens_est": 64, "n_tiles": 6, "output_text": "The scene is set in an indoor sports facility, likely a tennis court, where a woman in a wheelchair is actively playing tennis. She is holding a tennis racket and appears to be focused on the game. The setting is simple and functional, with a plain background that does not distract from the main subject.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24958.6, "ram_available_mb": 100813.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 24958.9, "ram_available_mb": 100813.3, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.85, "peak": 15.24, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.46, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 29.52, "peak": 40.16, "min": 23.25}, "VIN": {"avg": 71.66, "peak": 119.42, "min": 58.3}}, "power_watts_avg": 29.52, "energy_joules_est": 72.43, "sample_count": 18, "duration_seconds": 2.454}, "timestamp": "2026-01-17T15:49:03.470045"}
{"image_index": 398, "image_name": "000000044877.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044877.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2223.2, "latencies_ms": [2223.2], "images_per_second": 0.45, "prompt_tokens": 19, "response_tokens_est": 56, "n_tiles": 6, "output_text": "The woman in the wheelchair is wearing a gray t-shirt with a white logo and a pair of shorts. The lighting in the image is natural, suggesting it is daytime. The background is out of focus, but it appears to be an outdoor setting with a light-colored wall.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24958.9, "ram_available_mb": 100813.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 24958.6, "ram_available_mb": 100813.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.85, "peak": 15.24, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.4, "peak": 15.75, "min": 14.56}, "VDD_GPU": {"avg": 30.01, "peak": 40.57, "min": 23.24}, "VIN": {"avg": 70.84, "peak": 116.4, "min": 53.63}}, "power_watts_avg": 30.01, "energy_joules_est": 66.73, "sample_count": 17, "duration_seconds": 2.223}, "timestamp": "2026-01-17T15:49:05.699130"}
{"image_index": 399, "image_name": "000000045070.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000045070.jpg", "image_width": 640, "image_height": 596, "image_resolution": "640x596", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 847.597, "latencies_ms": [847.597], "images_per_second": 1.18, "prompt_tokens": 9, "response_tokens_est": 35, "n_tiles": 1, "output_text": "A young girl in a black helmet and plaid shirt is sitting on a purple horse, with a brown saddle and a purple blanket, in a lush green forest setting.", "error": null, "sys_before": {"cpu_percent": 24.2, "ram_used_mb": 24958.6, "ram_available_mb": 100813.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24959.1, "ram_available_mb": 100813.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 4845.7, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.06, "peak": 15.42, "min": 14.51}, "VDD_CPU_SOC_MSS": {"avg": 15.81, "peak": 16.54, "min": 14.96}, "VDD_GPU": {"avg": 22.66, "peak": 25.22, "min": 20.88}, "VIN": {"avg": 63.13, "peak": 68.52, "min": 61.37}}, "power_watts_avg": 22.66, "energy_joules_est": 19.21, "sample_count": 6, "duration_seconds": 0.848}, "timestamp": "2026-01-17T15:49:06.574108"}
{"image_index": 399, "image_name": "000000045070.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000045070.jpg", "image_width": 640, "image_height": 596, "image_resolution": "640x596", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1317.848, "latencies_ms": [1317.848], "images_per_second": 0.759, "prompt_tokens": 23, "response_tokens_est": 57, "n_tiles": 1, "output_text": "- child: 1\n- helmet: 1\n- shirt: 1\n- pants: 1\n- saddle: 1\n- horse: 1\n- saddle pad: 1\n- bridle: 1\n- reins: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24959.1, "ram_available_mb": 100813.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24959.4, "ram_available_mb": 100812.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 4856.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.56, "peak": 15.72, "min": 15.22}, "VDD_CPU_SOC_MSS": {"avg": 16.61, "peak": 16.94, "min": 16.14}, "VDD_GPU": {"avg": 21.51, "peak": 24.43, "min": 20.09}, "VIN": {"avg": 63.86, "peak": 67.58, "min": 62.14}}, "power_watts_avg": 21.51, "energy_joules_est": 28.35, "sample_count": 10, "duration_seconds": 1.318}, "timestamp": "2026-01-17T15:49:07.898446"}
{"image_index": 399, "image_name": "000000045070.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000045070.jpg", "image_width": 640, "image_height": 596, "image_resolution": "640x596", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1079.044, "latencies_ms": [1079.044], "images_per_second": 0.927, "prompt_tokens": 27, "response_tokens_est": 46, "n_tiles": 1, "output_text": "The main object in the foreground is a young child sitting on a purple horse saddle. The child is wearing a black helmet and a pink and white plaid shirt. The background features a blurred natural setting with green trees.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 24959.4, "ram_available_mb": 100812.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24959.3, "ram_available_mb": 100812.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 4859.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.56, "peak": 15.72, "min": 15.22}, "VDD_CPU_SOC_MSS": {"avg": 16.54, "peak": 16.94, "min": 16.14}, "VDD_GPU": {"avg": 21.57, "peak": 24.03, "min": 20.09}, "VIN": {"avg": 63.26, "peak": 66.15, "min": 61.12}}, "power_watts_avg": 21.57, "energy_joules_est": 23.28, "sample_count": 8, "duration_seconds": 1.079}, "timestamp": "2026-01-17T15:49:08.983546"}
{"image_index": 399, "image_name": "000000045070.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000045070.jpg", "image_width": 640, "image_height": 596, "image_resolution": "640x596", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1405.551, "latencies_ms": [1405.551], "images_per_second": 0.711, "prompt_tokens": 21, "response_tokens_est": 61, "n_tiles": 1, "output_text": "In the image, a young child is seated on a purple horse, wearing a black helmet and a pink and white plaid shirt. The child is looking down, possibly at something on the ground. The background features lush green trees, indicating that the scene takes place outdoors in a forested area.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24959.3, "ram_available_mb": 100812.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24959.3, "ram_available_mb": 100812.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 4854.7, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.55, "peak": 15.72, "min": 15.22}, "VDD_CPU_SOC_MSS": {"avg": 16.66, "peak": 16.94, "min": 16.15}, "VDD_GPU": {"avg": 21.47, "peak": 24.43, "min": 20.09}, "VIN": {"avg": 64.1, "peak": 67.31, "min": 61.49}}, "power_watts_avg": 21.47, "energy_joules_est": 30.18, "sample_count": 10, "duration_seconds": 1.406}, "timestamp": "2026-01-17T15:49:10.399013"}
{"image_index": 399, "image_name": "000000045070.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000045070.jpg", "image_width": 640, "image_height": 596, "image_resolution": "640x596", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1273.549, "latencies_ms": [1273.549], "images_per_second": 0.785, "prompt_tokens": 19, "response_tokens_est": 55, "n_tiles": 1, "output_text": "The notable visual attributes of the image include a young child wearing a black helmet, a pink and white plaid shirt, blue jeans, and brown boots. The child is seated on a purple horse saddle, with a green tree background and a hint of overcast lighting.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 24959.3, "ram_available_mb": 100812.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24959.6, "ram_available_mb": 100812.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 4853.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.54, "peak": 15.62, "min": 15.22}, "VDD_CPU_SOC_MSS": {"avg": 16.54, "peak": 16.93, "min": 16.14}, "VDD_GPU": {"avg": 21.41, "peak": 24.03, "min": 20.1}, "VIN": {"avg": 63.72, "peak": 69.52, "min": 57.47}}, "power_watts_avg": 21.41, "energy_joules_est": 27.27, "sample_count": 9, "duration_seconds": 1.274}, "timestamp": "2026-01-17T15:49:11.678486"}
{"image_index": 400, "image_name": "000000045090.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000045090.jpg", "image_width": 500, "image_height": 259, "image_resolution": "500x259", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1084.173, "latencies_ms": [1084.173], "images_per_second": 0.922, "prompt_tokens": 9, "response_tokens_est": 40, "n_tiles": 2, "output_text": "The image captures a dynamic scene of surfers riding waves in the ocean, with one surfer in the foreground skillfully maneuvering a surfboard amidst the frothy, blue-green waters.", "error": null, "sys_before": {"cpu_percent": 44.0, "ram_used_mb": 24959.6, "ram_available_mb": 100812.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 24960.6, "ram_available_mb": 100811.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5055.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.13, "peak": 15.52, "min": 14.71}, "VDD_CPU_SOC_MSS": {"avg": 15.9, "peak": 16.14, "min": 15.74}, "VDD_GPU": {"avg": 24.82, "peak": 29.94, "min": 21.67}, "VIN": {"avg": 64.48, "peak": 78.25, "min": 58.07}}, "power_watts_avg": 24.82, "energy_joules_est": 26.92, "sample_count": 8, "duration_seconds": 1.085}, "timestamp": "2026-01-17T15:49:12.782768"}
{"image_index": 400, "image_name": "000000045090.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000045090.jpg", "image_width": 500, "image_height": 259, "image_resolution": "500x259", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1095.809, "latencies_ms": [1095.809], "images_per_second": 0.913, "prompt_tokens": 23, "response_tokens_est": 40, "n_tiles": 2, "output_text": "object: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24960.6, "ram_available_mb": 100811.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 24960.6, "ram_available_mb": 100811.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5064.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.08, "peak": 15.32, "min": 14.71}, "VDD_CPU_SOC_MSS": {"avg": 15.85, "peak": 16.14, "min": 15.35}, "VDD_GPU": {"avg": 25.41, "peak": 30.33, "min": 22.07}, "VIN": {"avg": 66.05, "peak": 83.88, "min": 61.7}}, "power_watts_avg": 25.41, "energy_joules_est": 27.86, "sample_count": 8, "duration_seconds": 1.096}, "timestamp": "2026-01-17T15:49:13.885524"}
{"image_index": 400, "image_name": "000000045090.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000045090.jpg", "image_width": 500, "image_height": 259, "image_resolution": "500x259", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1132.63, "latencies_ms": [1132.63], "images_per_second": 0.883, "prompt_tokens": 27, "response_tokens_est": 41, "n_tiles": 2, "output_text": "The main objects in the image are the ocean waves and the surfers. The surfers are positioned in the foreground, riding the waves. The waves are in the background, creating a dynamic scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24960.6, "ram_available_mb": 100811.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 24960.8, "ram_available_mb": 100811.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5067.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.8, "peak": 15.11, "min": 14.61}, "VDD_CPU_SOC_MSS": {"avg": 15.85, "peak": 16.14, "min": 15.36}, "VDD_GPU": {"avg": 25.32, "peak": 31.12, "min": 21.68}, "VIN": {"avg": 65.99, "peak": 93.76, "min": 55.72}}, "power_watts_avg": 25.32, "energy_joules_est": 28.69, "sample_count": 8, "duration_seconds": 1.133}, "timestamp": "2026-01-17T15:49:15.024008"}
{"image_index": 400, "image_name": "000000045090.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000045090.jpg", "image_width": 500, "image_height": 259, "image_resolution": "500x259", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1655.119, "latencies_ms": [1655.119], "images_per_second": 0.604, "prompt_tokens": 21, "response_tokens_est": 64, "n_tiles": 2, "output_text": "The image captures a dynamic scene of surfers riding waves in a large body of water, likely the ocean. The surfers are dressed in wetsuits, and the waves are breaking powerfully, creating white foam. The setting is a coastal area, and the surfers are engaged in the sport of surfing.", "error": null, "sys_before": {"cpu_percent": 50.0, "ram_used_mb": 24960.8, "ram_available_mb": 100811.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 24960.8, "ram_available_mb": 100811.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5063.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.06, "peak": 15.32, "min": 14.51}, "VDD_CPU_SOC_MSS": {"avg": 15.88, "peak": 16.14, "min": 15.35}, "VDD_GPU": {"avg": 23.74, "peak": 30.33, "min": 20.89}, "VIN": {"avg": 65.97, "peak": 93.18, "min": 57.08}}, "power_watts_avg": 23.74, "energy_joules_est": 39.3, "sample_count": 12, "duration_seconds": 1.655}, "timestamp": "2026-01-17T15:49:16.685426"}
{"image_index": 400, "image_name": "000000045090.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000045090.jpg", "image_width": 500, "image_height": 259, "image_resolution": "500x259", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1214.149, "latencies_ms": [1214.149], "images_per_second": 0.824, "prompt_tokens": 19, "response_tokens_est": 45, "n_tiles": 2, "output_text": "The image captures a dynamic scene of surfers riding waves in a blue ocean. The surfers are dressed in black wetsuits, and the waves are breaking with white foam, indicating a sunny day with clear skies.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24960.8, "ram_available_mb": 100811.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 24960.8, "ram_available_mb": 100811.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5062.6, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.05, "peak": 15.32, "min": 14.61}, "VDD_CPU_SOC_MSS": {"avg": 15.84, "peak": 16.14, "min": 15.35}, "VDD_GPU": {"avg": 24.69, "peak": 30.73, "min": 21.28}, "VIN": {"avg": 65.11, "peak": 87.03, "min": 54.86}}, "power_watts_avg": 24.69, "energy_joules_est": 29.99, "sample_count": 9, "duration_seconds": 1.215}, "timestamp": "2026-01-17T15:49:17.914276"}
{"image_index": 401, "image_name": "000000045229.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000045229.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2472.01, "latencies_ms": [2472.01], "images_per_second": 0.405, "prompt_tokens": 9, "response_tokens_est": 25, "n_tiles": 12, "output_text": "The image shows a cozy kitchen with a window, a refrigerator, and various kitchen items on the countertops and shelves.", "error": null, "sys_before": {"cpu_percent": 44.2, "ram_used_mb": 24915.7, "ram_available_mb": 100856.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.5, "ram_used_mb": 24916.7, "ram_available_mb": 100855.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.21, "peak": 16.66, "min": 13.9}, "VDD_CPU_SOC_MSS": {"avg": 15.97, "peak": 17.71, "min": 14.96}, "VDD_GPU": {"avg": 36.01, "peak": 43.33, "min": 27.18}, "VIN": {"avg": 78.69, "peak": 106.65, "min": 64.04}}, "power_watts_avg": 36.01, "energy_joules_est": 89.03, "sample_count": 19, "duration_seconds": 2.472}, "timestamp": "2026-01-17T15:49:20.458337"}
{"image_index": 401, "image_name": "000000045229.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000045229.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 6089.499, "latencies_ms": [6089.499], "images_per_second": 0.164, "prompt_tokens": 23, "response_tokens_est": 129, "n_tiles": 12, "output_text": "- window: 2\n- cabinet: 1\n- plant: 1\n- shelf: 1\n- drawer: 1\n- stove: 1\n- cup: 1\n- drawer: 1\n- drawer: 1\n- drawer: 1\n- drawer: 1\n- drawer: 1\n- drawer: 1\n- drawer: 1\n- drawer: 1\n- drawer: 1\n- drawer: 1\n- drawer: 1\n- drawer: 1\n- drawer: 1\n- drawer: 1\n- drawer", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24916.7, "ram_available_mb": 100855.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 24917.9, "ram_available_mb": 100854.3, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.83, "peak": 16.26, "min": 14.12}, "VDD_CPU_SOC_MSS": {"avg": 15.23, "peak": 16.92, "min": 14.56}, "VDD_GPU": {"avg": 30.55, "peak": 45.3, "min": 25.6}, "VIN": {"avg": 71.27, "peak": 125.9, "min": 56.71}}, "power_watts_avg": 30.55, "energy_joules_est": 186.05, "sample_count": 47, "duration_seconds": 6.09}, "timestamp": "2026-01-17T15:49:26.554659"}
{"image_index": 401, "image_name": "000000045229.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000045229.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3730.064, "latencies_ms": [3730.064], "images_per_second": 0.268, "prompt_tokens": 27, "response_tokens_est": 60, "n_tiles": 12, "output_text": "The main objects in the image are located in the kitchen. The refrigerator is situated to the left, with various items stored inside. The stove is in the foreground, with a few pots and pans on it. The window is in the background, allowing natural light to illuminate the kitchen.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24917.9, "ram_available_mb": 100854.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 24918.9, "ram_available_mb": 100853.3, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.9, "peak": 16.36, "min": 13.92}, "VDD_CPU_SOC_MSS": {"avg": 15.34, "peak": 17.32, "min": 14.17}, "VDD_GPU": {"avg": 33.6, "peak": 45.3, "min": 25.6}, "VIN": {"avg": 75.03, "peak": 114.11, "min": 64.4}}, "power_watts_avg": 33.6, "energy_joules_est": 125.34, "sample_count": 29, "duration_seconds": 3.73}, "timestamp": "2026-01-17T15:49:30.295626"}
{"image_index": 401, "image_name": "000000045229.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000045229.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3649.225, "latencies_ms": [3649.225], "images_per_second": 0.274, "prompt_tokens": 21, "response_tokens_est": 58, "n_tiles": 12, "output_text": "The image depicts a cozy kitchen scene with warm lighting. The kitchen is well-organized, featuring a wooden cabinet with a window, a refrigerator, and various kitchen items on the countertops. The overall atmosphere is homely and inviting, with a focus on functionality and comfort.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24918.9, "ram_available_mb": 100853.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 24918.9, "ram_available_mb": 100853.3, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.9, "peak": 16.26, "min": 13.9}, "VDD_CPU_SOC_MSS": {"avg": 15.27, "peak": 16.92, "min": 14.17}, "VDD_GPU": {"avg": 33.59, "peak": 45.69, "min": 25.6}, "VIN": {"avg": 73.75, "peak": 111.35, "min": 61.82}}, "power_watts_avg": 33.59, "energy_joules_est": 122.59, "sample_count": 29, "duration_seconds": 3.65}, "timestamp": "2026-01-17T15:49:33.951830"}
{"image_index": 401, "image_name": "000000045229.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000045229.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3024.923, "latencies_ms": [3024.923], "images_per_second": 0.331, "prompt_tokens": 19, "response_tokens_est": 40, "n_tiles": 12, "output_text": "The kitchen is warmly lit with a soft, yellowish glow, creating a cozy atmosphere. The wooden cabinets and countertops are made of a light-colored wood, complementing the warm lighting.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24918.9, "ram_available_mb": 100853.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 24918.9, "ram_available_mb": 100853.3, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.95, "peak": 16.26, "min": 13.92}, "VDD_CPU_SOC_MSS": {"avg": 15.44, "peak": 17.32, "min": 14.17}, "VDD_GPU": {"avg": 35.54, "peak": 44.91, "min": 26.0}, "VIN": {"avg": 76.5, "peak": 111.8, "min": 64.6}}, "power_watts_avg": 35.54, "energy_joules_est": 107.52, "sample_count": 23, "duration_seconds": 3.025}, "timestamp": "2026-01-17T15:49:36.983309"}
{"image_index": 402, "image_name": "000000045472.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000045472.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1497.879, "latencies_ms": [1497.879], "images_per_second": 0.668, "prompt_tokens": 9, "response_tokens_est": 33, "n_tiles": 6, "output_text": "The image shows a rustic setting with a red ceramic pot containing numerous pink wooden sticks, a pile of fresh oranges, and a red bowl on the table.", "error": null, "sys_before": {"cpu_percent": 44.1, "ram_used_mb": 24947.2, "ram_available_mb": 100825.0, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 24950.1, "ram_available_mb": 100822.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.57, "peak": 15.04, "min": 13.39}, "VDD_CPU_SOC_MSS": {"avg": 15.25, "peak": 15.75, "min": 14.17}, "VDD_GPU": {"avg": 33.16, "peak": 40.56, "min": 25.6}, "VIN": {"avg": 74.98, "peak": 106.0, "min": 62.15}}, "power_watts_avg": 33.16, "energy_joules_est": 49.68, "sample_count": 11, "duration_seconds": 1.498}, "timestamp": "2026-01-17T15:49:38.551372"}
{"image_index": 402, "image_name": "000000045472.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000045472.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1702.629, "latencies_ms": [1702.629], "images_per_second": 0.587, "prompt_tokens": 23, "response_tokens_est": 39, "n_tiles": 6, "output_text": "1. Pineapple\n2. Orange\n3. Red cup\n4. Red cup\n5. Red cup\n6. Red cup\n7. Red cup\n8. Red cup", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24950.1, "ram_available_mb": 100822.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 24951.1, "ram_available_mb": 100821.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.0, "peak": 15.54, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.58, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 32.93, "peak": 40.95, "min": 25.22}, "VIN": {"avg": 73.49, "peak": 102.09, "min": 55.52}}, "power_watts_avg": 32.93, "energy_joules_est": 56.08, "sample_count": 12, "duration_seconds": 1.703}, "timestamp": "2026-01-17T15:49:40.260297"}
{"image_index": 402, "image_name": "000000045472.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000045472.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2405.105, "latencies_ms": [2405.105], "images_per_second": 0.416, "prompt_tokens": 27, "response_tokens_est": 65, "n_tiles": 6, "output_text": "The main objects in the image are a pineapple, a red bowl, a red cup, and a plate of oranges. The pineapple is positioned in the left foreground, while the red bowl and cup are in the foreground as well. The plate of oranges is in the background, slightly to the right.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24951.1, "ram_available_mb": 100821.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 24952.1, "ram_available_mb": 100820.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.09, "peak": 15.44, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.62, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 29.96, "peak": 40.57, "min": 23.64}, "VIN": {"avg": 72.17, "peak": 123.22, "min": 54.32}}, "power_watts_avg": 29.96, "energy_joules_est": 72.07, "sample_count": 18, "duration_seconds": 2.406}, "timestamp": "2026-01-17T15:49:42.672467"}
{"image_index": 402, "image_name": "000000045472.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000045472.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2564.643, "latencies_ms": [2564.643], "images_per_second": 0.39, "prompt_tokens": 21, "response_tokens_est": 71, "n_tiles": 6, "output_text": "The image depicts a rustic and somewhat cluttered setting with a red table or countertop. On the table, there is a large pineapple, a pile of bright orange oranges, a red bowl, and several red cups. The background is dark, and the overall scene appears to be indoors, possibly in a kitchen or a dining area.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 24952.1, "ram_available_mb": 100820.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 24959.9, "ram_available_mb": 100812.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.1, "peak": 15.54, "min": 14.43}, "VDD_CPU_SOC_MSS": {"avg": 15.64, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 29.57, "peak": 40.16, "min": 23.64}, "VIN": {"avg": 68.56, "peak": 109.67, "min": 55.57}}, "power_watts_avg": 29.57, "energy_joules_est": 75.85, "sample_count": 19, "duration_seconds": 2.565}, "timestamp": "2026-01-17T15:49:45.243264"}
{"image_index": 402, "image_name": "000000045472.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000045472.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2893.706, "latencies_ms": [2893.706], "images_per_second": 0.346, "prompt_tokens": 19, "response_tokens_est": 83, "n_tiles": 6, "output_text": "The image features a vibrant red background with a large, dried pineapple placed on the left side. The pineapple has green and brown hues, and its stem is visible. On the right side, there is a pile of fresh, ripe oranges with a glossy surface, and a red ceramic bowl containing a red liquid. The lighting is dim, casting shadows and highlighting the textures of the objects.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24959.9, "ram_available_mb": 100812.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 24960.0, "ram_available_mb": 100812.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.08, "peak": 15.54, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.66, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 28.79, "peak": 40.16, "min": 23.25}, "VIN": {"avg": 70.39, "peak": 100.75, "min": 56.0}}, "power_watts_avg": 28.79, "energy_joules_est": 83.32, "sample_count": 22, "duration_seconds": 2.894}, "timestamp": "2026-01-17T15:49:48.143183"}
{"image_index": 403, "image_name": "000000045550.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000045550.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2800.767, "latencies_ms": [2800.767], "images_per_second": 0.357, "prompt_tokens": 9, "response_tokens_est": 35, "n_tiles": 12, "output_text": "The image shows a man holding a plate of food, which includes a piece of fried chicken and a side of fries, accompanied by a small container of ketchup.", "error": null, "sys_before": {"cpu_percent": 48.7, "ram_used_mb": 24919.2, "ram_available_mb": 100853.0, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 24920.7, "ram_available_mb": 100851.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.19, "peak": 16.66, "min": 13.59}, "VDD_CPU_SOC_MSS": {"avg": 15.85, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 34.94, "peak": 43.71, "min": 26.8}, "VIN": {"avg": 76.7, "peak": 109.44, "min": 52.85}}, "power_watts_avg": 34.94, "energy_joules_est": 97.87, "sample_count": 22, "duration_seconds": 2.801}, "timestamp": "2026-01-17T15:49:51.028420"}
{"image_index": 403, "image_name": "000000045550.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000045550.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 6038.456, "latencies_ms": [6038.456], "images_per_second": 0.166, "prompt_tokens": 23, "response_tokens_est": 129, "n_tiles": 12, "output_text": "- plate: 1\n- sandwich: 1\n- fork: 0\n- knife: 0\n- napkin: 0\n- cup: 1\n- spoon: 0\n- plate: 1\n- fork: 0\n- knife: 0\n- napkin: 0\n- cup: 1\n- spoon: 0\n- plate: 1\n- fork: 0\n- knife: 0\n- napkin: 0\n- cup: 1\n- spoon: 0\n- plate: 1\n- fork: 0", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24920.7, "ram_available_mb": 100851.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 24921.6, "ram_available_mb": 100850.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.13, "peak": 16.56, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.54, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 30.73, "peak": 45.3, "min": 25.6}, "VIN": {"avg": 73.81, "peak": 131.09, "min": 59.93}}, "power_watts_avg": 30.73, "energy_joules_est": 185.58, "sample_count": 48, "duration_seconds": 6.039}, "timestamp": "2026-01-17T15:49:57.073703"}
{"image_index": 403, "image_name": "000000045550.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000045550.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3828.403, "latencies_ms": [3828.403], "images_per_second": 0.261, "prompt_tokens": 27, "response_tokens_est": 63, "n_tiles": 12, "output_text": "The main object in the foreground is a plate of fried chicken, which is being held by a person. The plate is placed on a table. In the background, there are other people and a clock on the wall. The person holding the plate is in the foreground, while the background is slightly blurred.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24921.6, "ram_available_mb": 100850.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 24921.6, "ram_available_mb": 100850.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.16, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.69, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 33.44, "peak": 44.89, "min": 25.6}, "VIN": {"avg": 77.43, "peak": 115.92, "min": 62.69}}, "power_watts_avg": 33.44, "energy_joules_est": 128.03, "sample_count": 29, "duration_seconds": 3.829}, "timestamp": "2026-01-17T15:50:00.908383"}
{"image_index": 403, "image_name": "000000045550.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000045550.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3449.859, "latencies_ms": [3449.859], "images_per_second": 0.29, "prompt_tokens": 21, "response_tokens_est": 52, "n_tiles": 12, "output_text": "The image depicts a man holding a plate of food, which includes a piece of fried chicken and a piece of breaded chicken. The setting appears to be an indoor dining area, possibly a restaurant, with other patrons and tables visible in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24921.6, "ram_available_mb": 100850.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 24922.6, "ram_available_mb": 100849.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.92, "peak": 16.26, "min": 13.92}, "VDD_CPU_SOC_MSS": {"avg": 15.31, "peak": 16.92, "min": 14.17}, "VDD_GPU": {"avg": 33.99, "peak": 44.51, "min": 25.6}, "VIN": {"avg": 74.23, "peak": 93.69, "min": 59.16}}, "power_watts_avg": 33.99, "energy_joules_est": 117.28, "sample_count": 27, "duration_seconds": 3.45}, "timestamp": "2026-01-17T15:50:04.366879"}
{"image_index": 403, "image_name": "000000045550.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000045550.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3242.504, "latencies_ms": [3242.504], "images_per_second": 0.308, "prompt_tokens": 19, "response_tokens_est": 46, "n_tiles": 12, "output_text": "The image features a man with a beard and glasses, holding a plate of fried chicken and a side of fries. The lighting is warm, and the background is slightly blurred, with a clock and a menu board visible.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24922.6, "ram_available_mb": 100849.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 24923.6, "ram_available_mb": 100848.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.92, "peak": 16.36, "min": 13.82}, "VDD_CPU_SOC_MSS": {"avg": 15.34, "peak": 17.32, "min": 14.17}, "VDD_GPU": {"avg": 34.85, "peak": 45.69, "min": 26.0}, "VIN": {"avg": 74.3, "peak": 110.46, "min": 60.57}}, "power_watts_avg": 34.85, "energy_joules_est": 113.01, "sample_count": 25, "duration_seconds": 3.243}, "timestamp": "2026-01-17T15:50:07.615762"}
{"image_index": 404, "image_name": "000000045596.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000045596.jpg", "image_width": 408, "image_height": 640, "image_resolution": "408x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1441.042, "latencies_ms": [1441.042], "images_per_second": 0.694, "prompt_tokens": 9, "response_tokens_est": 31, "n_tiles": 6, "output_text": "The image shows a rainy day with a person holding an umbrella, a bicycle parked on the sidewalk, and a building with a large glass window.", "error": null, "sys_before": {"cpu_percent": 29.6, "ram_used_mb": 24956.6, "ram_available_mb": 100815.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 24957.4, "ram_available_mb": 100814.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.64, "peak": 15.11, "min": 13.61}, "VDD_CPU_SOC_MSS": {"avg": 15.25, "peak": 15.75, "min": 14.17}, "VDD_GPU": {"avg": 33.34, "peak": 40.56, "min": 26.0}, "VIN": {"avg": 73.29, "peak": 96.08, "min": 60.43}}, "power_watts_avg": 33.34, "energy_joules_est": 48.07, "sample_count": 11, "duration_seconds": 1.442}, "timestamp": "2026-01-17T15:50:09.115826"}
{"image_index": 404, "image_name": "000000045596.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000045596.jpg", "image_width": 408, "image_height": 640, "image_resolution": "408x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1700.6, "latencies_ms": [1700.6], "images_per_second": 0.588, "prompt_tokens": 23, "response_tokens_est": 39, "n_tiles": 6, "output_text": "1. Building\n2. Window\n3. Pedestrian\n4. Bicycle\n5. Bicycle rack\n6. Greenery\n7. Street\n8. Street light", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24957.4, "ram_available_mb": 100814.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 24957.6, "ram_available_mb": 100814.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.01, "peak": 15.44, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.63, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 32.42, "peak": 40.56, "min": 24.82}, "VIN": {"avg": 75.08, "peak": 111.18, "min": 61.79}}, "power_watts_avg": 32.42, "energy_joules_est": 55.15, "sample_count": 13, "duration_seconds": 1.701}, "timestamp": "2026-01-17T15:50:10.822735"}
{"image_index": 404, "image_name": "000000045596.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000045596.jpg", "image_width": 408, "image_height": 640, "image_resolution": "408x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2615.064, "latencies_ms": [2615.064], "images_per_second": 0.382, "prompt_tokens": 27, "response_tokens_est": 70, "n_tiles": 6, "output_text": "The main objects in the image are a building, a bicycle rack, and a person holding an umbrella. The bicycle rack is located near the building, while the person is near the bicycle rack. The person is holding an umbrella, which is positioned near the building. The bicycle rack is in the foreground, and the person is in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24957.6, "ram_available_mb": 100814.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 24958.8, "ram_available_mb": 100813.3, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.93, "peak": 15.24, "min": 14.43}, "VDD_CPU_SOC_MSS": {"avg": 15.54, "peak": 15.75, "min": 14.96}, "VDD_GPU": {"avg": 29.15, "peak": 41.74, "min": 22.85}, "VIN": {"avg": 72.53, "peak": 115.7, "min": 62.82}}, "power_watts_avg": 29.15, "energy_joules_est": 76.24, "sample_count": 21, "duration_seconds": 2.616}, "timestamp": "2026-01-17T15:50:13.445180"}
{"image_index": 404, "image_name": "000000045596.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000045596.jpg", "image_width": 408, "image_height": 640, "image_resolution": "408x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3129.486, "latencies_ms": [3129.486], "images_per_second": 0.32, "prompt_tokens": 21, "response_tokens_est": 88, "n_tiles": 6, "output_text": "The image depicts an urban scene viewed through a large window. The setting appears to be a modern building with a glass facade, and the weather is rainy, as evidenced by the wet ground and the presence of raindrops. There are several bicycles parked along the sidewalk, and a person is seen walking in the rain. The overall atmosphere is quiet and somewhat gloomy due to the overcast sky and the rain.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24958.8, "ram_available_mb": 100813.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 24959.6, "ram_available_mb": 100812.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.87, "peak": 15.24, "min": 14.2}, "VDD_CPU_SOC_MSS": {"avg": 15.52, "peak": 15.75, "min": 14.56}, "VDD_GPU": {"avg": 27.88, "peak": 40.16, "min": 22.85}, "VIN": {"avg": 70.21, "peak": 101.86, "min": 56.49}}, "power_watts_avg": 27.88, "energy_joules_est": 87.26, "sample_count": 24, "duration_seconds": 3.13}, "timestamp": "2026-01-17T15:50:16.581234"}
{"image_index": 404, "image_name": "000000045596.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000045596.jpg", "image_width": 408, "image_height": 640, "image_resolution": "408x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2077.44, "latencies_ms": [2077.44], "images_per_second": 0.481, "prompt_tokens": 19, "response_tokens_est": 51, "n_tiles": 6, "output_text": "The image shows a rainy day with a grey, overcast sky. The ground is wet, reflecting the grey tones of the sky. The building's exterior is made of glass and metal, with a metal fence and a metal staircase visible.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24959.6, "ram_available_mb": 100812.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 24960.8, "ram_available_mb": 100811.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.84, "peak": 15.24, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.33, "peak": 15.75, "min": 14.56}, "VDD_GPU": {"avg": 30.33, "peak": 40.56, "min": 23.25}, "VIN": {"avg": 68.98, "peak": 91.32, "min": 62.43}}, "power_watts_avg": 30.33, "energy_joules_est": 63.02, "sample_count": 16, "duration_seconds": 2.078}, "timestamp": "2026-01-17T15:50:18.665320"}
{"image_index": 405, "image_name": "000000045728.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000045728.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2965.314, "latencies_ms": [2965.314], "images_per_second": 0.337, "prompt_tokens": 9, "response_tokens_est": 40, "n_tiles": 12, "output_text": "The image shows a close-up of a dish containing a creamy, white substance, likely a type of pasta, with a few visible pieces of a yellowish ingredient, possibly cheese or a sauce.", "error": null, "sys_before": {"cpu_percent": 40.0, "ram_used_mb": 24916.4, "ram_available_mb": 100855.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 24917.7, "ram_available_mb": 100854.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.19, "peak": 16.66, "min": 13.59}, "VDD_CPU_SOC_MSS": {"avg": 15.88, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 34.63, "peak": 43.73, "min": 26.39}, "VIN": {"avg": 76.09, "peak": 103.02, "min": 59.95}}, "power_watts_avg": 34.63, "energy_joules_est": 102.7, "sample_count": 23, "duration_seconds": 2.966}, "timestamp": "2026-01-17T15:50:21.710139"}
{"image_index": 405, "image_name": "000000045728.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000045728.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 3028.808, "latencies_ms": [3028.808], "images_per_second": 0.33, "prompt_tokens": 23, "response_tokens_est": 40, "n_tiles": 12, "output_text": "1. Pasta\n2. Pasta\n3. Pasta\n4. Pasta\n5. Pasta\n6. Pasta\n7. Pasta\n8. Pasta", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24917.7, "ram_available_mb": 100854.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 24918.4, "ram_available_mb": 100853.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.23, "peak": 16.56, "min": 14.12}, "VDD_CPU_SOC_MSS": {"avg": 15.83, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 35.68, "peak": 45.68, "min": 26.39}, "VIN": {"avg": 78.24, "peak": 112.2, "min": 55.68}}, "power_watts_avg": 35.68, "energy_joules_est": 108.09, "sample_count": 24, "duration_seconds": 3.029}, "timestamp": "2026-01-17T15:50:24.745763"}
{"image_index": 405, "image_name": "000000045728.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000045728.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3680.483, "latencies_ms": [3680.483], "images_per_second": 0.272, "prompt_tokens": 27, "response_tokens_est": 59, "n_tiles": 12, "output_text": "The main objects in the image are a bowl of macaroni and cheese. The macaroni is in the foreground, while the cheese sauce is in the background. The macaroni is near the bottom of the image, and the cheese sauce is slightly blurred in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24918.4, "ram_available_mb": 100853.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 24918.6, "ram_available_mb": 100853.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.19, "peak": 16.56, "min": 14.12}, "VDD_CPU_SOC_MSS": {"avg": 15.7, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 33.92, "peak": 45.68, "min": 26.01}, "VIN": {"avg": 77.76, "peak": 119.54, "min": 62.27}}, "power_watts_avg": 33.92, "energy_joules_est": 124.85, "sample_count": 29, "duration_seconds": 3.681}, "timestamp": "2026-01-17T15:50:28.432744"}
{"image_index": 405, "image_name": "000000045728.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000045728.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 4078.641, "latencies_ms": [4078.641], "images_per_second": 0.245, "prompt_tokens": 21, "response_tokens_est": 71, "n_tiles": 12, "output_text": "The image depicts a close-up view of a bowl of macaroni and cheese. The macaroni is coated in a creamy white sauce, and the dish is presented on a dark surface, possibly a table. The focus is on the macaroni, with the background slightly blurred, emphasizing the texture and color of the dish.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24918.6, "ram_available_mb": 100853.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 24919.6, "ram_available_mb": 100852.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.19, "peak": 16.66, "min": 14.12}, "VDD_CPU_SOC_MSS": {"avg": 15.73, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 33.07, "peak": 45.28, "min": 26.0}, "VIN": {"avg": 73.5, "peak": 109.95, "min": 56.88}}, "power_watts_avg": 33.07, "energy_joules_est": 134.89, "sample_count": 32, "duration_seconds": 4.079}, "timestamp": "2026-01-17T15:50:32.519473"}
{"image_index": 405, "image_name": "000000045728.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000045728.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3467.42, "latencies_ms": [3467.42], "images_per_second": 0.288, "prompt_tokens": 19, "response_tokens_est": 53, "n_tiles": 12, "output_text": "The image features a close-up view of a creamy, white pasta dish with a glossy sheen, indicating it might be coated with a sauce or cream. The lighting is soft and diffused, casting gentle shadows and highlighting the smooth texture of the pasta.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24919.6, "ram_available_mb": 100852.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 24919.9, "ram_available_mb": 100852.3, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.21, "peak": 16.56, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.76, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 34.42, "peak": 44.89, "min": 26.0}, "VIN": {"avg": 75.04, "peak": 111.92, "min": 61.98}}, "power_watts_avg": 34.42, "energy_joules_est": 119.36, "sample_count": 26, "duration_seconds": 3.468}, "timestamp": "2026-01-17T15:50:35.993271"}
{"image_index": 406, "image_name": "000000046031.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046031.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2833.061, "latencies_ms": [2833.061], "images_per_second": 0.353, "prompt_tokens": 9, "response_tokens_est": 36, "n_tiles": 12, "output_text": "The image shows a cluttered desk with a laptop, a desktop computer monitor, a smartphone, a keyboard, and a mouse, all displaying green leaf patterns on their screens.", "error": null, "sys_before": {"cpu_percent": 40.6, "ram_used_mb": 24937.9, "ram_available_mb": 100834.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 24933.8, "ram_available_mb": 100838.3, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.13, "peak": 16.66, "min": 13.19}, "VDD_CPU_SOC_MSS": {"avg": 15.84, "peak": 17.71, "min": 14.17}, "VDD_GPU": {"avg": 35.2, "peak": 44.51, "min": 26.79}, "VIN": {"avg": 79.88, "peak": 140.48, "min": 62.12}}, "power_watts_avg": 35.2, "energy_joules_est": 99.74, "sample_count": 22, "duration_seconds": 2.834}, "timestamp": "2026-01-17T15:50:38.936080"}
{"image_index": 406, "image_name": "000000046031.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046031.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2883.13, "latencies_ms": [2883.13], "images_per_second": 0.347, "prompt_tokens": 23, "response_tokens_est": 36, "n_tiles": 12, "output_text": "1. Laptop\n2. Computer mouse\n3. Keyboard\n4. Monitor\n5. Laptop mouse\n6. Computer mouse\n7. Monitor\n8. Laptop mouse", "error": null, "sys_before": {"cpu_percent": 16.7, "ram_used_mb": 24933.8, "ram_available_mb": 100838.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 24927.6, "ram_available_mb": 100844.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.27, "peak": 16.66, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.89, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 36.18, "peak": 45.68, "min": 26.79}, "VIN": {"avg": 80.47, "peak": 115.56, "min": 59.45}}, "power_watts_avg": 36.18, "energy_joules_est": 104.32, "sample_count": 22, "duration_seconds": 2.883}, "timestamp": "2026-01-17T15:50:41.826408"}
{"image_index": 406, "image_name": "000000046031.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046031.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3850.81, "latencies_ms": [3850.81], "images_per_second": 0.26, "prompt_tokens": 27, "response_tokens_est": 64, "n_tiles": 12, "output_text": "The main objects in the image are a laptop, a mouse, and a computer monitor. The laptop is positioned in the foreground, slightly to the right, with the mouse placed near the bottom left corner of the laptop. The computer monitor is in the background, slightly to the left, displaying a green leaf pattern.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24927.6, "ram_available_mb": 100844.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 24926.9, "ram_available_mb": 100845.3, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.18, "peak": 16.56, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.68, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 33.53, "peak": 45.28, "min": 26.0}, "VIN": {"avg": 78.75, "peak": 135.79, "min": 59.15}}, "power_watts_avg": 33.53, "energy_joules_est": 129.13, "sample_count": 30, "duration_seconds": 3.851}, "timestamp": "2026-01-17T15:50:45.684173"}
{"image_index": 406, "image_name": "000000046031.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046031.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 4350.708, "latencies_ms": [4350.708], "images_per_second": 0.23, "prompt_tokens": 21, "response_tokens_est": 79, "n_tiles": 12, "output_text": "The image depicts a cluttered workspace with a laptop, a desktop computer, a smartphone, and a mouse. The workspace appears to be in a room with a white wall and a curtain, and the items are arranged on a white surface, possibly a desk. The scene suggests a casual, possibly home office environment where the user is engaged in various tasks, including using multiple devices simultaneously.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24926.9, "ram_available_mb": 100845.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 24927.4, "ram_available_mb": 100844.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.17, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.65, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 32.59, "peak": 44.89, "min": 26.0}, "VIN": {"avg": 73.26, "peak": 113.33, "min": 57.04}}, "power_watts_avg": 32.59, "energy_joules_est": 141.8, "sample_count": 33, "duration_seconds": 4.351}, "timestamp": "2026-01-17T15:50:50.041397"}
{"image_index": 406, "image_name": "000000046031.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046031.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3836.478, "latencies_ms": [3836.478], "images_per_second": 0.261, "prompt_tokens": 19, "response_tokens_est": 64, "n_tiles": 12, "output_text": "The image shows a workspace with a laptop, a desktop computer monitor, a desktop computer monitor, a keyboard, a mouse, and a computer mouse. The workspace is well-lit with natural light coming from a window, and the overall color scheme is neutral with a mix of white, black, and gray tones.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24927.4, "ram_available_mb": 100844.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 24928.7, "ram_available_mb": 100843.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.2, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.76, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 33.5, "peak": 44.49, "min": 26.0}, "VIN": {"avg": 74.36, "peak": 95.22, "min": 56.41}}, "power_watts_avg": 33.5, "energy_joules_est": 128.53, "sample_count": 29, "duration_seconds": 3.837}, "timestamp": "2026-01-17T15:50:53.884890"}
{"image_index": 407, "image_name": "000000046048.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046048.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2738.897, "latencies_ms": [2738.897], "images_per_second": 0.365, "prompt_tokens": 9, "response_tokens_est": 33, "n_tiles": 12, "output_text": "A young girl is sitting on a bed in a cozy bedroom, surrounded by toys and books, while a lamp and a small table with a lamp are nearby.", "error": null, "sys_before": {"cpu_percent": 26.8, "ram_used_mb": 24928.7, "ram_available_mb": 100843.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 24926.5, "ram_available_mb": 100845.7, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.18, "peak": 16.66, "min": 13.59}, "VDD_CPU_SOC_MSS": {"avg": 15.84, "peak": 17.71, "min": 14.17}, "VDD_GPU": {"avg": 35.62, "peak": 44.1, "min": 26.79}, "VIN": {"avg": 75.33, "peak": 97.24, "min": 60.78}}, "power_watts_avg": 35.62, "energy_joules_est": 97.58, "sample_count": 21, "duration_seconds": 2.739}, "timestamp": "2026-01-17T15:50:56.699230"}
{"image_index": 407, "image_name": "000000046048.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046048.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2988.408, "latencies_ms": [2988.408], "images_per_second": 0.335, "prompt_tokens": 23, "response_tokens_est": 39, "n_tiles": 12, "output_text": "bed: 1\npillow: 1\nnightstand: 1\nlamp: 1\nbook: 1\ntoy: 1\nbox: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24926.5, "ram_available_mb": 100845.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 24923.2, "ram_available_mb": 100849.0, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.26, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.88, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 35.74, "peak": 44.91, "min": 26.39}, "VIN": {"avg": 77.01, "peak": 110.13, "min": 60.97}}, "power_watts_avg": 35.74, "energy_joules_est": 106.82, "sample_count": 23, "duration_seconds": 2.989}, "timestamp": "2026-01-17T15:50:59.694728"}
{"image_index": 407, "image_name": "000000046048.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046048.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3652.257, "latencies_ms": [3652.257], "images_per_second": 0.274, "prompt_tokens": 27, "response_tokens_est": 58, "n_tiles": 12, "output_text": "The main objects in the image are a bed, a nightstand, and a chair. The bed is positioned in the background, with the nightstand and chair positioned in the foreground. The chair is near the bed, while the nightstand is positioned to the left of the bed.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24923.7, "ram_available_mb": 100848.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 24924.9, "ram_available_mb": 100847.3, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.98, "peak": 16.36, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.51, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 33.95, "peak": 45.3, "min": 25.6}, "VIN": {"avg": 75.41, "peak": 117.41, "min": 65.96}}, "power_watts_avg": 33.95, "energy_joules_est": 124.01, "sample_count": 28, "duration_seconds": 3.653}, "timestamp": "2026-01-17T15:51:03.354874"}
{"image_index": 407, "image_name": "000000046048.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046048.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3857.586, "latencies_ms": [3857.586], "images_per_second": 0.259, "prompt_tokens": 21, "response_tokens_est": 64, "n_tiles": 12, "output_text": "The image depicts a cozy bedroom setting with a bed, a nightstand, and a chair. A young girl is sitting on the bed, seemingly engaged in an activity, possibly reading or playing with toys. The room has warm, earthy tones, and the bed is adorned with a patterned blanket.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24924.9, "ram_available_mb": 100847.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 24925.7, "ram_available_mb": 100846.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.92, "peak": 16.36, "min": 14.02}, "VDD_CPU_SOC_MSS": {"avg": 15.42, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 33.28, "peak": 45.3, "min": 25.6}, "VIN": {"avg": 74.8, "peak": 115.36, "min": 48.76}}, "power_watts_avg": 33.28, "energy_joules_est": 128.4, "sample_count": 30, "duration_seconds": 3.858}, "timestamp": "2026-01-17T15:51:07.219961"}
{"image_index": 407, "image_name": "000000046048.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046048.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3166.175, "latencies_ms": [3166.175], "images_per_second": 0.316, "prompt_tokens": 19, "response_tokens_est": 44, "n_tiles": 12, "output_text": "The room is warmly lit with a soft orange hue, creating a cozy and inviting atmosphere. The walls are adorned with a patterned red and gold fabric, adding a touch of elegance to the space.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24925.7, "ram_available_mb": 100846.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 24925.6, "ram_available_mb": 100846.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.96, "peak": 16.36, "min": 13.92}, "VDD_CPU_SOC_MSS": {"avg": 15.58, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 35.24, "peak": 45.3, "min": 26.01}, "VIN": {"avg": 79.59, "peak": 135.51, "min": 63.93}}, "power_watts_avg": 35.24, "energy_joules_est": 111.59, "sample_count": 24, "duration_seconds": 3.166}, "timestamp": "2026-01-17T15:51:10.392815"}
{"image_index": 408, "image_name": "000000046252.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046252.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2891.902, "latencies_ms": [2891.902], "images_per_second": 0.346, "prompt_tokens": 9, "response_tokens_est": 38, "n_tiles": 12, "output_text": "A baseball player in a red uniform is in the middle of a swing, while a catcher and an umpire are crouched behind him, ready to catch the ball.", "error": null, "sys_before": {"cpu_percent": 33.6, "ram_used_mb": 24916.9, "ram_available_mb": 100855.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 24918.4, "ram_available_mb": 100853.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.16, "peak": 16.76, "min": 13.19}, "VDD_CPU_SOC_MSS": {"avg": 15.94, "peak": 18.11, "min": 14.17}, "VDD_GPU": {"avg": 35.36, "peak": 44.49, "min": 26.8}, "VIN": {"avg": 77.75, "peak": 111.35, "min": 57.72}}, "power_watts_avg": 35.36, "energy_joules_est": 102.27, "sample_count": 22, "duration_seconds": 2.892}, "timestamp": "2026-01-17T15:51:13.375912"}
{"image_index": 408, "image_name": "000000046252.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046252.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 3255.009, "latencies_ms": [3255.009], "images_per_second": 0.307, "prompt_tokens": 23, "response_tokens_est": 47, "n_tiles": 12, "output_text": "baseball bat: 1\ncatcher's mitt: 1\numpire: 1\nplayer: 1\npitcher: 1\nhome plate: 1\npitcher's mound: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24918.4, "ram_available_mb": 100853.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 24919.1, "ram_available_mb": 100853.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.25, "peak": 16.66, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.83, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 35.1, "peak": 45.28, "min": 26.39}, "VIN": {"avg": 78.98, "peak": 138.15, "min": 58.52}}, "power_watts_avg": 35.1, "energy_joules_est": 114.26, "sample_count": 25, "duration_seconds": 3.255}, "timestamp": "2026-01-17T15:51:16.638490"}
{"image_index": 408, "image_name": "000000046252.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046252.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 4881.387, "latencies_ms": [4881.387], "images_per_second": 0.205, "prompt_tokens": 27, "response_tokens_est": 95, "n_tiles": 12, "output_text": "The main object in the foreground is a baseball player in a red uniform, crouched near the home plate. The player is holding a bat and appears to be in the middle of a swing. In the background, there is a catcher in a gray uniform, positioned near the pitcher's mound. The catcher is wearing a helmet and catching gloves. The baseball field is well-maintained, with a dirt infield and a grassy outfield.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24919.1, "ram_available_mb": 100853.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 24919.8, "ram_available_mb": 100852.3, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.22, "peak": 16.66, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.85, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 31.91, "peak": 44.89, "min": 26.0}, "VIN": {"avg": 75.68, "peak": 115.57, "min": 58.02}}, "power_watts_avg": 31.91, "energy_joules_est": 155.79, "sample_count": 38, "duration_seconds": 4.882}, "timestamp": "2026-01-17T15:51:21.526093"}
{"image_index": 408, "image_name": "000000046252.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046252.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3870.52, "latencies_ms": [3870.52], "images_per_second": 0.258, "prompt_tokens": 21, "response_tokens_est": 65, "n_tiles": 12, "output_text": "The image captures a baseball game in progress on a sunny day. The batter is in the midst of swinging at the pitch, while the catcher is crouched behind home plate, ready to catch the ball. The scene is set on a dirt field with a well-maintained grassy area in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24919.8, "ram_available_mb": 100852.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 24920.3, "ram_available_mb": 100851.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.21, "peak": 16.66, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.69, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 33.49, "peak": 44.91, "min": 26.0}, "VIN": {"avg": 74.84, "peak": 117.01, "min": 59.22}}, "power_watts_avg": 33.49, "energy_joules_est": 129.63, "sample_count": 30, "duration_seconds": 3.871}, "timestamp": "2026-01-17T15:51:25.403272"}
{"image_index": 408, "image_name": "000000046252.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046252.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2781.103, "latencies_ms": [2781.103], "images_per_second": 0.36, "prompt_tokens": 19, "response_tokens_est": 33, "n_tiles": 12, "output_text": "The baseball field is well-maintained with a vibrant green grass and a dirt infield. The sunlight casts sharp shadows, indicating a bright, sunny day.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24920.3, "ram_available_mb": 100851.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 24920.6, "ram_available_mb": 100851.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.29, "peak": 16.56, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.86, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 36.39, "peak": 44.89, "min": 26.79}, "VIN": {"avg": 84.57, "peak": 127.07, "min": 62.78}}, "power_watts_avg": 36.39, "energy_joules_est": 101.21, "sample_count": 21, "duration_seconds": 2.781}, "timestamp": "2026-01-17T15:51:28.190794"}
{"image_index": 409, "image_name": "000000046378.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046378.jpg", "image_width": 640, "image_height": 360, "image_resolution": "640x360", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 992.6, "latencies_ms": [992.6], "images_per_second": 1.007, "prompt_tokens": 9, "response_tokens_est": 36, "n_tiles": 2, "output_text": "A cat is seen in the image, its mouth open and its teeth visible as it appears to be in the process of eating a fish, which is lying on the ground.", "error": null, "sys_before": {"cpu_percent": 22.2, "ram_used_mb": 24930.9, "ram_available_mb": 100841.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24932.6, "ram_available_mb": 100839.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5055.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.67, "peak": 15.11, "min": 14.0}, "VDD_CPU_SOC_MSS": {"avg": 15.41, "peak": 16.15, "min": 14.57}, "VDD_GPU": {"avg": 27.18, "peak": 34.27, "min": 22.86}, "VIN": {"avg": 65.99, "peak": 84.36, "min": 61.69}}, "power_watts_avg": 27.18, "energy_joules_est": 26.99, "sample_count": 7, "duration_seconds": 0.993}, "timestamp": "2026-01-17T15:51:29.216598"}
{"image_index": 409, "image_name": "000000046378.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046378.jpg", "image_width": 640, "image_height": 360, "image_resolution": "640x360", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 537.097, "latencies_ms": [537.097], "images_per_second": 1.862, "prompt_tokens": 23, "response_tokens_est": 15, "n_tiles": 2, "output_text": "cat: 1\nfish: 1\nground: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24932.6, "ram_available_mb": 100839.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 24933.4, "ram_available_mb": 100838.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5064.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.54, "peak": 14.71, "min": 14.4}, "VDD_CPU_SOC_MSS": {"avg": 15.49, "peak": 15.75, "min": 15.36}, "VDD_GPU": {"avg": 29.28, "peak": 31.51, "min": 27.19}, "VIN": {"avg": 73.26, "peak": 99.67, "min": 58.19}}, "power_watts_avg": 29.28, "energy_joules_est": 15.73, "sample_count": 3, "duration_seconds": 0.537}, "timestamp": "2026-01-17T15:51:29.759475"}
{"image_index": 409, "image_name": "000000046378.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046378.jpg", "image_width": 640, "image_height": 360, "image_resolution": "640x360", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2163.949, "latencies_ms": [2163.949], "images_per_second": 0.462, "prompt_tokens": 27, "response_tokens_est": 87, "n_tiles": 2, "output_text": "The main object in the foreground is a cat, which is positioned near the center of the image. The cat is interacting with a dead bird, which is located in the background. The cat's body is slightly off-center to the right, and the bird is positioned to the left of the cat. The cat's fur is predominantly white with gray and black stripes, and it appears to be in the process of eating the bird.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24933.4, "ram_available_mb": 100838.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24933.6, "ram_available_mb": 100838.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5067.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.12, "peak": 15.52, "min": 14.2}, "VDD_CPU_SOC_MSS": {"avg": 16.07, "peak": 16.54, "min": 14.96}, "VDD_GPU": {"avg": 23.96, "peak": 33.48, "min": 20.88}, "VIN": {"avg": 66.05, "peak": 98.64, "min": 61.35}}, "power_watts_avg": 23.96, "energy_joules_est": 51.86, "sample_count": 16, "duration_seconds": 2.165}, "timestamp": "2026-01-17T15:51:31.930554"}
{"image_index": 409, "image_name": "000000046378.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046378.jpg", "image_width": 640, "image_height": 360, "image_resolution": "640x360", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1706.892, "latencies_ms": [1706.892], "images_per_second": 0.586, "prompt_tokens": 21, "response_tokens_est": 67, "n_tiles": 2, "output_text": "The image depicts a cat in a natural outdoor setting, possibly a park or garden, where it is seen eating a fish. The cat is focused on its meal, with its mouth open and the fish partially submerged in its mouth. The surrounding area includes a concrete surface and scattered leaves, indicating a natural, outdoor environment.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24933.6, "ram_available_mb": 100838.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 24933.3, "ram_available_mb": 100838.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5063.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.0, "peak": 15.32, "min": 14.4}, "VDD_CPU_SOC_MSS": {"avg": 16.02, "peak": 16.54, "min": 15.35}, "VDD_GPU": {"avg": 23.61, "peak": 29.95, "min": 20.89}, "VIN": {"avg": 67.32, "peak": 101.11, "min": 61.24}}, "power_watts_avg": 23.61, "energy_joules_est": 40.31, "sample_count": 13, "duration_seconds": 1.707}, "timestamp": "2026-01-17T15:51:33.643616"}
{"image_index": 409, "image_name": "000000046378.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046378.jpg", "image_width": 640, "image_height": 360, "image_resolution": "640x360", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1682.433, "latencies_ms": [1682.433], "images_per_second": 0.594, "prompt_tokens": 19, "response_tokens_est": 66, "n_tiles": 2, "output_text": "The image features a cat with a predominantly gray and white fur coat, which is highlighted by the natural light casting soft shadows on the ground. The cat is seen in a close-up, with its head resting on a piece of wood, surrounded by a mix of dried leaves and twigs, indicating a natural, outdoor setting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24933.3, "ram_available_mb": 100838.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 24934.3, "ram_available_mb": 100837.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5062.6, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.22, "peak": 15.72, "min": 14.51}, "VDD_CPU_SOC_MSS": {"avg": 15.91, "peak": 16.54, "min": 15.35}, "VDD_GPU": {"avg": 24.03, "peak": 30.34, "min": 21.27}, "VIN": {"avg": 66.78, "peak": 89.23, "min": 61.08}}, "power_watts_avg": 24.03, "energy_joules_est": 40.44, "sample_count": 12, "duration_seconds": 1.683}, "timestamp": "2026-01-17T15:51:35.332293"}
{"image_index": 410, "image_name": "000000046463.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046463.jpg", "image_width": 500, "image_height": 400, "image_resolution": "500x400", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2159.489, "latencies_ms": [2159.489], "images_per_second": 0.463, "prompt_tokens": 9, "response_tokens_est": 16, "n_tiles": 12, "output_text": "A person is holding a piece of food with a green leaf on top.", "error": null, "sys_before": {"cpu_percent": 47.4, "ram_used_mb": 24946.5, "ram_available_mb": 100825.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.3, "ram_used_mb": 24947.2, "ram_available_mb": 100824.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.29, "peak": 16.66, "min": 13.79}, "VDD_CPU_SOC_MSS": {"avg": 16.06, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 37.31, "peak": 43.31, "min": 27.97}, "VIN": {"avg": 82.71, "peak": 115.12, "min": 54.89}}, "power_watts_avg": 37.31, "energy_joules_est": 80.59, "sample_count": 16, "duration_seconds": 2.16}, "timestamp": "2026-01-17T15:51:37.580967"}
{"image_index": 410, "image_name": "000000046463.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046463.jpg", "image_width": 500, "image_height": 400, "image_resolution": "500x400", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2826.381, "latencies_ms": [2826.381], "images_per_second": 0.354, "prompt_tokens": 23, "response_tokens_est": 34, "n_tiles": 12, "output_text": "1. Bread\n2. Knife\n3. Cutting board\n4. Knife\n5. Knife\n6. Knife\n7. Knife\n8. Knife", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24947.2, "ram_available_mb": 100824.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 24941.2, "ram_available_mb": 100831.0, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.26, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.8, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 36.91, "peak": 46.07, "min": 27.19}, "VIN": {"avg": 82.9, "peak": 119.53, "min": 55.23}}, "power_watts_avg": 36.91, "energy_joules_est": 104.33, "sample_count": 21, "duration_seconds": 2.827}, "timestamp": "2026-01-17T15:51:40.414949"}
{"image_index": 410, "image_name": "000000046463.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046463.jpg", "image_width": 500, "image_height": 400, "image_resolution": "500x400", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 4011.367, "latencies_ms": [4011.367], "images_per_second": 0.249, "prompt_tokens": 27, "response_tokens_est": 69, "n_tiles": 12, "output_text": "The main object in the foreground is a piece of food, which appears to be a slice of bread with a green leafy topping. The background is slightly blurred, but it seems to be a kitchen counter or table. The food is being held by a person's hand, which is partially visible on the left side of the image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24941.1, "ram_available_mb": 100831.0, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 24939.8, "ram_available_mb": 100832.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.19, "peak": 16.66, "min": 14.12}, "VDD_CPU_SOC_MSS": {"avg": 15.8, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 33.25, "peak": 45.28, "min": 26.0}, "VIN": {"avg": 74.31, "peak": 124.54, "min": 61.73}}, "power_watts_avg": 33.25, "energy_joules_est": 133.4, "sample_count": 31, "duration_seconds": 4.012}, "timestamp": "2026-01-17T15:51:44.433945"}
{"image_index": 410, "image_name": "000000046463.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046463.jpg", "image_width": 500, "image_height": 400, "image_resolution": "500x400", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3566.348, "latencies_ms": [3566.348], "images_per_second": 0.28, "prompt_tokens": 21, "response_tokens_est": 56, "n_tiles": 12, "output_text": "The image shows a close-up of a person's hand holding a piece of food, which appears to be a slice of pizza. The hand is positioned on a white surface, possibly a cutting board, and the focus is on the food, making the background slightly blurred.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24939.8, "ram_available_mb": 100832.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 24940.3, "ram_available_mb": 100831.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.24, "peak": 16.56, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.76, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 34.0, "peak": 44.89, "min": 26.0}, "VIN": {"avg": 74.01, "peak": 117.87, "min": 54.17}}, "power_watts_avg": 34.0, "energy_joules_est": 121.27, "sample_count": 27, "duration_seconds": 3.567}, "timestamp": "2026-01-17T15:51:48.007682"}
{"image_index": 410, "image_name": "000000046463.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046463.jpg", "image_width": 500, "image_height": 400, "image_resolution": "500x400", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 4184.061, "latencies_ms": [4184.061], "images_per_second": 0.239, "prompt_tokens": 19, "response_tokens_est": 74, "n_tiles": 12, "output_text": "The image shows a close-up of a person's hand holding a piece of food with a green leafy vegetable on top. The food appears to be a slice of a sandwich or a similar item, and the background is blurred, with a white surface and some indistinct objects. The lighting is soft and natural, suggesting an indoor setting with ambient light.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24940.3, "ram_available_mb": 100831.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 24940.3, "ram_available_mb": 100831.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.16, "peak": 16.66, "min": 14.12}, "VDD_CPU_SOC_MSS": {"avg": 15.65, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 33.03, "peak": 44.91, "min": 26.01}, "VIN": {"avg": 74.45, "peak": 114.02, "min": 54.78}}, "power_watts_avg": 33.03, "energy_joules_est": 138.22, "sample_count": 32, "duration_seconds": 4.185}, "timestamp": "2026-01-17T15:51:52.199030"}
{"image_index": 411, "image_name": "000000046497.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046497.jpg", "image_width": 500, "image_height": 332, "image_resolution": "500x332", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1801.932, "latencies_ms": [1801.932], "images_per_second": 0.555, "prompt_tokens": 9, "response_tokens_est": 44, "n_tiles": 6, "output_text": "Two young girls are sitting on a gray inflatable ring on the deck of a sailboat, with one of them wearing a pink shirt and the other a white shirt, both looking out over the calm blue water.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24951.7, "ram_available_mb": 100820.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 24953.4, "ram_available_mb": 100818.7, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.81, "peak": 15.24, "min": 13.92}, "VDD_CPU_SOC_MSS": {"avg": 15.44, "peak": 16.14, "min": 14.17}, "VDD_GPU": {"avg": 31.82, "peak": 40.95, "min": 24.42}, "VIN": {"avg": 69.82, "peak": 107.68, "min": 56.23}}, "power_watts_avg": 31.82, "energy_joules_est": 57.35, "sample_count": 13, "duration_seconds": 1.802}, "timestamp": "2026-01-17T15:51:54.042312"}
{"image_index": 411, "image_name": "000000046497.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046497.jpg", "image_width": 500, "image_height": 332, "image_resolution": "500x332", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1538.077, "latencies_ms": [1538.077], "images_per_second": 0.65, "prompt_tokens": 23, "response_tokens_est": 33, "n_tiles": 6, "output_text": "1. Sailboat\n2. Sail\n3. Woman\n4. Woman\n5. Woman\n6. Woman\n7. Woman\n8. Woman", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24953.4, "ram_available_mb": 100818.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 24954.2, "ram_available_mb": 100818.0, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.0, "peak": 15.44, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.57, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 33.77, "peak": 40.56, "min": 26.0}, "VIN": {"avg": 76.89, "peak": 114.94, "min": 61.63}}, "power_watts_avg": 33.77, "energy_joules_est": 51.96, "sample_count": 11, "duration_seconds": 1.539}, "timestamp": "2026-01-17T15:51:55.587675"}
{"image_index": 411, "image_name": "000000046497.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046497.jpg", "image_width": 500, "image_height": 332, "image_resolution": "500x332", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2919.854, "latencies_ms": [2919.854], "images_per_second": 0.342, "prompt_tokens": 27, "response_tokens_est": 84, "n_tiles": 6, "output_text": "The main objects in the image are the two young girls and the boat. The girls are positioned in the foreground, with the girl on the left sitting on a gray cushion. The girl on the right is sitting on a pink cushion. The boat is in the background, with its sail and rigging visible. The girl on the left is closer to the camera, while the girl on the right is further away.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 24954.2, "ram_available_mb": 100818.0, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 24955.4, "ram_available_mb": 100816.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.12, "peak": 15.44, "min": 14.43}, "VDD_CPU_SOC_MSS": {"avg": 15.82, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 28.99, "peak": 40.56, "min": 23.64}, "VIN": {"avg": 72.46, "peak": 119.38, "min": 62.64}}, "power_watts_avg": 28.99, "energy_joules_est": 84.65, "sample_count": 22, "duration_seconds": 2.92}, "timestamp": "2026-01-17T15:51:58.513710"}
{"image_index": 411, "image_name": "000000046497.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046497.jpg", "image_width": 500, "image_height": 332, "image_resolution": "500x332", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2493.947, "latencies_ms": [2493.947], "images_per_second": 0.401, "prompt_tokens": 21, "response_tokens_est": 67, "n_tiles": 6, "output_text": "The image depicts a serene scene on a boat, with two young girls sitting on a gray cushion. They are enjoying the view of the blue water, and one of them is wearing a straw hat. The setting is outdoors, likely on a sunny day, with the girls engaged in a leisurely activity on the water.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24955.4, "ram_available_mb": 100816.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 24956.1, "ram_available_mb": 100816.0, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.06, "peak": 15.54, "min": 14.43}, "VDD_CPU_SOC_MSS": {"avg": 15.66, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 29.65, "peak": 40.56, "min": 23.25}, "VIN": {"avg": 71.22, "peak": 119.37, "min": 57.81}}, "power_watts_avg": 29.65, "energy_joules_est": 73.95, "sample_count": 19, "duration_seconds": 2.494}, "timestamp": "2026-01-17T15:52:01.014093"}
{"image_index": 411, "image_name": "000000046497.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046497.jpg", "image_width": 500, "image_height": 332, "image_resolution": "500x332", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1633.026, "latencies_ms": [1633.026], "images_per_second": 0.612, "prompt_tokens": 19, "response_tokens_est": 36, "n_tiles": 6, "output_text": "The image depicts a sunny day on a boat with clear blue water. The boat's sails are white, and the ropes and rigging are made of metal.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24956.1, "ram_available_mb": 100816.0, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 24956.1, "ram_available_mb": 100816.0, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.92, "peak": 15.34, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.48, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 32.76, "peak": 40.56, "min": 25.22}, "VIN": {"avg": 79.29, "peak": 117.63, "min": 59.94}}, "power_watts_avg": 32.76, "energy_joules_est": 53.51, "sample_count": 12, "duration_seconds": 1.633}, "timestamp": "2026-01-17T15:52:02.653912"}
{"image_index": 412, "image_name": "000000046804.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046804.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2072.573, "latencies_ms": [2072.573], "images_per_second": 0.482, "prompt_tokens": 9, "response_tokens_est": 14, "n_tiles": 12, "output_text": "A sheep stands in a grassy field with a rocky background.", "error": null, "sys_before": {"cpu_percent": 43.2, "ram_used_mb": 24922.6, "ram_available_mb": 100849.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.1, "ram_used_mb": 24923.8, "ram_available_mb": 100848.3, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.06, "peak": 16.46, "min": 13.59}, "VDD_CPU_SOC_MSS": {"avg": 15.9, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 38.36, "peak": 44.1, "min": 30.33}, "VIN": {"avg": 81.29, "peak": 118.58, "min": 65.32}}, "power_watts_avg": 38.36, "energy_joules_est": 79.52, "sample_count": 15, "duration_seconds": 2.073}, "timestamp": "2026-01-17T15:52:04.819372"}
{"image_index": 412, "image_name": "000000046804.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046804.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2960.229, "latencies_ms": [2960.229], "images_per_second": 0.338, "prompt_tokens": 23, "response_tokens_est": 38, "n_tiles": 12, "output_text": "1. Sheep\n2. Grass\n3. Stone wall\n4. Lichen\n5. Bushes\n6. Tree\n7. Rocks\n8. Shadows", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24923.8, "ram_available_mb": 100848.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 24924.1, "ram_available_mb": 100848.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.01, "peak": 16.36, "min": 14.02}, "VDD_CPU_SOC_MSS": {"avg": 15.57, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 36.07, "peak": 46.09, "min": 26.0}, "VIN": {"avg": 78.01, "peak": 116.41, "min": 57.01}}, "power_watts_avg": 36.07, "energy_joules_est": 106.79, "sample_count": 23, "duration_seconds": 2.961}, "timestamp": "2026-01-17T15:52:07.787343"}
{"image_index": 412, "image_name": "000000046804.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046804.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3794.368, "latencies_ms": [3794.368], "images_per_second": 0.264, "prompt_tokens": 27, "response_tokens_est": 62, "n_tiles": 12, "output_text": "The main object in the foreground is a sheep standing on a grassy field. The sheep is positioned near the center of the image, with its body facing the camera and its head turned slightly to the side. The background features a rocky wall with patches of yellow moss, adding depth to the scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24924.1, "ram_available_mb": 100848.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 24924.1, "ram_available_mb": 100848.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.92, "peak": 16.26, "min": 13.92}, "VDD_CPU_SOC_MSS": {"avg": 15.43, "peak": 17.32, "min": 14.17}, "VDD_GPU": {"avg": 33.62, "peak": 45.3, "min": 25.6}, "VIN": {"avg": 73.72, "peak": 117.28, "min": 61.32}}, "power_watts_avg": 33.62, "energy_joules_est": 127.58, "sample_count": 29, "duration_seconds": 3.795}, "timestamp": "2026-01-17T15:52:11.589731"}
{"image_index": 412, "image_name": "000000046804.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046804.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2959.893, "latencies_ms": [2959.893], "images_per_second": 0.338, "prompt_tokens": 21, "response_tokens_est": 38, "n_tiles": 12, "output_text": "The image depicts a sheep standing in a grassy field with a rocky, yellowish wall in the background. The sheep appears to be calm and is looking directly at the camera.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24924.1, "ram_available_mb": 100848.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 24925.3, "ram_available_mb": 100846.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.98, "peak": 16.36, "min": 13.92}, "VDD_CPU_SOC_MSS": {"avg": 15.49, "peak": 17.32, "min": 14.17}, "VDD_GPU": {"avg": 35.5, "peak": 45.3, "min": 26.0}, "VIN": {"avg": 77.17, "peak": 111.29, "min": 65.41}}, "power_watts_avg": 35.5, "energy_joules_est": 105.09, "sample_count": 23, "duration_seconds": 2.96}, "timestamp": "2026-01-17T15:52:14.556531"}
{"image_index": 412, "image_name": "000000046804.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046804.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3723.207, "latencies_ms": [3723.207], "images_per_second": 0.269, "prompt_tokens": 19, "response_tokens_est": 60, "n_tiles": 12, "output_text": "The sheep in the image is predominantly white, with a fluffy, dense wool coat that appears soft and well-groomed. The lighting is bright and natural, suggesting a sunny day with clear skies. The grass is green and appears to be well-maintained, indicating a healthy environment.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24925.3, "ram_available_mb": 100846.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 24925.8, "ram_available_mb": 100846.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.91, "peak": 16.26, "min": 13.92}, "VDD_CPU_SOC_MSS": {"avg": 15.41, "peak": 17.32, "min": 14.17}, "VDD_GPU": {"avg": 33.58, "peak": 44.91, "min": 25.61}, "VIN": {"avg": 75.14, "peak": 119.94, "min": 63.02}}, "power_watts_avg": 33.58, "energy_joules_est": 125.04, "sample_count": 29, "duration_seconds": 3.724}, "timestamp": "2026-01-17T15:52:18.286189"}
{"image_index": 413, "image_name": "000000046872.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046872.jpg", "image_width": 640, "image_height": 391, "image_resolution": "640x391", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1764.034, "latencies_ms": [1764.034], "images_per_second": 0.567, "prompt_tokens": 9, "response_tokens_est": 43, "n_tiles": 6, "output_text": "A man in a white t-shirt and plaid shorts is standing next to a large, yellow and black trailer, holding onto a yellow rope, while another man in a blue shirt is pointing towards the trailer.", "error": null, "sys_before": {"cpu_percent": 36.4, "ram_used_mb": 24961.7, "ram_available_mb": 100810.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 24962.9, "ram_available_mb": 100809.3, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.75, "peak": 15.11, "min": 13.61}, "VDD_CPU_SOC_MSS": {"avg": 15.41, "peak": 16.14, "min": 14.17}, "VDD_GPU": {"avg": 31.87, "peak": 40.95, "min": 24.82}, "VIN": {"avg": 72.18, "peak": 104.84, "min": 63.02}}, "power_watts_avg": 31.87, "energy_joules_est": 56.23, "sample_count": 13, "duration_seconds": 1.764}, "timestamp": "2026-01-17T15:52:20.107253"}
{"image_index": 413, "image_name": "000000046872.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046872.jpg", "image_width": 640, "image_height": 391, "image_resolution": "640x391", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1619.758, "latencies_ms": [1619.758], "images_per_second": 0.617, "prompt_tokens": 23, "response_tokens_est": 35, "n_tiles": 6, "output_text": "1. Truck\n2. Person\n3. Tire\n4. Wheel\n5. Trailer\n6. Tire\n7. Wheel\n8. Tire", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24962.9, "ram_available_mb": 100809.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 24963.9, "ram_available_mb": 100808.3, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.0, "peak": 15.54, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.58, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 32.89, "peak": 40.57, "min": 25.21}, "VIN": {"avg": 75.54, "peak": 106.94, "min": 57.26}}, "power_watts_avg": 32.89, "energy_joules_est": 53.29, "sample_count": 12, "duration_seconds": 1.62}, "timestamp": "2026-01-17T15:52:21.733786"}
{"image_index": 413, "image_name": "000000046872.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046872.jpg", "image_width": 640, "image_height": 391, "image_resolution": "640x391", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2305.059, "latencies_ms": [2305.059], "images_per_second": 0.434, "prompt_tokens": 27, "response_tokens_est": 59, "n_tiles": 6, "output_text": "The main objects in the image are a large yellow and black trailer, a man standing on the trailer, and a parked truck in the background. The trailer is positioned in the foreground, with the man standing on it. The truck is located in the background, further away from the trailer.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24963.9, "ram_available_mb": 100808.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 24963.9, "ram_available_mb": 100808.3, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.9, "peak": 15.34, "min": 14.43}, "VDD_CPU_SOC_MSS": {"avg": 15.47, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 30.28, "peak": 40.95, "min": 23.24}, "VIN": {"avg": 73.14, "peak": 113.88, "min": 58.52}}, "power_watts_avg": 30.28, "energy_joules_est": 69.81, "sample_count": 17, "duration_seconds": 2.305}, "timestamp": "2026-01-17T15:52:24.044284"}
{"image_index": 413, "image_name": "000000046872.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046872.jpg", "image_width": 640, "image_height": 391, "image_resolution": "640x391", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2478.294, "latencies_ms": [2478.294], "images_per_second": 0.404, "prompt_tokens": 21, "response_tokens_est": 65, "n_tiles": 6, "output_text": "The scene depicts a man standing next to a large, yellow and black trailer on a paved surface, possibly a parking lot. He is holding a hose, suggesting he is either preparing to fill the trailer or has just finished. The background features a variety of vehicles and trailers, indicating a storage or distribution area.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24963.9, "ram_available_mb": 100808.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 24964.9, "ram_available_mb": 100807.3, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.87, "peak": 15.24, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.5, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 29.27, "peak": 40.16, "min": 23.24}, "VIN": {"avg": 69.18, "peak": 89.34, "min": 62.17}}, "power_watts_avg": 29.27, "energy_joules_est": 72.55, "sample_count": 19, "duration_seconds": 2.479}, "timestamp": "2026-01-17T15:52:26.529275"}
{"image_index": 413, "image_name": "000000046872.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046872.jpg", "image_width": 640, "image_height": 391, "image_resolution": "640x391", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2276.445, "latencies_ms": [2276.445], "images_per_second": 0.439, "prompt_tokens": 19, "response_tokens_est": 58, "n_tiles": 6, "output_text": "The image features a large, yellow and black trailer with black tires, mounted on a flatbed truck. The trailer is being loaded or unloaded, and the truck is parked on a paved surface. The lighting is natural, suggesting daytime, and the weather appears to be clear.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24964.9, "ram_available_mb": 100807.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 24965.8, "ram_available_mb": 100806.3, "ram_percent": 19.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.86, "peak": 15.34, "min": 14.3}, "VDD_CPU_SOC_MSS": {"avg": 15.42, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 29.98, "peak": 40.56, "min": 23.24}, "VIN": {"avg": 68.52, "peak": 99.65, "min": 58.72}}, "power_watts_avg": 29.98, "energy_joules_est": 68.26, "sample_count": 17, "duration_seconds": 2.277}, "timestamp": "2026-01-17T15:52:28.812060"}
{"image_index": 414, "image_name": "000000047010.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047010.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2389.022, "latencies_ms": [2389.022], "images_per_second": 0.419, "prompt_tokens": 9, "response_tokens_est": 23, "n_tiles": 12, "output_text": "The image depicts a giraffe walking in a natural environment with a small goat lying on the ground nearby.", "error": null, "sys_before": {"cpu_percent": 44.1, "ram_used_mb": 24913.6, "ram_available_mb": 100858.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.4, "ram_used_mb": 24915.3, "ram_available_mb": 100856.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.19, "peak": 16.66, "min": 13.39}, "VDD_CPU_SOC_MSS": {"avg": 15.96, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 36.63, "peak": 43.71, "min": 28.36}, "VIN": {"avg": 77.49, "peak": 102.64, "min": 65.86}}, "power_watts_avg": 36.63, "energy_joules_est": 87.52, "sample_count": 18, "duration_seconds": 2.389}, "timestamp": "2026-01-17T15:52:31.294403"}
{"image_index": 414, "image_name": "000000047010.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047010.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2103.526, "latencies_ms": [2103.526], "images_per_second": 0.475, "prompt_tokens": 23, "response_tokens_est": 13, "n_tiles": 12, "output_text": "giraffe: 2\ngoat: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24915.3, "ram_available_mb": 100856.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.0, "ram_used_mb": 24916.3, "ram_available_mb": 100855.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.34, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.99, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 39.33, "peak": 45.28, "min": 31.51}, "VIN": {"avg": 81.97, "peak": 112.79, "min": 66.45}}, "power_watts_avg": 39.33, "energy_joules_est": 82.75, "sample_count": 16, "duration_seconds": 2.104}, "timestamp": "2026-01-17T15:52:33.404704"}
{"image_index": 414, "image_name": "000000047010.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047010.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3783.156, "latencies_ms": [3783.156], "images_per_second": 0.264, "prompt_tokens": 27, "response_tokens_est": 63, "n_tiles": 12, "output_text": "In the image, the giraffes are positioned in the foreground, with the closest giraffe standing near the water's edge. The second giraffe is further back, walking on the dirt. The background features a dense forest with various trees and shrubs, creating a natural habitat for the giraffes.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24916.3, "ram_available_mb": 100855.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 24917.0, "ram_available_mb": 100855.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.29, "peak": 16.66, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.88, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 34.2, "peak": 46.07, "min": 26.39}, "VIN": {"avg": 78.59, "peak": 127.28, "min": 63.58}}, "power_watts_avg": 34.2, "energy_joules_est": 129.39, "sample_count": 29, "duration_seconds": 3.783}, "timestamp": "2026-01-17T15:52:37.194482"}
{"image_index": 414, "image_name": "000000047010.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047010.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 4209.069, "latencies_ms": [4209.069], "images_per_second": 0.238, "prompt_tokens": 21, "response_tokens_est": 76, "n_tiles": 12, "output_text": "The image depicts a serene and natural setting, likely within a zoo or wildlife reserve, featuring a dirt path surrounded by lush greenery. The path is flanked by trees and bushes, with a small pond reflecting the surrounding foliage. On the path, there are giraffes and a goat, with the giraffes walking and the goat resting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24917.0, "ram_available_mb": 100855.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 24917.5, "ram_available_mb": 100854.7, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.28, "peak": 16.66, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.87, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 33.05, "peak": 44.89, "min": 26.01}, "VIN": {"avg": 73.57, "peak": 104.42, "min": 62.25}}, "power_watts_avg": 33.05, "energy_joules_est": 139.13, "sample_count": 32, "duration_seconds": 4.21}, "timestamp": "2026-01-17T15:52:41.410296"}
{"image_index": 414, "image_name": "000000047010.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047010.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3709.271, "latencies_ms": [3709.271], "images_per_second": 0.27, "prompt_tokens": 19, "response_tokens_est": 60, "n_tiles": 12, "output_text": "The image depicts a giraffe standing in a natural, grassy environment with a dirt path. The giraffe has a brown and white coat, and its long neck and legs are prominently visible. The lighting is natural, suggesting it is daytime, and the weather appears to be clear.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24917.5, "ram_available_mb": 100854.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 24918.0, "ram_available_mb": 100854.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.22, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.83, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 33.97, "peak": 44.89, "min": 26.0}, "VIN": {"avg": 76.46, "peak": 114.28, "min": 66.58}}, "power_watts_avg": 33.97, "energy_joules_est": 126.02, "sample_count": 28, "duration_seconds": 3.71}, "timestamp": "2026-01-17T15:52:45.126269"}
{"image_index": 415, "image_name": "000000047112.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047112.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 3401.106, "latencies_ms": [3401.106], "images_per_second": 0.294, "prompt_tokens": 9, "response_tokens_est": 53, "n_tiles": 12, "output_text": "The image shows a freshly baked pizza with melted cheese, sliced mushrooms, and pieces of ham, all resting on a white plate, set on a table covered with a blue tablecloth, with a glass of beer and a glass of water in the background.", "error": null, "sys_before": {"cpu_percent": 41.3, "ram_used_mb": 24936.4, "ram_available_mb": 100835.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 24930.6, "ram_available_mb": 100841.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.15, "peak": 16.66, "min": 13.39}, "VDD_CPU_SOC_MSS": {"avg": 15.81, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 33.66, "peak": 43.73, "min": 26.0}, "VIN": {"avg": 72.92, "peak": 103.5, "min": 62.63}}, "power_watts_avg": 33.66, "energy_joules_est": 114.49, "sample_count": 26, "duration_seconds": 3.401}, "timestamp": "2026-01-17T15:52:48.629201"}
{"image_index": 415, "image_name": "000000047112.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047112.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 6027.494, "latencies_ms": [6027.494], "images_per_second": 0.166, "prompt_tokens": 23, "response_tokens_est": 128, "n_tiles": 12, "output_text": "object: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject:", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24930.6, "ram_available_mb": 100841.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 24926.0, "ram_available_mb": 100846.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.16, "peak": 16.56, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.59, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 30.64, "peak": 44.89, "min": 26.0}, "VIN": {"avg": 72.14, "peak": 118.13, "min": 56.16}}, "power_watts_avg": 30.64, "energy_joules_est": 184.69, "sample_count": 47, "duration_seconds": 6.028}, "timestamp": "2026-01-17T15:52:54.664994"}
{"image_index": 415, "image_name": "000000047112.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047112.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 4739.577, "latencies_ms": [4739.577], "images_per_second": 0.211, "prompt_tokens": 27, "response_tokens_est": 90, "n_tiles": 12, "output_text": "The main object in the foreground is a large, freshly baked pizza with melted cheese and toppings. The pizza is placed on a white plate, which is placed on a table covered with a blue tablecloth. In the background, there are two wine glasses and a bottle of wine on a bar counter, indicating that the setting is a restaurant. The pizza is positioned near the center of the image, drawing attention to its size and appearance.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 24926.0, "ram_available_mb": 100846.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 24926.8, "ram_available_mb": 100845.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.17, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.65, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 31.95, "peak": 44.89, "min": 26.0}, "VIN": {"avg": 73.1, "peak": 117.96, "min": 54.47}}, "power_watts_avg": 31.95, "energy_joules_est": 151.44, "sample_count": 37, "duration_seconds": 4.74}, "timestamp": "2026-01-17T15:52:59.411290"}
{"image_index": 415, "image_name": "000000047112.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047112.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 4917.852, "latencies_ms": [4917.852], "images_per_second": 0.203, "prompt_tokens": 21, "response_tokens_est": 94, "n_tiles": 12, "output_text": "The image depicts a cozy dining setting in a restaurant, with a table set for two. The table is covered with a white tablecloth, and there is a large, freshly baked pizza placed on a white plate. The pizza is topped with melted cheese, sliced mushrooms, and pieces of ham, and it appears to be freshly baked. The background shows other tables and chairs, and a bar with various bottles and glasses, suggesting a lively and inviting atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24926.8, "ram_available_mb": 100845.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 24927.0, "ram_available_mb": 100845.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.87, "peak": 16.46, "min": 14.12}, "VDD_CPU_SOC_MSS": {"avg": 15.35, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 31.52, "peak": 45.3, "min": 25.22}, "VIN": {"avg": 74.33, "peak": 125.01, "min": 59.27}}, "power_watts_avg": 31.52, "energy_joules_est": 155.02, "sample_count": 38, "duration_seconds": 4.918}, "timestamp": "2026-01-17T15:53:04.335864"}
{"image_index": 415, "image_name": "000000047112.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047112.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 4075.932, "latencies_ms": [4075.932], "images_per_second": 0.245, "prompt_tokens": 19, "response_tokens_est": 70, "n_tiles": 12, "output_text": "The image features a large, golden-brown pizza with melted cheese and various toppings, including mushrooms and possibly ham, resting on a white plate. The lighting is warm and inviting, casting a soft glow on the pizza and creating a cozy atmosphere. The background includes a dimly lit bar with bottles and glasses, adding to the ambiance.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 24927.0, "ram_available_mb": 100845.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 24924.4, "ram_available_mb": 100847.7, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.89, "peak": 16.26, "min": 13.9}, "VDD_CPU_SOC_MSS": {"avg": 15.35, "peak": 17.32, "min": 14.17}, "VDD_GPU": {"avg": 32.92, "peak": 44.49, "min": 25.6}, "VIN": {"avg": 74.64, "peak": 124.8, "min": 57.16}}, "power_watts_avg": 32.92, "energy_joules_est": 134.2, "sample_count": 31, "duration_seconds": 4.076}, "timestamp": "2026-01-17T15:53:08.418544"}
{"image_index": 416, "image_name": "000000047121.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047121.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2601.3, "latencies_ms": [2601.3], "images_per_second": 0.384, "prompt_tokens": 9, "response_tokens_est": 29, "n_tiles": 12, "output_text": "A black cat is resting on the edge of a white bathtub, with a bottle of shampoo and a soap dispenser nearby.", "error": null, "sys_before": {"cpu_percent": 43.5, "ram_used_mb": 24935.4, "ram_available_mb": 100836.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 24933.1, "ram_available_mb": 100839.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.09, "peak": 16.66, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 15.78, "peak": 17.71, "min": 13.78}, "VDD_GPU": {"avg": 35.88, "peak": 44.12, "min": 27.18}, "VIN": {"avg": 78.98, "peak": 110.39, "min": 59.86}}, "power_watts_avg": 35.88, "energy_joules_est": 93.35, "sample_count": 20, "duration_seconds": 2.602}, "timestamp": "2026-01-17T15:53:11.133621"}
{"image_index": 416, "image_name": "000000047121.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047121.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 3165.507, "latencies_ms": [3165.507], "images_per_second": 0.316, "prompt_tokens": 23, "response_tokens_est": 44, "n_tiles": 12, "output_text": "- cat: 1\n- sink: 1\n- water tap: 1\n- soap bottle: 1\n- glass: 1\n- bowl: 1\n- wall: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24933.1, "ram_available_mb": 100839.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 24928.6, "ram_available_mb": 100843.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.23, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.74, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 35.19, "peak": 45.68, "min": 26.39}, "VIN": {"avg": 77.6, "peak": 117.04, "min": 58.36}}, "power_watts_avg": 35.19, "energy_joules_est": 111.41, "sample_count": 24, "duration_seconds": 3.166}, "timestamp": "2026-01-17T15:53:14.306115"}
{"image_index": 416, "image_name": "000000047121.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047121.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 5024.005, "latencies_ms": [5024.005], "images_per_second": 0.199, "prompt_tokens": 27, "response_tokens_est": 98, "n_tiles": 12, "output_text": "The main object in the foreground is a black cat, which is perched on the edge of a white bathtub. The cat is positioned near the edge of the tub, with its head resting on the edge and its body leaning slightly towards the water. To the left of the cat, there is a white soap dispenser and a small white dish on the countertop. The background is mostly out of focus, with a light-colored wall and a window with a white frame.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24928.6, "ram_available_mb": 100843.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 24927.6, "ram_available_mb": 100844.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.14, "peak": 16.66, "min": 14.12}, "VDD_CPU_SOC_MSS": {"avg": 15.63, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 31.66, "peak": 44.91, "min": 25.6}, "VIN": {"avg": 74.7, "peak": 113.82, "min": 61.7}}, "power_watts_avg": 31.66, "energy_joules_est": 159.07, "sample_count": 39, "duration_seconds": 5.024}, "timestamp": "2026-01-17T15:53:19.338555"}
{"image_index": 416, "image_name": "000000047121.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047121.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3815.324, "latencies_ms": [3815.324], "images_per_second": 0.262, "prompt_tokens": 21, "response_tokens_est": 63, "n_tiles": 12, "output_text": "The image depicts a bathroom setting with a white bathtub and a white wall. A black cat is seen resting on the edge of the bathtub, seemingly enjoying the view or simply taking a nap. The scene is calm and serene, with the cat appearing relaxed and comfortable in its environment.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24927.6, "ram_available_mb": 100844.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 24929.0, "ram_available_mb": 100843.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.19, "peak": 16.66, "min": 14.12}, "VDD_CPU_SOC_MSS": {"avg": 15.79, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 33.52, "peak": 44.91, "min": 26.0}, "VIN": {"avg": 74.77, "peak": 96.1, "min": 56.63}}, "power_watts_avg": 33.52, "energy_joules_est": 127.9, "sample_count": 29, "duration_seconds": 3.816}, "timestamp": "2026-01-17T15:53:23.161037"}
{"image_index": 416, "image_name": "000000047121.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047121.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3606.232, "latencies_ms": [3606.232], "images_per_second": 0.277, "prompt_tokens": 19, "response_tokens_est": 57, "n_tiles": 12, "output_text": "The image features a black cat resting on a white bathtub. The bathtub is situated against a light-colored wall, and the cat is positioned near a silver faucet and a clear glass. The lighting in the room is soft and warm, creating a cozy atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24929.0, "ram_available_mb": 100843.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 24930.3, "ram_available_mb": 100841.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.19, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.75, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 33.82, "peak": 44.91, "min": 26.0}, "VIN": {"avg": 80.28, "peak": 133.35, "min": 60.27}}, "power_watts_avg": 33.82, "energy_joules_est": 121.97, "sample_count": 28, "duration_seconds": 3.607}, "timestamp": "2026-01-17T15:53:26.774258"}
{"image_index": 417, "image_name": "000000047571.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047571.jpg", "image_width": 639, "image_height": 640, "image_resolution": "639x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1346.859, "latencies_ms": [1346.859], "images_per_second": 0.742, "prompt_tokens": 9, "response_tokens_est": 39, "n_tiles": 4, "output_text": "A horse-drawn cart is seen in a muddy field, with a woman and a man seated inside, both wearing hats and boots, and the horse is equipped with a harness.", "error": null, "sys_before": {"cpu_percent": 33.8, "ram_used_mb": 24940.0, "ram_available_mb": 100832.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 24940.7, "ram_available_mb": 100831.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4606.7, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5444.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.62, "peak": 15.11, "min": 13.82}, "VDD_CPU_SOC_MSS": {"avg": 15.39, "peak": 16.14, "min": 14.57}, "VDD_GPU": {"avg": 29.47, "peak": 37.03, "min": 23.64}, "VIN": {"avg": 68.92, "peak": 94.86, "min": 57.23}}, "power_watts_avg": 29.47, "energy_joules_est": 39.71, "sample_count": 10, "duration_seconds": 1.347}, "timestamp": "2026-01-17T15:53:28.178095"}
{"image_index": 417, "image_name": "000000047571.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047571.jpg", "image_width": 639, "image_height": 640, "image_resolution": "639x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1208.676, "latencies_ms": [1208.676], "images_per_second": 0.827, "prompt_tokens": 23, "response_tokens_est": 33, "n_tiles": 4, "output_text": "1. Carriage\n2. Horse\n3. Person\n4. Person\n5. Person\n6. Person\n7. Person\n8. Person", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24940.7, "ram_available_mb": 100831.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 24941.4, "ram_available_mb": 100830.7, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4606.7, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5455.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.81, "peak": 15.22, "min": 14.51}, "VDD_CPU_SOC_MSS": {"avg": 15.57, "peak": 15.75, "min": 15.35}, "VDD_GPU": {"avg": 30.59, "peak": 37.82, "min": 24.82}, "VIN": {"avg": 70.14, "peak": 89.07, "min": 58.25}}, "power_watts_avg": 30.59, "energy_joules_est": 36.99, "sample_count": 9, "duration_seconds": 1.209}, "timestamp": "2026-01-17T15:53:29.393171"}
{"image_index": 417, "image_name": "000000047571.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047571.jpg", "image_width": 639, "image_height": 640, "image_resolution": "639x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2088.501, "latencies_ms": [2088.501], "images_per_second": 0.479, "prompt_tokens": 27, "response_tokens_est": 68, "n_tiles": 4, "output_text": "In the image, the main objects are a horse-drawn cart and a horse. The horse-drawn cart is positioned in the foreground, with its wheels and the horse in the middle ground. The horse is near the cart, and the background features a clear blue sky and some structures, possibly a barn or farm building.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24941.4, "ram_available_mb": 100830.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 24941.7, "ram_available_mb": 100830.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4606.7, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5458.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.01, "peak": 15.32, "min": 14.51}, "VDD_CPU_SOC_MSS": {"avg": 15.85, "peak": 16.14, "min": 15.35}, "VDD_GPU": {"avg": 27.18, "peak": 37.42, "min": 22.46}, "VIN": {"avg": 68.26, "peak": 104.25, "min": 58.42}}, "power_watts_avg": 27.18, "energy_joules_est": 56.77, "sample_count": 16, "duration_seconds": 2.089}, "timestamp": "2026-01-17T15:53:31.488972"}
{"image_index": 417, "image_name": "000000047571.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047571.jpg", "image_width": 639, "image_height": 640, "image_resolution": "639x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1654.978, "latencies_ms": [1654.978], "images_per_second": 0.604, "prompt_tokens": 21, "response_tokens_est": 51, "n_tiles": 4, "output_text": "The image depicts a scene in a rural setting with a horse-drawn cart and a person riding it. The cart is in a muddy area with water reflecting the surroundings, and the person is wearing a hat and a plaid shirt.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24941.7, "ram_available_mb": 100830.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 24942.9, "ram_available_mb": 100829.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4606.7, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5453.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.91, "peak": 15.22, "min": 14.51}, "VDD_CPU_SOC_MSS": {"avg": 15.71, "peak": 16.14, "min": 15.35}, "VDD_GPU": {"avg": 28.42, "peak": 37.01, "min": 22.85}, "VIN": {"avg": 70.83, "peak": 111.22, "min": 60.64}}, "power_watts_avg": 28.42, "energy_joules_est": 47.05, "sample_count": 12, "duration_seconds": 1.655}, "timestamp": "2026-01-17T15:53:33.154136"}
{"image_index": 417, "image_name": "000000047571.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047571.jpg", "image_width": 639, "image_height": 640, "image_resolution": "639x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1652.765, "latencies_ms": [1652.765], "images_per_second": 0.605, "prompt_tokens": 19, "response_tokens_est": 51, "n_tiles": 4, "output_text": "The image depicts a scene with a horse-drawn cart in a muddy, wet environment. The cart is made of wood, and the horse is brown with a shiny coat. The sky is clear and blue, indicating a sunny day.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24942.9, "ram_available_mb": 100829.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 24942.7, "ram_available_mb": 100829.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4606.7, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5451.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.88, "peak": 15.22, "min": 14.51}, "VDD_CPU_SOC_MSS": {"avg": 15.72, "peak": 16.14, "min": 15.35}, "VDD_GPU": {"avg": 28.4, "peak": 36.63, "min": 22.86}, "VIN": {"avg": 70.32, "peak": 109.54, "min": 57.4}}, "power_watts_avg": 28.4, "energy_joules_est": 46.95, "sample_count": 12, "duration_seconds": 1.653}, "timestamp": "2026-01-17T15:53:34.813399"}
{"image_index": 418, "image_name": "000000047585.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047585.jpg", "image_width": 424, "image_height": 640, "image_resolution": "424x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1480.534, "latencies_ms": [1480.534], "images_per_second": 0.675, "prompt_tokens": 9, "response_tokens_est": 31, "n_tiles": 6, "output_text": "A bride and groom are walking down the aisle in a wedding ceremony, with a man in a suit holding an umbrella to shield them from the rain.", "error": null, "sys_before": {"cpu_percent": 45.8, "ram_used_mb": 24954.7, "ram_available_mb": 100817.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 24955.9, "ram_available_mb": 100816.3, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.65, "peak": 15.14, "min": 14.0}, "VDD_CPU_SOC_MSS": {"avg": 15.35, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 32.55, "peak": 40.16, "min": 25.21}, "VIN": {"avg": 71.92, "peak": 101.92, "min": 54.2}}, "power_watts_avg": 32.55, "energy_joules_est": 48.2, "sample_count": 11, "duration_seconds": 1.481}, "timestamp": "2026-01-17T15:53:36.345661"}
{"image_index": 418, "image_name": "000000047585.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047585.jpg", "image_width": 424, "image_height": 640, "image_resolution": "424x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2517.095, "latencies_ms": [2517.095], "images_per_second": 0.397, "prompt_tokens": 23, "response_tokens_est": 66, "n_tiles": 6, "output_text": "1. Wedding dress: 1\n2. Wedding umbrella: 1\n3. Man: 1\n4. Woman: 1\n5. Flower bouquet: 1\n6. Man's hand: 1\n7. Woman's hand: 1\n8. Man's face: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24955.9, "ram_available_mb": 100816.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 24956.6, "ram_available_mb": 100815.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.84, "peak": 15.24, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.44, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 29.52, "peak": 40.56, "min": 22.85}, "VIN": {"avg": 71.17, "peak": 114.82, "min": 62.95}}, "power_watts_avg": 29.52, "energy_joules_est": 74.31, "sample_count": 19, "duration_seconds": 2.517}, "timestamp": "2026-01-17T15:53:38.873097"}
{"image_index": 418, "image_name": "000000047585.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047585.jpg", "image_width": 424, "image_height": 640, "image_resolution": "424x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2461.575, "latencies_ms": [2461.575], "images_per_second": 0.406, "prompt_tokens": 27, "response_tokens_est": 64, "n_tiles": 6, "output_text": "The main objects in the image are a bride and groom, standing in the foreground. The groom is holding an umbrella, and the bride is holding a bouquet of flowers. The background features a stone building and a group of people, with the bride and groom positioned slightly to the left and right of the center.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24956.6, "ram_available_mb": 100815.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 24957.4, "ram_available_mb": 100814.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.81, "peak": 15.24, "min": 14.2}, "VDD_CPU_SOC_MSS": {"avg": 15.29, "peak": 15.74, "min": 14.56}, "VDD_GPU": {"avg": 29.17, "peak": 40.56, "min": 22.85}, "VIN": {"avg": 72.98, "peak": 113.5, "min": 52.61}}, "power_watts_avg": 29.17, "energy_joules_est": 71.82, "sample_count": 19, "duration_seconds": 2.462}, "timestamp": "2026-01-17T15:53:41.340499"}
{"image_index": 418, "image_name": "000000047585.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047585.jpg", "image_width": 424, "image_height": 640, "image_resolution": "424x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2628.46, "latencies_ms": [2628.46], "images_per_second": 0.38, "prompt_tokens": 21, "response_tokens_est": 70, "n_tiles": 6, "output_text": "The scene depicts a wedding ceremony taking place outdoors on a grassy area. The bride, dressed in a white gown, is holding a bouquet of flowers, while the groom stands beside her, dressed in a black suit. The setting includes a stone building in the background, and the atmosphere appears to be joyful and celebratory.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24957.4, "ram_available_mb": 100814.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 24957.4, "ram_available_mb": 100814.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.83, "peak": 15.14, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.35, "peak": 15.75, "min": 14.56}, "VDD_GPU": {"avg": 28.93, "peak": 40.57, "min": 22.85}, "VIN": {"avg": 67.86, "peak": 93.18, "min": 49.41}}, "power_watts_avg": 28.93, "energy_joules_est": 76.05, "sample_count": 20, "duration_seconds": 2.629}, "timestamp": "2026-01-17T15:53:43.975012"}
{"image_index": 418, "image_name": "000000047585.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047585.jpg", "image_width": 424, "image_height": 640, "image_resolution": "424x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1857.916, "latencies_ms": [1857.916], "images_per_second": 0.538, "prompt_tokens": 19, "response_tokens_est": 43, "n_tiles": 6, "output_text": "The image features a bride and groom standing under a black and white striped umbrella, holding a bouquet of flowers. The setting appears to be outdoors during the daytime, with natural lighting illuminating the scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24957.4, "ram_available_mb": 100814.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 24957.6, "ram_available_mb": 100814.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.82, "peak": 15.24, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.3, "peak": 15.74, "min": 14.56}, "VDD_GPU": {"avg": 31.23, "peak": 40.16, "min": 23.64}, "VIN": {"avg": 67.91, "peak": 81.08, "min": 59.11}}, "power_watts_avg": 31.23, "energy_joules_est": 58.04, "sample_count": 14, "duration_seconds": 1.858}, "timestamp": "2026-01-17T15:53:45.839365"}
{"image_index": 419, "image_name": "000000047740.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047740.jpg", "image_width": 640, "image_height": 359, "image_resolution": "640x359", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1148.633, "latencies_ms": [1148.633], "images_per_second": 0.871, "prompt_tokens": 9, "response_tokens_est": 43, "n_tiles": 2, "output_text": "Two individuals are relaxing on a sandy beach, with one lying on their back and the other sitting on the sand, both enjoying the tranquil atmosphere of the ocean and the colorful kite in the sky.", "error": null, "sys_before": {"cpu_percent": 32.4, "ram_used_mb": 24957.6, "ram_available_mb": 100814.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 24958.1, "ram_available_mb": 100814.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5055.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.84, "peak": 15.22, "min": 14.2}, "VDD_CPU_SOC_MSS": {"avg": 15.6, "peak": 16.54, "min": 14.96}, "VDD_GPU": {"avg": 25.71, "peak": 31.91, "min": 22.07}, "VIN": {"avg": 65.44, "peak": 78.89, "min": 59.83}}, "power_watts_avg": 25.71, "energy_joules_est": 29.54, "sample_count": 8, "duration_seconds": 1.149}, "timestamp": "2026-01-17T15:53:47.019461"}
{"image_index": 419, "image_name": "000000047740.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047740.jpg", "image_width": 640, "image_height": 359, "image_resolution": "640x359", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1335.244, "latencies_ms": [1335.244], "images_per_second": 0.749, "prompt_tokens": 23, "response_tokens_est": 50, "n_tiles": 2, "output_text": "beach: 1\nsand: 1\nocean: 1\nkite: 1\nperson: 2\ntrunks: 1\nshirt: 1\nwatch: 1\nbag: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24958.1, "ram_available_mb": 100814.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24958.3, "ram_available_mb": 100813.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5064.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.93, "peak": 15.32, "min": 14.3}, "VDD_CPU_SOC_MSS": {"avg": 16.07, "peak": 16.54, "min": 15.36}, "VDD_GPU": {"avg": 24.63, "peak": 30.74, "min": 21.28}, "VIN": {"avg": 67.2, "peak": 108.36, "min": 58.33}}, "power_watts_avg": 24.63, "energy_joules_est": 32.9, "sample_count": 10, "duration_seconds": 1.336}, "timestamp": "2026-01-17T15:53:48.360818"}
{"image_index": 419, "image_name": "000000047740.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047740.jpg", "image_width": 640, "image_height": 359, "image_resolution": "640x359", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1790.374, "latencies_ms": [1790.374], "images_per_second": 0.559, "prompt_tokens": 27, "response_tokens_est": 70, "n_tiles": 2, "output_text": "The main objects in the image are a person lying on the sandy beach, a colorful kite in the foreground, and another person in the background. The person on the beach is positioned in the foreground, while the kite is in the foreground as well. The person in the background is further away, indicating a depth of field effect.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24958.3, "ram_available_mb": 100813.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 24958.8, "ram_available_mb": 100813.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5067.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.22, "peak": 15.52, "min": 14.51}, "VDD_CPU_SOC_MSS": {"avg": 15.93, "peak": 16.14, "min": 15.35}, "VDD_GPU": {"avg": 23.79, "peak": 30.33, "min": 20.89}, "VIN": {"avg": 65.88, "peak": 104.18, "min": 58.38}}, "power_watts_avg": 23.79, "energy_joules_est": 42.6, "sample_count": 13, "duration_seconds": 1.791}, "timestamp": "2026-01-17T15:53:50.157344"}
{"image_index": 419, "image_name": "000000047740.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047740.jpg", "image_width": 640, "image_height": 359, "image_resolution": "640x359", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1600.487, "latencies_ms": [1600.487], "images_per_second": 0.625, "prompt_tokens": 21, "response_tokens_est": 62, "n_tiles": 2, "output_text": "The image depicts a sunny beach scene with two individuals lying on the sand. One person is lying on their back, while the other is lying on their stomach, both appearing relaxed and enjoying the beach. A colorful kite is visible in the background, adding to the leisurely atmosphere of the setting.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 24958.8, "ram_available_mb": 100813.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 24958.8, "ram_available_mb": 100813.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5063.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.2, "peak": 15.62, "min": 14.81}, "VDD_CPU_SOC_MSS": {"avg": 16.01, "peak": 16.14, "min": 15.35}, "VDD_GPU": {"avg": 23.94, "peak": 30.34, "min": 20.88}, "VIN": {"avg": 65.94, "peak": 96.1, "min": 56.48}}, "power_watts_avg": 23.94, "energy_joules_est": 38.32, "sample_count": 12, "duration_seconds": 1.601}, "timestamp": "2026-01-17T15:53:51.763671"}
{"image_index": 419, "image_name": "000000047740.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047740.jpg", "image_width": 640, "image_height": 359, "image_resolution": "640x359", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1313.611, "latencies_ms": [1313.611], "images_per_second": 0.761, "prompt_tokens": 19, "response_tokens_est": 49, "n_tiles": 2, "output_text": "The image depicts a sunny beach scene with sandy terrain, clear blue skies, and gentle waves. The sandy beach is adorned with colorful beach paraphernalia, including a striped beach towel and a colorful beach ball.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24958.8, "ram_available_mb": 100813.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 24959.8, "ram_available_mb": 100812.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5062.6, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.11, "peak": 15.62, "min": 14.61}, "VDD_CPU_SOC_MSS": {"avg": 15.7, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 24.39, "peak": 30.73, "min": 21.27}, "VIN": {"avg": 67.01, "peak": 102.22, "min": 60.12}}, "power_watts_avg": 24.39, "energy_joules_est": 32.05, "sample_count": 10, "duration_seconds": 1.314}, "timestamp": "2026-01-17T15:53:53.084101"}
{"image_index": 420, "image_name": "000000047769.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047769.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2852.952, "latencies_ms": [2852.952], "images_per_second": 0.351, "prompt_tokens": 9, "response_tokens_est": 37, "n_tiles": 12, "output_text": "The image shows a cozy living room with a brown sofa, a red armchair, a black stool, a small table with a lamp, and a television on a stand.", "error": null, "sys_before": {"cpu_percent": 44.7, "ram_used_mb": 24921.1, "ram_available_mb": 100851.0, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 24921.4, "ram_available_mb": 100850.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.29, "peak": 16.66, "min": 14.1}, "VDD_CPU_SOC_MSS": {"avg": 16.01, "peak": 17.71, "min": 14.96}, "VDD_GPU": {"avg": 34.85, "peak": 43.71, "min": 26.79}, "VIN": {"avg": 79.88, "peak": 129.75, "min": 63.86}}, "power_watts_avg": 34.85, "energy_joules_est": 99.44, "sample_count": 22, "duration_seconds": 2.853}, "timestamp": "2026-01-17T15:53:56.007591"}
{"image_index": 420, "image_name": "000000047769.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047769.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2920.574, "latencies_ms": [2920.574], "images_per_second": 0.342, "prompt_tokens": 23, "response_tokens_est": 37, "n_tiles": 12, "output_text": "1. TV stand\n2. TV\n3. Coffee table\n4. Stools\n5. Lamps\n6. Couch\n7. Bookshelf\n8. Book", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24921.4, "ram_available_mb": 100850.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 24922.6, "ram_available_mb": 100849.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.25, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.76, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 35.89, "peak": 45.68, "min": 26.8}, "VIN": {"avg": 78.06, "peak": 126.38, "min": 62.76}}, "power_watts_avg": 35.89, "energy_joules_est": 104.83, "sample_count": 23, "duration_seconds": 2.921}, "timestamp": "2026-01-17T15:53:58.937997"}
{"image_index": 420, "image_name": "000000047769.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047769.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 5472.53, "latencies_ms": [5472.53], "images_per_second": 0.183, "prompt_tokens": 27, "response_tokens_est": 110, "n_tiles": 12, "output_text": "The main objects in the image include a brown sofa, a red chair, a black stool, a small table, a lamp, a television, and a window with blinds. The sofa is positioned in the foreground, the red chair is to the right of the sofa, the black stool is in front of the red chair, the small table is between the stool and the window, the lamp is on the table, the television is on the right side of the image, and the window with blinds is in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24922.6, "ram_available_mb": 100849.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 24922.3, "ram_available_mb": 100849.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.91, "peak": 16.46, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.3, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 31.05, "peak": 45.28, "min": 25.6}, "VIN": {"avg": 73.81, "peak": 120.03, "min": 56.01}}, "power_watts_avg": 31.05, "energy_joules_est": 169.94, "sample_count": 43, "duration_seconds": 5.473}, "timestamp": "2026-01-17T15:54:04.417665"}
{"image_index": 420, "image_name": "000000047769.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047769.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 4312.326, "latencies_ms": [4312.326], "images_per_second": 0.232, "prompt_tokens": 21, "response_tokens_est": 77, "n_tiles": 12, "output_text": "The image depicts a cozy living room with a warm, inviting ambiance. The room features a brown leather sofa, a black wooden stool, a red armchair, a small table with a lamp, and a television on a stand. The room is well-lit, with natural light coming through a window with white blinds, creating a serene and comfortable atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24922.3, "ram_available_mb": 100849.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 24923.1, "ram_available_mb": 100849.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.87, "peak": 16.36, "min": 14.02}, "VDD_CPU_SOC_MSS": {"avg": 15.3, "peak": 17.32, "min": 14.17}, "VDD_GPU": {"avg": 32.4, "peak": 45.3, "min": 25.6}, "VIN": {"avg": 76.11, "peak": 125.46, "min": 57.41}}, "power_watts_avg": 32.4, "energy_joules_est": 139.73, "sample_count": 34, "duration_seconds": 4.313}, "timestamp": "2026-01-17T15:54:08.736227"}
{"image_index": 420, "image_name": "000000047769.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047769.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2993.541, "latencies_ms": [2993.541], "images_per_second": 0.334, "prompt_tokens": 19, "response_tokens_est": 39, "n_tiles": 12, "output_text": "The room is well-lit with warm lighting, creating a cozy atmosphere. The walls are painted in a light beige color, complementing the wooden window shutters and the wooden floor.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24923.1, "ram_available_mb": 100849.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 24924.0, "ram_available_mb": 100848.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.96, "peak": 16.26, "min": 13.92}, "VDD_CPU_SOC_MSS": {"avg": 15.54, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 35.5, "peak": 45.3, "min": 26.01}, "VIN": {"avg": 80.99, "peak": 126.11, "min": 64.98}}, "power_watts_avg": 35.5, "energy_joules_est": 106.28, "sample_count": 23, "duration_seconds": 2.994}, "timestamp": "2026-01-17T15:54:11.736849"}
{"image_index": 421, "image_name": "000000047801.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047801.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 690.143, "latencies_ms": [690.143], "images_per_second": 1.449, "prompt_tokens": 9, "response_tokens_est": 27, "n_tiles": 1, "output_text": "A man is eating a slice of cake outdoors, holding a fork and a plate, with trees and greenery in the background.", "error": null, "sys_before": {"cpu_percent": 22.9, "ram_used_mb": 24925.5, "ram_available_mb": 100846.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 24926.8, "ram_available_mb": 100845.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 4845.7, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.71, "peak": 15.11, "min": 14.1}, "VDD_CPU_SOC_MSS": {"avg": 15.43, "peak": 16.14, "min": 14.57}, "VDD_GPU": {"avg": 23.88, "peak": 26.8, "min": 21.67}, "VIN": {"avg": 62.32, "peak": 64.2, "min": 59.95}}, "power_watts_avg": 23.88, "energy_joules_est": 16.49, "sample_count": 5, "duration_seconds": 0.69}, "timestamp": "2026-01-17T15:54:12.456661"}
{"image_index": 421, "image_name": "000000047801.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047801.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1064.148, "latencies_ms": [1064.148], "images_per_second": 0.94, "prompt_tokens": 23, "response_tokens_est": 45, "n_tiles": 1, "output_text": "object: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24926.8, "ram_available_mb": 100845.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24927.2, "ram_available_mb": 100844.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 4856.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.32, "peak": 15.52, "min": 14.91}, "VDD_CPU_SOC_MSS": {"avg": 16.34, "peak": 16.54, "min": 15.75}, "VDD_GPU": {"avg": 22.02, "peak": 24.83, "min": 20.49}, "VIN": {"avg": 63.5, "peak": 72.38, "min": 55.89}}, "power_watts_avg": 22.02, "energy_joules_est": 23.44, "sample_count": 8, "duration_seconds": 1.065}, "timestamp": "2026-01-17T15:54:13.527623"}
{"image_index": 421, "image_name": "000000047801.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047801.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1038.106, "latencies_ms": [1038.106], "images_per_second": 0.963, "prompt_tokens": 27, "response_tokens_est": 44, "n_tiles": 1, "output_text": "The main object in the foreground is a man holding a plate with a slice of cake. He is wearing glasses and a blue t-shirt. The background features a lush green park with trees and a clear blue sky.", "error": null, "sys_before": {"cpu_percent": 16.7, "ram_used_mb": 24927.2, "ram_available_mb": 100844.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24928.5, "ram_available_mb": 100843.7, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 4859.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.39, "peak": 15.72, "min": 14.91}, "VDD_CPU_SOC_MSS": {"avg": 16.49, "peak": 16.93, "min": 16.14}, "VDD_GPU": {"avg": 21.62, "peak": 24.04, "min": 20.48}, "VIN": {"avg": 62.52, "peak": 67.42, "min": 56.14}}, "power_watts_avg": 21.62, "energy_joules_est": 22.45, "sample_count": 8, "duration_seconds": 1.038}, "timestamp": "2026-01-17T15:54:14.572668"}
{"image_index": 421, "image_name": "000000047801.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047801.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1252.427, "latencies_ms": [1252.427], "images_per_second": 0.798, "prompt_tokens": 21, "response_tokens_est": 54, "n_tiles": 1, "output_text": "The image depicts a man sitting outdoors, enjoying a slice of cake. He is holding a white plate with a slice of cake and a spoon, and is wearing glasses. The setting appears to be a park or garden with greenery and trees in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24928.5, "ram_available_mb": 100843.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24929.4, "ram_available_mb": 100842.7, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 4854.7, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.51, "peak": 15.62, "min": 15.11}, "VDD_CPU_SOC_MSS": {"avg": 16.67, "peak": 16.93, "min": 16.14}, "VDD_GPU": {"avg": 21.49, "peak": 24.03, "min": 20.09}, "VIN": {"avg": 63.9, "peak": 67.31, "min": 61.69}}, "power_watts_avg": 21.49, "energy_joules_est": 26.92, "sample_count": 9, "duration_seconds": 1.253}, "timestamp": "2026-01-17T15:54:15.831268"}
{"image_index": 421, "image_name": "000000047801.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047801.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1158.041, "latencies_ms": [1158.041], "images_per_second": 0.864, "prompt_tokens": 19, "response_tokens_est": 50, "n_tiles": 1, "output_text": "The notable visual attributes of the image include a person with a bald head wearing glasses, a blue t-shirt, and holding a plate with a slice of cake. The background features lush green trees under a clear blue sky, indicating a sunny day.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 24929.4, "ram_available_mb": 100842.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24930.2, "ram_available_mb": 100842.0, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 4853.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.55, "peak": 15.72, "min": 15.22}, "VDD_CPU_SOC_MSS": {"avg": 16.54, "peak": 16.93, "min": 16.14}, "VDD_GPU": {"avg": 21.72, "peak": 24.04, "min": 20.49}, "VIN": {"avg": 63.42, "peak": 68.02, "min": 61.14}}, "power_watts_avg": 21.72, "energy_joules_est": 25.17, "sample_count": 8, "duration_seconds": 1.159}, "timestamp": "2026-01-17T15:54:16.995563"}
{"image_index": 422, "image_name": "000000047819.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047819.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2252.479, "latencies_ms": [2252.479], "images_per_second": 0.444, "prompt_tokens": 9, "response_tokens_est": 19, "n_tiles": 12, "output_text": "A man is standing next to a brown horse, holding a basket filled with various items.", "error": null, "sys_before": {"cpu_percent": 42.7, "ram_used_mb": 24926.0, "ram_available_mb": 100846.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.1, "ram_used_mb": 24925.4, "ram_available_mb": 100846.7, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.3, "peak": 16.76, "min": 14.0}, "VDD_CPU_SOC_MSS": {"avg": 16.14, "peak": 17.71, "min": 14.96}, "VDD_GPU": {"avg": 36.94, "peak": 43.33, "min": 27.59}, "VIN": {"avg": 78.0, "peak": 108.61, "min": 65.41}}, "power_watts_avg": 36.94, "energy_joules_est": 83.22, "sample_count": 17, "duration_seconds": 2.253}, "timestamp": "2026-01-17T15:54:19.336633"}
{"image_index": 422, "image_name": "000000047819.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047819.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2956.192, "latencies_ms": [2956.192], "images_per_second": 0.338, "prompt_tokens": 23, "response_tokens_est": 38, "n_tiles": 12, "output_text": "1. Man\n2. Shelter\n3. Tree\n4. Shelter\n5. Shelter\n6. Shelter\n7. Shelter\n8. Shelter", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 24925.4, "ram_available_mb": 100846.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 24920.4, "ram_available_mb": 100851.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.25, "peak": 16.66, "min": 14.12}, "VDD_CPU_SOC_MSS": {"avg": 15.85, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 35.95, "peak": 45.69, "min": 26.79}, "VIN": {"avg": 79.14, "peak": 125.82, "min": 67.73}}, "power_watts_avg": 35.95, "energy_joules_est": 106.29, "sample_count": 23, "duration_seconds": 2.957}, "timestamp": "2026-01-17T15:54:22.302546"}
{"image_index": 422, "image_name": "000000047819.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047819.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3489.125, "latencies_ms": [3489.125], "images_per_second": 0.287, "prompt_tokens": 27, "response_tokens_est": 54, "n_tiles": 12, "output_text": "The main objects in the image are a man and a brown horse. The man is positioned in the foreground, standing near the horse. The horse is in the background, slightly to the right. The man is near the horse, and the horse is further away.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24920.4, "ram_available_mb": 100851.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 24921.0, "ram_available_mb": 100851.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.23, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.73, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 34.4, "peak": 44.89, "min": 26.39}, "VIN": {"avg": 76.29, "peak": 106.3, "min": 65.2}}, "power_watts_avg": 34.4, "energy_joules_est": 120.04, "sample_count": 27, "duration_seconds": 3.49}, "timestamp": "2026-01-17T15:54:25.798741"}
{"image_index": 422, "image_name": "000000047819.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047819.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3871.813, "latencies_ms": [3871.813], "images_per_second": 0.258, "prompt_tokens": 21, "response_tokens_est": 65, "n_tiles": 12, "output_text": "The image depicts a man standing next to a brown horse in a natural, outdoor setting. The man is wearing a purple vest over a white shirt and gray pants, and he appears to be holding a basket filled with various items. The horse is adorned with a colorful blanket and is being led by the man.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24921.0, "ram_available_mb": 100851.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 24921.7, "ram_available_mb": 100850.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.19, "peak": 16.56, "min": 14.12}, "VDD_CPU_SOC_MSS": {"avg": 15.72, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 33.43, "peak": 45.69, "min": 26.0}, "VIN": {"avg": 74.21, "peak": 110.91, "min": 66.46}}, "power_watts_avg": 33.43, "energy_joules_est": 129.46, "sample_count": 30, "duration_seconds": 3.872}, "timestamp": "2026-01-17T15:54:29.677966"}
{"image_index": 422, "image_name": "000000047819.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047819.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3260.745, "latencies_ms": [3260.745], "images_per_second": 0.307, "prompt_tokens": 19, "response_tokens_est": 47, "n_tiles": 12, "output_text": "The man is wearing a purple vest over a white shirt, gray pants, and brown shoes. The sunlight casts a warm glow on the scene, highlighting the textures of the man's clothing and the earthy tones of the surroundings.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24921.7, "ram_available_mb": 100850.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 24922.5, "ram_available_mb": 100849.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.21, "peak": 16.66, "min": 14.12}, "VDD_CPU_SOC_MSS": {"avg": 15.7, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 34.91, "peak": 45.68, "min": 26.39}, "VIN": {"avg": 75.73, "peak": 109.91, "min": 62.36}}, "power_watts_avg": 34.91, "energy_joules_est": 113.86, "sample_count": 25, "duration_seconds": 3.261}, "timestamp": "2026-01-17T15:54:32.945700"}
{"image_index": 423, "image_name": "000000047828.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047828.jpg", "image_width": 640, "image_height": 318, "image_resolution": "640x318", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1950.509, "latencies_ms": [1950.509], "images_per_second": 0.513, "prompt_tokens": 9, "response_tokens_est": 78, "n_tiles": 2, "output_text": "The image depicts a night scene featuring a beautifully illuminated bridge with a series of arches, reflecting the blue lights on the water below. The bridge is adorned with a series of lights that create a mesmerizing and serene atmosphere, while a group of people can be seen walking along the bridge's walkway, adding a sense of scale and life to the scene.", "error": null, "sys_before": {"cpu_percent": 40.9, "ram_used_mb": 24926.7, "ram_available_mb": 100845.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24927.7, "ram_available_mb": 100844.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5055.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.05, "peak": 15.42, "min": 14.2}, "VDD_CPU_SOC_MSS": {"avg": 15.93, "peak": 16.54, "min": 14.57}, "VDD_GPU": {"avg": 23.96, "peak": 33.48, "min": 20.88}, "VIN": {"avg": 65.6, "peak": 85.58, "min": 61.89}}, "power_watts_avg": 23.96, "energy_joules_est": 46.74, "sample_count": 15, "duration_seconds": 1.951}, "timestamp": "2026-01-17T15:54:34.927584"}
{"image_index": 423, "image_name": "000000047828.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047828.jpg", "image_width": 640, "image_height": 318, "image_resolution": "640x318", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1206.232, "latencies_ms": [1206.232], "images_per_second": 0.829, "prompt_tokens": 23, "response_tokens_est": 45, "n_tiles": 2, "output_text": "object: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24927.7, "ram_available_mb": 100844.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 24929.2, "ram_available_mb": 100843.0, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5064.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.14, "peak": 15.32, "min": 14.81}, "VDD_CPU_SOC_MSS": {"avg": 15.97, "peak": 16.15, "min": 15.75}, "VDD_GPU": {"avg": 24.69, "peak": 30.33, "min": 21.67}, "VIN": {"avg": 65.46, "peak": 95.08, "min": 56.45}}, "power_watts_avg": 24.69, "energy_joules_est": 29.79, "sample_count": 9, "duration_seconds": 1.207}, "timestamp": "2026-01-17T15:54:36.139921"}
{"image_index": 423, "image_name": "000000047828.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047828.jpg", "image_width": 640, "image_height": 318, "image_resolution": "640x318", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2341.565, "latencies_ms": [2341.565], "images_per_second": 0.427, "prompt_tokens": 27, "response_tokens_est": 95, "n_tiles": 2, "output_text": "The main objects in the image are a group of people on the left side, a large bridge in the background, and a body of water on the right side. The bridge is the most prominent object, extending across the water and illuminated with blue lights. The people are situated on the left side of the bridge, near the water's edge. The water is the central feature, reflecting the bridge and the lights, and the people are near the water's edge.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24929.2, "ram_available_mb": 100843.0, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 24929.2, "ram_available_mb": 100843.0, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5067.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.28, "peak": 15.62, "min": 14.51}, "VDD_CPU_SOC_MSS": {"avg": 16.03, "peak": 16.54, "min": 15.35}, "VDD_GPU": {"avg": 23.03, "peak": 30.73, "min": 20.88}, "VIN": {"avg": 63.87, "peak": 97.84, "min": 57.22}}, "power_watts_avg": 23.03, "energy_joules_est": 53.94, "sample_count": 18, "duration_seconds": 2.342}, "timestamp": "2026-01-17T15:54:38.486934"}
{"image_index": 423, "image_name": "000000047828.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047828.jpg", "image_width": 640, "image_height": 318, "image_resolution": "640x318", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1448.025, "latencies_ms": [1448.025], "images_per_second": 0.691, "prompt_tokens": 21, "response_tokens_est": 56, "n_tiles": 2, "output_text": "The image depicts a nighttime scene of a large, illuminated bridge spanning a body of water. The bridge is adorned with blue lights, creating a striking reflection on the water's surface. People are walking along the bridge's walkway, enjoying the scenic view.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24929.2, "ram_available_mb": 100843.0, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 24929.9, "ram_available_mb": 100842.3, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5063.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.01, "peak": 15.22, "min": 14.71}, "VDD_CPU_SOC_MSS": {"avg": 16.18, "peak": 16.54, "min": 15.75}, "VDD_GPU": {"avg": 24.18, "peak": 29.54, "min": 21.28}, "VIN": {"avg": 65.18, "peak": 95.15, "min": 54.57}}, "power_watts_avg": 24.18, "energy_joules_est": 35.03, "sample_count": 11, "duration_seconds": 1.449}, "timestamp": "2026-01-17T15:54:39.941434"}
{"image_index": 423, "image_name": "000000047828.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047828.jpg", "image_width": 640, "image_height": 318, "image_resolution": "640x318", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1694.75, "latencies_ms": [1694.75], "images_per_second": 0.59, "prompt_tokens": 19, "response_tokens_est": 67, "n_tiles": 2, "output_text": "The image showcases a nighttime scene featuring a strikingly illuminated bridge with a series of arches. The bridge is adorned with blue lights, creating a mesmerizing reflection on the calm water below. The sky is dark, indicating it is nighttime, and the overall atmosphere is serene and captivating.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24929.9, "ram_available_mb": 100842.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24929.9, "ram_available_mb": 100842.3, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5062.6, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.11, "peak": 15.32, "min": 14.51}, "VDD_CPU_SOC_MSS": {"avg": 16.08, "peak": 16.54, "min": 15.36}, "VDD_GPU": {"avg": 23.67, "peak": 30.74, "min": 20.89}, "VIN": {"avg": 66.23, "peak": 93.71, "min": 58.46}}, "power_watts_avg": 23.67, "energy_joules_est": 40.12, "sample_count": 13, "duration_seconds": 1.695}, "timestamp": "2026-01-17T15:54:41.643128"}
{"image_index": 424, "image_name": "000000048153.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000048153.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 3034.808, "latencies_ms": [3034.808], "images_per_second": 0.33, "prompt_tokens": 9, "response_tokens_est": 42, "n_tiles": 12, "output_text": "The image shows a close-up of a person's lower torso and legs, wearing blue denim jeans, with a pink, shiny object on their back, possibly a bag or a piece of clothing.", "error": null, "sys_before": {"cpu_percent": 44.0, "ram_used_mb": 24930.5, "ram_available_mb": 100841.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 24930.6, "ram_available_mb": 100841.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.22, "peak": 16.66, "min": 13.79}, "VDD_CPU_SOC_MSS": {"avg": 15.92, "peak": 17.71, "min": 14.96}, "VDD_GPU": {"avg": 34.39, "peak": 43.33, "min": 26.39}, "VIN": {"avg": 81.18, "peak": 125.71, "min": 63.42}}, "power_watts_avg": 34.39, "energy_joules_est": 104.38, "sample_count": 23, "duration_seconds": 3.035}, "timestamp": "2026-01-17T15:54:44.755174"}
{"image_index": 424, "image_name": "000000048153.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000048153.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 3026.873, "latencies_ms": [3026.873], "images_per_second": 0.33, "prompt_tokens": 23, "response_tokens_est": 40, "n_tiles": 12, "output_text": "1. Pink chair\n2. Blue chair\n3. Blue blanket\n4. Blue table\n5. Blue floor\n6. Green floor\n7. Pink object\n8. Pink object", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24930.6, "ram_available_mb": 100841.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 24925.8, "ram_available_mb": 100846.3, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.23, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.76, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 35.7, "peak": 44.89, "min": 26.39}, "VIN": {"avg": 78.69, "peak": 113.08, "min": 67.34}}, "power_watts_avg": 35.7, "energy_joules_est": 108.07, "sample_count": 23, "duration_seconds": 3.027}, "timestamp": "2026-01-17T15:54:47.788768"}
{"image_index": 424, "image_name": "000000048153.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000048153.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3744.117, "latencies_ms": [3744.117], "images_per_second": 0.267, "prompt_tokens": 27, "response_tokens_est": 61, "n_tiles": 12, "output_text": "The main object in the foreground is a blue denim garment, which appears to be a pair of jeans. The background features a colorful, painted surface with a map of Europe and parts of Africa visible. The pink object, likely a chair or bench, is situated near the blue denim garment.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24925.8, "ram_available_mb": 100846.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 24926.2, "ram_available_mb": 100846.0, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.19, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.64, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 33.79, "peak": 44.89, "min": 26.01}, "VIN": {"avg": 75.73, "peak": 108.83, "min": 60.95}}, "power_watts_avg": 33.79, "energy_joules_est": 126.53, "sample_count": 29, "duration_seconds": 3.745}, "timestamp": "2026-01-17T15:54:51.539733"}
{"image_index": 424, "image_name": "000000048153.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000048153.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 4013.394, "latencies_ms": [4013.394], "images_per_second": 0.249, "prompt_tokens": 21, "response_tokens_est": 69, "n_tiles": 12, "output_text": "The image depicts a scene with a person sitting on a bench, which is painted in a colorful, abstract design. The person is wearing a blue denim garment, and the bench has a vibrant blue and green pattern. The setting appears to be outdoors, possibly in a park or public area, with the bench being a focal point.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24926.2, "ram_available_mb": 100846.0, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 24926.6, "ram_available_mb": 100845.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.18, "peak": 16.56, "min": 14.12}, "VDD_CPU_SOC_MSS": {"avg": 15.63, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 33.19, "peak": 45.69, "min": 26.0}, "VIN": {"avg": 73.61, "peak": 104.97, "min": 57.3}}, "power_watts_avg": 33.19, "energy_joules_est": 133.22, "sample_count": 31, "duration_seconds": 4.014}, "timestamp": "2026-01-17T15:54:55.559737"}
{"image_index": 424, "image_name": "000000048153.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000048153.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3569.449, "latencies_ms": [3569.449], "images_per_second": 0.28, "prompt_tokens": 19, "response_tokens_est": 56, "n_tiles": 12, "output_text": "The image features a vividly colored bench with a blue and green painted surface. The bench is adorned with a bright pink cushion, adding a pop of color to the scene. The lighting is bright, casting a clear view of the bench's details and the surrounding area.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24926.6, "ram_available_mb": 100845.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 24927.2, "ram_available_mb": 100845.0, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.2, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.67, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 34.14, "peak": 45.28, "min": 26.0}, "VIN": {"avg": 80.55, "peak": 118.91, "min": 65.07}}, "power_watts_avg": 34.14, "energy_joules_est": 121.88, "sample_count": 27, "duration_seconds": 3.57}, "timestamp": "2026-01-17T15:54:59.136256"}
{"image_index": 425, "image_name": "000000048396.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000048396.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1916.346, "latencies_ms": [1916.346], "images_per_second": 0.522, "prompt_tokens": 9, "response_tokens_est": 47, "n_tiles": 6, "output_text": "A woman with a smile is holding a knife, standing in front of a red wall, while a young boy with a cheerful expression is sitting at a table, with a red cake and a white cup in front of him.", "error": null, "sys_before": {"cpu_percent": 30.7, "ram_used_mb": 24953.7, "ram_available_mb": 100818.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 24953.8, "ram_available_mb": 100818.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.71, "peak": 15.14, "min": 13.82}, "VDD_CPU_SOC_MSS": {"avg": 15.44, "peak": 16.14, "min": 14.17}, "VDD_GPU": {"avg": 31.37, "peak": 40.95, "min": 24.03}, "VIN": {"avg": 70.74, "peak": 101.17, "min": 54.67}}, "power_watts_avg": 31.37, "energy_joules_est": 60.13, "sample_count": 14, "duration_seconds": 1.917}, "timestamp": "2026-01-17T15:55:01.115637"}
{"image_index": 425, "image_name": "000000048396.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000048396.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1827.944, "latencies_ms": [1827.944], "images_per_second": 0.547, "prompt_tokens": 23, "response_tokens_est": 42, "n_tiles": 6, "output_text": "- Woman: 1\n- Knife: 1\n- Man: 1\n- Cake: 1\n- Table: 1\n- Cup: 1\n- Child: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24953.8, "ram_available_mb": 100818.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 24954.3, "ram_available_mb": 100817.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.83, "peak": 15.24, "min": 14.3}, "VDD_CPU_SOC_MSS": {"avg": 15.41, "peak": 15.75, "min": 14.56}, "VDD_GPU": {"avg": 31.68, "peak": 41.34, "min": 24.03}, "VIN": {"avg": 73.28, "peak": 113.54, "min": 62.84}}, "power_watts_avg": 31.68, "energy_joules_est": 57.92, "sample_count": 14, "duration_seconds": 1.828}, "timestamp": "2026-01-17T15:55:02.949990"}
{"image_index": 425, "image_name": "000000048396.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000048396.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1941.119, "latencies_ms": [1941.119], "images_per_second": 0.515, "prompt_tokens": 27, "response_tokens_est": 46, "n_tiles": 6, "output_text": "The main objects in the image are a woman and a young boy. The woman is in the foreground, standing next to a table with a red cake. The young boy is in the background, standing near a doorway.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24954.3, "ram_available_mb": 100817.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 24955.5, "ram_available_mb": 100816.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.85, "peak": 15.24, "min": 14.3}, "VDD_CPU_SOC_MSS": {"avg": 15.49, "peak": 15.75, "min": 14.56}, "VDD_GPU": {"avg": 30.96, "peak": 40.56, "min": 23.64}, "VIN": {"avg": 69.13, "peak": 100.18, "min": 58.64}}, "power_watts_avg": 30.96, "energy_joules_est": 60.12, "sample_count": 15, "duration_seconds": 1.942}, "timestamp": "2026-01-17T15:55:04.897836"}
{"image_index": 425, "image_name": "000000048396.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000048396.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3331.615, "latencies_ms": [3331.615], "images_per_second": 0.3, "prompt_tokens": 21, "response_tokens_est": 95, "n_tiles": 6, "output_text": "The scene is set in a cozy, indoor environment, likely a home or a small gathering space. A woman and a young boy are present, with the woman smiling and holding a knife, while the boy is looking at the camera with a slight smile. The woman is wearing a green cardigan over a white top, and the boy is dressed in a blue and white plaid shirt. There is a cake on the table, and a white cup is also visible.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24955.5, "ram_available_mb": 100816.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 24955.5, "ram_available_mb": 100816.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.89, "peak": 15.24, "min": 14.3}, "VDD_CPU_SOC_MSS": {"avg": 15.52, "peak": 15.75, "min": 14.56}, "VDD_GPU": {"avg": 27.64, "peak": 40.97, "min": 22.85}, "VIN": {"avg": 68.74, "peak": 117.17, "min": 58.59}}, "power_watts_avg": 27.64, "energy_joules_est": 92.1, "sample_count": 26, "duration_seconds": 3.332}, "timestamp": "2026-01-17T15:55:08.235834"}
{"image_index": 425, "image_name": "000000048396.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000048396.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2420.849, "latencies_ms": [2420.849], "images_per_second": 0.413, "prompt_tokens": 19, "response_tokens_est": 63, "n_tiles": 6, "output_text": "The image features a woman with long brown hair, wearing a light green cardigan over a white top, standing in front of a red wall. She is smiling and holding a knife in her right hand. The lighting is warm, and the background includes a wooden table with a red cake and a white cup.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24955.5, "ram_available_mb": 100816.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 24956.5, "ram_available_mb": 100815.7, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.84, "peak": 15.24, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.44, "peak": 15.75, "min": 14.57}, "VDD_GPU": {"avg": 29.5, "peak": 40.56, "min": 23.24}, "VIN": {"avg": 73.65, "peak": 113.03, "min": 60.71}}, "power_watts_avg": 29.5, "energy_joules_est": 71.42, "sample_count": 18, "duration_seconds": 2.421}, "timestamp": "2026-01-17T15:55:10.663074"}
{"image_index": 426, "image_name": "000000048504.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000048504.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1199.356, "latencies_ms": [1199.356], "images_per_second": 0.834, "prompt_tokens": 9, "response_tokens_est": 22, "n_tiles": 6, "output_text": "A woman is seen riding an elephant in a large arena, with the elephant being led by another person.", "error": null, "sys_before": {"cpu_percent": 38.9, "ram_used_mb": 24956.5, "ram_available_mb": 100815.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 24956.5, "ram_available_mb": 100815.7, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.55, "peak": 15.04, "min": 13.69}, "VDD_CPU_SOC_MSS": {"avg": 15.22, "peak": 15.75, "min": 14.17}, "VDD_GPU": {"avg": 34.09, "peak": 39.77, "min": 27.97}, "VIN": {"avg": 77.41, "peak": 106.59, "min": 61.59}}, "power_watts_avg": 34.09, "energy_joules_est": 40.9, "sample_count": 9, "duration_seconds": 1.2}, "timestamp": "2026-01-17T15:55:11.922065"}
{"image_index": 426, "image_name": "000000048504.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000048504.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2421.605, "latencies_ms": [2421.605], "images_per_second": 0.413, "prompt_tokens": 23, "response_tokens_est": 63, "n_tiles": 6, "output_text": "1. Elephant: 1\n2. Person: 1\n3. Rope: 1\n4. Stirrups: 1\n5. Ramp: 1\n6. Platform: 1\n7. Floor: 1\n8. Barrier: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24956.5, "ram_available_mb": 100815.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 24957.0, "ram_available_mb": 100815.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.88, "peak": 15.24, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.5, "peak": 15.75, "min": 14.56}, "VDD_GPU": {"avg": 29.98, "peak": 41.76, "min": 23.25}, "VIN": {"avg": 75.2, "peak": 114.56, "min": 60.72}}, "power_watts_avg": 29.98, "energy_joules_est": 72.62, "sample_count": 19, "duration_seconds": 2.422}, "timestamp": "2026-01-17T15:55:14.350430"}
{"image_index": 426, "image_name": "000000048504.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000048504.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3158.07, "latencies_ms": [3158.07], "images_per_second": 0.317, "prompt_tokens": 27, "response_tokens_est": 89, "n_tiles": 6, "output_text": "In the image, the main focus is on the two elephants in the foreground, with one elephant being led by a person in a red vest. The person is guiding the elephant towards the right side of the frame. The background features a large arena with a concrete floor, metal railings, and a person standing near the left side of the frame. The arena is illuminated by artificial lighting, creating a bright and well-lit environment.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24957.0, "ram_available_mb": 100815.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 24957.2, "ram_available_mb": 100814.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.9, "peak": 15.24, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.56, "peak": 15.75, "min": 14.56}, "VDD_GPU": {"avg": 27.78, "peak": 40.57, "min": 22.85}, "VIN": {"avg": 66.94, "peak": 93.79, "min": 57.09}}, "power_watts_avg": 27.78, "energy_joules_est": 87.74, "sample_count": 25, "duration_seconds": 3.158}, "timestamp": "2026-01-17T15:55:17.515696"}
{"image_index": 426, "image_name": "000000048504.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000048504.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2443.434, "latencies_ms": [2443.434], "images_per_second": 0.409, "prompt_tokens": 21, "response_tokens_est": 64, "n_tiles": 6, "output_text": "The image depicts an indoor arena with a dirt-covered floor, where a man is seen standing near a large elephant, likely preparing it for a performance or event. Another person, dressed in a red and black outfit, is seen interacting with the elephant, possibly guiding or preparing it for a ride or show.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24957.2, "ram_available_mb": 100814.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24957.2, "ram_available_mb": 100814.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.88, "peak": 15.24, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.6, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 29.25, "peak": 40.16, "min": 23.25}, "VIN": {"avg": 73.72, "peak": 114.91, "min": 61.52}}, "power_watts_avg": 29.25, "energy_joules_est": 71.48, "sample_count": 19, "duration_seconds": 2.444}, "timestamp": "2026-01-17T15:55:19.965906"}
{"image_index": 426, "image_name": "000000048504.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000048504.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2134.466, "latencies_ms": [2134.466], "images_per_second": 0.469, "prompt_tokens": 19, "response_tokens_est": 53, "n_tiles": 6, "output_text": "The image depicts an indoor arena with a dirt floor and metal railings. The lighting is artificial, with spotlights illuminating the scene. The arena is adorned with a colorful, geometric-patterned floor and a red and yellow striped barrier.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24957.2, "ram_available_mb": 100814.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 24957.7, "ram_available_mb": 100814.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.87, "peak": 15.24, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.5, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 30.31, "peak": 40.16, "min": 23.25}, "VIN": {"avg": 70.9, "peak": 104.69, "min": 55.06}}, "power_watts_avg": 30.31, "energy_joules_est": 64.71, "sample_count": 16, "duration_seconds": 2.135}, "timestamp": "2026-01-17T15:55:22.106927"}
{"image_index": 427, "image_name": "000000048555.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000048555.jpg", "image_width": 640, "image_height": 465, "image_resolution": "640x465", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2805.958, "latencies_ms": [2805.958], "images_per_second": 0.356, "prompt_tokens": 9, "response_tokens_est": 35, "n_tiles": 12, "output_text": "A group of jockeys on horseback are racing across a sandy beach, with the horses galloping vigorously and the riders leaning forward in concentration.", "error": null, "sys_before": {"cpu_percent": 43.8, "ram_used_mb": 24917.9, "ram_available_mb": 100854.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 24919.1, "ram_available_mb": 100853.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.14, "peak": 16.66, "min": 13.39}, "VDD_CPU_SOC_MSS": {"avg": 15.88, "peak": 17.71, "min": 14.18}, "VDD_GPU": {"avg": 35.15, "peak": 43.73, "min": 26.79}, "VIN": {"avg": 82.06, "peak": 127.52, "min": 67.36}}, "power_watts_avg": 35.15, "energy_joules_est": 98.65, "sample_count": 21, "duration_seconds": 2.806}, "timestamp": "2026-01-17T15:55:25.010944"}
{"image_index": 427, "image_name": "000000048555.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000048555.jpg", "image_width": 640, "image_height": 465, "image_resolution": "640x465", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2754.651, "latencies_ms": [2754.651], "images_per_second": 0.363, "prompt_tokens": 23, "response_tokens_est": 32, "n_tiles": 12, "output_text": "1. Horse\n2. Rider\n3. Horse\n4. Rider\n5. Horse\n6. Rider\n7. Horse\n8. Rider", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24919.1, "ram_available_mb": 100853.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 24919.6, "ram_available_mb": 100852.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.27, "peak": 16.56, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.88, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 36.55, "peak": 44.89, "min": 27.18}, "VIN": {"avg": 80.85, "peak": 113.76, "min": 65.56}}, "power_watts_avg": 36.55, "energy_joules_est": 100.71, "sample_count": 20, "duration_seconds": 2.755}, "timestamp": "2026-01-17T15:55:27.776979"}
{"image_index": 427, "image_name": "000000048555.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000048555.jpg", "image_width": 640, "image_height": 465, "image_resolution": "640x465", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3379.145, "latencies_ms": [3379.145], "images_per_second": 0.296, "prompt_tokens": 27, "response_tokens_est": 50, "n_tiles": 12, "output_text": "The main objects in the image are the horses and the riders. The horses are in the foreground, while the riders are positioned slightly behind them. The riders are near the center of the image, with the horses positioned to the left and right.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24919.6, "ram_available_mb": 100852.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 24920.8, "ram_available_mb": 100851.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.2, "peak": 16.66, "min": 14.12}, "VDD_CPU_SOC_MSS": {"avg": 15.73, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 34.52, "peak": 45.3, "min": 26.0}, "VIN": {"avg": 81.32, "peak": 134.53, "min": 59.1}}, "power_watts_avg": 34.52, "energy_joules_est": 116.66, "sample_count": 26, "duration_seconds": 3.379}, "timestamp": "2026-01-17T15:55:31.162589"}
{"image_index": 427, "image_name": "000000048555.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000048555.jpg", "image_width": 640, "image_height": 465, "image_resolution": "640x465", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3464.968, "latencies_ms": [3464.968], "images_per_second": 0.289, "prompt_tokens": 21, "response_tokens_est": 53, "n_tiles": 12, "output_text": "The image depicts a horse race in progress, with a group of jockeys on horseback racing towards the finish line. The scene is set on a track, and the horses and riders are captured in motion, creating a dynamic and energetic atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24920.8, "ram_available_mb": 100851.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 24921.8, "ram_available_mb": 100850.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.24, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.79, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 34.34, "peak": 44.89, "min": 26.0}, "VIN": {"avg": 81.42, "peak": 138.98, "min": 63.99}}, "power_watts_avg": 34.34, "energy_joules_est": 119.01, "sample_count": 26, "duration_seconds": 3.466}, "timestamp": "2026-01-17T15:55:34.635895"}
{"image_index": 427, "image_name": "000000048555.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000048555.jpg", "image_width": 640, "image_height": 465, "image_resolution": "640x465", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3434.649, "latencies_ms": [3434.649], "images_per_second": 0.291, "prompt_tokens": 19, "response_tokens_est": 52, "n_tiles": 12, "output_text": "The image is a black and white photograph featuring a horse race. The horses and jockeys are in motion, creating a blur effect in the background. The lighting is even, with no harsh shadows, and the overall atmosphere is dynamic and energetic.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24921.8, "ram_available_mb": 100850.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 24921.8, "ram_available_mb": 100850.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.22, "peak": 16.56, "min": 14.12}, "VDD_CPU_SOC_MSS": {"avg": 15.78, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 34.4, "peak": 44.49, "min": 26.0}, "VIN": {"avg": 72.49, "peak": 104.74, "min": 53.83}}, "power_watts_avg": 34.4, "energy_joules_est": 118.17, "sample_count": 26, "duration_seconds": 3.435}, "timestamp": "2026-01-17T15:55:38.077287"}
{"image_index": 428, "image_name": "000000048564.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000048564.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2108.627, "latencies_ms": [2108.627], "images_per_second": 0.474, "prompt_tokens": 9, "response_tokens_est": 56, "n_tiles": 6, "output_text": "A man is standing outdoors in a snowy environment, wearing a black beanie and a black jacket with a blue scarf around his neck. He is holding a phone to his ear with his left hand, and his facial expression appears to be one of surprise or shock.", "error": null, "sys_before": {"cpu_percent": 36.8, "ram_used_mb": 24950.6, "ram_available_mb": 100821.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 24951.1, "ram_available_mb": 100821.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.84, "peak": 15.22, "min": 13.72}, "VDD_CPU_SOC_MSS": {"avg": 15.45, "peak": 16.14, "min": 14.17}, "VDD_GPU": {"avg": 30.48, "peak": 40.56, "min": 24.03}, "VIN": {"avg": 72.07, "peak": 110.23, "min": 60.13}}, "power_watts_avg": 30.48, "energy_joules_est": 64.28, "sample_count": 16, "duration_seconds": 2.109}, "timestamp": "2026-01-17T15:55:40.253208"}
{"image_index": 428, "image_name": "000000048564.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000048564.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2223.945, "latencies_ms": [2223.945], "images_per_second": 0.45, "prompt_tokens": 23, "response_tokens_est": 58, "n_tiles": 6, "output_text": "1. Snowboarder\n2. Snowboard\n3. Snow\n4. Snowboarder's helmet\n5. Snowboarder's goggles\n6. Snowboarder's jacket\n7. Snowboarder's beanie\n8. Snowboarder's scarf", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24951.1, "ram_available_mb": 100821.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 24952.1, "ram_available_mb": 100820.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.05, "peak": 15.54, "min": 14.43}, "VDD_CPU_SOC_MSS": {"avg": 15.66, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 30.26, "peak": 40.16, "min": 23.64}, "VIN": {"avg": 73.22, "peak": 115.39, "min": 57.59}}, "power_watts_avg": 30.26, "energy_joules_est": 67.31, "sample_count": 17, "duration_seconds": 2.224}, "timestamp": "2026-01-17T15:55:42.483530"}
{"image_index": 428, "image_name": "000000048564.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000048564.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1921.082, "latencies_ms": [1921.082], "images_per_second": 0.521, "prompt_tokens": 27, "response_tokens_est": 47, "n_tiles": 6, "output_text": "The main object in the foreground is a person wearing a black jacket and a blue scarf. The person is holding a phone to their ear. In the background, there is a snowy landscape with trees and a house visible.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24952.1, "ram_available_mb": 100820.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 24953.1, "ram_available_mb": 100819.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.03, "peak": 15.54, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.66, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 31.57, "peak": 40.18, "min": 24.42}, "VIN": {"avg": 73.0, "peak": 106.09, "min": 61.13}}, "power_watts_avg": 31.57, "energy_joules_est": 60.66, "sample_count": 14, "duration_seconds": 1.921}, "timestamp": "2026-01-17T15:55:44.409936"}
{"image_index": 428, "image_name": "000000048564.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000048564.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2435.884, "latencies_ms": [2435.884], "images_per_second": 0.411, "prompt_tokens": 21, "response_tokens_est": 66, "n_tiles": 6, "output_text": "The image depicts a man in a winter setting, standing outdoors with snow on the ground. He is wearing a black jacket and a beanie, and appears to be in the middle of a conversation on his phone. The background shows a snowy landscape with trees and a building, suggesting a cold, wintry environment.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24953.1, "ram_available_mb": 100819.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 24954.3, "ram_available_mb": 100817.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.08, "peak": 15.44, "min": 14.43}, "VDD_CPU_SOC_MSS": {"avg": 15.66, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 29.83, "peak": 40.16, "min": 23.64}, "VIN": {"avg": 74.71, "peak": 121.94, "min": 55.51}}, "power_watts_avg": 29.83, "energy_joules_est": 72.68, "sample_count": 18, "duration_seconds": 2.437}, "timestamp": "2026-01-17T15:55:46.852364"}
{"image_index": 428, "image_name": "000000048564.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000048564.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1863.278, "latencies_ms": [1863.278], "images_per_second": 0.537, "prompt_tokens": 19, "response_tokens_est": 45, "n_tiles": 6, "output_text": "The notable visual attributes in the image include a person wearing a black beanie and a black jacket with a green button on the left side. The lighting is bright, and the person is outdoors in a snowy environment.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24954.3, "ram_available_mb": 100817.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 24955.3, "ram_available_mb": 100816.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.04, "peak": 15.54, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.61, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 31.46, "peak": 40.16, "min": 24.42}, "VIN": {"avg": 72.04, "peak": 113.15, "min": 51.21}}, "power_watts_avg": 31.46, "energy_joules_est": 58.63, "sample_count": 14, "duration_seconds": 1.864}, "timestamp": "2026-01-17T15:55:48.722091"}
{"image_index": 429, "image_name": "000000048924.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000048924.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1094.962, "latencies_ms": [1094.962], "images_per_second": 0.913, "prompt_tokens": 9, "response_tokens_est": 18, "n_tiles": 6, "output_text": "A motorcycle is parked next to a green tent in a forested area during sunset.", "error": null, "sys_before": {"cpu_percent": 42.7, "ram_used_mb": 24955.3, "ram_available_mb": 100816.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 24955.3, "ram_available_mb": 100816.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.56, "peak": 15.14, "min": 13.79}, "VDD_CPU_SOC_MSS": {"avg": 15.3, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 35.3, "peak": 40.16, "min": 29.54}, "VIN": {"avg": 79.06, "peak": 115.33, "min": 62.02}}, "power_watts_avg": 35.3, "energy_joules_est": 38.67, "sample_count": 8, "duration_seconds": 1.095}, "timestamp": "2026-01-17T15:55:49.878858"}
{"image_index": 429, "image_name": "000000048924.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000048924.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1623.934, "latencies_ms": [1623.934], "images_per_second": 0.616, "prompt_tokens": 23, "response_tokens_est": 35, "n_tiles": 6, "output_text": "1. Motorcycle\n2. Tent\n3. Bushes\n4. Trees\n5. Sun\n6. Sky\n7. Grass\n8. Water", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24955.3, "ram_available_mb": 100816.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 24955.3, "ram_available_mb": 100816.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.85, "peak": 15.34, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.35, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 33.54, "peak": 42.13, "min": 24.83}, "VIN": {"avg": 72.48, "peak": 99.68, "min": 60.41}}, "power_watts_avg": 33.54, "energy_joules_est": 54.48, "sample_count": 12, "duration_seconds": 1.624}, "timestamp": "2026-01-17T15:55:51.509260"}
{"image_index": 429, "image_name": "000000048924.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000048924.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2428.702, "latencies_ms": [2428.702], "images_per_second": 0.412, "prompt_tokens": 27, "response_tokens_est": 63, "n_tiles": 6, "output_text": "The main object in the foreground is a motorcycle, which is positioned to the right side of the image. The motorcycle is parked on a patch of dry grass. In the background, there is a green tent, which is located to the left of the motorcycle. The tent is partially obscured by the motorcycle.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24955.3, "ram_available_mb": 100816.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 24956.5, "ram_available_mb": 100815.7, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.87, "peak": 15.24, "min": 14.3}, "VDD_CPU_SOC_MSS": {"avg": 15.44, "peak": 15.75, "min": 14.56}, "VDD_GPU": {"avg": 29.5, "peak": 40.95, "min": 22.85}, "VIN": {"avg": 70.51, "peak": 115.39, "min": 60.85}}, "power_watts_avg": 29.5, "energy_joules_est": 71.66, "sample_count": 19, "duration_seconds": 2.429}, "timestamp": "2026-01-17T15:55:53.944511"}
{"image_index": 429, "image_name": "000000048924.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000048924.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2031.855, "latencies_ms": [2031.855], "images_per_second": 0.492, "prompt_tokens": 21, "response_tokens_est": 49, "n_tiles": 6, "output_text": "The image depicts a serene outdoor setting during twilight, with a green tent and a motorcycle parked on a grassy area. The sky is partially cloudy, and the sun is setting, casting a warm glow over the scene.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24956.5, "ram_available_mb": 100815.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 24956.8, "ram_available_mb": 100815.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.81, "peak": 15.24, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.43, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 30.78, "peak": 40.56, "min": 23.64}, "VIN": {"avg": 75.37, "peak": 115.84, "min": 60.81}}, "power_watts_avg": 30.78, "energy_joules_est": 62.55, "sample_count": 15, "duration_seconds": 2.032}, "timestamp": "2026-01-17T15:55:55.983230"}
{"image_index": 429, "image_name": "000000048924.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000048924.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2913.993, "latencies_ms": [2913.993], "images_per_second": 0.343, "prompt_tokens": 19, "response_tokens_est": 80, "n_tiles": 6, "output_text": "The image depicts a green tent set up in a natural, wooded area during sunset. The tent is illuminated by the warm, golden light of the setting sun, casting a soft glow on the surrounding foliage. The scene is characterized by the contrast between the vibrant green of the tent and the muted tones of the forest, as well as the gentle hues of the sky.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24956.8, "ram_available_mb": 100815.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 24957.0, "ram_available_mb": 100815.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.85, "peak": 15.24, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.35, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 28.36, "peak": 40.57, "min": 22.85}, "VIN": {"avg": 68.13, "peak": 85.03, "min": 63.22}}, "power_watts_avg": 28.36, "energy_joules_est": 82.65, "sample_count": 22, "duration_seconds": 2.914}, "timestamp": "2026-01-17T15:55:58.903609"}
{"image_index": 430, "image_name": "000000049060.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000049060.jpg", "image_width": 640, "image_height": 411, "image_resolution": "640x411", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1839.572, "latencies_ms": [1839.572], "images_per_second": 0.544, "prompt_tokens": 9, "response_tokens_est": 44, "n_tiles": 6, "output_text": "A vintage black and white photograph captures a scene at a train station where a train is arriving or departing, with passengers waiting on the platform, and a steam locomotive emitting smoke from its chimney.", "error": null, "sys_before": {"cpu_percent": 38.7, "ram_used_mb": 24956.7, "ram_available_mb": 100815.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 24956.7, "ram_available_mb": 100815.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.68, "peak": 15.14, "min": 13.69}, "VDD_CPU_SOC_MSS": {"avg": 15.44, "peak": 16.14, "min": 14.18}, "VDD_GPU": {"avg": 30.67, "peak": 40.18, "min": 23.64}, "VIN": {"avg": 73.97, "peak": 104.8, "min": 61.05}}, "power_watts_avg": 30.67, "energy_joules_est": 56.43, "sample_count": 14, "duration_seconds": 1.84}, "timestamp": "2026-01-17T15:56:00.807519"}
{"image_index": 430, "image_name": "000000049060.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000049060.jpg", "image_width": 640, "image_height": 411, "image_resolution": "640x411", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 4300.959, "latencies_ms": [4300.959], "images_per_second": 0.233, "prompt_tokens": 23, "response_tokens_est": 129, "n_tiles": 6, "output_text": "- Train: 1\n- Train Car: 1\n- Train Engine: 1\n- Train Carriage: 1\n- Train Car: 1\n- Train Car: 1\n- Train Car: 1\n- Train Car: 1\n- Train Car: 1\n- Train Car: 1\n- Train Car: 1\n- Train Car: 1\n- Train Car: 1\n- Train Car: 1\n- Train Car: 1\n- Train Car: 1\n- Train Car: 1\n- Train Car: 1\n- Train", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24956.7, "ram_available_mb": 100815.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 24957.0, "ram_available_mb": 100815.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.85, "peak": 15.14, "min": 14.2}, "VDD_CPU_SOC_MSS": {"avg": 15.34, "peak": 15.75, "min": 14.56}, "VDD_GPU": {"avg": 26.44, "peak": 40.56, "min": 22.85}, "VIN": {"avg": 68.41, "peak": 108.3, "min": 54.84}}, "power_watts_avg": 26.44, "energy_joules_est": 113.73, "sample_count": 34, "duration_seconds": 4.301}, "timestamp": "2026-01-17T15:56:05.115094"}
{"image_index": 430, "image_name": "000000049060.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000049060.jpg", "image_width": 640, "image_height": 411, "image_resolution": "640x411", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2691.363, "latencies_ms": [2691.363], "images_per_second": 0.372, "prompt_tokens": 27, "response_tokens_est": 72, "n_tiles": 6, "output_text": "The main objects in the image are a train and a group of people. The train is positioned in the foreground, with its front end visible and a few people standing near the train's entrance. The people are standing on the platform, facing the train. The background features a residential area with houses and trees, providing a sense of depth to the scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24957.0, "ram_available_mb": 100815.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 24957.0, "ram_available_mb": 100815.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.85, "peak": 15.24, "min": 14.2}, "VDD_CPU_SOC_MSS": {"avg": 15.37, "peak": 15.75, "min": 14.56}, "VDD_GPU": {"avg": 28.51, "peak": 40.18, "min": 22.86}, "VIN": {"avg": 69.39, "peak": 115.7, "min": 59.75}}, "power_watts_avg": 28.51, "energy_joules_est": 76.74, "sample_count": 21, "duration_seconds": 2.692}, "timestamp": "2026-01-17T15:56:07.812770"}
{"image_index": 430, "image_name": "000000049060.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000049060.jpg", "image_width": 640, "image_height": 411, "image_resolution": "640x411", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1956.882, "latencies_ms": [1956.882], "images_per_second": 0.511, "prompt_tokens": 21, "response_tokens_est": 47, "n_tiles": 6, "output_text": "The image depicts a vintage scene at a train station, featuring a classic steam locomotive with a train car visible in the background. A group of people is gathered at the platform, possibly waiting for the train to arrive.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24957.0, "ram_available_mb": 100815.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 24958.0, "ram_available_mb": 100814.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.86, "peak": 15.24, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.54, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 30.78, "peak": 40.18, "min": 23.64}, "VIN": {"avg": 73.56, "peak": 107.15, "min": 62.57}}, "power_watts_avg": 30.78, "energy_joules_est": 60.24, "sample_count": 15, "duration_seconds": 1.957}, "timestamp": "2026-01-17T15:56:09.775632"}
{"image_index": 430, "image_name": "000000049060.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000049060.jpg", "image_width": 640, "image_height": 411, "image_resolution": "640x411", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2422.544, "latencies_ms": [2422.544], "images_per_second": 0.413, "prompt_tokens": 19, "response_tokens_est": 63, "n_tiles": 6, "output_text": "The image is a black and white photograph featuring a vintage steam locomotive with a prominent smokestack and a large, rounded front. The locomotive is stationed at a platform with a few people standing nearby, and the scene is set against a backdrop of residential buildings and trees under a clear sky.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24958.0, "ram_available_mb": 100814.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 24957.7, "ram_available_mb": 100814.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.85, "peak": 15.24, "min": 14.3}, "VDD_CPU_SOC_MSS": {"avg": 15.44, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 29.32, "peak": 40.57, "min": 22.86}, "VIN": {"avg": 70.54, "peak": 112.94, "min": 54.74}}, "power_watts_avg": 29.32, "energy_joules_est": 71.04, "sample_count": 19, "duration_seconds": 2.423}, "timestamp": "2026-01-17T15:56:12.204669"}
{"image_index": 431, "image_name": "000000049091.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000049091.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1628.65, "latencies_ms": [1628.65], "images_per_second": 0.614, "prompt_tokens": 9, "response_tokens_est": 38, "n_tiles": 6, "output_text": "The image depicts a densely packed urban street scene, filled with numerous signs in both English and Chinese characters, indicating a bustling commercial area with a variety of businesses and services.", "error": null, "sys_before": {"cpu_percent": 44.2, "ram_used_mb": 24957.7, "ram_available_mb": 100814.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 24958.9, "ram_available_mb": 100813.3, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.68, "peak": 15.11, "min": 13.69}, "VDD_CPU_SOC_MSS": {"avg": 15.45, "peak": 16.14, "min": 14.57}, "VDD_GPU": {"avg": 32.1, "peak": 40.18, "min": 25.21}, "VIN": {"avg": 73.27, "peak": 112.89, "min": 56.59}}, "power_watts_avg": 32.1, "energy_joules_est": 52.3, "sample_count": 12, "duration_seconds": 1.629}, "timestamp": "2026-01-17T15:56:13.892403"}
{"image_index": 431, "image_name": "000000049091.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000049091.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2898.021, "latencies_ms": [2898.021], "images_per_second": 0.345, "prompt_tokens": 23, "response_tokens_est": 80, "n_tiles": 6, "output_text": "- 1. Signboard: 8\n- 2. Signboard: 8\n- 3. Signboard: 8\n- 4. Signboard: 8\n- 5. Signboard: 8\n- 6. Signboard: 8\n- 7. Signboard: 8\n- 8. Signboard: 8", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24958.9, "ram_available_mb": 100813.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 24959.2, "ram_available_mb": 100813.0, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.9, "peak": 15.24, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.35, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 28.63, "peak": 40.57, "min": 22.86}, "VIN": {"avg": 69.48, "peak": 103.12, "min": 58.96}}, "power_watts_avg": 28.63, "energy_joules_est": 82.98, "sample_count": 22, "duration_seconds": 2.898}, "timestamp": "2026-01-17T15:56:16.796833"}
{"image_index": 431, "image_name": "000000049091.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000049091.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2387.747, "latencies_ms": [2387.747], "images_per_second": 0.419, "prompt_tokens": 27, "response_tokens_est": 62, "n_tiles": 6, "output_text": "The main objects in the image are numerous signs with Japanese characters, which are densely packed and appear to be hanging from a metal framework. The signs are in the foreground, with the background consisting of a multi-story building. The signs are positioned close to each other, creating a crowded and busy atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24959.2, "ram_available_mb": 100813.0, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 24959.4, "ram_available_mb": 100812.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.86, "peak": 15.24, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.29, "peak": 15.74, "min": 14.56}, "VDD_GPU": {"avg": 29.59, "peak": 40.18, "min": 23.24}, "VIN": {"avg": 70.1, "peak": 103.44, "min": 55.51}}, "power_watts_avg": 29.59, "energy_joules_est": 70.67, "sample_count": 18, "duration_seconds": 2.388}, "timestamp": "2026-01-17T15:56:19.191218"}
{"image_index": 431, "image_name": "000000049091.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000049091.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2697.961, "latencies_ms": [2697.961], "images_per_second": 0.371, "prompt_tokens": 21, "response_tokens_est": 73, "n_tiles": 6, "output_text": "The image depicts a bustling urban street scene, likely in a Chinatown area, as indicated by the numerous signs in Chinese characters. The street is crowded with pedestrians, and the buildings are multi-storied, with balconies and windows visible. The signs are densely packed, creating a chaotic yet vibrant atmosphere typical of a busy city street.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24959.4, "ram_available_mb": 100812.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 24959.9, "ram_available_mb": 100812.3, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.87, "peak": 15.24, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.49, "peak": 15.75, "min": 14.56}, "VDD_GPU": {"avg": 28.66, "peak": 40.16, "min": 22.85}, "VIN": {"avg": 67.4, "peak": 92.44, "min": 52.45}}, "power_watts_avg": 28.66, "energy_joules_est": 77.34, "sample_count": 21, "duration_seconds": 2.698}, "timestamp": "2026-01-17T15:56:21.895715"}
{"image_index": 431, "image_name": "000000049091.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000049091.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2905.973, "latencies_ms": [2905.973], "images_per_second": 0.344, "prompt_tokens": 19, "response_tokens_est": 80, "n_tiles": 6, "output_text": "The image is a black and white photograph capturing a bustling urban street scene. The notable visual attributes include the dense array of signage in various languages, predominantly in black and white, with some signs featuring red and blue colors. The lighting is dim, likely due to the overcast sky, and the materials used for the signs appear to be metal or plastic, typical of urban street signs.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24959.9, "ram_available_mb": 100812.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 24960.1, "ram_available_mb": 100812.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.85, "peak": 15.24, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.34, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 28.25, "peak": 40.16, "min": 22.85}, "VIN": {"avg": 72.41, "peak": 112.47, "min": 63.11}}, "power_watts_avg": 28.25, "energy_joules_est": 82.11, "sample_count": 22, "duration_seconds": 2.906}, "timestamp": "2026-01-17T15:56:24.808402"}
{"image_index": 432, "image_name": "000000049259.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000049259.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1386.738, "latencies_ms": [1386.738], "images_per_second": 0.721, "prompt_tokens": 9, "response_tokens_est": 29, "n_tiles": 6, "output_text": "The image shows a sign that reads \"CLOSED\" on a grassy area, indicating that the area is not open for public use.", "error": null, "sys_before": {"cpu_percent": 36.1, "ram_used_mb": 24960.1, "ram_available_mb": 100812.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 24961.4, "ram_available_mb": 100810.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.66, "peak": 15.14, "min": 13.9}, "VDD_CPU_SOC_MSS": {"avg": 15.35, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 33.25, "peak": 39.78, "min": 26.39}, "VIN": {"avg": 75.02, "peak": 103.45, "min": 61.7}}, "power_watts_avg": 33.25, "energy_joules_est": 46.12, "sample_count": 10, "duration_seconds": 1.387}, "timestamp": "2026-01-17T15:56:26.243981"}
{"image_index": 432, "image_name": "000000049259.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000049259.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1943.402, "latencies_ms": [1943.402], "images_per_second": 0.515, "prompt_tokens": 23, "response_tokens_est": 48, "n_tiles": 6, "output_text": "- sign: 2\n- grass: 1\n- bench: 1\n- water: 1\n- person: 1\n- sign: 1\n- tree: 1\n- bench: 1", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24961.4, "ram_available_mb": 100810.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 24961.6, "ram_available_mb": 100810.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.0, "peak": 15.54, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.61, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 31.88, "peak": 40.95, "min": 24.43}, "VIN": {"avg": 72.11, "peak": 111.11, "min": 61.46}}, "power_watts_avg": 31.88, "energy_joules_est": 61.97, "sample_count": 14, "duration_seconds": 1.944}, "timestamp": "2026-01-17T15:56:28.194775"}
{"image_index": 432, "image_name": "000000049259.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000049259.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2457.884, "latencies_ms": [2457.884], "images_per_second": 0.407, "prompt_tokens": 27, "response_tokens_est": 67, "n_tiles": 6, "output_text": "The main objects in the image are a sign and a bench. The sign is placed on the ground in the foreground, while the bench is situated in the background. The sign is near the ground, and the bench is further away, creating a spatial relationship where the sign is closer to the viewer and the bench is farther away.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24961.6, "ram_available_mb": 100810.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 24961.3, "ram_available_mb": 100810.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.06, "peak": 15.44, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.67, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 29.65, "peak": 40.95, "min": 23.64}, "VIN": {"avg": 73.46, "peak": 120.43, "min": 58.57}}, "power_watts_avg": 29.65, "energy_joules_est": 72.9, "sample_count": 19, "duration_seconds": 2.459}, "timestamp": "2026-01-17T15:56:30.659630"}
{"image_index": 432, "image_name": "000000049259.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000049259.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2997.456, "latencies_ms": [2997.456], "images_per_second": 0.334, "prompt_tokens": 21, "response_tokens_est": 87, "n_tiles": 6, "output_text": "The image depicts a scene at a lakeside park with a man lounging on a concrete bench. The man is shirtless and wearing shorts, and he appears to be enjoying the sunny day by the water. In the foreground, there is a sign that reads \"CLOSED\" and \"Laketon Trail Cross,\" indicating that the area is closed to the public and is part of the Laketon Trail Crossing.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24961.3, "ram_available_mb": 100810.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 24961.3, "ram_available_mb": 100810.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.12, "peak": 15.54, "min": 14.43}, "VDD_CPU_SOC_MSS": {"avg": 15.66, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 28.42, "peak": 40.18, "min": 23.25}, "VIN": {"avg": 69.99, "peak": 115.75, "min": 56.07}}, "power_watts_avg": 28.42, "energy_joules_est": 85.2, "sample_count": 23, "duration_seconds": 2.998}, "timestamp": "2026-01-17T15:56:33.663558"}
{"image_index": 432, "image_name": "000000049259.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000049259.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2024.327, "latencies_ms": [2024.327], "images_per_second": 0.494, "prompt_tokens": 19, "response_tokens_est": 51, "n_tiles": 6, "output_text": "The image features a blue wooden sign with white lettering that reads \"CLOSED\" and \"Laketon Trail Crossing.\" The sign is placed on a grassy area with a clear blue sky overhead, indicating a sunny day with good lighting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24961.3, "ram_available_mb": 100810.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 24961.3, "ram_available_mb": 100810.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.03, "peak": 15.54, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.62, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 30.91, "peak": 40.16, "min": 24.04}, "VIN": {"avg": 73.05, "peak": 115.95, "min": 60.3}}, "power_watts_avg": 30.91, "energy_joules_est": 62.59, "sample_count": 15, "duration_seconds": 2.025}, "timestamp": "2026-01-17T15:56:35.694374"}
{"image_index": 433, "image_name": "000000049269.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000049269.jpg", "image_width": 431, "image_height": 640, "image_resolution": "431x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1549.71, "latencies_ms": [1549.71], "images_per_second": 0.645, "prompt_tokens": 9, "response_tokens_est": 35, "n_tiles": 6, "output_text": "A golden retriever is lying on the grass, panting with its tongue out, while a brown horse stands behind it, both appearing to be enjoying the sunny day.", "error": null, "sys_before": {"cpu_percent": 44.3, "ram_used_mb": 24961.3, "ram_available_mb": 100810.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 24962.1, "ram_available_mb": 100810.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.74, "peak": 15.14, "min": 13.9}, "VDD_CPU_SOC_MSS": {"avg": 15.42, "peak": 16.14, "min": 14.57}, "VDD_GPU": {"avg": 32.17, "peak": 39.77, "min": 25.21}, "VIN": {"avg": 75.61, "peak": 113.66, "min": 57.67}}, "power_watts_avg": 32.17, "energy_joules_est": 49.87, "sample_count": 12, "duration_seconds": 1.55}, "timestamp": "2026-01-17T15:56:37.303408"}
{"image_index": 433, "image_name": "000000049269.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000049269.jpg", "image_width": 431, "image_height": 640, "image_resolution": "431x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1053.439, "latencies_ms": [1053.439], "images_per_second": 0.949, "prompt_tokens": 23, "response_tokens_est": 15, "n_tiles": 6, "output_text": "dog: 1\nhorse: 2\ngrass: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24962.3, "ram_available_mb": 100809.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 24962.6, "ram_available_mb": 100809.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.89, "peak": 15.44, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.41, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 37.2, "peak": 40.57, "min": 32.7}, "VIN": {"avg": 85.45, "peak": 116.49, "min": 63.46}}, "power_watts_avg": 37.2, "energy_joules_est": 39.2, "sample_count": 7, "duration_seconds": 1.054}, "timestamp": "2026-01-17T15:56:38.364153"}
{"image_index": 433, "image_name": "000000049269.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000049269.jpg", "image_width": 431, "image_height": 640, "image_resolution": "431x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2403.833, "latencies_ms": [2403.833], "images_per_second": 0.416, "prompt_tokens": 27, "response_tokens_est": 65, "n_tiles": 6, "output_text": "The main objects in the image are a brown horse and a golden retriever. The brown horse is in the background, while the golden retriever is in the foreground. The horse is slightly out of focus, indicating it is farther away, while the dog is in sharp focus, suggesting it is closer to the camera.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24962.6, "ram_available_mb": 100809.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 24962.8, "ram_available_mb": 100809.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.06, "peak": 15.44, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.64, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 30.83, "peak": 41.76, "min": 23.64}, "VIN": {"avg": 75.31, "peak": 113.8, "min": 61.28}}, "power_watts_avg": 30.83, "energy_joules_est": 74.12, "sample_count": 18, "duration_seconds": 2.404}, "timestamp": "2026-01-17T15:56:40.773444"}
{"image_index": 433, "image_name": "000000049269.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000049269.jpg", "image_width": 431, "image_height": 640, "image_resolution": "431x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2103.938, "latencies_ms": [2103.938], "images_per_second": 0.475, "prompt_tokens": 21, "response_tokens_est": 54, "n_tiles": 6, "output_text": "The image captures a serene scene in a lush green field, where a golden retriever and a brown horse are the main subjects. The dog is lying down, seemingly relaxed, while the horse stands nearby, both appearing content and at ease in their natural environment.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24962.8, "ram_available_mb": 100809.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 24962.8, "ram_available_mb": 100809.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.05, "peak": 15.44, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.62, "peak": 16.14, "min": 14.57}, "VDD_GPU": {"avg": 30.78, "peak": 40.56, "min": 24.04}, "VIN": {"avg": 70.57, "peak": 111.41, "min": 54.71}}, "power_watts_avg": 30.78, "energy_joules_est": 64.77, "sample_count": 16, "duration_seconds": 2.104}, "timestamp": "2026-01-17T15:56:42.883720"}
{"image_index": 433, "image_name": "000000049269.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000049269.jpg", "image_width": 431, "image_height": 640, "image_resolution": "431x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1834.244, "latencies_ms": [1834.244], "images_per_second": 0.545, "prompt_tokens": 19, "response_tokens_est": 44, "n_tiles": 6, "output_text": "The image features a golden retriever dog with a fluffy coat, lying on a lush green field under a clear blue sky. The lighting is bright and natural, casting soft shadows on the dog and the grass.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24962.8, "ram_available_mb": 100809.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 24962.8, "ram_available_mb": 100809.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.03, "peak": 15.44, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.61, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 31.71, "peak": 40.18, "min": 24.43}, "VIN": {"avg": 72.68, "peak": 95.36, "min": 58.15}}, "power_watts_avg": 31.71, "energy_joules_est": 58.17, "sample_count": 14, "duration_seconds": 1.835}, "timestamp": "2026-01-17T15:56:44.724152"}
{"image_index": 434, "image_name": "000000049759.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000049759.jpg", "image_width": 640, "image_height": 457, "image_resolution": "640x457", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2160.64, "latencies_ms": [2160.64], "images_per_second": 0.463, "prompt_tokens": 9, "response_tokens_est": 16, "n_tiles": 12, "output_text": "A group of people are playing volleyball in an indoor gymnasium.", "error": null, "sys_before": {"cpu_percent": 36.7, "ram_used_mb": 24924.6, "ram_available_mb": 100847.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.1, "ram_used_mb": 24926.1, "ram_available_mb": 100846.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.22, "peak": 16.66, "min": 13.59}, "VDD_CPU_SOC_MSS": {"avg": 15.99, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 37.49, "peak": 43.73, "min": 29.54}, "VIN": {"avg": 83.19, "peak": 114.42, "min": 61.54}}, "power_watts_avg": 37.49, "energy_joules_est": 81.02, "sample_count": 16, "duration_seconds": 2.161}, "timestamp": "2026-01-17T15:56:46.972788"}
{"image_index": 434, "image_name": "000000049759.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000049759.jpg", "image_width": 640, "image_height": 457, "image_resolution": "640x457", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2963.308, "latencies_ms": [2963.308], "images_per_second": 0.337, "prompt_tokens": 23, "response_tokens_est": 38, "n_tiles": 12, "output_text": "1. Volleyball\n2. Court\n3. Players\n4. Court lines\n5. Floor\n6. Spectators\n7. Spectator area\n8. Lighting", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24926.1, "ram_available_mb": 100846.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 24926.8, "ram_available_mb": 100845.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.25, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.78, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 36.03, "peak": 46.07, "min": 26.39}, "VIN": {"avg": 80.63, "peak": 115.2, "min": 65.21}}, "power_watts_avg": 36.03, "energy_joules_est": 106.79, "sample_count": 23, "duration_seconds": 2.964}, "timestamp": "2026-01-17T15:56:49.943810"}
{"image_index": 434, "image_name": "000000049759.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000049759.jpg", "image_width": 640, "image_height": 457, "image_resolution": "640x457", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 4804.415, "latencies_ms": [4804.415], "images_per_second": 0.208, "prompt_tokens": 27, "response_tokens_est": 92, "n_tiles": 12, "output_text": "The main objects in the image are a group of people playing volleyball in an indoor gymnasium. The foreground features a man in a blue sleeveless top and shorts, who is standing near the net, preparing to serve the volleyball. The background shows other players and spectators, with some standing near the walls and others near the net. The net is prominently placed in the center of the image, dividing the space into two halves.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24926.8, "ram_available_mb": 100845.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 24927.3, "ram_available_mb": 100844.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.15, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.6, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 31.92, "peak": 44.91, "min": 26.0}, "VIN": {"avg": 76.5, "peak": 129.47, "min": 57.17}}, "power_watts_avg": 31.92, "energy_joules_est": 153.37, "sample_count": 38, "duration_seconds": 4.805}, "timestamp": "2026-01-17T15:56:54.754984"}
{"image_index": 434, "image_name": "000000049759.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000049759.jpg", "image_width": 640, "image_height": 457, "image_resolution": "640x457", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3397.045, "latencies_ms": [3397.045], "images_per_second": 0.294, "prompt_tokens": 21, "response_tokens_est": 51, "n_tiles": 12, "output_text": "The image depicts an indoor sports facility with a blue floor marked with white lines, likely a basketball court. Several individuals, including both men and women, are present, some standing and others in motion, possibly engaged in a game or practice session.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24927.3, "ram_available_mb": 100844.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 24927.5, "ram_available_mb": 100844.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.22, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.78, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 34.54, "peak": 44.89, "min": 26.0}, "VIN": {"avg": 78.81, "peak": 127.98, "min": 59.7}}, "power_watts_avg": 34.54, "energy_joules_est": 117.35, "sample_count": 26, "duration_seconds": 3.397}, "timestamp": "2026-01-17T15:56:58.158999"}
{"image_index": 434, "image_name": "000000049759.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000049759.jpg", "image_width": 640, "image_height": 457, "image_resolution": "640x457", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2948.067, "latencies_ms": [2948.067], "images_per_second": 0.339, "prompt_tokens": 19, "response_tokens_est": 37, "n_tiles": 12, "output_text": "The image depicts a brightly lit indoor sports court with a blue floor and white boundary lines. The lighting is artificial, providing ample illumination for the players and spectators.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24927.5, "ram_available_mb": 100844.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 24928.2, "ram_available_mb": 100843.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.14, "peak": 16.66, "min": 14.12}, "VDD_CPU_SOC_MSS": {"avg": 15.71, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 35.72, "peak": 45.68, "min": 26.39}, "VIN": {"avg": 81.65, "peak": 132.91, "min": 65.69}}, "power_watts_avg": 35.72, "energy_joules_est": 105.33, "sample_count": 23, "duration_seconds": 2.949}, "timestamp": "2026-01-17T15:57:01.114452"}
{"image_index": 435, "image_name": "000000049761.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000049761.jpg", "image_width": 640, "image_height": 479, "image_resolution": "640x479", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 3301.534, "latencies_ms": [3301.534], "images_per_second": 0.303, "prompt_tokens": 9, "response_tokens_est": 50, "n_tiles": 12, "output_text": "The image depicts a serene savanna landscape with a herd of zebras grazing on the grassy plains, while a large flock of pink flamingos stands in the background, creating a striking contrast against the greenery.", "error": null, "sys_before": {"cpu_percent": 43.4, "ram_used_mb": 24932.8, "ram_available_mb": 100839.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 24929.3, "ram_available_mb": 100842.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.1, "peak": 16.66, "min": 13.19}, "VDD_CPU_SOC_MSS": {"avg": 15.78, "peak": 17.71, "min": 14.17}, "VDD_GPU": {"avg": 33.83, "peak": 44.1, "min": 26.01}, "VIN": {"avg": 73.34, "peak": 96.29, "min": 60.43}}, "power_watts_avg": 33.83, "energy_joules_est": 111.7, "sample_count": 26, "duration_seconds": 3.302}, "timestamp": "2026-01-17T15:57:04.518637"}
{"image_index": 435, "image_name": "000000049761.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000049761.jpg", "image_width": 640, "image_height": 479, "image_resolution": "640x479", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 6040.485, "latencies_ms": [6040.485], "images_per_second": 0.166, "prompt_tokens": 23, "response_tokens_est": 129, "n_tiles": 12, "output_text": "zebra: 3\nbuffalo: 2\nantelope: 1\nantelope: 1\nantelope: 1\nantelope: 1\nantelope: 1\nantelope: 1\nantelope: 1\nantelope: 1\nantelope: 1\nantelope: 1\nantelope: 1\nantelope: 1\nantelope: 1\nantelope: 1\nantelope: 1\nantelope: 1\nantelope:", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24929.3, "ram_available_mb": 100842.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 24925.9, "ram_available_mb": 100846.3, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.13, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.63, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 30.68, "peak": 45.3, "min": 25.6}, "VIN": {"avg": 72.96, "peak": 112.7, "min": 54.03}}, "power_watts_avg": 30.68, "energy_joules_est": 185.34, "sample_count": 48, "duration_seconds": 6.041}, "timestamp": "2026-01-17T15:57:10.565849"}
{"image_index": 435, "image_name": "000000049761.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000049761.jpg", "image_width": 640, "image_height": 479, "image_resolution": "640x479", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3848.038, "latencies_ms": [3848.038], "images_per_second": 0.26, "prompt_tokens": 27, "response_tokens_est": 64, "n_tiles": 12, "output_text": "In the image, the main objects are a herd of zebras and a large flock of pink flamingos. The zebras are in the foreground, with one zebra standing near the water's edge. The flamingos are in the background, with their long necks extending over the water.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24925.9, "ram_available_mb": 100846.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 24924.6, "ram_available_mb": 100847.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.18, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.77, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 33.54, "peak": 45.28, "min": 26.0}, "VIN": {"avg": 76.16, "peak": 112.04, "min": 56.84}}, "power_watts_avg": 33.54, "energy_joules_est": 129.08, "sample_count": 30, "duration_seconds": 3.849}, "timestamp": "2026-01-17T15:57:14.420060"}
{"image_index": 435, "image_name": "000000049761.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000049761.jpg", "image_width": 640, "image_height": 479, "image_resolution": "640x479", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3502.006, "latencies_ms": [3502.006], "images_per_second": 0.286, "prompt_tokens": 21, "response_tokens_est": 54, "n_tiles": 12, "output_text": "The image depicts a serene savanna landscape with a herd of zebras grazing on the grassy plains. In the background, a large flock of pink flamingos is standing in the water, creating a striking contrast against the greenery.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24924.6, "ram_available_mb": 100847.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 24925.9, "ram_available_mb": 100846.3, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.18, "peak": 16.66, "min": 14.12}, "VDD_CPU_SOC_MSS": {"avg": 15.8, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 34.09, "peak": 45.28, "min": 26.0}, "VIN": {"avg": 74.79, "peak": 109.1, "min": 56.74}}, "power_watts_avg": 34.09, "energy_joules_est": 119.4, "sample_count": 27, "duration_seconds": 3.503}, "timestamp": "2026-01-17T15:57:17.928975"}
{"image_index": 435, "image_name": "000000049761.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000049761.jpg", "image_width": 640, "image_height": 479, "image_resolution": "640x479", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3160.049, "latencies_ms": [3160.049], "images_per_second": 0.316, "prompt_tokens": 19, "response_tokens_est": 44, "n_tiles": 12, "output_text": "The image depicts a serene savanna landscape with a clear sky, a few scattered clouds, and a gentle breeze. The grass is a vibrant green, and the water in the background is calm and reflective.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24925.9, "ram_available_mb": 100846.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 24924.1, "ram_available_mb": 100848.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.23, "peak": 16.56, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.79, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 35.29, "peak": 45.3, "min": 26.39}, "VIN": {"avg": 77.16, "peak": 119.75, "min": 65.26}}, "power_watts_avg": 35.29, "energy_joules_est": 111.53, "sample_count": 24, "duration_seconds": 3.161}, "timestamp": "2026-01-17T15:57:21.095728"}
{"image_index": 436, "image_name": "000000049810.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000049810.jpg", "image_width": 640, "image_height": 424, "image_resolution": "640x424", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1189.674, "latencies_ms": [1189.674], "images_per_second": 0.841, "prompt_tokens": 9, "response_tokens_est": 22, "n_tiles": 6, "output_text": "A ginger and white cat is sitting on a wooden bench, looking at its reflection in a glass surface.", "error": null, "sys_before": {"cpu_percent": 35.1, "ram_used_mb": 24948.2, "ram_available_mb": 100824.0, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 24949.4, "ram_available_mb": 100822.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.6, "peak": 15.04, "min": 13.82}, "VDD_CPU_SOC_MSS": {"avg": 15.3, "peak": 16.14, "min": 14.18}, "VDD_GPU": {"avg": 35.65, "peak": 41.34, "min": 29.54}, "VIN": {"avg": 73.77, "peak": 90.05, "min": 62.74}}, "power_watts_avg": 35.65, "energy_joules_est": 42.43, "sample_count": 8, "duration_seconds": 1.19}, "timestamp": "2026-01-17T15:57:22.348040"}
{"image_index": 436, "image_name": "000000049810.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000049810.jpg", "image_width": 640, "image_height": 424, "image_resolution": "640x424", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1498.348, "latencies_ms": [1498.348], "images_per_second": 0.667, "prompt_tokens": 23, "response_tokens_est": 32, "n_tiles": 6, "output_text": "1. Cat\n2. Cat\n3. Cat\n4. Cat\n5. Cat\n6. Cat\n7. Cat\n8. Cat", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24949.4, "ram_available_mb": 100822.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 24950.1, "ram_available_mb": 100822.0, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.02, "peak": 15.44, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.57, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 34.63, "peak": 41.36, "min": 26.39}, "VIN": {"avg": 73.14, "peak": 95.14, "min": 63.16}}, "power_watts_avg": 34.63, "energy_joules_est": 51.9, "sample_count": 11, "duration_seconds": 1.499}, "timestamp": "2026-01-17T15:57:23.852875"}
{"image_index": 436, "image_name": "000000049810.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000049810.jpg", "image_width": 640, "image_height": 424, "image_resolution": "640x424", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2443.178, "latencies_ms": [2443.178], "images_per_second": 0.409, "prompt_tokens": 27, "response_tokens_est": 64, "n_tiles": 6, "output_text": "The main object in the foreground is a ginger and white cat, which is sitting on a wooden bench. The cat is positioned near the center of the image, with its head turned towards the camera. The background features a blurred wooden bench, indicating that the cat is sitting closer to the camera than the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24950.1, "ram_available_mb": 100822.0, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 24951.1, "ram_available_mb": 100821.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.95, "peak": 15.34, "min": 14.43}, "VDD_CPU_SOC_MSS": {"avg": 15.62, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 29.69, "peak": 40.97, "min": 23.24}, "VIN": {"avg": 69.09, "peak": 88.06, "min": 59.34}}, "power_watts_avg": 29.69, "energy_joules_est": 72.55, "sample_count": 19, "duration_seconds": 2.444}, "timestamp": "2026-01-17T15:57:26.302691"}
{"image_index": 436, "image_name": "000000049810.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000049810.jpg", "image_width": 640, "image_height": 424, "image_resolution": "640x424", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2332.125, "latencies_ms": [2332.125], "images_per_second": 0.429, "prompt_tokens": 21, "response_tokens_est": 60, "n_tiles": 6, "output_text": "The image depicts a cozy scene with a ginger and white cat sitting on a wooden deck. The cat is looking at its reflection in a glass surface, creating a sense of curiosity and playfulness. The deck's wooden planks and the cat's relaxed posture suggest a peaceful, domestic setting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24951.1, "ram_available_mb": 100821.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 24951.4, "ram_available_mb": 100820.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.87, "peak": 15.24, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.46, "peak": 15.75, "min": 14.56}, "VDD_GPU": {"avg": 29.66, "peak": 40.56, "min": 23.25}, "VIN": {"avg": 66.24, "peak": 76.18, "min": 61.1}}, "power_watts_avg": 29.66, "energy_joules_est": 69.18, "sample_count": 18, "duration_seconds": 2.333}, "timestamp": "2026-01-17T15:57:28.641304"}
{"image_index": 436, "image_name": "000000049810.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000049810.jpg", "image_width": 640, "image_height": 424, "image_resolution": "640x424", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1737.536, "latencies_ms": [1737.536], "images_per_second": 0.576, "prompt_tokens": 19, "response_tokens_est": 39, "n_tiles": 6, "output_text": "The image features a ginger and white cat sitting on a wooden deck. The cat's fur appears soft and slightly shiny, and the lighting is natural, casting a warm glow on the scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24951.4, "ram_available_mb": 100820.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 24951.4, "ram_available_mb": 100820.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.84, "peak": 15.24, "min": 14.3}, "VDD_CPU_SOC_MSS": {"avg": 15.35, "peak": 15.75, "min": 14.56}, "VDD_GPU": {"avg": 32.09, "peak": 40.57, "min": 24.42}, "VIN": {"avg": 75.52, "peak": 117.0, "min": 61.2}}, "power_watts_avg": 32.09, "energy_joules_est": 55.77, "sample_count": 13, "duration_seconds": 1.738}, "timestamp": "2026-01-17T15:57:30.385174"}
{"image_index": 437, "image_name": "000000050006.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050006.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2553.458, "latencies_ms": [2553.458], "images_per_second": 0.392, "prompt_tokens": 9, "response_tokens_est": 28, "n_tiles": 12, "output_text": "A row of boats are moored in a marina by a calm body of water, with a small town visible in the background.", "error": null, "sys_before": {"cpu_percent": 51.1, "ram_used_mb": 24910.8, "ram_available_mb": 100861.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 24912.0, "ram_available_mb": 100860.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.17, "peak": 16.66, "min": 13.39}, "VDD_CPU_SOC_MSS": {"avg": 15.97, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 36.13, "peak": 43.73, "min": 27.57}, "VIN": {"avg": 78.46, "peak": 103.0, "min": 60.1}}, "power_watts_avg": 36.13, "energy_joules_est": 92.27, "sample_count": 19, "duration_seconds": 2.554}, "timestamp": "2026-01-17T15:57:33.037738"}
{"image_index": 437, "image_name": "000000050006.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050006.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 3014.992, "latencies_ms": [3014.992], "images_per_second": 0.332, "prompt_tokens": 23, "response_tokens_est": 40, "n_tiles": 12, "output_text": "object: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 24912.0, "ram_available_mb": 100860.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 24913.2, "ram_available_mb": 100859.0, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.29, "peak": 16.66, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.92, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 35.69, "peak": 45.28, "min": 26.79}, "VIN": {"avg": 77.55, "peak": 117.17, "min": 59.11}}, "power_watts_avg": 35.69, "energy_joules_est": 107.62, "sample_count": 23, "duration_seconds": 3.015}, "timestamp": "2026-01-17T15:57:36.059384"}
{"image_index": 437, "image_name": "000000050006.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050006.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3773.148, "latencies_ms": [3773.148], "images_per_second": 0.265, "prompt_tokens": 27, "response_tokens_est": 62, "n_tiles": 12, "output_text": "The main objects in the image are a boat and a dock. The boat is located in the foreground, near the dock, and is docked. The dock is situated in the middle ground, extending into the water. The background features a hilly landscape with trees and buildings, creating a scenic view.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24913.2, "ram_available_mb": 100859.0, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 24913.5, "ram_available_mb": 100858.7, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.19, "peak": 16.56, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.73, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 33.69, "peak": 46.09, "min": 26.0}, "VIN": {"avg": 76.39, "peak": 112.85, "min": 59.75}}, "power_watts_avg": 33.69, "energy_joules_est": 127.13, "sample_count": 30, "duration_seconds": 3.774}, "timestamp": "2026-01-17T15:57:39.839637"}
{"image_index": 437, "image_name": "000000050006.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050006.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 4303.792, "latencies_ms": [4303.792], "images_per_second": 0.232, "prompt_tokens": 21, "response_tokens_est": 78, "n_tiles": 12, "output_text": "The image depicts a serene lakeside scene with a calm body of water in the foreground, surrounded by lush greenery and a few buildings. Several boats are moored along the shore, and a lamppost stands near the water's edge. The overall atmosphere is peaceful and tranquil, with the natural beauty of the lake and the surrounding landscape providing a picturesque setting.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24913.5, "ram_available_mb": 100858.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 24913.5, "ram_available_mb": 100858.7, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.2, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.67, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 32.67, "peak": 45.68, "min": 26.0}, "VIN": {"avg": 75.42, "peak": 120.71, "min": 64.55}}, "power_watts_avg": 32.67, "energy_joules_est": 140.61, "sample_count": 34, "duration_seconds": 4.304}, "timestamp": "2026-01-17T15:57:44.150523"}
{"image_index": 437, "image_name": "000000050006.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050006.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3599.73, "latencies_ms": [3599.73], "images_per_second": 0.278, "prompt_tokens": 19, "response_tokens_est": 57, "n_tiles": 12, "output_text": "The image depicts a serene lakeside scene with a calm, blue-green water body. The sky is overcast, casting a soft, diffused light over the entire scene. The boats are white, with some featuring red accents, and are moored along the shore.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24913.5, "ram_available_mb": 100858.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 24914.7, "ram_available_mb": 100857.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.23, "peak": 16.56, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.8, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 34.01, "peak": 45.28, "min": 26.0}, "VIN": {"avg": 76.47, "peak": 118.45, "min": 59.64}}, "power_watts_avg": 34.01, "energy_joules_est": 122.44, "sample_count": 28, "duration_seconds": 3.6}, "timestamp": "2026-01-17T15:57:47.757109"}
{"image_index": 438, "image_name": "000000050145.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050145.jpg", "image_width": 480, "image_height": 320, "image_resolution": "480x320", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1963.577, "latencies_ms": [1963.577], "images_per_second": 0.509, "prompt_tokens": 9, "response_tokens_est": 51, "n_tiles": 6, "output_text": "A man stands in the foreground, shirtless, with a bicycle in front of him, while a woman in a dress and a child in a stroller are visible in the background, all set against a backdrop of various signs and a street scene.", "error": null, "sys_before": {"cpu_percent": 43.2, "ram_used_mb": 24945.1, "ram_available_mb": 100827.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 24945.8, "ram_available_mb": 100826.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.87, "peak": 15.32, "min": 13.72}, "VDD_CPU_SOC_MSS": {"avg": 15.51, "peak": 16.14, "min": 14.17}, "VDD_GPU": {"avg": 30.78, "peak": 40.56, "min": 24.03}, "VIN": {"avg": 77.66, "peak": 120.69, "min": 60.98}}, "power_watts_avg": 30.78, "energy_joules_est": 60.45, "sample_count": 15, "duration_seconds": 1.964}, "timestamp": "2026-01-17T15:57:49.786246"}
{"image_index": 438, "image_name": "000000050145.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050145.jpg", "image_width": 480, "image_height": 320, "image_resolution": "480x320", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2540.748, "latencies_ms": [2540.748], "images_per_second": 0.394, "prompt_tokens": 23, "response_tokens_est": 71, "n_tiles": 6, "output_text": "1. Bicycle: 1\n2. Man: 1\n3. Street sign: 1\n4. Motorcycle: 1\n5. Motorcycle rider: 1\n6. Pedestrian: 1\n7. Pedestrian with umbrella: 1\n8. Pedestrian with backpack: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24945.8, "ram_available_mb": 100826.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 24947.0, "ram_available_mb": 100825.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.17, "peak": 15.54, "min": 14.43}, "VDD_CPU_SOC_MSS": {"avg": 15.81, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 29.66, "peak": 40.16, "min": 23.64}, "VIN": {"avg": 71.99, "peak": 113.55, "min": 60.91}}, "power_watts_avg": 29.66, "energy_joules_est": 75.37, "sample_count": 19, "duration_seconds": 2.541}, "timestamp": "2026-01-17T15:57:52.333071"}
{"image_index": 438, "image_name": "000000050145.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050145.jpg", "image_width": 480, "image_height": 320, "image_resolution": "480x320", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2232.275, "latencies_ms": [2232.275], "images_per_second": 0.448, "prompt_tokens": 27, "response_tokens_est": 58, "n_tiles": 6, "output_text": "The main object in the foreground is a man standing next to a bicycle. He is wearing a towel around his waist. In the background, there are various signs and a person riding a bicycle. The signs are located near the man, and the person is further back in the scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24947.0, "ram_available_mb": 100825.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 24947.3, "ram_available_mb": 100824.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.03, "peak": 15.54, "min": 14.43}, "VDD_CPU_SOC_MSS": {"avg": 15.68, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 30.21, "peak": 40.16, "min": 23.64}, "VIN": {"avg": 71.55, "peak": 108.27, "min": 62.62}}, "power_watts_avg": 30.21, "energy_joules_est": 67.45, "sample_count": 17, "duration_seconds": 2.233}, "timestamp": "2026-01-17T15:57:54.570880"}
{"image_index": 438, "image_name": "000000050145.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050145.jpg", "image_width": 480, "image_height": 320, "image_resolution": "480x320", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3288.982, "latencies_ms": [3288.982], "images_per_second": 0.304, "prompt_tokens": 21, "response_tokens_est": 97, "n_tiles": 6, "output_text": "The image depicts a street scene in an urban area, likely in a developing country given the presence of traditional clothing and the language on the signs. A man stands in the foreground, shirtless and wearing light-colored pants, holding a bicycle. In the background, there are various signs in a non-English language, and a few people are visible, including one person sitting on a bicycle. The overall setting appears to be a bustling, possibly informal marketplace or street market.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24947.3, "ram_available_mb": 100824.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 24948.8, "ram_available_mb": 100823.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.06, "peak": 15.44, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.69, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 28.08, "peak": 40.95, "min": 23.24}, "VIN": {"avg": 70.18, "peak": 108.01, "min": 58.94}}, "power_watts_avg": 28.08, "energy_joules_est": 92.37, "sample_count": 25, "duration_seconds": 3.29}, "timestamp": "2026-01-17T15:57:57.865981"}
{"image_index": 438, "image_name": "000000050145.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050145.jpg", "image_width": 480, "image_height": 320, "image_resolution": "480x320", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2395.144, "latencies_ms": [2395.144], "images_per_second": 0.418, "prompt_tokens": 19, "response_tokens_est": 64, "n_tiles": 6, "output_text": "The image is a black and white photograph featuring a man standing in front of a store with a sign in Chinese. The man is shirtless, wearing light-colored pants, and holding a bicycle. The store has a signboard with Chinese characters, and there are other indistinct figures and objects in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24948.8, "ram_available_mb": 100823.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 24949.0, "ram_available_mb": 100823.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.04, "peak": 15.54, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.66, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 29.78, "peak": 40.18, "min": 23.25}, "VIN": {"avg": 71.83, "peak": 115.04, "min": 59.65}}, "power_watts_avg": 29.78, "energy_joules_est": 71.34, "sample_count": 18, "duration_seconds": 2.395}, "timestamp": "2026-01-17T15:58:00.267328"}
{"image_index": 439, "image_name": "000000050149.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050149.jpg", "image_width": 500, "image_height": 376, "image_resolution": "500x376", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2490.972, "latencies_ms": [2490.972], "images_per_second": 0.401, "prompt_tokens": 9, "response_tokens_est": 26, "n_tiles": 12, "output_text": "The image shows a bunch of ripe bananas hanging from a wire, with a few bunches hanging from a wooden pole.", "error": null, "sys_before": {"cpu_percent": 46.4, "ram_used_mb": 24915.6, "ram_available_mb": 100856.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.5, "ram_used_mb": 24917.5, "ram_available_mb": 100854.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.21, "peak": 16.66, "min": 13.69}, "VDD_CPU_SOC_MSS": {"avg": 15.99, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 36.19, "peak": 43.71, "min": 27.97}, "VIN": {"avg": 76.83, "peak": 112.65, "min": 63.62}}, "power_watts_avg": 36.19, "energy_joules_est": 90.17, "sample_count": 19, "duration_seconds": 2.491}, "timestamp": "2026-01-17T15:58:02.837583"}
{"image_index": 439, "image_name": "000000050149.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050149.jpg", "image_width": 500, "image_height": 376, "image_resolution": "500x376", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 6049.178, "latencies_ms": [6049.178], "images_per_second": 0.165, "prompt_tokens": 23, "response_tokens_est": 129, "n_tiles": 12, "output_text": "bananas: 20\nbottle-water: 1\nbucket: 1\ndoor: 1\ndoorframe: 1\ndoorframe: 1\ndoorframe: 1\ndoorframe: 1\ndoorframe: 1\ndoorframe: 1\ndoorframe: 1\ndoorframe: 1\ndoorframe: 1\ndoorframe: 1\ndoorframe: 1\ndoorframe: 1\ndoorframe: 1\ndoorframe: 1\ndoorframe: 1\ndoorframe: 1\ndoorframe: 1\ndoorframe", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24917.5, "ram_available_mb": 100854.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 24918.3, "ram_available_mb": 100853.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.12, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.56, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 30.81, "peak": 45.68, "min": 25.6}, "VIN": {"avg": 73.36, "peak": 143.78, "min": 56.15}}, "power_watts_avg": 30.81, "energy_joules_est": 186.39, "sample_count": 47, "duration_seconds": 6.05}, "timestamp": "2026-01-17T15:58:08.893441"}
{"image_index": 439, "image_name": "000000050149.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050149.jpg", "image_width": 500, "image_height": 376, "image_resolution": "500x376", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3408.738, "latencies_ms": [3408.738], "images_per_second": 0.293, "prompt_tokens": 27, "response_tokens_est": 51, "n_tiles": 12, "output_text": "The bananas are hanging from a wire or a string, with some bunches closer to the foreground and others further back. The basket containing the bananas is placed near the left side of the image, while the basket itself is in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24918.3, "ram_available_mb": 100853.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 24919.0, "ram_available_mb": 100853.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.2, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.67, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 34.21, "peak": 44.89, "min": 26.0}, "VIN": {"avg": 78.19, "peak": 133.08, "min": 66.08}}, "power_watts_avg": 34.21, "energy_joules_est": 116.62, "sample_count": 27, "duration_seconds": 3.409}, "timestamp": "2026-01-17T15:58:12.308506"}
{"image_index": 439, "image_name": "000000050149.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050149.jpg", "image_width": 500, "image_height": 376, "image_resolution": "500x376", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3912.614, "latencies_ms": [3912.614], "images_per_second": 0.256, "prompt_tokens": 21, "response_tokens_est": 66, "n_tiles": 12, "output_text": "The image depicts a market stall with a large number of ripe, yellow bananas hanging from a wire. The bananas are displayed in a neat and organized manner, with some hanging from hooks and others hanging freely. The stall appears to be located in a small, rustic market setting, with a simple and functional design.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24919.0, "ram_available_mb": 100853.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 24919.2, "ram_available_mb": 100852.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.16, "peak": 16.56, "min": 14.12}, "VDD_CPU_SOC_MSS": {"avg": 15.66, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 33.22, "peak": 45.68, "min": 26.0}, "VIN": {"avg": 73.27, "peak": 102.25, "min": 58.34}}, "power_watts_avg": 33.22, "energy_joules_est": 130.0, "sample_count": 31, "duration_seconds": 3.913}, "timestamp": "2026-01-17T15:58:16.227631"}
{"image_index": 439, "image_name": "000000050149.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050149.jpg", "image_width": 500, "image_height": 376, "image_resolution": "500x376", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3200.068, "latencies_ms": [3200.068], "images_per_second": 0.312, "prompt_tokens": 19, "response_tokens_est": 45, "n_tiles": 12, "output_text": "The image showcases a bunch of ripe, yellow bananas hanging from a wire, with a rustic, weathered look. The lighting is natural, casting soft shadows and highlighting the vibrant yellow color of the bananas.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24919.2, "ram_available_mb": 100852.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 24920.0, "ram_available_mb": 100852.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.19, "peak": 16.56, "min": 14.12}, "VDD_CPU_SOC_MSS": {"avg": 15.81, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 34.87, "peak": 45.69, "min": 26.39}, "VIN": {"avg": 73.28, "peak": 102.92, "min": 66.65}}, "power_watts_avg": 34.87, "energy_joules_est": 111.6, "sample_count": 25, "duration_seconds": 3.2}, "timestamp": "2026-01-17T15:58:19.434311"}
{"image_index": 440, "image_name": "000000050165.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050165.jpg", "image_width": 640, "image_height": 431, "image_resolution": "640x431", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1281.596, "latencies_ms": [1281.596], "images_per_second": 0.78, "prompt_tokens": 9, "response_tokens_est": 25, "n_tiles": 6, "output_text": "A vintage green and white train is traveling on a track through a rural landscape with green fields and a clear blue sky.", "error": null, "sys_before": {"cpu_percent": 36.6, "ram_used_mb": 24942.4, "ram_available_mb": 100829.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 24943.1, "ram_available_mb": 100829.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.59, "peak": 15.14, "min": 13.72}, "VDD_CPU_SOC_MSS": {"avg": 15.31, "peak": 16.14, "min": 14.17}, "VDD_GPU": {"avg": 34.8, "peak": 40.97, "min": 27.97}, "VIN": {"avg": 76.18, "peak": 107.38, "min": 61.43}}, "power_watts_avg": 34.8, "energy_joules_est": 44.61, "sample_count": 9, "duration_seconds": 1.282}, "timestamp": "2026-01-17T15:58:20.787351"}
{"image_index": 440, "image_name": "000000050165.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050165.jpg", "image_width": 640, "image_height": 431, "image_resolution": "640x431", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1735.921, "latencies_ms": [1735.921], "images_per_second": 0.576, "prompt_tokens": 23, "response_tokens_est": 40, "n_tiles": 6, "output_text": "object: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24943.1, "ram_available_mb": 100829.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 24945.1, "ram_available_mb": 100827.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.95, "peak": 15.34, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.56, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 32.99, "peak": 41.74, "min": 24.82}, "VIN": {"avg": 77.06, "peak": 104.78, "min": 58.17}}, "power_watts_avg": 32.99, "energy_joules_est": 57.28, "sample_count": 13, "duration_seconds": 1.736}, "timestamp": "2026-01-17T15:58:22.529244"}
{"image_index": 440, "image_name": "000000050165.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050165.jpg", "image_width": 640, "image_height": 431, "image_resolution": "640x431", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2552.455, "latencies_ms": [2552.455], "images_per_second": 0.392, "prompt_tokens": 27, "response_tokens_est": 70, "n_tiles": 6, "output_text": "The main object in the foreground is a green and white train car, which is positioned near the middle of the image. The train car is connected to a series of red freight cars that are positioned further back on the tracks. The background features a lush green field and a line of trees, while a distant mountain range is visible in the distance.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24945.1, "ram_available_mb": 100827.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 24946.3, "ram_available_mb": 100825.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.04, "peak": 15.44, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.63, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 29.37, "peak": 40.56, "min": 23.64}, "VIN": {"avg": 71.05, "peak": 108.04, "min": 58.66}}, "power_watts_avg": 29.37, "energy_joules_est": 74.98, "sample_count": 20, "duration_seconds": 2.553}, "timestamp": "2026-01-17T15:58:25.087860"}
{"image_index": 440, "image_name": "000000050165.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050165.jpg", "image_width": 640, "image_height": 431, "image_resolution": "640x431", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2824.352, "latencies_ms": [2824.352], "images_per_second": 0.354, "prompt_tokens": 21, "response_tokens_est": 80, "n_tiles": 6, "output_text": "The image depicts a serene countryside scene with a train traveling on a track through a lush green field. The train, which appears to be a vintage or historical model, is moving away from the viewer, with its red and green color scheme standing out against the greenery. The background features rolling hills and a clear blue sky, adding to the peaceful and picturesque atmosphere of the setting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24946.3, "ram_available_mb": 100825.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 24947.3, "ram_available_mb": 100824.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.07, "peak": 15.44, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.64, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 28.61, "peak": 40.16, "min": 23.25}, "VIN": {"avg": 71.92, "peak": 123.46, "min": 60.39}}, "power_watts_avg": 28.61, "energy_joules_est": 80.82, "sample_count": 22, "duration_seconds": 2.825}, "timestamp": "2026-01-17T15:58:27.918082"}
{"image_index": 440, "image_name": "000000050165.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050165.jpg", "image_width": 640, "image_height": 431, "image_resolution": "640x431", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2166.123, "latencies_ms": [2166.123], "images_per_second": 0.462, "prompt_tokens": 19, "response_tokens_est": 56, "n_tiles": 6, "output_text": "The image depicts a green and white train moving on a track through a rural landscape. The sky is clear with a few clouds, and the lighting suggests it is daytime. The train's color contrasts with the green grass and the blue sky, making it stand out.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24947.3, "ram_available_mb": 100824.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 24947.8, "ram_available_mb": 100824.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.03, "peak": 15.44, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.66, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 30.29, "peak": 40.97, "min": 23.64}, "VIN": {"avg": 73.63, "peak": 118.98, "min": 56.46}}, "power_watts_avg": 30.29, "energy_joules_est": 65.62, "sample_count": 17, "duration_seconds": 2.167}, "timestamp": "2026-01-17T15:58:30.090193"}
{"image_index": 441, "image_name": "000000050326.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050326.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2432.795, "latencies_ms": [2432.795], "images_per_second": 0.411, "prompt_tokens": 9, "response_tokens_est": 24, "n_tiles": 12, "output_text": "A man is standing on a sandy beach, holding a kite, with the ocean waves in the background.", "error": null, "sys_before": {"cpu_percent": 43.8, "ram_used_mb": 24916.3, "ram_available_mb": 100855.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.5, "ram_used_mb": 24917.4, "ram_available_mb": 100854.7, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.21, "peak": 16.66, "min": 13.59}, "VDD_CPU_SOC_MSS": {"avg": 15.97, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 36.26, "peak": 44.12, "min": 27.97}, "VIN": {"avg": 77.56, "peak": 106.41, "min": 65.26}}, "power_watts_avg": 36.26, "energy_joules_est": 88.23, "sample_count": 19, "duration_seconds": 2.433}, "timestamp": "2026-01-17T15:58:32.604417"}
{"image_index": 441, "image_name": "000000050326.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050326.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 3203.014, "latencies_ms": [3203.014], "images_per_second": 0.312, "prompt_tokens": 23, "response_tokens_est": 45, "n_tiles": 12, "output_text": "beach: 1\nman: 1\nhat: 1\nshorts: 1\nshirt: 1\nwristband: 1\ncamera: 1\nkite: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24917.4, "ram_available_mb": 100854.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 24918.2, "ram_available_mb": 100854.0, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.22, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.76, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 35.04, "peak": 45.28, "min": 26.4}, "VIN": {"avg": 77.57, "peak": 118.75, "min": 61.92}}, "power_watts_avg": 35.04, "energy_joules_est": 112.25, "sample_count": 25, "duration_seconds": 3.204}, "timestamp": "2026-01-17T15:58:35.813860"}
{"image_index": 441, "image_name": "000000050326.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050326.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 4976.845, "latencies_ms": [4976.845], "images_per_second": 0.201, "prompt_tokens": 27, "response_tokens_est": 97, "n_tiles": 12, "output_text": "The main object in the foreground is a person standing on the sandy beach. The person is wearing a black shirt, khaki shorts, and black shoes. The person is holding a white object in their right hand and appears to be looking towards the ocean. The background features the ocean with waves crashing onto the shore, and a green surfboard is partially visible. The person is standing near the edge of the beach, with the surfboard slightly to the right of the person.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24918.2, "ram_available_mb": 100854.0, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 24918.9, "ram_available_mb": 100853.3, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.14, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.57, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 31.73, "peak": 45.68, "min": 26.0}, "VIN": {"avg": 72.71, "peak": 113.33, "min": 58.85}}, "power_watts_avg": 31.73, "energy_joules_est": 157.92, "sample_count": 39, "duration_seconds": 4.977}, "timestamp": "2026-01-17T15:58:40.798081"}
{"image_index": 441, "image_name": "000000050326.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050326.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2846.803, "latencies_ms": [2846.803], "images_per_second": 0.351, "prompt_tokens": 21, "response_tokens_est": 35, "n_tiles": 12, "output_text": "The image depicts a person standing on a sandy beach with the ocean in the background. The person is holding a kite, which is flying in the air.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24918.9, "ram_available_mb": 100853.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 24919.4, "ram_available_mb": 100852.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.29, "peak": 16.56, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.86, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 36.3, "peak": 44.91, "min": 27.18}, "VIN": {"avg": 77.29, "peak": 112.02, "min": 66.2}}, "power_watts_avg": 36.3, "energy_joules_est": 103.35, "sample_count": 21, "duration_seconds": 2.847}, "timestamp": "2026-01-17T15:58:43.651568"}
{"image_index": 441, "image_name": "000000050326.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050326.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3840.155, "latencies_ms": [3840.155], "images_per_second": 0.26, "prompt_tokens": 19, "response_tokens_est": 64, "n_tiles": 12, "output_text": "The image features a person standing on a sandy beach with the ocean in the background. The person is wearing a black shirt, khaki shorts, and black sandals, and is holding a green and black surfboard. The sky is partly cloudy, and the lighting is bright, indicating a sunny day.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24919.4, "ram_available_mb": 100852.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 24920.1, "ram_available_mb": 100852.0, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.19, "peak": 16.56, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.71, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 33.57, "peak": 45.3, "min": 26.0}, "VIN": {"avg": 79.28, "peak": 131.18, "min": 61.34}}, "power_watts_avg": 33.57, "energy_joules_est": 128.92, "sample_count": 30, "duration_seconds": 3.84}, "timestamp": "2026-01-17T15:58:47.498144"}
{"image_index": 442, "image_name": "000000050331.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050331.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2765.137, "latencies_ms": [2765.137], "images_per_second": 0.362, "prompt_tokens": 9, "response_tokens_est": 34, "n_tiles": 12, "output_text": "The image shows a close-up of a plant with large, dark green leaves and a cluster of small, green broccoli florets growing in a red pot.", "error": null, "sys_before": {"cpu_percent": 40.0, "ram_used_mb": 24922.0, "ram_available_mb": 100850.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 24923.2, "ram_available_mb": 100849.0, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.19, "peak": 16.66, "min": 13.39}, "VDD_CPU_SOC_MSS": {"avg": 15.8, "peak": 17.71, "min": 14.17}, "VDD_GPU": {"avg": 35.45, "peak": 44.12, "min": 26.79}, "VIN": {"avg": 79.83, "peak": 127.24, "min": 65.94}}, "power_watts_avg": 35.45, "energy_joules_est": 98.03, "sample_count": 21, "duration_seconds": 2.765}, "timestamp": "2026-01-17T15:58:50.359509"}
{"image_index": 442, "image_name": "000000050331.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050331.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2287.24, "latencies_ms": [2287.24], "images_per_second": 0.437, "prompt_tokens": 23, "response_tokens_est": 18, "n_tiles": 12, "output_text": "broccoli: 1\npot: 1\npotting soil: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24923.2, "ram_available_mb": 100849.0, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.3, "ram_used_mb": 24923.5, "ram_available_mb": 100848.7, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.31, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.84, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 38.59, "peak": 45.28, "min": 30.33}, "VIN": {"avg": 88.74, "peak": 139.42, "min": 66.38}}, "power_watts_avg": 38.59, "energy_joules_est": 88.28, "sample_count": 17, "duration_seconds": 2.288}, "timestamp": "2026-01-17T15:58:52.654457"}
{"image_index": 442, "image_name": "000000050331.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050331.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 4597.137, "latencies_ms": [4597.137], "images_per_second": 0.218, "prompt_tokens": 27, "response_tokens_est": 86, "n_tiles": 12, "output_text": "The main object in the foreground is a large, leafy green vegetable, likely a type of kale or collard greens, with a rich, dark green color. The vegetable is planted in a terracotta pot, which is placed on a dirt surface. In the background, there is another similar plant, also in a terracotta pot, indicating that these plants are part of a larger garden or farm setting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24923.5, "ram_available_mb": 100848.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 24923.7, "ram_available_mb": 100848.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.18, "peak": 16.56, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.6, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 32.62, "peak": 46.46, "min": 26.0}, "VIN": {"avg": 74.59, "peak": 130.08, "min": 58.23}}, "power_watts_avg": 32.62, "energy_joules_est": 149.99, "sample_count": 36, "duration_seconds": 4.598}, "timestamp": "2026-01-17T15:58:57.258126"}
{"image_index": 442, "image_name": "000000050331.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050331.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3557.025, "latencies_ms": [3557.025], "images_per_second": 0.281, "prompt_tokens": 21, "response_tokens_est": 55, "n_tiles": 12, "output_text": "The image depicts a garden scene with a variety of leafy greens, including broccoli and kale, growing in terracotta pots. The setting appears to be outdoors, with the soil and plants visible, suggesting a well-maintained and cared-for garden.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24923.7, "ram_available_mb": 100848.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 24923.9, "ram_available_mb": 100848.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.14, "peak": 16.66, "min": 14.12}, "VDD_CPU_SOC_MSS": {"avg": 15.63, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 33.98, "peak": 45.69, "min": 25.61}, "VIN": {"avg": 78.82, "peak": 119.6, "min": 66.13}}, "power_watts_avg": 33.98, "energy_joules_est": 120.89, "sample_count": 28, "duration_seconds": 3.558}, "timestamp": "2026-01-17T15:59:00.821780"}
{"image_index": 442, "image_name": "000000050331.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050331.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3477.53, "latencies_ms": [3477.53], "images_per_second": 0.288, "prompt_tokens": 19, "response_tokens_est": 53, "n_tiles": 12, "output_text": "The image showcases a vibrant garden bed with a variety of leafy greens, including a large, leafy broccoli head and other leafy vegetables. The scene is illuminated by natural daylight, casting soft shadows and highlighting the rich colors of the plants.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24923.9, "ram_available_mb": 100848.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 24924.7, "ram_available_mb": 100847.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.93, "peak": 16.26, "min": 14.02}, "VDD_CPU_SOC_MSS": {"avg": 15.48, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 34.21, "peak": 45.69, "min": 25.6}, "VIN": {"avg": 75.35, "peak": 133.28, "min": 54.95}}, "power_watts_avg": 34.21, "energy_joules_est": 118.98, "sample_count": 27, "duration_seconds": 3.478}, "timestamp": "2026-01-17T15:59:04.305719"}
{"image_index": 443, "image_name": "000000050380.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050380.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1221.064, "latencies_ms": [1221.064], "images_per_second": 0.819, "prompt_tokens": 9, "response_tokens_est": 23, "n_tiles": 6, "output_text": "An elderly man is holding a young boy on a horse, both looking down at the horse's reins.", "error": null, "sys_before": {"cpu_percent": 35.6, "ram_used_mb": 24954.3, "ram_available_mb": 100817.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 24955.2, "ram_available_mb": 100816.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.55, "peak": 15.04, "min": 13.61}, "VDD_CPU_SOC_MSS": {"avg": 15.18, "peak": 15.75, "min": 14.18}, "VDD_GPU": {"avg": 34.88, "peak": 40.56, "min": 27.97}, "VIN": {"avg": 75.56, "peak": 107.27, "min": 62.7}}, "power_watts_avg": 34.88, "energy_joules_est": 42.6, "sample_count": 9, "duration_seconds": 1.221}, "timestamp": "2026-01-17T15:59:05.585059"}
{"image_index": 443, "image_name": "000000050380.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050380.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1696.185, "latencies_ms": [1696.185], "images_per_second": 0.59, "prompt_tokens": 23, "response_tokens_est": 39, "n_tiles": 6, "output_text": "1. Man\n2. Boy\n3. Horse\n4. Pony\n5. Ponytail\n6. Shirt\n7. Pants\n8. Lifeguard", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24955.2, "ram_available_mb": 100816.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 24956.7, "ram_available_mb": 100815.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.03, "peak": 15.54, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.59, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 32.96, "peak": 41.34, "min": 24.82}, "VIN": {"avg": 74.38, "peak": 117.84, "min": 54.64}}, "power_watts_avg": 32.96, "energy_joules_est": 55.92, "sample_count": 13, "duration_seconds": 1.697}, "timestamp": "2026-01-17T15:59:07.289518"}
{"image_index": 443, "image_name": "000000050380.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050380.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2627.294, "latencies_ms": [2627.294], "images_per_second": 0.381, "prompt_tokens": 27, "response_tokens_est": 73, "n_tiles": 6, "output_text": "In the image, the man is standing in the foreground, holding the reins of the horse. The horse is in the middle ground, with the woman seated on it. The woman is near the man, and the horse is further back. The background features a building with a red facade and a white door, along with a window and a small lamp.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24956.7, "ram_available_mb": 100815.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 24957.0, "ram_available_mb": 100815.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.08, "peak": 15.44, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.67, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 29.41, "peak": 40.57, "min": 23.64}, "VIN": {"avg": 71.49, "peak": 122.22, "min": 57.88}}, "power_watts_avg": 29.41, "energy_joules_est": 77.28, "sample_count": 20, "duration_seconds": 2.628}, "timestamp": "2026-01-17T15:59:09.922529"}
{"image_index": 443, "image_name": "000000050380.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050380.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2597.037, "latencies_ms": [2597.037], "images_per_second": 0.385, "prompt_tokens": 21, "response_tokens_est": 72, "n_tiles": 6, "output_text": "The image depicts an outdoor scene with an elderly man and a young boy standing beside a small, light-colored horse. The man is holding the horse's reins, while the boy is looking down at the horse. The setting appears to be a quaint, possibly rural area, with a red building and a cobblestone path in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24957.0, "ram_available_mb": 100815.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 24956.7, "ram_available_mb": 100815.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.1, "peak": 15.54, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.64, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 29.48, "peak": 40.16, "min": 23.64}, "VIN": {"avg": 70.77, "peak": 103.61, "min": 58.35}}, "power_watts_avg": 29.48, "energy_joules_est": 76.57, "sample_count": 19, "duration_seconds": 2.597}, "timestamp": "2026-01-17T15:59:12.525559"}
{"image_index": 443, "image_name": "000000050380.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050380.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2669.181, "latencies_ms": [2669.181], "images_per_second": 0.375, "prompt_tokens": 19, "response_tokens_est": 75, "n_tiles": 6, "output_text": "The image features a man and a young boy standing beside a horse. The man is wearing a blue polo shirt and dark pants, while the boy is in a gray shirt and blue jeans. The horse is brown with a white patch on its face, and it is wearing a red halter. The scene is set outdoors during the daytime under clear, bright lighting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24956.7, "ram_available_mb": 100815.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 24957.4, "ram_available_mb": 100814.7, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.09, "peak": 15.44, "min": 14.43}, "VDD_CPU_SOC_MSS": {"avg": 15.65, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 29.15, "peak": 40.95, "min": 23.64}, "VIN": {"avg": 71.0, "peak": 109.31, "min": 59.37}}, "power_watts_avg": 29.15, "energy_joules_est": 77.82, "sample_count": 21, "duration_seconds": 2.67}, "timestamp": "2026-01-17T15:59:15.200974"}
{"image_index": 444, "image_name": "000000050638.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050638.jpg", "image_width": 640, "image_height": 393, "image_resolution": "640x393", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1273.42, "latencies_ms": [1273.42], "images_per_second": 0.785, "prompt_tokens": 9, "response_tokens_est": 25, "n_tiles": 6, "output_text": "A young child is walking through a field of tall grass and blue flowers, holding a brown stuffed animal in their hands.", "error": null, "sys_before": {"cpu_percent": 34.5, "ram_used_mb": 24957.4, "ram_available_mb": 100814.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 24957.7, "ram_available_mb": 100814.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.72, "peak": 15.24, "min": 14.0}, "VDD_CPU_SOC_MSS": {"avg": 15.4, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 34.4, "peak": 40.16, "min": 27.97}, "VIN": {"avg": 77.37, "peak": 105.45, "min": 62.86}}, "power_watts_avg": 34.4, "energy_joules_est": 43.81, "sample_count": 9, "duration_seconds": 1.274}, "timestamp": "2026-01-17T15:59:16.523866"}
{"image_index": 444, "image_name": "000000050638.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050638.jpg", "image_width": 640, "image_height": 393, "image_resolution": "640x393", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2104.904, "latencies_ms": [2104.904], "images_per_second": 0.475, "prompt_tokens": 23, "response_tokens_est": 54, "n_tiles": 6, "output_text": "- child: 1\n- teddy bear: 1\n- blue flowers: 1\n- green grass: 1\n- dirt path: 1\n- tree: 1\n- bushes: 1\n- sky: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24957.7, "ram_available_mb": 100814.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 24957.7, "ram_available_mb": 100814.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.04, "peak": 15.44, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.6, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 31.42, "peak": 41.76, "min": 24.04}, "VIN": {"avg": 70.56, "peak": 110.2, "min": 62.2}}, "power_watts_avg": 31.42, "energy_joules_est": 66.14, "sample_count": 16, "duration_seconds": 2.105}, "timestamp": "2026-01-17T15:59:18.635958"}
{"image_index": 444, "image_name": "000000050638.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050638.jpg", "image_width": 640, "image_height": 393, "image_resolution": "640x393", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2153.107, "latencies_ms": [2153.107], "images_per_second": 0.464, "prompt_tokens": 27, "response_tokens_est": 56, "n_tiles": 6, "output_text": "The main object in the foreground is a young child holding a brown teddy bear. The child is standing on a dirt path surrounded by lush green grass and tall purple flowers. The background features a blurred field of flowers and greenery, indicating a natural, outdoor setting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24957.7, "ram_available_mb": 100814.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 24957.7, "ram_available_mb": 100814.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.09, "peak": 15.44, "min": 14.4}, "VDD_CPU_SOC_MSS": {"avg": 15.63, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 30.61, "peak": 40.97, "min": 24.03}, "VIN": {"avg": 71.93, "peak": 110.85, "min": 50.97}}, "power_watts_avg": 30.61, "energy_joules_est": 65.92, "sample_count": 17, "duration_seconds": 2.154}, "timestamp": "2026-01-17T15:59:20.795054"}
{"image_index": 444, "image_name": "000000050638.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050638.jpg", "image_width": 640, "image_height": 393, "image_resolution": "640x393", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2155.185, "latencies_ms": [2155.185], "images_per_second": 0.464, "prompt_tokens": 21, "response_tokens_est": 56, "n_tiles": 6, "output_text": "The image depicts a young child, likely a boy, standing in a lush, green field filled with tall grass and purple flowers. The child is holding a brown teddy bear in his arms, and the setting appears to be a park or a garden during the daytime.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24957.7, "ram_available_mb": 100814.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 24957.9, "ram_available_mb": 100814.3, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.09, "peak": 15.44, "min": 14.43}, "VDD_CPU_SOC_MSS": {"avg": 15.65, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 30.49, "peak": 40.95, "min": 24.03}, "VIN": {"avg": 73.54, "peak": 113.45, "min": 55.5}}, "power_watts_avg": 30.49, "energy_joules_est": 65.73, "sample_count": 17, "duration_seconds": 2.156}, "timestamp": "2026-01-17T15:59:22.957312"}
{"image_index": 444, "image_name": "000000050638.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050638.jpg", "image_width": 640, "image_height": 393, "image_resolution": "640x393", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2297.104, "latencies_ms": [2297.104], "images_per_second": 0.435, "prompt_tokens": 19, "response_tokens_est": 61, "n_tiles": 6, "output_text": "The image depicts a young child with light-colored hair, wearing a blue and white striped shirt and blue jeans. The child is holding a brown teddy bear. The scene is set in a lush, green environment with sunlight filtering through the foliage, creating a warm and inviting atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24957.9, "ram_available_mb": 100814.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 24958.9, "ram_available_mb": 100813.3, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.07, "peak": 15.44, "min": 14.43}, "VDD_CPU_SOC_MSS": {"avg": 15.66, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 30.4, "peak": 40.57, "min": 23.64}, "VIN": {"avg": 69.22, "peak": 101.91, "min": 61.62}}, "power_watts_avg": 30.4, "energy_joules_est": 69.84, "sample_count": 17, "duration_seconds": 2.297}, "timestamp": "2026-01-17T15:59:25.264327"}
{"image_index": 445, "image_name": "000000050679.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050679.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1226.26, "latencies_ms": [1226.26], "images_per_second": 0.815, "prompt_tokens": 9, "response_tokens_est": 23, "n_tiles": 6, "output_text": "A large, ripe orange lies on the asphalt, surrounded by a blurry background of parked cars and trees.", "error": null, "sys_before": {"cpu_percent": 40.0, "ram_used_mb": 24958.9, "ram_available_mb": 100813.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 24958.9, "ram_available_mb": 100813.3, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.62, "peak": 15.14, "min": 13.79}, "VDD_CPU_SOC_MSS": {"avg": 15.35, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 34.09, "peak": 40.16, "min": 27.59}, "VIN": {"avg": 79.08, "peak": 110.64, "min": 61.35}}, "power_watts_avg": 34.09, "energy_joules_est": 41.81, "sample_count": 9, "duration_seconds": 1.227}, "timestamp": "2026-01-17T15:59:26.549714"}
{"image_index": 445, "image_name": "000000050679.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050679.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1513.852, "latencies_ms": [1513.852], "images_per_second": 0.661, "prompt_tokens": 23, "response_tokens_est": 32, "n_tiles": 6, "output_text": "1. Orange\n2. Car\n3. Car\n4. Car\n5. Car\n6. Car\n7. Car\n8. Car", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24958.9, "ram_available_mb": 100813.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 24958.9, "ram_available_mb": 100813.3, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.96, "peak": 15.44, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.57, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 34.3, "peak": 41.34, "min": 26.0}, "VIN": {"avg": 74.23, "peak": 108.21, "min": 62.52}}, "power_watts_avg": 34.3, "energy_joules_est": 51.94, "sample_count": 11, "duration_seconds": 1.514}, "timestamp": "2026-01-17T15:59:28.070654"}
{"image_index": 445, "image_name": "000000050679.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050679.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2732.132, "latencies_ms": [2732.132], "images_per_second": 0.366, "prompt_tokens": 27, "response_tokens_est": 77, "n_tiles": 6, "output_text": "The main object in the foreground is an orange lying on the road. The orange is positioned near the center of the image, slightly to the left. In the background, there are several cars parked along the side of the road. The cars are parked in a line, extending towards the horizon. The orange is the closest object to the camera, while the cars are further away.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24958.9, "ram_available_mb": 100813.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 24958.9, "ram_available_mb": 100813.3, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.08, "peak": 15.44, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.67, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 29.3, "peak": 41.36, "min": 23.64}, "VIN": {"avg": 73.26, "peak": 103.48, "min": 63.69}}, "power_watts_avg": 29.3, "energy_joules_est": 80.07, "sample_count": 21, "duration_seconds": 2.733}, "timestamp": "2026-01-17T15:59:30.811096"}
{"image_index": 445, "image_name": "000000050679.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050679.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2672.739, "latencies_ms": [2672.739], "images_per_second": 0.374, "prompt_tokens": 21, "response_tokens_est": 75, "n_tiles": 6, "output_text": "The image depicts a scene on a city street with a large, ripe orange lying on the asphalt. The orange is positioned in the foreground, while a line of parked cars can be seen in the background, suggesting a parking lot or a street with vehicles. The setting appears to be during the daytime, with overcast skies and a few trees visible in the distance.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24958.9, "ram_available_mb": 100813.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 24959.8, "ram_available_mb": 100812.3, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.09, "peak": 15.54, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.69, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 29.21, "peak": 40.18, "min": 23.64}, "VIN": {"avg": 72.6, "peak": 123.01, "min": 57.55}}, "power_watts_avg": 29.21, "energy_joules_est": 78.08, "sample_count": 20, "duration_seconds": 2.673}, "timestamp": "2026-01-17T15:59:33.489925"}
{"image_index": 445, "image_name": "000000050679.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050679.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2401.604, "latencies_ms": [2401.604], "images_per_second": 0.416, "prompt_tokens": 19, "response_tokens_est": 65, "n_tiles": 6, "output_text": "The image features a large, ripe orange lying on the asphalt road. The orange has a rich, vibrant orange color, with a slightly textured surface that suggests it may be slightly weathered. The lighting in the image is natural, with soft, diffused light illuminating the scene, indicating it might be daytime.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24959.8, "ram_available_mb": 100812.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 24960.1, "ram_available_mb": 100812.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.09, "peak": 15.44, "min": 14.43}, "VDD_CPU_SOC_MSS": {"avg": 15.64, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 29.98, "peak": 40.57, "min": 23.64}, "VIN": {"avg": 70.22, "peak": 93.72, "min": 54.46}}, "power_watts_avg": 29.98, "energy_joules_est": 72.01, "sample_count": 18, "duration_seconds": 2.402}, "timestamp": "2026-01-17T15:59:35.897696"}
{"image_index": 446, "image_name": "000000050811.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050811.jpg", "image_width": 640, "image_height": 605, "image_resolution": "640x605", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1612.78, "latencies_ms": [1612.78], "images_per_second": 0.62, "prompt_tokens": 9, "response_tokens_est": 70, "n_tiles": 1, "output_text": "A man in a gray suit and white shirt sits at a wooden table, smiling at the camera, with a white bowl containing a white substance on the table in front of him. On the table, there are two bottles of beer, one with a brown label and the other with a clear label, and a pair of keys on the table.", "error": null, "sys_before": {"cpu_percent": 29.0, "ram_used_mb": 24960.1, "ram_available_mb": 100812.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24959.8, "ram_available_mb": 100812.3, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 4845.7, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.44, "peak": 15.82, "min": 14.71}, "VDD_CPU_SOC_MSS": {"avg": 16.24, "peak": 16.54, "min": 15.36}, "VDD_GPU": {"avg": 21.61, "peak": 26.01, "min": 20.09}, "VIN": {"avg": 63.29, "peak": 66.4, "min": 54.49}}, "power_watts_avg": 21.61, "energy_joules_est": 34.86, "sample_count": 12, "duration_seconds": 1.613}, "timestamp": "2026-01-17T15:59:37.534720"}
{"image_index": 446, "image_name": "000000050811.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050811.jpg", "image_width": 640, "image_height": 605, "image_resolution": "640x605", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2876.457, "latencies_ms": [2876.457], "images_per_second": 0.348, "prompt_tokens": 23, "response_tokens_est": 129, "n_tiles": 1, "output_text": "- man: 1\n- suit: 1\n- tie: 1\n- shirt: 1\n- blazer: 1\n- pocket watch: 1\n- watch: 1\n- glass: 1\n- bowl: 1\n- bottle: 2\n- bottle cap: 1\n- bottle neck: 1\n- bottle label: 1\n- bottle cap: 1\n- bottle neck: 1\n- bottle cap: 1\n- bottle neck: 1\n- bottle neck: 1\n- bottle neck: 1\n- bottle neck", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24959.8, "ram_available_mb": 100812.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24960.3, "ram_available_mb": 100811.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 4856.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.68, "peak": 15.82, "min": 15.42}, "VDD_CPU_SOC_MSS": {"avg": 16.75, "peak": 16.94, "min": 16.14}, "VDD_GPU": {"avg": 20.58, "peak": 23.64, "min": 19.7}, "VIN": {"avg": 63.89, "peak": 69.33, "min": 60.1}}, "power_watts_avg": 20.58, "energy_joules_est": 59.2, "sample_count": 22, "duration_seconds": 2.877}, "timestamp": "2026-01-17T15:59:40.416915"}
{"image_index": 446, "image_name": "000000050811.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050811.jpg", "image_width": 640, "image_height": 605, "image_resolution": "640x605", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1686.232, "latencies_ms": [1686.232], "images_per_second": 0.593, "prompt_tokens": 27, "response_tokens_est": 74, "n_tiles": 1, "output_text": "The main object in the foreground is a white bowl containing a small white object, possibly a piece of food or a decorative item. To the right of the bowl, there are two bottles of beer, one of which is partially visible and the other is fully visible. The background is a plain wall, and there is a wooden surface that the man is sitting on.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24960.3, "ram_available_mb": 100811.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24960.3, "ram_available_mb": 100811.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 4859.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.57, "peak": 15.82, "min": 15.22}, "VDD_CPU_SOC_MSS": {"avg": 16.64, "peak": 16.93, "min": 16.54}, "VDD_GPU": {"avg": 21.05, "peak": 24.03, "min": 20.1}, "VIN": {"avg": 63.58, "peak": 65.9, "min": 56.45}}, "power_watts_avg": 21.05, "energy_joules_est": 35.5, "sample_count": 12, "duration_seconds": 1.687}, "timestamp": "2026-01-17T15:59:42.108693"}
{"image_index": 446, "image_name": "000000050811.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050811.jpg", "image_width": 640, "image_height": 605, "image_resolution": "640x605", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1738.525, "latencies_ms": [1738.525], "images_per_second": 0.575, "prompt_tokens": 21, "response_tokens_est": 76, "n_tiles": 1, "output_text": "The image depicts a man sitting at a table in a room with a plain white wall and wooden paneling. He is dressed in a gray suit and tie, with a white shirt underneath. On the table, there are two bottles of beer, a white bowl with a spoon, and a few keys. The man is smiling and appears to be in a relaxed setting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24960.3, "ram_available_mb": 100811.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24960.1, "ram_available_mb": 100812.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 4854.7, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.57, "peak": 15.82, "min": 15.11}, "VDD_CPU_SOC_MSS": {"avg": 16.6, "peak": 16.94, "min": 16.14}, "VDD_GPU": {"avg": 20.88, "peak": 23.64, "min": 20.09}, "VIN": {"avg": 64.35, "peak": 71.7, "min": 61.03}}, "power_watts_avg": 20.88, "energy_joules_est": 36.31, "sample_count": 13, "duration_seconds": 1.739}, "timestamp": "2026-01-17T15:59:43.853141"}
{"image_index": 446, "image_name": "000000050811.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050811.jpg", "image_width": 640, "image_height": 605, "image_resolution": "640x605", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1122.352, "latencies_ms": [1122.352], "images_per_second": 0.891, "prompt_tokens": 19, "response_tokens_est": 48, "n_tiles": 1, "output_text": "The man is wearing a gray suit with a white shirt and a light blue tie. The background is plain and white, with a wooden table and a white bowl on it. The lighting is soft and even, creating a calm atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24960.1, "ram_available_mb": 100812.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24961.1, "ram_available_mb": 100811.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 4853.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.51, "peak": 15.62, "min": 15.32}, "VDD_CPU_SOC_MSS": {"avg": 16.44, "peak": 16.54, "min": 16.14}, "VDD_GPU": {"avg": 21.48, "peak": 23.64, "min": 20.1}, "VIN": {"avg": 63.25, "peak": 71.73, "min": 55.66}}, "power_watts_avg": 21.48, "energy_joules_est": 24.11, "sample_count": 8, "duration_seconds": 1.123}, "timestamp": "2026-01-17T15:59:44.980975"}
{"image_index": 447, "image_name": "000000050828.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050828.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 3360.996, "latencies_ms": [3360.996], "images_per_second": 0.298, "prompt_tokens": 9, "response_tokens_est": 52, "n_tiles": 12, "output_text": "The image shows a neatly made bed with white linens and neatly stacked pillows, set against a light-colored wall with a small window above it, and a chair and a small table with a phone and some items on it in the background.", "error": null, "sys_before": {"cpu_percent": 45.8, "ram_used_mb": 24929.8, "ram_available_mb": 100842.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 24930.5, "ram_available_mb": 100841.7, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.23, "peak": 16.76, "min": 13.9}, "VDD_CPU_SOC_MSS": {"avg": 15.93, "peak": 17.71, "min": 14.96}, "VDD_GPU": {"avg": 33.38, "peak": 42.92, "min": 26.0}, "VIN": {"avg": 73.04, "peak": 103.57, "min": 55.79}}, "power_watts_avg": 33.38, "energy_joules_est": 112.2, "sample_count": 26, "duration_seconds": 3.361}, "timestamp": "2026-01-17T15:59:48.433603"}
{"image_index": 447, "image_name": "000000050828.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050828.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 3295.489, "latencies_ms": [3295.489], "images_per_second": 0.303, "prompt_tokens": 23, "response_tokens_est": 48, "n_tiles": 12, "output_text": "bed: 1\npillows: 4\ntowels: 2\ncouch: 1\ndrawer: 1\ndrawer: 1\ndrawer: 1\ndrawer: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24930.5, "ram_available_mb": 100841.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 24930.7, "ram_available_mb": 100841.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.22, "peak": 16.56, "min": 14.12}, "VDD_CPU_SOC_MSS": {"avg": 15.79, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 34.91, "peak": 45.3, "min": 26.39}, "VIN": {"avg": 77.63, "peak": 115.07, "min": 55.16}}, "power_watts_avg": 34.91, "energy_joules_est": 115.06, "sample_count": 25, "duration_seconds": 3.296}, "timestamp": "2026-01-17T15:59:51.735776"}
{"image_index": 447, "image_name": "000000050828.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050828.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 4897.655, "latencies_ms": [4897.655], "images_per_second": 0.204, "prompt_tokens": 27, "response_tokens_est": 95, "n_tiles": 12, "output_text": "The main objects in the image are a neatly made bed with white linens and pillows, positioned in the foreground. The bed is placed against a light-colored wall, and there is a small table with a chair to the left of the bed. The table holds various items, including a phone, a small black bag, and some papers. The room appears to be well-lit, with natural light coming from a window on the right side of the image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24930.7, "ram_available_mb": 100841.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 24930.7, "ram_available_mb": 100841.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.17, "peak": 16.56, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.6, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 31.86, "peak": 45.28, "min": 26.0}, "VIN": {"avg": 73.03, "peak": 112.18, "min": 61.77}}, "power_watts_avg": 31.86, "energy_joules_est": 156.05, "sample_count": 38, "duration_seconds": 4.898}, "timestamp": "2026-01-17T15:59:56.639703"}
{"image_index": 447, "image_name": "000000050828.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050828.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 4265.663, "latencies_ms": [4265.663], "images_per_second": 0.234, "prompt_tokens": 21, "response_tokens_est": 76, "n_tiles": 12, "output_text": "The image depicts a neatly arranged hotel room with a white bed and white pillows. The room is well-lit, with a window providing natural light. The bed is neatly made with white sheets and pillows, and there is a small table with a phone and some items on it. The room appears clean and tidy, suggesting a comfortable and relaxing environment.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24931.0, "ram_available_mb": 100841.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 24931.9, "ram_available_mb": 100840.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.16, "peak": 16.56, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.62, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 32.62, "peak": 44.91, "min": 25.6}, "VIN": {"avg": 76.92, "peak": 132.93, "min": 54.98}}, "power_watts_avg": 32.62, "energy_joules_est": 139.16, "sample_count": 32, "duration_seconds": 4.266}, "timestamp": "2026-01-17T16:00:00.912283"}
{"image_index": 447, "image_name": "000000050828.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050828.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3436.034, "latencies_ms": [3436.034], "images_per_second": 0.291, "prompt_tokens": 19, "response_tokens_est": 52, "n_tiles": 12, "output_text": "The image depicts a neatly made bed with white linens and pillows, set against a light-colored wall. The room is well-lit, with natural light coming from a window on the right side, creating a bright and airy atmosphere.", "error": null, "sys_before": {"cpu_percent": 40.0, "ram_used_mb": 24931.9, "ram_available_mb": 100840.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 24932.2, "ram_available_mb": 100840.0, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.95, "peak": 16.26, "min": 14.02}, "VDD_CPU_SOC_MSS": {"avg": 15.46, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 34.42, "peak": 44.91, "min": 25.6}, "VIN": {"avg": 79.2, "peak": 136.71, "min": 51.88}}, "power_watts_avg": 34.42, "energy_joules_est": 118.28, "sample_count": 26, "duration_seconds": 3.436}, "timestamp": "2026-01-17T16:00:04.355066"}
{"image_index": 448, "image_name": "000000050844.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050844.jpg", "image_width": 334, "image_height": 500, "image_resolution": "334x500", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2066.412, "latencies_ms": [2066.412], "images_per_second": 0.484, "prompt_tokens": 9, "response_tokens_est": 54, "n_tiles": 6, "output_text": "The image features a collection of plush toys, including a teddy bear wearing a green hat, a teddy bear wearing a red scarf, and a snowman wearing a green hat, all of which are adorned with the Coca-Cola logo.", "error": null, "sys_before": {"cpu_percent": 35.9, "ram_used_mb": 24949.9, "ram_available_mb": 100822.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 24951.1, "ram_available_mb": 100821.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.76, "peak": 15.11, "min": 13.49}, "VDD_CPU_SOC_MSS": {"avg": 15.43, "peak": 15.75, "min": 14.17}, "VDD_GPU": {"avg": 30.33, "peak": 40.56, "min": 23.64}, "VIN": {"avg": 72.54, "peak": 112.08, "min": 57.02}}, "power_watts_avg": 30.33, "energy_joules_est": 62.68, "sample_count": 16, "duration_seconds": 2.067}, "timestamp": "2026-01-17T16:00:06.483574"}
{"image_index": 448, "image_name": "000000050844.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050844.jpg", "image_width": 334, "image_height": 500, "image_resolution": "334x500", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2594.627, "latencies_ms": [2594.627], "images_per_second": 0.385, "prompt_tokens": 23, "response_tokens_est": 72, "n_tiles": 6, "output_text": "1. teddy bear: 1\n2. teddy bear: 1\n3. teddy bear: 1\n4. teddy bear: 1\n5. teddy bear: 1\n6. teddy bear: 1\n7. teddy bear: 1\n8. teddy bear: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24951.1, "ram_available_mb": 100821.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 24951.6, "ram_available_mb": 100820.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.09, "peak": 15.54, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.69, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 29.27, "peak": 40.16, "min": 23.64}, "VIN": {"avg": 68.6, "peak": 85.84, "min": 59.51}}, "power_watts_avg": 29.27, "energy_joules_est": 75.96, "sample_count": 20, "duration_seconds": 2.595}, "timestamp": "2026-01-17T16:00:09.084384"}
{"image_index": 448, "image_name": "000000050844.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050844.jpg", "image_width": 334, "image_height": 500, "image_resolution": "334x500", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2511.167, "latencies_ms": [2511.167], "images_per_second": 0.398, "prompt_tokens": 27, "response_tokens_est": 69, "n_tiles": 6, "output_text": "The main objects in the image are a teddy bear, a snowman, and a bottle. The teddy bear is positioned in the foreground, slightly to the left, with the snowman and the bottle in the background. The teddy bear is the closest to the viewer, while the snowman and the bottle are further away.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24951.6, "ram_available_mb": 100820.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 24951.8, "ram_available_mb": 100820.3, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.09, "peak": 15.44, "min": 14.43}, "VDD_CPU_SOC_MSS": {"avg": 15.64, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 29.54, "peak": 40.57, "min": 23.64}, "VIN": {"avg": 67.95, "peak": 85.58, "min": 51.07}}, "power_watts_avg": 29.54, "energy_joules_est": 74.2, "sample_count": 19, "duration_seconds": 2.512}, "timestamp": "2026-01-17T16:00:11.601491"}
{"image_index": 448, "image_name": "000000050844.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050844.jpg", "image_width": 334, "image_height": 500, "image_resolution": "334x500", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2753.071, "latencies_ms": [2753.071], "images_per_second": 0.363, "prompt_tokens": 21, "response_tokens_est": 78, "n_tiles": 6, "output_text": "The image depicts a cozy indoor scene featuring a plush teddy bear wearing a green hat and a red scarf, sitting on a blue fabric surface. The bear is holding a bottle in its paw, and beside it, there is another plush bear wearing a Santa hat and a red scarf. The background includes a patterned orange fabric and a white snowman.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24951.8, "ram_available_mb": 100820.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 24952.8, "ram_available_mb": 100819.3, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.11, "peak": 15.54, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.67, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 28.85, "peak": 40.16, "min": 23.24}, "VIN": {"avg": 70.68, "peak": 103.44, "min": 63.02}}, "power_watts_avg": 28.85, "energy_joules_est": 79.43, "sample_count": 21, "duration_seconds": 2.753}, "timestamp": "2026-01-17T16:00:14.360717"}
{"image_index": 448, "image_name": "000000050844.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050844.jpg", "image_width": 334, "image_height": 500, "image_resolution": "334x500", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1856.505, "latencies_ms": [1856.505], "images_per_second": 0.539, "prompt_tokens": 19, "response_tokens_est": 45, "n_tiles": 6, "output_text": "The image features a teddy bear dressed in a green hat and a red scarf, standing against a blue background. The lighting is soft and warm, casting gentle shadows that enhance the texture of the bear's fur.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24952.8, "ram_available_mb": 100819.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 24953.6, "ram_available_mb": 100818.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.04, "peak": 15.54, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.64, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 31.54, "peak": 40.16, "min": 24.42}, "VIN": {"avg": 70.43, "peak": 110.69, "min": 56.12}}, "power_watts_avg": 31.54, "energy_joules_est": 58.56, "sample_count": 14, "duration_seconds": 1.857}, "timestamp": "2026-01-17T16:00:16.222984"}
{"image_index": 449, "image_name": "000000050896.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050896.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1018.158, "latencies_ms": [1018.158], "images_per_second": 0.982, "prompt_tokens": 9, "response_tokens_est": 26, "n_tiles": 4, "output_text": "The image shows a clear glass bowl containing several ripe, orange fruits, likely oranges, placed on a textured surface.", "error": null, "sys_before": {"cpu_percent": 25.5, "ram_used_mb": 24953.6, "ram_available_mb": 100818.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 24953.8, "ram_available_mb": 100818.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4606.7, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5444.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.61, "peak": 15.01, "min": 14.2}, "VDD_CPU_SOC_MSS": {"avg": 15.41, "peak": 15.75, "min": 14.96}, "VDD_GPU": {"avg": 31.68, "peak": 37.03, "min": 26.4}, "VIN": {"avg": 70.16, "peak": 89.55, "min": 62.92}}, "power_watts_avg": 31.68, "energy_joules_est": 32.26, "sample_count": 7, "duration_seconds": 1.018}, "timestamp": "2026-01-17T16:00:17.286435"}
{"image_index": 449, "image_name": "000000050896.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050896.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 538.429, "latencies_ms": [538.429], "images_per_second": 1.857, "prompt_tokens": 23, "response_tokens_est": 6, "n_tiles": 4, "output_text": "oranges: 8", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24953.8, "ram_available_mb": 100818.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 24953.8, "ram_available_mb": 100818.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4606.7, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5455.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.55, "peak": 14.73, "min": 14.4}, "VDD_CPU_SOC_MSS": {"avg": 15.22, "peak": 15.36, "min": 14.96}, "VDD_GPU": {"avg": 36.37, "peak": 37.82, "min": 33.48}, "VIN": {"avg": 86.29, "peak": 110.72, "min": 46.47}}, "power_watts_avg": 36.37, "energy_joules_est": 19.6, "sample_count": 3, "duration_seconds": 0.539}, "timestamp": "2026-01-17T16:00:17.831183"}
{"image_index": 449, "image_name": "000000050896.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050896.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1773.997, "latencies_ms": [1773.997], "images_per_second": 0.564, "prompt_tokens": 27, "response_tokens_est": 53, "n_tiles": 4, "output_text": "The main objects in the image are a bowl of oranges. The bowl is placed on a textured surface, likely a tablecloth. The oranges are arranged in the bowl, with the closest ones to the viewer being the ones in the foreground.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24953.8, "ram_available_mb": 100818.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 24954.8, "ram_available_mb": 100817.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4606.7, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5458.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.72, "peak": 15.11, "min": 14.12}, "VDD_CPU_SOC_MSS": {"avg": 15.38, "peak": 15.75, "min": 14.56}, "VDD_GPU": {"avg": 30.0, "peak": 41.36, "min": 22.45}, "VIN": {"avg": 69.77, "peak": 103.98, "min": 59.64}}, "power_watts_avg": 30.0, "energy_joules_est": 53.23, "sample_count": 13, "duration_seconds": 1.774}, "timestamp": "2026-01-17T16:00:19.611223"}
{"image_index": 449, "image_name": "000000050896.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050896.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1326.331, "latencies_ms": [1326.331], "images_per_second": 0.754, "prompt_tokens": 21, "response_tokens_est": 36, "n_tiles": 4, "output_text": "The image shows a clear glass bowl filled with several oranges placed on a textured silver surface. The oranges appear fresh and are likely being prepared for consumption or display.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24954.8, "ram_available_mb": 100817.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 24955.0, "ram_available_mb": 100817.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4606.7, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5453.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.74, "peak": 15.01, "min": 14.51}, "VDD_CPU_SOC_MSS": {"avg": 15.47, "peak": 15.75, "min": 14.96}, "VDD_GPU": {"avg": 29.23, "peak": 37.03, "min": 23.24}, "VIN": {"avg": 68.48, "peak": 98.94, "min": 59.89}}, "power_watts_avg": 29.23, "energy_joules_est": 38.78, "sample_count": 10, "duration_seconds": 1.327}, "timestamp": "2026-01-17T16:00:20.943777"}
{"image_index": 449, "image_name": "000000050896.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050896.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2425.096, "latencies_ms": [2425.096], "images_per_second": 0.412, "prompt_tokens": 19, "response_tokens_est": 78, "n_tiles": 4, "output_text": "The image features a bowl of bright orange oranges, which are vividly colored and have a glossy, smooth surface. The lighting is soft and diffused, casting gentle shadows and highlighting the freshness of the oranges. The bowl is placed on a textured, metallic surface, possibly a tablecloth, which adds a subtle contrast to the oranges' vibrant hue.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24955.0, "ram_available_mb": 100817.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 24955.8, "ram_available_mb": 100816.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4606.7, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5451.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.91, "peak": 15.11, "min": 14.4}, "VDD_CPU_SOC_MSS": {"avg": 15.75, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 26.02, "peak": 37.82, "min": 21.66}, "VIN": {"avg": 65.8, "peak": 106.23, "min": 55.13}}, "power_watts_avg": 26.02, "energy_joules_est": 63.11, "sample_count": 19, "duration_seconds": 2.425}, "timestamp": "2026-01-17T16:00:23.375312"}
{"image_index": 450, "image_name": "000000050943.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050943.jpg", "image_width": 640, "image_height": 319, "image_resolution": "640x319", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 629.725, "latencies_ms": [629.725], "images_per_second": 1.588, "prompt_tokens": 9, "response_tokens_est": 20, "n_tiles": 2, "output_text": "A surfer is riding a wave in the ocean, with the water splashing around him.", "error": null, "sys_before": {"cpu_percent": 37.9, "ram_used_mb": 24955.5, "ram_available_mb": 100816.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 24955.8, "ram_available_mb": 100816.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5055.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.53, "peak": 15.01, "min": 14.3}, "VDD_CPU_SOC_MSS": {"avg": 15.36, "peak": 15.75, "min": 14.96}, "VDD_GPU": {"avg": 27.57, "peak": 30.73, "min": 24.82}, "VIN": {"avg": 67.47, "peak": 78.8, "min": 62.98}}, "power_watts_avg": 27.57, "energy_joules_est": 17.37, "sample_count": 4, "duration_seconds": 0.63}, "timestamp": "2026-01-17T16:00:24.030568"}
{"image_index": 450, "image_name": "000000050943.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050943.jpg", "image_width": 640, "image_height": 319, "image_resolution": "640x319", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1115.611, "latencies_ms": [1115.611], "images_per_second": 0.896, "prompt_tokens": 23, "response_tokens_est": 40, "n_tiles": 2, "output_text": "object: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24955.8, "ram_available_mb": 100816.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 24956.5, "ram_available_mb": 100815.7, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5064.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.84, "peak": 15.32, "min": 14.4}, "VDD_CPU_SOC_MSS": {"avg": 15.6, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 26.0, "peak": 32.3, "min": 22.07}, "VIN": {"avg": 67.34, "peak": 105.72, "min": 57.8}}, "power_watts_avg": 26.0, "energy_joules_est": 29.02, "sample_count": 8, "duration_seconds": 1.116}, "timestamp": "2026-01-17T16:00:25.151996"}
{"image_index": 450, "image_name": "000000050943.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050943.jpg", "image_width": 640, "image_height": 319, "image_resolution": "640x319", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1385.29, "latencies_ms": [1385.29], "images_per_second": 0.722, "prompt_tokens": 27, "response_tokens_est": 52, "n_tiles": 2, "output_text": "The main objects in the image are two surfers riding waves. The surfer in the foreground is closer to the viewer, while the surfer in the background is further away. The waves are in the foreground, and the ocean is in the background.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24956.5, "ram_available_mb": 100815.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 24957.7, "ram_available_mb": 100814.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5067.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.05, "peak": 15.32, "min": 14.61}, "VDD_CPU_SOC_MSS": {"avg": 16.03, "peak": 16.54, "min": 15.35}, "VDD_GPU": {"avg": 24.62, "peak": 31.12, "min": 21.28}, "VIN": {"avg": 66.45, "peak": 99.74, "min": 59.16}}, "power_watts_avg": 24.62, "energy_joules_est": 34.11, "sample_count": 10, "duration_seconds": 1.386}, "timestamp": "2026-01-17T16:00:26.542978"}
{"image_index": 450, "image_name": "000000050943.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050943.jpg", "image_width": 640, "image_height": 319, "image_resolution": "640x319", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1797.054, "latencies_ms": [1797.054], "images_per_second": 0.556, "prompt_tokens": 21, "response_tokens_est": 70, "n_tiles": 2, "output_text": "The image captures a dynamic scene of a surfer riding a large wave in the ocean. The surfer is skillfully maneuvering their surfboard, while the wave is breaking powerfully to the right, creating a frothy white crest. The overcast sky suggests a potentially challenging weather condition, adding to the intensity of the surfing activity.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24957.7, "ram_available_mb": 100814.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 24957.7, "ram_available_mb": 100814.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5063.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.32, "peak": 15.72, "min": 14.71}, "VDD_CPU_SOC_MSS": {"avg": 15.95, "peak": 16.54, "min": 15.34}, "VDD_GPU": {"avg": 23.5, "peak": 29.94, "min": 20.88}, "VIN": {"avg": 66.64, "peak": 95.87, "min": 58.76}}, "power_watts_avg": 23.5, "energy_joules_est": 42.24, "sample_count": 14, "duration_seconds": 1.798}, "timestamp": "2026-01-17T16:00:28.346214"}
{"image_index": 450, "image_name": "000000050943.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050943.jpg", "image_width": 640, "image_height": 319, "image_resolution": "640x319", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1669.478, "latencies_ms": [1669.478], "images_per_second": 0.599, "prompt_tokens": 19, "response_tokens_est": 64, "n_tiles": 2, "output_text": "The image depicts a large, frothy wave in a stormy ocean, with a surfer riding it. The wave is a deep blue-green color, and the surfer is wearing a black wetsuit. The sky is overcast, with a grayish hue, suggesting a gloomy weather condition.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24957.7, "ram_available_mb": 100814.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 24957.9, "ram_available_mb": 100814.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5062.6, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.08, "peak": 15.42, "min": 14.71}, "VDD_CPU_SOC_MSS": {"avg": 15.9, "peak": 16.14, "min": 15.36}, "VDD_GPU": {"avg": 23.49, "peak": 30.33, "min": 20.49}, "VIN": {"avg": 66.5, "peak": 96.25, "min": 60.62}}, "power_watts_avg": 23.49, "energy_joules_est": 39.23, "sample_count": 13, "duration_seconds": 1.67}, "timestamp": "2026-01-17T16:00:30.021355"}
{"image_index": 451, "image_name": "000000051008.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051008.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2289.925, "latencies_ms": [2289.925], "images_per_second": 0.437, "prompt_tokens": 9, "response_tokens_est": 20, "n_tiles": 12, "output_text": "A tabby cat is sitting on a laptop keyboard, seemingly engrossed in the screen.", "error": null, "sys_before": {"cpu_percent": 44.7, "ram_used_mb": 24923.1, "ram_available_mb": 100849.0, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.3, "ram_used_mb": 24923.6, "ram_available_mb": 100848.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.31, "peak": 16.66, "min": 14.0}, "VDD_CPU_SOC_MSS": {"avg": 16.09, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 36.86, "peak": 43.31, "min": 28.76}, "VIN": {"avg": 84.22, "peak": 130.92, "min": 54.34}}, "power_watts_avg": 36.86, "energy_joules_est": 84.42, "sample_count": 17, "duration_seconds": 2.29}, "timestamp": "2026-01-17T16:00:32.385912"}
{"image_index": 451, "image_name": "000000051008.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051008.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 3543.851, "latencies_ms": [3543.851], "images_per_second": 0.282, "prompt_tokens": 23, "response_tokens_est": 55, "n_tiles": 12, "output_text": "- laptop: 1\n- computer mouse: 1\n- keyboard: 1\n- computer monitor: 1\n- cat: 1\n- computer mouse pad: 1\n- computer mouse: 1\n- computer mouse pad: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24923.6, "ram_available_mb": 100848.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 24924.6, "ram_available_mb": 100847.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.17, "peak": 16.56, "min": 14.12}, "VDD_CPU_SOC_MSS": {"avg": 15.67, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 34.12, "peak": 46.46, "min": 26.0}, "VIN": {"avg": 76.49, "peak": 124.74, "min": 61.24}}, "power_watts_avg": 34.12, "energy_joules_est": 120.93, "sample_count": 28, "duration_seconds": 3.544}, "timestamp": "2026-01-17T16:00:35.936363"}
{"image_index": 451, "image_name": "000000051008.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051008.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3431.947, "latencies_ms": [3431.947], "images_per_second": 0.291, "prompt_tokens": 27, "response_tokens_est": 52, "n_tiles": 12, "output_text": "The main object in the foreground is a cat, which is positioned near the laptop keyboard. The laptop is situated in the background, with its screen displaying a Twitter page. The cat appears to be resting on the keyboard, suggesting a comfortable and familiar environment.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24924.6, "ram_available_mb": 100847.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 24924.6, "ram_available_mb": 100847.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.2, "peak": 16.56, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.72, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 34.34, "peak": 45.69, "min": 26.0}, "VIN": {"avg": 74.27, "peak": 104.32, "min": 56.28}}, "power_watts_avg": 34.34, "energy_joules_est": 117.87, "sample_count": 27, "duration_seconds": 3.433}, "timestamp": "2026-01-17T16:00:39.374803"}
{"image_index": 451, "image_name": "000000051008.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051008.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 4766.931, "latencies_ms": [4766.931], "images_per_second": 0.21, "prompt_tokens": 21, "response_tokens_est": 91, "n_tiles": 12, "output_text": "The image depicts a domestic scene where a tabby cat is resting on a laptop keyboard. The cat is positioned on the right side of the laptop, with its head resting on the keyboard. The laptop is open, displaying a Twitter page, and the cat appears to be curious or interested in the content on the screen. The setting is likely a home environment, with the cat comfortably perched on the keyboard, suggesting a relaxed and familiar atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24924.6, "ram_available_mb": 100847.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 24924.6, "ram_available_mb": 100847.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.16, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.62, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 31.93, "peak": 44.49, "min": 25.6}, "VIN": {"avg": 73.95, "peak": 128.77, "min": 58.96}}, "power_watts_avg": 31.93, "energy_joules_est": 152.22, "sample_count": 37, "duration_seconds": 4.767}, "timestamp": "2026-01-17T16:00:44.149559"}
{"image_index": 451, "image_name": "000000051008.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051008.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3437.834, "latencies_ms": [3437.834], "images_per_second": 0.291, "prompt_tokens": 19, "response_tokens_est": 52, "n_tiles": 12, "output_text": "The image features a cat with a white and brown tabby coat, resting on a laptop keyboard. The laptop screen displays a Twitter page, and the cat is wearing a purple collar. The lighting is dim, and the overall setting appears to be indoors.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 24924.6, "ram_available_mb": 100847.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 24924.6, "ram_available_mb": 100847.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.19, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.73, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 34.18, "peak": 44.89, "min": 26.0}, "VIN": {"avg": 79.74, "peak": 133.27, "min": 65.37}}, "power_watts_avg": 34.18, "energy_joules_est": 117.52, "sample_count": 27, "duration_seconds": 3.438}, "timestamp": "2026-01-17T16:00:47.593706"}
{"image_index": 452, "image_name": "000000051309.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051309.jpg", "image_width": 640, "image_height": 360, "image_resolution": "640x360", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 567.669, "latencies_ms": [567.669], "images_per_second": 1.762, "prompt_tokens": 9, "response_tokens_est": 17, "n_tiles": 2, "output_text": "A group of horses is gathered in a field, with some feeding on hay.", "error": null, "sys_before": {"cpu_percent": 30.4, "ram_used_mb": 24927.6, "ram_available_mb": 100844.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 24928.0, "ram_available_mb": 100844.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5055.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.26, "peak": 14.61, "min": 13.9}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.36, "min": 14.57}, "VDD_GPU": {"avg": 29.45, "peak": 33.48, "min": 26.0}, "VIN": {"avg": 69.8, "peak": 95.38, "min": 57.66}}, "power_watts_avg": 29.45, "energy_joules_est": 16.73, "sample_count": 4, "duration_seconds": 0.568}, "timestamp": "2026-01-17T16:00:48.198303"}
{"image_index": 452, "image_name": "000000051309.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051309.jpg", "image_width": 640, "image_height": 360, "image_resolution": "640x360", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1040.083, "latencies_ms": [1040.083], "images_per_second": 0.961, "prompt_tokens": 23, "response_tokens_est": 38, "n_tiles": 2, "output_text": "1. Horses\n2. Hay\n3. Haystack\n4. Hay\n5. Horses\n6. Horses\n7. Horses\n8. Horses", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24928.0, "ram_available_mb": 100844.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 24929.5, "ram_available_mb": 100842.7, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5064.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.82, "peak": 15.11, "min": 14.4}, "VDD_CPU_SOC_MSS": {"avg": 15.52, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 27.24, "peak": 33.48, "min": 22.85}, "VIN": {"avg": 68.6, "peak": 92.66, "min": 62.36}}, "power_watts_avg": 27.24, "energy_joules_est": 28.34, "sample_count": 7, "duration_seconds": 1.04}, "timestamp": "2026-01-17T16:00:49.244177"}
{"image_index": 452, "image_name": "000000051309.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051309.jpg", "image_width": 640, "image_height": 360, "image_resolution": "640x360", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1769.199, "latencies_ms": [1769.199], "images_per_second": 0.565, "prompt_tokens": 27, "response_tokens_est": 70, "n_tiles": 2, "output_text": "In the image, there is a group of horses in a field. The horses are in the foreground, with the brown horse in the center being the most prominent. The brown horse is near a pile of hay, which is located in the background. The horses are walking in a line, with the brown horse in the center leading the group.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 24929.5, "ram_available_mb": 100842.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24931.0, "ram_available_mb": 100841.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5067.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.14, "peak": 15.62, "min": 14.51}, "VDD_CPU_SOC_MSS": {"avg": 15.96, "peak": 16.54, "min": 15.36}, "VDD_GPU": {"avg": 24.07, "peak": 31.12, "min": 21.28}, "VIN": {"avg": 65.36, "peak": 97.21, "min": 59.85}}, "power_watts_avg": 24.07, "energy_joules_est": 42.59, "sample_count": 13, "duration_seconds": 1.77}, "timestamp": "2026-01-17T16:00:51.019164"}
{"image_index": 452, "image_name": "000000051309.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051309.jpg", "image_width": 640, "image_height": 360, "image_resolution": "640x360", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1280.952, "latencies_ms": [1280.952], "images_per_second": 0.781, "prompt_tokens": 21, "response_tokens_est": 49, "n_tiles": 2, "output_text": "The image depicts a group of horses in a pasture, with one horse in the foreground eating hay. The horses are gathered together, and the setting appears to be a rural area with greenery and a clear sky in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24931.0, "ram_available_mb": 100841.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24932.2, "ram_available_mb": 100840.0, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5063.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.42, "peak": 16.33, "min": 14.61}, "VDD_CPU_SOC_MSS": {"avg": 15.79, "peak": 16.15, "min": 15.35}, "VDD_GPU": {"avg": 24.86, "peak": 30.34, "min": 22.05}, "VIN": {"avg": 68.15, "peak": 108.32, "min": 60.14}}, "power_watts_avg": 24.86, "energy_joules_est": 31.86, "sample_count": 9, "duration_seconds": 1.281}, "timestamp": "2026-01-17T16:00:52.311284"}
{"image_index": 452, "image_name": "000000051309.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051309.jpg", "image_width": 640, "image_height": 360, "image_resolution": "640x360", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1139.607, "latencies_ms": [1139.607], "images_per_second": 0.877, "prompt_tokens": 19, "response_tokens_est": 42, "n_tiles": 2, "output_text": "The image depicts a group of horses in a pasture, with the horses' coats appearing dark brown. The lighting is natural, suggesting it is daytime, and the horses are grazing on hay.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24932.2, "ram_available_mb": 100840.0, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 24932.5, "ram_available_mb": 100839.7, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5062.6, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.13, "peak": 15.42, "min": 14.71}, "VDD_CPU_SOC_MSS": {"avg": 15.8, "peak": 16.15, "min": 15.35}, "VDD_GPU": {"avg": 25.41, "peak": 30.73, "min": 22.06}, "VIN": {"avg": 66.44, "peak": 90.92, "min": 55.19}}, "power_watts_avg": 25.41, "energy_joules_est": 28.97, "sample_count": 8, "duration_seconds": 1.14}, "timestamp": "2026-01-17T16:00:53.456931"}
{"image_index": 453, "image_name": "000000051314.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051314.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2412.421, "latencies_ms": [2412.421], "images_per_second": 0.415, "prompt_tokens": 9, "response_tokens_est": 24, "n_tiles": 12, "output_text": "A surfer is riding a wave in the ocean, wearing a wetsuit and demonstrating skillful maneuvering.", "error": null, "sys_before": {"cpu_percent": 43.7, "ram_used_mb": 24916.8, "ram_available_mb": 100855.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.5, "ram_used_mb": 24917.3, "ram_available_mb": 100854.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.08, "peak": 16.46, "min": 13.79}, "VDD_CPU_SOC_MSS": {"avg": 15.83, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 36.61, "peak": 43.33, "min": 27.97}, "VIN": {"avg": 78.81, "peak": 116.72, "min": 54.75}}, "power_watts_avg": 36.61, "energy_joules_est": 88.33, "sample_count": 18, "duration_seconds": 2.413}, "timestamp": "2026-01-17T16:00:55.948783"}
{"image_index": 453, "image_name": "000000051314.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051314.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2684.102, "latencies_ms": [2684.102], "images_per_second": 0.373, "prompt_tokens": 23, "response_tokens_est": 30, "n_tiles": 12, "output_text": "surfboard: 1\nsurfer: 1\nwater: 1\nocean: 1\nbeach: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24917.3, "ram_available_mb": 100854.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 24919.0, "ram_available_mb": 100853.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.01, "peak": 16.26, "min": 14.02}, "VDD_CPU_SOC_MSS": {"avg": 15.45, "peak": 16.92, "min": 14.17}, "VDD_GPU": {"avg": 37.07, "peak": 45.3, "min": 27.18}, "VIN": {"avg": 76.62, "peak": 112.2, "min": 61.08}}, "power_watts_avg": 37.07, "energy_joules_est": 99.52, "sample_count": 20, "duration_seconds": 2.685}, "timestamp": "2026-01-17T16:00:58.639908"}
{"image_index": 453, "image_name": "000000051314.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051314.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3344.188, "latencies_ms": [3344.188], "images_per_second": 0.299, "prompt_tokens": 27, "response_tokens_est": 49, "n_tiles": 12, "output_text": "The main object in the foreground is a surfer riding a wave. The surfer is positioned on a white surfboard, which is partially submerged in the water. The background features a rocky shoreline and a partly cloudy sky.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24919.0, "ram_available_mb": 100853.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 24919.5, "ram_available_mb": 100852.7, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.94, "peak": 16.36, "min": 13.92}, "VDD_CPU_SOC_MSS": {"avg": 15.35, "peak": 17.32, "min": 14.17}, "VDD_GPU": {"avg": 34.74, "peak": 45.3, "min": 25.61}, "VIN": {"avg": 72.89, "peak": 90.94, "min": 59.58}}, "power_watts_avg": 34.74, "energy_joules_est": 116.19, "sample_count": 26, "duration_seconds": 3.345}, "timestamp": "2026-01-17T16:01:01.992436"}
{"image_index": 453, "image_name": "000000051314.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051314.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 4524.268, "latencies_ms": [4524.268], "images_per_second": 0.221, "prompt_tokens": 21, "response_tokens_est": 83, "n_tiles": 12, "output_text": "The image captures a surfer in action on a choppy ocean wave, skillfully maneuvering a surfboard. The surfer, dressed in a black wetsuit, is leaning forward with his arms extended, showcasing his expertise in surfing. The scene takes place in a coastal environment, with the surfer navigating the challenging waves against a backdrop of a rugged coastline and a partly cloudy sky.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24919.5, "ram_available_mb": 100852.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 24919.5, "ram_available_mb": 100852.7, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.87, "peak": 16.26, "min": 13.92}, "VDD_CPU_SOC_MSS": {"avg": 15.27, "peak": 17.32, "min": 14.17}, "VDD_GPU": {"avg": 32.14, "peak": 44.91, "min": 25.21}, "VIN": {"avg": 75.17, "peak": 131.3, "min": 57.0}}, "power_watts_avg": 32.14, "energy_joules_est": 145.43, "sample_count": 35, "duration_seconds": 4.525}, "timestamp": "2026-01-17T16:01:06.523395"}
{"image_index": 453, "image_name": "000000051314.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051314.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 4140.546, "latencies_ms": [4140.546], "images_per_second": 0.242, "prompt_tokens": 19, "response_tokens_est": 72, "n_tiles": 12, "output_text": "The image captures a surfer in a black wetsuit riding a wave on a surfboard. The surfer is partially submerged in the water, with splashes and waves around him. The lighting is bright, indicating a sunny day, and the colors are vibrant, with the green of the ocean contrasting against the white foam of the wave.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24919.5, "ram_available_mb": 100852.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 24920.0, "ram_available_mb": 100852.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.87, "peak": 16.26, "min": 13.92}, "VDD_CPU_SOC_MSS": {"avg": 15.3, "peak": 17.32, "min": 14.17}, "VDD_GPU": {"avg": 32.84, "peak": 44.91, "min": 25.6}, "VIN": {"avg": 74.26, "peak": 118.87, "min": 60.92}}, "power_watts_avg": 32.84, "energy_joules_est": 135.99, "sample_count": 32, "duration_seconds": 4.141}, "timestamp": "2026-01-17T16:01:10.674592"}
{"image_index": 454, "image_name": "000000051326.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051326.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2127.878, "latencies_ms": [2127.878], "images_per_second": 0.47, "prompt_tokens": 9, "response_tokens_est": 56, "n_tiles": 6, "output_text": "The image features a collection of Halloween-themed pumpkins, including a carved pumpkin with a smiling face, a pumpkin with a jack-o'-lantern face, and a pumpkin with a creature's face, all arranged on a table with a pink flower arrangement in the background.", "error": null, "sys_before": {"cpu_percent": 36.6, "ram_used_mb": 24943.3, "ram_available_mb": 100828.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 24945.0, "ram_available_mb": 100827.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.7, "peak": 15.11, "min": 13.39}, "VDD_CPU_SOC_MSS": {"avg": 15.43, "peak": 15.75, "min": 14.18}, "VDD_GPU": {"avg": 30.38, "peak": 40.16, "min": 23.64}, "VIN": {"avg": 71.93, "peak": 116.87, "min": 57.52}}, "power_watts_avg": 30.38, "energy_joules_est": 64.67, "sample_count": 16, "duration_seconds": 2.129}, "timestamp": "2026-01-17T16:01:12.872762"}
{"image_index": 454, "image_name": "000000051326.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051326.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2415.369, "latencies_ms": [2415.369], "images_per_second": 0.414, "prompt_tokens": 23, "response_tokens_est": 65, "n_tiles": 6, "output_text": "1. Pumpkin: 2\n2. Floral arrangement: 1\n3. Man figure: 1\n4. Man figure: 1\n5. Man figure: 1\n6. Man figure: 1\n7. Man figure: 1\n8. Man figure: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24945.0, "ram_available_mb": 100827.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 24946.2, "ram_available_mb": 100825.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.01, "peak": 15.44, "min": 14.3}, "VDD_CPU_SOC_MSS": {"avg": 15.64, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 29.83, "peak": 40.16, "min": 23.64}, "VIN": {"avg": 72.09, "peak": 116.41, "min": 57.15}}, "power_watts_avg": 29.83, "energy_joules_est": 72.06, "sample_count": 18, "duration_seconds": 2.416}, "timestamp": "2026-01-17T16:01:15.298330"}
{"image_index": 454, "image_name": "000000051326.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051326.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2171.377, "latencies_ms": [2171.377], "images_per_second": 0.461, "prompt_tokens": 27, "response_tokens_est": 56, "n_tiles": 6, "output_text": "The main objects in the image are a collection of pumpkins and a floral arrangement. The pumpkins are arranged in a row, with the one in the foreground being the most prominent. The floral arrangement is placed in the background, slightly to the right of the pumpkins.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24946.2, "ram_available_mb": 100825.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 24947.2, "ram_available_mb": 100825.0, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.0, "peak": 15.34, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.68, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 30.38, "peak": 40.95, "min": 23.64}, "VIN": {"avg": 74.2, "peak": 117.85, "min": 62.41}}, "power_watts_avg": 30.38, "energy_joules_est": 65.98, "sample_count": 17, "duration_seconds": 2.172}, "timestamp": "2026-01-17T16:01:17.477682"}
{"image_index": 454, "image_name": "000000051326.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051326.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2655.676, "latencies_ms": [2655.676], "images_per_second": 0.377, "prompt_tokens": 21, "response_tokens_est": 74, "n_tiles": 6, "output_text": "The image depicts a festive scene with a collection of carved pumpkins arranged on a table. The pumpkins are decorated with various faces, including a smiling face, a scary face, and a character's face. The setting appears to be a home or a decorated room, possibly for a Halloween celebration, as the pumpkins are adorned with Halloween-themed designs.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24947.2, "ram_available_mb": 100825.0, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 24947.9, "ram_available_mb": 100824.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.05, "peak": 15.44, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.67, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 29.04, "peak": 40.56, "min": 23.24}, "VIN": {"avg": 69.31, "peak": 103.09, "min": 56.36}}, "power_watts_avg": 29.04, "energy_joules_est": 77.13, "sample_count": 21, "duration_seconds": 2.656}, "timestamp": "2026-01-17T16:01:20.139315"}
{"image_index": 454, "image_name": "000000051326.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051326.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2299.18, "latencies_ms": [2299.18], "images_per_second": 0.435, "prompt_tokens": 19, "response_tokens_est": 61, "n_tiles": 6, "output_text": "The image showcases a collection of pumpkins, each uniquely designed with Halloween-themed faces. The pumpkins are illuminated by soft, warm lighting, creating a cozy and inviting atmosphere. The colors are vibrant, with the pumpkins' orange hues contrasting against the dark, shadowy faces.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24947.9, "ram_available_mb": 100824.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 24948.9, "ram_available_mb": 100823.3, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.03, "peak": 15.44, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.66, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 29.94, "peak": 40.16, "min": 23.64}, "VIN": {"avg": 69.74, "peak": 110.47, "min": 61.12}}, "power_watts_avg": 29.94, "energy_joules_est": 68.85, "sample_count": 18, "duration_seconds": 2.299}, "timestamp": "2026-01-17T16:01:22.444553"}
{"image_index": 455, "image_name": "000000051598.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051598.jpg", "image_width": 360, "image_height": 640, "image_resolution": "360x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1028.695, "latencies_ms": [1028.695], "images_per_second": 0.972, "prompt_tokens": 9, "response_tokens_est": 37, "n_tiles": 2, "output_text": "The image depicts a small, dimly lit bathroom with a white sink, a black trash bag, and a door slightly ajar, revealing a glimpse of the room beyond.", "error": null, "sys_before": {"cpu_percent": 29.2, "ram_used_mb": 24948.9, "ram_available_mb": 100823.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 24949.9, "ram_available_mb": 100822.3, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5055.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.78, "peak": 15.42, "min": 14.2}, "VDD_CPU_SOC_MSS": {"avg": 15.75, "peak": 16.15, "min": 14.96}, "VDD_GPU": {"avg": 25.75, "peak": 31.91, "min": 22.06}, "VIN": {"avg": 66.89, "peak": 86.0, "min": 60.97}}, "power_watts_avg": 25.75, "energy_joules_est": 26.5, "sample_count": 8, "duration_seconds": 1.029}, "timestamp": "2026-01-17T16:01:23.497008"}
{"image_index": 455, "image_name": "000000051598.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051598.jpg", "image_width": 360, "image_height": 640, "image_resolution": "360x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1402.959, "latencies_ms": [1402.959], "images_per_second": 0.713, "prompt_tokens": 23, "response_tokens_est": 53, "n_tiles": 2, "output_text": "- sink: 1\n- mirror: 1\n- soap dispenser: 1\n- soap: 1\n- soap bottle: 1\n- soap box: 1\n- trash bag: 1\n- door: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24949.9, "ram_available_mb": 100822.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24950.1, "ram_available_mb": 100822.0, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5064.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.2, "peak": 15.52, "min": 14.71}, "VDD_CPU_SOC_MSS": {"avg": 15.86, "peak": 16.54, "min": 15.35}, "VDD_GPU": {"avg": 24.7, "peak": 31.11, "min": 21.28}, "VIN": {"avg": 65.58, "peak": 93.61, "min": 55.54}}, "power_watts_avg": 24.7, "energy_joules_est": 34.66, "sample_count": 10, "duration_seconds": 1.403}, "timestamp": "2026-01-17T16:01:24.905688"}
{"image_index": 455, "image_name": "000000051598.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051598.jpg", "image_width": 360, "image_height": 640, "image_resolution": "360x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1633.291, "latencies_ms": [1633.291], "images_per_second": 0.612, "prompt_tokens": 27, "response_tokens_est": 63, "n_tiles": 2, "output_text": "The main objects in the image are a sink, a trash bag, and a door. The sink is located on the left side of the image, near the wall. The trash bag is in the foreground, close to the sink. The door is on the right side of the image, near the wall.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24950.1, "ram_available_mb": 100822.0, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 24949.9, "ram_available_mb": 100822.3, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5067.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.33, "peak": 15.72, "min": 14.81}, "VDD_CPU_SOC_MSS": {"avg": 16.01, "peak": 16.14, "min": 15.36}, "VDD_GPU": {"avg": 23.9, "peak": 30.33, "min": 20.89}, "VIN": {"avg": 66.57, "peak": 97.94, "min": 56.79}}, "power_watts_avg": 23.9, "energy_joules_est": 39.04, "sample_count": 12, "duration_seconds": 1.634}, "timestamp": "2026-01-17T16:01:26.545503"}
{"image_index": 455, "image_name": "000000051598.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051598.jpg", "image_width": 360, "image_height": 640, "image_resolution": "360x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1620.566, "latencies_ms": [1620.566], "images_per_second": 0.617, "prompt_tokens": 21, "response_tokens_est": 62, "n_tiles": 2, "output_text": "The image depicts a small, dimly lit bathroom with a white sink and a mirror above it. The walls are bare, and there is a black trash bag on the floor. The door is slightly ajar, and the overall setting appears to be in a state of disrepair or renovation.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24949.9, "ram_available_mb": 100822.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 24951.1, "ram_available_mb": 100821.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5063.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.16, "peak": 15.62, "min": 14.51}, "VDD_CPU_SOC_MSS": {"avg": 15.98, "peak": 16.54, "min": 15.36}, "VDD_GPU": {"avg": 23.8, "peak": 29.95, "min": 20.88}, "VIN": {"avg": 67.46, "peak": 107.43, "min": 60.64}}, "power_watts_avg": 23.8, "energy_joules_est": 38.58, "sample_count": 12, "duration_seconds": 1.621}, "timestamp": "2026-01-17T16:01:28.172039"}
{"image_index": 455, "image_name": "000000051598.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051598.jpg", "image_width": 360, "image_height": 640, "image_resolution": "360x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1363.229, "latencies_ms": [1363.229], "images_per_second": 0.734, "prompt_tokens": 19, "response_tokens_est": 51, "n_tiles": 2, "output_text": "The image depicts a small, dimly lit bathroom with a white sink and faucet, a black trash bag, and a white door. The walls are plain and have a yellowish tint, and there is a small mirror above the sink.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24951.1, "ram_available_mb": 100821.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 24951.1, "ram_available_mb": 100821.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5062.6, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.94, "peak": 15.22, "min": 14.71}, "VDD_CPU_SOC_MSS": {"avg": 15.98, "peak": 16.54, "min": 15.36}, "VDD_GPU": {"avg": 24.23, "peak": 29.93, "min": 21.28}, "VIN": {"avg": 66.92, "peak": 111.82, "min": 53.4}}, "power_watts_avg": 24.23, "energy_joules_est": 33.04, "sample_count": 10, "duration_seconds": 1.364}, "timestamp": "2026-01-17T16:01:29.543098"}
{"image_index": 456, "image_name": "000000051610.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051610.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1250.756, "latencies_ms": [1250.756], "images_per_second": 0.8, "prompt_tokens": 9, "response_tokens_est": 24, "n_tiles": 6, "output_text": "A young child is sitting on a bed, looking at a laptop screen, with their feet resting on the bed.", "error": null, "sys_before": {"cpu_percent": 44.9, "ram_used_mb": 24951.1, "ram_available_mb": 100821.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 24951.8, "ram_available_mb": 100820.3, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.79, "peak": 15.34, "min": 14.2}, "VDD_CPU_SOC_MSS": {"avg": 15.7, "peak": 16.53, "min": 14.96}, "VDD_GPU": {"avg": 33.61, "peak": 39.77, "min": 27.57}, "VIN": {"avg": 74.58, "peak": 94.71, "min": 61.56}}, "power_watts_avg": 33.61, "energy_joules_est": 42.05, "sample_count": 9, "duration_seconds": 1.251}, "timestamp": "2026-01-17T16:01:30.832638"}
{"image_index": 456, "image_name": "000000051610.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051610.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1772.812, "latencies_ms": [1772.812], "images_per_second": 0.564, "prompt_tokens": 23, "response_tokens_est": 40, "n_tiles": 6, "output_text": "1. Baby\n2. Laptop\n3. Bed\n4. Child\n5. Child's feet\n6. Child's hands\n7. Child's legs\n8. Child's body", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24951.8, "ram_available_mb": 100820.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 24951.8, "ram_available_mb": 100820.3, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.83, "peak": 15.24, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.35, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 32.54, "peak": 41.34, "min": 24.42}, "VIN": {"avg": 71.28, "peak": 113.84, "min": 54.9}}, "power_watts_avg": 32.54, "energy_joules_est": 57.7, "sample_count": 13, "duration_seconds": 1.773}, "timestamp": "2026-01-17T16:01:32.611394"}
{"image_index": 456, "image_name": "000000051610.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051610.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2605.446, "latencies_ms": [2605.446], "images_per_second": 0.384, "prompt_tokens": 27, "response_tokens_est": 69, "n_tiles": 6, "output_text": "The main objects in the image are a laptop and a baby. The laptop is positioned on the left side of the image, with its screen facing the baby. The baby is sitting on the right side of the image, with their feet resting on the laptop. The background is plain and white, providing a neutral setting for the main subjects.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24951.8, "ram_available_mb": 100820.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 24952.1, "ram_available_mb": 100820.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.83, "peak": 15.24, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.33, "peak": 15.75, "min": 14.56}, "VDD_GPU": {"avg": 28.93, "peak": 40.56, "min": 22.85}, "VIN": {"avg": 68.54, "peak": 82.82, "min": 63.33}}, "power_watts_avg": 28.93, "energy_joules_est": 75.39, "sample_count": 20, "duration_seconds": 2.606}, "timestamp": "2026-01-17T16:01:35.224101"}
{"image_index": 456, "image_name": "000000051610.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051610.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2542.242, "latencies_ms": [2542.242], "images_per_second": 0.393, "prompt_tokens": 21, "response_tokens_est": 67, "n_tiles": 6, "output_text": "The image depicts a black and white scene of a young child sitting on a bed, with a laptop open in front of them. The child appears to be focused on the screen, possibly engaged in an activity on the laptop. The setting is a simple, unadorned bedroom with a plain white wall in the background.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 24952.1, "ram_available_mb": 100820.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 24951.8, "ram_available_mb": 100820.3, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.82, "peak": 15.24, "min": 14.2}, "VDD_CPU_SOC_MSS": {"avg": 15.42, "peak": 15.75, "min": 14.56}, "VDD_GPU": {"avg": 29.07, "peak": 40.18, "min": 22.85}, "VIN": {"avg": 68.23, "peak": 94.58, "min": 54.53}}, "power_watts_avg": 29.07, "energy_joules_est": 73.91, "sample_count": 19, "duration_seconds": 2.543}, "timestamp": "2026-01-17T16:01:37.772698"}
{"image_index": 456, "image_name": "000000051610.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051610.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1886.509, "latencies_ms": [1886.509], "images_per_second": 0.53, "prompt_tokens": 19, "response_tokens_est": 44, "n_tiles": 6, "output_text": "The image is a black and white photograph featuring a young child sitting on a bed. The child is wearing a white shirt and has dark hair. The lighting is soft and even, creating a calm and intimate atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24951.8, "ram_available_mb": 100820.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 24952.1, "ram_available_mb": 100820.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.8, "peak": 15.24, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.3, "peak": 15.74, "min": 14.56}, "VDD_GPU": {"avg": 31.24, "peak": 40.57, "min": 23.64}, "VIN": {"avg": 72.81, "peak": 116.25, "min": 62.11}}, "power_watts_avg": 31.24, "energy_joules_est": 58.94, "sample_count": 14, "duration_seconds": 1.887}, "timestamp": "2026-01-17T16:01:39.665925"}
{"image_index": 457, "image_name": "000000051712.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051712.jpg", "image_width": 640, "image_height": 368, "image_resolution": "640x368", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1305.699, "latencies_ms": [1305.699], "images_per_second": 0.766, "prompt_tokens": 9, "response_tokens_est": 26, "n_tiles": 6, "output_text": "A person is skiing down a snowy slope, wearing a brown jacket and a helmet, with snow flying around them.", "error": null, "sys_before": {"cpu_percent": 42.9, "ram_used_mb": 24952.1, "ram_available_mb": 100820.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 24953.3, "ram_available_mb": 100818.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.66, "peak": 15.14, "min": 13.9}, "VDD_CPU_SOC_MSS": {"avg": 15.43, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 33.56, "peak": 40.16, "min": 26.79}, "VIN": {"avg": 76.33, "peak": 120.34, "min": 49.19}}, "power_watts_avg": 33.56, "energy_joules_est": 43.83, "sample_count": 10, "duration_seconds": 1.306}, "timestamp": "2026-01-17T16:01:41.018665"}
{"image_index": 457, "image_name": "000000051712.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051712.jpg", "image_width": 640, "image_height": 368, "image_resolution": "640x368", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1731.44, "latencies_ms": [1731.44], "images_per_second": 0.578, "prompt_tokens": 23, "response_tokens_est": 40, "n_tiles": 6, "output_text": "object: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 24953.3, "ram_available_mb": 100818.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 24953.5, "ram_available_mb": 100818.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.99, "peak": 15.44, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.62, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 32.72, "peak": 40.95, "min": 24.82}, "VIN": {"avg": 77.0, "peak": 116.99, "min": 61.21}}, "power_watts_avg": 32.72, "energy_joules_est": 56.66, "sample_count": 13, "duration_seconds": 1.732}, "timestamp": "2026-01-17T16:01:42.758147"}
{"image_index": 457, "image_name": "000000051712.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051712.jpg", "image_width": 640, "image_height": 368, "image_resolution": "640x368", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2716.395, "latencies_ms": [2716.395], "images_per_second": 0.368, "prompt_tokens": 27, "response_tokens_est": 73, "n_tiles": 6, "output_text": "The main object in the foreground is a person skiing down a snowy slope. The person is wearing a brown jacket and a blue and orange helmet. The background features a snowy forest with trees and snow-covered branches. The person is positioned near the center of the image, slightly to the left, and the snowy slope extends to the right.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24953.5, "ram_available_mb": 100818.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 24953.8, "ram_available_mb": 100818.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.85, "peak": 15.24, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.34, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 28.7, "peak": 40.57, "min": 22.85}, "VIN": {"avg": 71.73, "peak": 113.91, "min": 62.23}}, "power_watts_avg": 28.7, "energy_joules_est": 77.97, "sample_count": 21, "duration_seconds": 2.717}, "timestamp": "2026-01-17T16:01:45.480175"}
{"image_index": 457, "image_name": "000000051712.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051712.jpg", "image_width": 640, "image_height": 368, "image_resolution": "640x368", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2739.847, "latencies_ms": [2739.847], "images_per_second": 0.365, "prompt_tokens": 21, "response_tokens_est": 74, "n_tiles": 6, "output_text": "The image depicts a snowy mountainous landscape with a person skiing down a slope. The individual is dressed in a brown jacket, black pants, and a colorful beanie, and is using ski poles to navigate the snowy terrain. The scene is set in a winter environment, with snow-covered trees and a clear blue sky visible in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24953.8, "ram_available_mb": 100818.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 24954.0, "ram_available_mb": 100818.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.84, "peak": 15.24, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.49, "peak": 15.75, "min": 14.57}, "VDD_GPU": {"avg": 28.49, "peak": 40.18, "min": 22.85}, "VIN": {"avg": 70.29, "peak": 118.95, "min": 55.07}}, "power_watts_avg": 28.49, "energy_joules_est": 78.07, "sample_count": 21, "duration_seconds": 2.74}, "timestamp": "2026-01-17T16:01:48.226334"}
{"image_index": 457, "image_name": "000000051712.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051712.jpg", "image_width": 640, "image_height": 368, "image_resolution": "640x368", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1926.22, "latencies_ms": [1926.22], "images_per_second": 0.519, "prompt_tokens": 19, "response_tokens_est": 46, "n_tiles": 6, "output_text": "The skier is dressed in a brown jacket and black pants, with a colorful beanie and goggles. The snowy landscape is illuminated by soft, diffused lighting, suggesting a calm and serene winter day.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24954.0, "ram_available_mb": 100818.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 24953.8, "ram_available_mb": 100818.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.85, "peak": 15.24, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.38, "peak": 15.75, "min": 14.56}, "VDD_GPU": {"avg": 31.32, "peak": 40.16, "min": 24.04}, "VIN": {"avg": 74.95, "peak": 115.7, "min": 63.41}}, "power_watts_avg": 31.32, "energy_joules_est": 60.34, "sample_count": 14, "duration_seconds": 1.927}, "timestamp": "2026-01-17T16:01:50.158542"}
{"image_index": 458, "image_name": "000000051738.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051738.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2624.071, "latencies_ms": [2624.071], "images_per_second": 0.381, "prompt_tokens": 9, "response_tokens_est": 30, "n_tiles": 12, "output_text": "The image shows a neatly arranged hotel room with a bed, a lamp, and a small table, all set against a light-colored wall.", "error": null, "sys_before": {"cpu_percent": 50.8, "ram_used_mb": 24920.4, "ram_available_mb": 100851.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.5, "ram_used_mb": 24921.5, "ram_available_mb": 100850.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.17, "peak": 16.66, "min": 13.39}, "VDD_CPU_SOC_MSS": {"avg": 15.9, "peak": 17.71, "min": 14.18}, "VDD_GPU": {"avg": 35.94, "peak": 43.73, "min": 27.57}, "VIN": {"avg": 77.25, "peak": 105.12, "min": 64.11}}, "power_watts_avg": 35.94, "energy_joules_est": 94.32, "sample_count": 20, "duration_seconds": 2.624}, "timestamp": "2026-01-17T16:01:52.875766"}
{"image_index": 458, "image_name": "000000051738.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051738.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2612.449, "latencies_ms": [2612.449], "images_per_second": 0.383, "prompt_tokens": 23, "response_tokens_est": 28, "n_tiles": 12, "output_text": "bed: 1\npillow: 2\ncouch: 1\nlamp: 1\ntable: 1", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 24921.5, "ram_available_mb": 100850.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.5, "ram_used_mb": 24921.8, "ram_available_mb": 100850.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.29, "peak": 16.56, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.94, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 37.16, "peak": 45.28, "min": 27.97}, "VIN": {"avg": 81.89, "peak": 133.61, "min": 66.38}}, "power_watts_avg": 37.16, "energy_joules_est": 97.1, "sample_count": 20, "duration_seconds": 2.613}, "timestamp": "2026-01-17T16:01:55.494552"}
{"image_index": 458, "image_name": "000000051738.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051738.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 5187.116, "latencies_ms": [5187.116], "images_per_second": 0.193, "prompt_tokens": 27, "response_tokens_est": 103, "n_tiles": 12, "output_text": "The main objects in the image are a bed and a nightstand. The bed is positioned in the foreground, with a white bedspread and multiple pillows. The nightstand is located to the right of the bed, with a lamp and a small book or box on top. The background features a window with curtains, a blue armchair, and a small table. The overall spatial relationship is that the bed is the central focus, with the nightstand and other objects providing context and depth to the scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24921.8, "ram_available_mb": 100850.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 24921.8, "ram_available_mb": 100850.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.14, "peak": 16.56, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.65, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 31.57, "peak": 45.28, "min": 25.6}, "VIN": {"avg": 74.63, "peak": 129.67, "min": 59.68}}, "power_watts_avg": 31.57, "energy_joules_est": 163.77, "sample_count": 41, "duration_seconds": 5.187}, "timestamp": "2026-01-17T16:02:00.687947"}
{"image_index": 458, "image_name": "000000051738.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051738.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 4134.194, "latencies_ms": [4134.194], "images_per_second": 0.242, "prompt_tokens": 21, "response_tokens_est": 72, "n_tiles": 12, "output_text": "The image depicts a neatly arranged hotel room with a large bed at the center. The room is well-lit, featuring a white bedspread, a light-colored wall, and a small table with a lamp on it. The bed is adorned with various pillows, and there is a window with curtains on the left side of the room.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24921.8, "ram_available_mb": 100850.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 24922.8, "ram_available_mb": 100849.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.89, "peak": 16.36, "min": 14.02}, "VDD_CPU_SOC_MSS": {"avg": 15.39, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 32.87, "peak": 44.91, "min": 25.6}, "VIN": {"avg": 71.86, "peak": 116.15, "min": 55.79}}, "power_watts_avg": 32.87, "energy_joules_est": 135.91, "sample_count": 32, "duration_seconds": 4.135}, "timestamp": "2026-01-17T16:02:04.828382"}
{"image_index": 458, "image_name": "000000051738.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051738.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3743.888, "latencies_ms": [3743.888], "images_per_second": 0.267, "prompt_tokens": 19, "response_tokens_est": 61, "n_tiles": 12, "output_text": "The room is well-lit with natural light streaming in through large windows adorned with light-colored curtains. The walls are painted in a light, neutral color, and the furniture is made of a combination of materials, including a light-colored sofa, a wooden bed frame, and a blue armchair.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24922.8, "ram_available_mb": 100849.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 24923.5, "ram_available_mb": 100848.7, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.94, "peak": 16.36, "min": 14.02}, "VDD_CPU_SOC_MSS": {"avg": 15.38, "peak": 16.92, "min": 14.56}, "VDD_GPU": {"avg": 33.39, "peak": 44.91, "min": 25.6}, "VIN": {"avg": 74.26, "peak": 109.97, "min": 60.61}}, "power_watts_avg": 33.39, "energy_joules_est": 125.02, "sample_count": 29, "duration_seconds": 3.744}, "timestamp": "2026-01-17T16:02:08.578778"}
{"image_index": 459, "image_name": "000000051938.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051938.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2601.528, "latencies_ms": [2601.528], "images_per_second": 0.384, "prompt_tokens": 9, "response_tokens_est": 29, "n_tiles": 12, "output_text": "A skier is performing a jump over a red railing on a snowy slope, surrounded by other skiers and snowboarders.", "error": null, "sys_before": {"cpu_percent": 41.2, "ram_used_mb": 24926.2, "ram_available_mb": 100846.0, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.6, "ram_used_mb": 24927.4, "ram_available_mb": 100844.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.08, "peak": 16.66, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 15.74, "peak": 17.71, "min": 13.78}, "VDD_GPU": {"avg": 35.98, "peak": 43.73, "min": 27.57}, "VIN": {"avg": 76.6, "peak": 112.52, "min": 57.21}}, "power_watts_avg": 35.98, "energy_joules_est": 93.62, "sample_count": 20, "duration_seconds": 2.602}, "timestamp": "2026-01-17T16:02:11.297276"}
{"image_index": 459, "image_name": "000000051938.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051938.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 3030.985, "latencies_ms": [3030.985], "images_per_second": 0.33, "prompt_tokens": 23, "response_tokens_est": 40, "n_tiles": 12, "output_text": "object: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24927.4, "ram_available_mb": 100844.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 24928.6, "ram_available_mb": 100843.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.23, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.73, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 35.7, "peak": 44.89, "min": 26.39}, "VIN": {"avg": 79.7, "peak": 122.05, "min": 55.66}}, "power_watts_avg": 35.7, "energy_joules_est": 108.22, "sample_count": 23, "duration_seconds": 3.031}, "timestamp": "2026-01-17T16:02:14.335452"}
{"image_index": 459, "image_name": "000000051938.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051938.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 4325.754, "latencies_ms": [4325.754], "images_per_second": 0.231, "prompt_tokens": 27, "response_tokens_est": 78, "n_tiles": 12, "output_text": "The main object in the foreground is a person skiing down a snowy slope. The person is positioned near the center of the image, slightly to the right. In the background, there is a ski lift with several people on it, and a red fence is visible. The ski lift is located further back on the slope, and the fence is near the top of the slope.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24928.6, "ram_available_mb": 100843.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 24929.6, "ram_available_mb": 100842.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.16, "peak": 16.56, "min": 14.12}, "VDD_CPU_SOC_MSS": {"avg": 15.61, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 32.78, "peak": 45.68, "min": 26.0}, "VIN": {"avg": 75.42, "peak": 127.71, "min": 58.59}}, "power_watts_avg": 32.78, "energy_joules_est": 141.82, "sample_count": 34, "duration_seconds": 4.326}, "timestamp": "2026-01-17T16:02:18.667959"}
{"image_index": 459, "image_name": "000000051938.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051938.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3772.337, "latencies_ms": [3772.337], "images_per_second": 0.265, "prompt_tokens": 21, "response_tokens_est": 62, "n_tiles": 12, "output_text": "The image depicts a snowy mountain slope with a skier in mid-air, performing a jump. The skier is dressed in a white jacket and orange pants, and is surrounded by other skiers and snowboarders on the slope. The sky is clear and blue, indicating a sunny day.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24929.6, "ram_available_mb": 100842.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 24929.6, "ram_available_mb": 100842.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.19, "peak": 16.56, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.66, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 33.64, "peak": 45.68, "min": 26.0}, "VIN": {"avg": 72.65, "peak": 98.18, "min": 63.98}}, "power_watts_avg": 33.64, "energy_joules_est": 126.92, "sample_count": 29, "duration_seconds": 3.773}, "timestamp": "2026-01-17T16:02:22.446917"}
{"image_index": 459, "image_name": "000000051938.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051938.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3669.926, "latencies_ms": [3669.926], "images_per_second": 0.272, "prompt_tokens": 19, "response_tokens_est": 59, "n_tiles": 12, "output_text": "The image depicts a snowy mountain landscape under a clear blue sky. The snow is pristine and undisturbed, with a few scattered skiers and snowboarders in the background. The lighting is bright and natural, casting shadows on the snow, indicating a sunny day.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24929.6, "ram_available_mb": 100842.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 24929.6, "ram_available_mb": 100842.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.19, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.68, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 33.93, "peak": 45.3, "min": 26.0}, "VIN": {"avg": 76.94, "peak": 113.53, "min": 56.66}}, "power_watts_avg": 33.93, "energy_joules_est": 124.54, "sample_count": 28, "duration_seconds": 3.67}, "timestamp": "2026-01-17T16:02:26.127496"}
{"image_index": 460, "image_name": "000000051961.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051961.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1554.08, "latencies_ms": [1554.08], "images_per_second": 0.643, "prompt_tokens": 9, "response_tokens_est": 35, "n_tiles": 6, "output_text": "The image depicts a close-up view of a metal parking meter with graffiti on its side, set against a backdrop of a colorful and heavily adorned urban wall.", "error": null, "sys_before": {"cpu_percent": 44.0, "ram_used_mb": 24950.0, "ram_available_mb": 100822.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 24951.2, "ram_available_mb": 100821.0, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.66, "peak": 15.11, "min": 13.61}, "VDD_CPU_SOC_MSS": {"avg": 15.35, "peak": 16.14, "min": 14.17}, "VDD_GPU": {"avg": 32.46, "peak": 40.56, "min": 25.22}, "VIN": {"avg": 78.33, "peak": 117.04, "min": 61.3}}, "power_watts_avg": 32.46, "energy_joules_est": 50.46, "sample_count": 12, "duration_seconds": 1.554}, "timestamp": "2026-01-17T16:02:27.749092"}
{"image_index": 460, "image_name": "000000051961.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051961.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1708.785, "latencies_ms": [1708.785], "images_per_second": 0.585, "prompt_tokens": 23, "response_tokens_est": 38, "n_tiles": 6, "output_text": "1. Graffiti\n2. Metal parking meter\n3. Wall\n4. Street\n5. Sidewalk\n6. Urban environment\n7. Art\n8. Street", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24951.2, "ram_available_mb": 100821.0, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 24952.2, "ram_available_mb": 100820.0, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.85, "peak": 15.24, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.53, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 32.54, "peak": 41.34, "min": 24.43}, "VIN": {"avg": 73.1, "peak": 121.88, "min": 54.51}}, "power_watts_avg": 32.54, "energy_joules_est": 55.62, "sample_count": 13, "duration_seconds": 1.709}, "timestamp": "2026-01-17T16:02:29.464911"}
{"image_index": 460, "image_name": "000000051961.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051961.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2675.083, "latencies_ms": [2675.083], "images_per_second": 0.374, "prompt_tokens": 27, "response_tokens_est": 72, "n_tiles": 6, "output_text": "The main objects in the image are a metal parking meter and a wall adorned with graffiti. The parking meter is positioned near the bottom left corner of the image, while the graffiti is prominently displayed on the wall, occupying the majority of the background. The graffiti is in the foreground, with the parking meter situated in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24952.2, "ram_available_mb": 100820.0, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 24952.4, "ram_available_mb": 100819.7, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.88, "peak": 15.24, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.54, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 28.83, "peak": 40.56, "min": 22.85}, "VIN": {"avg": 68.12, "peak": 108.63, "min": 56.6}}, "power_watts_avg": 28.83, "energy_joules_est": 77.14, "sample_count": 21, "duration_seconds": 2.676}, "timestamp": "2026-01-17T16:02:32.146197"}
{"image_index": 460, "image_name": "000000051961.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051961.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2924.761, "latencies_ms": [2924.761], "images_per_second": 0.342, "prompt_tokens": 21, "response_tokens_est": 81, "n_tiles": 6, "output_text": "The image depicts a vibrant urban street scene with a graffiti-covered wall as the backdrop. The wall is adorned with various colorful graffiti tags, including a prominent one that reads \"THE ONE\" in bold, white letters. The scene is set in an area with a mix of urban elements, including a metal parking meter with graffiti on it, suggesting a lively and creative atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24952.4, "ram_available_mb": 100819.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 24953.7, "ram_available_mb": 100818.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.91, "peak": 15.24, "min": 14.3}, "VDD_CPU_SOC_MSS": {"avg": 15.53, "peak": 15.75, "min": 14.56}, "VDD_GPU": {"avg": 28.26, "peak": 40.56, "min": 22.85}, "VIN": {"avg": 68.93, "peak": 110.82, "min": 55.86}}, "power_watts_avg": 28.26, "energy_joules_est": 82.66, "sample_count": 23, "duration_seconds": 2.925}, "timestamp": "2026-01-17T16:02:35.077192"}
{"image_index": 460, "image_name": "000000051961.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051961.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2930.798, "latencies_ms": [2930.798], "images_per_second": 0.341, "prompt_tokens": 19, "response_tokens_est": 81, "n_tiles": 6, "output_text": "The image showcases a vibrant urban street scene with a graffiti-covered wall. The wall is adorned with various colors, including blue, yellow, and black, creating a dynamic and colorful backdrop. The lighting is bright, casting sharp shadows and highlighting the textures of the graffiti. The materials used in the graffiti appear to be painted on a metal pole, adding to the urban aesthetic.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24953.7, "ram_available_mb": 100818.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 24953.7, "ram_available_mb": 100818.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.86, "peak": 15.24, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.53, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 28.29, "peak": 40.16, "min": 22.85}, "VIN": {"avg": 71.34, "peak": 112.78, "min": 58.85}}, "power_watts_avg": 28.29, "energy_joules_est": 82.93, "sample_count": 22, "duration_seconds": 2.931}, "timestamp": "2026-01-17T16:02:38.014036"}
{"image_index": 461, "image_name": "000000051976.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051976.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 929.431, "latencies_ms": [929.431], "images_per_second": 1.076, "prompt_tokens": 9, "response_tokens_est": 12, "n_tiles": 6, "output_text": "A surfer is riding a wave in the ocean.", "error": null, "sys_before": {"cpu_percent": 42.4, "ram_used_mb": 24953.7, "ram_available_mb": 100818.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 24953.7, "ram_available_mb": 100818.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.52, "peak": 15.14, "min": 13.79}, "VDD_CPU_SOC_MSS": {"avg": 15.18, "peak": 16.14, "min": 14.18}, "VDD_GPU": {"avg": 35.79, "peak": 39.77, "min": 30.73}, "VIN": {"avg": 83.92, "peak": 112.38, "min": 61.49}}, "power_watts_avg": 35.79, "energy_joules_est": 33.28, "sample_count": 7, "duration_seconds": 0.93}, "timestamp": "2026-01-17T16:02:38.992463"}
{"image_index": 461, "image_name": "000000051976.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051976.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1729.126, "latencies_ms": [1729.126], "images_per_second": 0.578, "prompt_tokens": 23, "response_tokens_est": 40, "n_tiles": 6, "output_text": "object: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24953.7, "ram_available_mb": 100818.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 24953.9, "ram_available_mb": 100818.3, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.96, "peak": 15.34, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.56, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 33.87, "peak": 42.53, "min": 25.21}, "VIN": {"avg": 71.4, "peak": 103.45, "min": 52.57}}, "power_watts_avg": 33.87, "energy_joules_est": 58.58, "sample_count": 13, "duration_seconds": 1.73}, "timestamp": "2026-01-17T16:02:40.727634"}
{"image_index": 461, "image_name": "000000051976.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051976.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2704.879, "latencies_ms": [2704.879], "images_per_second": 0.37, "prompt_tokens": 27, "response_tokens_est": 73, "n_tiles": 6, "output_text": "The main object in the foreground is a surfer riding a wave. The surfer is positioned near the center of the image, slightly to the right. The background consists of the vast blue ocean, which extends to the right and left of the surfer. The wave is breaking to the right, creating a white foam that partially obscures the surfer.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24953.9, "ram_available_mb": 100818.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 24954.6, "ram_available_mb": 100817.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.91, "peak": 15.34, "min": 14.4}, "VDD_CPU_SOC_MSS": {"avg": 15.6, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 28.96, "peak": 40.97, "min": 22.86}, "VIN": {"avg": 71.39, "peak": 117.96, "min": 60.89}}, "power_watts_avg": 28.96, "energy_joules_est": 78.35, "sample_count": 21, "duration_seconds": 2.705}, "timestamp": "2026-01-17T16:02:43.439285"}
{"image_index": 461, "image_name": "000000051976.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051976.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2610.134, "latencies_ms": [2610.134], "images_per_second": 0.383, "prompt_tokens": 21, "response_tokens_est": 70, "n_tiles": 6, "output_text": "The image captures a dynamic scene of a surfer riding a wave in the ocean. The surfer is positioned on a white surfboard, skillfully navigating the wave, which is breaking to the right side of the surfer. The ocean is a deep blue, and the water is relatively calm, with small ripples around the surfer.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24954.6, "ram_available_mb": 100817.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 24955.4, "ram_available_mb": 100816.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.89, "peak": 15.24, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.51, "peak": 15.75, "min": 14.56}, "VDD_GPU": {"avg": 28.94, "peak": 40.18, "min": 22.86}, "VIN": {"avg": 70.21, "peak": 117.37, "min": 55.05}}, "power_watts_avg": 28.94, "energy_joules_est": 75.55, "sample_count": 20, "duration_seconds": 2.61}, "timestamp": "2026-01-17T16:02:46.055282"}
{"image_index": 461, "image_name": "000000051976.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051976.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2360.173, "latencies_ms": [2360.173], "images_per_second": 0.424, "prompt_tokens": 19, "response_tokens_est": 61, "n_tiles": 6, "output_text": "The image depicts a scene of a surfer riding a wave in the ocean. The water is a deep blue, and the wave is white, indicating a strong and powerful wave. The lighting is natural, with the sun shining from the left side, casting a blue hue over the entire scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24955.6, "ram_available_mb": 100816.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 24955.6, "ram_available_mb": 100816.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.86, "peak": 15.24, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.31, "peak": 15.74, "min": 14.57}, "VDD_GPU": {"avg": 29.59, "peak": 40.57, "min": 23.24}, "VIN": {"avg": 70.09, "peak": 112.17, "min": 61.52}}, "power_watts_avg": 29.59, "energy_joules_est": 69.86, "sample_count": 18, "duration_seconds": 2.361}, "timestamp": "2026-01-17T16:02:48.421951"}
{"image_index": 462, "image_name": "000000052007.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052007.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 4326.555, "latencies_ms": [4326.555], "images_per_second": 0.231, "prompt_tokens": 9, "response_tokens_est": 80, "n_tiles": 12, "output_text": "A yellow double-decker bus is parked on a street, with a sign displaying the destination \"Lytham 11\" and the number \"346\" on its front. Two men are standing on the sidewalk, one of whom is looking at the bus while the other is engaged in a conversation. The street is lined with a row of flowers, and there is a red brick pavement.", "error": null, "sys_before": {"cpu_percent": 41.5, "ram_used_mb": 24920.4, "ram_available_mb": 100851.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 24921.6, "ram_available_mb": 100850.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.11, "peak": 16.76, "min": 13.49}, "VDD_CPU_SOC_MSS": {"avg": 15.65, "peak": 17.71, "min": 14.18}, "VDD_GPU": {"avg": 31.95, "peak": 44.12, "min": 25.6}, "VIN": {"avg": 72.46, "peak": 97.14, "min": 58.03}}, "power_watts_avg": 31.95, "energy_joules_est": 138.24, "sample_count": 34, "duration_seconds": 4.327}, "timestamp": "2026-01-17T16:02:52.832041"}
{"image_index": 462, "image_name": "000000052007.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052007.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2721.092, "latencies_ms": [2721.092], "images_per_second": 0.367, "prompt_tokens": 23, "response_tokens_est": 31, "n_tiles": 12, "output_text": "bus: 1\npeople: 2\nbuilding: 1\nstreet: 1\nsign: 1\ntrees: 0", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24921.6, "ram_available_mb": 100850.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.6, "ram_used_mb": 24922.1, "ram_available_mb": 100850.0, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.23, "peak": 16.56, "min": 14.12}, "VDD_CPU_SOC_MSS": {"avg": 15.82, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 36.69, "peak": 45.68, "min": 27.18}, "VIN": {"avg": 78.2, "peak": 112.64, "min": 56.43}}, "power_watts_avg": 36.69, "energy_joules_est": 99.85, "sample_count": 21, "duration_seconds": 2.722}, "timestamp": "2026-01-17T16:02:55.559442"}
{"image_index": 462, "image_name": "000000052007.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052007.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 4603.666, "latencies_ms": [4603.666], "images_per_second": 0.217, "prompt_tokens": 27, "response_tokens_est": 86, "n_tiles": 12, "output_text": "The bus is parked on the side of the road, with the number 346 displayed on its front. The bus is positioned near the curb, with the number 11 on the digital display. In the background, there is a building with a red roof and a blue structure, possibly a part of the bus station or a nearby building. The sidewalk is visible in the foreground, with a few people standing near the bus.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24922.1, "ram_available_mb": 100850.0, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 24922.1, "ram_available_mb": 100850.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.16, "peak": 16.56, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.64, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 32.37, "peak": 45.3, "min": 25.6}, "VIN": {"avg": 76.39, "peak": 138.73, "min": 59.18}}, "power_watts_avg": 32.37, "energy_joules_est": 149.03, "sample_count": 36, "duration_seconds": 4.604}, "timestamp": "2026-01-17T16:03:00.169538"}
{"image_index": 462, "image_name": "000000052007.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052007.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 4792.874, "latencies_ms": [4792.874], "images_per_second": 0.209, "prompt_tokens": 21, "response_tokens_est": 91, "n_tiles": 12, "output_text": "The image depicts a scene on a city street with a yellow double-decker bus displaying the destination \"Lytham 11\" on its digital sign. The bus is parked on the side of the road, and two men are standing nearby, one of whom is looking at the bus while the other is engaged in a conversation. The street is lined with a mix of brick and paving stones, and there are some flowers in the foreground.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24922.1, "ram_available_mb": 100850.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 24923.3, "ram_available_mb": 100848.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.88, "peak": 16.36, "min": 14.02}, "VDD_CPU_SOC_MSS": {"avg": 15.27, "peak": 16.92, "min": 14.56}, "VDD_GPU": {"avg": 31.78, "peak": 45.69, "min": 25.6}, "VIN": {"avg": 73.43, "peak": 114.87, "min": 56.5}}, "power_watts_avg": 31.78, "energy_joules_est": 152.33, "sample_count": 38, "duration_seconds": 4.793}, "timestamp": "2026-01-17T16:03:04.968993"}
{"image_index": 462, "image_name": "000000052007.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052007.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3099.574, "latencies_ms": [3099.574], "images_per_second": 0.323, "prompt_tokens": 19, "response_tokens_est": 42, "n_tiles": 12, "output_text": "The bus in the image is yellow with black accents, featuring a large digital display showing the route number and destination. The scene is well-lit, with a clear sky and a bright, sunny day.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24923.3, "ram_available_mb": 100848.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 24924.3, "ram_available_mb": 100847.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.92, "peak": 16.26, "min": 13.79}, "VDD_CPU_SOC_MSS": {"avg": 15.34, "peak": 17.32, "min": 14.17}, "VDD_GPU": {"avg": 35.42, "peak": 46.09, "min": 26.0}, "VIN": {"avg": 76.92, "peak": 119.53, "min": 65.35}}, "power_watts_avg": 35.42, "energy_joules_est": 109.81, "sample_count": 24, "duration_seconds": 3.1}, "timestamp": "2026-01-17T16:03:08.075591"}
{"image_index": 463, "image_name": "000000052017.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052017.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1439.014, "latencies_ms": [1439.014], "images_per_second": 0.695, "prompt_tokens": 9, "response_tokens_est": 31, "n_tiles": 6, "output_text": "A small yellow and red airplane is captured in mid-flight against a cloudy sky, with its landing gear extended and the propeller in motion.", "error": null, "sys_before": {"cpu_percent": 41.9, "ram_used_mb": 24956.2, "ram_available_mb": 100815.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 24957.5, "ram_available_mb": 100814.7, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.64, "peak": 15.11, "min": 13.61}, "VDD_CPU_SOC_MSS": {"avg": 15.25, "peak": 15.75, "min": 14.17}, "VDD_GPU": {"avg": 33.26, "peak": 40.95, "min": 26.01}, "VIN": {"avg": 78.52, "peak": 120.94, "min": 61.35}}, "power_watts_avg": 33.26, "energy_joules_est": 47.87, "sample_count": 11, "duration_seconds": 1.439}, "timestamp": "2026-01-17T16:03:09.577938"}
{"image_index": 463, "image_name": "000000052017.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052017.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1727.986, "latencies_ms": [1727.986], "images_per_second": 0.579, "prompt_tokens": 23, "response_tokens_est": 40, "n_tiles": 6, "output_text": "object: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24957.5, "ram_available_mb": 100814.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 24958.7, "ram_available_mb": 100813.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.01, "peak": 15.44, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.6, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 32.51, "peak": 40.57, "min": 24.82}, "VIN": {"avg": 71.81, "peak": 108.2, "min": 59.99}}, "power_watts_avg": 32.51, "energy_joules_est": 56.19, "sample_count": 13, "duration_seconds": 1.728}, "timestamp": "2026-01-17T16:03:11.312188"}
{"image_index": 463, "image_name": "000000052017.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052017.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2381.122, "latencies_ms": [2381.122], "images_per_second": 0.42, "prompt_tokens": 27, "response_tokens_est": 64, "n_tiles": 6, "output_text": "The main object in the image is a small yellow airplane with a red tail and landing gear. The airplane is positioned in the foreground, slightly to the left, and is in the process of landing. The background is a cloudy sky, providing a neutral backdrop that contrasts with the bright colors of the airplane.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 24958.7, "ram_available_mb": 100813.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 24958.9, "ram_available_mb": 100813.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.07, "peak": 15.54, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.64, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 29.96, "peak": 40.18, "min": 23.64}, "VIN": {"avg": 72.1, "peak": 116.69, "min": 62.27}}, "power_watts_avg": 29.96, "energy_joules_est": 71.35, "sample_count": 18, "duration_seconds": 2.381}, "timestamp": "2026-01-17T16:03:13.698802"}
{"image_index": 463, "image_name": "000000052017.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052017.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2005.552, "latencies_ms": [2005.552], "images_per_second": 0.499, "prompt_tokens": 21, "response_tokens_est": 51, "n_tiles": 6, "output_text": "The image depicts a small, yellow and red airplane in mid-flight against a cloudy sky. The airplane is captured in a side profile view, with its landing gear retracted, indicating that it is either taking off or preparing to land.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24958.9, "ram_available_mb": 100813.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 24958.9, "ram_available_mb": 100813.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.1, "peak": 15.54, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.64, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 30.93, "peak": 40.16, "min": 24.42}, "VIN": {"avg": 73.45, "peak": 115.1, "min": 58.37}}, "power_watts_avg": 30.93, "energy_joules_est": 62.04, "sample_count": 15, "duration_seconds": 2.006}, "timestamp": "2026-01-17T16:03:15.710416"}
{"image_index": 463, "image_name": "000000052017.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052017.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2109.738, "latencies_ms": [2109.738], "images_per_second": 0.474, "prompt_tokens": 19, "response_tokens_est": 55, "n_tiles": 6, "output_text": "The notable visual attributes of the aircraft in the image include its bright yellow and red color scheme, which stands out against the overcast sky. The lighting is soft and diffused, likely due to the cloudy weather, casting a gentle glow on the aircraft's surface.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24958.9, "ram_available_mb": 100813.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 24959.2, "ram_available_mb": 100813.0, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.14, "peak": 15.54, "min": 14.43}, "VDD_CPU_SOC_MSS": {"avg": 15.72, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 30.77, "peak": 40.16, "min": 24.03}, "VIN": {"avg": 71.15, "peak": 103.3, "min": 63.15}}, "power_watts_avg": 30.77, "energy_joules_est": 64.93, "sample_count": 16, "duration_seconds": 2.11}, "timestamp": "2026-01-17T16:03:17.827228"}
{"image_index": 464, "image_name": "000000052412.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052412.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1378.232, "latencies_ms": [1378.232], "images_per_second": 0.726, "prompt_tokens": 9, "response_tokens_est": 29, "n_tiles": 6, "output_text": "The image shows a large parking lot filled with numerous cars, with a view of a cityscape and a clear blue sky in the background.", "error": null, "sys_before": {"cpu_percent": 47.1, "ram_used_mb": 24959.2, "ram_available_mb": 100813.0, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 24960.4, "ram_available_mb": 100811.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.79, "peak": 15.24, "min": 14.0}, "VDD_CPU_SOC_MSS": {"avg": 15.47, "peak": 16.14, "min": 14.57}, "VDD_GPU": {"avg": 33.6, "peak": 40.16, "min": 26.79}, "VIN": {"avg": 77.48, "peak": 108.34, "min": 52.65}}, "power_watts_avg": 33.6, "energy_joules_est": 46.32, "sample_count": 10, "duration_seconds": 1.379}, "timestamp": "2026-01-17T16:03:19.255782"}
{"image_index": 464, "image_name": "000000052412.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052412.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1710.231, "latencies_ms": [1710.231], "images_per_second": 0.585, "prompt_tokens": 23, "response_tokens_est": 40, "n_tiles": 6, "output_text": "object: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24960.4, "ram_available_mb": 100811.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 24960.6, "ram_available_mb": 100811.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.07, "peak": 15.44, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.6, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 32.69, "peak": 40.95, "min": 24.82}, "VIN": {"avg": 71.99, "peak": 111.42, "min": 61.9}}, "power_watts_avg": 32.69, "energy_joules_est": 55.92, "sample_count": 13, "duration_seconds": 1.711}, "timestamp": "2026-01-17T16:03:20.972142"}
{"image_index": 464, "image_name": "000000052412.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052412.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2644.771, "latencies_ms": [2644.771], "images_per_second": 0.378, "prompt_tokens": 27, "response_tokens_est": 74, "n_tiles": 6, "output_text": "The main objects in the image are a parking lot filled with numerous cars, a large building in the background, and a clear blue sky. The parking lot is located in the foreground, with the cars occupying the majority of the space. The building is situated in the background, slightly to the right, and the clear blue sky is visible above the parking lot.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24960.6, "ram_available_mb": 100811.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 24960.9, "ram_available_mb": 100811.3, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.11, "peak": 15.44, "min": 14.43}, "VDD_CPU_SOC_MSS": {"avg": 15.84, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 29.37, "peak": 41.34, "min": 23.64}, "VIN": {"avg": 72.93, "peak": 112.29, "min": 63.6}}, "power_watts_avg": 29.37, "energy_joules_est": 77.69, "sample_count": 21, "duration_seconds": 2.645}, "timestamp": "2026-01-17T16:03:23.622985"}
{"image_index": 464, "image_name": "000000052412.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052412.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1843.144, "latencies_ms": [1843.144], "images_per_second": 0.543, "prompt_tokens": 21, "response_tokens_est": 45, "n_tiles": 6, "output_text": "The image depicts an aerial view of a large parking lot filled with numerous cars, situated in an urban area. The parking lot is surrounded by a mix of commercial and residential buildings, with a clear blue sky above.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24960.9, "ram_available_mb": 100811.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 24961.1, "ram_available_mb": 100811.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.08, "peak": 15.44, "min": 14.43}, "VDD_CPU_SOC_MSS": {"avg": 15.66, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 31.94, "peak": 40.97, "min": 24.83}, "VIN": {"avg": 75.96, "peak": 120.59, "min": 57.5}}, "power_watts_avg": 31.94, "energy_joules_est": 58.88, "sample_count": 14, "duration_seconds": 1.844}, "timestamp": "2026-01-17T16:03:25.472319"}
{"image_index": 464, "image_name": "000000052412.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052412.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1823.74, "latencies_ms": [1823.74], "images_per_second": 0.548, "prompt_tokens": 19, "response_tokens_est": 44, "n_tiles": 6, "output_text": "The image shows a large parking lot filled with numerous cars, with a clear blue sky overhead. The parking lot is surrounded by green trees and buildings, and the lighting is bright and natural, indicating a sunny day.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24961.1, "ram_available_mb": 100811.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 24961.4, "ram_available_mb": 100810.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.1, "peak": 15.65, "min": 14.43}, "VDD_CPU_SOC_MSS": {"avg": 15.63, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 32.15, "peak": 40.56, "min": 24.82}, "VIN": {"avg": 71.94, "peak": 109.59, "min": 59.2}}, "power_watts_avg": 32.15, "energy_joules_est": 58.65, "sample_count": 13, "duration_seconds": 1.824}, "timestamp": "2026-01-17T16:03:27.302236"}
{"image_index": 465, "image_name": "000000052413.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052413.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2384.835, "latencies_ms": [2384.835], "images_per_second": 0.419, "prompt_tokens": 9, "response_tokens_est": 23, "n_tiles": 12, "output_text": "A child is holding a pink toy phone with a cartoon character on the screen, while sitting on a couch.", "error": null, "sys_before": {"cpu_percent": 46.3, "ram_used_mb": 24921.9, "ram_available_mb": 100850.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.4, "ram_used_mb": 24922.4, "ram_available_mb": 100849.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.23, "peak": 16.76, "min": 13.59}, "VDD_CPU_SOC_MSS": {"avg": 16.07, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 36.7, "peak": 43.71, "min": 28.36}, "VIN": {"avg": 78.72, "peak": 106.82, "min": 67.1}}, "power_watts_avg": 36.7, "energy_joules_est": 87.54, "sample_count": 18, "duration_seconds": 2.385}, "timestamp": "2026-01-17T16:03:29.780219"}
{"image_index": 465, "image_name": "000000052413.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052413.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 3089.842, "latencies_ms": [3089.842], "images_per_second": 0.324, "prompt_tokens": 23, "response_tokens_est": 42, "n_tiles": 12, "output_text": "- hand: 1\n- phone: 1\n- book: 1\n- cup: 1\n- chair: 1\n- gift: 1\n- blanket: 1", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 24922.4, "ram_available_mb": 100849.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 24923.6, "ram_available_mb": 100848.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.27, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.86, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 35.46, "peak": 45.28, "min": 26.39}, "VIN": {"avg": 76.17, "peak": 109.95, "min": 66.01}}, "power_watts_avg": 35.46, "energy_joules_est": 109.58, "sample_count": 24, "duration_seconds": 3.09}, "timestamp": "2026-01-17T16:03:32.876370"}
{"image_index": 465, "image_name": "000000052413.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052413.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3918.996, "latencies_ms": [3918.996], "images_per_second": 0.255, "prompt_tokens": 27, "response_tokens_est": 66, "n_tiles": 12, "output_text": "The main object in the foreground is a child's hand holding a pink mobile phone with a cartoon character on the screen. The child is sitting on a couch, and there is a magazine or book with a visible title on the couch. The background features a can of soda and a cup, as well as a wrapped gift.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24923.6, "ram_available_mb": 100848.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 24923.6, "ram_available_mb": 100848.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.19, "peak": 16.56, "min": 14.12}, "VDD_CPU_SOC_MSS": {"avg": 15.79, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 33.26, "peak": 44.89, "min": 26.0}, "VIN": {"avg": 75.91, "peak": 113.51, "min": 57.12}}, "power_watts_avg": 33.26, "energy_joules_est": 130.36, "sample_count": 30, "duration_seconds": 3.919}, "timestamp": "2026-01-17T16:03:36.801826"}
{"image_index": 465, "image_name": "000000052413.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052413.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3639.146, "latencies_ms": [3639.146], "images_per_second": 0.275, "prompt_tokens": 21, "response_tokens_est": 58, "n_tiles": 12, "output_text": "The image depicts a cozy indoor setting where a person is seated on a couch, holding a pink mobile phone with a cartoon character on the screen. The person is also holding a magazine or book, and there are various items on the couch, including a can and a wrapped gift.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24923.6, "ram_available_mb": 100848.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 24924.1, "ram_available_mb": 100848.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.21, "peak": 16.56, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.87, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 33.78, "peak": 44.91, "min": 26.0}, "VIN": {"avg": 78.37, "peak": 114.95, "min": 58.03}}, "power_watts_avg": 33.78, "energy_joules_est": 122.95, "sample_count": 28, "duration_seconds": 3.64}, "timestamp": "2026-01-17T16:03:40.447352"}
{"image_index": 465, "image_name": "000000052413.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052413.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3567.387, "latencies_ms": [3567.387], "images_per_second": 0.28, "prompt_tokens": 19, "response_tokens_est": 56, "n_tiles": 12, "output_text": "The image features a person holding a pink mobile phone with a cartoon character on the screen. The phone is encased in a pink case, and the person is wearing a white shirt. The lighting is dim, and the background is blurred, suggesting a cozy indoor setting.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24924.1, "ram_available_mb": 100848.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 24923.9, "ram_available_mb": 100848.3, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.2, "peak": 16.56, "min": 14.12}, "VDD_CPU_SOC_MSS": {"avg": 15.72, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 34.1, "peak": 44.89, "min": 26.0}, "VIN": {"avg": 76.35, "peak": 103.1, "min": 62.61}}, "power_watts_avg": 34.1, "energy_joules_est": 121.66, "sample_count": 27, "duration_seconds": 3.568}, "timestamp": "2026-01-17T16:03:44.022384"}
{"image_index": 466, "image_name": "000000052462.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052462.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1060.393, "latencies_ms": [1060.393], "images_per_second": 0.943, "prompt_tokens": 9, "response_tokens_est": 17, "n_tiles": 6, "output_text": "A zebra is standing in a field with tall, dry grass around it.", "error": null, "sys_before": {"cpu_percent": 37.1, "ram_used_mb": 24949.0, "ram_available_mb": 100823.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 24949.8, "ram_available_mb": 100822.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.5, "peak": 15.04, "min": 13.72}, "VDD_CPU_SOC_MSS": {"avg": 15.07, "peak": 15.74, "min": 14.17}, "VDD_GPU": {"avg": 36.24, "peak": 40.95, "min": 30.73}, "VIN": {"avg": 82.11, "peak": 104.7, "min": 65.78}}, "power_watts_avg": 36.24, "energy_joules_est": 38.44, "sample_count": 7, "duration_seconds": 1.061}, "timestamp": "2026-01-17T16:03:45.154503"}
{"image_index": 466, "image_name": "000000052462.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052462.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1081.191, "latencies_ms": [1081.191], "images_per_second": 0.925, "prompt_tokens": 23, "response_tokens_est": 16, "n_tiles": 6, "output_text": "zebra: 1\ntree: 1\ngrass: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24949.8, "ram_available_mb": 100822.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 24951.2, "ram_available_mb": 100820.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.86, "peak": 15.44, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.46, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 38.31, "peak": 41.74, "min": 32.3}, "VIN": {"avg": 82.42, "peak": 116.25, "min": 58.36}}, "power_watts_avg": 38.31, "energy_joules_est": 41.44, "sample_count": 7, "duration_seconds": 1.082}, "timestamp": "2026-01-17T16:03:46.243988"}
{"image_index": 466, "image_name": "000000052462.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052462.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2570.731, "latencies_ms": [2570.731], "images_per_second": 0.389, "prompt_tokens": 27, "response_tokens_est": 68, "n_tiles": 6, "output_text": "The main objects in the image are two zebras standing in a grassy field. The foreground features a zebra with a black and white striped coat, while the background shows another zebra with a similar pattern. The zebra in the foreground is closer to the camera, while the one in the background is further away.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24951.2, "ram_available_mb": 100820.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 24951.0, "ram_available_mb": 100821.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.87, "peak": 15.34, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.39, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 30.02, "peak": 42.13, "min": 22.86}, "VIN": {"avg": 69.0, "peak": 110.89, "min": 57.35}}, "power_watts_avg": 30.02, "energy_joules_est": 77.19, "sample_count": 19, "duration_seconds": 2.571}, "timestamp": "2026-01-17T16:03:48.820636"}
{"image_index": 466, "image_name": "000000052462.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052462.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1996.608, "latencies_ms": [1996.608], "images_per_second": 0.501, "prompt_tokens": 21, "response_tokens_est": 48, "n_tiles": 6, "output_text": "The image depicts a serene African savanna landscape with a zebra standing in the foreground. The zebra is surrounded by tall, dry grass, and the scene is set under a clear blue sky with a few scattered clouds.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24951.0, "ram_available_mb": 100821.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 24951.2, "ram_available_mb": 100820.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.82, "peak": 15.24, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.46, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 30.62, "peak": 40.16, "min": 23.64}, "VIN": {"avg": 70.58, "peak": 98.28, "min": 60.41}}, "power_watts_avg": 30.62, "energy_joules_est": 61.15, "sample_count": 15, "duration_seconds": 1.997}, "timestamp": "2026-01-17T16:03:50.826244"}
{"image_index": 466, "image_name": "000000052462.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052462.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2321.266, "latencies_ms": [2321.266], "images_per_second": 0.431, "prompt_tokens": 19, "response_tokens_est": 60, "n_tiles": 6, "output_text": "The image features a zebra standing in a dry, grassy savanna. The zebra's distinctive black and white stripes are clearly visible against the golden-brown grass. The lighting is bright and natural, suggesting a sunny day, with the sun casting shadows on the zebra's body.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24951.2, "ram_available_mb": 100820.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 24951.2, "ram_available_mb": 100820.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.9, "peak": 15.24, "min": 14.2}, "VDD_CPU_SOC_MSS": {"avg": 15.56, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 29.89, "peak": 40.56, "min": 23.24}, "VIN": {"avg": 70.76, "peak": 120.4, "min": 55.21}}, "power_watts_avg": 29.89, "energy_joules_est": 69.4, "sample_count": 17, "duration_seconds": 2.322}, "timestamp": "2026-01-17T16:03:53.157758"}
{"image_index": 467, "image_name": "000000052507.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052507.jpg", "image_width": 640, "image_height": 438, "image_resolution": "640x438", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1148.534, "latencies_ms": [1148.534], "images_per_second": 0.871, "prompt_tokens": 9, "response_tokens_est": 20, "n_tiles": 6, "output_text": "A man is standing in the water holding a yellow surfboard, ready to catch a wave.", "error": null, "sys_before": {"cpu_percent": 43.4, "ram_used_mb": 24951.2, "ram_available_mb": 100820.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 24951.2, "ram_available_mb": 100821.0, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.54, "peak": 15.14, "min": 13.69}, "VDD_CPU_SOC_MSS": {"avg": 15.25, "peak": 16.14, "min": 14.18}, "VDD_GPU": {"avg": 34.76, "peak": 40.16, "min": 28.76}, "VIN": {"avg": 78.53, "peak": 110.37, "min": 63.42}}, "power_watts_avg": 34.76, "energy_joules_est": 39.93, "sample_count": 8, "duration_seconds": 1.149}, "timestamp": "2026-01-17T16:03:54.368924"}
{"image_index": 467, "image_name": "000000052507.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052507.jpg", "image_width": 640, "image_height": 438, "image_resolution": "640x438", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1732.64, "latencies_ms": [1732.64], "images_per_second": 0.577, "prompt_tokens": 23, "response_tokens_est": 40, "n_tiles": 6, "output_text": "object: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24951.2, "ram_available_mb": 100821.0, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 24951.5, "ram_available_mb": 100820.7, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.95, "peak": 15.44, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.58, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 33.61, "peak": 41.34, "min": 25.21}, "VIN": {"avg": 73.01, "peak": 89.91, "min": 63.04}}, "power_watts_avg": 33.61, "energy_joules_est": 58.25, "sample_count": 12, "duration_seconds": 1.733}, "timestamp": "2026-01-17T16:03:56.107590"}
{"image_index": 467, "image_name": "000000052507.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052507.jpg", "image_width": 640, "image_height": 438, "image_resolution": "640x438", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2631.741, "latencies_ms": [2631.741], "images_per_second": 0.38, "prompt_tokens": 27, "response_tokens_est": 73, "n_tiles": 6, "output_text": "The main object in the foreground is a person standing in shallow water, holding a bright yellow surfboard. The person is positioned near the center of the image, slightly to the left. The background features the vast expanse of the ocean, with waves crashing towards the shore. The surfboard is positioned near the person, closer to the water's edge.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24951.5, "ram_available_mb": 100820.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 24951.7, "ram_available_mb": 100820.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.05, "peak": 15.44, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.66, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 29.48, "peak": 40.16, "min": 23.64}, "VIN": {"avg": 70.96, "peak": 115.16, "min": 51.6}}, "power_watts_avg": 29.48, "energy_joules_est": 77.59, "sample_count": 19, "duration_seconds": 2.632}, "timestamp": "2026-01-17T16:03:58.745307"}
{"image_index": 467, "image_name": "000000052507.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052507.jpg", "image_width": 640, "image_height": 438, "image_resolution": "640x438", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2374.418, "latencies_ms": [2374.418], "images_per_second": 0.421, "prompt_tokens": 21, "response_tokens_est": 62, "n_tiles": 6, "output_text": "The image depicts a scene at the beach with a person standing in shallow water, holding a bright yellow surfboard. The person is facing away from the camera, and the ocean waves are crashing around them. The setting is a sunny day at the beach, with clear blue skies and calm ocean waters.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24951.7, "ram_available_mb": 100820.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 24952.0, "ram_available_mb": 100820.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.98, "peak": 15.54, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.66, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 29.87, "peak": 40.16, "min": 23.25}, "VIN": {"avg": 68.96, "peak": 89.26, "min": 50.91}}, "power_watts_avg": 29.87, "energy_joules_est": 70.94, "sample_count": 17, "duration_seconds": 2.375}, "timestamp": "2026-01-17T16:04:01.125892"}
{"image_index": 467, "image_name": "000000052507.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052507.jpg", "image_width": 640, "image_height": 438, "image_resolution": "640x438", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1914.193, "latencies_ms": [1914.193], "images_per_second": 0.522, "prompt_tokens": 19, "response_tokens_est": 45, "n_tiles": 6, "output_text": "The image depicts a bright yellow surfboard with a black stripe and a black handle, being held by a person in the water. The scene is set on a sunny day with clear blue skies and calm ocean waves.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24952.0, "ram_available_mb": 100820.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 24952.0, "ram_available_mb": 100820.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.83, "peak": 15.24, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.33, "peak": 15.75, "min": 14.56}, "VDD_GPU": {"avg": 31.09, "peak": 40.18, "min": 23.64}, "VIN": {"avg": 73.47, "peak": 107.0, "min": 60.81}}, "power_watts_avg": 31.09, "energy_joules_est": 59.53, "sample_count": 14, "duration_seconds": 1.915}, "timestamp": "2026-01-17T16:04:03.046197"}
{"image_index": 468, "image_name": "000000052565.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052565.jpg", "image_width": 640, "image_height": 458, "image_resolution": "640x458", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2264.612, "latencies_ms": [2264.612], "images_per_second": 0.442, "prompt_tokens": 9, "response_tokens_est": 19, "n_tiles": 12, "output_text": "A black and white image of a cow standing on a sandy beach near the water.", "error": null, "sys_before": {"cpu_percent": 45.8, "ram_used_mb": 24925.4, "ram_available_mb": 100846.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.3, "ram_used_mb": 24926.9, "ram_available_mb": 100845.3, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.16, "peak": 16.66, "min": 13.29}, "VDD_CPU_SOC_MSS": {"avg": 15.88, "peak": 17.71, "min": 14.17}, "VDD_GPU": {"avg": 37.02, "peak": 43.31, "min": 29.15}, "VIN": {"avg": 80.56, "peak": 103.33, "min": 66.97}}, "power_watts_avg": 37.02, "energy_joules_est": 83.85, "sample_count": 17, "duration_seconds": 2.265}, "timestamp": "2026-01-17T16:04:05.408474"}
{"image_index": 468, "image_name": "000000052565.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052565.jpg", "image_width": 640, "image_height": 458, "image_resolution": "640x458", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2758.937, "latencies_ms": [2758.937], "images_per_second": 0.362, "prompt_tokens": 23, "response_tokens_est": 32, "n_tiles": 12, "output_text": "1. Cow\n2. Cow\n3. Cow\n4. Cow\n5. Cow\n6. Cow\n7. Cow\n8. Cow", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24926.9, "ram_available_mb": 100845.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.6, "ram_used_mb": 24927.8, "ram_available_mb": 100844.3, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.26, "peak": 16.56, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.86, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 36.84, "peak": 45.28, "min": 27.18}, "VIN": {"avg": 82.12, "peak": 127.82, "min": 53.26}}, "power_watts_avg": 36.84, "energy_joules_est": 101.65, "sample_count": 20, "duration_seconds": 2.759}, "timestamp": "2026-01-17T16:04:08.173992"}
{"image_index": 468, "image_name": "000000052565.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052565.jpg", "image_width": 640, "image_height": 458, "image_resolution": "640x458", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 4456.132, "latencies_ms": [4456.132], "images_per_second": 0.224, "prompt_tokens": 27, "response_tokens_est": 82, "n_tiles": 12, "output_text": "The main object in the image is a cow standing on the sandy beach. The cow is positioned in the foreground, with its body facing the camera and its head turned slightly to the side. The background consists of the calm sea, which is slightly blurred, indicating a shallow depth of field. The cow is near the water's edge, with its legs and tail extending into the sandy beach.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24927.8, "ram_available_mb": 100844.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 24927.8, "ram_available_mb": 100844.3, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.17, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.66, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 32.47, "peak": 45.28, "min": 26.0}, "VIN": {"avg": 74.44, "peak": 116.43, "min": 56.92}}, "power_watts_avg": 32.47, "energy_joules_est": 144.7, "sample_count": 35, "duration_seconds": 4.456}, "timestamp": "2026-01-17T16:04:12.637177"}
{"image_index": 468, "image_name": "000000052565.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052565.jpg", "image_width": 640, "image_height": 458, "image_resolution": "640x458", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3932.855, "latencies_ms": [3932.855], "images_per_second": 0.254, "prompt_tokens": 21, "response_tokens_est": 67, "n_tiles": 12, "output_text": "The image depicts a black and white scene of a cow standing on a sandy beach near a body of water. The cow appears calm and is facing the camera, with its body partially submerged in the water. The setting is serene, with the cow's presence adding a touch of rural tranquility to the scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24927.8, "ram_available_mb": 100844.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 24928.1, "ram_available_mb": 100844.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.22, "peak": 16.56, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.69, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 33.26, "peak": 44.91, "min": 26.0}, "VIN": {"avg": 76.74, "peak": 112.32, "min": 62.21}}, "power_watts_avg": 33.26, "energy_joules_est": 130.82, "sample_count": 30, "duration_seconds": 3.933}, "timestamp": "2026-01-17T16:04:16.576673"}
{"image_index": 468, "image_name": "000000052565.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052565.jpg", "image_width": 640, "image_height": 458, "image_resolution": "640x458", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3868.151, "latencies_ms": [3868.151], "images_per_second": 0.259, "prompt_tokens": 19, "response_tokens_est": 65, "n_tiles": 12, "output_text": "The image is a black and white photograph featuring a cow standing on a sandy beach near the water. The cow has a predominantly black and white coat with white patches on its face and legs. The lighting is soft and diffused, casting gentle shadows on the sand, and the overall atmosphere is calm and serene.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24928.1, "ram_available_mb": 100844.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 24928.1, "ram_available_mb": 100844.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.2, "peak": 16.56, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.69, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 33.32, "peak": 44.89, "min": 26.0}, "VIN": {"avg": 72.42, "peak": 94.08, "min": 58.27}}, "power_watts_avg": 33.32, "energy_joules_est": 128.9, "sample_count": 30, "duration_seconds": 3.869}, "timestamp": "2026-01-17T16:04:20.451716"}
{"image_index": 469, "image_name": "000000052591.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052591.jpg", "image_width": 359, "image_height": 640, "image_resolution": "359x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1249.888, "latencies_ms": [1249.888], "images_per_second": 0.8, "prompt_tokens": 9, "response_tokens_est": 48, "n_tiles": 2, "output_text": "A woman in a black and white photo is standing on a snowy slope, holding a pair of skis and wearing a white sweater with a fur collar, a black belt, and black pants, with a pair of black boots.", "error": null, "sys_before": {"cpu_percent": 34.2, "ram_used_mb": 24934.0, "ram_available_mb": 100838.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 24934.2, "ram_available_mb": 100838.0, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5055.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.8, "peak": 15.62, "min": 14.1}, "VDD_CPU_SOC_MSS": {"avg": 15.62, "peak": 16.54, "min": 14.56}, "VDD_GPU": {"avg": 25.82, "peak": 33.48, "min": 22.06}, "VIN": {"avg": 66.47, "peak": 78.73, "min": 62.49}}, "power_watts_avg": 25.82, "energy_joules_est": 32.28, "sample_count": 9, "duration_seconds": 1.25}, "timestamp": "2026-01-17T16:04:21.734804"}
{"image_index": 469, "image_name": "000000052591.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052591.jpg", "image_width": 359, "image_height": 640, "image_resolution": "359x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 3051.626, "latencies_ms": [3051.626], "images_per_second": 0.328, "prompt_tokens": 23, "response_tokens_est": 128, "n_tiles": 2, "output_text": "woman: 1\nsnowsuit: 1\nglove: 1\nskis: 1\nskis pole: 1\nskis handle: 1\nskis belt: 1\nskis belt buckle: 1\nskis belt loop: 1\nskis belt strap: 1\nskis belt buckle: 1\nskis belt loop: 1\nskis belt strap: 1\nskis belt buckle: 1\nskis belt loop: 1\nskis belt strap: 1\nskis belt buckle:", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24934.2, "ram_available_mb": 100838.0, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24934.2, "ram_available_mb": 100838.0, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5064.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.25, "peak": 15.72, "min": 14.61}, "VDD_CPU_SOC_MSS": {"avg": 16.46, "peak": 16.95, "min": 15.75}, "VDD_GPU": {"avg": 22.57, "peak": 30.73, "min": 20.88}, "VIN": {"avg": 65.2, "peak": 91.82, "min": 60.24}}, "power_watts_avg": 22.57, "energy_joules_est": 68.89, "sample_count": 24, "duration_seconds": 3.052}, "timestamp": "2026-01-17T16:04:24.792897"}
{"image_index": 469, "image_name": "000000052591.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052591.jpg", "image_width": 359, "image_height": 640, "image_resolution": "359x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1260.682, "latencies_ms": [1260.682], "images_per_second": 0.793, "prompt_tokens": 27, "response_tokens_est": 48, "n_tiles": 2, "output_text": "The main object in the foreground is a woman standing on a snowy slope. She is holding a pair of skis and a ski pole. The background features a snowy landscape with a few trees and a partly cloudy sky.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24934.2, "ram_available_mb": 100838.0, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 24934.2, "ram_available_mb": 100838.0, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5067.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.22, "peak": 15.52, "min": 14.81}, "VDD_CPU_SOC_MSS": {"avg": 16.14, "peak": 16.54, "min": 15.74}, "VDD_GPU": {"avg": 25.0, "peak": 30.73, "min": 21.67}, "VIN": {"avg": 67.48, "peak": 89.75, "min": 61.67}}, "power_watts_avg": 25.0, "energy_joules_est": 31.53, "sample_count": 9, "duration_seconds": 1.261}, "timestamp": "2026-01-17T16:04:26.059342"}
{"image_index": 469, "image_name": "000000052591.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052591.jpg", "image_width": 359, "image_height": 640, "image_resolution": "359x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1939.307, "latencies_ms": [1939.307], "images_per_second": 0.516, "prompt_tokens": 21, "response_tokens_est": 78, "n_tiles": 2, "output_text": "The image depicts a black-and-white scene set in a snowy landscape, likely a ski resort or a snowy mountain area. A woman is standing on a snowy slope, holding a ski pole and wearing a winter outfit with a fur-lined jacket, a scarf, and gloves. She appears to be enjoying the snowy environment, smiling and looking towards the camera.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24934.2, "ram_available_mb": 100838.0, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 24934.9, "ram_available_mb": 100837.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5063.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.28, "peak": 15.62, "min": 14.91}, "VDD_CPU_SOC_MSS": {"avg": 16.06, "peak": 16.54, "min": 15.34}, "VDD_GPU": {"avg": 23.61, "peak": 31.12, "min": 20.88}, "VIN": {"avg": 64.25, "peak": 78.21, "min": 59.53}}, "power_watts_avg": 23.61, "energy_joules_est": 45.79, "sample_count": 14, "duration_seconds": 1.94}, "timestamp": "2026-01-17T16:04:28.004240"}
{"image_index": 469, "image_name": "000000052591.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052591.jpg", "image_width": 359, "image_height": 640, "image_resolution": "359x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1459.248, "latencies_ms": [1459.248], "images_per_second": 0.685, "prompt_tokens": 19, "response_tokens_est": 56, "n_tiles": 2, "output_text": "The image is a black and white photograph featuring a woman standing in a snowy landscape. She is dressed in a white sweater with a fur collar and a black skirt, accessorized with a necklace and gloves. The lighting is soft, and the sky is partly cloudy.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24934.9, "ram_available_mb": 100837.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 24935.2, "ram_available_mb": 100837.0, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5062.6, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.18, "peak": 15.42, "min": 14.61}, "VDD_CPU_SOC_MSS": {"avg": 16.0, "peak": 16.54, "min": 15.35}, "VDD_GPU": {"avg": 24.17, "peak": 29.94, "min": 21.28}, "VIN": {"avg": 65.85, "peak": 102.72, "min": 50.2}}, "power_watts_avg": 24.17, "energy_joules_est": 35.28, "sample_count": 11, "duration_seconds": 1.46}, "timestamp": "2026-01-17T16:04:29.469138"}
{"image_index": 470, "image_name": "000000052891.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052891.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1279.822, "latencies_ms": [1279.822], "images_per_second": 0.781, "prompt_tokens": 9, "response_tokens_est": 25, "n_tiles": 6, "output_text": "A dog is seen on a sandy beach, holding a yellow frisbee, with the ocean in the background.", "error": null, "sys_before": {"cpu_percent": 48.7, "ram_used_mb": 24954.9, "ram_available_mb": 100817.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 24956.8, "ram_available_mb": 100815.3, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.72, "peak": 15.24, "min": 14.0}, "VDD_CPU_SOC_MSS": {"avg": 15.44, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 33.79, "peak": 39.78, "min": 27.97}, "VIN": {"avg": 78.93, "peak": 115.58, "min": 62.49}}, "power_watts_avg": 33.79, "energy_joules_est": 43.26, "sample_count": 9, "duration_seconds": 1.28}, "timestamp": "2026-01-17T16:04:30.807392"}
{"image_index": 470, "image_name": "000000052891.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052891.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1766.643, "latencies_ms": [1766.643], "images_per_second": 0.566, "prompt_tokens": 23, "response_tokens_est": 40, "n_tiles": 6, "output_text": "object: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24956.8, "ram_available_mb": 100815.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 24957.6, "ram_available_mb": 100814.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.85, "peak": 15.34, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.32, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 32.51, "peak": 41.34, "min": 24.42}, "VIN": {"avg": 73.37, "peak": 113.32, "min": 63.01}}, "power_watts_avg": 32.51, "energy_joules_est": 57.45, "sample_count": 13, "duration_seconds": 1.767}, "timestamp": "2026-01-17T16:04:32.580115"}
{"image_index": 470, "image_name": "000000052891.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052891.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2392.381, "latencies_ms": [2392.381], "images_per_second": 0.418, "prompt_tokens": 27, "response_tokens_est": 62, "n_tiles": 6, "output_text": "The main object in the foreground is a yellow frisbee, which is held by a dog. The dog is positioned on the sandy beach, with its head close to the frisbee. In the background, the ocean and a small island are visible, indicating the dog is near the water.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24957.6, "ram_available_mb": 100814.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 24957.8, "ram_available_mb": 100814.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.88, "peak": 15.34, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.31, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 29.54, "peak": 40.56, "min": 23.25}, "VIN": {"avg": 69.77, "peak": 105.1, "min": 56.13}}, "power_watts_avg": 29.54, "energy_joules_est": 70.68, "sample_count": 18, "duration_seconds": 2.393}, "timestamp": "2026-01-17T16:04:34.978306"}
{"image_index": 470, "image_name": "000000052891.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052891.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2731.057, "latencies_ms": [2731.057], "images_per_second": 0.366, "prompt_tokens": 21, "response_tokens_est": 74, "n_tiles": 6, "output_text": "The image depicts a beach scene with a dog playing with a yellow frisbee. The dog, with its fur appearing wet and matted, is lying on the sandy beach, focusing intently on the frisbee. The beach is surrounded by calm, turquoise waters, and a small island is visible in the distance under a cloudy sky.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24957.8, "ram_available_mb": 100814.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 24957.8, "ram_available_mb": 100814.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.86, "peak": 15.24, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.48, "peak": 15.75, "min": 14.56}, "VDD_GPU": {"avg": 28.53, "peak": 40.95, "min": 22.85}, "VIN": {"avg": 69.73, "peak": 92.77, "min": 60.64}}, "power_watts_avg": 28.53, "energy_joules_est": 77.94, "sample_count": 21, "duration_seconds": 2.732}, "timestamp": "2026-01-17T16:04:37.715700"}
{"image_index": 470, "image_name": "000000052891.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052891.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2529.053, "latencies_ms": [2529.053], "images_per_second": 0.395, "prompt_tokens": 19, "response_tokens_est": 67, "n_tiles": 6, "output_text": "The image features a dog with a coat of black, white, and gray fur, standing on a sandy beach. The dog is holding a bright yellow frisbee, which contrasts sharply with the sandy background. The lighting is natural, suggesting daytime, and the sky appears overcast, with no direct sunlight visible.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24957.8, "ram_available_mb": 100814.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 24957.8, "ram_available_mb": 100814.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.87, "peak": 15.24, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.37, "peak": 15.75, "min": 14.56}, "VDD_GPU": {"avg": 29.15, "peak": 40.16, "min": 22.86}, "VIN": {"avg": 70.4, "peak": 105.33, "min": 58.92}}, "power_watts_avg": 29.15, "energy_joules_est": 73.73, "sample_count": 19, "duration_seconds": 2.53}, "timestamp": "2026-01-17T16:04:40.251298"}
{"image_index": 471, "image_name": "000000052996.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052996.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1940.406, "latencies_ms": [1940.406], "images_per_second": 0.515, "prompt_tokens": 9, "response_tokens_est": 50, "n_tiles": 6, "output_text": "The image depicts a group of individuals in a kitchen setting, with one person in a military uniform standing next to a large stainless steel refrigerator, while others are engaged in conversation around the counter, surrounded by various cooking utensils and ingredients.", "error": null, "sys_before": {"cpu_percent": 47.2, "ram_used_mb": 24957.8, "ram_available_mb": 100814.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 24958.3, "ram_available_mb": 100813.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.89, "peak": 15.32, "min": 13.9}, "VDD_CPU_SOC_MSS": {"avg": 15.52, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 30.98, "peak": 40.57, "min": 24.42}, "VIN": {"avg": 77.13, "peak": 109.36, "min": 59.78}}, "power_watts_avg": 30.98, "energy_joules_est": 60.12, "sample_count": 14, "duration_seconds": 1.941}, "timestamp": "2026-01-17T16:04:42.243169"}
{"image_index": 471, "image_name": "000000052996.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052996.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 4137.413, "latencies_ms": [4137.413], "images_per_second": 0.242, "prompt_tokens": 23, "response_tokens_est": 128, "n_tiles": 6, "output_text": "- Food: 1\n- Bowl: 1\n- Cutting board: 1\n- Pot: 1\n- Green bowl: 1\n- Green cup: 1\n- Green plastic container: 1\n- Green plastic bag: 1\n- Green plastic bottle: 1\n- Green plastic container: 1\n- Green plastic bag: 1\n- Green plastic bottle: 1\n- Green plastic container: 1\n- Green plastic bag: 1\n- Green plastic bottle: 1\n- Green plastic container: 1\n- Green plastic bag: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24958.3, "ram_available_mb": 100813.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 24958.3, "ram_available_mb": 100813.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.15, "peak": 15.54, "min": 14.43}, "VDD_CPU_SOC_MSS": {"avg": 15.69, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 27.01, "peak": 40.16, "min": 23.24}, "VIN": {"avg": 70.06, "peak": 114.48, "min": 57.57}}, "power_watts_avg": 27.01, "energy_joules_est": 111.76, "sample_count": 32, "duration_seconds": 4.138}, "timestamp": "2026-01-17T16:04:46.387300"}
{"image_index": 471, "image_name": "000000052996.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052996.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2409.354, "latencies_ms": [2409.354], "images_per_second": 0.415, "prompt_tokens": 27, "response_tokens_est": 65, "n_tiles": 6, "output_text": "In the image, the main objects are located in the foreground and background. The individuals in the foreground are engaged in a discussion, while the individuals in the background are observing the scene. The refrigerator is situated to the right of the individuals in the foreground, and various kitchen items and supplies are scattered around the countertop.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24958.3, "ram_available_mb": 100813.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 24959.0, "ram_available_mb": 100813.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.09, "peak": 15.54, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.64, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 29.87, "peak": 40.18, "min": 23.64}, "VIN": {"avg": 71.81, "peak": 112.74, "min": 60.01}}, "power_watts_avg": 29.87, "energy_joules_est": 71.98, "sample_count": 18, "duration_seconds": 2.41}, "timestamp": "2026-01-17T16:04:48.802932"}
{"image_index": 471, "image_name": "000000052996.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052996.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2378.615, "latencies_ms": [2378.615], "images_per_second": 0.42, "prompt_tokens": 21, "response_tokens_est": 64, "n_tiles": 6, "output_text": "The image depicts a group of people in a kitchen setting, likely in a military or humanitarian context. They are engaged in a discussion, with one person holding a clipboard, possibly taking notes or discussing a plan. The kitchen is equipped with various cooking utensils and equipment, indicating a functional and practical environment.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24959.0, "ram_available_mb": 100813.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 24958.8, "ram_available_mb": 100813.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.07, "peak": 15.44, "min": 14.43}, "VDD_CPU_SOC_MSS": {"avg": 15.7, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 29.89, "peak": 40.95, "min": 23.64}, "VIN": {"avg": 71.95, "peak": 124.51, "min": 58.66}}, "power_watts_avg": 29.89, "energy_joules_est": 71.12, "sample_count": 18, "duration_seconds": 2.379}, "timestamp": "2026-01-17T16:04:51.192305"}
{"image_index": 471, "image_name": "000000052996.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052996.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1973.587, "latencies_ms": [1973.587], "images_per_second": 0.507, "prompt_tokens": 19, "response_tokens_est": 49, "n_tiles": 6, "output_text": "The image depicts a group of people in a kitchen setting, with a focus on a stainless steel refrigerator and various cooking utensils and containers. The lighting is bright, and the materials include stainless steel, plastic, and wooden elements.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24958.8, "ram_available_mb": 100813.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 24958.8, "ram_available_mb": 100813.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.04, "peak": 15.44, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.62, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 30.96, "peak": 39.77, "min": 24.03}, "VIN": {"avg": 70.09, "peak": 104.55, "min": 51.73}}, "power_watts_avg": 30.96, "energy_joules_est": 61.11, "sample_count": 15, "duration_seconds": 1.974}, "timestamp": "2026-01-17T16:04:53.172012"}
{"image_index": 472, "image_name": "000000053505.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000053505.jpg", "image_width": 428, "image_height": 640, "image_resolution": "428x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1336.826, "latencies_ms": [1336.826], "images_per_second": 0.748, "prompt_tokens": 9, "response_tokens_est": 27, "n_tiles": 6, "output_text": "The image shows a bathroom with a toilet, a toilet paper holder, a soap dispenser, and a towel rack with towels.", "error": null, "sys_before": {"cpu_percent": 44.4, "ram_used_mb": 24958.8, "ram_available_mb": 100813.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 24959.3, "ram_available_mb": 100812.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.75, "peak": 15.24, "min": 14.0}, "VDD_CPU_SOC_MSS": {"avg": 15.39, "peak": 16.14, "min": 14.57}, "VDD_GPU": {"avg": 33.52, "peak": 40.57, "min": 26.39}, "VIN": {"avg": 77.61, "peak": 106.28, "min": 61.54}}, "power_watts_avg": 33.52, "energy_joules_est": 44.82, "sample_count": 10, "duration_seconds": 1.337}, "timestamp": "2026-01-17T16:04:54.557016"}
{"image_index": 472, "image_name": "000000053505.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000053505.jpg", "image_width": 428, "image_height": 640, "image_resolution": "428x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1730.623, "latencies_ms": [1730.623], "images_per_second": 0.578, "prompt_tokens": 23, "response_tokens_est": 40, "n_tiles": 6, "output_text": "object: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24959.3, "ram_available_mb": 100812.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 24960.0, "ram_available_mb": 100812.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.02, "peak": 15.54, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.62, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 32.45, "peak": 40.95, "min": 24.83}, "VIN": {"avg": 74.45, "peak": 112.63, "min": 54.71}}, "power_watts_avg": 32.45, "energy_joules_est": 56.17, "sample_count": 13, "duration_seconds": 1.731}, "timestamp": "2026-01-17T16:04:56.293744"}
{"image_index": 472, "image_name": "000000053505.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000053505.jpg", "image_width": 428, "image_height": 640, "image_resolution": "428x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3226.331, "latencies_ms": [3226.331], "images_per_second": 0.31, "prompt_tokens": 27, "response_tokens_est": 95, "n_tiles": 6, "output_text": "The main objects in the image are a toilet, a toilet paper holder, and a soap dispenser. The toilet is positioned in the foreground, slightly to the right, while the toilet paper holder is mounted on the wall to the left of the toilet. The soap dispenser is placed on the wall near the toilet paper holder. The background features a shower area with a glass door, and the wall tiles are a light color, creating a cohesive and clean aesthetic.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24960.0, "ram_available_mb": 100812.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 24959.8, "ram_available_mb": 100812.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.12, "peak": 15.44, "min": 14.43}, "VDD_CPU_SOC_MSS": {"avg": 15.69, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 28.32, "peak": 41.36, "min": 23.24}, "VIN": {"avg": 69.81, "peak": 102.89, "min": 64.5}}, "power_watts_avg": 28.32, "energy_joules_est": 91.38, "sample_count": 25, "duration_seconds": 3.227}, "timestamp": "2026-01-17T16:04:59.526100"}
{"image_index": 472, "image_name": "000000053505.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000053505.jpg", "image_width": 428, "image_height": 640, "image_resolution": "428x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2674.776, "latencies_ms": [2674.776], "images_per_second": 0.374, "prompt_tokens": 21, "response_tokens_est": 72, "n_tiles": 6, "output_text": "The image depicts a well-organized bathroom with a clean and modern design. The setting is a small, well-lit bathroom with a beige wall and tiled floor. The bathroom features a toilet, a toilet paper holder, a soap dispenser, and a towel rack with neatly folded towels. The overall atmosphere is tidy and inviting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24959.8, "ram_available_mb": 100812.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 24960.2, "ram_available_mb": 100811.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.92, "peak": 15.34, "min": 14.43}, "VDD_CPU_SOC_MSS": {"avg": 15.67, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 28.55, "peak": 40.16, "min": 22.85}, "VIN": {"avg": 69.08, "peak": 113.01, "min": 55.56}}, "power_watts_avg": 28.55, "energy_joules_est": 76.39, "sample_count": 21, "duration_seconds": 2.675}, "timestamp": "2026-01-17T16:05:02.207459"}
{"image_index": 472, "image_name": "000000053505.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000053505.jpg", "image_width": 428, "image_height": 640, "image_resolution": "428x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1538.029, "latencies_ms": [1538.029], "images_per_second": 0.65, "prompt_tokens": 19, "response_tokens_est": 32, "n_tiles": 6, "output_text": "The bathroom features a warm, beige color scheme with light-colored walls and tiled floors. The lighting is soft and ambient, creating a cozy atmosphere.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24960.2, "ram_available_mb": 100811.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 24960.2, "ram_available_mb": 100811.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.8, "peak": 15.24, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.35, "peak": 15.75, "min": 14.56}, "VDD_GPU": {"avg": 32.72, "peak": 40.56, "min": 25.21}, "VIN": {"avg": 71.46, "peak": 106.84, "min": 53.22}}, "power_watts_avg": 32.72, "energy_joules_est": 50.33, "sample_count": 12, "duration_seconds": 1.538}, "timestamp": "2026-01-17T16:05:03.755735"}
{"image_index": 473, "image_name": "000000053529.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000053529.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1321.574, "latencies_ms": [1321.574], "images_per_second": 0.757, "prompt_tokens": 9, "response_tokens_est": 26, "n_tiles": 6, "output_text": "A woman wearing a green hat is sitting in the back of a car, and a dog is looking out of the window.", "error": null, "sys_before": {"cpu_percent": 46.4, "ram_used_mb": 24960.2, "ram_available_mb": 100811.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 24961.0, "ram_available_mb": 100811.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.62, "peak": 15.04, "min": 13.79}, "VDD_CPU_SOC_MSS": {"avg": 15.27, "peak": 16.14, "min": 14.17}, "VDD_GPU": {"avg": 33.64, "peak": 40.56, "min": 26.39}, "VIN": {"avg": 78.02, "peak": 115.23, "min": 62.03}}, "power_watts_avg": 33.64, "energy_joules_est": 44.47, "sample_count": 10, "duration_seconds": 1.322}, "timestamp": "2026-01-17T16:05:05.129020"}
{"image_index": 473, "image_name": "000000053529.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000053529.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2646.502, "latencies_ms": [2646.502], "images_per_second": 0.378, "prompt_tokens": 23, "response_tokens_est": 72, "n_tiles": 6, "output_text": "1. St. Patrick's Day hat\n2. St. Patrick's Day hat\n3. St. Patrick's Day hat\n4. St. Patrick's Day hat\n5. St. Patrick's Day hat\n6. St. Patrick's Day hat\n7. St. Patrick's Day hat\n8. St. Patrick's Day hat", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24961.0, "ram_available_mb": 100811.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 24961.0, "ram_available_mb": 100811.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.95, "peak": 15.24, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.55, "peak": 15.75, "min": 14.56}, "VDD_GPU": {"avg": 29.35, "peak": 40.95, "min": 23.25}, "VIN": {"avg": 72.74, "peak": 116.54, "min": 62.74}}, "power_watts_avg": 29.35, "energy_joules_est": 77.69, "sample_count": 20, "duration_seconds": 2.647}, "timestamp": "2026-01-17T16:05:07.781979"}
{"image_index": 473, "image_name": "000000053529.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000053529.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2419.763, "latencies_ms": [2419.763], "images_per_second": 0.413, "prompt_tokens": 27, "response_tokens_est": 63, "n_tiles": 6, "output_text": "The main objects in the image are a car and a dog. The car is positioned in the foreground, with the dog sitting inside it. The dog is near the car's front window, looking out. The car's front window is near the dog, and the dog is near the car's front wheel.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24961.0, "ram_available_mb": 100811.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 24961.0, "ram_available_mb": 100811.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.86, "peak": 15.24, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.55, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 29.56, "peak": 40.16, "min": 23.24}, "VIN": {"avg": 71.57, "peak": 111.8, "min": 63.02}}, "power_watts_avg": 29.56, "energy_joules_est": 71.54, "sample_count": 18, "duration_seconds": 2.42}, "timestamp": "2026-01-17T16:05:10.207340"}
{"image_index": 473, "image_name": "000000053529.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000053529.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2245.85, "latencies_ms": [2245.85], "images_per_second": 0.445, "prompt_tokens": 21, "response_tokens_est": 57, "n_tiles": 6, "output_text": "The image depicts a scene inside a vehicle, likely a car, with a person wearing a green hat and a dog looking out of the window. The vehicle is adorned with a large green four-leaf clover on its side, suggesting a festive or celebratory occasion.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24961.0, "ram_available_mb": 100811.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 24961.5, "ram_available_mb": 100810.7, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.88, "peak": 15.24, "min": 14.3}, "VDD_CPU_SOC_MSS": {"avg": 15.56, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 30.05, "peak": 40.56, "min": 23.25}, "VIN": {"avg": 75.52, "peak": 122.07, "min": 61.69}}, "power_watts_avg": 30.05, "energy_joules_est": 67.5, "sample_count": 17, "duration_seconds": 2.246}, "timestamp": "2026-01-17T16:05:12.459236"}
{"image_index": 473, "image_name": "000000053529.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000053529.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2444.503, "latencies_ms": [2444.503], "images_per_second": 0.409, "prompt_tokens": 19, "response_tokens_est": 64, "n_tiles": 6, "output_text": "The image features a vehicle with a shiny, reflective surface, likely made of metal, and a green St. Patrick's Day-themed decoration on the front. The vehicle's interior is visible, with a person wearing a green hat and a dog looking out the window. The lighting is bright, suggesting it is daytime.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24961.5, "ram_available_mb": 100810.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 24962.2, "ram_available_mb": 100810.0, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.87, "peak": 15.24, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.5, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 29.15, "peak": 40.16, "min": 22.85}, "VIN": {"avg": 68.77, "peak": 106.61, "min": 56.19}}, "power_watts_avg": 29.15, "energy_joules_est": 71.26, "sample_count": 19, "duration_seconds": 2.445}, "timestamp": "2026-01-17T16:05:14.909733"}
{"image_index": 474, "image_name": "000000053624.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000053624.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1465.737, "latencies_ms": [1465.737], "images_per_second": 0.682, "prompt_tokens": 9, "response_tokens_est": 31, "n_tiles": 6, "output_text": "A group of elephants are standing in a shallow pool of water, with their trunks raised in the air, appearing to be enjoying the refreshing water.", "error": null, "sys_before": {"cpu_percent": 43.1, "ram_used_mb": 24962.2, "ram_available_mb": 100810.0, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 24962.6, "ram_available_mb": 100809.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.65, "peak": 15.14, "min": 13.79}, "VDD_CPU_SOC_MSS": {"avg": 15.32, "peak": 16.14, "min": 14.18}, "VDD_GPU": {"avg": 32.66, "peak": 40.16, "min": 25.6}, "VIN": {"avg": 72.59, "peak": 110.97, "min": 58.69}}, "power_watts_avg": 32.66, "energy_joules_est": 47.89, "sample_count": 11, "duration_seconds": 1.466}, "timestamp": "2026-01-17T16:05:16.427108"}
{"image_index": 474, "image_name": "000000053624.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000053624.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1796.074, "latencies_ms": [1796.074], "images_per_second": 0.557, "prompt_tokens": 23, "response_tokens_est": 41, "n_tiles": 6, "output_text": "elephant: 1\nwater: 1\nrock: 2\nrock: 1\nrock: 1\nrock: 1\nrock: 1\nrock: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24962.6, "ram_available_mb": 100809.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 24962.6, "ram_available_mb": 100809.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.82, "peak": 15.24, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.3, "peak": 15.74, "min": 14.56}, "VDD_GPU": {"avg": 31.82, "peak": 40.95, "min": 24.03}, "VIN": {"avg": 75.11, "peak": 111.97, "min": 61.88}}, "power_watts_avg": 31.82, "energy_joules_est": 57.16, "sample_count": 14, "duration_seconds": 1.796}, "timestamp": "2026-01-17T16:05:18.229282"}
{"image_index": 474, "image_name": "000000053624.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000053624.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3078.595, "latencies_ms": [3078.595], "images_per_second": 0.325, "prompt_tokens": 27, "response_tokens_est": 86, "n_tiles": 6, "output_text": "In the image, the main object is an elephant standing in a shallow water puddle. The elephant is positioned in the foreground, with its trunk slightly raised and its body facing the camera. The background features a sandy area with some rocks and a fence, while a group of people can be seen in the distance. The water puddle is located near the elephant, and there are a few other elephants in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24962.6, "ram_available_mb": 100809.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 24962.6, "ram_available_mb": 100809.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.9, "peak": 15.24, "min": 14.3}, "VDD_CPU_SOC_MSS": {"avg": 15.32, "peak": 15.75, "min": 14.56}, "VDD_GPU": {"avg": 27.98, "peak": 40.16, "min": 22.85}, "VIN": {"avg": 69.41, "peak": 102.1, "min": 63.02}}, "power_watts_avg": 27.98, "energy_joules_est": 86.15, "sample_count": 24, "duration_seconds": 3.079}, "timestamp": "2026-01-17T16:05:21.313987"}
{"image_index": 474, "image_name": "000000053624.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000053624.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2245.484, "latencies_ms": [2245.484], "images_per_second": 0.445, "prompt_tokens": 21, "response_tokens_est": 57, "n_tiles": 6, "output_text": "The image depicts an elephant standing in a shallow water-filled enclosure, likely at a zoo or wildlife sanctuary. The elephant is surrounded by rocks and vegetation, with a few other animals nearby. The setting appears to be a naturalistic environment designed to mimic the elephant's natural habitat.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24962.6, "ram_available_mb": 100809.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 24962.8, "ram_available_mb": 100809.3, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.83, "peak": 15.14, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.29, "peak": 15.75, "min": 14.57}, "VDD_GPU": {"avg": 29.69, "peak": 40.95, "min": 23.24}, "VIN": {"avg": 70.67, "peak": 117.93, "min": 59.15}}, "power_watts_avg": 29.69, "energy_joules_est": 66.68, "sample_count": 18, "duration_seconds": 2.246}, "timestamp": "2026-01-17T16:05:23.565685"}
{"image_index": 474, "image_name": "000000053624.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000053624.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2277.398, "latencies_ms": [2277.398], "images_per_second": 0.439, "prompt_tokens": 19, "response_tokens_est": 58, "n_tiles": 6, "output_text": "The image depicts a large elephant standing in a shallow water pool, with its trunk raised. The surrounding area is sandy with patches of grass, and there are rocks visible in the foreground. The lighting is natural, suggesting it is daytime, and the weather appears to be clear.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24962.8, "ram_available_mb": 100809.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 24963.6, "ram_available_mb": 100808.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.86, "peak": 15.24, "min": 14.3}, "VDD_CPU_SOC_MSS": {"avg": 15.31, "peak": 15.74, "min": 14.56}, "VDD_GPU": {"avg": 29.65, "peak": 40.56, "min": 23.24}, "VIN": {"avg": 72.02, "peak": 111.62, "min": 62.49}}, "power_watts_avg": 29.65, "energy_joules_est": 67.53, "sample_count": 18, "duration_seconds": 2.278}, "timestamp": "2026-01-17T16:05:25.851275"}
{"image_index": 475, "image_name": "000000053626.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000053626.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 3069.489, "latencies_ms": [3069.489], "images_per_second": 0.326, "prompt_tokens": 9, "response_tokens_est": 43, "n_tiles": 12, "output_text": "The image depicts a group of people standing on a snowy slope, each holding ski poles and wearing appropriate winter attire, including jackets and gloves, suggesting they are engaged in skiing or snowboarding.", "error": null, "sys_before": {"cpu_percent": 47.5, "ram_used_mb": 24927.4, "ram_available_mb": 100844.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 24928.1, "ram_available_mb": 100844.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.14, "peak": 16.66, "min": 13.39}, "VDD_CPU_SOC_MSS": {"avg": 15.88, "peak": 17.71, "min": 14.18}, "VDD_GPU": {"avg": 34.51, "peak": 43.73, "min": 26.39}, "VIN": {"avg": 79.13, "peak": 112.31, "min": 67.2}}, "power_watts_avg": 34.51, "energy_joules_est": 105.94, "sample_count": 23, "duration_seconds": 3.07}, "timestamp": "2026-01-17T16:05:29.008864"}
{"image_index": 475, "image_name": "000000053626.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000053626.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 3027.639, "latencies_ms": [3027.639], "images_per_second": 0.33, "prompt_tokens": 23, "response_tokens_est": 40, "n_tiles": 12, "output_text": "1. Ski poles\n2. Ski poles\n3. Ski poles\n4. Ski poles\n5. Ski poles\n6. Ski poles\n7. Ski poles\n8. Ski poles", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24928.1, "ram_available_mb": 100844.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 24928.1, "ram_available_mb": 100844.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.21, "peak": 16.56, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.84, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 35.53, "peak": 45.69, "min": 26.39}, "VIN": {"avg": 76.66, "peak": 123.71, "min": 62.06}}, "power_watts_avg": 35.53, "energy_joules_est": 107.59, "sample_count": 24, "duration_seconds": 3.028}, "timestamp": "2026-01-17T16:05:32.042923"}
{"image_index": 475, "image_name": "000000053626.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000053626.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3885.054, "latencies_ms": [3885.054], "images_per_second": 0.257, "prompt_tokens": 27, "response_tokens_est": 66, "n_tiles": 12, "output_text": "The main objects in the image are a group of people skiing on a snowy slope. The foreground features a person standing on a ski, holding a ski pole, while the background shows more skiers and a mountainous landscape. The ski slope is covered in snow, and there are ski lifts visible in the distance.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24928.1, "ram_available_mb": 100844.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 24928.1, "ram_available_mb": 100844.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.24, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.8, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 33.59, "peak": 45.28, "min": 26.0}, "VIN": {"avg": 79.38, "peak": 139.41, "min": 60.56}}, "power_watts_avg": 33.59, "energy_joules_est": 130.52, "sample_count": 30, "duration_seconds": 3.886}, "timestamp": "2026-01-17T16:05:35.935197"}
{"image_index": 475, "image_name": "000000053626.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000053626.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3906.536, "latencies_ms": [3906.536], "images_per_second": 0.256, "prompt_tokens": 21, "response_tokens_est": 67, "n_tiles": 12, "output_text": "The image depicts a group of people skiing down a snowy mountain slope. They are dressed in winter sports attire, including jackets, pants, and ski boots, and are holding ski poles. The setting is a snowy mountain with a clear blue sky, and the individuals appear to be enjoying their time skiing.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24928.1, "ram_available_mb": 100844.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 24929.0, "ram_available_mb": 100843.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.28, "peak": 16.66, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.88, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 33.45, "peak": 44.89, "min": 26.0}, "VIN": {"avg": 76.99, "peak": 125.93, "min": 65.51}}, "power_watts_avg": 33.45, "energy_joules_est": 130.69, "sample_count": 30, "duration_seconds": 3.907}, "timestamp": "2026-01-17T16:05:39.848396"}
{"image_index": 475, "image_name": "000000053626.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000053626.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3307.347, "latencies_ms": [3307.347], "images_per_second": 0.302, "prompt_tokens": 19, "response_tokens_est": 49, "n_tiles": 12, "output_text": "The image depicts a group of people skiing down a snowy mountain slope. The sky is clear and blue, indicating a sunny day. The snow is pristine and untouched, with a few scattered trees dotting the landscape.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24929.0, "ram_available_mb": 100843.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 24929.8, "ram_available_mb": 100842.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.28, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.89, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 34.93, "peak": 44.49, "min": 26.39}, "VIN": {"avg": 77.71, "peak": 102.73, "min": 66.6}}, "power_watts_avg": 34.93, "energy_joules_est": 115.54, "sample_count": 25, "duration_seconds": 3.308}, "timestamp": "2026-01-17T16:05:43.166640"}
{"image_index": 476, "image_name": "000000053909.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000053909.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2426.883, "latencies_ms": [2426.883], "images_per_second": 0.412, "prompt_tokens": 9, "response_tokens_est": 24, "n_tiles": 12, "output_text": "The image shows a person holding a smartphone with a screen displaying a date and time, and a blurred background.", "error": null, "sys_before": {"cpu_percent": 43.4, "ram_used_mb": 24932.6, "ram_available_mb": 100839.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.4, "ram_used_mb": 24934.0, "ram_available_mb": 100838.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.18, "peak": 16.66, "min": 13.49}, "VDD_CPU_SOC_MSS": {"avg": 15.97, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 36.44, "peak": 44.12, "min": 27.97}, "VIN": {"avg": 79.48, "peak": 117.2, "min": 59.82}}, "power_watts_avg": 36.44, "energy_joules_est": 88.45, "sample_count": 19, "duration_seconds": 2.427}, "timestamp": "2026-01-17T16:05:45.693786"}
{"image_index": 476, "image_name": "000000053909.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000053909.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 3290.611, "latencies_ms": [3290.611], "images_per_second": 0.304, "prompt_tokens": 23, "response_tokens_est": 48, "n_tiles": 12, "output_text": "- Phone: 1\n- Keyboard: 1\n- Screen: 1\n- Battery: 1\n- Buttons: 1\n- Port: 1\n- Case: 1\n- Hand: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24934.0, "ram_available_mb": 100838.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 24934.3, "ram_available_mb": 100837.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.26, "peak": 16.66, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.89, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 35.12, "peak": 45.28, "min": 26.39}, "VIN": {"avg": 77.01, "peak": 106.31, "min": 62.97}}, "power_watts_avg": 35.12, "energy_joules_est": 115.58, "sample_count": 25, "duration_seconds": 3.291}, "timestamp": "2026-01-17T16:05:48.990978"}
{"image_index": 476, "image_name": "000000053909.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000053909.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3339.176, "latencies_ms": [3339.176], "images_per_second": 0.299, "prompt_tokens": 27, "response_tokens_est": 49, "n_tiles": 12, "output_text": "The main object in the image is a smartphone held in the foreground. The background is blurred, indicating that the focus is on the smartphone. The smartphone is positioned near the center of the image, with the person's hand holding it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24934.3, "ram_available_mb": 100837.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 24934.0, "ram_available_mb": 100838.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.22, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.78, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 34.52, "peak": 44.89, "min": 26.0}, "VIN": {"avg": 77.49, "peak": 114.7, "min": 56.91}}, "power_watts_avg": 34.52, "energy_joules_est": 115.28, "sample_count": 26, "duration_seconds": 3.34}, "timestamp": "2026-01-17T16:05:52.336719"}
{"image_index": 476, "image_name": "000000053909.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000053909.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3094.008, "latencies_ms": [3094.008], "images_per_second": 0.323, "prompt_tokens": 21, "response_tokens_est": 42, "n_tiles": 12, "output_text": "The image shows a person holding a smartphone with a screen displaying a calendar. The person's hand is visible, and the background is blurred, indicating that the focus is on the smartphone and the calendar.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24934.0, "ram_available_mb": 100838.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 24934.2, "ram_available_mb": 100837.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.23, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.76, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 35.23, "peak": 45.68, "min": 26.39}, "VIN": {"avg": 77.78, "peak": 115.12, "min": 65.91}}, "power_watts_avg": 35.23, "energy_joules_est": 109.01, "sample_count": 24, "duration_seconds": 3.094}, "timestamp": "2026-01-17T16:05:55.437186"}
{"image_index": 476, "image_name": "000000053909.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000053909.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3984.412, "latencies_ms": [3984.412], "images_per_second": 0.251, "prompt_tokens": 19, "response_tokens_est": 67, "n_tiles": 12, "output_text": "The image shows a person holding a smartphone with a black case. The screen displays a wallpaper of a tree with bare branches, and the time reads \"19:45\" with the date \"mondag 6. m\u00e4nd.\" The lighting is natural, and the background is blurred, indicating a shallow depth of field.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24934.2, "ram_available_mb": 100837.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 24935.2, "ram_available_mb": 100836.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.18, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.66, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 33.21, "peak": 44.89, "min": 25.6}, "VIN": {"avg": 74.97, "peak": 111.37, "min": 54.95}}, "power_watts_avg": 33.21, "energy_joules_est": 132.33, "sample_count": 31, "duration_seconds": 3.985}, "timestamp": "2026-01-17T16:05:59.427905"}
{"image_index": 477, "image_name": "000000053994.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000053994.jpg", "image_width": 481, "image_height": 640, "image_resolution": "481x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2866.951, "latencies_ms": [2866.951], "images_per_second": 0.349, "prompt_tokens": 9, "response_tokens_est": 37, "n_tiles": 12, "output_text": "The image shows a red parking meter with a sign that reads \"End Homelessness\" attached to a post, situated in a garden area with green plants and trees in the background.", "error": null, "sys_before": {"cpu_percent": 41.1, "ram_used_mb": 24944.2, "ram_available_mb": 100827.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 24937.3, "ram_available_mb": 100834.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.94, "peak": 16.36, "min": 13.49}, "VDD_CPU_SOC_MSS": {"avg": 15.69, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 35.18, "peak": 44.51, "min": 26.39}, "VIN": {"avg": 78.0, "peak": 123.14, "min": 63.65}}, "power_watts_avg": 35.18, "energy_joules_est": 100.87, "sample_count": 22, "duration_seconds": 2.867}, "timestamp": "2026-01-17T16:06:02.386665"}
{"image_index": 477, "image_name": "000000053994.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000053994.jpg", "image_width": 481, "image_height": 640, "image_resolution": "481x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 3034.253, "latencies_ms": [3034.253], "images_per_second": 0.33, "prompt_tokens": 23, "response_tokens_est": 40, "n_tiles": 12, "output_text": "object: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24937.3, "ram_available_mb": 100834.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 24931.7, "ram_available_mb": 100840.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.95, "peak": 16.36, "min": 13.92}, "VDD_CPU_SOC_MSS": {"avg": 15.55, "peak": 17.32, "min": 14.17}, "VDD_GPU": {"avg": 35.34, "peak": 45.69, "min": 26.0}, "VIN": {"avg": 76.34, "peak": 116.64, "min": 59.53}}, "power_watts_avg": 35.34, "energy_joules_est": 107.25, "sample_count": 24, "duration_seconds": 3.035}, "timestamp": "2026-01-17T16:06:05.427606"}
{"image_index": 477, "image_name": "000000053994.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000053994.jpg", "image_width": 481, "image_height": 640, "image_resolution": "481x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3871.792, "latencies_ms": [3871.792], "images_per_second": 0.258, "prompt_tokens": 27, "response_tokens_est": 64, "n_tiles": 12, "output_text": "The main object in the foreground is a red parking meter. It is positioned near a sidewalk and is surrounded by greenery. In the background, there is a sign that reads \"End Homelessness\" and a concrete wall. The parking meter is placed on the sidewalk, and the sign is mounted on a pole.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24932.2, "ram_available_mb": 100840.0, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 24930.4, "ram_available_mb": 100841.7, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.89, "peak": 16.36, "min": 13.79}, "VDD_CPU_SOC_MSS": {"avg": 15.43, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 33.38, "peak": 45.69, "min": 25.6}, "VIN": {"avg": 74.12, "peak": 108.0, "min": 60.08}}, "power_watts_avg": 33.38, "energy_joules_est": 129.26, "sample_count": 30, "duration_seconds": 3.872}, "timestamp": "2026-01-17T16:06:09.306495"}
{"image_index": 477, "image_name": "000000053994.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000053994.jpg", "image_width": 481, "image_height": 640, "image_resolution": "481x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3893.592, "latencies_ms": [3893.592], "images_per_second": 0.257, "prompt_tokens": 21, "response_tokens_est": 65, "n_tiles": 12, "output_text": "The image depicts a red parking meter situated in a well-maintained garden area. The meter is placed on a concrete sidewalk, with a small sign attached to the pole that reads \"END HOMELESSNESS.\" The surrounding area is lush with greenery, and there are trees and bushes visible in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24930.4, "ram_available_mb": 100841.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 24930.7, "ram_available_mb": 100841.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.91, "peak": 16.36, "min": 13.92}, "VDD_CPU_SOC_MSS": {"avg": 15.35, "peak": 17.31, "min": 14.17}, "VDD_GPU": {"avg": 33.29, "peak": 45.3, "min": 25.6}, "VIN": {"avg": 74.76, "peak": 111.43, "min": 61.86}}, "power_watts_avg": 33.29, "energy_joules_est": 129.63, "sample_count": 30, "duration_seconds": 3.894}, "timestamp": "2026-01-17T16:06:13.206469"}
{"image_index": 477, "image_name": "000000053994.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000053994.jpg", "image_width": 481, "image_height": 640, "image_resolution": "481x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3654.544, "latencies_ms": [3654.544], "images_per_second": 0.274, "prompt_tokens": 19, "response_tokens_est": 58, "n_tiles": 12, "output_text": "The image features a red parking meter with a white and black label. The meter is situated on a concrete sidewalk, surrounded by greenery and a small sign that reads \"END HOMELESSNESS.\" The lighting is natural, suggesting it is daytime, and the weather appears to be clear.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24930.7, "ram_available_mb": 100841.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 24933.2, "ram_available_mb": 100839.0, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.92, "peak": 16.26, "min": 13.92}, "VDD_CPU_SOC_MSS": {"avg": 15.34, "peak": 16.92, "min": 14.17}, "VDD_GPU": {"avg": 33.57, "peak": 45.3, "min": 25.6}, "VIN": {"avg": 77.9, "peak": 127.84, "min": 59.44}}, "power_watts_avg": 33.57, "energy_joules_est": 122.71, "sample_count": 28, "duration_seconds": 3.655}, "timestamp": "2026-01-17T16:06:16.869137"}
{"image_index": 478, "image_name": "000000054123.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054123.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1542.096, "latencies_ms": [1542.096], "images_per_second": 0.648, "prompt_tokens": 9, "response_tokens_est": 35, "n_tiles": 6, "output_text": "The image depicts a group of zebras grazing in a savanna-like environment, with their distinctive black and white striped patterns clearly visible on their bodies.", "error": null, "sys_before": {"cpu_percent": 36.9, "ram_used_mb": 24953.1, "ram_available_mb": 100819.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 24954.0, "ram_available_mb": 100818.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.67, "peak": 15.11, "min": 13.49}, "VDD_CPU_SOC_MSS": {"avg": 15.32, "peak": 15.75, "min": 14.17}, "VDD_GPU": {"avg": 32.56, "peak": 40.56, "min": 25.22}, "VIN": {"avg": 73.11, "peak": 102.46, "min": 58.43}}, "power_watts_avg": 32.56, "energy_joules_est": 50.23, "sample_count": 12, "duration_seconds": 1.543}, "timestamp": "2026-01-17T16:06:18.478625"}
{"image_index": 478, "image_name": "000000054123.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054123.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 943.899, "latencies_ms": [943.899], "images_per_second": 1.059, "prompt_tokens": 23, "response_tokens_est": 11, "n_tiles": 6, "output_text": "zebra: 4\ngrass: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24954.0, "ram_available_mb": 100818.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 24954.0, "ram_available_mb": 100818.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.89, "peak": 15.44, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.52, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 37.25, "peak": 40.57, "min": 32.3}, "VIN": {"avg": 82.15, "peak": 95.52, "min": 63.66}}, "power_watts_avg": 37.25, "energy_joules_est": 35.17, "sample_count": 7, "duration_seconds": 0.944}, "timestamp": "2026-01-17T16:06:19.428322"}
{"image_index": 478, "image_name": "000000054123.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054123.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2919.485, "latencies_ms": [2919.485], "images_per_second": 0.343, "prompt_tokens": 27, "response_tokens_est": 84, "n_tiles": 6, "output_text": "The main objects in the image are zebras, with the foreground and background being the most prominent. The foreground features a zebra with a prominent black and white striped pattern, while the background shows other zebras grazing in the same area. The zebra in the foreground is closer to the camera, while the other zebras are further away, creating a sense of depth in the image.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 24954.0, "ram_available_mb": 100818.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 24954.3, "ram_available_mb": 100817.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.09, "peak": 15.44, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.66, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 29.65, "peak": 42.92, "min": 23.64}, "VIN": {"avg": 72.75, "peak": 112.39, "min": 57.84}}, "power_watts_avg": 29.65, "energy_joules_est": 86.58, "sample_count": 23, "duration_seconds": 2.92}, "timestamp": "2026-01-17T16:06:22.353810"}
{"image_index": 478, "image_name": "000000054123.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054123.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2101.946, "latencies_ms": [2101.946], "images_per_second": 0.476, "prompt_tokens": 21, "response_tokens_est": 54, "n_tiles": 6, "output_text": "The image depicts a serene African savanna landscape with a group of zebras grazing on tall, dry grass. The zebras are in a natural setting, surrounded by sparse vegetation and a clear sky, creating a peaceful and tranquil atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24954.3, "ram_available_mb": 100817.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 24955.5, "ram_available_mb": 100816.7, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.08, "peak": 15.44, "min": 14.43}, "VDD_CPU_SOC_MSS": {"avg": 15.65, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 30.68, "peak": 40.57, "min": 24.03}, "VIN": {"avg": 74.41, "peak": 124.32, "min": 61.62}}, "power_watts_avg": 30.68, "energy_joules_est": 64.5, "sample_count": 16, "duration_seconds": 2.102}, "timestamp": "2026-01-17T16:06:24.462567"}
{"image_index": 478, "image_name": "000000054123.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054123.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2127.892, "latencies_ms": [2127.892], "images_per_second": 0.47, "prompt_tokens": 19, "response_tokens_est": 55, "n_tiles": 6, "output_text": "The image depicts a group of zebras grazing in a savanna-like environment. The zebras have black and white striped patterns, and the lighting is natural, suggesting it is daytime. The grass is tall and dry, indicating a dry season.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24955.5, "ram_available_mb": 100816.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 24956.5, "ram_available_mb": 100815.7, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.08, "peak": 15.54, "min": 14.4}, "VDD_CPU_SOC_MSS": {"avg": 15.65, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 30.55, "peak": 40.16, "min": 24.03}, "VIN": {"avg": 71.81, "peak": 110.58, "min": 54.82}}, "power_watts_avg": 30.55, "energy_joules_est": 65.02, "sample_count": 16, "duration_seconds": 2.128}, "timestamp": "2026-01-17T16:06:26.596647"}
{"image_index": 479, "image_name": "000000054164.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054164.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1227.261, "latencies_ms": [1227.261], "images_per_second": 0.815, "prompt_tokens": 9, "response_tokens_est": 23, "n_tiles": 6, "output_text": "A young man is surfing on a wave, wearing a black wetsuit and standing on a surfboard.", "error": null, "sys_before": {"cpu_percent": 46.3, "ram_used_mb": 24956.5, "ram_available_mb": 100815.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 24956.7, "ram_available_mb": 100815.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.7, "peak": 15.24, "min": 14.0}, "VDD_CPU_SOC_MSS": {"avg": 15.44, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 34.31, "peak": 40.16, "min": 27.97}, "VIN": {"avg": 80.06, "peak": 118.68, "min": 62.12}}, "power_watts_avg": 34.31, "energy_joules_est": 42.12, "sample_count": 9, "duration_seconds": 1.228}, "timestamp": "2026-01-17T16:06:27.872965"}
{"image_index": 479, "image_name": "000000054164.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054164.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1704.8, "latencies_ms": [1704.8], "images_per_second": 0.587, "prompt_tokens": 23, "response_tokens_est": 38, "n_tiles": 6, "output_text": "1. Surfer\n2. Wetsuit\n3. Ocean\n4. Wave\n5. Water\n6. Surfboard\n7. Wave\n8. Sunlight", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24956.7, "ram_available_mb": 100815.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 24957.2, "ram_available_mb": 100815.0, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.88, "peak": 15.24, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.5, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 32.78, "peak": 41.34, "min": 24.42}, "VIN": {"avg": 71.3, "peak": 109.88, "min": 63.2}}, "power_watts_avg": 32.78, "energy_joules_est": 55.89, "sample_count": 13, "duration_seconds": 1.705}, "timestamp": "2026-01-17T16:06:29.583809"}
{"image_index": 479, "image_name": "000000054164.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054164.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2307.691, "latencies_ms": [2307.691], "images_per_second": 0.433, "prompt_tokens": 27, "response_tokens_est": 59, "n_tiles": 6, "output_text": "The main object in the foreground is a person riding a surfboard on a wave. The person is wearing a black wetsuit and appears to be in motion, with water splashing around them. The background shows a large wave breaking, with the water and foam creating a dynamic scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24957.2, "ram_available_mb": 100815.0, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 24957.0, "ram_available_mb": 100815.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.85, "peak": 15.34, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.45, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 30.1, "peak": 40.56, "min": 23.24}, "VIN": {"avg": 71.29, "peak": 106.94, "min": 59.95}}, "power_watts_avg": 30.1, "energy_joules_est": 69.47, "sample_count": 17, "duration_seconds": 2.308}, "timestamp": "2026-01-17T16:06:31.897738"}
{"image_index": 479, "image_name": "000000054164.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054164.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2841.508, "latencies_ms": [2841.508], "images_per_second": 0.352, "prompt_tokens": 21, "response_tokens_est": 78, "n_tiles": 6, "output_text": "The image captures a dynamic scene of a surfer riding a wave in the ocean. The surfer, dressed in a black wetsuit, is skillfully maneuvering on a surfboard, with the wave's crest and spray creating a visually striking backdrop. The setting is a large, open ocean with clear blue water, indicating a sunny day with good weather conditions for surfing.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24957.0, "ram_available_mb": 100815.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 24957.2, "ram_available_mb": 100815.0, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.9, "peak": 15.24, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.5, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 28.31, "peak": 40.16, "min": 22.85}, "VIN": {"avg": 70.19, "peak": 108.39, "min": 61.34}}, "power_watts_avg": 28.31, "energy_joules_est": 80.45, "sample_count": 22, "duration_seconds": 2.842}, "timestamp": "2026-01-17T16:06:34.745229"}
{"image_index": 479, "image_name": "000000054164.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054164.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2277.438, "latencies_ms": [2277.438], "images_per_second": 0.439, "prompt_tokens": 19, "response_tokens_est": 58, "n_tiles": 6, "output_text": "The image captures a surfer in a black wetsuit riding a wave, with the ocean's deep blue-green color dominating the scene. The sunlight illuminates the scene, casting a bright and clear light on the surfer and the wave, creating a dynamic and energetic atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24957.2, "ram_available_mb": 100815.0, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 24957.9, "ram_available_mb": 100814.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.87, "peak": 15.24, "min": 14.3}, "VDD_CPU_SOC_MSS": {"avg": 15.47, "peak": 15.75, "min": 14.56}, "VDD_GPU": {"avg": 29.91, "peak": 40.16, "min": 23.25}, "VIN": {"avg": 73.1, "peak": 108.22, "min": 62.54}}, "power_watts_avg": 29.91, "energy_joules_est": 68.14, "sample_count": 17, "duration_seconds": 2.278}, "timestamp": "2026-01-17T16:06:37.029415"}
{"image_index": 480, "image_name": "000000054592.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054592.jpg", "image_width": 640, "image_height": 418, "image_resolution": "640x418", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1385.832, "latencies_ms": [1385.832], "images_per_second": 0.722, "prompt_tokens": 9, "response_tokens_est": 29, "n_tiles": 6, "output_text": "A skier is standing on a snowy slope, holding a pair of skis and wearing a jacket and pants suitable for winter sports.", "error": null, "sys_before": {"cpu_percent": 37.5, "ram_used_mb": 24957.7, "ram_available_mb": 100814.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 24958.4, "ram_available_mb": 100813.7, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.7, "peak": 15.14, "min": 13.9}, "VDD_CPU_SOC_MSS": {"avg": 15.43, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 33.48, "peak": 39.77, "min": 26.79}, "VIN": {"avg": 75.32, "peak": 105.04, "min": 59.63}}, "power_watts_avg": 33.48, "energy_joules_est": 46.42, "sample_count": 10, "duration_seconds": 1.387}, "timestamp": "2026-01-17T16:06:38.461863"}
{"image_index": 480, "image_name": "000000054592.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054592.jpg", "image_width": 640, "image_height": 418, "image_resolution": "640x418", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1657.699, "latencies_ms": [1657.699], "images_per_second": 0.603, "prompt_tokens": 23, "response_tokens_est": 38, "n_tiles": 6, "output_text": "1. Ski poles\n2. Ski\n3. Ski jacket\n4. Ski boots\n5. Ski pants\n6. Backpack\n7. Gloves\n8. Camera", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24958.4, "ram_available_mb": 100813.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 24958.7, "ram_available_mb": 100813.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.06, "peak": 15.44, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.65, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 33.38, "peak": 40.95, "min": 25.6}, "VIN": {"avg": 78.55, "peak": 122.72, "min": 62.53}}, "power_watts_avg": 33.38, "energy_joules_est": 55.35, "sample_count": 12, "duration_seconds": 1.658}, "timestamp": "2026-01-17T16:06:40.125734"}
{"image_index": 480, "image_name": "000000054592.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054592.jpg", "image_width": 640, "image_height": 418, "image_resolution": "640x418", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2950.881, "latencies_ms": [2950.881], "images_per_second": 0.339, "prompt_tokens": 27, "response_tokens_est": 86, "n_tiles": 6, "output_text": "The main object in the foreground is a person standing on a snowy slope, holding a pair of skis. The person is wearing a white jacket and pants, and is positioned near the center of the image. In the background, there is a mountain range with a clear sky above it. The person is slightly off-center to the right, and the mountains are further away, creating a sense of depth in the image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24958.7, "ram_available_mb": 100813.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 24958.7, "ram_available_mb": 100813.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.2, "peak": 15.54, "min": 14.43}, "VDD_CPU_SOC_MSS": {"avg": 15.78, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 28.68, "peak": 40.56, "min": 23.64}, "VIN": {"avg": 70.47, "peak": 112.36, "min": 60.45}}, "power_watts_avg": 28.68, "energy_joules_est": 84.64, "sample_count": 23, "duration_seconds": 2.951}, "timestamp": "2026-01-17T16:06:43.082854"}
{"image_index": 480, "image_name": "000000054592.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054592.jpg", "image_width": 640, "image_height": 418, "image_resolution": "640x418", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2729.963, "latencies_ms": [2729.963], "images_per_second": 0.366, "prompt_tokens": 21, "response_tokens_est": 77, "n_tiles": 6, "output_text": "The image depicts a person dressed in winter gear, including a jacket and pants, standing on a snowy slope. The individual is holding a pair of skis, suggesting they are preparing to ski or have just finished skiing. The scene is set during twilight, with the sun low on the horizon, casting a warm glow and long shadows across the snowy landscape.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24958.7, "ram_available_mb": 100813.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 24958.7, "ram_available_mb": 100813.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.08, "peak": 15.44, "min": 14.43}, "VDD_CPU_SOC_MSS": {"avg": 15.71, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 29.0, "peak": 40.18, "min": 23.24}, "VIN": {"avg": 70.85, "peak": 109.41, "min": 62.72}}, "power_watts_avg": 29.0, "energy_joules_est": 79.18, "sample_count": 21, "duration_seconds": 2.73}, "timestamp": "2026-01-17T16:06:45.818779"}
{"image_index": 480, "image_name": "000000054592.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054592.jpg", "image_width": 640, "image_height": 418, "image_resolution": "640x418", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2458.661, "latencies_ms": [2458.661], "images_per_second": 0.407, "prompt_tokens": 19, "response_tokens_est": 67, "n_tiles": 6, "output_text": "The image depicts a person dressed in a white ski suit and a dark jacket, standing on a snowy slope during what appears to be either sunrise or sunset. The lighting is soft and warm, casting a golden hue over the scene, while the snow is undisturbed, indicating a peaceful and serene environment.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24958.7, "ram_available_mb": 100813.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 24958.7, "ram_available_mb": 100813.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.08, "peak": 15.54, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.64, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 29.68, "peak": 40.16, "min": 23.64}, "VIN": {"avg": 70.92, "peak": 97.78, "min": 64.21}}, "power_watts_avg": 29.68, "energy_joules_est": 72.99, "sample_count": 18, "duration_seconds": 2.459}, "timestamp": "2026-01-17T16:06:48.283725"}
{"image_index": 481, "image_name": "000000054593.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054593.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1764.342, "latencies_ms": [1764.342], "images_per_second": 0.567, "prompt_tokens": 9, "response_tokens_est": 43, "n_tiles": 6, "output_text": "A young baseball player in a red jersey and white pants is in the middle of a swing, with a catcher and an umpire crouched behind him, all set up on a baseball field.", "error": null, "sys_before": {"cpu_percent": 37.3, "ram_used_mb": 24958.7, "ram_available_mb": 100813.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 24958.7, "ram_available_mb": 100813.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.81, "peak": 15.14, "min": 14.0}, "VDD_CPU_SOC_MSS": {"avg": 15.52, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 31.26, "peak": 40.18, "min": 24.42}, "VIN": {"avg": 76.61, "peak": 122.71, "min": 54.52}}, "power_watts_avg": 31.26, "energy_joules_est": 55.16, "sample_count": 14, "duration_seconds": 1.765}, "timestamp": "2026-01-17T16:06:50.096127"}
{"image_index": 481, "image_name": "000000054593.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054593.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1891.864, "latencies_ms": [1891.864], "images_per_second": 0.529, "prompt_tokens": 23, "response_tokens_est": 46, "n_tiles": 6, "output_text": "baseball bat: 1\ncatcher's mask: 1\ncatcher's gear: 1\numpire: 1\nplayer: 1\nbaseball: 1\npitcher: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24958.7, "ram_available_mb": 100813.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 24959.2, "ram_available_mb": 100813.0, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.02, "peak": 15.54, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.66, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 31.68, "peak": 40.56, "min": 24.42}, "VIN": {"avg": 72.76, "peak": 106.37, "min": 63.46}}, "power_watts_avg": 31.68, "energy_joules_est": 59.94, "sample_count": 14, "duration_seconds": 1.892}, "timestamp": "2026-01-17T16:06:51.994039"}
{"image_index": 481, "image_name": "000000054593.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054593.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2948.436, "latencies_ms": [2948.436], "images_per_second": 0.339, "prompt_tokens": 27, "response_tokens_est": 85, "n_tiles": 6, "output_text": "The main object in the foreground is a baseball player in a red jersey and white pants, who is in the middle of swinging at a pitch. The player is positioned near the batter's box. In the background, there is a chain-link fence, a parked car, and a group of people sitting on folding chairs. The fence separates the field from the spectators, while the car and people are further away.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24959.2, "ram_available_mb": 100813.0, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 24959.6, "ram_available_mb": 100812.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.1, "peak": 15.44, "min": 14.43}, "VDD_CPU_SOC_MSS": {"avg": 15.66, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 28.66, "peak": 40.57, "min": 23.64}, "VIN": {"avg": 70.27, "peak": 114.46, "min": 55.89}}, "power_watts_avg": 28.66, "energy_joules_est": 84.51, "sample_count": 23, "duration_seconds": 2.949}, "timestamp": "2026-01-17T16:06:54.949239"}
{"image_index": 481, "image_name": "000000054593.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054593.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2894.404, "latencies_ms": [2894.404], "images_per_second": 0.345, "prompt_tokens": 21, "response_tokens_est": 83, "n_tiles": 6, "output_text": "The image captures a moment during a baseball game played on a dirt field enclosed by a chain-link fence. A young baseball player in a red jersey and white pants is in the midst of swinging at a pitch, while a catcher in a black helmet and protective gear is crouched behind him, ready to catch the ball. In the background, spectators are seated on folding chairs, watching the game.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 24959.6, "ram_available_mb": 100812.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 24959.4, "ram_available_mb": 100812.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.08, "peak": 15.44, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.66, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 28.59, "peak": 40.97, "min": 23.25}, "VIN": {"avg": 70.23, "peak": 96.49, "min": 61.5}}, "power_watts_avg": 28.59, "energy_joules_est": 82.76, "sample_count": 23, "duration_seconds": 2.895}, "timestamp": "2026-01-17T16:06:57.849876"}
{"image_index": 481, "image_name": "000000054593.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054593.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1669.37, "latencies_ms": [1669.37], "images_per_second": 0.599, "prompt_tokens": 19, "response_tokens_est": 38, "n_tiles": 6, "output_text": "The baseball player is wearing a red jersey and white pants, with a black helmet and black cleats. The scene is brightly lit by natural sunlight, casting shadows on the ground.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24959.4, "ram_available_mb": 100812.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 24959.1, "ram_available_mb": 100813.0, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.01, "peak": 15.44, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.6, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 32.33, "peak": 40.56, "min": 24.82}, "VIN": {"avg": 78.08, "peak": 118.39, "min": 55.66}}, "power_watts_avg": 32.33, "energy_joules_est": 53.99, "sample_count": 13, "duration_seconds": 1.67}, "timestamp": "2026-01-17T16:06:59.525483"}
{"image_index": 482, "image_name": "000000054605.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054605.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1026.508, "latencies_ms": [1026.508], "images_per_second": 0.974, "prompt_tokens": 9, "response_tokens_est": 43, "n_tiles": 1, "output_text": "A woman is sitting at a table, enjoying a chocolate and vanilla ice cream cone with a dollop of whipped cream on top, while a slice of cake is placed on a plate in front of her.", "error": null, "sys_before": {"cpu_percent": 28.0, "ram_used_mb": 24959.1, "ram_available_mb": 100813.0, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24959.6, "ram_available_mb": 100812.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 4845.7, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.29, "peak": 15.62, "min": 14.71}, "VDD_CPU_SOC_MSS": {"avg": 16.09, "peak": 16.54, "min": 15.36}, "VDD_GPU": {"avg": 23.14, "peak": 26.8, "min": 20.89}, "VIN": {"avg": 62.89, "peak": 69.22, "min": 58.59}}, "power_watts_avg": 23.14, "energy_joules_est": 23.76, "sample_count": 7, "duration_seconds": 1.027}, "timestamp": "2026-01-17T16:07:00.577146"}
{"image_index": 482, "image_name": "000000054605.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054605.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1354.161, "latencies_ms": [1354.161], "images_per_second": 0.738, "prompt_tokens": 23, "response_tokens_est": 58, "n_tiles": 1, "output_text": "- Chocolate cake: 1\n- Whipped cream: 1\n- Whisk: 1\n- Glass: 1\n- Plate: 1\n- Fork: 2\n- Knife: 1\n- Table: 1\n- Person: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24959.6, "ram_available_mb": 100812.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24959.4, "ram_available_mb": 100812.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 4856.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.48, "peak": 15.72, "min": 15.01}, "VDD_CPU_SOC_MSS": {"avg": 16.42, "peak": 16.94, "min": 16.14}, "VDD_GPU": {"avg": 21.24, "peak": 24.04, "min": 20.1}, "VIN": {"avg": 62.93, "peak": 66.22, "min": 55.64}}, "power_watts_avg": 21.24, "energy_joules_est": 28.77, "sample_count": 10, "duration_seconds": 1.354}, "timestamp": "2026-01-17T16:07:01.937140"}
{"image_index": 482, "image_name": "000000054605.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054605.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1954.369, "latencies_ms": [1954.369], "images_per_second": 0.512, "prompt_tokens": 27, "response_tokens_est": 85, "n_tiles": 1, "output_text": "The main objects in the image are a chocolate-flavored milkshake and a slice of cake. The milkshake is placed on the left side of the image, with a glass of water and a white napkin in the foreground. The cake is on the right side, with a fork and knife resting on the napkin. The background features a table with a person sitting, chairs, and a blurred background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24959.4, "ram_available_mb": 100812.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24959.4, "ram_available_mb": 100812.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 4859.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.59, "peak": 15.82, "min": 15.11}, "VDD_CPU_SOC_MSS": {"avg": 16.48, "peak": 16.93, "min": 16.14}, "VDD_GPU": {"avg": 20.78, "peak": 24.04, "min": 19.7}, "VIN": {"avg": 64.02, "peak": 74.04, "min": 60.29}}, "power_watts_avg": 20.78, "energy_joules_est": 40.62, "sample_count": 15, "duration_seconds": 1.955}, "timestamp": "2026-01-17T16:07:03.897280"}
{"image_index": 482, "image_name": "000000054605.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054605.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1616.43, "latencies_ms": [1616.43], "images_per_second": 0.619, "prompt_tokens": 21, "response_tokens_est": 70, "n_tiles": 1, "output_text": "The image depicts a cozy cafe setting with a person seated at a table, enjoying a chocolate and vanilla ice cream cone. In the foreground, there is a plate with a slice of cake, and a glass of a similar ice cream cone is placed on the table. The background shows other patrons and tables, creating a relaxed and inviting atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24959.4, "ram_available_mb": 100812.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24959.6, "ram_available_mb": 100812.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 4854.7, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.57, "peak": 15.72, "min": 15.32}, "VDD_CPU_SOC_MSS": {"avg": 16.47, "peak": 16.54, "min": 16.14}, "VDD_GPU": {"avg": 21.01, "peak": 24.03, "min": 19.7}, "VIN": {"avg": 64.25, "peak": 69.66, "min": 59.86}}, "power_watts_avg": 21.01, "energy_joules_est": 33.97, "sample_count": 12, "duration_seconds": 1.617}, "timestamp": "2026-01-17T16:07:05.519618"}
{"image_index": 482, "image_name": "000000054605.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054605.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1770.418, "latencies_ms": [1770.418], "images_per_second": 0.565, "prompt_tokens": 19, "response_tokens_est": 77, "n_tiles": 1, "output_text": "The image features a dessert on a white plate, which is placed on a dark table. The dessert is a layered cake with a creamy white frosting. The background includes a glass of a creamy, chocolate-flavored drink with a straw, a brown table, and a person sitting at a table. The lighting is bright, and the overall setting appears to be indoors.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24959.6, "ram_available_mb": 100812.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24959.9, "ram_available_mb": 100812.3, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 4853.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.67, "peak": 15.93, "min": 15.11}, "VDD_CPU_SOC_MSS": {"avg": 16.48, "peak": 16.54, "min": 16.14}, "VDD_GPU": {"avg": 20.88, "peak": 24.04, "min": 19.7}, "VIN": {"avg": 62.97, "peak": 67.78, "min": 55.48}}, "power_watts_avg": 20.88, "energy_joules_est": 36.97, "sample_count": 13, "duration_seconds": 1.771}, "timestamp": "2026-01-17T16:07:07.295895"}
{"image_index": 483, "image_name": "000000054628.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054628.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2835.187, "latencies_ms": [2835.187], "images_per_second": 0.353, "prompt_tokens": 9, "response_tokens_est": 36, "n_tiles": 12, "output_text": "The image shows a beautifully decorated cake with intricate designs and a variety of flowers, placed on a table surrounded by other tables and chairs, all set up for a special event.", "error": null, "sys_before": {"cpu_percent": 50.9, "ram_used_mb": 24936.7, "ram_available_mb": 100835.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 24936.9, "ram_available_mb": 100835.3, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.25, "peak": 16.76, "min": 14.0}, "VDD_CPU_SOC_MSS": {"avg": 15.94, "peak": 17.71, "min": 14.96}, "VDD_GPU": {"avg": 34.57, "peak": 43.31, "min": 26.39}, "VIN": {"avg": 77.67, "peak": 118.26, "min": 66.56}}, "power_watts_avg": 34.57, "energy_joules_est": 98.03, "sample_count": 22, "duration_seconds": 2.836}, "timestamp": "2026-01-17T16:07:10.213171"}
{"image_index": 483, "image_name": "000000054628.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054628.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 6044.146, "latencies_ms": [6044.146], "images_per_second": 0.165, "prompt_tokens": 23, "response_tokens_est": 128, "n_tiles": 12, "output_text": "object: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject:", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24936.9, "ram_available_mb": 100835.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 24937.1, "ram_available_mb": 100835.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.13, "peak": 16.56, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.55, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 30.53, "peak": 44.89, "min": 25.6}, "VIN": {"avg": 73.76, "peak": 130.23, "min": 54.33}}, "power_watts_avg": 30.53, "energy_joules_est": 184.54, "sample_count": 48, "duration_seconds": 6.044}, "timestamp": "2026-01-17T16:07:16.263832"}
{"image_index": 483, "image_name": "000000054628.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054628.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 4803.729, "latencies_ms": [4803.729], "images_per_second": 0.208, "prompt_tokens": 27, "response_tokens_est": 92, "n_tiles": 12, "output_text": "The main object in the foreground is a large, intricately decorated cake placed on a blue cloth. The cake is positioned near the center of the image, drawing attention to its elaborate design. In the background, there are multiple tables set up with various items, chairs, and decorations, suggesting a larger event space. The cake is the focal point of the foreground, while the background features additional elements that contribute to the overall ambiance of the setting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24937.1, "ram_available_mb": 100835.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 24938.1, "ram_available_mb": 100834.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.14, "peak": 16.56, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.61, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 31.75, "peak": 45.28, "min": 25.6}, "VIN": {"avg": 74.11, "peak": 115.73, "min": 58.19}}, "power_watts_avg": 31.75, "energy_joules_est": 152.53, "sample_count": 38, "duration_seconds": 4.804}, "timestamp": "2026-01-17T16:07:21.074048"}
{"image_index": 483, "image_name": "000000054628.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054628.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3680.677, "latencies_ms": [3680.677], "images_per_second": 0.272, "prompt_tokens": 21, "response_tokens_est": 59, "n_tiles": 12, "output_text": "The image depicts a beautifully decorated wedding reception venue with a large, intricately designed cake placed on a table. The room is filled with guests seated at round tables covered with white tablecloths, and a grand chandelier hangs from the ceiling, adding to the elegant atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24938.1, "ram_available_mb": 100834.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 24938.8, "ram_available_mb": 100833.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.16, "peak": 16.56, "min": 14.12}, "VDD_CPU_SOC_MSS": {"avg": 15.68, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 33.7, "peak": 45.69, "min": 26.0}, "VIN": {"avg": 75.58, "peak": 109.63, "min": 59.84}}, "power_watts_avg": 33.7, "energy_joules_est": 124.07, "sample_count": 29, "duration_seconds": 3.681}, "timestamp": "2026-01-17T16:07:24.761760"}
{"image_index": 483, "image_name": "000000054628.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054628.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3365.512, "latencies_ms": [3365.512], "images_per_second": 0.297, "prompt_tokens": 19, "response_tokens_est": 50, "n_tiles": 12, "output_text": "The cake in the image is predominantly white with intricate golden patterns. The lighting is warm, creating a cozy atmosphere. The setting appears to be indoors, with a blue fabric covering the table and chairs, and a chandelier providing additional lighting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24938.8, "ram_available_mb": 100833.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 24938.8, "ram_available_mb": 100833.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.19, "peak": 16.56, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.69, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 34.54, "peak": 45.28, "min": 26.0}, "VIN": {"avg": 75.7, "peak": 127.1, "min": 56.79}}, "power_watts_avg": 34.54, "energy_joules_est": 116.26, "sample_count": 26, "duration_seconds": 3.366}, "timestamp": "2026-01-17T16:07:28.134173"}
{"image_index": 484, "image_name": "000000054654.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054654.jpg", "image_width": 407, "image_height": 640, "image_resolution": "407x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1385.381, "latencies_ms": [1385.381], "images_per_second": 0.722, "prompt_tokens": 9, "response_tokens_est": 29, "n_tiles": 6, "output_text": "A woman is standing in a kitchen, smiling at the camera, holding a plate of food in one hand and a fork in the other.", "error": null, "sys_before": {"cpu_percent": 42.4, "ram_used_mb": 24962.4, "ram_available_mb": 100809.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 24963.1, "ram_available_mb": 100809.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.6, "peak": 15.14, "min": 13.61}, "VDD_CPU_SOC_MSS": {"avg": 15.27, "peak": 16.14, "min": 14.17}, "VDD_GPU": {"avg": 33.84, "peak": 40.56, "min": 26.8}, "VIN": {"avg": 75.93, "peak": 99.47, "min": 60.92}}, "power_watts_avg": 33.84, "energy_joules_est": 46.9, "sample_count": 10, "duration_seconds": 1.386}, "timestamp": "2026-01-17T16:07:29.591251"}
{"image_index": 484, "image_name": "000000054654.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054654.jpg", "image_width": 407, "image_height": 640, "image_resolution": "407x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1703.665, "latencies_ms": [1703.665], "images_per_second": 0.587, "prompt_tokens": 23, "response_tokens_est": 39, "n_tiles": 6, "output_text": "1. Woman\n2. Plate\n3. Stove\n4. Pot\n5. Garbage can\n6. Pots\n7. Pans\n8. Stove top", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24963.1, "ram_available_mb": 100809.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 24964.3, "ram_available_mb": 100807.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.99, "peak": 15.44, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.6, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 32.72, "peak": 40.97, "min": 24.83}, "VIN": {"avg": 71.14, "peak": 97.66, "min": 59.95}}, "power_watts_avg": 32.72, "energy_joules_est": 55.76, "sample_count": 13, "duration_seconds": 1.704}, "timestamp": "2026-01-17T16:07:31.301112"}
{"image_index": 484, "image_name": "000000054654.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054654.jpg", "image_width": 407, "image_height": 640, "image_resolution": "407x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2248.023, "latencies_ms": [2248.023], "images_per_second": 0.445, "prompt_tokens": 27, "response_tokens_est": 59, "n_tiles": 6, "output_text": "The main object in the foreground is a woman wearing a blue sweater with red and white patterns. She is standing near a stove with a pot on it. In the background, there is a kitchen counter with various items, including a white dish rack holding dishes and a green poster with text.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24964.3, "ram_available_mb": 100807.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 24964.3, "ram_available_mb": 100807.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.05, "peak": 15.54, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.66, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 30.4, "peak": 40.56, "min": 23.64}, "VIN": {"avg": 72.9, "peak": 109.83, "min": 52.11}}, "power_watts_avg": 30.4, "energy_joules_est": 68.35, "sample_count": 17, "duration_seconds": 2.248}, "timestamp": "2026-01-17T16:07:33.554911"}
{"image_index": 484, "image_name": "000000054654.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054654.jpg", "image_width": 407, "image_height": 640, "image_resolution": "407x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2192.442, "latencies_ms": [2192.442], "images_per_second": 0.456, "prompt_tokens": 21, "response_tokens_est": 57, "n_tiles": 6, "output_text": "The image depicts a cozy kitchen scene with a woman standing in front of a stove. She is wearing a blue sweater with red and white patterns and is holding a plate of food. The kitchen has a rustic feel, with wooden cabinets and a patterned rug on the floor.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24964.3, "ram_available_mb": 100807.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 24965.1, "ram_available_mb": 100807.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.03, "peak": 15.44, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.66, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 30.42, "peak": 40.95, "min": 23.64}, "VIN": {"avg": 70.94, "peak": 107.11, "min": 55.1}}, "power_watts_avg": 30.42, "energy_joules_est": 66.71, "sample_count": 17, "duration_seconds": 2.193}, "timestamp": "2026-01-17T16:07:35.753727"}
{"image_index": 484, "image_name": "000000054654.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054654.jpg", "image_width": 407, "image_height": 640, "image_resolution": "407x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2273.488, "latencies_ms": [2273.488], "images_per_second": 0.44, "prompt_tokens": 19, "response_tokens_est": 60, "n_tiles": 6, "output_text": "The image features a woman in a blue and red knitted sweater standing in a kitchen. The kitchen has a white stove with a black pot on it, and various kitchen items and utensils are visible in the background. The lighting is dim, and the overall atmosphere appears warm and cozy.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24965.1, "ram_available_mb": 100807.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 24965.3, "ram_available_mb": 100806.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.02, "peak": 15.44, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.63, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 30.24, "peak": 40.56, "min": 23.64}, "VIN": {"avg": 74.42, "peak": 117.17, "min": 62.62}}, "power_watts_avg": 30.24, "energy_joules_est": 68.76, "sample_count": 17, "duration_seconds": 2.274}, "timestamp": "2026-01-17T16:07:38.033440"}
{"image_index": 485, "image_name": "000000054931.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054931.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1061.104, "latencies_ms": [1061.104], "images_per_second": 0.942, "prompt_tokens": 9, "response_tokens_est": 17, "n_tiles": 6, "output_text": "A woman is holding a rope and walking on a dirt path in a park.", "error": null, "sys_before": {"cpu_percent": 49.4, "ram_used_mb": 24965.3, "ram_available_mb": 100806.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 24965.3, "ram_available_mb": 100806.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.59, "peak": 15.14, "min": 13.9}, "VDD_CPU_SOC_MSS": {"avg": 15.35, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 35.21, "peak": 40.18, "min": 29.94}, "VIN": {"avg": 82.29, "peak": 117.01, "min": 60.77}}, "power_watts_avg": 35.21, "energy_joules_est": 37.38, "sample_count": 8, "duration_seconds": 1.062}, "timestamp": "2026-01-17T16:07:39.155867"}
{"image_index": 485, "image_name": "000000054931.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054931.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1776.037, "latencies_ms": [1776.037], "images_per_second": 0.563, "prompt_tokens": 23, "response_tokens_est": 41, "n_tiles": 6, "output_text": "woman: 1\nshirt: 1\njeans: 1\nboot: 1\nrope: 1\nwoman's hair: 1\nwoman's face: 1", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 24965.3, "ram_available_mb": 100806.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 24965.3, "ram_available_mb": 100806.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.96, "peak": 15.34, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.56, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 33.36, "peak": 41.74, "min": 24.82}, "VIN": {"avg": 74.23, "peak": 110.45, "min": 53.51}}, "power_watts_avg": 33.36, "energy_joules_est": 59.26, "sample_count": 13, "duration_seconds": 1.776}, "timestamp": "2026-01-17T16:07:40.938027"}
{"image_index": 485, "image_name": "000000054931.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054931.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1975.25, "latencies_ms": [1975.25], "images_per_second": 0.506, "prompt_tokens": 27, "response_tokens_est": 47, "n_tiles": 6, "output_text": "The main object in the foreground is a woman holding a rope. She is standing on a dirt ground, with a wooden fence visible in the background. The fence is partially obscured by trees, creating a natural and rustic setting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24965.3, "ram_available_mb": 100806.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 24966.3, "ram_available_mb": 100805.9, "ram_percent": 19.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.83, "peak": 15.24, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.35, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 30.83, "peak": 40.57, "min": 23.64}, "VIN": {"avg": 73.29, "peak": 112.46, "min": 62.85}}, "power_watts_avg": 30.83, "energy_joules_est": 60.91, "sample_count": 15, "duration_seconds": 1.976}, "timestamp": "2026-01-17T16:07:42.919325"}
{"image_index": 485, "image_name": "000000054931.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054931.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1858.591, "latencies_ms": [1858.591], "images_per_second": 0.538, "prompt_tokens": 21, "response_tokens_est": 43, "n_tiles": 6, "output_text": "The image depicts a woman standing in a natural, outdoor setting with a dirt ground and a wooden fence in the background. She is holding a rope and appears to be preparing to perform a jump rope activity.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24966.3, "ram_available_mb": 100805.9, "ram_percent": 19.9}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24966.3, "ram_available_mb": 100805.9, "ram_percent": 19.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.86, "peak": 15.24, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.58, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 31.15, "peak": 40.95, "min": 23.64}, "VIN": {"avg": 69.97, "peak": 90.75, "min": 54.71}}, "power_watts_avg": 31.15, "energy_joules_est": 57.9, "sample_count": 14, "duration_seconds": 1.859}, "timestamp": "2026-01-17T16:07:44.784618"}
{"image_index": 485, "image_name": "000000054931.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054931.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1744.223, "latencies_ms": [1744.223], "images_per_second": 0.573, "prompt_tokens": 19, "response_tokens_est": 39, "n_tiles": 6, "output_text": "The woman in the image is wearing a pink button-up shirt and black pants. She is holding a pair of brown boots. The lighting is bright, and the weather appears to be sunny.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24966.3, "ram_available_mb": 100805.9, "ram_percent": 19.9}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 24966.3, "ram_available_mb": 100805.9, "ram_percent": 19.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.84, "peak": 15.24, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.35, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 32.07, "peak": 40.57, "min": 24.42}, "VIN": {"avg": 72.7, "peak": 117.98, "min": 57.92}}, "power_watts_avg": 32.07, "energy_joules_est": 55.95, "sample_count": 12, "duration_seconds": 1.745}, "timestamp": "2026-01-17T16:07:46.535012"}
{"image_index": 486, "image_name": "000000054967.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054967.jpg", "image_width": 424, "image_height": 640, "image_resolution": "424x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2940.409, "latencies_ms": [2940.409], "images_per_second": 0.34, "prompt_tokens": 9, "response_tokens_est": 87, "n_tiles": 6, "output_text": "The image depicts a busy urban street scene with multiple vehicles, including a truck, cars, and a bus, all traveling in the same direction. The street is lined with tall buildings, and there are various traffic signs and signals visible, indicating a well-organized traffic system. The weather appears to be overcast, with a foggy or misty atmosphere, which adds to the overall urban ambiance of the scene.", "error": null, "sys_before": {"cpu_percent": 40.3, "ram_used_mb": 24966.3, "ram_available_mb": 100805.9, "ram_percent": 19.9}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 24966.8, "ram_available_mb": 100805.4, "ram_percent": 19.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.96, "peak": 15.22, "min": 13.69}, "VDD_CPU_SOC_MSS": {"avg": 15.75, "peak": 16.14, "min": 14.18}, "VDD_GPU": {"avg": 28.23, "peak": 40.56, "min": 23.64}, "VIN": {"avg": 70.73, "peak": 110.15, "min": 57.53}}, "power_watts_avg": 28.23, "energy_joules_est": 83.02, "sample_count": 23, "duration_seconds": 2.941}, "timestamp": "2026-01-17T16:07:49.536168"}
{"image_index": 486, "image_name": "000000054967.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054967.jpg", "image_width": 424, "image_height": 640, "image_resolution": "424x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1504.733, "latencies_ms": [1504.733], "images_per_second": 0.665, "prompt_tokens": 23, "response_tokens_est": 32, "n_tiles": 6, "output_text": "1. Car\n2. Car\n3. Car\n4. Car\n5. Car\n6. Car\n7. Car\n8. Car", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 24966.8, "ram_available_mb": 100805.4, "ram_percent": 19.9}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 24967.3, "ram_available_mb": 100804.9, "ram_percent": 19.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.03, "peak": 15.44, "min": 14.43}, "VDD_CPU_SOC_MSS": {"avg": 15.64, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 33.45, "peak": 40.18, "min": 26.01}, "VIN": {"avg": 75.33, "peak": 109.08, "min": 62.88}}, "power_watts_avg": 33.45, "energy_joules_est": 50.35, "sample_count": 11, "duration_seconds": 1.505}, "timestamp": "2026-01-17T16:07:51.047184"}
{"image_index": 486, "image_name": "000000054967.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054967.jpg", "image_width": 424, "image_height": 640, "image_resolution": "424x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2283.287, "latencies_ms": [2283.287], "images_per_second": 0.438, "prompt_tokens": 27, "response_tokens_est": 61, "n_tiles": 6, "output_text": "The main objects in the image are a city street, a traffic light, and a pedestrian. The traffic light is positioned in the foreground, near the sidewalk, while the pedestrian is near the sidewalk, slightly to the right. The city street is in the background, with buildings and other vehicles visible.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24967.3, "ram_available_mb": 100804.9, "ram_percent": 19.9}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 24967.5, "ram_available_mb": 100804.7, "ram_percent": 19.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.11, "peak": 15.44, "min": 14.43}, "VDD_CPU_SOC_MSS": {"avg": 15.66, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 30.68, "peak": 40.56, "min": 24.03}, "VIN": {"avg": 73.71, "peak": 113.84, "min": 61.33}}, "power_watts_avg": 30.68, "energy_joules_est": 70.06, "sample_count": 17, "duration_seconds": 2.284}, "timestamp": "2026-01-17T16:07:53.340835"}
{"image_index": 486, "image_name": "000000054967.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054967.jpg", "image_width": 424, "image_height": 640, "image_resolution": "424x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2659.284, "latencies_ms": [2659.284], "images_per_second": 0.376, "prompt_tokens": 21, "response_tokens_est": 75, "n_tiles": 6, "output_text": "The image depicts a bustling urban street scene during the daytime. The street is lined with tall buildings, and there are multiple vehicles, including cars and a bus, moving along the road. Traffic lights are visible, and there are signs indicating directions and regulations for the area. The overall atmosphere suggests a typical city environment with a mix of pedestrian and vehicular activity.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24967.5, "ram_available_mb": 100804.7, "ram_percent": 19.9}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 24968.0, "ram_available_mb": 100804.2, "ram_percent": 19.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.09, "peak": 15.44, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.77, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 29.31, "peak": 40.16, "min": 23.64}, "VIN": {"avg": 71.22, "peak": 108.97, "min": 61.87}}, "power_watts_avg": 29.31, "energy_joules_est": 77.96, "sample_count": 20, "duration_seconds": 2.66}, "timestamp": "2026-01-17T16:07:56.006098"}
{"image_index": 486, "image_name": "000000054967.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054967.jpg", "image_width": 424, "image_height": 640, "image_resolution": "424x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2225.535, "latencies_ms": [2225.535], "images_per_second": 0.449, "prompt_tokens": 19, "response_tokens_est": 59, "n_tiles": 6, "output_text": "The image depicts a city street scene during the daytime, with a clear blue sky and a few scattered clouds. The street is lined with tall buildings, and the overall lighting is bright and natural. The weather appears to be clear and sunny, with no visible signs of rain or snow.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24968.0, "ram_available_mb": 100804.2, "ram_percent": 19.9}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 24968.2, "ram_available_mb": 100804.0, "ram_percent": 19.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.09, "peak": 15.44, "min": 14.43}, "VDD_CPU_SOC_MSS": {"avg": 15.65, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 30.42, "peak": 40.56, "min": 24.04}, "VIN": {"avg": 72.9, "peak": 103.73, "min": 49.86}}, "power_watts_avg": 30.42, "energy_joules_est": 67.71, "sample_count": 17, "duration_seconds": 2.226}, "timestamp": "2026-01-17T16:07:58.237802"}
{"image_index": 487, "image_name": "000000055002.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055002.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2879.911, "latencies_ms": [2879.911], "images_per_second": 0.347, "prompt_tokens": 9, "response_tokens_est": 37, "n_tiles": 12, "output_text": "The image shows a close-up view of a stainless steel toilet with a blue toilet brush placed on the side, and the floor is tiled with square tiles in a light color.", "error": null, "sys_before": {"cpu_percent": 45.2, "ram_used_mb": 24924.5, "ram_available_mb": 100847.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 24925.0, "ram_available_mb": 100847.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.1, "peak": 16.76, "min": 13.59}, "VDD_CPU_SOC_MSS": {"avg": 15.84, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 35.02, "peak": 44.1, "min": 26.39}, "VIN": {"avg": 78.36, "peak": 118.96, "min": 64.85}}, "power_watts_avg": 35.02, "energy_joules_est": 100.86, "sample_count": 22, "duration_seconds": 2.88}, "timestamp": "2026-01-17T16:08:01.210657"}
{"image_index": 487, "image_name": "000000055002.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055002.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2887.533, "latencies_ms": [2887.533], "images_per_second": 0.346, "prompt_tokens": 23, "response_tokens_est": 36, "n_tiles": 12, "output_text": "toilet brush: 1\ntoilet: 1\ntoilet paper: 1\ntoilet paper roll: 1\ntoilet paper holder: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24925.0, "ram_available_mb": 100847.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 24926.2, "ram_available_mb": 100846.0, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.0, "peak": 16.36, "min": 14.0}, "VDD_CPU_SOC_MSS": {"avg": 15.59, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 36.04, "peak": 45.3, "min": 26.39}, "VIN": {"avg": 81.2, "peak": 119.88, "min": 60.12}}, "power_watts_avg": 36.04, "energy_joules_est": 104.08, "sample_count": 22, "duration_seconds": 2.888}, "timestamp": "2026-01-17T16:08:04.104900"}
{"image_index": 487, "image_name": "000000055002.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055002.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3623.651, "latencies_ms": [3623.651], "images_per_second": 0.276, "prompt_tokens": 27, "response_tokens_est": 57, "n_tiles": 12, "output_text": "The main object in the foreground is a toilet with a blue seat and lid. The toilet is situated on a tiled floor, and there is a metal toilet paper holder attached to the wall next to it. The background features a tiled wall and a metal toilet paper holder.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24926.2, "ram_available_mb": 100846.0, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 24926.4, "ram_available_mb": 100845.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.93, "peak": 16.36, "min": 13.92}, "VDD_CPU_SOC_MSS": {"avg": 15.42, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 33.9, "peak": 45.3, "min": 25.6}, "VIN": {"avg": 75.5, "peak": 122.18, "min": 54.39}}, "power_watts_avg": 33.9, "energy_joules_est": 122.86, "sample_count": 28, "duration_seconds": 3.624}, "timestamp": "2026-01-17T16:08:07.735112"}
{"image_index": 487, "image_name": "000000055002.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055002.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3824.758, "latencies_ms": [3824.758], "images_per_second": 0.261, "prompt_tokens": 21, "response_tokens_est": 63, "n_tiles": 12, "output_text": "The image depicts a bathroom setting with a toilet and a handrail. The toilet is situated on a tiled floor, and there is a handrail adjacent to it. The overall scene appears to be in a public restroom, possibly a public restroom with a handrail for support.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24926.4, "ram_available_mb": 100845.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 24926.4, "ram_available_mb": 100845.8, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.9, "peak": 16.36, "min": 14.02}, "VDD_CPU_SOC_MSS": {"avg": 15.33, "peak": 16.92, "min": 14.17}, "VDD_GPU": {"avg": 33.28, "peak": 45.69, "min": 25.6}, "VIN": {"avg": 74.04, "peak": 118.52, "min": 57.58}}, "power_watts_avg": 33.28, "energy_joules_est": 127.3, "sample_count": 30, "duration_seconds": 3.825}, "timestamp": "2026-01-17T16:08:11.566352"}
{"image_index": 487, "image_name": "000000055002.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055002.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3097.439, "latencies_ms": [3097.439], "images_per_second": 0.323, "prompt_tokens": 19, "response_tokens_est": 42, "n_tiles": 12, "output_text": "The image shows a metallic toilet with a blue brush holder attached to it. The floor is tiled with light beige tiles, and the lighting is bright, casting a clear reflection on the metallic surfaces.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24926.4, "ram_available_mb": 100845.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 24926.7, "ram_available_mb": 100845.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.96, "peak": 16.36, "min": 13.92}, "VDD_CPU_SOC_MSS": {"avg": 15.52, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 35.25, "peak": 44.91, "min": 26.01}, "VIN": {"avg": 77.36, "peak": 116.99, "min": 62.96}}, "power_watts_avg": 35.25, "energy_joules_est": 109.21, "sample_count": 23, "duration_seconds": 3.098}, "timestamp": "2026-01-17T16:08:14.672209"}
{"image_index": 488, "image_name": "000000055022.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055022.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2318.788, "latencies_ms": [2318.788], "images_per_second": 0.431, "prompt_tokens": 9, "response_tokens_est": 21, "n_tiles": 12, "output_text": "A pink bicycle is parked in a room with other bicycles and a woman walking in the background.", "error": null, "sys_before": {"cpu_percent": 36.0, "ram_used_mb": 24938.2, "ram_available_mb": 100834.0, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.3, "ram_used_mb": 24935.8, "ram_available_mb": 100836.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.14, "peak": 16.66, "min": 13.19}, "VDD_CPU_SOC_MSS": {"avg": 15.93, "peak": 17.71, "min": 14.18}, "VDD_GPU": {"avg": 37.28, "peak": 44.51, "min": 29.15}, "VIN": {"avg": 84.12, "peak": 119.97, "min": 66.43}}, "power_watts_avg": 37.28, "energy_joules_est": 86.46, "sample_count": 17, "duration_seconds": 2.319}, "timestamp": "2026-01-17T16:08:17.093992"}
{"image_index": 488, "image_name": "000000055022.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055022.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 3247.508, "latencies_ms": [3247.508], "images_per_second": 0.308, "prompt_tokens": 23, "response_tokens_est": 47, "n_tiles": 12, "output_text": "1. Pink bicycle\n2. Bicycle rack\n3. Bicycle\n4. Bicycle wheel\n5. Bicycle seat\n6. Bicycle handlebars\n7. Bicycle saddle\n8. Bicycle chain", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24935.8, "ram_available_mb": 100836.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 24928.1, "ram_available_mb": 100844.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.27, "peak": 16.66, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.81, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 35.42, "peak": 46.07, "min": 26.39}, "VIN": {"avg": 75.97, "peak": 113.8, "min": 57.09}}, "power_watts_avg": 35.42, "energy_joules_est": 115.04, "sample_count": 25, "duration_seconds": 3.248}, "timestamp": "2026-01-17T16:08:20.348439"}
{"image_index": 488, "image_name": "000000055022.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055022.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3257.436, "latencies_ms": [3257.436], "images_per_second": 0.307, "prompt_tokens": 27, "response_tokens_est": 47, "n_tiles": 12, "output_text": "The main objects in the image are a pink bicycle and a bicycle rack. The bicycle is positioned in the foreground, with the rack attached to its frame. The rack is near the bicycle, and the bicycle is in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24928.1, "ram_available_mb": 100844.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 24929.2, "ram_available_mb": 100843.0, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.25, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.92, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 34.94, "peak": 45.28, "min": 26.39}, "VIN": {"avg": 78.14, "peak": 108.67, "min": 67.14}}, "power_watts_avg": 34.94, "energy_joules_est": 113.83, "sample_count": 25, "duration_seconds": 3.258}, "timestamp": "2026-01-17T16:08:23.611546"}
{"image_index": 488, "image_name": "000000055022.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055022.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 4007.959, "latencies_ms": [4007.959], "images_per_second": 0.25, "prompt_tokens": 21, "response_tokens_est": 69, "n_tiles": 12, "output_text": "The image depicts a well-lit indoor setting with a wooden floor and a white wall in the background. There are several bicycles parked in a row, with one prominently featuring a pink frame and a basket. A woman is seen walking past the bicycles, and there is a bicycle rack with a pink bicycle leaning against it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24929.2, "ram_available_mb": 100843.0, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 24928.6, "ram_available_mb": 100843.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.2, "peak": 16.66, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.72, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 33.23, "peak": 45.69, "min": 26.0}, "VIN": {"avg": 76.28, "peak": 118.57, "min": 64.17}}, "power_watts_avg": 33.23, "energy_joules_est": 133.19, "sample_count": 32, "duration_seconds": 4.008}, "timestamp": "2026-01-17T16:08:27.626249"}
{"image_index": 488, "image_name": "000000055022.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055022.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3025.13, "latencies_ms": [3025.13], "images_per_second": 0.331, "prompt_tokens": 19, "response_tokens_est": 40, "n_tiles": 12, "output_text": "The image features a pink bicycle with a basket, which is prominently displayed on a wooden floor. The lighting is bright, casting clear shadows, and the weather appears to be clear and sunny.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24928.6, "ram_available_mb": 100843.5, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 24928.0, "ram_available_mb": 100844.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.25, "peak": 16.56, "min": 14.12}, "VDD_CPU_SOC_MSS": {"avg": 15.85, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 35.74, "peak": 44.91, "min": 26.79}, "VIN": {"avg": 77.84, "peak": 106.16, "min": 63.99}}, "power_watts_avg": 35.74, "energy_joules_est": 108.13, "sample_count": 22, "duration_seconds": 3.026}, "timestamp": "2026-01-17T16:08:30.657960"}
{"image_index": 489, "image_name": "000000055072.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055072.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1543.623, "latencies_ms": [1543.623], "images_per_second": 0.648, "prompt_tokens": 9, "response_tokens_est": 35, "n_tiles": 6, "output_text": "A giraffe is standing in a savannah, with its long neck stretched out and its distinctive spotted coat visible against the backdrop of dry, brown grass and sparse trees.", "error": null, "sys_before": {"cpu_percent": 38.7, "ram_used_mb": 24955.0, "ram_available_mb": 100817.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 24956.5, "ram_available_mb": 100815.7, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.73, "peak": 15.14, "min": 13.82}, "VDD_CPU_SOC_MSS": {"avg": 15.46, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 33.37, "peak": 40.97, "min": 26.0}, "VIN": {"avg": 80.39, "peak": 111.39, "min": 61.52}}, "power_watts_avg": 33.37, "energy_joules_est": 51.52, "sample_count": 11, "duration_seconds": 1.544}, "timestamp": "2026-01-17T16:08:32.258763"}
{"image_index": 489, "image_name": "000000055072.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055072.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1188.153, "latencies_ms": [1188.153], "images_per_second": 0.842, "prompt_tokens": 23, "response_tokens_est": 20, "n_tiles": 6, "output_text": "giraffe: 1\ntrees: 1\nbushes: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24956.5, "ram_available_mb": 100815.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 24973.2, "ram_available_mb": 100798.9, "ram_percent": 19.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.95, "peak": 15.44, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.7, "peak": 16.53, "min": 14.96}, "VDD_GPU": {"avg": 35.45, "peak": 40.56, "min": 28.36}, "VIN": {"avg": 79.79, "peak": 110.16, "min": 59.73}}, "power_watts_avg": 35.45, "energy_joules_est": 42.13, "sample_count": 9, "duration_seconds": 1.188}, "timestamp": "2026-01-17T16:08:33.453476"}
{"image_index": 489, "image_name": "000000055072.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055072.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2377.21, "latencies_ms": [2377.21], "images_per_second": 0.421, "prompt_tokens": 27, "response_tokens_est": 64, "n_tiles": 6, "output_text": "The giraffe is positioned in the foreground, with its long neck and legs extending towards the background. The background features a mix of trees and shrubs, creating a natural and serene setting. The giraffe appears to be standing in a grassy area, with the trees and shrubs providing a lush backdrop.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24973.2, "ram_available_mb": 100798.9, "ram_percent": 19.9}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 24973.0, "ram_available_mb": 100799.2, "ram_percent": 19.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.07, "peak": 15.44, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.7, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 30.51, "peak": 41.34, "min": 23.64}, "VIN": {"avg": 73.66, "peak": 122.1, "min": 60.39}}, "power_watts_avg": 30.51, "energy_joules_est": 72.55, "sample_count": 18, "duration_seconds": 2.378}, "timestamp": "2026-01-17T16:08:35.837784"}
{"image_index": 489, "image_name": "000000055072.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055072.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2347.418, "latencies_ms": [2347.418], "images_per_second": 0.426, "prompt_tokens": 21, "response_tokens_est": 63, "n_tiles": 6, "output_text": "The image depicts a giraffe standing in a savanna-like environment, characterized by dry grass and sparse vegetation. The giraffe is facing to the right, with its long neck and distinctive spotted coat clearly visible. The setting suggests a natural habitat, likely in Africa, where giraffes are commonly found.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24973.0, "ram_available_mb": 100799.2, "ram_percent": 19.9}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 24973.2, "ram_available_mb": 100798.9, "ram_percent": 19.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.09, "peak": 15.54, "min": 14.4}, "VDD_CPU_SOC_MSS": {"avg": 15.66, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 29.98, "peak": 40.16, "min": 23.64}, "VIN": {"avg": 72.82, "peak": 113.72, "min": 58.94}}, "power_watts_avg": 29.98, "energy_joules_est": 70.38, "sample_count": 18, "duration_seconds": 2.348}, "timestamp": "2026-01-17T16:08:38.193217"}
{"image_index": 489, "image_name": "000000055072.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055072.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2339.169, "latencies_ms": [2339.169], "images_per_second": 0.428, "prompt_tokens": 19, "response_tokens_est": 63, "n_tiles": 6, "output_text": "The giraffe in the image has a distinctive pattern of brown spots on a lighter background, which is typical of its species. The lighting is natural, with a soft, diffused quality that suggests either early morning or late afternoon. The weather appears to be clear, with no visible signs of rain or storm.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24973.2, "ram_available_mb": 100798.9, "ram_percent": 19.9}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 24973.5, "ram_available_mb": 100798.7, "ram_percent": 19.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.08, "peak": 15.44, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.64, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 29.96, "peak": 40.18, "min": 23.64}, "VIN": {"avg": 70.37, "peak": 100.32, "min": 54.5}}, "power_watts_avg": 29.96, "energy_joules_est": 70.09, "sample_count": 18, "duration_seconds": 2.339}, "timestamp": "2026-01-17T16:08:40.538774"}
{"image_index": 490, "image_name": "000000055150.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055150.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1839.197, "latencies_ms": [1839.197], "images_per_second": 0.544, "prompt_tokens": 9, "response_tokens_est": 46, "n_tiles": 6, "output_text": "A young girl with red hair is sitting in a black suitcase, smiling as she looks at the camera, while a young boy sits in a blue suitcase beside her, both seemingly enjoying their time in the parking lot.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 24973.5, "ram_available_mb": 100798.7, "ram_percent": 19.9}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 24974.2, "ram_available_mb": 100798.0, "ram_percent": 19.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.91, "peak": 15.34, "min": 14.2}, "VDD_CPU_SOC_MSS": {"avg": 15.6, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 31.85, "peak": 40.97, "min": 24.82}, "VIN": {"avg": 68.82, "peak": 112.36, "min": 54.64}}, "power_watts_avg": 31.85, "energy_joules_est": 58.59, "sample_count": 13, "duration_seconds": 1.839}, "timestamp": "2026-01-17T16:08:42.411090"}
{"image_index": 490, "image_name": "000000055150.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055150.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1506.961, "latencies_ms": [1506.961], "images_per_second": 0.664, "prompt_tokens": 23, "response_tokens_est": 32, "n_tiles": 6, "output_text": "1. Car\n2. Car\n3. Car\n4. Car\n5. Car\n6. Car\n7. Car\n8. Car", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24974.2, "ram_available_mb": 100798.0, "ram_percent": 19.9}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 24974.0, "ram_available_mb": 100798.2, "ram_percent": 19.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.0, "peak": 15.44, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.57, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 33.87, "peak": 40.97, "min": 26.39}, "VIN": {"avg": 73.29, "peak": 107.43, "min": 63.71}}, "power_watts_avg": 33.87, "energy_joules_est": 51.06, "sample_count": 11, "duration_seconds": 1.507}, "timestamp": "2026-01-17T16:08:43.924488"}
{"image_index": 490, "image_name": "000000055150.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055150.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2970.064, "latencies_ms": [2970.064], "images_per_second": 0.337, "prompt_tokens": 27, "response_tokens_est": 83, "n_tiles": 6, "output_text": "In the image, the main objects are a child in a car seat, a child in a car seat, and a car with a child in the car seat. The child in the car seat is positioned in the foreground, with the car and the child in the car seat being the main subjects. The car is parked in the background, and there are other cars and a Budget store visible in the distance.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24974.0, "ram_available_mb": 100798.2, "ram_percent": 19.9}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 24974.0, "ram_available_mb": 100798.2, "ram_percent": 19.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.96, "peak": 15.34, "min": 14.43}, "VDD_CPU_SOC_MSS": {"avg": 15.63, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 28.53, "peak": 40.95, "min": 22.86}, "VIN": {"avg": 69.55, "peak": 117.1, "min": 56.48}}, "power_watts_avg": 28.53, "energy_joules_est": 84.75, "sample_count": 23, "duration_seconds": 2.971}, "timestamp": "2026-01-17T16:08:46.900848"}
{"image_index": 490, "image_name": "000000055150.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055150.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2551.063, "latencies_ms": [2551.063], "images_per_second": 0.392, "prompt_tokens": 21, "response_tokens_est": 68, "n_tiles": 6, "output_text": "The image depicts a busy parking lot scene with several cars parked in a line. A young girl with red hair is sitting in a black car's rear passenger seat, holding a piece of paper. The setting appears to be a commercial area, possibly near a shopping center, with various cars and a Budget store visible in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24974.0, "ram_available_mb": 100798.2, "ram_percent": 19.9}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 24974.9, "ram_available_mb": 100797.2, "ram_percent": 19.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.9, "peak": 15.24, "min": 14.3}, "VDD_CPU_SOC_MSS": {"avg": 15.61, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 29.02, "peak": 40.57, "min": 23.25}, "VIN": {"avg": 68.86, "peak": 108.42, "min": 61.32}}, "power_watts_avg": 29.02, "energy_joules_est": 74.04, "sample_count": 20, "duration_seconds": 2.551}, "timestamp": "2026-01-17T16:08:49.459075"}
{"image_index": 490, "image_name": "000000055150.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055150.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2197.524, "latencies_ms": [2197.524], "images_per_second": 0.455, "prompt_tokens": 19, "response_tokens_est": 55, "n_tiles": 6, "output_text": "The image depicts a scene of a parking lot with wet ground, indicating recent rain. The lighting is soft and diffused, likely due to overcast weather. The colors are muted, with a mix of gray, blue, and brown tones dominating the scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24974.9, "ram_available_mb": 100797.2, "ram_percent": 19.9}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 24975.9, "ram_available_mb": 100796.3, "ram_percent": 19.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.84, "peak": 15.34, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.48, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 30.12, "peak": 40.18, "min": 23.25}, "VIN": {"avg": 69.14, "peak": 101.47, "min": 61.4}}, "power_watts_avg": 30.12, "energy_joules_est": 66.2, "sample_count": 16, "duration_seconds": 2.198}, "timestamp": "2026-01-17T16:08:51.663242"}
{"image_index": 491, "image_name": "000000055167.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055167.jpg", "image_width": 640, "image_height": 360, "image_resolution": "640x360", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 776.255, "latencies_ms": [776.255], "images_per_second": 1.288, "prompt_tokens": 9, "response_tokens_est": 26, "n_tiles": 2, "output_text": "A yellow and blue bus is driving down a street lined with houses and trees, with a person walking on the sidewalk nearby.", "error": null, "sys_before": {"cpu_percent": 35.3, "ram_used_mb": 24975.9, "ram_available_mb": 100796.3, "ram_percent": 19.9}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 24976.7, "ram_available_mb": 100795.5, "ram_percent": 19.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5055.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.55, "peak": 15.11, "min": 14.1}, "VDD_CPU_SOC_MSS": {"avg": 15.12, "peak": 15.75, "min": 14.57}, "VDD_GPU": {"avg": 27.1, "peak": 31.91, "min": 23.64}, "VIN": {"avg": 65.74, "peak": 79.11, "min": 58.85}}, "power_watts_avg": 27.1, "energy_joules_est": 21.04, "sample_count": 5, "duration_seconds": 0.777}, "timestamp": "2026-01-17T16:08:52.466529"}
{"image_index": 491, "image_name": "000000055167.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055167.jpg", "image_width": 640, "image_height": 360, "image_resolution": "640x360", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1053.307, "latencies_ms": [1053.307], "images_per_second": 0.949, "prompt_tokens": 23, "response_tokens_est": 38, "n_tiles": 2, "output_text": "bus: 1\nvan: 1\nhouse: 2\ntree: 1\nstreet light: 1\nbench: 1\ngarbage can: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24976.7, "ram_available_mb": 100795.5, "ram_percent": 19.9}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 24977.2, "ram_available_mb": 100795.0, "ram_percent": 19.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5064.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.82, "peak": 15.11, "min": 14.51}, "VDD_CPU_SOC_MSS": {"avg": 15.86, "peak": 16.14, "min": 15.36}, "VDD_GPU": {"avg": 26.17, "peak": 31.91, "min": 22.46}, "VIN": {"avg": 68.48, "peak": 92.75, "min": 58.17}}, "power_watts_avg": 26.17, "energy_joules_est": 27.57, "sample_count": 7, "duration_seconds": 1.054}, "timestamp": "2026-01-17T16:08:53.525895"}
{"image_index": 491, "image_name": "000000055167.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055167.jpg", "image_width": 640, "image_height": 360, "image_resolution": "640x360", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1810.002, "latencies_ms": [1810.002], "images_per_second": 0.552, "prompt_tokens": 27, "response_tokens_est": 71, "n_tiles": 2, "output_text": "The main objects in the image are a yellow and blue bus, a white van, and a small white building. The bus is in the foreground, slightly to the right, while the van is in the background, closer to the left. The small white building is near the van, and the background features a hill with trees and a few buildings.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24977.2, "ram_available_mb": 100795.0, "ram_percent": 19.9}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 24976.9, "ram_available_mb": 100795.3, "ram_percent": 19.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5067.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.21, "peak": 15.42, "min": 14.61}, "VDD_CPU_SOC_MSS": {"avg": 16.14, "peak": 16.54, "min": 15.36}, "VDD_GPU": {"avg": 23.91, "peak": 31.12, "min": 20.89}, "VIN": {"avg": 65.43, "peak": 90.73, "min": 59.92}}, "power_watts_avg": 23.91, "energy_joules_est": 43.28, "sample_count": 13, "duration_seconds": 1.81}, "timestamp": "2026-01-17T16:08:55.345556"}
{"image_index": 491, "image_name": "000000055167.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055167.jpg", "image_width": 640, "image_height": 360, "image_resolution": "640x360", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2250.167, "latencies_ms": [2250.167], "images_per_second": 0.444, "prompt_tokens": 21, "response_tokens_est": 90, "n_tiles": 2, "output_text": "The image depicts a vibrant scene of a bus in motion on a road, surrounded by a picturesque landscape. The setting is a quaint, rural area with a mix of residential and commercial buildings, a well-maintained green lawn, and a backdrop of trees and hills. The bus, with its bright yellow and blue color scheme, is likely part of a public transportation system, and the overall atmosphere suggests a peaceful, suburban environment.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24976.9, "ram_available_mb": 100795.3, "ram_percent": 19.9}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 24977.1, "ram_available_mb": 100795.0, "ram_percent": 19.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5063.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.19, "peak": 15.52, "min": 14.61}, "VDD_CPU_SOC_MSS": {"avg": 16.24, "peak": 16.54, "min": 15.36}, "VDD_GPU": {"avg": 22.87, "peak": 29.94, "min": 20.88}, "VIN": {"avg": 65.12, "peak": 95.43, "min": 58.95}}, "power_watts_avg": 22.87, "energy_joules_est": 51.47, "sample_count": 17, "duration_seconds": 2.25}, "timestamp": "2026-01-17T16:08:57.601552"}
{"image_index": 491, "image_name": "000000055167.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055167.jpg", "image_width": 640, "image_height": 360, "image_resolution": "640x360", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1633.503, "latencies_ms": [1633.503], "images_per_second": 0.612, "prompt_tokens": 19, "response_tokens_est": 63, "n_tiles": 2, "output_text": "The image features a vibrant yellow and blue bus with the word \"CIVILINK\" on its side. The bus is driving on a road surrounded by a lush green landscape with trees and a hill in the background. The lighting is soft and diffused, suggesting an overcast day with no harsh shadows.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24977.1, "ram_available_mb": 100795.0, "ram_percent": 19.9}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 24976.9, "ram_available_mb": 100795.3, "ram_percent": 19.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5062.6, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.15, "peak": 15.52, "min": 14.61}, "VDD_CPU_SOC_MSS": {"avg": 15.78, "peak": 16.14, "min": 15.35}, "VDD_GPU": {"avg": 23.64, "peak": 30.33, "min": 20.89}, "VIN": {"avg": 67.41, "peak": 107.49, "min": 59.59}}, "power_watts_avg": 23.64, "energy_joules_est": 38.62, "sample_count": 12, "duration_seconds": 1.634}, "timestamp": "2026-01-17T16:08:59.241278"}
{"image_index": 492, "image_name": "000000055299.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055299.jpg", "image_width": 640, "image_height": 429, "image_resolution": "640x429", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1382.37, "latencies_ms": [1382.37], "images_per_second": 0.723, "prompt_tokens": 9, "response_tokens_est": 28, "n_tiles": 6, "output_text": "A solitary pelican perches on a rocky outcrop overlooking a tranquil beach and calm sea, with a cloudy sky overhead.", "error": null, "sys_before": {"cpu_percent": 40.8, "ram_used_mb": 24976.9, "ram_available_mb": 100795.3, "ram_percent": 19.9}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 24977.9, "ram_available_mb": 100794.3, "ram_percent": 19.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.78, "peak": 15.34, "min": 14.2}, "VDD_CPU_SOC_MSS": {"avg": 15.55, "peak": 16.53, "min": 14.96}, "VDD_GPU": {"avg": 32.74, "peak": 39.39, "min": 26.0}, "VIN": {"avg": 75.91, "peak": 115.51, "min": 60.15}}, "power_watts_avg": 32.74, "energy_joules_est": 45.27, "sample_count": 10, "duration_seconds": 1.383}, "timestamp": "2026-01-17T16:09:00.665469"}
{"image_index": 492, "image_name": "000000055299.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055299.jpg", "image_width": 640, "image_height": 429, "image_resolution": "640x429", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1773.146, "latencies_ms": [1773.146], "images_per_second": 0.564, "prompt_tokens": 23, "response_tokens_est": 40, "n_tiles": 6, "output_text": "object: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24977.9, "ram_available_mb": 100794.3, "ram_percent": 19.9}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 24977.9, "ram_available_mb": 100794.3, "ram_percent": 19.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.8, "peak": 15.24, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.32, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 32.27, "peak": 40.95, "min": 24.42}, "VIN": {"avg": 73.47, "peak": 112.04, "min": 55.93}}, "power_watts_avg": 32.27, "energy_joules_est": 57.23, "sample_count": 13, "duration_seconds": 1.773}, "timestamp": "2026-01-17T16:09:02.445002"}
{"image_index": 492, "image_name": "000000055299.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055299.jpg", "image_width": 640, "image_height": 429, "image_resolution": "640x429", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2932.339, "latencies_ms": [2932.339], "images_per_second": 0.341, "prompt_tokens": 27, "response_tokens_est": 81, "n_tiles": 6, "output_text": "The main objects in the image are a rocky outcrop, a seagull, and the ocean. The seagull is perched on the rocky outcrop, which is located in the foreground. The ocean is in the background, stretching out to meet the horizon. The rocky outcrop is near the seagull, providing a vantage point for the bird to observe the surrounding environment.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24977.9, "ram_available_mb": 100794.3, "ram_percent": 19.9}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 24977.9, "ram_available_mb": 100794.3, "ram_percent": 19.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.87, "peak": 15.24, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.51, "peak": 15.75, "min": 14.56}, "VDD_GPU": {"avg": 28.21, "peak": 40.57, "min": 22.85}, "VIN": {"avg": 69.3, "peak": 99.82, "min": 62.07}}, "power_watts_avg": 28.21, "energy_joules_est": 82.73, "sample_count": 23, "duration_seconds": 2.933}, "timestamp": "2026-01-17T16:09:05.383372"}
{"image_index": 492, "image_name": "000000055299.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055299.jpg", "image_width": 640, "image_height": 429, "image_resolution": "640x429", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2170.424, "latencies_ms": [2170.424], "images_per_second": 0.461, "prompt_tokens": 21, "response_tokens_est": 54, "n_tiles": 6, "output_text": "The image depicts a serene beach scene with a rocky shoreline in the foreground. A solitary bird is perched on a rock, overlooking the calm sea and distant mountains. The sky is filled with clouds, suggesting a tranquil and possibly overcast day.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24977.9, "ram_available_mb": 100794.3, "ram_percent": 19.9}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 24978.1, "ram_available_mb": 100794.1, "ram_percent": 19.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.81, "peak": 15.24, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.3, "peak": 15.75, "min": 14.56}, "VDD_GPU": {"avg": 30.21, "peak": 40.57, "min": 23.25}, "VIN": {"avg": 72.93, "peak": 113.64, "min": 61.0}}, "power_watts_avg": 30.21, "energy_joules_est": 65.59, "sample_count": 16, "duration_seconds": 2.171}, "timestamp": "2026-01-17T16:09:07.564494"}
{"image_index": 492, "image_name": "000000055299.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055299.jpg", "image_width": 640, "image_height": 429, "image_resolution": "640x429", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1825.931, "latencies_ms": [1825.931], "images_per_second": 0.548, "prompt_tokens": 19, "response_tokens_est": 42, "n_tiles": 6, "output_text": "The image depicts a serene beach scene with a rocky foreground and a calm sea in the background. The sky is partly cloudy, with sunlight filtering through, creating a soft, diffused light.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24978.1, "ram_available_mb": 100794.1, "ram_percent": 19.9}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 24977.9, "ram_available_mb": 100794.3, "ram_percent": 19.9}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.79, "peak": 15.14, "min": 14.2}, "VDD_CPU_SOC_MSS": {"avg": 15.24, "peak": 15.74, "min": 14.56}, "VDD_GPU": {"avg": 31.54, "peak": 40.57, "min": 24.03}, "VIN": {"avg": 68.17, "peak": 112.06, "min": 46.93}}, "power_watts_avg": 31.54, "energy_joules_est": 57.6, "sample_count": 14, "duration_seconds": 1.826}, "timestamp": "2026-01-17T16:09:09.396830"}
{"image_index": 493, "image_name": "000000055528.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055528.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2559.286, "latencies_ms": [2559.286], "images_per_second": 0.391, "prompt_tokens": 9, "response_tokens_est": 28, "n_tiles": 12, "output_text": "A young man is sitting on a couch, holding a piece of paper in his hand, and wearing a watch on his left wrist.", "error": null, "sys_before": {"cpu_percent": 45.7, "ram_used_mb": 24937.1, "ram_available_mb": 100835.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.6, "ram_used_mb": 24937.5, "ram_available_mb": 100834.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.2, "peak": 16.66, "min": 13.59}, "VDD_CPU_SOC_MSS": {"avg": 15.92, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 35.81, "peak": 43.73, "min": 27.19}, "VIN": {"avg": 80.62, "peak": 114.38, "min": 67.02}}, "power_watts_avg": 35.81, "energy_joules_est": 91.66, "sample_count": 20, "duration_seconds": 2.56}, "timestamp": "2026-01-17T16:09:12.044138"}
{"image_index": 493, "image_name": "000000055528.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055528.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 6007.616, "latencies_ms": [6007.616], "images_per_second": 0.166, "prompt_tokens": 23, "response_tokens_est": 129, "n_tiles": 12, "output_text": "- remote control: 1\n- remote control: 1\n- remote control: 1\n- remote control: 1\n- remote control: 1\n- remote control: 1\n- remote control: 1\n- remote control: 1\n- remote control: 1\n- remote control: 1\n- remote control: 1\n- remote control: 1\n- remote control: 1\n- remote control: 1\n- remote control: 1\n- remote control: 1\n- remote control: 1\n- remote control: 1\n- remote", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24937.5, "ram_available_mb": 100834.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 24938.0, "ram_available_mb": 100834.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.17, "peak": 16.66, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.62, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 30.83, "peak": 45.3, "min": 26.0}, "VIN": {"avg": 73.7, "peak": 133.71, "min": 55.38}}, "power_watts_avg": 30.83, "energy_joules_est": 185.22, "sample_count": 48, "duration_seconds": 6.008}, "timestamp": "2026-01-17T16:09:18.058399"}
{"image_index": 493, "image_name": "000000055528.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055528.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3046.318, "latencies_ms": [3046.318], "images_per_second": 0.328, "prompt_tokens": 27, "response_tokens_est": 41, "n_tiles": 12, "output_text": "The main object in the foreground is a person sitting on a couch. The person is holding a piece of paper in their right hand. In the background, there is a remote control on the couch.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24938.0, "ram_available_mb": 100834.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 24938.5, "ram_available_mb": 100833.7, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.24, "peak": 16.56, "min": 14.12}, "VDD_CPU_SOC_MSS": {"avg": 15.79, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 35.34, "peak": 44.89, "min": 26.79}, "VIN": {"avg": 78.42, "peak": 127.34, "min": 66.66}}, "power_watts_avg": 35.34, "energy_joules_est": 107.67, "sample_count": 24, "duration_seconds": 3.047}, "timestamp": "2026-01-17T16:09:21.111333"}
{"image_index": 493, "image_name": "000000055528.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055528.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 4335.285, "latencies_ms": [4335.285], "images_per_second": 0.231, "prompt_tokens": 21, "response_tokens_est": 79, "n_tiles": 12, "output_text": "The image depicts a young man sitting on a couch, holding a piece of paper in his hand. He is wearing glasses and a plaid shirt, and there is a remote control and a piece of paper on the couch next to him. The setting appears to be indoors, possibly in a living room, and the man seems to be engaged in an activity, possibly reading or writing.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24938.5, "ram_available_mb": 100833.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 24938.8, "ram_available_mb": 100833.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.22, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.8, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 32.65, "peak": 44.89, "min": 26.0}, "VIN": {"avg": 74.81, "peak": 111.99, "min": 65.21}}, "power_watts_avg": 32.65, "energy_joules_est": 141.56, "sample_count": 34, "duration_seconds": 4.336}, "timestamp": "2026-01-17T16:09:25.453504"}
{"image_index": 493, "image_name": "000000055528.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055528.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2717.256, "latencies_ms": [2717.256], "images_per_second": 0.368, "prompt_tokens": 19, "response_tokens_est": 31, "n_tiles": 12, "output_text": "The image features a person sitting on a couch with a gray and black checkered pattern. The lighting is soft and warm, creating a cozy atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24938.8, "ram_available_mb": 100833.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 24938.8, "ram_available_mb": 100833.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.25, "peak": 16.56, "min": 14.12}, "VDD_CPU_SOC_MSS": {"avg": 15.82, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 36.61, "peak": 45.28, "min": 27.19}, "VIN": {"avg": 79.97, "peak": 115.55, "min": 64.64}}, "power_watts_avg": 36.61, "energy_joules_est": 99.5, "sample_count": 21, "duration_seconds": 2.718}, "timestamp": "2026-01-17T16:09:28.177303"}
{"image_index": 494, "image_name": "000000055950.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055950.jpg", "image_width": 426, "image_height": 640, "image_resolution": "426x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1196.604, "latencies_ms": [1196.604], "images_per_second": 0.836, "prompt_tokens": 9, "response_tokens_est": 22, "n_tiles": 6, "output_text": "A tennis player in an orange shirt and black shorts is preparing to hit a tennis ball on a court.", "error": null, "sys_before": {"cpu_percent": 36.5, "ram_used_mb": 24962.9, "ram_available_mb": 100809.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 24963.9, "ram_available_mb": 100808.3, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.61, "peak": 15.14, "min": 13.82}, "VDD_CPU_SOC_MSS": {"avg": 15.35, "peak": 16.14, "min": 14.17}, "VDD_GPU": {"avg": 34.97, "peak": 40.95, "min": 27.97}, "VIN": {"avg": 80.64, "peak": 109.32, "min": 59.71}}, "power_watts_avg": 34.97, "energy_joules_est": 41.86, "sample_count": 9, "duration_seconds": 1.197}, "timestamp": "2026-01-17T16:09:29.441024"}
{"image_index": 494, "image_name": "000000055950.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055950.jpg", "image_width": 426, "image_height": 640, "image_resolution": "426x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1726.446, "latencies_ms": [1726.446], "images_per_second": 0.579, "prompt_tokens": 23, "response_tokens_est": 40, "n_tiles": 6, "output_text": "1. Tennis player\n2. Tennis racket\n3. Tennis ball\n4. Tennis court\n5. Net\n6. Tennis shoes\n7. Tennis shirt\n8. Tennis hat", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24963.9, "ram_available_mb": 100808.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 24963.8, "ram_available_mb": 100808.3, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.99, "peak": 15.44, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.6, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 32.93, "peak": 41.34, "min": 24.82}, "VIN": {"avg": 72.0, "peak": 95.73, "min": 62.61}}, "power_watts_avg": 32.93, "energy_joules_est": 56.87, "sample_count": 13, "duration_seconds": 1.727}, "timestamp": "2026-01-17T16:09:31.173740"}
{"image_index": 494, "image_name": "000000055950.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055950.jpg", "image_width": 426, "image_height": 640, "image_resolution": "426x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1891.9, "latencies_ms": [1891.9], "images_per_second": 0.529, "prompt_tokens": 27, "response_tokens_est": 46, "n_tiles": 6, "output_text": "The main object in the foreground is a tennis player holding a tennis racket. The player is positioned near the net, which is in the foreground. The background features a tennis court with a green surface and white boundary lines.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24963.8, "ram_available_mb": 100808.3, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 24964.1, "ram_available_mb": 100808.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.03, "peak": 15.44, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.61, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 31.85, "peak": 40.57, "min": 24.43}, "VIN": {"avg": 71.29, "peak": 100.2, "min": 61.98}}, "power_watts_avg": 31.85, "energy_joules_est": 60.27, "sample_count": 14, "duration_seconds": 1.892}, "timestamp": "2026-01-17T16:09:33.071673"}
{"image_index": 494, "image_name": "000000055950.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055950.jpg", "image_width": 426, "image_height": 640, "image_resolution": "426x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2644.021, "latencies_ms": [2644.021], "images_per_second": 0.378, "prompt_tokens": 21, "response_tokens_est": 74, "n_tiles": 6, "output_text": "The image captures a moment on an outdoor tennis court during daylight. A male tennis player, dressed in an orange shirt and black shorts, is in the process of hitting a tennis ball. The court is marked with white lines, and the player is positioned near the net, with his racket in hand, indicating he is in the middle of a serve or return.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24964.1, "ram_available_mb": 100808.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 24965.3, "ram_available_mb": 100806.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.09, "peak": 15.44, "min": 14.43}, "VDD_CPU_SOC_MSS": {"avg": 15.69, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 29.43, "peak": 40.95, "min": 23.64}, "VIN": {"avg": 72.99, "peak": 113.8, "min": 58.67}}, "power_watts_avg": 29.43, "energy_joules_est": 77.82, "sample_count": 20, "duration_seconds": 2.644}, "timestamp": "2026-01-17T16:09:35.721853"}
{"image_index": 494, "image_name": "000000055950.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055950.jpg", "image_width": 426, "image_height": 640, "image_resolution": "426x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1938.492, "latencies_ms": [1938.492], "images_per_second": 0.516, "prompt_tokens": 19, "response_tokens_est": 48, "n_tiles": 6, "output_text": "The image shows a tennis player in an orange shirt and black shorts, holding a tennis racket, standing on a green tennis court. The court is well-lit, and the player's attire suggests it is a sunny day.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24965.3, "ram_available_mb": 100806.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 24965.3, "ram_available_mb": 100806.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.06, "peak": 15.44, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.62, "peak": 16.14, "min": 14.56}, "VDD_GPU": {"avg": 31.01, "peak": 40.56, "min": 24.04}, "VIN": {"avg": 72.19, "peak": 90.99, "min": 61.13}}, "power_watts_avg": 31.01, "energy_joules_est": 60.13, "sample_count": 15, "duration_seconds": 1.939}, "timestamp": "2026-01-17T16:09:37.667104"}
{"image_index": 495, "image_name": "000000056127.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000056127.jpg", "image_width": 580, "image_height": 640, "image_resolution": "580x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1232.598, "latencies_ms": [1232.598], "images_per_second": 0.811, "prompt_tokens": 9, "response_tokens_est": 53, "n_tiles": 1, "output_text": "The image depicts a well-organized and functional kitchen area with a yellowish hue, featuring a variety of appliances and storage solutions, including cabinets, a stove, a sink, a refrigerator, and a wooden counter with a sink and a faucet.", "error": null, "sys_before": {"cpu_percent": 23.3, "ram_used_mb": 24965.3, "ram_available_mb": 100806.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24965.5, "ram_available_mb": 100806.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 4845.7, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.35, "peak": 15.72, "min": 14.61}, "VDD_CPU_SOC_MSS": {"avg": 16.23, "peak": 16.54, "min": 15.35}, "VDD_GPU": {"avg": 22.33, "peak": 26.01, "min": 20.48}, "VIN": {"avg": 63.61, "peak": 66.61, "min": 59.53}}, "power_watts_avg": 22.33, "energy_joules_est": 27.53, "sample_count": 9, "duration_seconds": 1.233}, "timestamp": "2026-01-17T16:09:38.925144"}
{"image_index": 495, "image_name": "000000056127.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000056127.jpg", "image_width": 580, "image_height": 640, "image_resolution": "580x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2874.784, "latencies_ms": [2874.784], "images_per_second": 0.348, "prompt_tokens": 23, "response_tokens_est": 128, "n_tiles": 1, "output_text": "- table: 1\n- chair: 1\n- stove: 1\n- sink: 1\n- refrigerator: 1\n- cabinet: 1\n- shelf: 1\n- drawer: 1\n- faucet: 1\n- tablecloth: 1\n- rug: 1\n- light fixture: 1\n- door: 1\n- window: 1\n- vent: 1\n- ceiling: 1\n- pipe: 1\n- cabinet door: 1\n- drawer front: 1\n- cabinet door handle: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24965.5, "ram_available_mb": 100806.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 24965.5, "ram_available_mb": 100806.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 4856.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.7, "peak": 15.93, "min": 15.42}, "VDD_CPU_SOC_MSS": {"avg": 16.57, "peak": 16.94, "min": 16.14}, "VDD_GPU": {"avg": 20.64, "peak": 24.03, "min": 19.7}, "VIN": {"avg": 64.17, "peak": 69.8, "min": 57.68}}, "power_watts_avg": 20.64, "energy_joules_est": 59.34, "sample_count": 23, "duration_seconds": 2.875}, "timestamp": "2026-01-17T16:09:41.805772"}
{"image_index": 495, "image_name": "000000056127.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000056127.jpg", "image_width": 580, "image_height": 640, "image_resolution": "580x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2483.107, "latencies_ms": [2483.107], "images_per_second": 0.403, "prompt_tokens": 27, "response_tokens_est": 110, "n_tiles": 1, "output_text": "The main objects in the image are located in the foreground, with the kitchen area being the most prominent. The kitchen features a wooden countertop, a sink, a stove, and various cabinets. The stove is positioned to the left of the sink, and the countertop extends towards the right. The sink is located near the stove, and the cabinets are placed on either side of the countertop. The background includes a wooden door and a wooden chair, while the far right side of the image shows a window with a view of the outside.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24965.5, "ram_available_mb": 100806.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 24965.3, "ram_available_mb": 100806.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 4859.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.67, "peak": 15.93, "min": 15.22}, "VDD_CPU_SOC_MSS": {"avg": 16.51, "peak": 16.93, "min": 16.14}, "VDD_GPU": {"avg": 20.7, "peak": 24.03, "min": 19.7}, "VIN": {"avg": 62.78, "peak": 69.35, "min": 56.78}}, "power_watts_avg": 20.7, "energy_joules_est": 51.41, "sample_count": 19, "duration_seconds": 2.483}, "timestamp": "2026-01-17T16:09:44.294497"}
{"image_index": 495, "image_name": "000000056127.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000056127.jpg", "image_width": 580, "image_height": 640, "image_resolution": "580x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2107.617, "latencies_ms": [2107.617], "images_per_second": 0.474, "prompt_tokens": 21, "response_tokens_est": 93, "n_tiles": 1, "output_text": "The image depicts the interior of a small, well-organized kitchen aboard a ship. The kitchen features wooden cabinets with glass doors, a black countertop, a sink with a faucet, a stove with a pot on it, and a refrigerator. The walls are painted in a light yellow color, and there are various kitchen utensils and appliances neatly arranged. The overall setting suggests a functional and compact space designed for cooking and food preparation.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24965.3, "ram_available_mb": 100806.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24965.3, "ram_available_mb": 100806.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 4854.7, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.63, "peak": 15.72, "min": 15.52}, "VDD_CPU_SOC_MSS": {"avg": 16.64, "peak": 16.94, "min": 16.14}, "VDD_GPU": {"avg": 20.71, "peak": 23.64, "min": 19.7}, "VIN": {"avg": 63.25, "peak": 73.62, "min": 50.73}}, "power_watts_avg": 20.71, "energy_joules_est": 43.66, "sample_count": 16, "duration_seconds": 2.108}, "timestamp": "2026-01-17T16:09:46.408108"}
{"image_index": 495, "image_name": "000000056127.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000056127.jpg", "image_width": 580, "image_height": 640, "image_resolution": "580x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 938.484, "latencies_ms": [938.484], "images_per_second": 1.066, "prompt_tokens": 19, "response_tokens_est": 40, "n_tiles": 1, "output_text": "The image depicts a well-lit, yellowish kitchen with wooden cabinets and a wooden countertop. The lighting is primarily from fluorescent lights, and the overall atmosphere is warm and inviting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24965.3, "ram_available_mb": 100806.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24965.3, "ram_available_mb": 100806.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 4853.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.45, "peak": 15.62, "min": 15.22}, "VDD_CPU_SOC_MSS": {"avg": 16.54, "peak": 16.93, "min": 16.14}, "VDD_GPU": {"avg": 21.73, "peak": 24.04, "min": 20.49}, "VIN": {"avg": 63.07, "peak": 65.62, "min": 60.99}}, "power_watts_avg": 21.73, "energy_joules_est": 20.4, "sample_count": 7, "duration_seconds": 0.939}, "timestamp": "2026-01-17T16:09:47.351967"}
{"image_index": 496, "image_name": "000000056288.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000056288.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2861.861, "latencies_ms": [2861.861], "images_per_second": 0.349, "prompt_tokens": 9, "response_tokens_est": 37, "n_tiles": 12, "output_text": "The image shows a sandwich with various ingredients, including lettuce, tomato, cucumber, and ham, all placed on a white paper plate, which is placed on a table.", "error": null, "sys_before": {"cpu_percent": 46.2, "ram_used_mb": 24932.4, "ram_available_mb": 100839.8, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 24934.3, "ram_available_mb": 100837.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.24, "peak": 16.66, "min": 13.9}, "VDD_CPU_SOC_MSS": {"avg": 15.96, "peak": 17.71, "min": 14.96}, "VDD_GPU": {"avg": 34.61, "peak": 43.33, "min": 26.39}, "VIN": {"avg": 79.61, "peak": 115.16, "min": 65.36}}, "power_watts_avg": 34.61, "energy_joules_est": 99.06, "sample_count": 22, "duration_seconds": 2.862}, "timestamp": "2026-01-17T16:09:50.303392"}
{"image_index": 496, "image_name": "000000056288.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000056288.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2622.981, "latencies_ms": [2622.981], "images_per_second": 0.381, "prompt_tokens": 23, "response_tokens_est": 28, "n_tiles": 12, "output_text": "- sandwich\n- lettuce\n- tomato\n- cucumber\n- carrot\n- pickles\n- onion\n- ham", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24934.3, "ram_available_mb": 100837.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.6, "ram_used_mb": 24935.5, "ram_available_mb": 100836.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.24, "peak": 16.56, "min": 14.12}, "VDD_CPU_SOC_MSS": {"avg": 15.84, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 36.84, "peak": 44.89, "min": 27.57}, "VIN": {"avg": 75.76, "peak": 110.17, "min": 56.21}}, "power_watts_avg": 36.84, "energy_joules_est": 96.65, "sample_count": 20, "duration_seconds": 2.623}, "timestamp": "2026-01-17T16:09:52.932856"}
{"image_index": 496, "image_name": "000000056288.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000056288.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 4009.875, "latencies_ms": [4009.875], "images_per_second": 0.249, "prompt_tokens": 27, "response_tokens_est": 69, "n_tiles": 12, "output_text": "The main object in the foreground is a sandwich, which is placed on a white paper plate. The sandwich is sandwiched between two white bread slices. In the background, there is a computer monitor displaying a white screen. The sandwich is positioned near the center of the image, with the computer monitor slightly to the right and behind the sandwich.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24935.5, "ram_available_mb": 100836.6, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 24935.8, "ram_available_mb": 100836.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.19, "peak": 16.56, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.73, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 33.36, "peak": 45.68, "min": 26.0}, "VIN": {"avg": 77.01, "peak": 111.97, "min": 65.56}}, "power_watts_avg": 33.36, "energy_joules_est": 133.79, "sample_count": 31, "duration_seconds": 4.01}, "timestamp": "2026-01-17T16:09:56.949243"}
{"image_index": 496, "image_name": "000000056288.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000056288.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 4132.917, "latencies_ms": [4132.917], "images_per_second": 0.242, "prompt_tokens": 21, "response_tokens_est": 72, "n_tiles": 12, "output_text": "The image depicts a sandwich placed on a white paper plate, which is situated on a wooden table. The sandwich is filled with various ingredients, including lettuce, tomato, cucumber, and possibly ham or turkey. In the background, there is a computer monitor displaying some content, suggesting that the setting might be a home office or a study area.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24935.8, "ram_available_mb": 100836.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 24935.8, "ram_available_mb": 100836.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.12, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.69, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 32.86, "peak": 44.91, "min": 25.6}, "VIN": {"avg": 76.6, "peak": 127.2, "min": 60.88}}, "power_watts_avg": 32.86, "energy_joules_est": 135.82, "sample_count": 32, "duration_seconds": 4.133}, "timestamp": "2026-01-17T16:10:01.088937"}
{"image_index": 496, "image_name": "000000056288.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000056288.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3899.358, "latencies_ms": [3899.358], "images_per_second": 0.256, "prompt_tokens": 19, "response_tokens_est": 65, "n_tiles": 12, "output_text": "The image features a sandwich with a variety of ingredients, including lettuce, tomato, cucumber, and ham, all neatly arranged on a white paper plate. The lighting is soft and natural, casting a warm glow on the food, while the background is blurred, emphasizing the sandwich as the focal point.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24935.8, "ram_available_mb": 100836.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 24935.8, "ram_available_mb": 100836.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.89, "peak": 16.36, "min": 13.92}, "VDD_CPU_SOC_MSS": {"avg": 15.35, "peak": 17.32, "min": 14.17}, "VDD_GPU": {"avg": 33.11, "peak": 44.51, "min": 25.6}, "VIN": {"avg": 76.46, "peak": 117.0, "min": 61.12}}, "power_watts_avg": 33.11, "energy_joules_est": 129.12, "sample_count": 30, "duration_seconds": 3.9}, "timestamp": "2026-01-17T16:10:04.994699"}
{"image_index": 497, "image_name": "000000056344.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000056344.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2890.799, "latencies_ms": [2890.799], "images_per_second": 0.346, "prompt_tokens": 9, "response_tokens_est": 38, "n_tiles": 12, "output_text": "The image shows a cluttered desk with various electronic devices, including a computer monitor, a keyboard, a mouse, a smartphone, and a tablet, all connected to a power source.", "error": null, "sys_before": {"cpu_percent": 45.2, "ram_used_mb": 24944.0, "ram_available_mb": 100828.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 24936.8, "ram_available_mb": 100835.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.11, "peak": 16.76, "min": 12.98}, "VDD_CPU_SOC_MSS": {"avg": 15.75, "peak": 17.71, "min": 13.78}, "VDD_GPU": {"avg": 35.16, "peak": 44.1, "min": 26.8}, "VIN": {"avg": 75.01, "peak": 112.84, "min": 62.13}}, "power_watts_avg": 35.16, "energy_joules_est": 101.65, "sample_count": 22, "duration_seconds": 2.891}, "timestamp": "2026-01-17T16:10:07.996644"}
{"image_index": 497, "image_name": "000000056344.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000056344.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 6006.824, "latencies_ms": [6006.824], "images_per_second": 0.166, "prompt_tokens": 23, "response_tokens_est": 129, "n_tiles": 12, "output_text": "- computer monitor: 2\n- computer monitor: 2\n- computer monitor: 2\n- computer monitor: 2\n- computer monitor: 2\n- computer monitor: 2\n- computer monitor: 2\n- computer monitor: 2\n- computer monitor: 2\n- computer monitor: 2\n- computer monitor: 2\n- computer monitor: 2\n- computer monitor: 2\n- computer monitor: 2\n- computer monitor: 2\n- computer monitor: 2\n- computer monitor: 2\n- computer monitor: 2\n- computer", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24936.8, "ram_available_mb": 100835.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 24930.4, "ram_available_mb": 100841.7, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.17, "peak": 16.56, "min": 14.12}, "VDD_CPU_SOC_MSS": {"avg": 15.76, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 30.76, "peak": 45.69, "min": 26.0}, "VIN": {"avg": 71.59, "peak": 94.38, "min": 60.5}}, "power_watts_avg": 30.76, "energy_joules_est": 184.78, "sample_count": 48, "duration_seconds": 6.007}, "timestamp": "2026-01-17T16:10:14.010273"}
{"image_index": 497, "image_name": "000000056344.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000056344.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 5322.019, "latencies_ms": [5322.019], "images_per_second": 0.188, "prompt_tokens": 27, "response_tokens_est": 108, "n_tiles": 12, "output_text": "The main objects in the image are arranged in a somewhat scattered manner on a desk. The leftmost object, a camera, is positioned near the left edge of the desk. The central object, a computer monitor, is placed in the middle of the desk. The rightmost object, a tablet, is placed on the right edge of the desk. The background objects, such as the keyboard and mouse, are placed in front of the computer monitor. The foreground objects, like the camera and tablet, are positioned closer to the camera.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24930.4, "ram_available_mb": 100841.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 24931.1, "ram_available_mb": 100841.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.19, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.77, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 31.43, "peak": 45.68, "min": 26.0}, "VIN": {"avg": 73.86, "peak": 117.57, "min": 61.8}}, "power_watts_avg": 31.43, "energy_joules_est": 167.28, "sample_count": 42, "duration_seconds": 5.322}, "timestamp": "2026-01-17T16:10:19.338771"}
{"image_index": 497, "image_name": "000000056344.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000056344.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3890.822, "latencies_ms": [3890.822], "images_per_second": 0.257, "prompt_tokens": 21, "response_tokens_est": 66, "n_tiles": 12, "output_text": "The image depicts a cluttered desk with various electronic devices and gadgets. The desk is cluttered with a white keyboard, a white mouse, a smartphone, an iPod, a tablet, and a laptop. The setting appears to be a personal workspace, possibly in a home or office, with a dimly lit environment.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24931.1, "ram_available_mb": 100841.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 24931.1, "ram_available_mb": 100841.1, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.24, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.84, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 33.48, "peak": 44.89, "min": 26.0}, "VIN": {"avg": 76.42, "peak": 118.31, "min": 62.99}}, "power_watts_avg": 33.48, "energy_joules_est": 130.28, "sample_count": 30, "duration_seconds": 3.891}, "timestamp": "2026-01-17T16:10:23.236092"}
{"image_index": 497, "image_name": "000000056344.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000056344.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3424.07, "latencies_ms": [3424.07], "images_per_second": 0.292, "prompt_tokens": 19, "response_tokens_est": 52, "n_tiles": 12, "output_text": "The image shows a cluttered desk with various electronic devices, including a computer monitor, a tablet, a smartphone, a keyboard, and a mouse. The desk is illuminated by a dim, warm light, creating a cozy and somewhat cluttered atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24931.1, "ram_available_mb": 100841.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 24931.6, "ram_available_mb": 100840.6, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.23, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.82, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 34.59, "peak": 44.91, "min": 26.01}, "VIN": {"avg": 80.29, "peak": 119.67, "min": 53.74}}, "power_watts_avg": 34.59, "energy_joules_est": 118.45, "sample_count": 26, "duration_seconds": 3.424}, "timestamp": "2026-01-17T16:10:26.667959"}
{"image_index": 498, "image_name": "000000056350.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000056350.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 715.786, "latencies_ms": [715.786], "images_per_second": 1.397, "prompt_tokens": 9, "response_tokens_est": 29, "n_tiles": 1, "output_text": "The image depicts a bathroom with a large, colorful mural of people in various poses and expressions, creating a lively and dynamic atmosphere.", "error": null, "sys_before": {"cpu_percent": 30.8, "ram_used_mb": 24935.3, "ram_available_mb": 100836.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24936.3, "ram_available_mb": 100835.9, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 4845.7, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.01, "peak": 15.52, "min": 14.4}, "VDD_CPU_SOC_MSS": {"avg": 15.67, "peak": 16.14, "min": 14.96}, "VDD_GPU": {"avg": 24.2, "peak": 27.19, "min": 22.07}, "VIN": {"avg": 63.01, "peak": 67.39, "min": 59.12}}, "power_watts_avg": 24.2, "energy_joules_est": 17.33, "sample_count": 5, "duration_seconds": 0.716}, "timestamp": "2026-01-17T16:10:27.413469"}
{"image_index": 498, "image_name": "000000056350.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000056350.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 669.265, "latencies_ms": [669.265], "images_per_second": 1.494, "prompt_tokens": 23, "response_tokens_est": 28, "n_tiles": 1, "output_text": "1. Toilet\n2. People\n3. White tiles\n4. White wall\n5. Ceiling\n6. Floor", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24936.3, "ram_available_mb": 100835.9, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 24936.5, "ram_available_mb": 100835.7, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 4856.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.44, "peak": 15.62, "min": 15.22}, "VDD_CPU_SOC_MSS": {"avg": 16.14, "peak": 16.54, "min": 15.74}, "VDD_GPU": {"avg": 23.35, "peak": 24.83, "min": 22.07}, "VIN": {"avg": 63.08, "peak": 68.3, "min": 56.58}}, "power_watts_avg": 23.35, "energy_joules_est": 15.63, "sample_count": 4, "duration_seconds": 0.67}, "timestamp": "2026-01-17T16:10:28.088368"}
{"image_index": 498, "image_name": "000000056350.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000056350.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1271.282, "latencies_ms": [1271.282], "images_per_second": 0.787, "prompt_tokens": 27, "response_tokens_est": 56, "n_tiles": 1, "output_text": "The main object in the image is a toilet, which is located in the background. The toilet is situated near the ceiling, slightly to the right. The foreground features a group of people lying on their backs on the floor, with the toilet being the closest object to them.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24936.5, "ram_available_mb": 100835.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24937.5, "ram_available_mb": 100834.7, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 4859.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.51, "peak": 15.72, "min": 15.11}, "VDD_CPU_SOC_MSS": {"avg": 16.62, "peak": 16.93, "min": 16.14}, "VDD_GPU": {"avg": 22.02, "peak": 24.83, "min": 20.49}, "VIN": {"avg": 64.53, "peak": 70.03, "min": 61.27}}, "power_watts_avg": 22.02, "energy_joules_est": 28.0, "sample_count": 9, "duration_seconds": 1.272}, "timestamp": "2026-01-17T16:10:29.368961"}
{"image_index": 498, "image_name": "000000056350.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000056350.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 957.064, "latencies_ms": [957.064], "images_per_second": 1.045, "prompt_tokens": 21, "response_tokens_est": 41, "n_tiles": 1, "output_text": "The image depicts a bathroom with a toilet and a tiled floor. A person is lying on their back on the tiled floor, seemingly enjoying the view of the toilet and the surrounding area.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24937.5, "ram_available_mb": 100834.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24937.5, "ram_available_mb": 100834.7, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 4854.7, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.38, "peak": 15.62, "min": 15.01}, "VDD_CPU_SOC_MSS": {"avg": 16.43, "peak": 16.54, "min": 16.14}, "VDD_GPU": {"avg": 22.07, "peak": 24.44, "min": 20.49}, "VIN": {"avg": 62.39, "peak": 65.47, "min": 58.27}}, "power_watts_avg": 22.07, "energy_joules_est": 21.13, "sample_count": 7, "duration_seconds": 0.957}, "timestamp": "2026-01-17T16:10:30.332027"}
{"image_index": 498, "image_name": "000000056350.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000056350.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 865.956, "latencies_ms": [865.956], "images_per_second": 1.155, "prompt_tokens": 19, "response_tokens_est": 37, "n_tiles": 1, "output_text": "The image features a bathroom with white tiled walls and floor, illuminated by bright lighting. The tiles are glossy, reflecting the light and creating a clean, modern aesthetic.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 24937.5, "ram_available_mb": 100834.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 24939.0, "ram_available_mb": 100833.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 4853.2, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.47, "peak": 15.62, "min": 15.22}, "VDD_CPU_SOC_MSS": {"avg": 16.41, "peak": 16.54, "min": 16.14}, "VDD_GPU": {"avg": 22.32, "peak": 24.42, "min": 20.88}, "VIN": {"avg": 62.81, "peak": 67.16, "min": 57.34}}, "power_watts_avg": 22.32, "energy_joules_est": 19.33, "sample_count": 6, "duration_seconds": 0.866}, "timestamp": "2026-01-17T16:10:31.203873"}
{"image_index": 499, "image_name": "000000056545.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000056545.jpg", "image_width": 505, "image_height": 640, "image_resolution": "505x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 3034.615, "latencies_ms": [3034.615], "images_per_second": 0.33, "prompt_tokens": 9, "response_tokens_est": 42, "n_tiles": 12, "output_text": "The image features a bird perched on a branch, with its feathers appearing to be a mix of grey and brown tones, and the background is a blurred green, suggesting a natural, outdoor setting.", "error": null, "sys_before": {"cpu_percent": 41.0, "ram_used_mb": 24924.0, "ram_available_mb": 100848.1, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 24925.5, "ram_available_mb": 100846.7, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.27, "peak": 16.76, "min": 14.1}, "VDD_CPU_SOC_MSS": {"avg": 15.92, "peak": 17.71, "min": 14.96}, "VDD_GPU": {"avg": 34.34, "peak": 43.31, "min": 26.4}, "VIN": {"avg": 81.71, "peak": 134.37, "min": 55.4}}, "power_watts_avg": 34.34, "energy_joules_est": 104.22, "sample_count": 23, "duration_seconds": 3.035}, "timestamp": "2026-01-17T16:10:34.317770"}
{"image_index": 499, "image_name": "000000056545.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000056545.jpg", "image_width": 505, "image_height": 640, "image_resolution": "505x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2044.159, "latencies_ms": [2044.159], "images_per_second": 0.489, "prompt_tokens": 23, "response_tokens_est": 11, "n_tiles": 12, "output_text": "bird: 1\ntree branch: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24925.5, "ram_available_mb": 100846.7, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 1.9, "ram_used_mb": 24926.7, "ram_available_mb": 100845.4, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.3, "peak": 16.56, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.84, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 39.17, "peak": 45.69, "min": 31.12}, "VIN": {"avg": 82.56, "peak": 113.51, "min": 67.62}}, "power_watts_avg": 39.17, "energy_joules_est": 80.08, "sample_count": 16, "duration_seconds": 2.044}, "timestamp": "2026-01-17T16:10:36.368263"}
{"image_index": 499, "image_name": "000000056545.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000056545.jpg", "image_width": 505, "image_height": 640, "image_resolution": "505x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3504.05, "latencies_ms": [3504.05], "images_per_second": 0.285, "prompt_tokens": 27, "response_tokens_est": 54, "n_tiles": 12, "output_text": "The main object in the foreground is a bird perched on a tree branch. The bird is positioned near the center of the image, with its head turned slightly towards the camera. The background is a blurred green, indicating a dense forest or foliage.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 24926.7, "ram_available_mb": 100845.4, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 24927.2, "ram_available_mb": 100845.0, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.22, "peak": 16.56, "min": 14.32}, "VDD_CPU_SOC_MSS": {"avg": 15.73, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 35.05, "peak": 46.86, "min": 26.0}, "VIN": {"avg": 78.03, "peak": 136.0, "min": 66.86}}, "power_watts_avg": 35.05, "energy_joules_est": 122.83, "sample_count": 27, "duration_seconds": 3.504}, "timestamp": "2026-01-17T16:10:39.879567"}
{"image_index": 499, "image_name": "000000056545.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000056545.jpg", "image_width": 505, "image_height": 640, "image_resolution": "505x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3264.688, "latencies_ms": [3264.688], "images_per_second": 0.306, "prompt_tokens": 21, "response_tokens_est": 47, "n_tiles": 12, "output_text": "The image captures a bird perched on a tree branch, with a blurred background of green foliage. The bird's feathers are a mix of gray and brown, and it appears to be looking slightly to its right.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 24927.2, "ram_available_mb": 100845.0, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 24927.9, "ram_available_mb": 100844.2, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.23, "peak": 16.66, "min": 14.22}, "VDD_CPU_SOC_MSS": {"avg": 15.79, "peak": 17.71, "min": 14.56}, "VDD_GPU": {"avg": 34.85, "peak": 44.89, "min": 26.0}, "VIN": {"avg": 81.16, "peak": 118.04, "min": 62.56}}, "power_watts_avg": 34.85, "energy_joules_est": 113.79, "sample_count": 25, "duration_seconds": 3.265}, "timestamp": "2026-01-17T16:10:43.151025"}
{"image_index": 499, "image_name": "000000056545.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000056545.jpg", "image_width": 505, "image_height": 640, "image_resolution": "505x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3130.497, "latencies_ms": [3130.497], "images_per_second": 0.319, "prompt_tokens": 19, "response_tokens_est": 43, "n_tiles": 12, "output_text": "The bird in the image has a grayish-brown plumage with a slightly darker back and lighter underparts. The lighting is soft and diffused, creating a gentle bokeh effect in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 24927.9, "ram_available_mb": 100844.2, "ram_percent": 19.8}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 24928.7, "ram_available_mb": 100843.5, "ram_percent": 19.8}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 12966.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 12966.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.2, "peak": 16.56, "min": 14.12}, "VDD_CPU_SOC_MSS": {"avg": 15.75, "peak": 17.32, "min": 14.56}, "VDD_GPU": {"avg": 35.37, "peak": 45.69, "min": 26.39}, "VIN": {"avg": 78.46, "peak": 115.25, "min": 57.02}}, "power_watts_avg": 35.37, "energy_joules_est": 110.74, "sample_count": 24, "duration_seconds": 3.131}, "timestamp": "2026-01-17T16:10:46.288705"}
