{"image_index": 0, "image_name": "000000000139.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000139.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1625.699, "latencies_ms": [1625.699], "images_per_second": 0.615, "prompt_tokens": 9, "response_tokens_est": 32, "n_tiles": 6, "output_text": "The image depicts a brightly lit, modern living room with a yellow wall, a wooden floor, and a dining area with a table and chairs.", "error": null, "sys_before": {"cpu_percent": 5.8, "ram_used_mb": 26376.6, "ram_available_mb": 99395.6, "ram_percent": 21.0}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 26380.5, "ram_available_mb": 99391.6, "ram_percent": 21.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 7584.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 7584.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.54, "peak": 14.1, "min": 12.88}, "VDD_GPU": {"avg": 28.38, "peak": 35.07, "min": 22.47}, "VIN": {"avg": 66.05, "peak": 92.5, "min": 58.85}, "VDD_CPU_SOC_MSS": {"avg": 13.82, "peak": 14.19, "min": 13.39}}, "power_watts_avg": 28.38, "energy_joules_est": 46.15, "sample_count": 12, "duration_seconds": 1.626}, "timestamp": "2026-01-16T15:07:16.384631"}
{"image_index": 0, "image_name": "000000000139.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000139.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 4641.339, "latencies_ms": [4641.339], "images_per_second": 0.215, "prompt_tokens": 23, "response_tokens_est": 129, "n_tiles": 6, "output_text": "- table: 1\n- vase: 1\n- chair: 3\n- table: 1\n- vase: 1\n- table: 1\n- vase: 1\n- table: 1\n- vase: 1\n- table: 1\n- vase: 1\n- table: 1\n- vase: 1\n- table: 1\n- vase: 1\n- table: 1\n- vase: 1\n- table: 1\n- vase: 1\n- table: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26380.5, "ram_available_mb": 99391.6, "ram_percent": 21.0}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 26383.3, "ram_available_mb": 99388.9, "ram_percent": 21.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 7584.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 7584.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.04, "peak": 14.3, "min": 13.31}, "VDD_GPU": {"avg": 24.03, "peak": 35.48, "min": 20.89}, "VIN": {"avg": 63.02, "peak": 107.42, "min": 57.58}, "VDD_CPU_SOC_MSS": {"avg": 14.14, "peak": 14.58, "min": 13.79}}, "power_watts_avg": 24.03, "energy_joules_est": 111.54, "sample_count": 37, "duration_seconds": 4.642}, "timestamp": "2026-01-16T15:07:21.033747"}
{"image_index": 0, "image_name": "000000000139.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000139.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3462.995, "latencies_ms": [3462.995], "images_per_second": 0.289, "prompt_tokens": 27, "response_tokens_est": 90, "n_tiles": 6, "output_text": "The main objects in the image are arranged in a way that showcases a harmonious spatial relationship. The television is positioned on the left side of the room, with the dining table and chairs in the foreground. The kitchen area is visible in the background, with the refrigerator and cabinets. The vase with the pink flower is placed on the table, adding a touch of color to the room. The overall layout creates a balanced and inviting space.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26383.3, "ram_available_mb": 99388.9, "ram_percent": 21.0}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 26383.9, "ram_available_mb": 99388.2, "ram_percent": 21.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 7584.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 7584.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.04, "peak": 14.3, "min": 13.41}, "VDD_GPU": {"avg": 24.94, "peak": 34.28, "min": 20.89}, "VIN": {"avg": 65.33, "peak": 109.95, "min": 58.07}, "VDD_CPU_SOC_MSS": {"avg": 14.09, "peak": 14.19, "min": 13.79}}, "power_watts_avg": 24.94, "energy_joules_est": 86.38, "sample_count": 27, "duration_seconds": 3.463}, "timestamp": "2026-01-16T15:07:24.507259"}
{"image_index": 0, "image_name": "000000000139.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000139.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3401.515, "latencies_ms": [3401.515], "images_per_second": 0.294, "prompt_tokens": 21, "response_tokens_est": 88, "n_tiles": 6, "output_text": "The image depicts a brightly lit, modern living room with a warm yellow hue. The room features a wooden floor, a large dining table with chairs, a television on a stand, a vase with a pink flower on a table, and a green vase on a table. In the background, there is a kitchen area with cabinets and a refrigerator. A person is visible in the kitchen, seemingly engaged in an activity.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 26383.9, "ram_available_mb": 99388.2, "ram_percent": 21.0}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 26386.1, "ram_available_mb": 99386.0, "ram_percent": 21.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 7584.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 7584.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.01, "peak": 14.3, "min": 13.39}, "VDD_GPU": {"avg": 24.94, "peak": 35.47, "min": 20.89}, "VIN": {"avg": 65.52, "peak": 102.41, "min": 51.79}, "VDD_CPU_SOC_MSS": {"avg": 14.21, "peak": 14.58, "min": 13.79}}, "power_watts_avg": 24.94, "energy_joules_est": 84.84, "sample_count": 26, "duration_seconds": 3.402}, "timestamp": "2026-01-16T15:07:27.915555"}
{"image_index": 0, "image_name": "000000000139.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000139.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2688.113, "latencies_ms": [2688.113], "images_per_second": 0.372, "prompt_tokens": 19, "response_tokens_est": 65, "n_tiles": 6, "output_text": "The room is brightly lit with natural light streaming in through large windows, creating a warm and inviting atmosphere. The walls are painted in a vibrant yellow, complemented by a green accent wall. The wooden floor adds warmth to the space, and the furniture is made of a combination of materials, including wood and metal.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 26386.1, "ram_available_mb": 99386.0, "ram_percent": 21.0}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 26398.4, "ram_available_mb": 99373.7, "ram_percent": 21.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 7584.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 7584.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.0, "peak": 14.3, "min": 13.41}, "VDD_GPU": {"avg": 25.77, "peak": 35.07, "min": 21.29}, "VIN": {"avg": 61.5, "peak": 71.14, "min": 54.6}, "VDD_CPU_SOC_MSS": {"avg": 14.15, "peak": 14.58, "min": 13.79}}, "power_watts_avg": 25.77, "energy_joules_est": 69.29, "sample_count": 21, "duration_seconds": 2.689}, "timestamp": "2026-01-16T15:07:30.609806"}
{"image_index": 1, "image_name": "000000000285.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000285.jpg", "image_width": 586, "image_height": 640, "image_resolution": "586x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1043.069, "latencies_ms": [1043.069], "images_per_second": 0.959, "prompt_tokens": 9, "response_tokens_est": 36, "n_tiles": 1, "output_text": "The image features a close-up of a brown bear with a soft, fluffy fur coat, sitting on a grassy surface, showcasing its distinctive facial features and ears.", "error": null, "sys_before": {"cpu_percent": 23.5, "ram_used_mb": 26398.4, "ram_available_mb": 99373.7, "ram_percent": 21.0}, "sys_after": {"cpu_percent": 6.8, "ram_used_mb": 26405.1, "ram_available_mb": 99367.0, "ram_percent": 21.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 7682.0, "gpu_max_mem_alloc_mb": 4845.7, "gpu_max_mem_reserved_mb": 7682.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.05, "peak": 14.4, "min": 13.39}, "VDD_GPU": {"avg": 20.06, "peak": 23.65, "min": 18.14}, "VIN": {"avg": 56.66, "peak": 59.04, "min": 54.62}, "VDD_CPU_SOC_MSS": {"avg": 14.33, "peak": 14.58, "min": 13.79}}, "power_watts_avg": 20.06, "energy_joules_est": 20.93, "sample_count": 8, "duration_seconds": 1.044}, "timestamp": "2026-01-16T15:07:31.680264"}
{"image_index": 1, "image_name": "000000000285.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000285.jpg", "image_width": 586, "image_height": 640, "image_resolution": "586x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 368.383, "latencies_ms": [368.383], "images_per_second": 2.715, "prompt_tokens": 23, "response_tokens_est": 11, "n_tiles": 1, "output_text": "bear: 1\ngrass: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26405.1, "ram_available_mb": 99367.0, "ram_percent": 21.0}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 26405.1, "ram_available_mb": 99367.0, "ram_percent": 21.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 7682.0, "gpu_max_mem_alloc_mb": 4856.2, "gpu_max_mem_reserved_mb": 7682.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.05, "peak": 14.1, "min": 14.0}, "VDD_GPU": {"avg": 21.88, "peak": 22.47, "min": 21.29}, "VIN": {"avg": 58.37, "peak": 58.85, "min": 57.89}, "VDD_CPU_SOC_MSS": {"avg": 14.38, "peak": 14.57, "min": 14.18}}, "power_watts_avg": 21.88, "energy_joules_est": 8.07, "sample_count": 2, "duration_seconds": 0.369}, "timestamp": "2026-01-16T15:07:32.054843"}
{"image_index": 1, "image_name": "000000000285.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000285.jpg", "image_width": 586, "image_height": 640, "image_resolution": "586x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1328.26, "latencies_ms": [1328.26], "images_per_second": 0.753, "prompt_tokens": 27, "response_tokens_est": 48, "n_tiles": 1, "output_text": "The main object in the image is a brown bear. The bear is positioned in the foreground, with its head and upper body visible. The background consists of a grassy area, indicating that the bear is likely in a natural habitat.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26405.1, "ram_available_mb": 99367.0, "ram_percent": 21.0}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 26405.1, "ram_available_mb": 99367.0, "ram_percent": 21.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 7682.0, "gpu_max_mem_alloc_mb": 4859.2, "gpu_max_mem_reserved_mb": 7682.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.28, "peak": 14.51, "min": 13.79}, "VDD_GPU": {"avg": 19.99, "peak": 24.05, "min": 18.14}, "VIN": {"avg": 57.1, "peak": 62.9, "min": 52.44}, "VDD_CPU_SOC_MSS": {"avg": 14.46, "peak": 14.58, "min": 14.18}}, "power_watts_avg": 19.99, "energy_joules_est": 26.56, "sample_count": 10, "duration_seconds": 1.329}, "timestamp": "2026-01-16T15:07:33.389907"}
{"image_index": 1, "image_name": "000000000285.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000285.jpg", "image_width": 586, "image_height": 640, "image_resolution": "586x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1604.519, "latencies_ms": [1604.519], "images_per_second": 0.623, "prompt_tokens": 21, "response_tokens_est": 59, "n_tiles": 1, "output_text": "The image depicts a brown bear lying on a grassy surface, likely in a natural habitat such as a forest or a park. The bear appears relaxed and is facing the camera, with its ears perked up and its mouth slightly open, giving a sense of calmness and curiosity.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26405.1, "ram_available_mb": 99367.0, "ram_percent": 21.0}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 26403.3, "ram_available_mb": 99368.9, "ram_percent": 21.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 7682.0, "gpu_max_mem_alloc_mb": 4854.7, "gpu_max_mem_reserved_mb": 7682.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.42, "peak": 14.61, "min": 14.1}, "VDD_GPU": {"avg": 18.86, "peak": 21.68, "min": 17.74}, "VIN": {"avg": 57.41, "peak": 66.35, "min": 54.03}, "VDD_CPU_SOC_MSS": {"avg": 14.51, "peak": 14.58, "min": 14.18}}, "power_watts_avg": 18.86, "energy_joules_est": 30.27, "sample_count": 12, "duration_seconds": 1.605}, "timestamp": "2026-01-16T15:07:35.000875"}
{"image_index": 1, "image_name": "000000000285.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000285.jpg", "image_width": 586, "image_height": 640, "image_resolution": "586x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1364.156, "latencies_ms": [1364.156], "images_per_second": 0.733, "prompt_tokens": 19, "response_tokens_est": 50, "n_tiles": 1, "output_text": "The image features a brown bear with a textured fur coat, standing on a grassy surface under natural daylight. The lighting is soft and diffused, casting gentle shadows on the bear's fur, enhancing the texture and depth of the image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26403.3, "ram_available_mb": 99368.9, "ram_percent": 21.0}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 26403.3, "ram_available_mb": 99368.9, "ram_percent": 21.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 7682.0, "gpu_max_mem_alloc_mb": 4853.2, "gpu_max_mem_reserved_mb": 7682.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.43, "peak": 14.61, "min": 14.0}, "VDD_GPU": {"avg": 19.2, "peak": 22.08, "min": 17.74}, "VIN": {"avg": 56.23, "peak": 63.57, "min": 46.01}, "VDD_CPU_SOC_MSS": {"avg": 14.58, "peak": 14.97, "min": 14.19}}, "power_watts_avg": 19.2, "energy_joules_est": 26.2, "sample_count": 10, "duration_seconds": 1.364}, "timestamp": "2026-01-16T15:07:36.371304"}
{"image_index": 2, "image_name": "000000000632.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000632.jpg", "image_width": 640, "image_height": 483, "image_resolution": "640x483", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2841.875, "latencies_ms": [2841.875], "images_per_second": 0.352, "prompt_tokens": 9, "response_tokens_est": 33, "n_tiles": 12, "output_text": "The image depicts a cozy bedroom with a large window that allows natural light to flood the room, illuminating the space and creating a warm and inviting atmosphere.", "error": null, "sys_before": {"cpu_percent": 34.2, "ram_used_mb": 26395.2, "ram_available_mb": 99377.0, "ram_percent": 21.0}, "sys_after": {"cpu_percent": 6.5, "ram_used_mb": 26517.2, "ram_available_mb": 99255.0, "ram_percent": 21.1}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 9386.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 9386.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.27, "peak": 15.14, "min": 13.39}, "VDD_GPU": {"avg": 30.56, "peak": 36.26, "min": 24.44}, "VIN": {"avg": 73.04, "peak": 114.84, "min": 60.63}, "VDD_CPU_SOC_MSS": {"avg": 15.13, "peak": 16.94, "min": 14.18}}, "power_watts_avg": 30.56, "energy_joules_est": 86.86, "sample_count": 22, "duration_seconds": 2.842}, "timestamp": "2026-01-16T15:07:39.274414"}
{"image_index": 2, "image_name": "000000000632.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000632.jpg", "image_width": 640, "image_height": 483, "image_resolution": "640x483", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 3126.274, "latencies_ms": [3126.274], "images_per_second": 0.32, "prompt_tokens": 23, "response_tokens_est": 38, "n_tiles": 12, "output_text": "bed: 1\ndresser: 1\nbookshelf: 1\nwindow: 1\nplant: 1\nchair: 1\nbasket: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26517.2, "ram_available_mb": 99255.0, "ram_percent": 21.1}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 26538.3, "ram_available_mb": 99233.9, "ram_percent": 21.1}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 10496.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 10496.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.13, "peak": 14.73, "min": 13.49}, "VDD_GPU": {"avg": 31.6, "peak": 37.83, "min": 24.05}, "VIN": {"avg": 69.46, "peak": 117.08, "min": 55.33}, "VDD_CPU_SOC_MSS": {"avg": 14.41, "peak": 15.36, "min": 13.79}}, "power_watts_avg": 31.6, "energy_joules_est": 98.81, "sample_count": 24, "duration_seconds": 3.127}, "timestamp": "2026-01-16T15:07:42.407448"}
{"image_index": 2, "image_name": "000000000632.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000632.jpg", "image_width": 640, "image_height": 483, "image_resolution": "640x483", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 4801.157, "latencies_ms": [4801.157], "images_per_second": 0.208, "prompt_tokens": 27, "response_tokens_est": 82, "n_tiles": 12, "output_text": "The main objects in the image are a bed, a dresser, a bookshelf, and a window. The bed is positioned in the foreground, with its headboard and footboard visible. The dresser is situated to the left of the bed, and the bookshelf is positioned to the right of the bed. The window is located in the background, allowing natural light to illuminate the room.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26538.3, "ram_available_mb": 99233.9, "ram_percent": 21.1}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 26556.4, "ram_available_mb": 99215.8, "ram_percent": 21.1}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.16, "peak": 14.73, "min": 13.49}, "VDD_GPU": {"avg": 28.77, "peak": 38.23, "min": 23.65}, "VIN": {"avg": 66.75, "peak": 112.43, "min": 55.84}, "VDD_CPU_SOC_MSS": {"avg": 14.36, "peak": 15.36, "min": 13.79}}, "power_watts_avg": 28.77, "energy_joules_est": 138.14, "sample_count": 37, "duration_seconds": 4.801}, "timestamp": "2026-01-16T15:07:47.215394"}
{"image_index": 2, "image_name": "000000000632.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000632.jpg", "image_width": 640, "image_height": 483, "image_resolution": "640x483", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 5437.397, "latencies_ms": [5437.397], "images_per_second": 0.184, "prompt_tokens": 21, "response_tokens_est": 99, "n_tiles": 12, "output_text": "The image depicts a cozy, well-organized bedroom with a large window that allows natural light to flood the room. The room is filled with various items, including a wooden dresser, a bookshelf filled with books, a small potted plant, and a wicker basket. The bed is neatly made with blue bedding, and there is a chair with a striped cushion nearby. The overall atmosphere is warm and inviting, with a mix of personal touches and functional decor.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 26556.4, "ram_available_mb": 99215.8, "ram_percent": 21.1}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 26560.3, "ram_available_mb": 99211.9, "ram_percent": 21.1}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.19, "peak": 14.73, "min": 13.49}, "VDD_GPU": {"avg": 28.22, "peak": 38.23, "min": 23.64}, "VIN": {"avg": 67.66, "peak": 122.84, "min": 54.37}, "VDD_CPU_SOC_MSS": {"avg": 14.44, "peak": 15.36, "min": 13.79}}, "power_watts_avg": 28.22, "energy_joules_est": 153.45, "sample_count": 43, "duration_seconds": 5.438}, "timestamp": "2026-01-16T15:07:52.660064"}
{"image_index": 2, "image_name": "000000000632.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000632.jpg", "image_width": 640, "image_height": 483, "image_resolution": "640x483", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3120.278, "latencies_ms": [3120.278], "images_per_second": 0.32, "prompt_tokens": 19, "response_tokens_est": 38, "n_tiles": 12, "output_text": "The room is bathed in natural light, creating a warm and inviting atmosphere. The walls are adorned with a floral wallpaper pattern, adding a touch of elegance to the space.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26560.3, "ram_available_mb": 99211.9, "ram_percent": 21.1}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 26561.8, "ram_available_mb": 99210.4, "ram_percent": 21.1}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.16, "peak": 14.73, "min": 13.72}, "VDD_GPU": {"avg": 31.58, "peak": 37.83, "min": 24.05}, "VIN": {"avg": 64.41, "peak": 109.34, "min": 53.66}, "VDD_CPU_SOC_MSS": {"avg": 14.39, "peak": 14.97, "min": 13.79}}, "power_watts_avg": 31.58, "energy_joules_est": 98.55, "sample_count": 24, "duration_seconds": 3.121}, "timestamp": "2026-01-16T15:07:55.786837"}
{"image_index": 3, "image_name": "000000000724.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000724.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2743.83, "latencies_ms": [2743.83], "images_per_second": 0.364, "prompt_tokens": 9, "response_tokens_est": 31, "n_tiles": 12, "output_text": "The image shows a red stop sign mounted on a pole, with a clear blue sky in the background and a few trees and bushes around it.", "error": null, "sys_before": {"cpu_percent": 46.2, "ram_used_mb": 26590.0, "ram_available_mb": 99182.2, "ram_percent": 21.1}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 26588.7, "ram_available_mb": 99183.4, "ram_percent": 21.1}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.21, "peak": 15.14, "min": 12.98}, "VDD_GPU": {"avg": 31.53, "peak": 37.83, "min": 24.44}, "VIN": {"avg": 70.92, "peak": 116.79, "min": 55.09}, "VDD_CPU_SOC_MSS": {"avg": 14.59, "peak": 15.76, "min": 13.79}}, "power_watts_avg": 31.53, "energy_joules_est": 86.52, "sample_count": 21, "duration_seconds": 2.744}, "timestamp": "2026-01-16T15:07:58.626154"}
{"image_index": 3, "image_name": "000000000724.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000724.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 3003.293, "latencies_ms": [3003.293], "images_per_second": 0.333, "prompt_tokens": 23, "response_tokens_est": 34, "n_tiles": 12, "output_text": "stop sign: 1\ntree: 1\ntrash can: 1\nbuilding: 1\nroad: 1\npole: 1", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 26588.7, "ram_available_mb": 99183.4, "ram_percent": 21.1}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 26592.8, "ram_available_mb": 99179.4, "ram_percent": 21.1}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.01, "peak": 14.73, "min": 13.49}, "VDD_GPU": {"avg": 32.01, "peak": 38.23, "min": 24.05}, "VIN": {"avg": 67.62, "peak": 113.04, "min": 55.42}, "VDD_CPU_SOC_MSS": {"avg": 14.27, "peak": 15.36, "min": 13.79}}, "power_watts_avg": 32.01, "energy_joules_est": 96.15, "sample_count": 23, "duration_seconds": 3.004}, "timestamp": "2026-01-16T15:08:01.636425"}
{"image_index": 3, "image_name": "000000000724.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000724.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 4839.884, "latencies_ms": [4839.884], "images_per_second": 0.207, "prompt_tokens": 27, "response_tokens_est": 82, "n_tiles": 12, "output_text": "The main objects in the image are a stop sign and a street lamp. The stop sign is positioned in the foreground, near the sidewalk, while the street lamp is situated in the background, slightly to the left. The stop sign is closer to the viewer, while the street lamp is further away, creating a spatial relationship where the stop sign is closer to the viewer and the street lamp is further away.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 26592.8, "ram_available_mb": 99179.4, "ram_percent": 21.1}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 26561.4, "ram_available_mb": 99210.8, "ram_percent": 21.1}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.87, "peak": 14.43, "min": 13.39}, "VDD_GPU": {"avg": 28.65, "peak": 38.23, "min": 23.25}, "VIN": {"avg": 65.65, "peak": 105.61, "min": 51.99}, "VDD_CPU_SOC_MSS": {"avg": 14.06, "peak": 14.97, "min": 13.79}}, "power_watts_avg": 28.65, "energy_joules_est": 138.68, "sample_count": 38, "duration_seconds": 4.84}, "timestamp": "2026-01-16T15:08:06.483360"}
{"image_index": 3, "image_name": "000000000724.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000724.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 4523.929, "latencies_ms": [4523.929], "images_per_second": 0.221, "prompt_tokens": 21, "response_tokens_est": 74, "n_tiles": 12, "output_text": "The image depicts a street scene with a stop sign prominently displayed in the foreground. The sign is red with white lettering, and it is mounted on a metal pole. In the background, there are trees, a parked white van, and a few other vehicles. The setting appears to be a suburban or urban area with clear skies and a calm atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26561.4, "ram_available_mb": 99210.8, "ram_percent": 21.1}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 26562.0, "ram_available_mb": 99210.2, "ram_percent": 21.1}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.87, "peak": 14.43, "min": 13.29}, "VDD_GPU": {"avg": 28.97, "peak": 38.23, "min": 23.26}, "VIN": {"avg": 65.27, "peak": 95.89, "min": 54.86}, "VDD_CPU_SOC_MSS": {"avg": 14.03, "peak": 14.97, "min": 13.39}}, "power_watts_avg": 28.97, "energy_joules_est": 131.07, "sample_count": 35, "duration_seconds": 4.524}, "timestamp": "2026-01-16T15:08:11.015715"}
{"image_index": 3, "image_name": "000000000724.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000724.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3939.969, "latencies_ms": [3939.969], "images_per_second": 0.254, "prompt_tokens": 19, "response_tokens_est": 59, "n_tiles": 12, "output_text": "The image features a red stop sign with white lettering, mounted on a metal pole. The sign is set against a backdrop of a clear blue sky with scattered clouds, indicating a sunny day. The surrounding area includes green trees, a sidewalk, and a road with a white line marking.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26562.0, "ram_available_mb": 99210.2, "ram_percent": 21.1}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 26475.6, "ram_available_mb": 99296.6, "ram_percent": 21.1}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.89, "peak": 14.53, "min": 13.31}, "VDD_GPU": {"avg": 29.91, "peak": 38.23, "min": 23.26}, "VIN": {"avg": 69.51, "peak": 128.69, "min": 55.67}, "VDD_CPU_SOC_MSS": {"avg": 14.07, "peak": 14.97, "min": 13.79}}, "power_watts_avg": 29.91, "energy_joules_est": 117.86, "sample_count": 30, "duration_seconds": 3.94}, "timestamp": "2026-01-16T15:08:14.962649"}
{"image_index": 4, "image_name": "000000000776.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000776.jpg", "image_width": 428, "image_height": 640, "image_resolution": "428x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1781.626, "latencies_ms": [1781.626], "images_per_second": 0.561, "prompt_tokens": 9, "response_tokens_est": 37, "n_tiles": 6, "output_text": "The image shows a close-up of two teddy bears, one in a light brown color and the other in a darker shade, both with soft, fuzzy textures and round ears.", "error": null, "sys_before": {"cpu_percent": 44.8, "ram_used_mb": 26475.3, "ram_available_mb": 99296.8, "ram_percent": 21.1}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 26475.8, "ram_available_mb": 99296.4, "ram_percent": 21.1}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.53, "peak": 14.2, "min": 12.78}, "VDD_GPU": {"avg": 28.47, "peak": 35.86, "min": 22.47}, "VIN": {"avg": 69.59, "peak": 96.04, "min": 57.64}, "VDD_CPU_SOC_MSS": {"avg": 13.91, "peak": 14.57, "min": 13.39}}, "power_watts_avg": 28.47, "energy_joules_est": 50.73, "sample_count": 13, "duration_seconds": 1.782}, "timestamp": "2026-01-16T15:08:16.812928"}
{"image_index": 4, "image_name": "000000000776.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000776.jpg", "image_width": 428, "image_height": 640, "image_resolution": "428x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2414.048, "latencies_ms": [2414.048], "images_per_second": 0.414, "prompt_tokens": 23, "response_tokens_est": 56, "n_tiles": 6, "output_text": "1. Brown teddy bear\n2. Brown teddy bear\n3. Brown teddy bear\n4. Brown teddy bear\n5. Brown teddy bear\n6. Brown teddy bear\n7. Brown teddy bear\n8. Brown teddy bear", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26475.8, "ram_available_mb": 99296.4, "ram_percent": 21.1}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 26475.6, "ram_available_mb": 99296.6, "ram_percent": 21.1}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.01, "peak": 14.3, "min": 13.51}, "VDD_GPU": {"avg": 26.91, "peak": 35.47, "min": 21.68}, "VIN": {"avg": 66.98, "peak": 112.32, "min": 53.78}, "VDD_CPU_SOC_MSS": {"avg": 14.12, "peak": 14.18, "min": 13.79}}, "power_watts_avg": 26.91, "energy_joules_est": 64.97, "sample_count": 18, "duration_seconds": 2.414}, "timestamp": "2026-01-16T15:08:19.233508"}
{"image_index": 4, "image_name": "000000000776.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000776.jpg", "image_width": 428, "image_height": 640, "image_resolution": "428x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2841.541, "latencies_ms": [2841.541], "images_per_second": 0.352, "prompt_tokens": 27, "response_tokens_est": 70, "n_tiles": 6, "output_text": "The main objects in the image are two teddy bears. The teddy bear on the left is in the foreground, while the teddy bear on the right is slightly behind and to the right of the left one. The teddy bear on the left is closer to the viewer, while the teddy bear on the right is further away.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26475.6, "ram_available_mb": 99296.6, "ram_percent": 21.1}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 26476.0, "ram_available_mb": 99296.2, "ram_percent": 21.1}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.09, "peak": 14.4, "min": 13.51}, "VDD_GPU": {"avg": 25.91, "peak": 35.47, "min": 21.29}, "VIN": {"avg": 63.95, "peak": 109.94, "min": 44.39}, "VDD_CPU_SOC_MSS": {"avg": 14.27, "peak": 14.58, "min": 13.79}}, "power_watts_avg": 25.91, "energy_joules_est": 73.63, "sample_count": 22, "duration_seconds": 2.842}, "timestamp": "2026-01-16T15:08:22.081622"}
{"image_index": 4, "image_name": "000000000776.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000776.jpg", "image_width": 428, "image_height": 640, "image_resolution": "428x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2968.64, "latencies_ms": [2968.64], "images_per_second": 0.337, "prompt_tokens": 21, "response_tokens_est": 74, "n_tiles": 6, "output_text": "The image depicts a cozy scene featuring two teddy bears. One bear is lying on its back, while the other is positioned close to it, both appearing to be in a comfortable and relaxed state. The setting suggests a soft, warm environment, possibly a bedroom or a living room, where the teddy bears are likely being cuddled or simply resting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26476.0, "ram_available_mb": 99296.2, "ram_percent": 21.1}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 26475.1, "ram_available_mb": 99297.0, "ram_percent": 21.1}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.07, "peak": 14.3, "min": 13.41}, "VDD_GPU": {"avg": 25.79, "peak": 35.07, "min": 21.29}, "VIN": {"avg": 64.69, "peak": 107.05, "min": 50.79}, "VDD_CPU_SOC_MSS": {"avg": 14.25, "peak": 14.57, "min": 13.79}}, "power_watts_avg": 25.79, "energy_joules_est": 76.57, "sample_count": 23, "duration_seconds": 2.969}, "timestamp": "2026-01-16T15:08:25.056536"}
{"image_index": 4, "image_name": "000000000776.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000776.jpg", "image_width": 428, "image_height": 640, "image_resolution": "428x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2516.098, "latencies_ms": [2516.098], "images_per_second": 0.397, "prompt_tokens": 19, "response_tokens_est": 60, "n_tiles": 6, "output_text": "The image features two teddy bears with a warm, inviting brown color palette. The lighting is soft and diffused, casting gentle shadows and highlighting the plush texture of the bears. The weather appears to be overcast, as the overall lighting is subdued and the shadows are soft.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26475.1, "ram_available_mb": 99297.0, "ram_percent": 21.1}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 26474.4, "ram_available_mb": 99297.7, "ram_percent": 21.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.09, "peak": 14.4, "min": 13.51}, "VDD_GPU": {"avg": 26.76, "peak": 35.47, "min": 21.68}, "VIN": {"avg": 64.72, "peak": 88.59, "min": 49.67}, "VDD_CPU_SOC_MSS": {"avg": 14.31, "peak": 14.57, "min": 13.79}}, "power_watts_avg": 26.76, "energy_joules_est": 67.34, "sample_count": 19, "duration_seconds": 2.517}, "timestamp": "2026-01-16T15:08:27.578673"}
{"image_index": 5, "image_name": "000000000785.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000785.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1735.564, "latencies_ms": [1735.564], "images_per_second": 0.576, "prompt_tokens": 9, "response_tokens_est": 36, "n_tiles": 6, "output_text": "A person is skiing down a snowy slope, wearing a red jacket, black pants, and a black beanie, with blue and red skis and ski poles.", "error": null, "sys_before": {"cpu_percent": 41.3, "ram_used_mb": 26474.2, "ram_available_mb": 99298.0, "ram_percent": 21.0}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 26474.2, "ram_available_mb": 99298.0, "ram_percent": 21.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.72, "peak": 14.3, "min": 13.0}, "VDD_GPU": {"avg": 28.2, "peak": 34.68, "min": 22.47}, "VIN": {"avg": 64.55, "peak": 95.68, "min": 51.97}, "VDD_CPU_SOC_MSS": {"avg": 14.03, "peak": 14.57, "min": 13.39}}, "power_watts_avg": 28.2, "energy_joules_est": 48.95, "sample_count": 13, "duration_seconds": 1.736}, "timestamp": "2026-01-16T15:08:29.374087"}
{"image_index": 5, "image_name": "000000000785.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000785.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1910.968, "latencies_ms": [1910.968], "images_per_second": 0.523, "prompt_tokens": 23, "response_tokens_est": 40, "n_tiles": 6, "output_text": "object: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26474.2, "ram_available_mb": 99298.0, "ram_percent": 21.0}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 26474.2, "ram_available_mb": 99298.0, "ram_percent": 21.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.01, "peak": 14.4, "min": 13.51}, "VDD_GPU": {"avg": 28.17, "peak": 35.07, "min": 22.08}, "VIN": {"avg": 67.56, "peak": 108.06, "min": 49.5}, "VDD_CPU_SOC_MSS": {"avg": 14.23, "peak": 14.57, "min": 13.78}}, "power_watts_avg": 28.17, "energy_joules_est": 53.84, "sample_count": 15, "duration_seconds": 1.911}, "timestamp": "2026-01-16T15:08:31.290862"}
{"image_index": 5, "image_name": "000000000785.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000785.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2734.899, "latencies_ms": [2734.899], "images_per_second": 0.366, "prompt_tokens": 27, "response_tokens_est": 67, "n_tiles": 6, "output_text": "The main subject, a person dressed in a red jacket and black pants, is positioned in the foreground of the image. They are skiing on a snowy slope, with their skis and poles visible in the foreground. The background features a snow-covered hill and a fence, which are further away from the main subject.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26474.2, "ram_available_mb": 99298.0, "ram_percent": 21.0}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 26474.2, "ram_available_mb": 99298.0, "ram_percent": 21.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.13, "peak": 14.4, "min": 13.51}, "VDD_GPU": {"avg": 26.33, "peak": 35.07, "min": 21.68}, "VIN": {"avg": 62.01, "peak": 87.99, "min": 54.02}, "VDD_CPU_SOC_MSS": {"avg": 14.35, "peak": 14.57, "min": 13.79}}, "power_watts_avg": 26.33, "energy_joules_est": 72.02, "sample_count": 21, "duration_seconds": 2.735}, "timestamp": "2026-01-16T15:08:34.032330"}
{"image_index": 5, "image_name": "000000000785.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000785.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3161.453, "latencies_ms": [3161.453], "images_per_second": 0.316, "prompt_tokens": 21, "response_tokens_est": 81, "n_tiles": 6, "output_text": "The image depicts a person skiing down a snowy slope. The individual is dressed in a red jacket, black pants, and a black beanie, with sunglasses and gloves. They are holding ski poles and appear to be in motion, creating a trail of snow behind them. The setting is a snowy mountain slope, with a clear sky and a few orange poles visible in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26474.2, "ram_available_mb": 99298.0, "ram_percent": 21.0}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 26474.9, "ram_available_mb": 99297.3, "ram_percent": 21.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.17, "peak": 14.51, "min": 13.51}, "VDD_GPU": {"avg": 25.64, "peak": 35.47, "min": 21.29}, "VIN": {"avg": 64.66, "peak": 100.4, "min": 58.0}, "VDD_CPU_SOC_MSS": {"avg": 14.36, "peak": 14.57, "min": 13.79}}, "power_watts_avg": 25.64, "energy_joules_est": 81.07, "sample_count": 24, "duration_seconds": 3.162}, "timestamp": "2026-01-16T15:08:37.201055"}
{"image_index": 5, "image_name": "000000000785.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000785.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2672.023, "latencies_ms": [2672.023], "images_per_second": 0.374, "prompt_tokens": 19, "response_tokens_est": 65, "n_tiles": 6, "output_text": "The notable visual attributes of the image include a person dressed in a red jacket and black pants, wearing a black beanie and sunglasses, and blue and red ski boots. The scene is set on a snowy slope with a bright, overcast sky, creating a contrast between the vibrant red jacket and the white snow.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26474.9, "ram_available_mb": 99297.3, "ram_percent": 21.0}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 26474.7, "ram_available_mb": 99297.5, "ram_percent": 21.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.11, "peak": 14.4, "min": 13.51}, "VDD_GPU": {"avg": 26.18, "peak": 35.47, "min": 21.29}, "VIN": {"avg": 64.28, "peak": 86.55, "min": 59.08}, "VDD_CPU_SOC_MSS": {"avg": 14.33, "peak": 14.57, "min": 13.79}}, "power_watts_avg": 26.18, "energy_joules_est": 69.97, "sample_count": 21, "duration_seconds": 2.672}, "timestamp": "2026-01-16T15:08:39.879622"}
{"image_index": 6, "image_name": "000000000802.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000802.jpg", "image_width": 424, "image_height": 640, "image_resolution": "424x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1469.311, "latencies_ms": [1469.311], "images_per_second": 0.681, "prompt_tokens": 9, "response_tokens_est": 27, "n_tiles": 6, "output_text": "The image shows a kitchen with a white refrigerator, wooden cabinets, and a white stove, all set against a beige wall.", "error": null, "sys_before": {"cpu_percent": 42.1, "ram_used_mb": 26474.7, "ram_available_mb": 99297.5, "ram_percent": 21.0}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 26474.7, "ram_available_mb": 99297.5, "ram_percent": 21.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.58, "peak": 14.2, "min": 13.0}, "VDD_GPU": {"avg": 29.17, "peak": 35.07, "min": 23.65}, "VIN": {"avg": 66.72, "peak": 113.08, "min": 47.81}, "VDD_CPU_SOC_MSS": {"avg": 13.93, "peak": 14.18, "min": 13.39}}, "power_watts_avg": 29.17, "energy_joules_est": 42.87, "sample_count": 11, "duration_seconds": 1.47}, "timestamp": "2026-01-16T15:08:41.408148"}
{"image_index": 6, "image_name": "000000000802.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000802.jpg", "image_width": 424, "image_height": 640, "image_resolution": "424x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1914.381, "latencies_ms": [1914.381], "images_per_second": 0.522, "prompt_tokens": 23, "response_tokens_est": 40, "n_tiles": 6, "output_text": "object: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 26474.7, "ram_available_mb": 99297.5, "ram_percent": 21.0}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 26478.2, "ram_available_mb": 99294.0, "ram_percent": 21.1}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.99, "peak": 14.4, "min": 13.41}, "VDD_GPU": {"avg": 28.56, "peak": 35.86, "min": 22.08}, "VIN": {"avg": 67.21, "peak": 97.33, "min": 55.55}, "VDD_CPU_SOC_MSS": {"avg": 14.23, "peak": 14.57, "min": 13.78}}, "power_watts_avg": 28.56, "energy_joules_est": 54.69, "sample_count": 15, "duration_seconds": 1.915}, "timestamp": "2026-01-16T15:08:43.332864"}
{"image_index": 6, "image_name": "000000000802.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000802.jpg", "image_width": 424, "image_height": 640, "image_resolution": "424x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2280.405, "latencies_ms": [2280.405], "images_per_second": 0.439, "prompt_tokens": 27, "response_tokens_est": 52, "n_tiles": 6, "output_text": "The main objects in the image are a white refrigerator and a white stove. The refrigerator is located to the right of the stove, and the stove is positioned in the foreground. The refrigerator is closer to the foreground, while the stove is in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26478.2, "ram_available_mb": 99294.0, "ram_percent": 21.1}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 26477.9, "ram_available_mb": 99294.3, "ram_percent": 21.1}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.06, "peak": 14.4, "min": 13.41}, "VDD_GPU": {"avg": 27.31, "peak": 35.48, "min": 21.68}, "VIN": {"avg": 64.54, "peak": 89.26, "min": 55.52}, "VDD_CPU_SOC_MSS": {"avg": 14.2, "peak": 14.57, "min": 13.78}}, "power_watts_avg": 27.31, "energy_joules_est": 62.29, "sample_count": 18, "duration_seconds": 2.281}, "timestamp": "2026-01-16T15:08:45.619299"}
{"image_index": 6, "image_name": "000000000802.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000802.jpg", "image_width": 424, "image_height": 640, "image_resolution": "424x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1970.255, "latencies_ms": [1970.255], "images_per_second": 0.508, "prompt_tokens": 21, "response_tokens_est": 42, "n_tiles": 6, "output_text": "The image depicts a small kitchen with a white refrigerator and a white stove. The kitchen has wooden cabinets and beige tiled flooring. The scene appears to be indoors, possibly in a residential setting.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 26477.9, "ram_available_mb": 99294.3, "ram_percent": 21.1}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 26480.1, "ram_available_mb": 99292.1, "ram_percent": 21.1}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.05, "peak": 14.4, "min": 13.51}, "VDD_GPU": {"avg": 28.01, "peak": 35.07, "min": 22.07}, "VIN": {"avg": 64.83, "peak": 92.68, "min": 56.89}, "VDD_CPU_SOC_MSS": {"avg": 14.26, "peak": 14.57, "min": 13.78}}, "power_watts_avg": 28.01, "energy_joules_est": 55.2, "sample_count": 15, "duration_seconds": 1.971}, "timestamp": "2026-01-16T15:08:47.595903"}
{"image_index": 6, "image_name": "000000000802.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000802.jpg", "image_width": 424, "image_height": 640, "image_resolution": "424x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1876.734, "latencies_ms": [1876.734], "images_per_second": 0.533, "prompt_tokens": 19, "response_tokens_est": 39, "n_tiles": 6, "output_text": "The kitchen features a warm, beige color scheme with wooden cabinets and a white refrigerator. The lighting is soft and natural, likely from a nearby window, creating a cozy and inviting atmosphere.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 26480.1, "ram_available_mb": 99292.1, "ram_percent": 21.1}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 26480.1, "ram_available_mb": 99292.1, "ram_percent": 21.1}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.0, "peak": 14.4, "min": 13.51}, "VDD_GPU": {"avg": 28.91, "peak": 36.27, "min": 22.47}, "VIN": {"avg": 67.64, "peak": 107.5, "min": 53.35}, "VDD_CPU_SOC_MSS": {"avg": 14.21, "peak": 14.57, "min": 13.78}}, "power_watts_avg": 28.91, "energy_joules_est": 54.27, "sample_count": 14, "duration_seconds": 1.877}, "timestamp": "2026-01-16T15:08:49.478746"}
{"image_index": 7, "image_name": "000000000872.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000872.jpg", "image_width": 621, "image_height": 640, "image_resolution": "621x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1414.311, "latencies_ms": [1414.311], "images_per_second": 0.707, "prompt_tokens": 9, "response_tokens_est": 52, "n_tiles": 1, "output_text": "Two baseball players are in the midst of a game, with one player in a white shirt and blue helmet running towards first base while the other player in a green shirt and blue cap is running towards second base, both wearing gloves and focused on the game.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 26480.1, "ram_available_mb": 99292.1, "ram_percent": 21.1}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 26480.4, "ram_available_mb": 99291.8, "ram_percent": 21.1}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 4845.7, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.42, "peak": 14.61, "min": 13.9}, "VDD_GPU": {"avg": 20.11, "peak": 24.84, "min": 18.14}, "VIN": {"avg": 56.34, "peak": 58.1, "min": 49.9}, "VDD_CPU_SOC_MSS": {"avg": 14.54, "peak": 14.97, "min": 14.18}}, "power_watts_avg": 20.11, "energy_joules_est": 28.45, "sample_count": 11, "duration_seconds": 1.415}, "timestamp": "2026-01-16T15:08:50.919170"}
{"image_index": 7, "image_name": "000000000872.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000872.jpg", "image_width": 621, "image_height": 640, "image_resolution": "621x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 3375.427, "latencies_ms": [3375.427], "images_per_second": 0.296, "prompt_tokens": 23, "response_tokens_est": 129, "n_tiles": 1, "output_text": "- baseball player: 2\n- baseball: 1\n- baseball glove: 1\n- baseball uniform: 1\n- baseball cap: 1\n- baseball bat: 1\n- baseball field: 1\n- baseball glove: 1\n- baseball: 1\n- baseball player: 1\n- baseball player: 1\n- baseball player: 1\n- baseball player: 1\n- baseball player: 1\n- baseball player: 1\n- baseball player: 1\n- baseball player: 1\n- baseball player: 1\n- baseball player:", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 26480.4, "ram_available_mb": 99291.8, "ram_percent": 21.1}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 26481.5, "ram_available_mb": 99290.6, "ram_percent": 21.1}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 4856.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.6, "peak": 14.71, "min": 14.2}, "VDD_GPU": {"avg": 18.56, "peak": 22.47, "min": 17.74}, "VIN": {"avg": 57.28, "peak": 64.68, "min": 47.74}, "VDD_CPU_SOC_MSS": {"avg": 14.8, "peak": 14.97, "min": 14.57}}, "power_watts_avg": 18.56, "energy_joules_est": 62.66, "sample_count": 26, "duration_seconds": 3.376}, "timestamp": "2026-01-16T15:08:54.301016"}
{"image_index": 7, "image_name": "000000000872.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000872.jpg", "image_width": 621, "image_height": 640, "image_resolution": "621x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2718.458, "latencies_ms": [2718.458], "images_per_second": 0.368, "prompt_tokens": 27, "response_tokens_est": 103, "n_tiles": 1, "output_text": "In the image, the main object, a baseball player in the foreground, is positioned near the center of the frame. The player is wearing a white shirt, gray pants, and a blue helmet. In the background, another player is seen running towards the left side of the frame. The second player is wearing a green shirt and a green cap. The player in the background is slightly farther away from the main subject, indicating a spatial relationship where the main subject is closer to the camera than the background player.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26481.5, "ram_available_mb": 99290.6, "ram_percent": 21.1}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 26481.8, "ram_available_mb": 99290.4, "ram_percent": 21.1}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 4859.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.61, "peak": 14.71, "min": 14.1}, "VDD_GPU": {"avg": 18.81, "peak": 22.47, "min": 18.14}, "VIN": {"avg": 58.02, "peak": 65.41, "min": 52.66}, "VDD_CPU_SOC_MSS": {"avg": 14.78, "peak": 14.97, "min": 14.18}}, "power_watts_avg": 18.81, "energy_joules_est": 51.14, "sample_count": 21, "duration_seconds": 2.719}, "timestamp": "2026-01-16T15:08:57.027270"}
{"image_index": 7, "image_name": "000000000872.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000872.jpg", "image_width": 621, "image_height": 640, "image_resolution": "621x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2300.004, "latencies_ms": [2300.004], "images_per_second": 0.435, "prompt_tokens": 21, "response_tokens_est": 87, "n_tiles": 1, "output_text": "The image captures a dynamic moment during a baseball game, with two players in mid-action on the field. One player, wearing a white shirt and blue helmet, is running towards first base while the other, dressed in a green shirt and blue cap, is running towards second base. The background features lush green trees and a well-maintained grassy area, indicating that the game is taking place in a park or recreational field.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 26481.8, "ram_available_mb": 99290.4, "ram_percent": 21.1}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 26473.4, "ram_available_mb": 99298.8, "ram_percent": 21.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 4854.7, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.61, "peak": 14.71, "min": 14.2}, "VDD_GPU": {"avg": 18.88, "peak": 22.47, "min": 18.14}, "VIN": {"avg": 58.6, "peak": 68.58, "min": 54.66}, "VDD_CPU_SOC_MSS": {"avg": 14.82, "peak": 14.97, "min": 14.57}}, "power_watts_avg": 18.88, "energy_joules_est": 43.43, "sample_count": 18, "duration_seconds": 2.3}, "timestamp": "2026-01-16T15:08:59.333247"}
{"image_index": 7, "image_name": "000000000872.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000872.jpg", "image_width": 621, "image_height": 640, "image_resolution": "621x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1469.098, "latencies_ms": [1469.098], "images_per_second": 0.681, "prompt_tokens": 19, "response_tokens_est": 54, "n_tiles": 1, "output_text": "The image captures a baseball game in action, with players in white and gray uniforms, and a player in a green cap and jersey. The scene is set outdoors under bright daylight, with lush green trees and a well-maintained grassy field in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26473.4, "ram_available_mb": 99298.8, "ram_percent": 21.0}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 26473.1, "ram_available_mb": 99299.0, "ram_percent": 21.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 4853.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.56, "peak": 14.71, "min": 14.2}, "VDD_GPU": {"avg": 19.35, "peak": 22.08, "min": 18.14}, "VIN": {"avg": 57.16, "peak": 62.3, "min": 54.12}, "VDD_CPU_SOC_MSS": {"avg": 14.65, "peak": 14.97, "min": 14.18}}, "power_watts_avg": 19.35, "energy_joules_est": 28.44, "sample_count": 11, "duration_seconds": 1.47}, "timestamp": "2026-01-16T15:09:00.808243"}
{"image_index": 8, "image_name": "000000000885.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000885.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1773.037, "latencies_ms": [1773.037], "images_per_second": 0.564, "prompt_tokens": 9, "response_tokens_est": 37, "n_tiles": 6, "output_text": "A tennis player is in the midst of a powerful serve, with his body leaning forward and his racket in motion, as he attempts to hit the ball with precision and power.", "error": null, "sys_before": {"cpu_percent": 43.2, "ram_used_mb": 26473.1, "ram_available_mb": 99299.0, "ram_percent": 21.0}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 26471.4, "ram_available_mb": 99300.7, "ram_percent": 21.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.75, "peak": 14.3, "min": 13.0}, "VDD_GPU": {"avg": 27.65, "peak": 34.68, "min": 22.47}, "VIN": {"avg": 67.42, "peak": 108.91, "min": 55.69}, "VDD_CPU_SOC_MSS": {"avg": 14.09, "peak": 14.57, "min": 13.39}}, "power_watts_avg": 27.65, "energy_joules_est": 49.04, "sample_count": 13, "duration_seconds": 1.774}, "timestamp": "2026-01-16T15:09:02.638077"}
{"image_index": 8, "image_name": "000000000885.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000885.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2042.906, "latencies_ms": [2042.906], "images_per_second": 0.489, "prompt_tokens": 23, "response_tokens_est": 44, "n_tiles": 6, "output_text": "1. Tennis player\n2. Tennis racket\n3. Tennis ball\n4. Tennis court\n5. Spectator\n6. Advertisement board\n7. Advertisement board\n8. Advertisement board", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26471.4, "ram_available_mb": 99300.7, "ram_percent": 21.0}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 26470.8, "ram_available_mb": 99301.3, "ram_percent": 21.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.99, "peak": 14.3, "min": 13.51}, "VDD_GPU": {"avg": 28.27, "peak": 35.86, "min": 22.08}, "VIN": {"avg": 65.45, "peak": 97.98, "min": 57.71}, "VDD_CPU_SOC_MSS": {"avg": 14.15, "peak": 14.57, "min": 13.78}}, "power_watts_avg": 28.27, "energy_joules_est": 57.76, "sample_count": 15, "duration_seconds": 2.043}, "timestamp": "2026-01-16T15:09:04.687464"}
{"image_index": 8, "image_name": "000000000885.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000885.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3668.704, "latencies_ms": [3668.704], "images_per_second": 0.273, "prompt_tokens": 27, "response_tokens_est": 97, "n_tiles": 6, "output_text": "The main object in the foreground is a tennis player in a white and blue outfit, who is in the process of hitting a tennis ball. The player is positioned near the net, with his racket in hand, preparing to strike the ball. In the background, there is a blue barrier with the word \"POLO\" written on it, indicating the location of the event. The player is slightly off-center to the left, with the barrier and spectators in the background.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 26470.8, "ram_available_mb": 99301.3, "ram_percent": 21.0}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 26474.8, "ram_available_mb": 99297.4, "ram_percent": 21.0}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.21, "peak": 14.51, "min": 13.61}, "VDD_GPU": {"avg": 24.96, "peak": 35.86, "min": 21.29}, "VIN": {"avg": 63.2, "peak": 98.0, "min": 58.17}, "VDD_CPU_SOC_MSS": {"avg": 14.41, "peak": 14.57, "min": 13.79}}, "power_watts_avg": 24.96, "energy_joules_est": 91.58, "sample_count": 28, "duration_seconds": 3.669}, "timestamp": "2026-01-16T15:09:08.366268"}
{"image_index": 8, "image_name": "000000000885.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000885.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3407.745, "latencies_ms": [3407.745], "images_per_second": 0.293, "prompt_tokens": 21, "response_tokens_est": 88, "n_tiles": 6, "output_text": "The image captures a moment during a tennis match on a well-maintained court. The player, dressed in a white shirt and blue shorts, is in the midst of a powerful serve, with his body leaning forward and his racket extended towards the ball. The background features a blue barrier with the \"J.P. Morgan\" logo, indicating sponsorship, and a few spectators are seated on the sidelines, observing the match.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26474.8, "ram_available_mb": 99297.4, "ram_percent": 21.0}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 26475.6, "ram_available_mb": 99296.6, "ram_percent": 21.1}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.14, "peak": 14.4, "min": 13.51}, "VDD_GPU": {"avg": 25.21, "peak": 35.86, "min": 21.28}, "VIN": {"avg": 63.53, "peak": 94.15, "min": 49.02}, "VDD_CPU_SOC_MSS": {"avg": 14.31, "peak": 14.57, "min": 13.78}}, "power_watts_avg": 25.21, "energy_joules_est": 85.93, "sample_count": 25, "duration_seconds": 3.409}, "timestamp": "2026-01-16T15:09:11.780948"}
{"image_index": 8, "image_name": "000000000885.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000000885.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2416.825, "latencies_ms": [2416.825], "images_per_second": 0.414, "prompt_tokens": 19, "response_tokens_est": 56, "n_tiles": 6, "output_text": "The image captures a tennis match on a bright, sunny day. The court is marked with white lines, and the surface appears to be a synthetic material, likely rubber. The lighting is natural, casting shadows on the court, indicating that the sun is high in the sky.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26475.6, "ram_available_mb": 99296.6, "ram_percent": 21.1}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 26475.6, "ram_available_mb": 99296.6, "ram_percent": 21.1}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.06, "peak": 14.3, "min": 13.61}, "VDD_GPU": {"avg": 26.78, "peak": 35.07, "min": 21.67}, "VIN": {"avg": 63.89, "peak": 94.28, "min": 50.14}, "VDD_CPU_SOC_MSS": {"avg": 14.22, "peak": 14.57, "min": 13.79}}, "power_watts_avg": 26.78, "energy_joules_est": 64.73, "sample_count": 18, "duration_seconds": 2.417}, "timestamp": "2026-01-16T15:09:14.204517"}
{"image_index": 9, "image_name": "000000001000.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001000.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2413.098, "latencies_ms": [2413.098], "images_per_second": 0.414, "prompt_tokens": 9, "response_tokens_est": 22, "n_tiles": 12, "output_text": "The image shows a group of young tennis players and their coach posing for a photo on a tennis court.", "error": null, "sys_before": {"cpu_percent": 40.6, "ram_used_mb": 26475.8, "ram_available_mb": 99296.4, "ram_percent": 21.1}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 26475.8, "ram_available_mb": 99296.4, "ram_percent": 21.1}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.24, "peak": 15.24, "min": 12.88}, "VDD_GPU": {"avg": 32.45, "peak": 37.04, "min": 26.01}, "VIN": {"avg": 74.76, "peak": 113.8, "min": 55.87}, "VDD_CPU_SOC_MSS": {"avg": 14.64, "peak": 15.76, "min": 13.39}}, "power_watts_avg": 32.45, "energy_joules_est": 78.32, "sample_count": 18, "duration_seconds": 2.413}, "timestamp": "2026-01-16T15:09:16.717039"}
{"image_index": 9, "image_name": "000000001000.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001000.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 3240.162, "latencies_ms": [3240.162], "images_per_second": 0.309, "prompt_tokens": 23, "response_tokens_est": 41, "n_tiles": 12, "output_text": "1. Tennis players\n2. Tennis rackets\n3. Tennis balls\n4. Tennis shoes\n5. Tennis bags\n6. Tennis balls\n7. Tennis balls\n8. Tennis balls", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26475.8, "ram_available_mb": 99296.4, "ram_percent": 21.1}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 26476.3, "ram_available_mb": 99295.9, "ram_percent": 21.1}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.25, "peak": 14.83, "min": 13.61}, "VDD_GPU": {"avg": 31.76, "peak": 38.62, "min": 24.43}, "VIN": {"avg": 74.83, "peak": 132.31, "min": 58.1}, "VDD_CPU_SOC_MSS": {"avg": 14.54, "peak": 15.36, "min": 14.18}}, "power_watts_avg": 31.76, "energy_joules_est": 102.92, "sample_count": 25, "duration_seconds": 3.241}, "timestamp": "2026-01-16T15:09:19.963927"}
{"image_index": 9, "image_name": "000000001000.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001000.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3846.943, "latencies_ms": [3846.943], "images_per_second": 0.26, "prompt_tokens": 27, "response_tokens_est": 57, "n_tiles": 12, "output_text": "The main objects in the image are a group of young boys and girls standing on a tennis court. The boys are positioned in the foreground, with the girls slightly behind them. The tennis court is surrounded by a green fence, and the background features trees and a clear blue sky.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26476.3, "ram_available_mb": 99295.9, "ram_percent": 21.1}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 26477.7, "ram_available_mb": 99294.5, "ram_percent": 21.1}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.27, "peak": 14.83, "min": 13.59}, "VDD_GPU": {"avg": 30.39, "peak": 38.62, "min": 24.05}, "VIN": {"avg": 72.77, "peak": 103.46, "min": 54.87}, "VDD_CPU_SOC_MSS": {"avg": 14.63, "peak": 15.36, "min": 14.18}}, "power_watts_avg": 30.39, "energy_joules_est": 116.92, "sample_count": 30, "duration_seconds": 3.847}, "timestamp": "2026-01-16T15:09:23.817680"}
{"image_index": 9, "image_name": "000000001000.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001000.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 4217.374, "latencies_ms": [4217.374], "images_per_second": 0.237, "prompt_tokens": 21, "response_tokens_est": 67, "n_tiles": 12, "output_text": "The scene depicts a group of young tennis players and their coach standing on a tennis court, likely after a match or practice session. The players are dressed in athletic gear, with some holding tennis rackets, and the coach is holding a trophy. The setting is outdoors, with trees and a green fence visible in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 26477.7, "ram_available_mb": 99294.5, "ram_percent": 21.1}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 25728.7, "ram_available_mb": 100043.5, "ram_percent": 20.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.28, "peak": 14.94, "min": 13.59}, "VDD_GPU": {"avg": 29.78, "peak": 38.23, "min": 24.05}, "VIN": {"avg": 74.22, "peak": 120.09, "min": 61.53}, "VDD_CPU_SOC_MSS": {"avg": 14.65, "peak": 15.36, "min": 13.79}}, "power_watts_avg": 29.78, "energy_joules_est": 125.6, "sample_count": 32, "duration_seconds": 4.218}, "timestamp": "2026-01-16T15:09:28.042445"}
{"image_index": 9, "image_name": "000000001000.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001000.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 4588.671, "latencies_ms": [4588.671], "images_per_second": 0.218, "prompt_tokens": 19, "response_tokens_est": 77, "n_tiles": 12, "output_text": "The image depicts a group of young tennis players and a woman standing on a tennis court. The court is surrounded by a green fence, and the players are dressed in athletic attire, including white shirts, red caps, and various sports shoes. The lighting is bright, indicating a sunny day, and the shadows cast by the trees on the court add depth to the scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25728.7, "ram_available_mb": 100043.5, "ram_percent": 20.5}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25729.5, "ram_available_mb": 100042.7, "ram_percent": 20.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.29, "peak": 14.83, "min": 13.82}, "VDD_GPU": {"avg": 29.32, "peak": 38.23, "min": 23.64}, "VIN": {"avg": 67.08, "peak": 103.08, "min": 53.99}, "VDD_CPU_SOC_MSS": {"avg": 14.63, "peak": 15.36, "min": 14.18}}, "power_watts_avg": 29.32, "energy_joules_est": 134.55, "sample_count": 36, "duration_seconds": 4.589}, "timestamp": "2026-01-16T15:09:32.637928"}
{"image_index": 10, "image_name": "000000001268.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001268.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1216.042, "latencies_ms": [1216.042], "images_per_second": 0.822, "prompt_tokens": 9, "response_tokens_est": 19, "n_tiles": 6, "output_text": "A woman is taking a photo of a white bird standing in the water near a bridge.", "error": null, "sys_before": {"cpu_percent": 39.1, "ram_used_mb": 25729.5, "ram_available_mb": 100042.7, "ram_percent": 20.5}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25725.8, "ram_available_mb": 100046.4, "ram_percent": 20.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.43, "peak": 14.0, "min": 12.9}, "VDD_GPU": {"avg": 30.91, "peak": 35.86, "min": 25.61}, "VIN": {"avg": 68.49, "peak": 111.52, "min": 50.7}, "VDD_CPU_SOC_MSS": {"avg": 13.87, "peak": 14.18, "min": 13.39}}, "power_watts_avg": 30.91, "energy_joules_est": 37.6, "sample_count": 9, "duration_seconds": 1.216}, "timestamp": "2026-01-16T15:09:33.920926"}
{"image_index": 10, "image_name": "000000001268.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001268.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1898.036, "latencies_ms": [1898.036], "images_per_second": 0.527, "prompt_tokens": 23, "response_tokens_est": 40, "n_tiles": 6, "output_text": "object: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25725.8, "ram_available_mb": 100046.4, "ram_percent": 20.5}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 25726.0, "ram_available_mb": 100046.1, "ram_percent": 20.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.01, "peak": 14.4, "min": 13.51}, "VDD_GPU": {"avg": 29.47, "peak": 36.65, "min": 22.86}, "VIN": {"avg": 65.86, "peak": 92.37, "min": 58.74}, "VDD_CPU_SOC_MSS": {"avg": 14.21, "peak": 14.57, "min": 13.78}}, "power_watts_avg": 29.47, "energy_joules_est": 55.95, "sample_count": 14, "duration_seconds": 1.898}, "timestamp": "2026-01-16T15:09:35.829604"}
{"image_index": 10, "image_name": "000000001268.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001268.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2022.315, "latencies_ms": [2022.315], "images_per_second": 0.494, "prompt_tokens": 27, "response_tokens_est": 44, "n_tiles": 6, "output_text": "The main objects in the image are a group of people sitting near a body of water, with a person taking a photo in the foreground. The water is in the background, and the people are in the foreground.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25726.0, "ram_available_mb": 100046.1, "ram_percent": 20.5}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25726.0, "ram_available_mb": 100046.1, "ram_percent": 20.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.1, "peak": 14.51, "min": 13.51}, "VDD_GPU": {"avg": 28.15, "peak": 35.86, "min": 22.08}, "VIN": {"avg": 65.86, "peak": 108.53, "min": 54.39}, "VDD_CPU_SOC_MSS": {"avg": 14.25, "peak": 14.57, "min": 13.78}}, "power_watts_avg": 28.15, "energy_joules_est": 56.94, "sample_count": 16, "duration_seconds": 2.023}, "timestamp": "2026-01-16T15:09:37.858406"}
{"image_index": 10, "image_name": "000000001268.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001268.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3197.515, "latencies_ms": [3197.515], "images_per_second": 0.313, "prompt_tokens": 21, "response_tokens_est": 83, "n_tiles": 6, "output_text": "The image depicts a serene riverside scene during twilight, with a group of people sitting on a stone bench by the water's edge. The setting is a riverside promenade, and the individuals are engaged in leisurely activities, with one person taking a photograph of the white swan. The lighting suggests it is either early morning or late afternoon, casting a warm glow over the scene.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25726.0, "ram_available_mb": 100046.1, "ram_percent": 20.5}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 25726.0, "ram_available_mb": 100046.1, "ram_percent": 20.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.27, "peak": 14.51, "min": 13.61}, "VDD_GPU": {"avg": 25.82, "peak": 35.86, "min": 21.67}, "VIN": {"avg": 64.41, "peak": 92.8, "min": 58.58}, "VDD_CPU_SOC_MSS": {"avg": 14.4, "peak": 14.57, "min": 13.79}}, "power_watts_avg": 25.82, "energy_joules_est": 82.57, "sample_count": 25, "duration_seconds": 3.198}, "timestamp": "2026-01-16T15:09:41.062378"}
{"image_index": 10, "image_name": "000000001268.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001268.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2746.481, "latencies_ms": [2746.481], "images_per_second": 0.364, "prompt_tokens": 19, "response_tokens_est": 68, "n_tiles": 6, "output_text": "The image features a serene scene with a woman taking a photograph of a white bird standing in a calm body of water. The lighting is soft and warm, casting a gentle glow on the scene. The woman is wearing a green dress and a brown shoulder bag, while the bird is white and appears to be in a peaceful state.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25726.0, "ram_available_mb": 100046.1, "ram_percent": 20.5}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 25726.0, "ram_available_mb": 100046.1, "ram_percent": 20.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.2, "peak": 14.51, "min": 13.61}, "VDD_GPU": {"avg": 26.31, "peak": 35.86, "min": 21.67}, "VIN": {"avg": 64.03, "peak": 97.8, "min": 52.56}, "VDD_CPU_SOC_MSS": {"avg": 14.37, "peak": 14.57, "min": 13.79}}, "power_watts_avg": 26.31, "energy_joules_est": 72.27, "sample_count": 21, "duration_seconds": 2.747}, "timestamp": "2026-01-16T15:09:43.815179"}
{"image_index": 11, "image_name": "000000001296.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001296.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1760.677, "latencies_ms": [1760.677], "images_per_second": 0.568, "prompt_tokens": 9, "response_tokens_est": 37, "n_tiles": 6, "output_text": "A woman is taking a selfie with a Hello Kitty phone case, wearing a white top with a black and gray pattern, and a green and yellow bracelet on her wrist.", "error": null, "sys_before": {"cpu_percent": 43.2, "ram_used_mb": 25725.8, "ram_available_mb": 100046.4, "ram_percent": 20.5}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 25725.8, "ram_available_mb": 100046.4, "ram_percent": 20.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.74, "peak": 14.4, "min": 13.11}, "VDD_GPU": {"avg": 28.5, "peak": 35.47, "min": 22.86}, "VIN": {"avg": 66.54, "peak": 103.66, "min": 54.51}, "VDD_CPU_SOC_MSS": {"avg": 14.0, "peak": 14.18, "min": 13.79}}, "power_watts_avg": 28.5, "energy_joules_est": 50.19, "sample_count": 13, "duration_seconds": 1.761}, "timestamp": "2026-01-16T15:09:45.639658"}
{"image_index": 11, "image_name": "000000001296.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001296.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2170.116, "latencies_ms": [2170.116], "images_per_second": 0.461, "prompt_tokens": 23, "response_tokens_est": 49, "n_tiles": 6, "output_text": "1. Phone (1)\n2. Phone (1)\n3. Phone (1)\n4. Phone (1)\n5. Phone (1)\n6. Phone (1)\n7. Phone (1)\n8. Phone (1)", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25725.8, "ram_available_mb": 100046.4, "ram_percent": 20.5}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 25726.0, "ram_available_mb": 100046.1, "ram_percent": 20.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.13, "peak": 14.51, "min": 13.61}, "VDD_GPU": {"avg": 28.11, "peak": 35.86, "min": 22.08}, "VIN": {"avg": 66.99, "peak": 101.06, "min": 54.96}, "VDD_CPU_SOC_MSS": {"avg": 14.25, "peak": 14.57, "min": 13.79}}, "power_watts_avg": 28.11, "energy_joules_est": 61.01, "sample_count": 16, "duration_seconds": 2.17}, "timestamp": "2026-01-16T15:09:47.816058"}
{"image_index": 11, "image_name": "000000001296.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001296.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2380.316, "latencies_ms": [2380.316], "images_per_second": 0.42, "prompt_tokens": 27, "response_tokens_est": 56, "n_tiles": 6, "output_text": "The main object in the foreground is a person holding a smartphone with a Hello Kitty sticker. The person's left hand is visible, and they are wearing a white sleeveless top. The background is blurred, but there are hints of other people and possibly a crowd.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25726.0, "ram_available_mb": 100046.1, "ram_percent": 20.5}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 25725.8, "ram_available_mb": 100046.4, "ram_percent": 20.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.18, "peak": 14.51, "min": 13.61}, "VDD_GPU": {"avg": 27.48, "peak": 35.86, "min": 22.08}, "VIN": {"avg": 66.61, "peak": 109.27, "min": 53.2}, "VDD_CPU_SOC_MSS": {"avg": 14.33, "peak": 14.57, "min": 13.79}}, "power_watts_avg": 27.48, "energy_joules_est": 65.42, "sample_count": 18, "duration_seconds": 2.381}, "timestamp": "2026-01-16T15:09:50.203589"}
{"image_index": 11, "image_name": "000000001296.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001296.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2503.85, "latencies_ms": [2503.85], "images_per_second": 0.399, "prompt_tokens": 21, "response_tokens_est": 60, "n_tiles": 6, "output_text": "The image depicts a young woman holding a smartphone, taking a selfie. She is wearing a white top with a graphic design and has a green and yellow bracelet on her wrist. The background is blurred, but it appears to be an outdoor setting with other people and possibly a crowd.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25725.8, "ram_available_mb": 100046.4, "ram_percent": 20.5}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 25726.0, "ram_available_mb": 100046.2, "ram_percent": 20.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.19, "peak": 14.51, "min": 13.61}, "VDD_GPU": {"avg": 26.89, "peak": 35.47, "min": 21.68}, "VIN": {"avg": 66.08, "peak": 94.7, "min": 60.3}, "VDD_CPU_SOC_MSS": {"avg": 14.35, "peak": 14.57, "min": 13.79}}, "power_watts_avg": 26.89, "energy_joules_est": 67.34, "sample_count": 19, "duration_seconds": 2.504}, "timestamp": "2026-01-16T15:09:52.713411"}
{"image_index": 11, "image_name": "000000001296.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001296.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2496.63, "latencies_ms": [2496.63], "images_per_second": 0.401, "prompt_tokens": 19, "response_tokens_est": 60, "n_tiles": 6, "output_text": "The notable visual attributes of the image include the woman's black hair, the white and pink Hello Kitty phone case, her green and yellow chunky bracelet, and her white sleeveless top. The lighting is bright, and the background is blurred, suggesting an outdoor setting with natural light.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25726.0, "ram_available_mb": 100046.2, "ram_percent": 20.5}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 25726.0, "ram_available_mb": 100046.2, "ram_percent": 20.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.21, "peak": 14.51, "min": 13.61}, "VDD_GPU": {"avg": 26.99, "peak": 35.86, "min": 21.68}, "VIN": {"avg": 64.66, "peak": 82.62, "min": 59.38}, "VDD_CPU_SOC_MSS": {"avg": 14.37, "peak": 14.57, "min": 13.79}}, "power_watts_avg": 26.99, "energy_joules_est": 67.4, "sample_count": 19, "duration_seconds": 2.497}, "timestamp": "2026-01-16T15:09:55.216303"}
{"image_index": 12, "image_name": "000000001353.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001353.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2446.517, "latencies_ms": [2446.517], "images_per_second": 0.409, "prompt_tokens": 9, "response_tokens_est": 23, "n_tiles": 12, "output_text": "A group of children are sitting in a red and white train carriage, enjoying a ride on a vintage train.", "error": null, "sys_before": {"cpu_percent": 46.1, "ram_used_mb": 25726.0, "ram_available_mb": 100046.2, "ram_percent": 20.5}, "sys_after": {"cpu_percent": 2.5, "ram_used_mb": 25726.0, "ram_available_mb": 100046.2, "ram_percent": 20.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.3, "peak": 15.24, "min": 12.98}, "VDD_GPU": {"avg": 32.11, "peak": 37.04, "min": 25.23}, "VIN": {"avg": 71.32, "peak": 103.86, "min": 55.34}, "VDD_CPU_SOC_MSS": {"avg": 14.8, "peak": 15.75, "min": 13.79}}, "power_watts_avg": 32.11, "energy_joules_est": 78.58, "sample_count": 19, "duration_seconds": 2.447}, "timestamp": "2026-01-16T15:09:57.757858"}
{"image_index": 12, "image_name": "000000001353.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001353.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 3221.938, "latencies_ms": [3221.938], "images_per_second": 0.31, "prompt_tokens": 23, "response_tokens_est": 40, "n_tiles": 12, "output_text": "object: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25726.0, "ram_available_mb": 100046.2, "ram_percent": 20.5}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 25726.0, "ram_available_mb": 100046.2, "ram_percent": 20.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.18, "peak": 14.83, "min": 13.72}, "VDD_GPU": {"avg": 31.67, "peak": 38.62, "min": 24.04}, "VIN": {"avg": 65.56, "peak": 120.43, "min": 51.4}, "VDD_CPU_SOC_MSS": {"avg": 14.51, "peak": 15.36, "min": 13.79}}, "power_watts_avg": 31.67, "energy_joules_est": 102.05, "sample_count": 25, "duration_seconds": 3.222}, "timestamp": "2026-01-16T15:10:00.986150"}
{"image_index": 12, "image_name": "000000001353.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001353.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 4336.29, "latencies_ms": [4336.29], "images_per_second": 0.231, "prompt_tokens": 27, "response_tokens_est": 69, "n_tiles": 12, "output_text": "The main object in the foreground is a red and white toy train, which is positioned near the center of the image. The background features a wooden wall and a black speaker, which are slightly out of focus. The train is located near the center of the image, with the speaker positioned to the left and the wooden wall to the right.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25726.0, "ram_available_mb": 100046.2, "ram_percent": 20.5}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25725.7, "ram_available_mb": 100046.4, "ram_percent": 20.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.97, "peak": 14.63, "min": 13.39}, "VDD_GPU": {"avg": 29.62, "peak": 38.6, "min": 23.64}, "VIN": {"avg": 72.21, "peak": 109.07, "min": 55.39}, "VDD_CPU_SOC_MSS": {"avg": 14.09, "peak": 14.96, "min": 13.78}}, "power_watts_avg": 29.62, "energy_joules_est": 128.45, "sample_count": 34, "duration_seconds": 4.337}, "timestamp": "2026-01-16T15:10:05.328733"}
{"image_index": 12, "image_name": "000000001353.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001353.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 4217.442, "latencies_ms": [4217.442], "images_per_second": 0.237, "prompt_tokens": 21, "response_tokens_est": 66, "n_tiles": 12, "output_text": "The image depicts a group of children sitting in a small, red and white train carriage, which is situated on a wooden floor. The children appear to be enjoying their time, with some looking towards the camera and others engaged in conversation. The setting suggests a playful and educational environment, possibly a museum or a themed attraction.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25725.7, "ram_available_mb": 100046.4, "ram_percent": 20.5}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25725.7, "ram_available_mb": 100046.4, "ram_percent": 20.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.95, "peak": 14.53, "min": 13.29}, "VDD_GPU": {"avg": 29.52, "peak": 38.21, "min": 23.64}, "VIN": {"avg": 65.0, "peak": 94.68, "min": 52.17}, "VDD_CPU_SOC_MSS": {"avg": 14.13, "peak": 14.96, "min": 13.39}}, "power_watts_avg": 29.52, "energy_joules_est": 124.51, "sample_count": 33, "duration_seconds": 4.218}, "timestamp": "2026-01-16T15:10:09.552528"}
{"image_index": 12, "image_name": "000000001353.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001353.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3134.653, "latencies_ms": [3134.653], "images_per_second": 0.319, "prompt_tokens": 19, "response_tokens_est": 38, "n_tiles": 12, "output_text": "The image features a vibrant red and white carousel with a shiny, glossy finish. The lighting is warm and ambient, casting a soft glow on the carousel and creating a cozy atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25725.7, "ram_available_mb": 100046.4, "ram_percent": 20.5}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 25726.0, "ram_available_mb": 100046.2, "ram_percent": 20.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.94, "peak": 14.53, "min": 13.29}, "VDD_GPU": {"avg": 31.74, "peak": 37.83, "min": 24.05}, "VIN": {"avg": 66.4, "peak": 120.95, "min": 55.51}, "VDD_CPU_SOC_MSS": {"avg": 14.19, "peak": 14.96, "min": 13.78}}, "power_watts_avg": 31.74, "energy_joules_est": 99.5, "sample_count": 24, "duration_seconds": 3.135}, "timestamp": "2026-01-16T15:10:12.694156"}
{"image_index": 13, "image_name": "000000001425.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001425.jpg", "image_width": 640, "image_height": 512, "image_resolution": "640x512", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2997.806, "latencies_ms": [2997.806], "images_per_second": 0.334, "prompt_tokens": 9, "response_tokens_est": 38, "n_tiles": 12, "output_text": "The image shows a close-up of a slice of bread with a dark filling, possibly a dessert or a savory dish, placed on a white plate, with a blurred background.", "error": null, "sys_before": {"cpu_percent": 40.4, "ram_used_mb": 25726.0, "ram_available_mb": 100046.2, "ram_percent": 20.5}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 25726.0, "ram_available_mb": 100046.2, "ram_percent": 20.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.28, "peak": 15.24, "min": 12.78}, "VDD_GPU": {"avg": 31.08, "peak": 37.42, "min": 24.43}, "VIN": {"avg": 70.56, "peak": 94.85, "min": 52.54}, "VDD_CPU_SOC_MSS": {"avg": 14.78, "peak": 16.14, "min": 13.39}}, "power_watts_avg": 31.08, "energy_joules_est": 93.19, "sample_count": 23, "duration_seconds": 2.998}, "timestamp": "2026-01-16T15:10:15.779072"}
{"image_index": 13, "image_name": "000000001425.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001425.jpg", "image_width": 640, "image_height": 512, "image_resolution": "640x512", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 3204.596, "latencies_ms": [3204.596], "images_per_second": 0.312, "prompt_tokens": 23, "response_tokens_est": 40, "n_tiles": 12, "output_text": "object: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25726.0, "ram_available_mb": 100046.2, "ram_percent": 20.5}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 25726.2, "ram_available_mb": 100045.9, "ram_percent": 20.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.29, "peak": 14.83, "min": 13.72}, "VDD_GPU": {"avg": 31.95, "peak": 38.62, "min": 24.43}, "VIN": {"avg": 76.51, "peak": 106.38, "min": 61.34}, "VDD_CPU_SOC_MSS": {"avg": 14.62, "peak": 15.36, "min": 14.18}}, "power_watts_avg": 31.95, "energy_joules_est": 102.4, "sample_count": 24, "duration_seconds": 3.205}, "timestamp": "2026-01-16T15:10:18.991071"}
{"image_index": 13, "image_name": "000000001425.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001425.jpg", "image_width": 640, "image_height": 512, "image_resolution": "640x512", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 4064.844, "latencies_ms": [4064.844], "images_per_second": 0.246, "prompt_tokens": 27, "response_tokens_est": 63, "n_tiles": 12, "output_text": "The main object in the foreground is a sandwich, which is placed on a white plate. The background features a blurred object, possibly a cup or container, which is out of focus. The sandwich is positioned near the center of the image, with the blurred object slightly to the right and behind it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25726.2, "ram_available_mb": 100045.9, "ram_percent": 20.5}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 25726.0, "ram_available_mb": 100046.2, "ram_percent": 20.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.29, "peak": 14.83, "min": 13.82}, "VDD_GPU": {"avg": 30.18, "peak": 38.21, "min": 24.04}, "VIN": {"avg": 65.54, "peak": 101.89, "min": 55.75}, "VDD_CPU_SOC_MSS": {"avg": 14.62, "peak": 15.36, "min": 14.18}}, "power_watts_avg": 30.18, "energy_joules_est": 122.69, "sample_count": 31, "duration_seconds": 4.065}, "timestamp": "2026-01-16T15:10:23.062425"}
{"image_index": 13, "image_name": "000000001425.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001425.jpg", "image_width": 640, "image_height": 512, "image_resolution": "640x512", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3734.779, "latencies_ms": [3734.779], "images_per_second": 0.268, "prompt_tokens": 21, "response_tokens_est": 54, "n_tiles": 12, "output_text": "The image depicts a close-up of a slice of bread with a dark filling, possibly a dessert or a savory dish, placed on a white plate. The background is blurred, with indistinct shapes and colors, suggesting a shallow depth of field.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25726.0, "ram_available_mb": 100046.2, "ram_percent": 20.5}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 25726.0, "ram_available_mb": 100046.2, "ram_percent": 20.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.27, "peak": 14.83, "min": 13.59}, "VDD_GPU": {"avg": 30.36, "peak": 38.21, "min": 24.04}, "VIN": {"avg": 69.55, "peak": 104.81, "min": 55.59}, "VDD_CPU_SOC_MSS": {"avg": 14.59, "peak": 15.36, "min": 13.79}}, "power_watts_avg": 30.36, "energy_joules_est": 113.4, "sample_count": 29, "duration_seconds": 3.735}, "timestamp": "2026-01-16T15:10:26.803894"}
{"image_index": 13, "image_name": "000000001425.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001425.jpg", "image_width": 640, "image_height": 512, "image_resolution": "640x512", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3921.946, "latencies_ms": [3921.946], "images_per_second": 0.255, "prompt_tokens": 19, "response_tokens_est": 59, "n_tiles": 12, "output_text": "The image is a black and white photograph featuring a close-up of a sandwich with a white, fluffy bread base. The lighting is soft and diffused, creating a warm and inviting atmosphere. The background is blurred, drawing attention to the sandwich and emphasizing its texture and details.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25726.0, "ram_available_mb": 100046.2, "ram_percent": 20.5}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25726.0, "ram_available_mb": 100046.2, "ram_percent": 20.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.29, "peak": 14.94, "min": 13.59}, "VDD_GPU": {"avg": 30.14, "peak": 38.21, "min": 24.04}, "VIN": {"avg": 73.24, "peak": 104.49, "min": 61.35}, "VDD_CPU_SOC_MSS": {"avg": 14.62, "peak": 15.36, "min": 14.18}}, "power_watts_avg": 30.14, "energy_joules_est": 118.22, "sample_count": 31, "duration_seconds": 3.922}, "timestamp": "2026-01-16T15:10:30.732645"}
{"image_index": 14, "image_name": "000000001490.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001490.jpg", "image_width": 640, "image_height": 315, "image_resolution": "640x315", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 694.443, "latencies_ms": [694.443], "images_per_second": 1.44, "prompt_tokens": 9, "response_tokens_est": 19, "n_tiles": 2, "output_text": "A person is standing on a paddleboard in the water, paddling with a paddle.", "error": null, "sys_before": {"cpu_percent": 35.1, "ram_used_mb": 25726.0, "ram_available_mb": 100046.2, "ram_percent": 20.5}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25725.9, "ram_available_mb": 100046.2, "ram_percent": 20.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5055.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.51, "peak": 14.0, "min": 13.09}, "VDD_GPU": {"avg": 26.48, "peak": 29.56, "min": 22.86}, "VIN": {"avg": 68.23, "peak": 111.71, "min": 56.36}, "VDD_CPU_SOC_MSS": {"avg": 13.79, "peak": 14.18, "min": 13.39}}, "power_watts_avg": 26.48, "energy_joules_est": 18.4, "sample_count": 5, "duration_seconds": 0.695}, "timestamp": "2026-01-16T15:10:31.457534"}
{"image_index": 14, "image_name": "000000001490.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001490.jpg", "image_width": 640, "image_height": 315, "image_resolution": "640x315", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1173.371, "latencies_ms": [1173.371], "images_per_second": 0.852, "prompt_tokens": 23, "response_tokens_est": 37, "n_tiles": 2, "output_text": "1. Paddleboard\n2. Person\n3. Water\n4. Surfboard\n5. Water\n6. Paddle\n7. Water\n8. Person", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25725.9, "ram_available_mb": 100046.2, "ram_percent": 20.5}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 25725.9, "ram_available_mb": 100046.2, "ram_percent": 20.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5064.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.02, "peak": 14.4, "min": 13.49}, "VDD_GPU": {"avg": 24.13, "peak": 30.74, "min": 20.1}, "VIN": {"avg": 61.67, "peak": 94.61, "min": 49.49}, "VDD_CPU_SOC_MSS": {"avg": 14.23, "peak": 14.58, "min": 13.79}}, "power_watts_avg": 24.13, "energy_joules_est": 28.32, "sample_count": 9, "duration_seconds": 1.174}, "timestamp": "2026-01-16T15:10:32.637273"}
{"image_index": 14, "image_name": "000000001490.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001490.jpg", "image_width": 640, "image_height": 315, "image_resolution": "640x315", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2395.119, "latencies_ms": [2395.119], "images_per_second": 0.418, "prompt_tokens": 27, "response_tokens_est": 83, "n_tiles": 2, "output_text": "The main object in the foreground is a person standing on a paddleboard, holding a paddle. The person is wearing a wetsuit and is positioned near the center of the image. In the background, there is a body of water extending to the horizon, with a line of trees and a distant shoreline visible. The paddleboard is positioned in the middle ground, slightly to the left of the person.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25725.9, "ram_available_mb": 100046.2, "ram_percent": 20.5}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25726.2, "ram_available_mb": 100046.0, "ram_percent": 20.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5067.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.33, "peak": 14.61, "min": 13.69}, "VDD_GPU": {"avg": 21.24, "peak": 29.16, "min": 18.91}, "VIN": {"avg": 62.01, "peak": 87.9, "min": 57.38}, "VDD_CPU_SOC_MSS": {"avg": 14.44, "peak": 14.57, "min": 13.79}}, "power_watts_avg": 21.24, "energy_joules_est": 50.88, "sample_count": 18, "duration_seconds": 2.395}, "timestamp": "2026-01-16T15:10:35.038115"}
{"image_index": 14, "image_name": "000000001490.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001490.jpg", "image_width": 640, "image_height": 315, "image_resolution": "640x315", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1711.884, "latencies_ms": [1711.884], "images_per_second": 0.584, "prompt_tokens": 21, "response_tokens_est": 58, "n_tiles": 2, "output_text": "The image depicts a person standing on a paddleboard in a body of water, likely a lake or sea, with a calm and expansive horizon. The individual is dressed in a wetsuit and is holding a paddle, suggesting they are engaged in water sports or a leisure activity.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25726.2, "ram_available_mb": 100046.0, "ram_percent": 20.5}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25725.9, "ram_available_mb": 100046.3, "ram_percent": 20.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5063.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.33, "peak": 14.51, "min": 13.9}, "VDD_GPU": {"avg": 22.01, "peak": 27.98, "min": 18.92}, "VIN": {"avg": 60.98, "peak": 90.65, "min": 56.26}, "VDD_CPU_SOC_MSS": {"avg": 14.51, "peak": 14.97, "min": 14.18}}, "power_watts_avg": 22.01, "energy_joules_est": 37.69, "sample_count": 13, "duration_seconds": 1.712}, "timestamp": "2026-01-16T15:10:36.755791"}
{"image_index": 14, "image_name": "000000001490.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001490.jpg", "image_width": 640, "image_height": 315, "image_resolution": "640x315", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1856.184, "latencies_ms": [1856.184], "images_per_second": 0.539, "prompt_tokens": 19, "response_tokens_est": 63, "n_tiles": 2, "output_text": "The image depicts a black and white scene of a person standing on a paddleboard in a body of water. The individual is wearing a wetsuit and holding a paddle, suggesting they are engaged in water sports. The lighting is natural, indicating daytime, and the water appears calm with minimal ripples.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25725.9, "ram_available_mb": 100046.3, "ram_percent": 20.5}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25725.9, "ram_available_mb": 100046.3, "ram_percent": 20.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5062.6, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.27, "peak": 14.51, "min": 13.69}, "VDD_GPU": {"avg": 21.87, "peak": 27.98, "min": 18.91}, "VIN": {"avg": 61.48, "peak": 90.86, "min": 55.26}, "VDD_CPU_SOC_MSS": {"avg": 14.43, "peak": 14.57, "min": 14.18}}, "power_watts_avg": 21.87, "energy_joules_est": 40.6, "sample_count": 14, "duration_seconds": 1.857}, "timestamp": "2026-01-16T15:10:38.617817"}
{"image_index": 15, "image_name": "000000001503.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001503.jpg", "image_width": 320, "image_height": 240, "image_resolution": "320x240", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 3004.589, "latencies_ms": [3004.589], "images_per_second": 0.333, "prompt_tokens": 9, "response_tokens_est": 38, "n_tiles": 12, "output_text": "The image shows a neatly arranged desk with a laptop, a white mouse, a keyboard, and a small figurine on the right side, all placed on a light-colored surface.", "error": null, "sys_before": {"cpu_percent": 48.4, "ram_used_mb": 25725.9, "ram_available_mb": 100046.3, "ram_percent": 20.5}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 25725.9, "ram_available_mb": 100046.3, "ram_percent": 20.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.37, "peak": 15.24, "min": 13.29}, "VDD_GPU": {"avg": 30.74, "peak": 36.65, "min": 24.43}, "VIN": {"avg": 75.73, "peak": 128.64, "min": 56.29}, "VDD_CPU_SOC_MSS": {"avg": 14.85, "peak": 15.75, "min": 14.18}}, "power_watts_avg": 30.74, "energy_joules_est": 92.38, "sample_count": 23, "duration_seconds": 3.005}, "timestamp": "2026-01-16T15:10:41.691435"}
{"image_index": 15, "image_name": "000000001503.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001503.jpg", "image_width": 320, "image_height": 240, "image_resolution": "320x240", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2712.307, "latencies_ms": [2712.307], "images_per_second": 0.369, "prompt_tokens": 23, "response_tokens_est": 27, "n_tiles": 12, "output_text": "laptop: 1\nkeyboard: 1\nmouse: 1\nprinter: 1\ndesk: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25725.9, "ram_available_mb": 100046.3, "ram_percent": 20.5}, "sys_after": {"cpu_percent": 2.6, "ram_used_mb": 25725.9, "ram_available_mb": 100046.3, "ram_percent": 20.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.26, "peak": 14.83, "min": 13.59}, "VDD_GPU": {"avg": 32.67, "peak": 37.82, "min": 25.22}, "VIN": {"avg": 65.25, "peak": 84.55, "min": 55.63}, "VDD_CPU_SOC_MSS": {"avg": 14.7, "peak": 15.36, "min": 14.18}}, "power_watts_avg": 32.67, "energy_joules_est": 88.63, "sample_count": 21, "duration_seconds": 2.713}, "timestamp": "2026-01-16T15:10:44.410278"}
{"image_index": 15, "image_name": "000000001503.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001503.jpg", "image_width": 320, "image_height": 240, "image_resolution": "320x240", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 4072.913, "latencies_ms": [4072.913], "images_per_second": 0.246, "prompt_tokens": 27, "response_tokens_est": 63, "n_tiles": 12, "output_text": "The main objects in the image are a laptop, a mouse, and a keyboard. The laptop is positioned on the left side of the image, with the mouse and keyboard placed in front of it. The laptop is the closest object to the viewer, while the mouse and keyboard are further back in the scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25725.9, "ram_available_mb": 100046.3, "ram_percent": 20.5}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25726.2, "ram_available_mb": 100046.0, "ram_percent": 20.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.3, "peak": 14.94, "min": 13.59}, "VDD_GPU": {"avg": 30.19, "peak": 38.6, "min": 24.04}, "VIN": {"avg": 65.42, "peak": 96.51, "min": 55.62}, "VDD_CPU_SOC_MSS": {"avg": 14.64, "peak": 15.36, "min": 14.18}}, "power_watts_avg": 30.19, "energy_joules_est": 122.99, "sample_count": 31, "duration_seconds": 4.074}, "timestamp": "2026-01-16T15:10:48.490101"}
{"image_index": 15, "image_name": "000000001503.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001503.jpg", "image_width": 320, "image_height": 240, "image_resolution": "320x240", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3844.022, "latencies_ms": [3844.022], "images_per_second": 0.26, "prompt_tokens": 21, "response_tokens_est": 57, "n_tiles": 12, "output_text": "The image shows a cluttered desk with a laptop, a white mouse, a keyboard, and a small figurine on top of the desk. The setting appears to be an office or a study room, with a window in the background allowing natural light to illuminate the workspace.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25726.2, "ram_available_mb": 100046.0, "ram_percent": 20.5}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25725.9, "ram_available_mb": 100046.3, "ram_percent": 20.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.33, "peak": 14.94, "min": 13.69}, "VDD_GPU": {"avg": 30.3, "peak": 38.21, "min": 24.04}, "VIN": {"avg": 75.36, "peak": 106.94, "min": 56.77}, "VDD_CPU_SOC_MSS": {"avg": 14.65, "peak": 15.36, "min": 14.18}}, "power_watts_avg": 30.3, "energy_joules_est": 116.48, "sample_count": 30, "duration_seconds": 3.844}, "timestamp": "2026-01-16T15:10:52.340429"}
{"image_index": 15, "image_name": "000000001503.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001503.jpg", "image_width": 320, "image_height": 240, "image_resolution": "320x240", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3163.109, "latencies_ms": [3163.109], "images_per_second": 0.316, "prompt_tokens": 19, "response_tokens_est": 39, "n_tiles": 12, "output_text": "The image shows a desk with a white computer monitor, keyboard, and mouse, all placed on a light-colored surface. The lighting is soft and ambient, creating a calm and tidy workspace.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25725.9, "ram_available_mb": 100046.3, "ram_percent": 20.5}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 25725.9, "ram_available_mb": 100046.3, "ram_percent": 20.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.29, "peak": 14.94, "min": 13.61}, "VDD_GPU": {"avg": 31.77, "peak": 37.83, "min": 24.44}, "VIN": {"avg": 73.14, "peak": 101.8, "min": 62.61}, "VDD_CPU_SOC_MSS": {"avg": 14.65, "peak": 15.36, "min": 14.18}}, "power_watts_avg": 31.77, "energy_joules_est": 100.51, "sample_count": 24, "duration_seconds": 3.164}, "timestamp": "2026-01-16T15:10:55.511273"}
{"image_index": 16, "image_name": "000000001532.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001532.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 3152.367, "latencies_ms": [3152.367], "images_per_second": 0.317, "prompt_tokens": 9, "response_tokens_est": 42, "n_tiles": 12, "output_text": "The image shows a busy highway with multiple vehicles, including a taxi and a black SUV, traveling under a bridge with green directional signs indicating directions to \"North Ventura\" and \"Sunset Blvd.\"", "error": null, "sys_before": {"cpu_percent": 43.2, "ram_used_mb": 25725.9, "ram_available_mb": 100046.3, "ram_percent": 20.5}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 25725.9, "ram_available_mb": 100046.3, "ram_percent": 20.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.31, "peak": 15.24, "min": 12.88}, "VDD_GPU": {"avg": 30.85, "peak": 37.83, "min": 24.43}, "VIN": {"avg": 70.9, "peak": 133.71, "min": 52.47}, "VDD_CPU_SOC_MSS": {"avg": 14.75, "peak": 15.75, "min": 13.79}}, "power_watts_avg": 30.85, "energy_joules_est": 97.26, "sample_count": 24, "duration_seconds": 3.153}, "timestamp": "2026-01-16T15:10:58.762127"}
{"image_index": 16, "image_name": "000000001532.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001532.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2959.935, "latencies_ms": [2959.935], "images_per_second": 0.338, "prompt_tokens": 23, "response_tokens_est": 33, "n_tiles": 12, "output_text": "1. Taxi\n2. SUV\n3. Van\n4. Car\n5. Car\n6. Car\n7. Car\n8. Car", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25725.9, "ram_available_mb": 100046.3, "ram_percent": 20.5}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 25725.9, "ram_available_mb": 100046.3, "ram_percent": 20.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.12, "peak": 14.83, "min": 13.51}, "VDD_GPU": {"avg": 32.4, "peak": 38.6, "min": 24.43}, "VIN": {"avg": 72.07, "peak": 100.11, "min": 49.21}, "VDD_CPU_SOC_MSS": {"avg": 14.44, "peak": 15.36, "min": 13.78}}, "power_watts_avg": 32.4, "energy_joules_est": 95.92, "sample_count": 22, "duration_seconds": 2.96}, "timestamp": "2026-01-16T15:11:01.728762"}
{"image_index": 16, "image_name": "000000001532.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001532.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 4495.15, "latencies_ms": [4495.15], "images_per_second": 0.222, "prompt_tokens": 27, "response_tokens_est": 73, "n_tiles": 12, "output_text": "The main objects in the image are a highway sign, a car, and a van. The highway sign is positioned in the foreground, with the car and van positioned in the background. The van is closer to the camera, while the car is further away. The highway sign is near the top of the image, while the van is closer to the bottom.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25725.9, "ram_available_mb": 100046.3, "ram_percent": 20.5}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25725.9, "ram_available_mb": 100046.3, "ram_percent": 20.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.0, "peak": 14.63, "min": 13.39}, "VDD_GPU": {"avg": 29.53, "peak": 38.62, "min": 23.64}, "VIN": {"avg": 69.65, "peak": 108.08, "min": 50.43}, "VDD_CPU_SOC_MSS": {"avg": 14.26, "peak": 14.96, "min": 13.78}}, "power_watts_avg": 29.53, "energy_joules_est": 132.75, "sample_count": 34, "duration_seconds": 4.495}, "timestamp": "2026-01-16T15:11:06.231242"}
{"image_index": 16, "image_name": "000000001532.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001532.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 4643.505, "latencies_ms": [4643.505], "images_per_second": 0.215, "prompt_tokens": 21, "response_tokens_est": 77, "n_tiles": 12, "output_text": "The image depicts a busy highway scene under a bridge with multiple green directional signs indicating directions to various locations such as \"North Ventura\" and \"Hollywood Blvd.\" The setting appears to be in a city, with trees and buildings visible in the background. The scene is bustling with traffic, including a taxi and several cars, suggesting a typical day on the road.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25725.9, "ram_available_mb": 100046.3, "ram_percent": 20.5}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25725.9, "ram_available_mb": 100046.3, "ram_percent": 20.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.99, "peak": 14.53, "min": 13.39}, "VDD_GPU": {"avg": 29.01, "peak": 38.21, "min": 23.64}, "VIN": {"avg": 62.69, "peak": 94.5, "min": 50.15}, "VDD_CPU_SOC_MSS": {"avg": 14.25, "peak": 14.96, "min": 13.78}}, "power_watts_avg": 29.01, "energy_joules_est": 134.72, "sample_count": 37, "duration_seconds": 4.644}, "timestamp": "2026-01-16T15:11:10.881546"}
{"image_index": 16, "image_name": "000000001532.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001532.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3717.311, "latencies_ms": [3717.311], "images_per_second": 0.269, "prompt_tokens": 19, "response_tokens_est": 56, "n_tiles": 12, "output_text": "The image depicts a sunny day on a highway with a clear blue sky. The road is covered with a green overhead sign indicating directions to \"North Ventura\" and \"Sunset Blvd.\" The vehicles are moving, and there are trees and buildings visible in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25725.9, "ram_available_mb": 100046.3, "ram_percent": 20.5}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 25726.1, "ram_available_mb": 100046.1, "ram_percent": 20.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.04, "peak": 14.63, "min": 13.39}, "VDD_GPU": {"avg": 30.72, "peak": 37.82, "min": 23.64}, "VIN": {"avg": 70.01, "peak": 124.33, "min": 55.75}, "VDD_CPU_SOC_MSS": {"avg": 14.45, "peak": 15.36, "min": 13.78}}, "power_watts_avg": 30.72, "energy_joules_est": 114.21, "sample_count": 29, "duration_seconds": 3.718}, "timestamp": "2026-01-16T15:11:14.605393"}
{"image_index": 17, "image_name": "000000001584.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001584.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2261.149, "latencies_ms": [2261.149], "images_per_second": 0.442, "prompt_tokens": 9, "response_tokens_est": 95, "n_tiles": 1, "output_text": "The image depicts a vibrant red double-decker bus, marked with the number 15, traveling on a city street. The bus is adorned with various advertisements and signage, including one that reads \"Metrobus\" and \"Always,\" indicating it is part of the London Metrobus network. The bus is in motion, with its front facing the viewer, and it is surrounded by a bustling urban environment with other vehicles and pedestrians visible in the background.", "error": null, "sys_before": {"cpu_percent": 19.4, "ram_used_mb": 25726.1, "ram_available_mb": 100046.1, "ram_percent": 20.5}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25726.4, "ram_available_mb": 100045.8, "ram_percent": 20.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 4845.7, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.7, "peak": 15.22, "min": 13.59}, "VDD_GPU": {"avg": 20.28, "peak": 25.61, "min": 19.31}, "VIN": {"avg": 58.76, "peak": 66.86, "min": 50.36}, "VDD_CPU_SOC_MSS": {"avg": 15.36, "peak": 16.15, "min": 13.78}}, "power_watts_avg": 20.28, "energy_joules_est": 45.87, "sample_count": 18, "duration_seconds": 2.262}, "timestamp": "2026-01-16T15:11:16.892673"}
{"image_index": 17, "image_name": "000000001584.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001584.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1699.19, "latencies_ms": [1699.19], "images_per_second": 0.589, "prompt_tokens": 23, "response_tokens_est": 75, "n_tiles": 1, "output_text": "1. Red double-decker bus: 1\n2. Bus number: 15\n3. Bus destination: Aldwych\n4. Bus route: 15\n5. Bus route sign: Aldwych\n6. Bus destination sign: Aldwych\n7. Bus route sign: 15\n8. Bus destination sign: Aldwych", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25726.4, "ram_available_mb": 100045.8, "ram_percent": 20.5}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25726.1, "ram_available_mb": 100046.1, "ram_percent": 20.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 4856.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.11, "peak": 15.22, "min": 14.81}, "VDD_GPU": {"avg": 20.56, "peak": 23.64, "min": 19.32}, "VIN": {"avg": 60.97, "peak": 63.31, "min": 59.12}, "VDD_CPU_SOC_MSS": {"avg": 16.08, "peak": 16.15, "min": 15.75}}, "power_watts_avg": 20.56, "energy_joules_est": 34.94, "sample_count": 13, "duration_seconds": 1.7}, "timestamp": "2026-01-16T15:11:18.600019"}
{"image_index": 17, "image_name": "000000001584.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001584.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1280.008, "latencies_ms": [1280.008], "images_per_second": 0.781, "prompt_tokens": 27, "response_tokens_est": 56, "n_tiles": 1, "output_text": "The main object in the image is a red double-decker bus, which is in the foreground. The bus is moving on a road, and there are other vehicles in the background. The bus is near the center of the image, and the background features buildings and trees.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25726.1, "ram_available_mb": 100046.1, "ram_percent": 20.5}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25726.4, "ram_available_mb": 100045.8, "ram_percent": 20.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 4859.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.07, "peak": 15.32, "min": 14.71}, "VDD_GPU": {"avg": 20.97, "peak": 23.64, "min": 19.7}, "VIN": {"avg": 61.15, "peak": 65.21, "min": 55.12}, "VDD_CPU_SOC_MSS": {"avg": 16.23, "peak": 16.54, "min": 15.75}}, "power_watts_avg": 20.97, "energy_joules_est": 26.85, "sample_count": 10, "duration_seconds": 1.28}, "timestamp": "2026-01-16T15:11:19.885376"}
{"image_index": 17, "image_name": "000000001584.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001584.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1538.504, "latencies_ms": [1538.504], "images_per_second": 0.65, "prompt_tokens": 21, "response_tokens_est": 68, "n_tiles": 1, "output_text": "The image depicts a vibrant red double-decker bus in motion on a city street, with a clear blue sky overhead. The bus is adorned with various advertisements and has the number \"15\" displayed on its front. In the background, there are buildings, trees, and a few pedestrians, suggesting a bustling urban environment.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25726.4, "ram_available_mb": 100045.8, "ram_percent": 20.5}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25726.1, "ram_available_mb": 100046.1, "ram_percent": 20.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 4854.7, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.1, "peak": 15.22, "min": 14.81}, "VDD_GPU": {"avg": 20.93, "peak": 23.64, "min": 19.71}, "VIN": {"avg": 62.49, "peak": 69.47, "min": 57.69}, "VDD_CPU_SOC_MSS": {"avg": 16.22, "peak": 16.54, "min": 15.75}}, "power_watts_avg": 20.93, "energy_joules_est": 32.21, "sample_count": 11, "duration_seconds": 1.539}, "timestamp": "2026-01-16T15:11:21.433711"}
{"image_index": 17, "image_name": "000000001584.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001584.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1300.182, "latencies_ms": [1300.182], "images_per_second": 0.769, "prompt_tokens": 19, "response_tokens_est": 57, "n_tiles": 1, "output_text": "The red double-decker bus in the image has a striking color contrast against the overcast sky, with its vibrant red hue standing out. The lighting is diffused, likely due to the cloudy weather, casting a soft, even illumination on the bus and its surroundings.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25726.1, "ram_available_mb": 100046.1, "ram_percent": 20.5}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25726.1, "ram_available_mb": 100046.1, "ram_percent": 20.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 4853.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.12, "peak": 15.22, "min": 14.81}, "VDD_GPU": {"avg": 20.85, "peak": 23.65, "min": 19.71}, "VIN": {"avg": 62.51, "peak": 67.41, "min": 59.7}, "VDD_CPU_SOC_MSS": {"avg": 16.02, "peak": 16.14, "min": 15.74}}, "power_watts_avg": 20.85, "energy_joules_est": 27.11, "sample_count": 10, "duration_seconds": 1.3}, "timestamp": "2026-01-16T15:11:22.739577"}
{"image_index": 18, "image_name": "000000001675.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001675.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2381.274, "latencies_ms": [2381.274], "images_per_second": 0.42, "prompt_tokens": 9, "response_tokens_est": 29, "n_tiles": 12, "output_text": "The image shows a black and white cat with yellow eyes, resting on top of a laptop keyboard, which is partially visible in the foreground.", "error": null, "sys_before": {"cpu_percent": 47.4, "ram_used_mb": 25726.1, "ram_available_mb": 100046.1, "ram_percent": 20.5}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 25726.1, "ram_available_mb": 100046.1, "ram_percent": 20.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.76, "peak": 15.75, "min": 14.1}, "VDD_GPU": {"avg": 34.11, "peak": 39.8, "min": 27.19}, "VIN": {"avg": 75.58, "peak": 143.62, "min": 56.85}, "VDD_CPU_SOC_MSS": {"avg": 15.86, "peak": 16.93, "min": 15.36}}, "power_watts_avg": 34.11, "energy_joules_est": 81.25, "sample_count": 18, "duration_seconds": 2.382}, "timestamp": "2026-01-16T15:11:25.182755"}
{"image_index": 18, "image_name": "000000001675.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001675.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2644.622, "latencies_ms": [2644.622], "images_per_second": 0.378, "prompt_tokens": 23, "response_tokens_est": 33, "n_tiles": 12, "output_text": "1. Laptop\n2. Keyboard\n3. Mouse\n4. Laptop screen\n5. Laptop\n6. Laptop\n7. Laptop\n8. Laptop", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25726.1, "ram_available_mb": 100046.1, "ram_percent": 20.5}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 25726.1, "ram_available_mb": 100046.1, "ram_percent": 20.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.57, "peak": 15.14, "min": 14.0}, "VDD_GPU": {"avg": 35.3, "peak": 42.17, "min": 27.19}, "VIN": {"avg": 72.83, "peak": 111.74, "min": 56.68}, "VDD_CPU_SOC_MSS": {"avg": 15.44, "peak": 16.14, "min": 14.57}}, "power_watts_avg": 35.3, "energy_joules_est": 93.37, "sample_count": 20, "duration_seconds": 2.645}, "timestamp": "2026-01-16T15:11:27.833542"}
{"image_index": 18, "image_name": "000000001675.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001675.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 4066.346, "latencies_ms": [4066.346], "images_per_second": 0.246, "prompt_tokens": 27, "response_tokens_est": 72, "n_tiles": 12, "output_text": "The main object in the image is a laptop, which is positioned in the foreground. The laptop is open, revealing its keyboard and screen. The keyboard is located near the bottom of the image, while the screen is positioned above it. The laptop is situated in the center of the frame, with the keyboard and screen occupying the majority of the space.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25726.1, "ram_available_mb": 100046.1, "ram_percent": 20.5}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25726.1, "ram_available_mb": 100046.1, "ram_percent": 20.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.53, "peak": 15.14, "min": 14.0}, "VDD_GPU": {"avg": 31.42, "peak": 41.76, "min": 25.61}, "VIN": {"avg": 68.95, "peak": 110.11, "min": 50.99}, "VDD_CPU_SOC_MSS": {"avg": 15.26, "peak": 16.14, "min": 14.57}}, "power_watts_avg": 31.42, "energy_joules_est": 127.77, "sample_count": 31, "duration_seconds": 4.067}, "timestamp": "2026-01-16T15:11:31.907464"}
{"image_index": 18, "image_name": "000000001675.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001675.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 5162.897, "latencies_ms": [5162.897], "images_per_second": 0.194, "prompt_tokens": 21, "response_tokens_est": 108, "n_tiles": 12, "output_text": "The image depicts a close-up view of a black and white cat sitting in front of a laptop keyboard. The cat's fur is predominantly black with white markings, and it appears to be looking directly at the camera with a somewhat curious or alert expression. The laptop keyboard is visible in the foreground, with the keys in focus, and the laptop itself is partially visible behind the cat. The setting seems to be indoors, possibly in a home or office environment, with a plain background that does not distract from the main subject.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25726.1, "ram_available_mb": 100046.1, "ram_percent": 20.5}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 25726.1, "ram_available_mb": 100046.1, "ram_percent": 20.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.65, "peak": 15.24, "min": 14.02}, "VDD_GPU": {"avg": 30.39, "peak": 41.76, "min": 25.61}, "VIN": {"avg": 73.45, "peak": 128.54, "min": 57.27}, "VDD_CPU_SOC_MSS": {"avg": 15.44, "peak": 16.14, "min": 14.96}}, "power_watts_avg": 30.39, "energy_joules_est": 156.91, "sample_count": 40, "duration_seconds": 5.163}, "timestamp": "2026-01-16T15:11:37.076605"}
{"image_index": 18, "image_name": "000000001675.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001675.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3177.922, "latencies_ms": [3177.922], "images_per_second": 0.315, "prompt_tokens": 19, "response_tokens_est": 49, "n_tiles": 12, "output_text": "The image features a black and white cat with striking yellow eyes, sitting in front of a laptop keyboard. The cat's fur is predominantly black with white accents, and the lighting is soft and natural, suggesting an indoor setting with ambient light.", "error": null, "sys_before": {"cpu_percent": 40.0, "ram_used_mb": 25726.1, "ram_available_mb": 100046.1, "ram_percent": 20.5}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 25726.1, "ram_available_mb": 100046.1, "ram_percent": 20.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.62, "peak": 15.14, "min": 14.02}, "VDD_GPU": {"avg": 33.59, "peak": 41.77, "min": 26.01}, "VIN": {"avg": 73.29, "peak": 129.32, "min": 56.97}, "VDD_CPU_SOC_MSS": {"avg": 15.44, "peak": 16.14, "min": 14.57}}, "power_watts_avg": 33.59, "energy_joules_est": 106.76, "sample_count": 24, "duration_seconds": 3.178}, "timestamp": "2026-01-16T15:11:40.264628"}
{"image_index": 19, "image_name": "000000001761.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001761.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1729.409, "latencies_ms": [1729.409], "images_per_second": 0.578, "prompt_tokens": 9, "response_tokens_est": 39, "n_tiles": 6, "output_text": "The image depicts a large, dark-colored steel arch bridge spanning across a body of water, with a city skyline in the background featuring a distinctive building with a unique, curved design.", "error": null, "sys_before": {"cpu_percent": 38.9, "ram_used_mb": 25726.1, "ram_available_mb": 100046.1, "ram_percent": 20.5}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25726.1, "ram_available_mb": 100046.1, "ram_percent": 20.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.91, "peak": 14.61, "min": 13.21}, "VDD_GPU": {"avg": 29.49, "peak": 35.86, "min": 24.04}, "VIN": {"avg": 72.59, "peak": 108.88, "min": 59.57}, "VDD_CPU_SOC_MSS": {"avg": 14.57, "peak": 15.36, "min": 13.78}}, "power_watts_avg": 29.49, "energy_joules_est": 51.02, "sample_count": 13, "duration_seconds": 1.73}, "timestamp": "2026-01-16T15:11:42.048942"}
{"image_index": 19, "image_name": "000000001761.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001761.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1736.998, "latencies_ms": [1736.998], "images_per_second": 0.576, "prompt_tokens": 23, "response_tokens_est": 40, "n_tiles": 6, "output_text": "object: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25726.1, "ram_available_mb": 100046.1, "ram_percent": 20.5}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 25726.1, "ram_available_mb": 100046.1, "ram_percent": 20.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.17, "peak": 14.4, "min": 13.61}, "VDD_GPU": {"avg": 30.65, "peak": 37.82, "min": 24.04}, "VIN": {"avg": 69.29, "peak": 109.05, "min": 49.91}, "VDD_CPU_SOC_MSS": {"avg": 14.87, "peak": 15.36, "min": 14.18}}, "power_watts_avg": 30.65, "energy_joules_est": 53.25, "sample_count": 13, "duration_seconds": 1.737}, "timestamp": "2026-01-16T15:11:43.791655"}
{"image_index": 19, "image_name": "000000001761.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001761.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2359.641, "latencies_ms": [2359.641], "images_per_second": 0.424, "prompt_tokens": 27, "response_tokens_est": 62, "n_tiles": 6, "output_text": "The main objects in the image are the Sydney Harbour Bridge and the Sydney Opera House. The Sydney Harbour Bridge is in the foreground, while the Sydney Opera House is in the background. The Sydney Opera House is located near the water, and the bridge spans across the water, connecting the two sides.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25726.1, "ram_available_mb": 100046.1, "ram_percent": 20.5}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 25726.3, "ram_available_mb": 100045.9, "ram_percent": 20.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.26, "peak": 14.51, "min": 13.61}, "VDD_GPU": {"avg": 28.35, "peak": 37.82, "min": 22.86}, "VIN": {"avg": 73.03, "peak": 115.91, "min": 59.16}, "VDD_CPU_SOC_MSS": {"avg": 15.05, "peak": 15.36, "min": 14.18}}, "power_watts_avg": 28.35, "energy_joules_est": 66.9, "sample_count": 18, "duration_seconds": 2.36}, "timestamp": "2026-01-16T15:11:46.157257"}
{"image_index": 19, "image_name": "000000001761.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001761.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1936.039, "latencies_ms": [1936.039], "images_per_second": 0.517, "prompt_tokens": 21, "response_tokens_est": 47, "n_tiles": 6, "output_text": "The image captures a serene scene of an overcast sky above a bridge, with a cityscape in the background. The bridge spans across a body of water, and the city features iconic buildings, including the Sydney Opera House.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25726.3, "ram_available_mb": 100045.9, "ram_percent": 20.5}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 25726.6, "ram_available_mb": 100045.6, "ram_percent": 20.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.21, "peak": 14.51, "min": 13.61}, "VDD_GPU": {"avg": 29.72, "peak": 37.42, "min": 23.64}, "VIN": {"avg": 73.1, "peak": 107.66, "min": 61.03}, "VDD_CPU_SOC_MSS": {"avg": 14.91, "peak": 15.36, "min": 14.18}}, "power_watts_avg": 29.72, "energy_joules_est": 57.55, "sample_count": 14, "duration_seconds": 1.936}, "timestamp": "2026-01-16T15:11:48.103273"}
{"image_index": 19, "image_name": "000000001761.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001761.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1734.508, "latencies_ms": [1734.508], "images_per_second": 0.577, "prompt_tokens": 19, "response_tokens_est": 40, "n_tiles": 6, "output_text": "The image features a large, dark-colored steel arch bridge with a cloudy sky in the background. The lighting is soft and diffused, with no harsh shadows, indicating an overcast day.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25726.6, "ram_available_mb": 100045.6, "ram_percent": 20.5}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25726.3, "ram_available_mb": 100045.9, "ram_percent": 20.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.17, "peak": 14.51, "min": 13.61}, "VDD_GPU": {"avg": 30.52, "peak": 37.82, "min": 24.04}, "VIN": {"avg": 72.47, "peak": 113.83, "min": 59.89}, "VDD_CPU_SOC_MSS": {"avg": 14.9, "peak": 15.36, "min": 14.18}}, "power_watts_avg": 30.52, "energy_joules_est": 52.96, "sample_count": 13, "duration_seconds": 1.735}, "timestamp": "2026-01-16T15:11:49.844715"}
{"image_index": 20, "image_name": "000000001818.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001818.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1606.108, "latencies_ms": [1606.108], "images_per_second": 0.623, "prompt_tokens": 9, "response_tokens_est": 38, "n_tiles": 6, "output_text": "The image captures a zebra with its distinctive black and white stripes, standing in a natural grassy environment, showcasing the unique patterns and markings that are characteristic of the species.", "error": null, "sys_before": {"cpu_percent": 37.7, "ram_used_mb": 25726.3, "ram_available_mb": 100045.9, "ram_percent": 20.5}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 25726.6, "ram_available_mb": 100045.6, "ram_percent": 20.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.06, "peak": 14.61, "min": 13.39}, "VDD_GPU": {"avg": 30.5, "peak": 36.63, "min": 24.43}, "VIN": {"avg": 71.92, "peak": 99.05, "min": 60.19}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.36, "min": 14.18}}, "power_watts_avg": 30.5, "energy_joules_est": 49.01, "sample_count": 12, "duration_seconds": 1.607}, "timestamp": "2026-01-16T15:11:51.499432"}
{"image_index": 20, "image_name": "000000001818.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001818.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 924.687, "latencies_ms": [924.687], "images_per_second": 1.081, "prompt_tokens": 23, "response_tokens_est": 11, "n_tiles": 6, "output_text": "zebra: 1\ngrass: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25726.6, "ram_available_mb": 100045.6, "ram_percent": 20.5}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 25726.8, "ram_available_mb": 100045.4, "ram_percent": 20.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.25, "peak": 14.53, "min": 13.82}, "VDD_GPU": {"avg": 35.52, "peak": 37.82, "min": 31.92}, "VIN": {"avg": 80.82, "peak": 114.79, "min": 57.9}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.36, "min": 14.57}}, "power_watts_avg": 35.52, "energy_joules_est": 32.86, "sample_count": 6, "duration_seconds": 0.925}, "timestamp": "2026-01-16T15:11:52.430260"}
{"image_index": 20, "image_name": "000000001818.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001818.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3390.024, "latencies_ms": [3390.024], "images_per_second": 0.295, "prompt_tokens": 27, "response_tokens_est": 103, "n_tiles": 6, "output_text": "The main object in the foreground is a zebra, with its distinctive black and white stripes clearly visible. The zebra is positioned slightly to the left of the center of the image, with its head turned towards the right side of the frame. The background consists of a grassy field, providing a natural setting for the zebra. The zebra is not directly in the center of the image, but it is positioned near the center, with its head and neck extending towards the right side of the frame.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25726.8, "ram_available_mb": 100045.4, "ram_percent": 20.5}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 25726.8, "ram_available_mb": 100045.4, "ram_percent": 20.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.54, "peak": 14.91, "min": 13.72}, "VDD_GPU": {"avg": 27.73, "peak": 39.39, "min": 23.25}, "VIN": {"avg": 69.1, "peak": 105.82, "min": 54.99}, "VDD_CPU_SOC_MSS": {"avg": 15.37, "peak": 15.76, "min": 14.57}}, "power_watts_avg": 27.73, "energy_joules_est": 94.05, "sample_count": 26, "duration_seconds": 3.392}, "timestamp": "2026-01-16T15:11:55.827969"}
{"image_index": 20, "image_name": "000000001818.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001818.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2657.775, "latencies_ms": [2657.775], "images_per_second": 0.376, "prompt_tokens": 21, "response_tokens_est": 76, "n_tiles": 6, "output_text": "The image captures a zebra in a natural setting, likely a savanna or grassland, with its distinctive black and white stripes clearly visible. The zebra is seen in profile, with its head turned slightly to the side, and its ears are perked up, indicating alertness. The background consists of tall grasses, providing a natural habitat for the zebra.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25726.8, "ram_available_mb": 100045.4, "ram_percent": 20.5}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 25726.8, "ram_available_mb": 100045.4, "ram_percent": 20.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.57, "peak": 14.91, "min": 13.82}, "VDD_GPU": {"avg": 28.15, "peak": 37.42, "min": 23.25}, "VIN": {"avg": 72.46, "peak": 117.26, "min": 57.45}, "VDD_CPU_SOC_MSS": {"avg": 15.34, "peak": 15.76, "min": 14.57}}, "power_watts_avg": 28.15, "energy_joules_est": 74.82, "sample_count": 20, "duration_seconds": 2.658}, "timestamp": "2026-01-16T15:11:58.491794"}
{"image_index": 20, "image_name": "000000001818.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001818.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2514.526, "latencies_ms": [2514.526], "images_per_second": 0.398, "prompt_tokens": 19, "response_tokens_est": 69, "n_tiles": 6, "output_text": "The image is a black and white photograph of a zebra, showcasing its distinctive black and white striped pattern. The lighting is natural, likely from the sun, casting shadows that enhance the texture of the stripes. The zebra's coat appears to be dry and well-maintained, with no visible signs of wear or damage.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25726.8, "ram_available_mb": 100045.4, "ram_percent": 20.5}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 25726.8, "ram_available_mb": 100045.4, "ram_percent": 20.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.51, "peak": 14.71, "min": 13.92}, "VDD_GPU": {"avg": 28.2, "peak": 37.82, "min": 22.86}, "VIN": {"avg": 71.58, "peak": 123.16, "min": 55.15}, "VDD_CPU_SOC_MSS": {"avg": 15.23, "peak": 15.36, "min": 14.57}}, "power_watts_avg": 28.2, "energy_joules_est": 70.92, "sample_count": 19, "duration_seconds": 2.515}, "timestamp": "2026-01-16T15:12:01.012483"}
{"image_index": 21, "image_name": "000000001993.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001993.jpg", "image_width": 640, "image_height": 419, "image_resolution": "640x419", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1625.369, "latencies_ms": [1625.369], "images_per_second": 0.615, "prompt_tokens": 9, "response_tokens_est": 39, "n_tiles": 6, "output_text": "The image shows a small, cozy room with a bed, a table, and a chair, all set against a backdrop of a brick wall and a window with a view of a balcony.", "error": null, "sys_before": {"cpu_percent": 44.7, "ram_used_mb": 25726.8, "ram_available_mb": 100045.4, "ram_percent": 20.5}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 25726.8, "ram_available_mb": 100045.4, "ram_percent": 20.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.1, "peak": 14.71, "min": 13.39}, "VDD_GPU": {"avg": 30.24, "peak": 36.63, "min": 24.43}, "VIN": {"avg": 72.89, "peak": 122.87, "min": 55.5}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.36, "min": 14.18}}, "power_watts_avg": 30.24, "energy_joules_est": 49.16, "sample_count": 12, "duration_seconds": 1.626}, "timestamp": "2026-01-16T15:12:02.694013"}
{"image_index": 21, "image_name": "000000001993.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001993.jpg", "image_width": 640, "image_height": 419, "image_resolution": "640x419", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1619.85, "latencies_ms": [1619.85], "images_per_second": 0.617, "prompt_tokens": 23, "response_tokens_est": 37, "n_tiles": 6, "output_text": "1. Bed\n2. Bedding\n3. Bed frame\n4. Bedspread\n5. Floor\n6. Flooring\n7. Rug\n8. Rug pad", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25726.8, "ram_available_mb": 100045.4, "ram_percent": 20.5}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25726.8, "ram_available_mb": 100045.4, "ram_percent": 20.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.35, "peak": 14.61, "min": 13.82}, "VDD_GPU": {"avg": 31.39, "peak": 37.82, "min": 24.83}, "VIN": {"avg": 68.15, "peak": 103.73, "min": 56.87}, "VDD_CPU_SOC_MSS": {"avg": 15.16, "peak": 15.36, "min": 14.57}}, "power_watts_avg": 31.39, "energy_joules_est": 50.86, "sample_count": 12, "duration_seconds": 1.62}, "timestamp": "2026-01-16T15:12:04.320600"}
{"image_index": 21, "image_name": "000000001993.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001993.jpg", "image_width": 640, "image_height": 419, "image_resolution": "640x419", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3012.911, "latencies_ms": [3012.911], "images_per_second": 0.332, "prompt_tokens": 27, "response_tokens_est": 89, "n_tiles": 6, "output_text": "The main objects in the image are a bed and a table. The bed is positioned in the foreground, with a colorful patterned bedspread. The table is located to the left of the bed, with a small black chair next to it. The room has a window on the left side, allowing natural light to enter. The bed is placed against a wall, and there is a doorway leading to another room on the right side.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25726.8, "ram_available_mb": 100045.4, "ram_percent": 20.5}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 25726.7, "ram_available_mb": 100045.4, "ram_percent": 20.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.56, "peak": 14.91, "min": 13.82}, "VDD_GPU": {"avg": 27.65, "peak": 37.82, "min": 23.25}, "VIN": {"avg": 63.54, "peak": 77.79, "min": 53.03}, "VDD_CPU_SOC_MSS": {"avg": 15.44, "peak": 15.76, "min": 14.57}}, "power_watts_avg": 27.65, "energy_joules_est": 83.32, "sample_count": 23, "duration_seconds": 3.013}, "timestamp": "2026-01-16T15:12:07.339414"}
{"image_index": 21, "image_name": "000000001993.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001993.jpg", "image_width": 640, "image_height": 419, "image_resolution": "640x419", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2632.523, "latencies_ms": [2632.523], "images_per_second": 0.38, "prompt_tokens": 21, "response_tokens_est": 75, "n_tiles": 6, "output_text": "The image depicts a cozy, well-lit room with a bed and a small table. The bed is adorned with a colorful patterned comforter, and the room has a purple carpet. There is a window with white frames on the left side, allowing natural light to enter the room. The overall setting appears to be a bedroom or a small living space.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25726.7, "ram_available_mb": 100045.4, "ram_percent": 20.5}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 25727.0, "ram_available_mb": 100045.2, "ram_percent": 20.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.55, "peak": 14.81, "min": 13.82}, "VDD_GPU": {"avg": 28.15, "peak": 37.42, "min": 23.25}, "VIN": {"avg": 67.88, "peak": 93.82, "min": 48.1}, "VDD_CPU_SOC_MSS": {"avg": 15.36, "peak": 15.76, "min": 14.57}}, "power_watts_avg": 28.15, "energy_joules_est": 74.12, "sample_count": 20, "duration_seconds": 2.633}, "timestamp": "2026-01-16T15:12:09.977894"}
{"image_index": 21, "image_name": "000000001993.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000001993.jpg", "image_width": 640, "image_height": 419, "image_resolution": "640x419", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2287.014, "latencies_ms": [2287.014], "images_per_second": 0.437, "prompt_tokens": 19, "response_tokens_est": 62, "n_tiles": 6, "output_text": "The room features a purple carpet, a bed with a colorful patterned duvet, a black metal frame bed, a small round wooden table, a window with white frames, and a white door. The lighting is soft and natural, coming from the window, and the overall atmosphere is cozy and inviting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25727.0, "ram_available_mb": 100045.2, "ram_percent": 20.5}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 25727.0, "ram_available_mb": 100045.2, "ram_percent": 20.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.52, "peak": 14.81, "min": 13.92}, "VDD_GPU": {"avg": 28.81, "peak": 37.42, "min": 23.64}, "VIN": {"avg": 66.15, "peak": 85.56, "min": 53.92}, "VDD_CPU_SOC_MSS": {"avg": 15.36, "peak": 15.76, "min": 14.57}}, "power_watts_avg": 28.81, "energy_joules_est": 65.9, "sample_count": 17, "duration_seconds": 2.287}, "timestamp": "2026-01-16T15:12:12.271382"}
{"image_index": 22, "image_name": "000000002006.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002006.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2567.51, "latencies_ms": [2567.51], "images_per_second": 0.389, "prompt_tokens": 9, "response_tokens_est": 31, "n_tiles": 12, "output_text": "A purple bus with the number 96 is driving on a street, with a person in the driver's seat and another person walking on the sidewalk.", "error": null, "sys_before": {"cpu_percent": 40.2, "ram_used_mb": 25727.0, "ram_available_mb": 100045.2, "ram_percent": 20.5}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 25727.0, "ram_available_mb": 100045.2, "ram_percent": 20.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.58, "peak": 15.65, "min": 13.49}, "VDD_GPU": {"avg": 33.37, "peak": 38.6, "min": 27.19}, "VIN": {"avg": 75.58, "peak": 120.35, "min": 57.02}, "VDD_CPU_SOC_MSS": {"avg": 15.34, "peak": 16.54, "min": 14.57}}, "power_watts_avg": 33.37, "energy_joules_est": 85.69, "sample_count": 19, "duration_seconds": 2.568}, "timestamp": "2026-01-16T15:12:14.918961"}
{"image_index": 22, "image_name": "000000002006.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002006.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 5861.61, "latencies_ms": [5861.61], "images_per_second": 0.171, "prompt_tokens": 23, "response_tokens_est": 129, "n_tiles": 12, "output_text": "- bus: 1\n- driver: 1\n- bus stop: 1\n- bus stop sign: 1\n- bus stop pole: 1\n- bus stop light: 1\n- bus stop signboard: 1\n- bus stop signboard pole: 1\n- bus stop signboard pole pole: 1\n- bus stop signboard pole pole pole: 1\n- bus stop signboard pole pole pole pole: 1\n- bus stop signboard pole pole pole pole pole pole pole pole pole pole pole pole pole pole pole pole pole pole pole pole pole pole pole pole pole", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 25727.0, "ram_available_mb": 100045.2, "ram_percent": 20.5}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 25727.5, "ram_available_mb": 100044.7, "ram_percent": 20.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.69, "peak": 15.14, "min": 14.1}, "VDD_GPU": {"avg": 29.99, "peak": 42.54, "min": 25.61}, "VIN": {"avg": 69.14, "peak": 120.83, "min": 56.75}, "VDD_CPU_SOC_MSS": {"avg": 15.4, "peak": 16.14, "min": 14.96}}, "power_watts_avg": 29.99, "energy_joules_est": 175.81, "sample_count": 45, "duration_seconds": 5.862}, "timestamp": "2026-01-16T15:12:20.787728"}
{"image_index": 22, "image_name": "000000002006.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002006.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3422.527, "latencies_ms": [3422.527], "images_per_second": 0.292, "prompt_tokens": 27, "response_tokens_est": 56, "n_tiles": 12, "output_text": "The main object in the foreground is a purple bus with the number 96 on its front. The bus is parked on the side of a street, with a person walking in front of it. The background features a tree-lined street and a building with a chimney.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25727.5, "ram_available_mb": 100044.7, "ram_percent": 20.5}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25727.2, "ram_available_mb": 100045.0, "ram_percent": 20.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.69, "peak": 15.24, "min": 14.12}, "VDD_GPU": {"avg": 33.04, "peak": 41.77, "min": 26.01}, "VIN": {"avg": 71.38, "peak": 105.87, "min": 51.04}, "VDD_CPU_SOC_MSS": {"avg": 15.42, "peak": 16.14, "min": 14.57}}, "power_watts_avg": 33.04, "energy_joules_est": 113.09, "sample_count": 26, "duration_seconds": 3.423}, "timestamp": "2026-01-16T15:12:24.216006"}
{"image_index": 22, "image_name": "000000002006.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002006.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3417.293, "latencies_ms": [3417.293], "images_per_second": 0.293, "prompt_tokens": 21, "response_tokens_est": 56, "n_tiles": 12, "output_text": "The image depicts a purple bus with the number 96 on its destination sign, traveling on a road. A man is seen driving the bus, and another person is walking on the sidewalk. The setting appears to be a city street with buildings and trees in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25727.2, "ram_available_mb": 100045.0, "ram_percent": 20.5}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25727.2, "ram_available_mb": 100045.0, "ram_percent": 20.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.7, "peak": 15.24, "min": 14.2}, "VDD_GPU": {"avg": 32.82, "peak": 42.15, "min": 26.01}, "VIN": {"avg": 75.23, "peak": 117.6, "min": 57.38}, "VDD_CPU_SOC_MSS": {"avg": 15.45, "peak": 16.14, "min": 14.96}}, "power_watts_avg": 32.82, "energy_joules_est": 112.17, "sample_count": 27, "duration_seconds": 3.418}, "timestamp": "2026-01-16T15:12:27.639702"}
{"image_index": 22, "image_name": "000000002006.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002006.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3279.265, "latencies_ms": [3279.265], "images_per_second": 0.305, "prompt_tokens": 19, "response_tokens_est": 52, "n_tiles": 12, "output_text": "The notable visual attributes of the bus include its purple color, which stands out against the background. The lighting is bright, indicating it is daytime. The materials used for the bus are likely metal and plastic, and the weather appears to be clear and sunny.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25727.2, "ram_available_mb": 100045.0, "ram_percent": 20.5}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 25727.5, "ram_available_mb": 100044.7, "ram_percent": 20.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.71, "peak": 15.24, "min": 14.2}, "VDD_GPU": {"avg": 33.25, "peak": 42.17, "min": 26.01}, "VIN": {"avg": 70.89, "peak": 111.32, "min": 56.76}, "VDD_CPU_SOC_MSS": {"avg": 15.43, "peak": 16.14, "min": 14.96}}, "power_watts_avg": 33.25, "energy_joules_est": 109.05, "sample_count": 26, "duration_seconds": 3.28}, "timestamp": "2026-01-16T15:12:30.925562"}
{"image_index": 23, "image_name": "000000002149.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002149.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1448.325, "latencies_ms": [1448.325], "images_per_second": 0.69, "prompt_tokens": 9, "response_tokens_est": 32, "n_tiles": 6, "output_text": "The image shows a close-up view of several green apples arranged in a bowl, with a focus on their smooth, glossy surfaces and uniform green color.", "error": null, "sys_before": {"cpu_percent": 39.0, "ram_used_mb": 25727.5, "ram_available_mb": 100044.7, "ram_percent": 20.5}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25727.5, "ram_available_mb": 100044.7, "ram_percent": 20.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.04, "peak": 14.61, "min": 13.41}, "VDD_GPU": {"avg": 31.31, "peak": 37.03, "min": 25.22}, "VIN": {"avg": 68.8, "peak": 120.5, "min": 51.78}, "VDD_CPU_SOC_MSS": {"avg": 14.78, "peak": 15.36, "min": 14.18}}, "power_watts_avg": 31.31, "energy_joules_est": 45.36, "sample_count": 11, "duration_seconds": 1.449}, "timestamp": "2026-01-16T15:12:32.435224"}
{"image_index": 23, "image_name": "000000002149.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002149.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1695.341, "latencies_ms": [1695.341], "images_per_second": 0.59, "prompt_tokens": 23, "response_tokens_est": 40, "n_tiles": 6, "output_text": "1. Green apples\n2. Green apples\n3. Green apples\n4. Green apples\n5. Green apples\n6. Green apples\n7. Green apples\n8. Green apples", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25727.5, "ram_available_mb": 100044.7, "ram_percent": 20.5}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 25727.2, "ram_available_mb": 100045.0, "ram_percent": 20.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.43, "peak": 14.81, "min": 13.92}, "VDD_GPU": {"avg": 31.16, "peak": 38.23, "min": 24.43}, "VIN": {"avg": 67.2, "peak": 102.27, "min": 56.38}, "VDD_CPU_SOC_MSS": {"avg": 15.08, "peak": 15.36, "min": 14.57}}, "power_watts_avg": 31.16, "energy_joules_est": 52.88, "sample_count": 13, "duration_seconds": 1.697}, "timestamp": "2026-01-16T15:12:34.138100"}
{"image_index": 23, "image_name": "000000002149.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002149.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2132.839, "latencies_ms": [2132.839], "images_per_second": 0.469, "prompt_tokens": 27, "response_tokens_est": 54, "n_tiles": 6, "output_text": "The main objects in the image are green apples. The apples are arranged in a way that the foreground has a few apples, while the background has more apples. The apples in the foreground are closer to the viewer, while the apples in the background are further away.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25727.2, "ram_available_mb": 100045.0, "ram_percent": 20.5}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 25727.5, "ram_available_mb": 100044.7, "ram_percent": 20.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.4, "peak": 14.61, "min": 13.82}, "VDD_GPU": {"avg": 29.36, "peak": 38.21, "min": 23.25}, "VIN": {"avg": 69.18, "peak": 115.06, "min": 59.12}, "VDD_CPU_SOC_MSS": {"avg": 15.09, "peak": 15.36, "min": 14.57}}, "power_watts_avg": 29.36, "energy_joules_est": 62.63, "sample_count": 16, "duration_seconds": 2.133}, "timestamp": "2026-01-16T15:12:36.276881"}
{"image_index": 23, "image_name": "000000002149.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002149.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2298.124, "latencies_ms": [2298.124], "images_per_second": 0.435, "prompt_tokens": 21, "response_tokens_est": 60, "n_tiles": 6, "output_text": "The image depicts a close-up view of a white bowl filled with fresh green apples. The apples are arranged in a neat pile, with some partially overlapping each other. The lighting is soft and warm, highlighting the glossy surface of the apples and creating a pleasing contrast with the white bowl.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25727.5, "ram_available_mb": 100044.7, "ram_percent": 20.5}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 25727.2, "ram_available_mb": 100045.0, "ram_percent": 20.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.36, "peak": 14.71, "min": 13.72}, "VDD_GPU": {"avg": 28.54, "peak": 37.82, "min": 22.86}, "VIN": {"avg": 68.96, "peak": 96.91, "min": 60.05}, "VDD_CPU_SOC_MSS": {"avg": 15.03, "peak": 15.36, "min": 14.18}}, "power_watts_avg": 28.54, "energy_joules_est": 65.6, "sample_count": 18, "duration_seconds": 2.299}, "timestamp": "2026-01-16T15:12:38.581181"}
{"image_index": 23, "image_name": "000000002149.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002149.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1822.249, "latencies_ms": [1822.249], "images_per_second": 0.549, "prompt_tokens": 19, "response_tokens_est": 43, "n_tiles": 6, "output_text": "The image features a bowl filled with fresh green apples, which are bright and vibrant in color. The lighting is soft and diffused, casting gentle shadows and highlighting the smooth, glossy surface of the apples.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25727.2, "ram_available_mb": 100045.0, "ram_percent": 20.5}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 25727.4, "ram_available_mb": 100044.7, "ram_percent": 20.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.29, "peak": 14.61, "min": 13.72}, "VDD_GPU": {"avg": 29.97, "peak": 37.42, "min": 23.64}, "VIN": {"avg": 68.35, "peak": 101.0, "min": 57.68}, "VDD_CPU_SOC_MSS": {"avg": 14.93, "peak": 15.36, "min": 14.18}}, "power_watts_avg": 29.97, "energy_joules_est": 54.63, "sample_count": 14, "duration_seconds": 1.823}, "timestamp": "2026-01-16T15:12:40.409667"}
{"image_index": 24, "image_name": "000000002153.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002153.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2517.257, "latencies_ms": [2517.257], "images_per_second": 0.397, "prompt_tokens": 9, "response_tokens_est": 33, "n_tiles": 12, "output_text": "A baseball player is standing on the pitcher's mound, preparing to swing at the pitch, while a umpire and catcher are positioned behind him.", "error": null, "sys_before": {"cpu_percent": 44.5, "ram_used_mb": 25727.4, "ram_available_mb": 100044.7, "ram_percent": 20.5}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 25727.4, "ram_available_mb": 100044.7, "ram_percent": 20.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.79, "peak": 15.75, "min": 13.59}, "VDD_GPU": {"avg": 33.93, "peak": 40.2, "min": 26.8}, "VIN": {"avg": 74.56, "peak": 98.33, "min": 57.19}, "VDD_CPU_SOC_MSS": {"avg": 15.73, "peak": 16.93, "min": 14.96}}, "power_watts_avg": 33.93, "energy_joules_est": 85.42, "sample_count": 19, "duration_seconds": 2.518}, "timestamp": "2026-01-16T15:12:43.015940"}
{"image_index": 24, "image_name": "000000002153.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002153.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2909.745, "latencies_ms": [2909.745], "images_per_second": 0.344, "prompt_tokens": 23, "response_tokens_est": 41, "n_tiles": 12, "output_text": "1. Baseball player\n2. Baseball bat\n3. Baseball field\n4. Netting\n5. Home plate\n6. Catcher\n7. umpire\n8. Green grass", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25727.4, "ram_available_mb": 100044.7, "ram_percent": 20.5}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 25727.4, "ram_available_mb": 100044.7, "ram_percent": 20.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.73, "peak": 15.34, "min": 14.2}, "VDD_GPU": {"avg": 34.31, "peak": 42.56, "min": 26.4}, "VIN": {"avg": 76.18, "peak": 111.86, "min": 56.97}, "VDD_CPU_SOC_MSS": {"avg": 15.48, "peak": 16.14, "min": 14.96}}, "power_watts_avg": 34.31, "energy_joules_est": 99.85, "sample_count": 23, "duration_seconds": 2.91}, "timestamp": "2026-01-16T15:12:45.932154"}
{"image_index": 24, "image_name": "000000002153.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002153.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 5372.326, "latencies_ms": [5372.326], "images_per_second": 0.186, "prompt_tokens": 27, "response_tokens_est": 114, "n_tiles": 12, "output_text": "The main object in the foreground is the baseball player, who is standing on the pitcher's mound. The player is wearing a red helmet and a white uniform with red accents. The baseball bat is held by the player, ready to swing. In the background, there is a catcher crouched behind home plate, wearing a gray uniform with a helmet. The umpire is standing near the catcher, wearing a light blue shirt and dark pants. The netting of the baseball field is visible in the foreground, separating the players from the audience.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25727.4, "ram_available_mb": 100044.7, "ram_percent": 20.5}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25731.4, "ram_available_mb": 100040.7, "ram_percent": 20.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.77, "peak": 15.34, "min": 14.2}, "VDD_GPU": {"avg": 30.46, "peak": 42.54, "min": 26.01}, "VIN": {"avg": 69.77, "peak": 111.97, "min": 57.13}, "VDD_CPU_SOC_MSS": {"avg": 15.39, "peak": 16.14, "min": 14.96}}, "power_watts_avg": 30.46, "energy_joules_est": 163.65, "sample_count": 43, "duration_seconds": 5.373}, "timestamp": "2026-01-16T15:12:51.310581"}
{"image_index": 24, "image_name": "000000002153.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002153.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 4289.951, "latencies_ms": [4289.951], "images_per_second": 0.233, "prompt_tokens": 21, "response_tokens_est": 82, "n_tiles": 12, "output_text": "The image captures a baseball game in progress, with a batter at the plate preparing to swing at a pitch. The batter is wearing a red helmet and a white uniform with red accents, while the catcher and umpire are positioned behind him, ready to catch the ball. The scene takes place on a baseball field with a green outfield and a dirt infield, surrounded by a mesh fence.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25731.4, "ram_available_mb": 100040.7, "ram_percent": 20.5}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 25731.2, "ram_available_mb": 100041.0, "ram_percent": 20.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.76, "peak": 15.34, "min": 14.12}, "VDD_GPU": {"avg": 31.59, "peak": 42.15, "min": 25.61}, "VIN": {"avg": 71.2, "peak": 113.0, "min": 56.82}, "VDD_CPU_SOC_MSS": {"avg": 15.45, "peak": 16.14, "min": 14.96}}, "power_watts_avg": 31.59, "energy_joules_est": 135.54, "sample_count": 33, "duration_seconds": 4.291}, "timestamp": "2026-01-16T15:12:55.610946"}
{"image_index": 24, "image_name": "000000002153.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002153.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3415.179, "latencies_ms": [3415.179], "images_per_second": 0.293, "prompt_tokens": 19, "response_tokens_est": 56, "n_tiles": 12, "output_text": "The image captures a baseball game in progress, with a player in a white uniform and red helmet preparing to swing at a pitch. The field is surrounded by a mesh fence, and the scene is illuminated by artificial lighting, highlighting the players and the action on the field.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25731.2, "ram_available_mb": 100041.0, "ram_percent": 20.5}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 25731.2, "ram_available_mb": 100041.0, "ram_percent": 20.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.74, "peak": 15.34, "min": 14.1}, "VDD_GPU": {"avg": 33.03, "peak": 42.15, "min": 26.0}, "VIN": {"avg": 71.78, "peak": 111.8, "min": 54.24}, "VDD_CPU_SOC_MSS": {"avg": 15.45, "peak": 16.14, "min": 14.96}}, "power_watts_avg": 33.03, "energy_joules_est": 112.83, "sample_count": 26, "duration_seconds": 3.416}, "timestamp": "2026-01-16T15:12:59.033083"}
{"image_index": 25, "image_name": "000000002157.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002157.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2171.222, "latencies_ms": [2171.222], "images_per_second": 0.461, "prompt_tokens": 9, "response_tokens_est": 57, "n_tiles": 6, "output_text": "The image shows a beautifully arranged table with a variety of food items, including a large white cake with fresh berries, a plate of cheese and crackers, a glass of water, and a selection of grapes and other fruits, all set on a red tablecloth.", "error": null, "sys_before": {"cpu_percent": 41.4, "ram_used_mb": 25731.2, "ram_available_mb": 100041.0, "ram_percent": 20.5}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 25731.4, "ram_available_mb": 100040.8, "ram_percent": 20.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.25, "peak": 14.71, "min": 13.51}, "VDD_GPU": {"avg": 28.83, "peak": 37.42, "min": 23.25}, "VIN": {"avg": 71.26, "peak": 112.38, "min": 62.0}, "VDD_CPU_SOC_MSS": {"avg": 15.01, "peak": 15.36, "min": 14.18}}, "power_watts_avg": 28.83, "energy_joules_est": 62.61, "sample_count": 16, "duration_seconds": 2.172}, "timestamp": "2026-01-16T15:13:01.258939"}
{"image_index": 25, "image_name": "000000002157.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002157.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 4221.835, "latencies_ms": [4221.835], "images_per_second": 0.237, "prompt_tokens": 23, "response_tokens_est": 128, "n_tiles": 6, "output_text": "object: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject:", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25731.4, "ram_available_mb": 100040.8, "ram_percent": 20.5}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 25731.2, "ram_available_mb": 100041.0, "ram_percent": 20.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.55, "peak": 14.71, "min": 13.72}, "VDD_GPU": {"avg": 25.8, "peak": 37.82, "min": 22.86}, "VIN": {"avg": 66.45, "peak": 111.22, "min": 51.1}, "VDD_CPU_SOC_MSS": {"avg": 15.23, "peak": 15.36, "min": 14.57}}, "power_watts_avg": 25.8, "energy_joules_est": 108.93, "sample_count": 33, "duration_seconds": 4.222}, "timestamp": "2026-01-16T15:13:05.488782"}
{"image_index": 25, "image_name": "000000002157.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002157.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2306.813, "latencies_ms": [2306.813], "images_per_second": 0.433, "prompt_tokens": 27, "response_tokens_est": 60, "n_tiles": 6, "output_text": "The main object in the foreground is a white cake with a blueberry and raspberry topping. To the right of the cake, there is a plate with various cheeses, grapes, and bread. In the background, there are multiple glasses and plates arranged on a red tablecloth.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25731.2, "ram_available_mb": 100041.0, "ram_percent": 20.5}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 25731.2, "ram_available_mb": 100041.0, "ram_percent": 20.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.38, "peak": 14.71, "min": 13.72}, "VDD_GPU": {"avg": 28.7, "peak": 37.44, "min": 22.86}, "VIN": {"avg": 65.41, "peak": 93.82, "min": 45.66}, "VDD_CPU_SOC_MSS": {"avg": 15.01, "peak": 15.36, "min": 14.18}}, "power_watts_avg": 28.7, "energy_joules_est": 66.22, "sample_count": 17, "duration_seconds": 2.307}, "timestamp": "2026-01-16T15:13:07.801788"}
{"image_index": 25, "image_name": "000000002157.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002157.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3229.548, "latencies_ms": [3229.548], "images_per_second": 0.31, "prompt_tokens": 21, "response_tokens_est": 93, "n_tiles": 6, "output_text": "The image depicts a well-arranged outdoor dining setup with a red tablecloth. The table is set with a variety of food items, including a large white cake with fresh berries, a plate of cheese and crackers, a bowl of grapes, and a selection of bread and other snacks. The scene suggests a casual, yet elegant gathering, possibly a wedding or a celebratory event, with the focus on the delicious spread of food.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25731.2, "ram_available_mb": 100041.0, "ram_percent": 20.5}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 25731.2, "ram_available_mb": 100041.0, "ram_percent": 20.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.49, "peak": 14.71, "min": 13.72}, "VDD_GPU": {"avg": 26.82, "peak": 37.82, "min": 22.86}, "VIN": {"avg": 66.1, "peak": 115.9, "min": 55.12}, "VDD_CPU_SOC_MSS": {"avg": 15.15, "peak": 15.36, "min": 14.57}}, "power_watts_avg": 26.82, "energy_joules_est": 86.62, "sample_count": 25, "duration_seconds": 3.23}, "timestamp": "2026-01-16T15:13:11.037360"}
{"image_index": 25, "image_name": "000000002157.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002157.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1934.06, "latencies_ms": [1934.06], "images_per_second": 0.517, "prompt_tokens": 19, "response_tokens_est": 47, "n_tiles": 6, "output_text": "The image showcases a vibrant and colorful spread of food and drinks on a red tablecloth. The lighting is bright and natural, suggesting a sunny day, with the sunlight casting shadows and highlighting the textures of the food items.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25731.2, "ram_available_mb": 100041.0, "ram_percent": 20.5}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 25731.2, "ram_available_mb": 100041.0, "ram_percent": 20.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.35, "peak": 14.61, "min": 13.72}, "VDD_GPU": {"avg": 29.64, "peak": 37.42, "min": 23.64}, "VIN": {"avg": 66.92, "peak": 92.93, "min": 58.32}, "VDD_CPU_SOC_MSS": {"avg": 15.07, "peak": 15.36, "min": 14.57}}, "power_watts_avg": 29.64, "energy_joules_est": 57.33, "sample_count": 14, "duration_seconds": 1.934}, "timestamp": "2026-01-16T15:13:12.977593"}
{"image_index": 26, "image_name": "000000002261.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002261.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 937.274, "latencies_ms": [937.274], "images_per_second": 1.067, "prompt_tokens": 9, "response_tokens_est": 13, "n_tiles": 6, "output_text": "A man is surfing in the ocean, riding a wave.", "error": null, "sys_before": {"cpu_percent": 43.2, "ram_used_mb": 25731.2, "ram_available_mb": 100041.0, "ram_percent": 20.5}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 25731.4, "ram_available_mb": 100040.8, "ram_percent": 20.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 13.84, "peak": 14.02, "min": 13.49}, "VDD_GPU": {"avg": 34.01, "peak": 37.03, "min": 29.95}, "VIN": {"avg": 81.12, "peak": 109.22, "min": 59.15}, "VDD_CPU_SOC_MSS": {"avg": 14.7, "peak": 14.96, "min": 14.18}}, "power_watts_avg": 34.01, "energy_joules_est": 31.89, "sample_count": 6, "duration_seconds": 0.938}, "timestamp": "2026-01-16T15:13:13.973925"}
{"image_index": 26, "image_name": "000000002261.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002261.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1695.265, "latencies_ms": [1695.265], "images_per_second": 0.59, "prompt_tokens": 23, "response_tokens_est": 40, "n_tiles": 6, "output_text": "object: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25731.4, "ram_available_mb": 100040.8, "ram_percent": 20.5}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 25731.4, "ram_available_mb": 100040.8, "ram_percent": 20.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.37, "peak": 14.81, "min": 13.82}, "VDD_GPU": {"avg": 32.08, "peak": 39.0, "min": 24.83}, "VIN": {"avg": 70.16, "peak": 110.58, "min": 58.43}, "VDD_CPU_SOC_MSS": {"avg": 15.16, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 32.08, "energy_joules_est": 54.4, "sample_count": 12, "duration_seconds": 1.696}, "timestamp": "2026-01-16T15:13:15.675309"}
{"image_index": 26, "image_name": "000000002261.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002261.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3159.446, "latencies_ms": [3159.446], "images_per_second": 0.317, "prompt_tokens": 27, "response_tokens_est": 94, "n_tiles": 6, "output_text": "The main object in the image is a person riding a surfboard in the water. The person is positioned in the foreground, slightly to the left, and is the closest to the viewer. The surfboard is also in the foreground, closer to the person. The background consists of the ocean, which is slightly blurred, indicating the depth of the water. The person is near the water's surface, and the surfboard is closer to the water's bottom.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 25731.4, "ram_available_mb": 100040.8, "ram_percent": 20.5}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 25731.6, "ram_available_mb": 100040.5, "ram_percent": 20.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.67, "peak": 14.91, "min": 13.92}, "VDD_GPU": {"avg": 27.55, "peak": 38.21, "min": 23.25}, "VIN": {"avg": 69.01, "peak": 117.57, "min": 56.02}, "VDD_CPU_SOC_MSS": {"avg": 15.42, "peak": 15.76, "min": 14.57}}, "power_watts_avg": 27.55, "energy_joules_est": 87.06, "sample_count": 24, "duration_seconds": 3.16}, "timestamp": "2026-01-16T15:13:18.841524"}
{"image_index": 26, "image_name": "000000002261.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002261.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1969.293, "latencies_ms": [1969.293], "images_per_second": 0.508, "prompt_tokens": 21, "response_tokens_est": 50, "n_tiles": 6, "output_text": "The image depicts a person surfing in the ocean. The individual is riding a wave, with water splashing around them. The scene is set in a natural environment, likely a beach, with the ocean's greenish hue indicating shallow waters.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25731.6, "ram_available_mb": 100040.5, "ram_percent": 20.5}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 25731.6, "ram_available_mb": 100040.5, "ram_percent": 20.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.55, "peak": 14.91, "min": 13.92}, "VDD_GPU": {"avg": 29.76, "peak": 37.42, "min": 23.64}, "VIN": {"avg": 69.71, "peak": 103.79, "min": 59.27}, "VDD_CPU_SOC_MSS": {"avg": 15.33, "peak": 15.75, "min": 14.57}}, "power_watts_avg": 29.76, "energy_joules_est": 58.62, "sample_count": 15, "duration_seconds": 1.97}, "timestamp": "2026-01-16T15:13:20.817105"}
{"image_index": 26, "image_name": "000000002261.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002261.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1967.705, "latencies_ms": [1967.705], "images_per_second": 0.508, "prompt_tokens": 19, "response_tokens_est": 50, "n_tiles": 6, "output_text": "The image depicts a person riding a surfboard in the ocean. The water is a vibrant green, indicating a possible algae bloom or the presence of marine life. The lighting is bright and natural, suggesting it is daytime with clear weather.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25731.6, "ram_available_mb": 100040.5, "ram_percent": 20.5}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 25731.4, "ram_available_mb": 100040.8, "ram_percent": 20.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.55, "peak": 14.91, "min": 13.92}, "VDD_GPU": {"avg": 29.97, "peak": 37.82, "min": 24.04}, "VIN": {"avg": 71.74, "peak": 114.16, "min": 56.05}, "VDD_CPU_SOC_MSS": {"avg": 15.22, "peak": 15.75, "min": 14.57}}, "power_watts_avg": 29.97, "energy_joules_est": 58.98, "sample_count": 15, "duration_seconds": 1.968}, "timestamp": "2026-01-16T15:13:22.790726"}
{"image_index": 27, "image_name": "000000002299.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002299.jpg", "image_width": 500, "image_height": 302, "image_resolution": "500x302", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1474.079, "latencies_ms": [1474.079], "images_per_second": 0.678, "prompt_tokens": 9, "response_tokens_est": 33, "n_tiles": 6, "output_text": "The image is a black and white photograph of a group of children, likely from a school, posing for a photo in front of a building with stone walls.", "error": null, "sys_before": {"cpu_percent": 45.7, "ram_used_mb": 25731.4, "ram_available_mb": 100040.8, "ram_percent": 20.5}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 25731.4, "ram_available_mb": 100040.8, "ram_percent": 20.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.15, "peak": 14.71, "min": 13.61}, "VDD_GPU": {"avg": 30.95, "peak": 37.03, "min": 25.22}, "VIN": {"avg": 69.38, "peak": 102.64, "min": 50.01}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.36, "min": 14.18}}, "power_watts_avg": 30.95, "energy_joules_est": 45.63, "sample_count": 11, "duration_seconds": 1.474}, "timestamp": "2026-01-16T15:13:24.320435"}
{"image_index": 27, "image_name": "000000002299.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002299.jpg", "image_width": 500, "image_height": 302, "image_resolution": "500x302", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 4066.497, "latencies_ms": [4066.497], "images_per_second": 0.246, "prompt_tokens": 23, "response_tokens_est": 128, "n_tiles": 6, "output_text": "- 1 boy: 2\n- 1 girl: 2\n- 1 boy: 2\n- 1 girl: 2\n- 1 boy: 2\n- 1 girl: 2\n- 1 boy: 2\n- 1 girl: 2\n- 1 boy: 2\n- 1 girl: 2\n- 1 boy: 2\n- 1 girl: 2\n- 1 boy: 2\n- 1 girl: 2\n- 1 boy: 2\n- 1 girl: 2", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25731.4, "ram_available_mb": 100040.8, "ram_percent": 20.5}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 25731.4, "ram_available_mb": 100040.8, "ram_percent": 20.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.73, "peak": 14.91, "min": 13.92}, "VDD_GPU": {"avg": 26.45, "peak": 37.82, "min": 23.25}, "VIN": {"avg": 64.91, "peak": 77.18, "min": 54.76}, "VDD_CPU_SOC_MSS": {"avg": 15.47, "peak": 15.76, "min": 14.57}}, "power_watts_avg": 26.45, "energy_joules_est": 107.57, "sample_count": 32, "duration_seconds": 4.067}, "timestamp": "2026-01-16T15:13:28.393184"}
{"image_index": 27, "image_name": "000000002299.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002299.jpg", "image_width": 500, "image_height": 302, "image_resolution": "500x302", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2132.475, "latencies_ms": [2132.475], "images_per_second": 0.469, "prompt_tokens": 27, "response_tokens_est": 56, "n_tiles": 6, "output_text": "The main objects in the image are the children, who are positioned in the foreground. The children are arranged in a semi-circular formation, with some sitting on the ground and others standing. The background features a stone wall, providing a solid and stable backdrop for the children.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25731.4, "ram_available_mb": 100040.8, "ram_percent": 20.5}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 25731.4, "ram_available_mb": 100040.8, "ram_percent": 20.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.61, "peak": 14.91, "min": 13.92}, "VDD_GPU": {"avg": 29.4, "peak": 37.42, "min": 23.64}, "VIN": {"avg": 67.06, "peak": 117.86, "min": 54.48}, "VDD_CPU_SOC_MSS": {"avg": 15.26, "peak": 15.75, "min": 14.57}}, "power_watts_avg": 29.4, "energy_joules_est": 62.71, "sample_count": 16, "duration_seconds": 2.133}, "timestamp": "2026-01-16T15:13:30.531381"}
{"image_index": 27, "image_name": "000000002299.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002299.jpg", "image_width": 500, "image_height": 302, "image_resolution": "500x302", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2404.378, "latencies_ms": [2404.378], "images_per_second": 0.416, "prompt_tokens": 21, "response_tokens_est": 66, "n_tiles": 6, "output_text": "The image depicts a group of children, likely from a school, posing for a group photo outdoors. The children are dressed in various school uniforms, and they are arranged in a semi-circular formation, with some sitting on the ground and others standing. The setting appears to be a schoolyard or a similar educational environment.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25731.4, "ram_available_mb": 100040.8, "ram_percent": 20.5}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 25731.4, "ram_available_mb": 100040.8, "ram_percent": 20.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.59, "peak": 14.91, "min": 13.92}, "VDD_GPU": {"avg": 28.7, "peak": 37.42, "min": 23.25}, "VIN": {"avg": 67.43, "peak": 89.9, "min": 58.05}, "VDD_CPU_SOC_MSS": {"avg": 15.36, "peak": 15.76, "min": 14.57}}, "power_watts_avg": 28.7, "energy_joules_est": 69.02, "sample_count": 18, "duration_seconds": 2.405}, "timestamp": "2026-01-16T15:13:32.941944"}
{"image_index": 27, "image_name": "000000002299.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002299.jpg", "image_width": 500, "image_height": 302, "image_resolution": "500x302", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2687.575, "latencies_ms": [2687.575], "images_per_second": 0.372, "prompt_tokens": 19, "response_tokens_est": 77, "n_tiles": 6, "output_text": "The black and white photograph features a group of children in a school setting, with notable visual attributes such as the use of black and white photography, a classic and timeless look, and a soft, diffused lighting that highlights the children's faces and clothing. The children are dressed in various school uniforms, with some wearing dresses and others in suits, creating a diverse and colorful appearance.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25731.4, "ram_available_mb": 100040.8, "ram_percent": 20.5}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 25731.4, "ram_available_mb": 100040.8, "ram_percent": 20.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.65, "peak": 15.01, "min": 13.92}, "VDD_GPU": {"avg": 28.14, "peak": 37.82, "min": 23.25}, "VIN": {"avg": 69.59, "peak": 103.19, "min": 62.37}, "VDD_CPU_SOC_MSS": {"avg": 15.38, "peak": 15.75, "min": 14.57}}, "power_watts_avg": 28.14, "energy_joules_est": 75.64, "sample_count": 21, "duration_seconds": 2.688}, "timestamp": "2026-01-16T15:13:35.635551"}
{"image_index": 28, "image_name": "000000002431.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002431.jpg", "image_width": 457, "image_height": 640, "image_resolution": "457x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2550.918, "latencies_ms": [2550.918], "images_per_second": 0.392, "prompt_tokens": 9, "response_tokens_est": 34, "n_tiles": 12, "output_text": "The image shows a plate of bread with a slice of grilled bread on it, accompanied by a small bowl of sauce, a wine glass, and a napkin.", "error": null, "sys_before": {"cpu_percent": 44.2, "ram_used_mb": 25731.4, "ram_available_mb": 100040.8, "ram_percent": 20.5}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 25731.4, "ram_available_mb": 100040.8, "ram_percent": 20.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.84, "peak": 15.85, "min": 13.69}, "VDD_GPU": {"avg": 33.88, "peak": 40.18, "min": 26.79}, "VIN": {"avg": 74.02, "peak": 124.21, "min": 57.35}, "VDD_CPU_SOC_MSS": {"avg": 15.77, "peak": 16.93, "min": 14.96}}, "power_watts_avg": 33.88, "energy_joules_est": 86.43, "sample_count": 20, "duration_seconds": 2.551}, "timestamp": "2026-01-16T15:13:38.278013"}
{"image_index": 28, "image_name": "000000002431.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002431.jpg", "image_width": 457, "image_height": 640, "image_resolution": "457x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2435.073, "latencies_ms": [2435.073], "images_per_second": 0.411, "prompt_tokens": 23, "response_tokens_est": 27, "n_tiles": 12, "output_text": "bread: 4\nbowl: 1\nglass: 1\nfork: 1\nspoon: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25731.4, "ram_available_mb": 100040.8, "ram_percent": 20.5}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 25731.4, "ram_available_mb": 100040.8, "ram_percent": 20.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.8, "peak": 15.34, "min": 14.32}, "VDD_GPU": {"avg": 35.87, "peak": 42.54, "min": 27.57}, "VIN": {"avg": 76.21, "peak": 116.06, "min": 57.45}, "VDD_CPU_SOC_MSS": {"avg": 15.54, "peak": 16.14, "min": 14.96}}, "power_watts_avg": 35.87, "energy_joules_est": 87.36, "sample_count": 19, "duration_seconds": 2.436}, "timestamp": "2026-01-16T15:13:40.720326"}
{"image_index": 28, "image_name": "000000002431.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002431.jpg", "image_width": 457, "image_height": 640, "image_resolution": "457x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 4051.505, "latencies_ms": [4051.505], "images_per_second": 0.247, "prompt_tokens": 27, "response_tokens_est": 75, "n_tiles": 12, "output_text": "The main objects in the image are a plate of bread, a glass of red wine, and a white bowl. The plate of bread is in the foreground, with the glass of red wine and the white bowl placed behind it. The glass of red wine is positioned slightly to the right of the plate, while the white bowl is placed near the center of the plate.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 25731.4, "ram_available_mb": 100040.8, "ram_percent": 20.5}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 25731.6, "ram_available_mb": 100040.6, "ram_percent": 20.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.8, "peak": 15.34, "min": 14.22}, "VDD_GPU": {"avg": 32.13, "peak": 42.15, "min": 26.01}, "VIN": {"avg": 69.31, "peak": 103.67, "min": 57.15}, "VDD_CPU_SOC_MSS": {"avg": 15.47, "peak": 16.14, "min": 14.96}}, "power_watts_avg": 32.13, "energy_joules_est": 130.19, "sample_count": 31, "duration_seconds": 4.052}, "timestamp": "2026-01-16T15:13:44.778421"}
{"image_index": 28, "image_name": "000000002431.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002431.jpg", "image_width": 457, "image_height": 640, "image_resolution": "457x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 4463.512, "latencies_ms": [4463.512], "images_per_second": 0.224, "prompt_tokens": 21, "response_tokens_est": 87, "n_tiles": 12, "output_text": "The image depicts a dining setting with a focus on a plate of bread and a glass of red wine. The bread is placed on a white plate, and there is a white napkin draped over the plate. The glass of red wine is positioned to the right of the plate, and a silver knife is resting on the table. The overall scene suggests a casual dining experience, possibly in a restaurant or a home setting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25731.6, "ram_available_mb": 100040.6, "ram_percent": 20.5}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 25731.3, "ram_available_mb": 100040.8, "ram_percent": 20.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.8, "peak": 15.34, "min": 14.22}, "VDD_GPU": {"avg": 31.28, "peak": 42.15, "min": 26.01}, "VIN": {"avg": 73.76, "peak": 133.72, "min": 56.98}, "VDD_CPU_SOC_MSS": {"avg": 15.45, "peak": 16.14, "min": 14.96}}, "power_watts_avg": 31.28, "energy_joules_est": 139.63, "sample_count": 35, "duration_seconds": 4.464}, "timestamp": "2026-01-16T15:13:49.248891"}
{"image_index": 28, "image_name": "000000002431.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002431.jpg", "image_width": 457, "image_height": 640, "image_resolution": "457x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3642.35, "latencies_ms": [3642.35], "images_per_second": 0.275, "prompt_tokens": 19, "response_tokens_est": 63, "n_tiles": 12, "output_text": "The image features a rustic wooden table with a white plate containing grilled bread slices. The lighting is warm and natural, casting a soft glow on the scene. The bread has a golden-brown crust, indicating it has been grilled, and the plate is placed on a white cloth, adding to the rustic aesthetic.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25731.3, "ram_available_mb": 100040.8, "ram_percent": 20.5}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25731.3, "ram_available_mb": 100040.8, "ram_percent": 20.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.79, "peak": 15.34, "min": 14.2}, "VDD_GPU": {"avg": 32.6, "peak": 42.15, "min": 26.01}, "VIN": {"avg": 72.7, "peak": 119.91, "min": 57.18}, "VDD_CPU_SOC_MSS": {"avg": 15.48, "peak": 16.14, "min": 14.96}}, "power_watts_avg": 32.6, "energy_joules_est": 118.76, "sample_count": 28, "duration_seconds": 3.643}, "timestamp": "2026-01-16T15:13:52.897583"}
{"image_index": 29, "image_name": "000000002473.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002473.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1254.772, "latencies_ms": [1254.772], "images_per_second": 0.797, "prompt_tokens": 9, "response_tokens_est": 25, "n_tiles": 6, "output_text": "A snowboarder is performing a jump over a snowy slope, with trees and clear blue skies in the background.", "error": null, "sys_before": {"cpu_percent": 40.0, "ram_used_mb": 25731.3, "ram_available_mb": 100040.8, "ram_percent": 20.5}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25731.8, "ram_available_mb": 100040.3, "ram_percent": 20.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.03, "peak": 14.51, "min": 13.51}, "VDD_GPU": {"avg": 32.57, "peak": 37.42, "min": 26.8}, "VIN": {"avg": 67.38, "peak": 110.92, "min": 51.0}, "VDD_CPU_SOC_MSS": {"avg": 14.83, "peak": 15.36, "min": 14.18}}, "power_watts_avg": 32.57, "energy_joules_est": 40.88, "sample_count": 9, "duration_seconds": 1.255}, "timestamp": "2026-01-16T15:13:54.216056"}
{"image_index": 29, "image_name": "000000002473.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002473.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1766.31, "latencies_ms": [1766.31], "images_per_second": 0.566, "prompt_tokens": 23, "response_tokens_est": 41, "n_tiles": 6, "output_text": "1. Person skiing\n2. Person standing\n3. Person standing\n4. Person standing\n5. Person standing\n6. Person standing\n7. Person standing\n8. Person standing", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25731.8, "ram_available_mb": 100040.3, "ram_percent": 20.5}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 25731.8, "ram_available_mb": 100040.3, "ram_percent": 20.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.33, "peak": 14.61, "min": 13.82}, "VDD_GPU": {"avg": 31.13, "peak": 38.6, "min": 24.04}, "VIN": {"avg": 68.15, "peak": 100.08, "min": 54.64}, "VDD_CPU_SOC_MSS": {"avg": 15.02, "peak": 15.36, "min": 14.57}}, "power_watts_avg": 31.13, "energy_joules_est": 55.0, "sample_count": 13, "duration_seconds": 1.767}, "timestamp": "2026-01-16T15:13:55.988377"}
{"image_index": 29, "image_name": "000000002473.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002473.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2464.775, "latencies_ms": [2464.775], "images_per_second": 0.406, "prompt_tokens": 27, "response_tokens_est": 66, "n_tiles": 6, "output_text": "The main objects in the image are a skier in mid-air, a snowboarder standing on the snow, and a group of people standing on the snowy slope. The skier is in the foreground, the snowboarder is in the background, and the group of people is near the bottom of the slope.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25731.8, "ram_available_mb": 100040.3, "ram_percent": 20.5}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 25731.8, "ram_available_mb": 100040.4, "ram_percent": 20.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.46, "peak": 14.81, "min": 13.72}, "VDD_GPU": {"avg": 28.35, "peak": 37.82, "min": 22.86}, "VIN": {"avg": 65.83, "peak": 84.98, "min": 59.17}, "VDD_CPU_SOC_MSS": {"avg": 15.09, "peak": 15.36, "min": 14.57}}, "power_watts_avg": 28.35, "energy_joules_est": 69.89, "sample_count": 19, "duration_seconds": 2.465}, "timestamp": "2026-01-16T15:13:58.459206"}
{"image_index": 29, "image_name": "000000002473.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002473.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1875.891, "latencies_ms": [1875.891], "images_per_second": 0.533, "prompt_tokens": 21, "response_tokens_est": 45, "n_tiles": 6, "output_text": "The image depicts a snowy mountainous landscape under a clear blue sky. A skier is captured mid-air, performing a jump, while two other individuals are standing on the snowy slope, observing the action.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25731.8, "ram_available_mb": 100040.4, "ram_percent": 20.5}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 25731.8, "ram_available_mb": 100040.4, "ram_percent": 20.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.4, "peak": 14.71, "min": 13.82}, "VDD_GPU": {"avg": 30.03, "peak": 37.82, "min": 23.64}, "VIN": {"avg": 73.77, "peak": 123.13, "min": 61.03}, "VDD_CPU_SOC_MSS": {"avg": 15.07, "peak": 15.36, "min": 14.57}}, "power_watts_avg": 30.03, "energy_joules_est": 56.35, "sample_count": 14, "duration_seconds": 1.876}, "timestamp": "2026-01-16T15:14:00.341090"}
{"image_index": 29, "image_name": "000000002473.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002473.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2211.691, "latencies_ms": [2211.691], "images_per_second": 0.452, "prompt_tokens": 19, "response_tokens_est": 57, "n_tiles": 6, "output_text": "The image depicts a snowy landscape with a clear blue sky. The snow is pristine and untouched, indicating a recent snowfall. The scene is illuminated by the bright sunlight, casting shadows and highlighting the textures of the snow and the skiers' attire.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25731.8, "ram_available_mb": 100040.4, "ram_percent": 20.5}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 25732.1, "ram_available_mb": 100040.1, "ram_percent": 20.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.42, "peak": 14.71, "min": 13.72}, "VDD_GPU": {"avg": 28.97, "peak": 37.82, "min": 23.25}, "VIN": {"avg": 70.96, "peak": 109.08, "min": 58.61}, "VDD_CPU_SOC_MSS": {"avg": 15.15, "peak": 15.36, "min": 14.57}}, "power_watts_avg": 28.97, "energy_joules_est": 64.08, "sample_count": 17, "duration_seconds": 2.212}, "timestamp": "2026-01-16T15:14:02.558482"}
{"image_index": 30, "image_name": "000000002532.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002532.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2346.772, "latencies_ms": [2346.772], "images_per_second": 0.426, "prompt_tokens": 9, "response_tokens_est": 28, "n_tiles": 12, "output_text": "A person is skiing down a snowy mountain slope, with their skis cutting through the snow and their poles guiding their movement.", "error": null, "sys_before": {"cpu_percent": 46.4, "ram_used_mb": 25732.1, "ram_available_mb": 100040.1, "ram_percent": 20.5}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 25732.3, "ram_available_mb": 100039.9, "ram_percent": 20.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.86, "peak": 15.85, "min": 13.59}, "VDD_GPU": {"avg": 34.76, "peak": 40.57, "min": 27.59}, "VIN": {"avg": 77.07, "peak": 117.47, "min": 57.23}, "VDD_CPU_SOC_MSS": {"avg": 15.82, "peak": 16.93, "min": 14.96}}, "power_watts_avg": 34.76, "energy_joules_est": 81.59, "sample_count": 17, "duration_seconds": 2.347}, "timestamp": "2026-01-16T15:14:04.997505"}
{"image_index": 30, "image_name": "000000002532.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002532.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2880.821, "latencies_ms": [2880.821], "images_per_second": 0.347, "prompt_tokens": 23, "response_tokens_est": 40, "n_tiles": 12, "output_text": "1. Skier\n2. Snow\n3. Ski poles\n4. Snowboard\n5. Snow\n6. Snowboard tracks\n7. Snowboard\n8. Snowboard tracks", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25732.3, "ram_available_mb": 100039.9, "ram_percent": 20.5}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 25732.3, "ram_available_mb": 100039.9, "ram_percent": 20.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.79, "peak": 15.34, "min": 14.2}, "VDD_GPU": {"avg": 34.67, "peak": 42.94, "min": 26.4}, "VIN": {"avg": 70.54, "peak": 101.61, "min": 55.5}, "VDD_CPU_SOC_MSS": {"avg": 15.48, "peak": 16.14, "min": 14.96}}, "power_watts_avg": 34.67, "energy_joules_est": 99.89, "sample_count": 22, "duration_seconds": 2.881}, "timestamp": "2026-01-16T15:14:07.885006"}
{"image_index": 30, "image_name": "000000002532.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002532.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 4024.471, "latencies_ms": [4024.471], "images_per_second": 0.248, "prompt_tokens": 27, "response_tokens_est": 74, "n_tiles": 12, "output_text": "The main objects in the image are a person skiing and a snowy landscape. The person is positioned in the foreground, with their back to the camera, skiing down the snowy slope. The snowy landscape is in the background, with mountains and trees visible. The person is near the center of the image, while the landscape extends towards the horizon.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25732.3, "ram_available_mb": 100039.9, "ram_percent": 20.5}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25732.0, "ram_available_mb": 100040.1, "ram_percent": 20.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.81, "peak": 15.34, "min": 14.2}, "VDD_GPU": {"avg": 32.09, "peak": 42.54, "min": 26.01}, "VIN": {"avg": 75.48, "peak": 127.76, "min": 57.72}, "VDD_CPU_SOC_MSS": {"avg": 15.44, "peak": 16.14, "min": 14.96}}, "power_watts_avg": 32.09, "energy_joules_est": 129.16, "sample_count": 31, "duration_seconds": 4.025}, "timestamp": "2026-01-16T15:14:11.919860"}
{"image_index": 30, "image_name": "000000002532.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002532.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 4019.781, "latencies_ms": [4019.781], "images_per_second": 0.249, "prompt_tokens": 21, "response_tokens_est": 74, "n_tiles": 12, "output_text": "The image depicts a snowy landscape with a person standing on skis, holding ski poles. The individual is dressed in winter attire, including a green top and black pants, and is surrounded by snow-covered terrain with a few scattered rocks and a few trees. The sky is mostly clear with a few scattered clouds, indicating a bright and sunny day.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25732.0, "ram_available_mb": 100040.1, "ram_percent": 20.5}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25732.0, "ram_available_mb": 100040.1, "ram_percent": 20.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.8, "peak": 15.44, "min": 14.2}, "VDD_GPU": {"avg": 31.93, "peak": 42.15, "min": 26.01}, "VIN": {"avg": 74.17, "peak": 118.48, "min": 57.11}, "VDD_CPU_SOC_MSS": {"avg": 15.52, "peak": 16.14, "min": 14.96}}, "power_watts_avg": 31.93, "energy_joules_est": 128.37, "sample_count": 31, "duration_seconds": 4.02}, "timestamp": "2026-01-16T15:14:15.945991"}
{"image_index": 30, "image_name": "000000002532.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002532.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2773.91, "latencies_ms": [2773.91], "images_per_second": 0.361, "prompt_tokens": 19, "response_tokens_est": 37, "n_tiles": 12, "output_text": "The image depicts a snowy landscape under a clear blue sky with scattered white clouds. The scene is illuminated by natural sunlight, casting shadows on the snow-covered ground.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25732.0, "ram_available_mb": 100040.1, "ram_percent": 20.5}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 25732.0, "ram_available_mb": 100040.1, "ram_percent": 20.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.82, "peak": 15.34, "min": 14.22}, "VDD_GPU": {"avg": 34.59, "peak": 42.15, "min": 26.8}, "VIN": {"avg": 75.72, "peak": 127.38, "min": 57.09}, "VDD_CPU_SOC_MSS": {"avg": 15.56, "peak": 16.14, "min": 14.96}}, "power_watts_avg": 34.59, "energy_joules_est": 95.97, "sample_count": 21, "duration_seconds": 2.774}, "timestamp": "2026-01-16T15:14:18.726183"}
{"image_index": 31, "image_name": "000000002587.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002587.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2548.025, "latencies_ms": [2548.025], "images_per_second": 0.392, "prompt_tokens": 9, "response_tokens_est": 34, "n_tiles": 12, "output_text": "The image shows a close-up of a brown, round object with a glossy surface, which appears to be a chocolate-covered donut or a similar treat.", "error": null, "sys_before": {"cpu_percent": 44.8, "ram_used_mb": 25732.0, "ram_available_mb": 100040.1, "ram_percent": 20.5}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 25732.3, "ram_available_mb": 100039.9, "ram_percent": 20.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.86, "peak": 15.85, "min": 13.59}, "VDD_GPU": {"avg": 34.65, "peak": 41.36, "min": 27.18}, "VIN": {"avg": 78.93, "peak": 125.57, "min": 57.37}, "VDD_CPU_SOC_MSS": {"avg": 15.79, "peak": 16.92, "min": 14.57}}, "power_watts_avg": 34.65, "energy_joules_est": 88.31, "sample_count": 19, "duration_seconds": 2.549}, "timestamp": "2026-01-16T15:14:21.368966"}
{"image_index": 31, "image_name": "000000002587.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002587.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1909.859, "latencies_ms": [1909.859], "images_per_second": 0.524, "prompt_tokens": 23, "response_tokens_est": 11, "n_tiles": 12, "output_text": "apple: 1\nbanana: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25732.3, "ram_available_mb": 100039.9, "ram_percent": 20.5}, "sys_after": {"cpu_percent": 2.1, "ram_used_mb": 25732.0, "ram_available_mb": 100040.1, "ram_percent": 20.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.86, "peak": 15.44, "min": 14.22}, "VDD_GPU": {"avg": 38.07, "peak": 42.15, "min": 31.92}, "VIN": {"avg": 82.38, "peak": 129.56, "min": 57.3}, "VDD_CPU_SOC_MSS": {"avg": 15.61, "peak": 16.14, "min": 14.96}}, "power_watts_avg": 38.07, "energy_joules_est": 72.73, "sample_count": 14, "duration_seconds": 1.91}, "timestamp": "2026-01-16T15:14:23.285111"}
{"image_index": 31, "image_name": "000000002587.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002587.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3552.947, "latencies_ms": [3552.947], "images_per_second": 0.281, "prompt_tokens": 27, "response_tokens_est": 60, "n_tiles": 12, "output_text": "The main object in the foreground is a banana, which is yellow and slightly curved. The banana is placed on a dark surface, possibly a table or countertop. In the background, there is a blurred object that appears to be a bag or container, partially visible and out of focus.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25732.0, "ram_available_mb": 100040.1, "ram_percent": 20.5}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25732.0, "ram_available_mb": 100040.1, "ram_percent": 20.5}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.79, "peak": 15.34, "min": 14.12}, "VDD_GPU": {"avg": 33.6, "peak": 44.12, "min": 26.0}, "VIN": {"avg": 72.18, "peak": 116.38, "min": 60.14}, "VDD_CPU_SOC_MSS": {"avg": 15.5, "peak": 16.14, "min": 14.96}}, "power_watts_avg": 33.6, "energy_joules_est": 119.4, "sample_count": 27, "duration_seconds": 3.553}, "timestamp": "2026-01-16T15:14:26.844372"}
{"image_index": 31, "image_name": "000000002587.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002587.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2879.006, "latencies_ms": [2879.006], "images_per_second": 0.347, "prompt_tokens": 21, "response_tokens_est": 40, "n_tiles": 12, "output_text": "The image shows a close-up of a chocolate-covered banana placed on a dark surface. The lighting is dim, and the focus is on the banana, making the background details indistinct.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25732.0, "ram_available_mb": 100040.1, "ram_percent": 20.5}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 25548.2, "ram_available_mb": 100224.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.82, "peak": 15.44, "min": 14.2}, "VDD_GPU": {"avg": 34.65, "peak": 42.54, "min": 26.8}, "VIN": {"avg": 72.23, "peak": 108.23, "min": 57.19}, "VDD_CPU_SOC_MSS": {"avg": 15.61, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 34.65, "energy_joules_est": 99.77, "sample_count": 22, "duration_seconds": 2.88}, "timestamp": "2026-01-16T15:14:29.729751"}
{"image_index": 31, "image_name": "000000002587.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002587.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3282.534, "latencies_ms": [3282.534], "images_per_second": 0.305, "prompt_tokens": 19, "response_tokens_est": 52, "n_tiles": 12, "output_text": "The image features a close-up of a chocolate-covered banana with a glossy, smooth surface. The lighting is dim, casting a warm, golden hue over the scene, enhancing the rich brown color of the chocolate and the creamy yellow of the banana.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25548.2, "ram_available_mb": 100224.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 25548.2, "ram_available_mb": 100224.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.8, "peak": 15.34, "min": 14.2}, "VDD_GPU": {"avg": 33.68, "peak": 42.54, "min": 26.01}, "VIN": {"avg": 71.75, "peak": 114.18, "min": 57.19}, "VDD_CPU_SOC_MSS": {"avg": 15.48, "peak": 16.14, "min": 14.96}}, "power_watts_avg": 33.68, "energy_joules_est": 110.57, "sample_count": 25, "duration_seconds": 3.283}, "timestamp": "2026-01-16T15:14:33.019250"}
{"image_index": 32, "image_name": "000000002592.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002592.jpg", "image_width": 640, "image_height": 366, "image_resolution": "640x366", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1448.401, "latencies_ms": [1448.401], "images_per_second": 0.69, "prompt_tokens": 9, "response_tokens_est": 32, "n_tiles": 6, "output_text": "The image shows a white mug with a black skull and crossbones design, and a black knife with a silver blade lying on a striped surface.", "error": null, "sys_before": {"cpu_percent": 37.7, "ram_used_mb": 25548.2, "ram_available_mb": 100224.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25548.4, "ram_available_mb": 100223.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.22, "peak": 14.71, "min": 13.61}, "VDD_GPU": {"avg": 31.45, "peak": 37.82, "min": 25.22}, "VIN": {"avg": 70.0, "peak": 111.43, "min": 56.66}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.36, "min": 14.18}}, "power_watts_avg": 31.45, "energy_joules_est": 45.57, "sample_count": 11, "duration_seconds": 1.449}, "timestamp": "2026-01-16T15:14:34.520077"}
{"image_index": 32, "image_name": "000000002592.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002592.jpg", "image_width": 640, "image_height": 366, "image_resolution": "640x366", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1705.989, "latencies_ms": [1705.989], "images_per_second": 0.586, "prompt_tokens": 23, "response_tokens_est": 40, "n_tiles": 6, "output_text": "1. Skull and crossed bones on mug\n2. Knife\n3. Mug\n4. Knife\n5. Mug\n6. Knife\n7. Mug\n8. Knife", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25548.4, "ram_available_mb": 100223.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25548.7, "ram_available_mb": 100223.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.54, "peak": 14.91, "min": 14.02}, "VDD_GPU": {"avg": 30.97, "peak": 37.82, "min": 24.43}, "VIN": {"avg": 69.07, "peak": 116.68, "min": 54.72}, "VDD_CPU_SOC_MSS": {"avg": 15.3, "peak": 15.75, "min": 14.57}}, "power_watts_avg": 30.97, "energy_joules_est": 52.85, "sample_count": 13, "duration_seconds": 1.706}, "timestamp": "2026-01-16T15:14:36.232257"}
{"image_index": 32, "image_name": "000000002592.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002592.jpg", "image_width": 640, "image_height": 366, "image_resolution": "640x366", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2559.447, "latencies_ms": [2559.447], "images_per_second": 0.391, "prompt_tokens": 27, "response_tokens_est": 72, "n_tiles": 6, "output_text": "The main objects in the image are a white mug with a skull and crossed bones design, and a black knife with a serrated edge. The white mug is positioned in the foreground, while the black knife is located in the background. The mug is placed on a surface with a striped pattern, and the knife is lying parallel to the mug.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25548.7, "ram_available_mb": 100223.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 25548.9, "ram_available_mb": 100223.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.73, "peak": 15.01, "min": 14.02}, "VDD_GPU": {"avg": 28.56, "peak": 38.21, "min": 23.25}, "VIN": {"avg": 70.74, "peak": 118.15, "min": 55.35}, "VDD_CPU_SOC_MSS": {"avg": 15.45, "peak": 15.76, "min": 14.57}}, "power_watts_avg": 28.56, "energy_joules_est": 73.11, "sample_count": 20, "duration_seconds": 2.56}, "timestamp": "2026-01-16T15:14:38.797035"}
{"image_index": 32, "image_name": "000000002592.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002592.jpg", "image_width": 640, "image_height": 366, "image_resolution": "640x366", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2348.552, "latencies_ms": [2348.552], "images_per_second": 0.426, "prompt_tokens": 21, "response_tokens_est": 64, "n_tiles": 6, "output_text": "The image shows a white mug with a black skull and crossbones design on it, placed on a surface with a striped pattern. To the right of the mug, there is a black knife with a silver blade lying on the same surface. The mug and knife are the only objects visible in the image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25548.9, "ram_available_mb": 100223.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 25556.2, "ram_available_mb": 100216.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.68, "peak": 15.01, "min": 14.02}, "VDD_GPU": {"avg": 28.79, "peak": 37.42, "min": 23.64}, "VIN": {"avg": 71.57, "peak": 106.22, "min": 53.0}, "VDD_CPU_SOC_MSS": {"avg": 15.36, "peak": 15.75, "min": 14.57}}, "power_watts_avg": 28.79, "energy_joules_est": 67.62, "sample_count": 18, "duration_seconds": 2.349}, "timestamp": "2026-01-16T15:14:41.151400"}
{"image_index": 32, "image_name": "000000002592.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002592.jpg", "image_width": 640, "image_height": 366, "image_resolution": "640x366", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2235.596, "latencies_ms": [2235.596], "images_per_second": 0.447, "prompt_tokens": 19, "response_tokens_est": 60, "n_tiles": 6, "output_text": "The image features a white mug with a black skull and crossbones design, and a black knife with a silver blade. The mug and knife are placed on a textured surface, possibly a tablecloth, and the lighting is soft and diffused, creating a calm and neutral atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25556.2, "ram_available_mb": 100216.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 25556.7, "ram_available_mb": 100215.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.67, "peak": 15.01, "min": 14.02}, "VDD_GPU": {"avg": 29.27, "peak": 37.82, "min": 23.64}, "VIN": {"avg": 71.31, "peak": 112.75, "min": 55.62}, "VDD_CPU_SOC_MSS": {"avg": 15.31, "peak": 15.75, "min": 14.57}}, "power_watts_avg": 29.27, "energy_joules_est": 65.45, "sample_count": 17, "duration_seconds": 2.236}, "timestamp": "2026-01-16T15:14:43.393035"}
{"image_index": 33, "image_name": "000000002685.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002685.jpg", "image_width": 640, "image_height": 555, "image_resolution": "640x555", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 814.925, "latencies_ms": [814.925], "images_per_second": 1.227, "prompt_tokens": 9, "response_tokens_est": 34, "n_tiles": 1, "output_text": "A group of people is gathered around a bar counter, with one person in a gray suit and glasses standing at the counter, seemingly engaged in conversation with the others.", "error": null, "sys_before": {"cpu_percent": 24.1, "ram_used_mb": 25556.7, "ram_available_mb": 100215.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25556.6, "ram_available_mb": 100215.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 4845.7, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.03, "peak": 15.42, "min": 14.4}, "VDD_GPU": {"avg": 22.86, "peak": 26.02, "min": 20.89}, "VIN": {"avg": 61.2, "peak": 62.11, "min": 59.78}, "VDD_CPU_SOC_MSS": {"avg": 15.69, "peak": 16.14, "min": 14.97}}, "power_watts_avg": 22.86, "energy_joules_est": 18.65, "sample_count": 6, "duration_seconds": 0.816}, "timestamp": "2026-01-16T15:14:44.232588"}
{"image_index": 33, "image_name": "000000002685.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002685.jpg", "image_width": 640, "image_height": 555, "image_resolution": "640x555", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2858.608, "latencies_ms": [2858.608], "images_per_second": 0.35, "prompt_tokens": 23, "response_tokens_est": 129, "n_tiles": 1, "output_text": "- man: 4\n- woman: 2\n- man: 2\n- woman: 2\n- man: 1\n- woman: 1\n- man: 1\n- woman: 1\n- man: 1\n- woman: 1\n- man: 1\n- woman: 1\n- man: 1\n- woman: 1\n- man: 1\n- woman: 1\n- man: 1\n- woman: 1\n- man: 1\n- woman: 1\n- man: 1\n- woman", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25556.6, "ram_available_mb": 100215.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25556.6, "ram_available_mb": 100215.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 4856.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.48, "peak": 15.72, "min": 15.01}, "VDD_GPU": {"avg": 20.45, "peak": 24.42, "min": 19.7}, "VIN": {"avg": 62.88, "peak": 71.1, "min": 60.23}, "VDD_CPU_SOC_MSS": {"avg": 16.4, "peak": 16.54, "min": 15.75}}, "power_watts_avg": 20.45, "energy_joules_est": 58.47, "sample_count": 22, "duration_seconds": 2.859}, "timestamp": "2026-01-16T15:14:47.097249"}
{"image_index": 33, "image_name": "000000002685.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002685.jpg", "image_width": 640, "image_height": 555, "image_resolution": "640x555", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2135.288, "latencies_ms": [2135.288], "images_per_second": 0.468, "prompt_tokens": 27, "response_tokens_est": 95, "n_tiles": 1, "output_text": "In the image, the main objects are the people and the wine display. The people are positioned in the foreground, with the man in the light blue shirt and the woman in the black top standing near the wine display. The wine bottles are arranged on the counter, and the display case is located to the left of the counter. The background features a wooden counter and shelves with various items, while the far right side shows a person standing near a counter with a laptop.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25556.6, "ram_available_mb": 100215.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25556.6, "ram_available_mb": 100215.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 4859.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.41, "peak": 15.52, "min": 15.22}, "VDD_GPU": {"avg": 20.59, "peak": 23.64, "min": 19.7}, "VIN": {"avg": 63.56, "peak": 69.62, "min": 59.79}, "VDD_CPU_SOC_MSS": {"avg": 16.39, "peak": 16.54, "min": 15.75}}, "power_watts_avg": 20.59, "energy_joules_est": 43.97, "sample_count": 16, "duration_seconds": 2.136}, "timestamp": "2026-01-16T15:14:49.238046"}
{"image_index": 33, "image_name": "000000002685.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002685.jpg", "image_width": 640, "image_height": 555, "image_resolution": "640x555", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1938.521, "latencies_ms": [1938.521], "images_per_second": 0.516, "prompt_tokens": 21, "response_tokens_est": 86, "n_tiles": 1, "output_text": "The scene depicts a lively indoor setting, likely a wine tasting room or a bar, where a group of people is engaged in a social interaction. The room is filled with various wine bottles displayed on shelves and a wooden counter where a person is standing. The individuals appear to be enjoying a conversation and possibly tasting the wine, with one person in a light blue shirt and another in a black shirt standing out among the group.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25556.6, "ram_available_mb": 100215.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25557.1, "ram_available_mb": 100215.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 4854.7, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.38, "peak": 15.62, "min": 15.01}, "VDD_GPU": {"avg": 20.54, "peak": 23.64, "min": 19.7}, "VIN": {"avg": 62.26, "peak": 69.54, "min": 53.93}, "VDD_CPU_SOC_MSS": {"avg": 16.36, "peak": 16.54, "min": 15.75}}, "power_watts_avg": 20.54, "energy_joules_est": 39.83, "sample_count": 15, "duration_seconds": 1.939}, "timestamp": "2026-01-16T15:14:51.186294"}
{"image_index": 33, "image_name": "000000002685.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002685.jpg", "image_width": 640, "image_height": 555, "image_resolution": "640x555", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 739.84, "latencies_ms": [739.84], "images_per_second": 1.352, "prompt_tokens": 19, "response_tokens_est": 31, "n_tiles": 1, "output_text": "The room is painted in a soft teal color, creating a calming atmosphere. The lighting is warm and inviting, enhancing the cozy ambiance.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25557.1, "ram_available_mb": 100215.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 25557.1, "ram_available_mb": 100215.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 4853.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.24, "peak": 15.32, "min": 15.01}, "VDD_GPU": {"avg": 22.14, "peak": 23.64, "min": 20.89}, "VIN": {"avg": 63.95, "peak": 72.04, "min": 61.18}, "VDD_CPU_SOC_MSS": {"avg": 16.23, "peak": 16.54, "min": 16.14}}, "power_watts_avg": 22.14, "energy_joules_est": 16.39, "sample_count": 5, "duration_seconds": 0.74}, "timestamp": "2026-01-16T15:14:51.931794"}
{"image_index": 34, "image_name": "000000002923.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002923.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2209.797, "latencies_ms": [2209.797], "images_per_second": 0.453, "prompt_tokens": 9, "response_tokens_est": 24, "n_tiles": 12, "output_text": "A large white bird is flying in the sky above a grassy field with a dock and boats in the background.", "error": null, "sys_before": {"cpu_percent": 41.9, "ram_used_mb": 25557.1, "ram_available_mb": 100215.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 2.6, "ram_used_mb": 25557.3, "ram_available_mb": 100214.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.01, "peak": 15.95, "min": 14.1}, "VDD_GPU": {"avg": 34.81, "peak": 39.78, "min": 27.97}, "VIN": {"avg": 75.58, "peak": 115.99, "min": 61.93}, "VDD_CPU_SOC_MSS": {"avg": 15.93, "peak": 16.93, "min": 15.35}}, "power_watts_avg": 34.81, "energy_joules_est": 76.93, "sample_count": 17, "duration_seconds": 2.21}, "timestamp": "2026-01-16T15:14:54.218847"}
{"image_index": 34, "image_name": "000000002923.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002923.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2435.112, "latencies_ms": [2435.112], "images_per_second": 0.411, "prompt_tokens": 23, "response_tokens_est": 27, "n_tiles": 12, "output_text": "bird: 1\nboat: 1\nbuilding: 1\ntower: 1\ntube: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25557.3, "ram_available_mb": 100214.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 25557.1, "ram_available_mb": 100215.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.87, "peak": 15.44, "min": 14.32}, "VDD_GPU": {"avg": 36.1, "peak": 42.54, "min": 27.57}, "VIN": {"avg": 80.06, "peak": 144.36, "min": 57.16}, "VDD_CPU_SOC_MSS": {"avg": 15.6, "peak": 16.14, "min": 14.96}}, "power_watts_avg": 36.1, "energy_joules_est": 87.92, "sample_count": 19, "duration_seconds": 2.435}, "timestamp": "2026-01-16T15:14:56.661925"}
{"image_index": 34, "image_name": "000000002923.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002923.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3807.76, "latencies_ms": [3807.76], "images_per_second": 0.263, "prompt_tokens": 27, "response_tokens_est": 67, "n_tiles": 12, "output_text": "The main objects in the image are a large field of tall grass, a white bird in flight, and a docked boat in the background. The docked boat is near the dock, while the white bird is in the foreground. The field of tall grass is in the background, and the docked boat is further back.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25557.1, "ram_available_mb": 100215.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25557.9, "ram_available_mb": 100214.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.82, "peak": 15.34, "min": 14.32}, "VDD_GPU": {"avg": 32.44, "peak": 41.76, "min": 25.6}, "VIN": {"avg": 72.01, "peak": 119.53, "min": 57.72}, "VDD_CPU_SOC_MSS": {"avg": 15.46, "peak": 16.14, "min": 14.96}}, "power_watts_avg": 32.44, "energy_joules_est": 123.53, "sample_count": 29, "duration_seconds": 3.808}, "timestamp": "2026-01-16T15:15:00.477041"}
{"image_index": 34, "image_name": "000000002923.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002923.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 4009.701, "latencies_ms": [4009.701], "images_per_second": 0.249, "prompt_tokens": 21, "response_tokens_est": 73, "n_tiles": 12, "output_text": "The image depicts a serene coastal scene with a large expanse of green grass in the foreground, likely a marsh or wetland area. In the background, there is a dock with various boats and fishing equipment, and a cloudy sky above. A white bird is seen flying in the distance, adding a dynamic element to the tranquil setting.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 25557.9, "ram_available_mb": 100214.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 25557.9, "ram_available_mb": 100214.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.54, "peak": 15.14, "min": 13.92}, "VDD_GPU": {"avg": 31.87, "peak": 41.36, "min": 25.6}, "VIN": {"avg": 70.18, "peak": 115.85, "min": 57.28}, "VDD_CPU_SOC_MSS": {"avg": 15.15, "peak": 16.14, "min": 14.57}}, "power_watts_avg": 31.87, "energy_joules_est": 127.8, "sample_count": 31, "duration_seconds": 4.01}, "timestamp": "2026-01-16T15:15:04.497409"}
{"image_index": 34, "image_name": "000000002923.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000002923.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3364.869, "latencies_ms": [3364.869], "images_per_second": 0.297, "prompt_tokens": 19, "response_tokens_est": 54, "n_tiles": 12, "output_text": "The image depicts a serene coastal scene with a mix of green and brown hues, indicating a mix of vegetation and sandy terrain. The sky is filled with a mix of white and gray clouds, suggesting a partly cloudy day with diffused sunlight.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25557.9, "ram_available_mb": 100214.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 25557.9, "ram_available_mb": 100214.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.5, "peak": 15.04, "min": 13.9}, "VDD_GPU": {"avg": 33.25, "peak": 42.15, "min": 25.61}, "VIN": {"avg": 71.4, "peak": 117.17, "min": 47.7}, "VDD_CPU_SOC_MSS": {"avg": 15.1, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 33.25, "energy_joules_est": 111.9, "sample_count": 26, "duration_seconds": 3.365}, "timestamp": "2026-01-16T15:15:07.868703"}
{"image_index": 35, "image_name": "000000003156.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000003156.jpg", "image_width": 443, "image_height": 640, "image_resolution": "443x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1259.144, "latencies_ms": [1259.144], "images_per_second": 0.794, "prompt_tokens": 9, "response_tokens_est": 25, "n_tiles": 6, "output_text": "A young man is sitting on the toilet, wearing a dark shirt and jeans, with his hands resting on his knees.", "error": null, "sys_before": {"cpu_percent": 38.7, "ram_used_mb": 25557.9, "ram_available_mb": 100214.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 25558.1, "ram_available_mb": 100214.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.03, "peak": 14.51, "min": 13.61}, "VDD_GPU": {"avg": 32.78, "peak": 37.42, "min": 27.19}, "VIN": {"avg": 72.63, "peak": 97.87, "min": 61.12}, "VDD_CPU_SOC_MSS": {"avg": 14.79, "peak": 15.36, "min": 14.18}}, "power_watts_avg": 32.78, "energy_joules_est": 41.28, "sample_count": 9, "duration_seconds": 1.259}, "timestamp": "2026-01-16T15:15:09.188452"}
{"image_index": 35, "image_name": "000000003156.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000003156.jpg", "image_width": 443, "image_height": 640, "image_resolution": "443x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1598.926, "latencies_ms": [1598.926], "images_per_second": 0.625, "prompt_tokens": 23, "response_tokens_est": 35, "n_tiles": 6, "output_text": "1. Man\n2. Toilet\n3. Clothes\n4. Shoes\n5. Floor\n6. Wall\n7. Sink\n8. Ceiling", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25558.1, "ram_available_mb": 100214.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 25559.8, "ram_available_mb": 100212.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.33, "peak": 14.61, "min": 13.82}, "VDD_GPU": {"avg": 31.74, "peak": 38.6, "min": 24.43}, "VIN": {"avg": 66.97, "peak": 96.61, "min": 52.01}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.36, "min": 14.57}}, "power_watts_avg": 31.74, "energy_joules_est": 50.76, "sample_count": 12, "duration_seconds": 1.599}, "timestamp": "2026-01-16T15:15:10.794156"}
{"image_index": 35, "image_name": "000000003156.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000003156.jpg", "image_width": 443, "image_height": 640, "image_resolution": "443x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2364.691, "latencies_ms": [2364.691], "images_per_second": 0.423, "prompt_tokens": 27, "response_tokens_est": 62, "n_tiles": 6, "output_text": "The main object in the foreground is a young man sitting on the toilet. He is wearing a dark-colored shirt and jeans, with his legs crossed. The toilet is positioned to the right of him. In the background, there is a wall with a checkered pattern and a sink visible to the left.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25559.8, "ram_available_mb": 100212.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 25559.0, "ram_available_mb": 100213.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.48, "peak": 14.71, "min": 13.82}, "VDD_GPU": {"avg": 28.63, "peak": 38.21, "min": 22.86}, "VIN": {"avg": 66.95, "peak": 97.28, "min": 61.64}, "VDD_CPU_SOC_MSS": {"avg": 15.07, "peak": 15.36, "min": 14.57}}, "power_watts_avg": 28.63, "energy_joules_est": 67.71, "sample_count": 18, "duration_seconds": 2.365}, "timestamp": "2026-01-16T15:15:13.165854"}
{"image_index": 35, "image_name": "000000003156.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000003156.jpg", "image_width": 443, "image_height": 640, "image_resolution": "443x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2978.563, "latencies_ms": [2978.563], "images_per_second": 0.336, "prompt_tokens": 21, "response_tokens_est": 84, "n_tiles": 6, "output_text": "The image depicts a young man sitting on the toilet in a bathroom. He is wearing a dark-colored shirt and jeans, and his legs are crossed. The setting is a bathroom with a black and white checkered tile wall, a white toilet, and a black and white checkered towel hanging on the wall. The man appears to be in a contemplative or relaxed state, possibly taking a moment to himself.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25559.0, "ram_available_mb": 100213.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 25559.0, "ram_available_mb": 100213.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.57, "peak": 14.81, "min": 13.82}, "VDD_GPU": {"avg": 27.25, "peak": 37.82, "min": 22.86}, "VIN": {"avg": 65.75, "peak": 116.24, "min": 52.53}, "VDD_CPU_SOC_MSS": {"avg": 15.17, "peak": 15.36, "min": 14.57}}, "power_watts_avg": 27.25, "energy_joules_est": 81.18, "sample_count": 23, "duration_seconds": 2.979}, "timestamp": "2026-01-16T15:15:16.150497"}
{"image_index": 35, "image_name": "000000003156.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000003156.jpg", "image_width": 443, "image_height": 640, "image_resolution": "443x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1967.091, "latencies_ms": [1967.091], "images_per_second": 0.508, "prompt_tokens": 19, "response_tokens_est": 48, "n_tiles": 6, "output_text": "The image is a black and white photograph featuring a young man sitting on the toilet. The man is wearing a dark-colored shirt, jeans, and sneakers. The lighting is soft and even, creating a monochromatic effect.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25559.0, "ram_available_mb": 100213.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 25559.2, "ram_available_mb": 100212.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.45, "peak": 14.71, "min": 13.82}, "VDD_GPU": {"avg": 29.44, "peak": 37.42, "min": 23.25}, "VIN": {"avg": 72.1, "peak": 114.17, "min": 55.76}, "VDD_CPU_SOC_MSS": {"avg": 15.17, "peak": 15.36, "min": 14.57}}, "power_watts_avg": 29.44, "energy_joules_est": 57.92, "sample_count": 15, "duration_seconds": 1.967}, "timestamp": "2026-01-16T15:15:18.123399"}
{"image_index": 36, "image_name": "000000003255.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000003255.jpg", "image_width": 640, "image_height": 363, "image_resolution": "640x363", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 827.12, "latencies_ms": [827.12], "images_per_second": 1.209, "prompt_tokens": 9, "response_tokens_est": 29, "n_tiles": 2, "output_text": "A group of people is standing on a snowy mountain slope, with tracks in the snow leading up to them, indicating recent human activity.", "error": null, "sys_before": {"cpu_percent": 24.0, "ram_used_mb": 25559.0, "ram_available_mb": 100213.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25559.0, "ram_available_mb": 100213.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5055.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.23, "peak": 14.81, "min": 13.79}, "VDD_GPU": {"avg": 26.79, "peak": 32.3, "min": 23.24}, "VIN": {"avg": 64.78, "peak": 75.57, "min": 61.66}, "VDD_CPU_SOC_MSS": {"avg": 15.16, "peak": 15.76, "min": 14.57}}, "power_watts_avg": 26.79, "energy_joules_est": 22.18, "sample_count": 6, "duration_seconds": 0.828}, "timestamp": "2026-01-16T15:15:18.975810"}
{"image_index": 36, "image_name": "000000003255.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000003255.jpg", "image_width": 640, "image_height": 363, "image_resolution": "640x363", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 3094.343, "latencies_ms": [3094.343], "images_per_second": 0.323, "prompt_tokens": 23, "response_tokens_est": 129, "n_tiles": 2, "output_text": "- Snow\n- Mountain\n- People\n- Tracks\n- Hills\n- Slopes\n- Snow\n- Path\n- Ridge\n- Ridge\n- Slope\n- Hill\n- Mountain\n- Snow\n- Path\n- Ridge\n- Ridge\n- Slope\n- Hill\n- Mountain\n- Snow\n- Path\n- Ridge\n- Ridge\n- Slope\n- Hill\n- Mountain\n- Snow\n- Path\n- Ridge\n- Ridge\n- Slope\n- Hill\n- Mountain\n- Snow\n- Path\n- Ridge\n- Ridge\n- Slope\n- Hill\n-", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25559.0, "ram_available_mb": 100213.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25557.4, "ram_available_mb": 100214.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5064.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.17, "peak": 15.72, "min": 14.3}, "VDD_GPU": {"avg": 22.41, "peak": 32.3, "min": 20.48}, "VIN": {"avg": 63.88, "peak": 101.45, "min": 51.67}, "VDD_CPU_SOC_MSS": {"avg": 15.83, "peak": 16.15, "min": 14.96}}, "power_watts_avg": 22.41, "energy_joules_est": 69.35, "sample_count": 24, "duration_seconds": 3.095}, "timestamp": "2026-01-16T15:15:22.075728"}
{"image_index": 36, "image_name": "000000003255.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000003255.jpg", "image_width": 640, "image_height": 363, "image_resolution": "640x363", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1698.599, "latencies_ms": [1698.599], "images_per_second": 0.589, "prompt_tokens": 27, "response_tokens_est": 67, "n_tiles": 2, "output_text": "The main objects in the image are a group of people standing on a snowy slope, with a prominent mountain range in the background. The snowy slope is in the foreground, while the mountain range is in the background. The tracks in the snow indicate that the people are near the mountain, likely on a trail or path.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25557.4, "ram_available_mb": 100214.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25557.4, "ram_available_mb": 100214.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5067.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.97, "peak": 15.22, "min": 14.61}, "VDD_GPU": {"avg": 23.68, "peak": 30.73, "min": 20.5}, "VIN": {"avg": 64.23, "peak": 101.64, "min": 53.47}, "VDD_CPU_SOC_MSS": {"avg": 15.63, "peak": 16.15, "min": 14.96}}, "power_watts_avg": 23.68, "energy_joules_est": 40.23, "sample_count": 13, "duration_seconds": 1.699}, "timestamp": "2026-01-16T15:15:23.780111"}
{"image_index": 36, "image_name": "000000003255.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000003255.jpg", "image_width": 640, "image_height": 363, "image_resolution": "640x363", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1077.197, "latencies_ms": [1077.197], "images_per_second": 0.928, "prompt_tokens": 21, "response_tokens_est": 40, "n_tiles": 2, "output_text": "The image depicts a snowy mountainous landscape under a clear blue sky. A group of people is gathered on the snowy slopes, with tracks visible in the snow, suggesting recent activity.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25557.4, "ram_available_mb": 100214.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 25557.9, "ram_available_mb": 100214.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5063.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.76, "peak": 15.01, "min": 14.51}, "VDD_GPU": {"avg": 25.22, "peak": 31.13, "min": 21.68}, "VIN": {"avg": 67.59, "peak": 102.13, "min": 60.74}, "VDD_CPU_SOC_MSS": {"avg": 15.8, "peak": 16.14, "min": 15.35}}, "power_watts_avg": 25.22, "energy_joules_est": 27.18, "sample_count": 8, "duration_seconds": 1.078}, "timestamp": "2026-01-16T15:15:24.863180"}
{"image_index": 36, "image_name": "000000003255.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000003255.jpg", "image_width": 640, "image_height": 363, "image_resolution": "640x363", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1396.436, "latencies_ms": [1396.436], "images_per_second": 0.716, "prompt_tokens": 19, "response_tokens_est": 54, "n_tiles": 2, "output_text": "The image showcases a snowy mountain landscape under a clear blue sky. The snow-covered slopes are illuminated by the sunlight, creating a stark contrast between the white snow and the blue sky. The terrain is marked by footprints, indicating recent human activity.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25557.9, "ram_available_mb": 100214.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25557.6, "ram_available_mb": 100214.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5062.6, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.87, "peak": 15.42, "min": 14.2}, "VDD_GPU": {"avg": 24.71, "peak": 31.93, "min": 21.28}, "VIN": {"avg": 67.86, "peak": 100.01, "min": 60.32}, "VDD_CPU_SOC_MSS": {"avg": 15.55, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 24.71, "energy_joules_est": 34.51, "sample_count": 10, "duration_seconds": 1.397}, "timestamp": "2026-01-16T15:15:26.265194"}
{"image_index": 37, "image_name": "000000003501.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000003501.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 834.032, "latencies_ms": [834.032], "images_per_second": 1.199, "prompt_tokens": 9, "response_tokens_est": 35, "n_tiles": 1, "output_text": "The image shows a bowl of food consisting of white rice, a portion of broccoli, and a portion of red-orange sauce, all arranged on a white plate.", "error": null, "sys_before": {"cpu_percent": 34.6, "ram_used_mb": 25557.6, "ram_available_mb": 100214.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25557.6, "ram_available_mb": 100214.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 4845.7, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.13, "peak": 15.42, "min": 14.71}, "VDD_GPU": {"avg": 22.13, "peak": 24.43, "min": 20.49}, "VIN": {"avg": 64.02, "peak": 71.8, "min": 54.68}, "VDD_CPU_SOC_MSS": {"avg": 16.08, "peak": 16.54, "min": 15.74}}, "power_watts_avg": 22.13, "energy_joules_est": 18.47, "sample_count": 6, "duration_seconds": 0.834}, "timestamp": "2026-01-16T15:15:27.119801"}
{"image_index": 37, "image_name": "000000003501.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000003501.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2848.14, "latencies_ms": [2848.14], "images_per_second": 0.351, "prompt_tokens": 23, "response_tokens_est": 128, "n_tiles": 1, "output_text": "object: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject:", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25557.6, "ram_available_mb": 100214.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25557.6, "ram_available_mb": 100214.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 4856.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.48, "peak": 15.72, "min": 14.91}, "VDD_GPU": {"avg": 20.49, "peak": 24.43, "min": 19.7}, "VIN": {"avg": 62.17, "peak": 65.73, "min": 55.37}, "VDD_CPU_SOC_MSS": {"avg": 16.43, "peak": 16.54, "min": 15.75}}, "power_watts_avg": 20.49, "energy_joules_est": 58.37, "sample_count": 22, "duration_seconds": 2.849}, "timestamp": "2026-01-16T15:15:29.973613"}
{"image_index": 37, "image_name": "000000003501.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000003501.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1421.485, "latencies_ms": [1421.485], "images_per_second": 0.703, "prompt_tokens": 27, "response_tokens_est": 62, "n_tiles": 1, "output_text": "The main objects in the image are a bowl of food and a bowl of broccoli. The bowl of food is placed in the foreground, while the bowl of broccoli is positioned in the background. The food is placed on a wooden surface, and the bowl of broccoli is slightly out of focus.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25557.6, "ram_available_mb": 100214.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25559.4, "ram_available_mb": 100212.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 4859.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.36, "peak": 15.52, "min": 15.01}, "VDD_GPU": {"avg": 20.99, "peak": 24.03, "min": 19.7}, "VIN": {"avg": 64.41, "peak": 71.36, "min": 59.93}, "VDD_CPU_SOC_MSS": {"avg": 16.4, "peak": 16.54, "min": 16.14}}, "power_watts_avg": 20.99, "energy_joules_est": 29.84, "sample_count": 11, "duration_seconds": 1.422}, "timestamp": "2026-01-16T15:15:31.400276"}
{"image_index": 37, "image_name": "000000003501.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000003501.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1283.32, "latencies_ms": [1283.32], "images_per_second": 0.779, "prompt_tokens": 21, "response_tokens_est": 56, "n_tiles": 1, "output_text": "The image shows a simple yet appetizing meal consisting of a bowl of white rice, a portion of broccoli, and a serving of red-orange stew. The meal is presented on a white plate, and the setting appears to be a dining table with a wooden surface.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25559.4, "ram_available_mb": 100212.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25559.6, "ram_available_mb": 100212.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 4854.7, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.44, "peak": 15.62, "min": 15.01}, "VDD_GPU": {"avg": 21.04, "peak": 23.64, "min": 19.7}, "VIN": {"avg": 62.98, "peak": 70.33, "min": 54.67}, "VDD_CPU_SOC_MSS": {"avg": 16.38, "peak": 16.54, "min": 16.14}}, "power_watts_avg": 21.04, "energy_joules_est": 27.01, "sample_count": 10, "duration_seconds": 1.284}, "timestamp": "2026-01-16T15:15:32.689308"}
{"image_index": 37, "image_name": "000000003501.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000003501.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1209.377, "latencies_ms": [1209.377], "images_per_second": 0.827, "prompt_tokens": 19, "response_tokens_est": 53, "n_tiles": 1, "output_text": "The image features a white bowl containing a meal consisting of broccoli, white rice, and a red-orange sauce. The bowl is placed on a wooden surface, and the lighting is soft and natural, casting gentle shadows and highlighting the textures of the food.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25559.6, "ram_available_mb": 100212.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25559.6, "ram_available_mb": 100212.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 4853.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.43, "peak": 15.52, "min": 15.11}, "VDD_GPU": {"avg": 21.37, "peak": 24.04, "min": 20.1}, "VIN": {"avg": 63.36, "peak": 67.94, "min": 60.09}, "VDD_CPU_SOC_MSS": {"avg": 16.41, "peak": 16.54, "min": 16.14}}, "power_watts_avg": 21.37, "energy_joules_est": 25.85, "sample_count": 9, "duration_seconds": 1.21}, "timestamp": "2026-01-16T15:15:33.904099"}
{"image_index": 38, "image_name": "000000003553.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000003553.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1846.68, "latencies_ms": [1846.68], "images_per_second": 0.542, "prompt_tokens": 9, "response_tokens_est": 48, "n_tiles": 6, "output_text": "The image shows a person's lower legs and feet, wearing black shoes with white soles, standing on a skateboard with a checkered pattern on the deck, positioned on a wooden surface with grass and a bench in the background.", "error": null, "sys_before": {"cpu_percent": 45.9, "ram_used_mb": 25559.6, "ram_available_mb": 100212.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 25559.6, "ram_available_mb": 100212.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.6, "peak": 15.11, "min": 13.82}, "VDD_GPU": {"avg": 29.21, "peak": 36.24, "min": 24.04}, "VIN": {"avg": 77.12, "peak": 122.38, "min": 59.58}, "VDD_CPU_SOC_MSS": {"avg": 15.38, "peak": 15.75, "min": 14.57}}, "power_watts_avg": 29.21, "energy_joules_est": 53.95, "sample_count": 14, "duration_seconds": 1.847}, "timestamp": "2026-01-16T15:15:35.799801"}
{"image_index": 38, "image_name": "000000003553.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000003553.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 4048.579, "latencies_ms": [4048.579], "images_per_second": 0.247, "prompt_tokens": 23, "response_tokens_est": 128, "n_tiles": 6, "output_text": "skateboard: 1\nskateboard wheels: 1\nskateboard deck: 1\nskateboard handle: 1\nskateboard footwear: 2\nskateboard strap: 1\nskateboard strap: 1\nskateboard strap: 1\nskateboard strap: 1\nskateboard strap: 1\nskateboard strap: 1\nskateboard strap: 1\nskateboard strap: 1\nskateboard strap: 1\nskateboard strap: 1\nskateboard strap: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25559.6, "ram_available_mb": 100212.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 25559.6, "ram_available_mb": 100212.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.92, "peak": 15.22, "min": 14.12}, "VDD_GPU": {"avg": 26.52, "peak": 37.82, "min": 23.25}, "VIN": {"avg": 68.01, "peak": 120.28, "min": 54.79}, "VDD_CPU_SOC_MSS": {"avg": 15.71, "peak": 16.15, "min": 14.56}}, "power_watts_avg": 26.52, "energy_joules_est": 107.38, "sample_count": 32, "duration_seconds": 4.049}, "timestamp": "2026-01-16T15:15:39.854231"}
{"image_index": 38, "image_name": "000000003553.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000003553.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2294.579, "latencies_ms": [2294.579], "images_per_second": 0.436, "prompt_tokens": 27, "response_tokens_est": 62, "n_tiles": 6, "output_text": "The main object in the foreground is a skateboard with a checkered pattern on the deck. The skateboard is positioned on a wooden surface, likely a bench or a ledge. In the background, there is a wooden plank and a grassy area, indicating that the scene is set outdoors.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25559.6, "ram_available_mb": 100212.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 25559.9, "ram_available_mb": 100212.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.69, "peak": 15.01, "min": 14.02}, "VDD_GPU": {"avg": 29.11, "peak": 37.42, "min": 23.64}, "VIN": {"avg": 69.5, "peak": 125.83, "min": 53.08}, "VDD_CPU_SOC_MSS": {"avg": 15.43, "peak": 15.75, "min": 14.57}}, "power_watts_avg": 29.11, "energy_joules_est": 66.8, "sample_count": 17, "duration_seconds": 2.295}, "timestamp": "2026-01-16T15:15:42.155064"}
{"image_index": 38, "image_name": "000000003553.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000003553.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2051.999, "latencies_ms": [2051.999], "images_per_second": 0.487, "prompt_tokens": 21, "response_tokens_est": 53, "n_tiles": 6, "output_text": "The image depicts a skateboard resting on a wooden surface, likely a deck or a bench, with a grassy background. The skateboard has a checkered pattern on its wheels, and the person wearing the skateboard is not visible in the frame.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25559.9, "ram_available_mb": 100212.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 25559.9, "ram_available_mb": 100212.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.66, "peak": 15.01, "min": 14.02}, "VDD_GPU": {"avg": 29.4, "peak": 37.42, "min": 23.64}, "VIN": {"avg": 64.97, "peak": 78.03, "min": 55.59}, "VDD_CPU_SOC_MSS": {"avg": 15.31, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 29.4, "energy_joules_est": 60.34, "sample_count": 16, "duration_seconds": 2.052}, "timestamp": "2026-01-16T15:15:44.212854"}
{"image_index": 38, "image_name": "000000003553.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000003553.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1918.993, "latencies_ms": [1918.993], "images_per_second": 0.521, "prompt_tokens": 19, "response_tokens_est": 48, "n_tiles": 6, "output_text": "The notable visual attributes of the image include a skateboard with a checkered pattern on the front, resting on a wooden surface. The lighting is bright and natural, suggesting it is daytime. The weather appears to be clear and sunny.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25559.9, "ram_available_mb": 100212.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 25559.6, "ram_available_mb": 100212.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.64, "peak": 14.91, "min": 14.02}, "VDD_GPU": {"avg": 30.25, "peak": 37.82, "min": 24.04}, "VIN": {"avg": 77.57, "peak": 119.76, "min": 55.36}, "VDD_CPU_SOC_MSS": {"avg": 15.21, "peak": 15.75, "min": 14.57}}, "power_watts_avg": 30.25, "energy_joules_est": 58.06, "sample_count": 14, "duration_seconds": 1.919}, "timestamp": "2026-01-16T15:15:46.141675"}
{"image_index": 39, "image_name": "000000003661.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000003661.jpg", "image_width": 640, "image_height": 384, "image_resolution": "640x384", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1639.71, "latencies_ms": [1639.71], "images_per_second": 0.61, "prompt_tokens": 9, "response_tokens_est": 39, "n_tiles": 6, "output_text": "The image shows a bunch of bananas with one partially cut and lying on its side, resting on a wooden surface, with a blurred background featuring a computer monitor and a blue cup.", "error": null, "sys_before": {"cpu_percent": 43.4, "ram_used_mb": 25559.6, "ram_available_mb": 100212.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25559.4, "ram_available_mb": 100212.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.26, "peak": 14.81, "min": 13.61}, "VDD_GPU": {"avg": 30.44, "peak": 36.63, "min": 24.43}, "VIN": {"avg": 68.45, "peak": 110.4, "min": 56.47}, "VDD_CPU_SOC_MSS": {"avg": 14.99, "peak": 15.36, "min": 14.18}}, "power_watts_avg": 30.44, "energy_joules_est": 49.93, "sample_count": 12, "duration_seconds": 1.64}, "timestamp": "2026-01-16T15:15:47.839794"}
{"image_index": 39, "image_name": "000000003661.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000003661.jpg", "image_width": 640, "image_height": 384, "image_resolution": "640x384", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1708.762, "latencies_ms": [1708.762], "images_per_second": 0.585, "prompt_tokens": 23, "response_tokens_est": 40, "n_tiles": 6, "output_text": "1. Banana\n2. Banana\n3. Banana\n4. Banana\n5. Banana\n6. Banana\n7. Banana\n8. Banana", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25559.4, "ram_available_mb": 100212.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 25559.4, "ram_available_mb": 100212.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.58, "peak": 14.91, "min": 14.02}, "VDD_GPU": {"avg": 30.85, "peak": 37.82, "min": 24.43}, "VIN": {"avg": 64.77, "peak": 73.06, "min": 55.39}, "VDD_CPU_SOC_MSS": {"avg": 15.3, "peak": 15.75, "min": 14.57}}, "power_watts_avg": 30.85, "energy_joules_est": 52.73, "sample_count": 13, "duration_seconds": 1.709}, "timestamp": "2026-01-16T15:15:49.554717"}
{"image_index": 39, "image_name": "000000003661.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000003661.jpg", "image_width": 640, "image_height": 384, "image_resolution": "640x384", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2597.552, "latencies_ms": [2597.552], "images_per_second": 0.385, "prompt_tokens": 27, "response_tokens_est": 73, "n_tiles": 6, "output_text": "The main object in the foreground is a bunch of bananas, which are positioned on a wooden surface. The bananas are slightly out of focus, indicating that they are not the main subject of the image. In the background, there is a blurred object that appears to be a computer monitor, suggesting that the setting might be an office or a workspace.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25559.4, "ram_available_mb": 100212.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 25558.6, "ram_available_mb": 100213.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.73, "peak": 15.01, "min": 14.02}, "VDD_GPU": {"avg": 28.29, "peak": 37.82, "min": 23.25}, "VIN": {"avg": 67.77, "peak": 109.79, "min": 47.58}, "VDD_CPU_SOC_MSS": {"avg": 15.4, "peak": 15.75, "min": 14.57}}, "power_watts_avg": 28.29, "energy_joules_est": 73.5, "sample_count": 20, "duration_seconds": 2.598}, "timestamp": "2026-01-16T15:15:52.158334"}
{"image_index": 39, "image_name": "000000003661.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000003661.jpg", "image_width": 640, "image_height": 384, "image_resolution": "640x384", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2536.53, "latencies_ms": [2536.53], "images_per_second": 0.394, "prompt_tokens": 21, "response_tokens_est": 71, "n_tiles": 6, "output_text": "The image shows a bunch of bananas placed on a wooden surface, possibly a table or countertop. The bananas are in focus, with one partially cut, revealing the inside of the fruit. In the background, there is a blurred image of a computer monitor and a blue cup, suggesting the setting is a home office or a kitchen.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25558.6, "ram_available_mb": 100213.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 25558.3, "ram_available_mb": 100213.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.75, "peak": 15.01, "min": 14.02}, "VDD_GPU": {"avg": 28.47, "peak": 37.82, "min": 23.25}, "VIN": {"avg": 68.53, "peak": 113.44, "min": 55.92}, "VDD_CPU_SOC_MSS": {"avg": 15.4, "peak": 15.75, "min": 14.57}}, "power_watts_avg": 28.47, "energy_joules_est": 72.23, "sample_count": 19, "duration_seconds": 2.537}, "timestamp": "2026-01-16T15:15:54.701201"}
{"image_index": 39, "image_name": "000000003661.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000003661.jpg", "image_width": 640, "image_height": 384, "image_resolution": "640x384", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2374.095, "latencies_ms": [2374.095], "images_per_second": 0.421, "prompt_tokens": 19, "response_tokens_est": 65, "n_tiles": 6, "output_text": "The image features a bunch of ripe bananas with a yellowish-green hue, indicating ripeness. The lighting is soft and natural, casting gentle shadows on the wooden surface. The bananas are placed on a table, and there is a blurred background with a blue object and a cup, suggesting an indoor setting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25558.3, "ram_available_mb": 100213.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 25558.3, "ram_available_mb": 100213.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.71, "peak": 15.01, "min": 14.12}, "VDD_GPU": {"avg": 28.78, "peak": 37.82, "min": 23.64}, "VIN": {"avg": 69.76, "peak": 120.39, "min": 50.28}, "VDD_CPU_SOC_MSS": {"avg": 15.42, "peak": 15.75, "min": 14.57}}, "power_watts_avg": 28.78, "energy_joules_est": 68.34, "sample_count": 18, "duration_seconds": 2.375}, "timestamp": "2026-01-16T15:15:57.082248"}
{"image_index": 40, "image_name": "000000003845.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000003845.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2618.339, "latencies_ms": [2618.339], "images_per_second": 0.382, "prompt_tokens": 9, "response_tokens_est": 36, "n_tiles": 12, "output_text": "The image shows a plate of food consisting of stir-fried chicken, broccoli, carrots, and rice, all served on a colorful plate with a red and white pattern.", "error": null, "sys_before": {"cpu_percent": 46.1, "ram_used_mb": 25558.3, "ram_available_mb": 100213.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 25558.3, "ram_available_mb": 100213.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.95, "peak": 15.95, "min": 13.79}, "VDD_GPU": {"avg": 34.06, "peak": 40.57, "min": 26.8}, "VIN": {"avg": 70.4, "peak": 104.44, "min": 55.29}, "VDD_CPU_SOC_MSS": {"avg": 15.75, "peak": 16.93, "min": 14.96}}, "power_watts_avg": 34.06, "energy_joules_est": 89.19, "sample_count": 20, "duration_seconds": 2.619}, "timestamp": "2026-01-16T15:15:59.784999"}
{"image_index": 40, "image_name": "000000003845.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000003845.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2716.751, "latencies_ms": [2716.751], "images_per_second": 0.368, "prompt_tokens": 23, "response_tokens_est": 35, "n_tiles": 12, "output_text": "chicken: 2\nrice: 1\ncarrot: 1\nbroccoli: 1\npepper: 1\ngarlic: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25558.3, "ram_available_mb": 100213.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 25560.9, "ram_available_mb": 100211.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.58, "peak": 15.14, "min": 13.92}, "VDD_GPU": {"avg": 35.19, "peak": 42.94, "min": 26.39}, "VIN": {"avg": 75.38, "peak": 107.13, "min": 58.19}, "VDD_CPU_SOC_MSS": {"avg": 15.17, "peak": 15.75, "min": 14.57}}, "power_watts_avg": 35.19, "energy_joules_est": 95.61, "sample_count": 21, "duration_seconds": 2.717}, "timestamp": "2026-01-16T15:16:02.510319"}
{"image_index": 40, "image_name": "000000003845.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000003845.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3927.254, "latencies_ms": [3927.254], "images_per_second": 0.255, "prompt_tokens": 27, "response_tokens_est": 70, "n_tiles": 12, "output_text": "The main objects in the image are a plate of food and a glass of water. The plate of food is placed in the foreground, with the glass of water positioned slightly behind it. The food is the central focus of the image, while the glass of water is positioned in the background, providing a sense of depth and context to the scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25560.9, "ram_available_mb": 100211.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25560.9, "ram_available_mb": 100211.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.53, "peak": 15.14, "min": 13.92}, "VDD_GPU": {"avg": 32.05, "peak": 42.54, "min": 25.6}, "VIN": {"avg": 71.15, "peak": 117.57, "min": 57.32}, "VDD_CPU_SOC_MSS": {"avg": 15.1, "peak": 16.14, "min": 14.56}}, "power_watts_avg": 32.05, "energy_joules_est": 125.88, "sample_count": 31, "duration_seconds": 3.928}, "timestamp": "2026-01-16T15:16:06.443787"}
{"image_index": 40, "image_name": "000000003845.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000003845.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3610.863, "latencies_ms": [3610.863], "images_per_second": 0.277, "prompt_tokens": 21, "response_tokens_est": 61, "n_tiles": 12, "output_text": "The image depicts a plate of food on a wooden table, likely in a dining setting. The plate contains a variety of dishes, including rice, broccoli, carrots, and chicken, all garnished with a sauce. The setting suggests a casual meal, possibly at home or in a restaurant.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25560.9, "ram_available_mb": 100211.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25560.9, "ram_available_mb": 100211.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.55, "peak": 15.14, "min": 14.0}, "VDD_GPU": {"avg": 32.64, "peak": 42.54, "min": 25.61}, "VIN": {"avg": 78.93, "peak": 124.96, "min": 57.76}, "VDD_CPU_SOC_MSS": {"avg": 15.1, "peak": 15.75, "min": 14.57}}, "power_watts_avg": 32.64, "energy_joules_est": 117.87, "sample_count": 28, "duration_seconds": 3.611}, "timestamp": "2026-01-16T15:16:10.061359"}
{"image_index": 40, "image_name": "000000003845.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000003845.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2888.082, "latencies_ms": [2888.082], "images_per_second": 0.346, "prompt_tokens": 19, "response_tokens_est": 40, "n_tiles": 12, "output_text": "The plate of food is colorful, featuring a mix of vibrant orange, green, and brown hues. The lighting is warm, casting a soft glow on the food and creating a cozy atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25560.9, "ram_available_mb": 100211.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 25560.8, "ram_available_mb": 100211.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.57, "peak": 15.14, "min": 14.0}, "VDD_GPU": {"avg": 34.31, "peak": 42.54, "min": 26.01}, "VIN": {"avg": 74.22, "peak": 124.26, "min": 57.72}, "VDD_CPU_SOC_MSS": {"avg": 15.15, "peak": 15.75, "min": 14.57}}, "power_watts_avg": 34.31, "energy_joules_est": 99.1, "sample_count": 23, "duration_seconds": 2.888}, "timestamp": "2026-01-16T15:16:12.955786"}
{"image_index": 41, "image_name": "000000003934.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000003934.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2248.714, "latencies_ms": [2248.714], "images_per_second": 0.445, "prompt_tokens": 9, "response_tokens_est": 25, "n_tiles": 12, "output_text": "A young girl is playing a video game on a couch in a cozy living room, surrounded by family members and friends.", "error": null, "sys_before": {"cpu_percent": 45.3, "ram_used_mb": 25560.8, "ram_available_mb": 100211.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 2.6, "ram_used_mb": 25560.8, "ram_available_mb": 100211.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.96, "peak": 15.95, "min": 13.59}, "VDD_GPU": {"avg": 35.69, "peak": 41.76, "min": 28.37}, "VIN": {"avg": 74.73, "peak": 119.83, "min": 57.99}, "VDD_CPU_SOC_MSS": {"avg": 15.75, "peak": 16.93, "min": 14.57}}, "power_watts_avg": 35.69, "energy_joules_est": 80.28, "sample_count": 17, "duration_seconds": 2.249}, "timestamp": "2026-01-16T15:16:15.283243"}
{"image_index": 41, "image_name": "000000003934.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000003934.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 5867.162, "latencies_ms": [5867.162], "images_per_second": 0.17, "prompt_tokens": 23, "response_tokens_est": 129, "n_tiles": 12, "output_text": "- girl: 1\n- woman: 2\n- man: 1\n- woman: 2\n- woman: 2\n- woman: 2\n- woman: 2\n- woman: 2\n- woman: 2\n- woman: 2\n- woman: 2\n- woman: 2\n- woman: 2\n- woman: 2\n- woman: 2\n- woman: 2\n- woman: 2\n- woman: 2\n- woman: 2\n- woman: 2\n- woman: 2\n- woman", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25560.8, "ram_available_mb": 100211.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 25560.8, "ram_available_mb": 100211.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.94, "peak": 15.54, "min": 14.32}, "VDD_GPU": {"avg": 30.23, "peak": 42.94, "min": 26.0}, "VIN": {"avg": 71.95, "peak": 131.65, "min": 55.68}, "VDD_CPU_SOC_MSS": {"avg": 15.42, "peak": 16.14, "min": 14.96}}, "power_watts_avg": 30.23, "energy_joules_est": 177.37, "sample_count": 46, "duration_seconds": 5.867}, "timestamp": "2026-01-16T15:16:21.156666"}
{"image_index": 41, "image_name": "000000003934.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000003934.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3694.421, "latencies_ms": [3694.421], "images_per_second": 0.271, "prompt_tokens": 27, "response_tokens_est": 64, "n_tiles": 12, "output_text": "The main objects in the image are a young girl, a man, and a woman. The girl is in the foreground, standing on a grey rug, while the man and woman are in the background, near a couch. The man is standing near the couch, and the woman is standing near the staircase.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25560.8, "ram_available_mb": 100211.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25561.3, "ram_available_mb": 100210.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.93, "peak": 15.44, "min": 14.43}, "VDD_GPU": {"avg": 32.52, "peak": 42.15, "min": 26.0}, "VIN": {"avg": 73.81, "peak": 108.58, "min": 57.78}, "VDD_CPU_SOC_MSS": {"avg": 15.46, "peak": 16.14, "min": 14.96}}, "power_watts_avg": 32.52, "energy_joules_est": 120.15, "sample_count": 29, "duration_seconds": 3.695}, "timestamp": "2026-01-16T15:16:24.857112"}
{"image_index": 41, "image_name": "000000003934.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000003934.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3927.051, "latencies_ms": [3927.051], "images_per_second": 0.255, "prompt_tokens": 21, "response_tokens_est": 71, "n_tiles": 12, "output_text": "The scene is set in a cozy living room with a gray sofa, a red patterned cushion, and a gray rug. A group of people, including a young girl in a colorful dress, are gathered around a television, with some standing and others sitting. The atmosphere suggests a casual, relaxed gathering, possibly a family event or a casual party.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25561.3, "ram_available_mb": 100210.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25561.3, "ram_available_mb": 100210.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.94, "peak": 15.54, "min": 14.32}, "VDD_GPU": {"avg": 32.16, "peak": 42.15, "min": 26.01}, "VIN": {"avg": 71.9, "peak": 114.1, "min": 55.1}, "VDD_CPU_SOC_MSS": {"avg": 15.47, "peak": 16.14, "min": 14.96}}, "power_watts_avg": 32.16, "energy_joules_est": 126.31, "sample_count": 30, "duration_seconds": 3.928}, "timestamp": "2026-01-16T15:16:28.790911"}
{"image_index": 41, "image_name": "000000003934.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000003934.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2984.32, "latencies_ms": [2984.32], "images_per_second": 0.335, "prompt_tokens": 19, "response_tokens_est": 43, "n_tiles": 12, "output_text": "The room is well-lit with warm, ambient lighting, creating a cozy atmosphere. The wooden floor and light-colored walls provide a neutral backdrop, while the gray sofa and red pillows add pops of color.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25561.3, "ram_available_mb": 100210.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 25561.3, "ram_available_mb": 100210.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.91, "peak": 15.44, "min": 14.32}, "VDD_GPU": {"avg": 34.1, "peak": 41.76, "min": 26.4}, "VIN": {"avg": 75.01, "peak": 119.45, "min": 56.01}, "VDD_CPU_SOC_MSS": {"avg": 15.51, "peak": 16.14, "min": 14.96}}, "power_watts_avg": 34.1, "energy_joules_est": 101.78, "sample_count": 23, "duration_seconds": 2.985}, "timestamp": "2026-01-16T15:16:31.781601"}
{"image_index": 42, "image_name": "000000004134.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000004134.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1664.971, "latencies_ms": [1664.971], "images_per_second": 0.601, "prompt_tokens": 9, "response_tokens_est": 40, "n_tiles": 6, "output_text": "A man in a dark suit and striped tie is shaking hands with another man in a light-colored shirt and tie, while a group of people are gathered around tables in a formal event setting.", "error": null, "sys_before": {"cpu_percent": 41.4, "ram_used_mb": 25561.3, "ram_available_mb": 100210.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 25561.6, "ram_available_mb": 100210.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.39, "peak": 14.91, "min": 13.72}, "VDD_GPU": {"avg": 30.61, "peak": 37.82, "min": 24.43}, "VIN": {"avg": 70.9, "peak": 120.33, "min": 49.8}, "VDD_CPU_SOC_MSS": {"avg": 15.08, "peak": 15.75, "min": 14.57}}, "power_watts_avg": 30.61, "energy_joules_est": 50.98, "sample_count": 13, "duration_seconds": 1.665}, "timestamp": "2026-01-16T15:16:33.504878"}
{"image_index": 42, "image_name": "000000004134.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000004134.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2350.481, "latencies_ms": [2350.481], "images_per_second": 0.425, "prompt_tokens": 23, "response_tokens_est": 64, "n_tiles": 6, "output_text": "1. Man in suit and tie\n2. Man in suit and tie\n3. Man in suit and tie\n4. Man in suit and tie\n5. Man in suit and tie\n6. Man in suit and tie\n7. Man in suit and tie\n8. Man in suit and tie", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25561.6, "ram_available_mb": 100210.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 25561.9, "ram_available_mb": 100210.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.78, "peak": 15.11, "min": 14.12}, "VDD_GPU": {"avg": 28.92, "peak": 37.82, "min": 23.64}, "VIN": {"avg": 70.9, "peak": 104.49, "min": 60.43}, "VDD_CPU_SOC_MSS": {"avg": 15.42, "peak": 15.76, "min": 14.56}}, "power_watts_avg": 28.92, "energy_joules_est": 67.99, "sample_count": 18, "duration_seconds": 2.351}, "timestamp": "2026-01-16T15:16:35.861207"}
{"image_index": 42, "image_name": "000000004134.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000004134.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2083.264, "latencies_ms": [2083.264], "images_per_second": 0.48, "prompt_tokens": 27, "response_tokens_est": 54, "n_tiles": 6, "output_text": "The main object in the foreground is a man in a dark suit and yellow tie, shaking hands with another man in a light blue shirt and dark jacket. The background features a crowded room with tables set for a formal event, where other attendees are engaged in conversations.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25561.9, "ram_available_mb": 100210.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 25561.6, "ram_available_mb": 100210.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.75, "peak": 15.01, "min": 14.12}, "VDD_GPU": {"avg": 29.6, "peak": 37.82, "min": 23.64}, "VIN": {"avg": 70.73, "peak": 115.38, "min": 50.68}, "VDD_CPU_SOC_MSS": {"avg": 15.38, "peak": 15.75, "min": 14.57}}, "power_watts_avg": 29.6, "energy_joules_est": 61.67, "sample_count": 16, "duration_seconds": 2.084}, "timestamp": "2026-01-16T15:16:37.950547"}
{"image_index": 42, "image_name": "000000004134.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000004134.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2971.122, "latencies_ms": [2971.122], "images_per_second": 0.337, "prompt_tokens": 21, "response_tokens_est": 87, "n_tiles": 6, "output_text": "The image depicts a formal event, likely a conference or banquet, taking place in a spacious, well-lit room with elegant decor. Several attendees are engaged in conversations, while others are seated at tables covered with white tablecloths, some with plates and glasses. The central focus is on two men shaking hands, one in a dark suit and the other in a striped suit, indicating a formal interaction or greeting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25561.6, "ram_available_mb": 100210.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 25561.6, "ram_available_mb": 100210.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.86, "peak": 15.11, "min": 14.12}, "VDD_GPU": {"avg": 27.75, "peak": 37.82, "min": 23.25}, "VIN": {"avg": 71.52, "peak": 118.7, "min": 56.86}, "VDD_CPU_SOC_MSS": {"avg": 15.44, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 27.75, "energy_joules_est": 82.47, "sample_count": 23, "duration_seconds": 2.972}, "timestamp": "2026-01-16T15:16:40.928044"}
{"image_index": 42, "image_name": "000000004134.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000004134.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1868.066, "latencies_ms": [1868.066], "images_per_second": 0.535, "prompt_tokens": 19, "response_tokens_est": 46, "n_tiles": 6, "output_text": "The image features a man in a dark suit and striped tie, holding a white folder. The background shows a well-lit indoor event with tables covered in white tablecloths, and other attendees in formal attire.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25561.6, "ram_available_mb": 100210.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 25562.2, "ram_available_mb": 100210.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.69, "peak": 15.01, "min": 14.12}, "VDD_GPU": {"avg": 30.08, "peak": 37.82, "min": 24.04}, "VIN": {"avg": 71.02, "peak": 122.77, "min": 59.09}, "VDD_CPU_SOC_MSS": {"avg": 15.3, "peak": 15.75, "min": 14.57}}, "power_watts_avg": 30.08, "energy_joules_est": 56.2, "sample_count": 14, "duration_seconds": 1.868}, "timestamp": "2026-01-16T15:16:42.802163"}
{"image_index": 43, "image_name": "000000004395.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000004395.jpg", "image_width": 425, "image_height": 640, "image_resolution": "425x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1258.318, "latencies_ms": [1258.318], "images_per_second": 0.795, "prompt_tokens": 9, "response_tokens_est": 25, "n_tiles": 6, "output_text": "The image shows a man wearing a white shirt and a striped tie, standing indoors with a slightly blurred background.", "error": null, "sys_before": {"cpu_percent": 41.8, "ram_used_mb": 25562.2, "ram_available_mb": 100210.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25562.0, "ram_available_mb": 100210.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.29, "peak": 14.71, "min": 13.82}, "VDD_GPU": {"avg": 32.48, "peak": 37.03, "min": 26.8}, "VIN": {"avg": 78.77, "peak": 109.95, "min": 64.98}, "VDD_CPU_SOC_MSS": {"avg": 15.09, "peak": 15.36, "min": 14.57}}, "power_watts_avg": 32.48, "energy_joules_est": 40.88, "sample_count": 9, "duration_seconds": 1.259}, "timestamp": "2026-01-16T15:16:44.104503"}
{"image_index": 43, "image_name": "000000004395.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000004395.jpg", "image_width": 425, "image_height": 640, "image_resolution": "425x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1516.367, "latencies_ms": [1516.367], "images_per_second": 0.659, "prompt_tokens": 23, "response_tokens_est": 32, "n_tiles": 6, "output_text": "1. Man\n2. Shirt\n3. Tie\n4. Background\n5. Lighting\n6. Person\n7. Face\n8. Hand", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25562.0, "ram_available_mb": 100210.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25562.2, "ram_available_mb": 100209.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.43, "peak": 14.71, "min": 13.92}, "VDD_GPU": {"avg": 32.45, "peak": 39.0, "min": 25.22}, "VIN": {"avg": 71.36, "peak": 109.06, "min": 56.35}, "VDD_CPU_SOC_MSS": {"avg": 15.1, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 32.45, "energy_joules_est": 49.22, "sample_count": 11, "duration_seconds": 1.517}, "timestamp": "2026-01-16T15:16:45.626956"}
{"image_index": 43, "image_name": "000000004395.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000004395.jpg", "image_width": 425, "image_height": 640, "image_resolution": "425x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2336.942, "latencies_ms": [2336.942], "images_per_second": 0.428, "prompt_tokens": 27, "response_tokens_est": 61, "n_tiles": 6, "output_text": "The main object in the foreground is a person wearing a white shirt and a striped tie. The person's face is slightly turned to the right, and their hand is holding a cigarette. The background is blurred, but it appears to be an indoor setting with indistinct shapes and lights.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25562.2, "ram_available_mb": 100209.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 25562.0, "ram_available_mb": 100210.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.56, "peak": 14.81, "min": 13.92}, "VDD_GPU": {"avg": 28.83, "peak": 38.21, "min": 22.86}, "VIN": {"avg": 67.72, "peak": 121.2, "min": 55.25}, "VDD_CPU_SOC_MSS": {"avg": 15.12, "peak": 15.36, "min": 14.57}}, "power_watts_avg": 28.83, "energy_joules_est": 67.39, "sample_count": 18, "duration_seconds": 2.337}, "timestamp": "2026-01-16T15:16:47.970766"}
{"image_index": 43, "image_name": "000000004395.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000004395.jpg", "image_width": 425, "image_height": 640, "image_resolution": "425x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2589.097, "latencies_ms": [2589.097], "images_per_second": 0.386, "prompt_tokens": 21, "response_tokens_est": 70, "n_tiles": 6, "output_text": "The image depicts a man in a professional setting, likely an office or a similar environment. He is dressed in a white shirt and a striped tie, and appears to be in a moment of contemplation or deep thought. The background is dimly lit, with indistinct shapes and colors, suggesting an indoor setting with artificial lighting.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 25562.0, "ram_available_mb": 100210.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 25562.2, "ram_available_mb": 100209.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.6, "peak": 14.91, "min": 13.92}, "VDD_GPU": {"avg": 27.89, "peak": 37.42, "min": 22.86}, "VIN": {"avg": 64.37, "peak": 77.3, "min": 55.09}, "VDD_CPU_SOC_MSS": {"avg": 15.12, "peak": 15.36, "min": 14.57}}, "power_watts_avg": 27.89, "energy_joules_est": 72.22, "sample_count": 20, "duration_seconds": 2.589}, "timestamp": "2026-01-16T15:16:50.566082"}
{"image_index": 43, "image_name": "000000004395.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000004395.jpg", "image_width": 425, "image_height": 640, "image_resolution": "425x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1659.423, "latencies_ms": [1659.423], "images_per_second": 0.603, "prompt_tokens": 19, "response_tokens_est": 37, "n_tiles": 6, "output_text": "The notable visual attributes of the image include a person wearing a white shirt and a striped tie, with a dark background. The lighting is dim, creating a moody atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25562.2, "ram_available_mb": 100209.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25562.0, "ram_available_mb": 100210.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.46, "peak": 14.81, "min": 13.92}, "VDD_GPU": {"avg": 31.13, "peak": 37.82, "min": 24.43}, "VIN": {"avg": 72.24, "peak": 113.86, "min": 56.3}, "VDD_CPU_SOC_MSS": {"avg": 15.09, "peak": 15.36, "min": 14.57}}, "power_watts_avg": 31.13, "energy_joules_est": 51.68, "sample_count": 12, "duration_seconds": 1.66}, "timestamp": "2026-01-16T15:16:52.235653"}
{"image_index": 44, "image_name": "000000004495.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000004495.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2381.719, "latencies_ms": [2381.719], "images_per_second": 0.42, "prompt_tokens": 9, "response_tokens_est": 29, "n_tiles": 12, "output_text": "The image shows a cozy living room with a television on a stand, a plaid-patterned couch, and a small wooden chair.", "error": null, "sys_before": {"cpu_percent": 41.9, "ram_used_mb": 25562.0, "ram_available_mb": 100210.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 25559.3, "ram_available_mb": 100212.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.99, "peak": 15.95, "min": 13.79}, "VDD_GPU": {"avg": 34.97, "peak": 40.97, "min": 27.57}, "VIN": {"avg": 78.37, "peak": 117.94, "min": 54.13}, "VDD_CPU_SOC_MSS": {"avg": 15.79, "peak": 16.92, "min": 14.96}}, "power_watts_avg": 34.97, "energy_joules_est": 83.31, "sample_count": 18, "duration_seconds": 2.382}, "timestamp": "2026-01-16T15:16:54.691345"}
{"image_index": 44, "image_name": "000000004495.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000004495.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2646.929, "latencies_ms": [2646.929], "images_per_second": 0.378, "prompt_tokens": 23, "response_tokens_est": 33, "n_tiles": 12, "output_text": "chair: 1\ntv stand: 1\ntv: 1\nwallpaper: 1\ncouch: 2\nfloor: 1", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25559.3, "ram_available_mb": 100212.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 25559.5, "ram_available_mb": 100212.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.94, "peak": 15.54, "min": 14.43}, "VDD_GPU": {"avg": 35.34, "peak": 42.94, "min": 27.18}, "VIN": {"avg": 74.27, "peak": 99.3, "min": 58.24}, "VDD_CPU_SOC_MSS": {"avg": 15.52, "peak": 16.14, "min": 14.96}}, "power_watts_avg": 35.34, "energy_joules_est": 93.55, "sample_count": 21, "duration_seconds": 2.647}, "timestamp": "2026-01-16T15:16:57.344466"}
{"image_index": 44, "image_name": "000000004495.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000004495.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3784.397, "latencies_ms": [3784.397], "images_per_second": 0.264, "prompt_tokens": 27, "response_tokens_est": 66, "n_tiles": 12, "output_text": "The main objects in the image are a television, a couch, and a chair. The television is positioned in the background, slightly to the right, while the couch and chair are in the foreground, closer to the viewer. The couch is placed near the television, and the chair is positioned to the left of the couch.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25559.5, "ram_available_mb": 100212.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25560.0, "ram_available_mb": 100212.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.87, "peak": 15.44, "min": 14.43}, "VDD_GPU": {"avg": 32.6, "peak": 42.94, "min": 25.61}, "VIN": {"avg": 70.91, "peak": 117.17, "min": 49.03}, "VDD_CPU_SOC_MSS": {"avg": 15.38, "peak": 16.14, "min": 14.96}}, "power_watts_avg": 32.6, "energy_joules_est": 123.38, "sample_count": 29, "duration_seconds": 3.785}, "timestamp": "2026-01-16T15:17:01.134860"}
{"image_index": 44, "image_name": "000000004495.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000004495.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3368.014, "latencies_ms": [3368.014], "images_per_second": 0.297, "prompt_tokens": 21, "response_tokens_est": 54, "n_tiles": 12, "output_text": "The image depicts a cozy living room with a television set on a wooden stand against a yellow wall. The room is furnished with a plaid-patterned couch and a red chair, and there is a whiteboard on the wall with some writing on it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25560.0, "ram_available_mb": 100212.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25560.2, "ram_available_mb": 100211.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.64, "peak": 15.24, "min": 14.02}, "VDD_GPU": {"avg": 33.03, "peak": 42.15, "min": 25.61}, "VIN": {"avg": 72.35, "peak": 115.5, "min": 52.14}, "VDD_CPU_SOC_MSS": {"avg": 15.19, "peak": 16.14, "min": 14.56}}, "power_watts_avg": 33.03, "energy_joules_est": 111.27, "sample_count": 26, "duration_seconds": 3.369}, "timestamp": "2026-01-16T15:17:04.509735"}
{"image_index": 44, "image_name": "000000004495.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000004495.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3130.734, "latencies_ms": [3130.734], "images_per_second": 0.319, "prompt_tokens": 19, "response_tokens_est": 47, "n_tiles": 12, "output_text": "The room is dimly lit, with a warm and cozy ambiance. The walls are painted in a light yellow color, and the furniture includes a plaid-patterned armchair and a blue and red checkered sofa.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25560.2, "ram_available_mb": 100211.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 25560.0, "ram_available_mb": 100212.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.62, "peak": 15.24, "min": 14.02}, "VDD_GPU": {"avg": 33.6, "peak": 41.76, "min": 26.01}, "VIN": {"avg": 75.01, "peak": 122.51, "min": 58.02}, "VDD_CPU_SOC_MSS": {"avg": 15.19, "peak": 16.14, "min": 14.57}}, "power_watts_avg": 33.6, "energy_joules_est": 105.2, "sample_count": 24, "duration_seconds": 3.131}, "timestamp": "2026-01-16T15:17:07.646697"}
{"image_index": 45, "image_name": "000000004765.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000004765.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 543.115, "latencies_ms": [543.115], "images_per_second": 1.841, "prompt_tokens": 9, "response_tokens_est": 21, "n_tiles": 1, "output_text": "A surfer is riding a wave on a surfboard, wearing a yellow shirt and red shorts.", "error": null, "sys_before": {"cpu_percent": 22.6, "ram_used_mb": 25560.0, "ram_available_mb": 100212.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 25560.0, "ram_available_mb": 100212.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 4845.7, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.63, "peak": 15.11, "min": 14.1}, "VDD_GPU": {"avg": 24.72, "peak": 27.18, "min": 22.85}, "VIN": {"avg": 63.02, "peak": 67.96, "min": 56.08}, "VDD_CPU_SOC_MSS": {"avg": 15.16, "peak": 15.75, "min": 14.57}}, "power_watts_avg": 24.72, "energy_joules_est": 13.44, "sample_count": 4, "duration_seconds": 0.544}, "timestamp": "2026-01-16T15:17:08.216529"}
{"image_index": 45, "image_name": "000000004765.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000004765.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 934.186, "latencies_ms": [934.186], "images_per_second": 1.07, "prompt_tokens": 23, "response_tokens_est": 40, "n_tiles": 1, "output_text": "object: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25560.0, "ram_available_mb": 100212.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25560.0, "ram_available_mb": 100212.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 4856.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.3, "peak": 15.52, "min": 14.81}, "VDD_GPU": {"avg": 22.74, "peak": 26.01, "min": 20.89}, "VIN": {"avg": 64.05, "peak": 65.84, "min": 62.16}, "VDD_CPU_SOC_MSS": {"avg": 15.97, "peak": 16.54, "min": 15.36}}, "power_watts_avg": 22.74, "energy_joules_est": 21.25, "sample_count": 7, "duration_seconds": 0.935}, "timestamp": "2026-01-16T15:17:09.156234"}
{"image_index": 45, "image_name": "000000004765.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000004765.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1388.302, "latencies_ms": [1388.302], "images_per_second": 0.72, "prompt_tokens": 27, "response_tokens_est": 61, "n_tiles": 1, "output_text": "The main object, the surfer, is positioned in the foreground, riding a wave. The wave is in the background, creating a dynamic and energetic scene. The surfboard is near the surfer, with the surfer's feet firmly on the board, indicating their active engagement in the sport.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25560.0, "ram_available_mb": 100212.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25560.0, "ram_available_mb": 100212.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 4859.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.53, "peak": 15.62, "min": 15.22}, "VDD_GPU": {"avg": 21.48, "peak": 24.43, "min": 20.09}, "VIN": {"avg": 62.73, "peak": 68.97, "min": 57.41}, "VDD_CPU_SOC_MSS": {"avg": 16.38, "peak": 16.54, "min": 15.74}}, "power_watts_avg": 21.48, "energy_joules_est": 29.83, "sample_count": 10, "duration_seconds": 1.389}, "timestamp": "2026-01-16T15:17:10.549809"}
{"image_index": 45, "image_name": "000000004765.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000004765.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1713.57, "latencies_ms": [1713.57], "images_per_second": 0.584, "prompt_tokens": 21, "response_tokens_est": 76, "n_tiles": 1, "output_text": "The scene captures a surfer riding a wave in the ocean. The surfer, dressed in a yellow shirt and red shorts, is skillfully maneuvering a surfboard amidst the frothy white waves. The setting is a dynamic and energetic environment typical of a surfing session, with the surfer's focused posture and the swirling water indicating the intensity of the ride.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25560.0, "ram_available_mb": 100212.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25560.0, "ram_available_mb": 100212.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 4854.7, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.57, "peak": 15.72, "min": 15.22}, "VDD_GPU": {"avg": 20.97, "peak": 24.43, "min": 19.7}, "VIN": {"avg": 62.24, "peak": 66.19, "min": 56.17}, "VDD_CPU_SOC_MSS": {"avg": 16.42, "peak": 16.54, "min": 16.15}}, "power_watts_avg": 20.97, "energy_joules_est": 35.94, "sample_count": 13, "duration_seconds": 1.714}, "timestamp": "2026-01-16T15:17:12.269000"}
{"image_index": 45, "image_name": "000000004765.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000004765.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 759.253, "latencies_ms": [759.253], "images_per_second": 1.317, "prompt_tokens": 19, "response_tokens_est": 32, "n_tiles": 1, "output_text": "The surfer is wearing a yellow shirt and red shorts, which stand out against the blue-green water. The lighting is bright, indicating it is daytime.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25560.0, "ram_available_mb": 100212.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25560.0, "ram_available_mb": 100212.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 4853.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.46, "peak": 15.62, "min": 15.32}, "VDD_GPU": {"avg": 22.3, "peak": 24.03, "min": 20.89}, "VIN": {"avg": 66.11, "peak": 72.85, "min": 63.24}, "VDD_CPU_SOC_MSS": {"avg": 16.3, "peak": 16.54, "min": 16.14}}, "power_watts_avg": 22.3, "energy_joules_est": 16.94, "sample_count": 5, "duration_seconds": 0.76}, "timestamp": "2026-01-16T15:17:13.033643"}
{"image_index": 46, "image_name": "000000004795.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000004795.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2214.092, "latencies_ms": [2214.092], "images_per_second": 0.452, "prompt_tokens": 9, "response_tokens_est": 24, "n_tiles": 12, "output_text": "The image shows a cat sitting on a desk with a laptop, a phone, and a keyboard in the background.", "error": null, "sys_before": {"cpu_percent": 45.6, "ram_used_mb": 25560.0, "ram_available_mb": 100212.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 2.6, "ram_used_mb": 25562.2, "ram_available_mb": 100210.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.19, "peak": 16.05, "min": 14.4}, "VDD_GPU": {"avg": 34.97, "peak": 40.57, "min": 27.59}, "VIN": {"avg": 77.58, "peak": 117.59, "min": 57.82}, "VDD_CPU_SOC_MSS": {"avg": 16.0, "peak": 16.92, "min": 15.35}}, "power_watts_avg": 34.97, "energy_joules_est": 77.45, "sample_count": 17, "duration_seconds": 2.215}, "timestamp": "2026-01-16T15:17:15.310394"}
{"image_index": 46, "image_name": "000000004795.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000004795.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 3246.413, "latencies_ms": [3246.413], "images_per_second": 0.308, "prompt_tokens": 23, "response_tokens_est": 51, "n_tiles": 12, "output_text": "- Laptop: 1\n- Keyboard: 1\n- Phone: 1\n- Computer mouse: 1\n- Computer monitor: 1\n- Computer: 1\n- Mouse: 1\n- Mouse pad: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25562.2, "ram_available_mb": 100210.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 25562.2, "ram_available_mb": 100210.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.97, "peak": 15.54, "min": 14.43}, "VDD_GPU": {"avg": 33.77, "peak": 42.54, "min": 26.39}, "VIN": {"avg": 76.04, "peak": 142.93, "min": 58.1}, "VDD_CPU_SOC_MSS": {"avg": 15.51, "peak": 16.14, "min": 14.96}}, "power_watts_avg": 33.77, "energy_joules_est": 109.65, "sample_count": 25, "duration_seconds": 3.247}, "timestamp": "2026-01-16T15:17:18.563903"}
{"image_index": 46, "image_name": "000000004795.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000004795.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3860.652, "latencies_ms": [3860.652], "images_per_second": 0.259, "prompt_tokens": 27, "response_tokens_est": 69, "n_tiles": 12, "output_text": "The main objects in the image are a laptop, a cat, and a phone. The laptop is positioned in the foreground, with the cat resting on top of it. The phone is placed to the right of the laptop, and the cat is near the phone. The background is out of focus, emphasizing the laptop and the cat.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 25562.2, "ram_available_mb": 100210.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25562.0, "ram_available_mb": 100210.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.0, "peak": 15.54, "min": 14.43}, "VDD_GPU": {"avg": 32.31, "peak": 42.54, "min": 26.0}, "VIN": {"avg": 79.08, "peak": 131.8, "min": 58.0}, "VDD_CPU_SOC_MSS": {"avg": 15.54, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 32.31, "energy_joules_est": 124.75, "sample_count": 30, "duration_seconds": 3.861}, "timestamp": "2026-01-16T15:17:22.430693"}
{"image_index": 46, "image_name": "000000004795.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000004795.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3154.681, "latencies_ms": [3154.681], "images_per_second": 0.317, "prompt_tokens": 21, "response_tokens_est": 48, "n_tiles": 12, "output_text": "The image depicts a cluttered office workspace with a laptop, a phone, and a cat. The cat is resting on the laptop, and the office environment includes various items such as a keyboard, a mouse, and a phone.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25562.0, "ram_available_mb": 100210.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 25562.5, "ram_available_mb": 100209.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.98, "peak": 15.65, "min": 14.43}, "VDD_GPU": {"avg": 33.78, "peak": 42.54, "min": 26.0}, "VIN": {"avg": 78.74, "peak": 110.0, "min": 57.93}, "VDD_CPU_SOC_MSS": {"avg": 15.57, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 33.78, "energy_joules_est": 106.59, "sample_count": 24, "duration_seconds": 3.156}, "timestamp": "2026-01-16T15:17:25.592164"}
{"image_index": 46, "image_name": "000000004795.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000004795.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3175.939, "latencies_ms": [3175.939], "images_per_second": 0.315, "prompt_tokens": 19, "response_tokens_est": 49, "n_tiles": 12, "output_text": "The image features a laptop with a white keyboard and a black mouse. The laptop is placed on a wooden desk, and the background includes a computer monitor displaying a webpage. The lighting is bright, and the overall setting appears to be indoors.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25562.5, "ram_available_mb": 100209.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 25562.7, "ram_available_mb": 100209.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.0, "peak": 15.54, "min": 14.43}, "VDD_GPU": {"avg": 33.78, "peak": 42.54, "min": 26.0}, "VIN": {"avg": 72.98, "peak": 111.81, "min": 56.08}, "VDD_CPU_SOC_MSS": {"avg": 15.6, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 33.78, "energy_joules_est": 107.3, "sample_count": 24, "duration_seconds": 3.177}, "timestamp": "2026-01-16T15:17:28.775065"}
{"image_index": 47, "image_name": "000000005001.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005001.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2393.801, "latencies_ms": [2393.801], "images_per_second": 0.418, "prompt_tokens": 9, "response_tokens_est": 29, "n_tiles": 12, "output_text": "A group of people is gathered outside a building, with a man holding a balloon and a young girl holding a ribbon, while others watch.", "error": null, "sys_before": {"cpu_percent": 21.6, "ram_used_mb": 25562.4, "ram_available_mb": 100209.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 25562.4, "ram_available_mb": 100209.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.09, "peak": 16.05, "min": 14.02}, "VDD_GPU": {"avg": 35.23, "peak": 41.76, "min": 27.57}, "VIN": {"avg": 75.78, "peak": 113.29, "min": 57.84}, "VDD_CPU_SOC_MSS": {"avg": 15.75, "peak": 16.92, "min": 14.57}}, "power_watts_avg": 35.23, "energy_joules_est": 84.35, "sample_count": 18, "duration_seconds": 2.394}, "timestamp": "2026-01-16T15:17:31.225886"}
{"image_index": 47, "image_name": "000000005001.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005001.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2872.239, "latencies_ms": [2872.239], "images_per_second": 0.348, "prompt_tokens": 23, "response_tokens_est": 40, "n_tiles": 12, "output_text": "object: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25562.4, "ram_available_mb": 100209.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 25566.4, "ram_available_mb": 100205.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.0, "peak": 15.54, "min": 14.53}, "VDD_GPU": {"avg": 34.9, "peak": 42.94, "min": 26.8}, "VIN": {"avg": 78.6, "peak": 113.01, "min": 58.01}, "VDD_CPU_SOC_MSS": {"avg": 15.57, "peak": 16.14, "min": 14.96}}, "power_watts_avg": 34.9, "energy_joules_est": 100.25, "sample_count": 22, "duration_seconds": 2.873}, "timestamp": "2026-01-16T15:17:34.108314"}
{"image_index": 47, "image_name": "000000005001.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005001.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3643.971, "latencies_ms": [3643.971], "images_per_second": 0.274, "prompt_tokens": 27, "response_tokens_est": 63, "n_tiles": 12, "output_text": "The main objects in the image are a group of people gathered around a table, with a child holding a ribbon and a man cutting it. The child is positioned near the table, while the man is in the foreground. The ribbon is held by the child, and the man is cutting it near the table.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25566.4, "ram_available_mb": 100205.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25566.7, "ram_available_mb": 100205.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.02, "peak": 15.65, "min": 14.43}, "VDD_GPU": {"avg": 32.78, "peak": 42.15, "min": 26.01}, "VIN": {"avg": 75.21, "peak": 115.49, "min": 57.27}, "VDD_CPU_SOC_MSS": {"avg": 15.52, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 32.78, "energy_joules_est": 119.46, "sample_count": 28, "duration_seconds": 3.644}, "timestamp": "2026-01-16T15:17:37.758598"}
{"image_index": 47, "image_name": "000000005001.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005001.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 4384.696, "latencies_ms": [4384.696], "images_per_second": 0.228, "prompt_tokens": 21, "response_tokens_est": 85, "n_tiles": 12, "output_text": "The scene depicts a group of people gathered outside a building, likely at a ribbon-cutting ceremony. The individuals are dressed in casual attire, with some wearing hats and jackets. The setting appears to be outdoors, with a concrete ground and a building in the background. The focus is on a man cutting a red ribbon with scissors, while a child holds a ribbon, and a woman is taking a photo.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25566.7, "ram_available_mb": 100205.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 25566.7, "ram_available_mb": 100205.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.04, "peak": 15.54, "min": 14.43}, "VDD_GPU": {"avg": 31.65, "peak": 42.15, "min": 26.01}, "VIN": {"avg": 76.33, "peak": 142.69, "min": 57.97}, "VDD_CPU_SOC_MSS": {"avg": 15.53, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 31.65, "energy_joules_est": 138.79, "sample_count": 33, "duration_seconds": 4.385}, "timestamp": "2026-01-16T15:17:42.149467"}
{"image_index": 47, "image_name": "000000005001.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005001.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3171.619, "latencies_ms": [3171.619], "images_per_second": 0.315, "prompt_tokens": 19, "response_tokens_est": 49, "n_tiles": 12, "output_text": "The image depicts a group of people gathered around a table, with a blue balloon and a red ribbon being held by a child. The scene is well-lit, with natural daylight illuminating the group and the objects on the table.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25566.7, "ram_available_mb": 100205.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25566.7, "ram_available_mb": 100205.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.04, "peak": 15.54, "min": 14.43}, "VDD_GPU": {"avg": 33.83, "peak": 41.76, "min": 26.0}, "VIN": {"avg": 72.0, "peak": 121.05, "min": 58.21}, "VDD_CPU_SOC_MSS": {"avg": 15.62, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 33.83, "energy_joules_est": 107.31, "sample_count": 24, "duration_seconds": 3.172}, "timestamp": "2026-01-16T15:17:45.331413"}
{"image_index": 48, "image_name": "000000005037.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005037.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2141.965, "latencies_ms": [2141.965], "images_per_second": 0.467, "prompt_tokens": 9, "response_tokens_est": 58, "n_tiles": 6, "output_text": "The image shows a modern city bus with the destination \"First Group\" displayed on its digital sign, indicating it is heading towards the First Group office. The bus is parked on a street with a pedestrian crossing in front of it, and there are buildings and other vehicles in the background.", "error": null, "sys_before": {"cpu_percent": 18.0, "ram_used_mb": 25566.7, "ram_available_mb": 100205.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 25566.7, "ram_available_mb": 100205.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.61, "peak": 15.11, "min": 13.82}, "VDD_GPU": {"avg": 29.28, "peak": 37.42, "min": 23.64}, "VIN": {"avg": 66.8, "peak": 89.83, "min": 52.06}, "VDD_CPU_SOC_MSS": {"avg": 15.28, "peak": 15.75, "min": 14.57}}, "power_watts_avg": 29.28, "energy_joules_est": 62.74, "sample_count": 16, "duration_seconds": 2.143}, "timestamp": "2026-01-16T15:17:47.515196"}
{"image_index": 48, "image_name": "000000005037.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005037.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1699.345, "latencies_ms": [1699.345], "images_per_second": 0.588, "prompt_tokens": 23, "response_tokens_est": 40, "n_tiles": 6, "output_text": "object: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25566.7, "ram_available_mb": 100205.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 25566.7, "ram_available_mb": 100205.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.75, "peak": 15.11, "min": 14.22}, "VDD_GPU": {"avg": 30.94, "peak": 37.82, "min": 24.43}, "VIN": {"avg": 64.81, "peak": 71.54, "min": 55.02}, "VDD_CPU_SOC_MSS": {"avg": 15.32, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 30.94, "energy_joules_est": 52.59, "sample_count": 13, "duration_seconds": 1.7}, "timestamp": "2026-01-16T15:17:49.224412"}
{"image_index": 48, "image_name": "000000005037.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005037.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2895.624, "latencies_ms": [2895.624], "images_per_second": 0.345, "prompt_tokens": 27, "response_tokens_est": 81, "n_tiles": 6, "output_text": "The main object in the foreground is a bus, which is positioned on the right side of the image. The bus is facing the left side of the frame, indicating that it is moving towards the viewer. In the background, there are buildings and a street, which are further away from the bus. The bus is parked on the street, and there are other vehicles and people visible in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25566.7, "ram_available_mb": 100205.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 25567.2, "ram_available_mb": 100205.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.72, "peak": 15.01, "min": 14.02}, "VDD_GPU": {"avg": 27.86, "peak": 38.21, "min": 22.86}, "VIN": {"avg": 67.08, "peak": 102.25, "min": 50.69}, "VDD_CPU_SOC_MSS": {"avg": 15.24, "peak": 15.36, "min": 14.57}}, "power_watts_avg": 27.86, "energy_joules_est": 80.69, "sample_count": 21, "duration_seconds": 2.896}, "timestamp": "2026-01-16T15:17:52.126058"}
{"image_index": 48, "image_name": "000000005037.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005037.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2833.58, "latencies_ms": [2833.58], "images_per_second": 0.353, "prompt_tokens": 21, "response_tokens_est": 79, "n_tiles": 6, "output_text": "The image depicts a modern city bus on a city street, with a clear sky overhead. The bus is adorned with a white and purple color scheme and displays the route number \"1\" and the destination \"First Group\" on its digital sign. The bus is parked on a street with a pedestrian crossing in front of it, and there are buildings and other vehicles in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25567.2, "ram_available_mb": 100205.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 25570.9, "ram_available_mb": 100201.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.7, "peak": 15.01, "min": 14.02}, "VDD_GPU": {"avg": 27.6, "peak": 37.82, "min": 22.86}, "VIN": {"avg": 66.86, "peak": 89.82, "min": 54.96}, "VDD_CPU_SOC_MSS": {"avg": 15.21, "peak": 15.36, "min": 14.57}}, "power_watts_avg": 27.6, "energy_joules_est": 78.22, "sample_count": 21, "duration_seconds": 2.834}, "timestamp": "2026-01-16T15:17:54.969942"}
{"image_index": 48, "image_name": "000000005037.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005037.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1487.51, "latencies_ms": [1487.51], "images_per_second": 0.672, "prompt_tokens": 19, "response_tokens_est": 31, "n_tiles": 6, "output_text": "The bus in the image is predominantly white with purple and pink accents. The lighting is bright, indicating daytime, and the weather appears to be clear.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25570.9, "ram_available_mb": 100201.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25571.2, "ram_available_mb": 100201.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.49, "peak": 14.71, "min": 13.92}, "VDD_GPU": {"avg": 31.56, "peak": 37.42, "min": 25.22}, "VIN": {"avg": 73.29, "peak": 113.72, "min": 47.79}, "VDD_CPU_SOC_MSS": {"avg": 15.07, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 31.56, "energy_joules_est": 46.96, "sample_count": 11, "duration_seconds": 1.488}, "timestamp": "2026-01-16T15:17:56.467497"}
{"image_index": 49, "image_name": "000000005060.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005060.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2088.363, "latencies_ms": [2088.363], "images_per_second": 0.479, "prompt_tokens": 9, "response_tokens_est": 20, "n_tiles": 12, "output_text": "A man is sitting on the floor in a room, taking a selfie with his smartphone.", "error": null, "sys_before": {"cpu_percent": 21.9, "ram_used_mb": 25570.9, "ram_available_mb": 100201.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 2.4, "ram_used_mb": 25571.1, "ram_available_mb": 100201.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.1, "peak": 16.05, "min": 13.9}, "VDD_GPU": {"avg": 36.12, "peak": 41.76, "min": 29.15}, "VIN": {"avg": 77.72, "peak": 106.5, "min": 58.27}, "VDD_CPU_SOC_MSS": {"avg": 15.8, "peak": 16.92, "min": 14.57}}, "power_watts_avg": 36.12, "energy_joules_est": 75.45, "sample_count": 16, "duration_seconds": 2.089}, "timestamp": "2026-01-16T15:17:58.607835"}
{"image_index": 49, "image_name": "000000005060.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005060.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 3355.084, "latencies_ms": [3355.084], "images_per_second": 0.298, "prompt_tokens": 23, "response_tokens_est": 53, "n_tiles": 12, "output_text": "- man: 1\n- green shirt: 1\n- black flip-flops: 2\n- phone: 1\n- mirror: 1\n- wooden floor: 1\n- wall: 1\n- door: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25571.1, "ram_available_mb": 100201.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 25570.9, "ram_available_mb": 100201.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.81, "peak": 15.54, "min": 14.32}, "VDD_GPU": {"avg": 33.52, "peak": 43.33, "min": 25.61}, "VIN": {"avg": 71.58, "peak": 120.67, "min": 56.58}, "VDD_CPU_SOC_MSS": {"avg": 15.34, "peak": 16.14, "min": 14.96}}, "power_watts_avg": 33.52, "energy_joules_est": 112.47, "sample_count": 26, "duration_seconds": 3.355}, "timestamp": "2026-01-16T15:18:01.971314"}
{"image_index": 49, "image_name": "000000005060.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005060.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 4020.394, "latencies_ms": [4020.394], "images_per_second": 0.249, "prompt_tokens": 27, "response_tokens_est": 73, "n_tiles": 12, "output_text": "The main object in the foreground is a man taking a selfie with his smartphone. He is sitting on the floor, with his legs crossed and his arms resting on his knees. The background features a wooden-framed mirror leaning against a wall, reflecting the room's interior. The mirror is positioned near the man, creating a sense of depth in the image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25570.9, "ram_available_mb": 100201.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25572.4, "ram_available_mb": 100199.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.67, "peak": 15.24, "min": 14.1}, "VDD_GPU": {"avg": 31.91, "peak": 42.15, "min": 25.6}, "VIN": {"avg": 72.81, "peak": 118.83, "min": 57.74}, "VDD_CPU_SOC_MSS": {"avg": 15.13, "peak": 16.14, "min": 14.57}}, "power_watts_avg": 31.91, "energy_joules_est": 128.3, "sample_count": 32, "duration_seconds": 4.021}, "timestamp": "2026-01-16T15:18:06.003345"}
{"image_index": 49, "image_name": "000000005060.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005060.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2955.845, "latencies_ms": [2955.845], "images_per_second": 0.338, "prompt_tokens": 21, "response_tokens_est": 42, "n_tiles": 12, "output_text": "The image depicts a man sitting on the floor in a room with wooden flooring and a wooden wall. He is taking a selfie with his smartphone, which is placed on the floor next to him.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25572.4, "ram_available_mb": 100199.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 25572.4, "ram_available_mb": 100199.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.68, "peak": 15.24, "min": 14.02}, "VDD_GPU": {"avg": 34.05, "peak": 42.15, "min": 26.01}, "VIN": {"avg": 73.52, "peak": 113.89, "min": 55.45}, "VDD_CPU_SOC_MSS": {"avg": 15.18, "peak": 16.14, "min": 14.57}}, "power_watts_avg": 34.05, "energy_joules_est": 100.66, "sample_count": 23, "duration_seconds": 2.956}, "timestamp": "2026-01-16T15:18:08.965682"}
{"image_index": 49, "image_name": "000000005060.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005060.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2727.253, "latencies_ms": [2727.253], "images_per_second": 0.367, "prompt_tokens": 19, "response_tokens_est": 36, "n_tiles": 12, "output_text": "The image features a wooden-framed mirror with a warm, natural finish, placed on a wooden floor. The room has a soft, ambient lighting, creating a cozy atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25572.4, "ram_available_mb": 100199.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 25572.4, "ram_available_mb": 100199.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.73, "peak": 15.24, "min": 14.02}, "VDD_GPU": {"avg": 35.08, "peak": 42.54, "min": 26.8}, "VIN": {"avg": 75.95, "peak": 115.73, "min": 58.22}, "VDD_CPU_SOC_MSS": {"avg": 15.33, "peak": 16.14, "min": 14.56}}, "power_watts_avg": 35.08, "energy_joules_est": 95.69, "sample_count": 21, "duration_seconds": 2.728}, "timestamp": "2026-01-16T15:18:11.700214"}
{"image_index": 50, "image_name": "000000005193.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005193.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1412.463, "latencies_ms": [1412.463], "images_per_second": 0.708, "prompt_tokens": 9, "response_tokens_est": 31, "n_tiles": 6, "output_text": "The image shows a group of young men posing with surfboards, with one holding a surfboard and another holding a surfboard with a colorful design.", "error": null, "sys_before": {"cpu_percent": 39.2, "ram_used_mb": 25572.4, "ram_available_mb": 100199.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 25572.7, "ram_available_mb": 100199.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.29, "peak": 14.81, "min": 13.72}, "VDD_GPU": {"avg": 32.34, "peak": 37.82, "min": 26.01}, "VIN": {"avg": 70.64, "peak": 120.43, "min": 50.73}, "VDD_CPU_SOC_MSS": {"avg": 14.88, "peak": 15.36, "min": 14.17}}, "power_watts_avg": 32.34, "energy_joules_est": 45.69, "sample_count": 10, "duration_seconds": 1.413}, "timestamp": "2026-01-16T15:18:13.168271"}
{"image_index": 50, "image_name": "000000005193.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005193.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1657.027, "latencies_ms": [1657.027], "images_per_second": 0.603, "prompt_tokens": 23, "response_tokens_est": 37, "n_tiles": 6, "output_text": "1. Surfboards\n2. People\n3. Water bottles\n4. Air conditioner\n5. Bottle\n6. Bottle\n7. Bottle\n8. Bottle", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25572.7, "ram_available_mb": 100199.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25572.4, "ram_available_mb": 100199.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.52, "peak": 14.81, "min": 13.92}, "VDD_GPU": {"avg": 31.61, "peak": 38.6, "min": 24.42}, "VIN": {"avg": 71.02, "peak": 102.02, "min": 57.93}, "VDD_CPU_SOC_MSS": {"avg": 15.19, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 31.61, "energy_joules_est": 52.39, "sample_count": 12, "duration_seconds": 1.657}, "timestamp": "2026-01-16T15:18:14.831052"}
{"image_index": 50, "image_name": "000000005193.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005193.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2331.277, "latencies_ms": [2331.277], "images_per_second": 0.429, "prompt_tokens": 27, "response_tokens_est": 61, "n_tiles": 6, "output_text": "The main objects in the image are a group of young men holding surfboards. The surfboards are positioned in the foreground, with the men standing in the background. The surfboards are near the men, and the background includes a wall with various items, such as a water bottle and a fan.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25572.4, "ram_available_mb": 100199.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 25572.7, "ram_available_mb": 100199.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.65, "peak": 15.01, "min": 14.02}, "VDD_GPU": {"avg": 28.78, "peak": 37.82, "min": 23.25}, "VIN": {"avg": 67.0, "peak": 96.71, "min": 56.38}, "VDD_CPU_SOC_MSS": {"avg": 15.29, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 28.78, "energy_joules_est": 67.12, "sample_count": 18, "duration_seconds": 2.332}, "timestamp": "2026-01-16T15:18:17.168360"}
{"image_index": 50, "image_name": "000000005193.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005193.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2329.253, "latencies_ms": [2329.253], "images_per_second": 0.429, "prompt_tokens": 21, "response_tokens_est": 61, "n_tiles": 6, "output_text": "The scene depicts a group of young men gathered indoors, likely in a garage or a similar space. They are holding surfboards and appear to be posing for a photo. The setting is casual and informal, with a white wall and various items like a water bottle and a fan in the background.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25572.7, "ram_available_mb": 100199.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 25572.4, "ram_available_mb": 100199.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.69, "peak": 15.01, "min": 14.02}, "VDD_GPU": {"avg": 28.63, "peak": 38.21, "min": 23.25}, "VIN": {"avg": 65.2, "peak": 85.49, "min": 54.68}, "VDD_CPU_SOC_MSS": {"avg": 15.25, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 28.63, "energy_joules_est": 66.69, "sample_count": 18, "duration_seconds": 2.33}, "timestamp": "2026-01-16T15:18:19.503391"}
{"image_index": 50, "image_name": "000000005193.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005193.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2519.0, "latencies_ms": [2519.0], "images_per_second": 0.397, "prompt_tokens": 19, "response_tokens_est": 68, "n_tiles": 6, "output_text": "The image depicts a group of young men holding surfboards, with one man in the foreground holding a surfboard with a vibrant yellow and red design. The lighting is dim, suggesting an indoor setting with artificial lighting. The surfboards appear to be made of wood or a similar material, and the overall atmosphere is casual and relaxed.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25572.4, "ram_available_mb": 100199.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 25572.7, "ram_available_mb": 100199.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.71, "peak": 15.01, "min": 14.02}, "VDD_GPU": {"avg": 28.29, "peak": 38.21, "min": 23.25}, "VIN": {"avg": 70.53, "peak": 115.57, "min": 55.64}, "VDD_CPU_SOC_MSS": {"avg": 15.24, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 28.29, "energy_joules_est": 71.28, "sample_count": 20, "duration_seconds": 2.52}, "timestamp": "2026-01-16T15:18:22.028789"}
{"image_index": 51, "image_name": "000000005477.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005477.jpg", "image_width": 640, "image_height": 349, "image_resolution": "640x349", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 949.163, "latencies_ms": [949.163], "images_per_second": 1.054, "prompt_tokens": 9, "response_tokens_est": 35, "n_tiles": 2, "output_text": "The image shows a large, yellow airplane with the word \"LOT\" written on its side, parked on a tarmac with a cloudy sky in the background.", "error": null, "sys_before": {"cpu_percent": 36.7, "ram_used_mb": 25572.7, "ram_available_mb": 100199.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25572.7, "ram_available_mb": 100199.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5055.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.74, "peak": 15.11, "min": 14.2}, "VDD_GPU": {"avg": 26.17, "peak": 31.91, "min": 22.46}, "VIN": {"avg": 64.42, "peak": 75.43, "min": 61.25}, "VDD_CPU_SOC_MSS": {"avg": 15.24, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 26.17, "energy_joules_est": 24.85, "sample_count": 7, "duration_seconds": 0.95}, "timestamp": "2026-01-16T15:18:23.003133"}
{"image_index": 51, "image_name": "000000005477.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005477.jpg", "image_width": 640, "image_height": 349, "image_resolution": "640x349", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1263.603, "latencies_ms": [1263.603], "images_per_second": 0.791, "prompt_tokens": 23, "response_tokens_est": 48, "n_tiles": 2, "output_text": "- airplane: 1\n- plane: 1\n- aircraft: 1\n- aircraft: 1\n- airplane: 1\n- plane: 1\n- airplane: 1\n- plane: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25572.7, "ram_available_mb": 100199.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25572.7, "ram_available_mb": 100199.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5064.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.89, "peak": 15.22, "min": 14.4}, "VDD_GPU": {"avg": 25.31, "peak": 31.92, "min": 21.68}, "VIN": {"avg": 66.99, "peak": 101.7, "min": 59.44}, "VDD_CPU_SOC_MSS": {"avg": 15.71, "peak": 16.14, "min": 14.96}}, "power_watts_avg": 25.31, "energy_joules_est": 32.0, "sample_count": 9, "duration_seconds": 1.264}, "timestamp": "2026-01-16T15:18:24.273164"}
{"image_index": 51, "image_name": "000000005477.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005477.jpg", "image_width": 640, "image_height": 349, "image_resolution": "640x349", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1851.331, "latencies_ms": [1851.331], "images_per_second": 0.54, "prompt_tokens": 27, "response_tokens_est": 74, "n_tiles": 2, "output_text": "The main object in the image is a yellow airplane with the word \"LOT\" written on its fuselage. The airplane is positioned in the foreground, slightly to the right. In the background, there are other airplanes, including a red and white airplane, which are further away. The sky is partly cloudy, providing a backdrop to the scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25572.7, "ram_available_mb": 100199.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25573.5, "ram_available_mb": 100198.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5067.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.13, "peak": 15.62, "min": 14.4}, "VDD_GPU": {"avg": 23.58, "peak": 30.73, "min": 20.88}, "VIN": {"avg": 65.81, "peak": 99.65, "min": 58.76}, "VDD_CPU_SOC_MSS": {"avg": 15.98, "peak": 16.54, "min": 15.36}}, "power_watts_avg": 23.58, "energy_joules_est": 43.66, "sample_count": 14, "duration_seconds": 1.852}, "timestamp": "2026-01-16T15:18:26.130127"}
{"image_index": 51, "image_name": "000000005477.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005477.jpg", "image_width": 640, "image_height": 349, "image_resolution": "640x349", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2006.114, "latencies_ms": [2006.114], "images_per_second": 0.498, "prompt_tokens": 21, "response_tokens_est": 81, "n_tiles": 2, "output_text": "The image depicts a large, yellow commercial airplane parked on a tarmac, with a cloudy sky in the background. The airplane is adorned with the logo and name \"LOT\" on its fuselage, and the word \"POLSKIE LINE\" is visible on the tail. The scene suggests that the airplane is either preparing for departure or has just arrived at the airport.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25573.5, "ram_available_mb": 100198.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25573.2, "ram_available_mb": 100198.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5063.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.34, "peak": 15.72, "min": 14.61}, "VDD_GPU": {"avg": 23.3, "peak": 31.13, "min": 20.48}, "VIN": {"avg": 64.75, "peak": 98.18, "min": 56.4}, "VDD_CPU_SOC_MSS": {"avg": 15.86, "peak": 16.55, "min": 14.96}}, "power_watts_avg": 23.3, "energy_joules_est": 46.75, "sample_count": 15, "duration_seconds": 2.007}, "timestamp": "2026-01-16T15:18:28.141941"}
{"image_index": 51, "image_name": "000000005477.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005477.jpg", "image_width": 640, "image_height": 349, "image_resolution": "640x349", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1455.032, "latencies_ms": [1455.032], "images_per_second": 0.687, "prompt_tokens": 19, "response_tokens_est": 57, "n_tiles": 2, "output_text": "The notable visual attributes of the airplane in the image include its golden color, which stands out against the cloudy sky. The lighting is bright, casting shadows on the runway, indicating it is daytime. The weather appears to be partly cloudy, with patches of blue sky visible.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25573.2, "ram_available_mb": 100198.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25573.2, "ram_available_mb": 100199.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5062.6, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.2, "peak": 15.42, "min": 14.91}, "VDD_GPU": {"avg": 24.28, "peak": 30.73, "min": 21.27}, "VIN": {"avg": 66.56, "peak": 99.85, "min": 57.46}, "VDD_CPU_SOC_MSS": {"avg": 15.89, "peak": 16.54, "min": 15.36}}, "power_watts_avg": 24.28, "energy_joules_est": 35.34, "sample_count": 11, "duration_seconds": 1.456}, "timestamp": "2026-01-16T15:18:29.603545"}
{"image_index": 52, "image_name": "000000005503.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005503.jpg", "image_width": 333, "image_height": 500, "image_resolution": "333x500", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 991.562, "latencies_ms": [991.562], "images_per_second": 1.009, "prompt_tokens": 9, "response_tokens_est": 15, "n_tiles": 6, "output_text": "A person is holding a toilet seat in a black and white photo.", "error": null, "sys_before": {"cpu_percent": 45.8, "ram_used_mb": 25573.2, "ram_available_mb": 100199.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 25573.2, "ram_available_mb": 100199.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.4, "peak": 14.63, "min": 13.92}, "VDD_GPU": {"avg": 33.09, "peak": 36.63, "min": 29.54}, "VIN": {"avg": 75.97, "peak": 121.65, "min": 53.8}, "VDD_CPU_SOC_MSS": {"avg": 15.19, "peak": 15.75, "min": 14.57}}, "power_watts_avg": 33.09, "energy_joules_est": 32.83, "sample_count": 7, "duration_seconds": 0.992}, "timestamp": "2026-01-16T15:18:30.635248"}
{"image_index": 52, "image_name": "000000005503.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005503.jpg", "image_width": 333, "image_height": 500, "image_resolution": "333x500", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1942.878, "latencies_ms": [1942.878], "images_per_second": 0.515, "prompt_tokens": 23, "response_tokens_est": 49, "n_tiles": 6, "output_text": "1. Toilet\n2. Hand\n3. Toilet brush\n4. Toilet seat\n5. Toilet paper\n6. Toilet paper roll\n7. Toilet paper holder\n8. Toilet paper dispenser", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25573.2, "ram_available_mb": 100199.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 25573.2, "ram_available_mb": 100199.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.73, "peak": 15.11, "min": 14.12}, "VDD_GPU": {"avg": 30.91, "peak": 39.39, "min": 24.04}, "VIN": {"avg": 71.48, "peak": 113.43, "min": 56.44}, "VDD_CPU_SOC_MSS": {"avg": 15.28, "peak": 15.75, "min": 14.57}}, "power_watts_avg": 30.91, "energy_joules_est": 60.06, "sample_count": 15, "duration_seconds": 1.943}, "timestamp": "2026-01-16T15:18:32.583996"}
{"image_index": 52, "image_name": "000000005503.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005503.jpg", "image_width": 333, "image_height": 500, "image_resolution": "333x500", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1915.86, "latencies_ms": [1915.86], "images_per_second": 0.522, "prompt_tokens": 27, "response_tokens_est": 48, "n_tiles": 6, "output_text": "The main object in the foreground is a toilet with a seat and lid. The person's legs are visible, suggesting the person is standing near the toilet. The background is indistinct, but it appears to be an indoor setting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25573.2, "ram_available_mb": 100199.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 25573.2, "ram_available_mb": 100199.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.77, "peak": 15.11, "min": 14.22}, "VDD_GPU": {"avg": 30.56, "peak": 37.82, "min": 24.43}, "VIN": {"avg": 69.72, "peak": 90.44, "min": 59.41}, "VDD_CPU_SOC_MSS": {"avg": 15.35, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 30.56, "energy_joules_est": 58.56, "sample_count": 14, "duration_seconds": 1.916}, "timestamp": "2026-01-16T15:18:34.505519"}
{"image_index": 52, "image_name": "000000005503.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005503.jpg", "image_width": 333, "image_height": 500, "image_resolution": "333x500", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1803.761, "latencies_ms": [1803.761], "images_per_second": 0.554, "prompt_tokens": 21, "response_tokens_est": 44, "n_tiles": 6, "output_text": "The image depicts a black and white scene of a toilet with a person's legs visible, holding the toilet seat. The setting appears to be a bathroom, and the person is likely preparing to use the toilet.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25573.2, "ram_available_mb": 100199.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 25572.4, "ram_available_mb": 100199.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.79, "peak": 15.11, "min": 14.22}, "VDD_GPU": {"avg": 31.0, "peak": 37.82, "min": 24.42}, "VIN": {"avg": 72.12, "peak": 116.88, "min": 60.96}, "VDD_CPU_SOC_MSS": {"avg": 15.32, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 31.0, "energy_joules_est": 55.92, "sample_count": 13, "duration_seconds": 1.804}, "timestamp": "2026-01-16T15:18:36.315682"}
{"image_index": 52, "image_name": "000000005503.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005503.jpg", "image_width": 333, "image_height": 500, "image_resolution": "333x500", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1722.42, "latencies_ms": [1722.42], "images_per_second": 0.581, "prompt_tokens": 19, "response_tokens_est": 41, "n_tiles": 6, "output_text": "The image is in black and white, with a focus on a toilet seat and a person's lower legs. The lighting is dim, and the materials appear to be a combination of metal and fabric.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25572.4, "ram_available_mb": 100199.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 25572.1, "ram_available_mb": 100200.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.75, "peak": 15.11, "min": 14.22}, "VDD_GPU": {"avg": 31.37, "peak": 38.21, "min": 24.83}, "VIN": {"avg": 77.51, "peak": 114.81, "min": 60.49}, "VDD_CPU_SOC_MSS": {"avg": 15.32, "peak": 15.75, "min": 14.57}}, "power_watts_avg": 31.37, "energy_joules_est": 54.05, "sample_count": 13, "duration_seconds": 1.723}, "timestamp": "2026-01-16T15:18:38.043994"}
{"image_index": 53, "image_name": "000000005529.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005529.jpg", "image_width": 444, "image_height": 640, "image_resolution": "444x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1537.679, "latencies_ms": [1537.679], "images_per_second": 0.65, "prompt_tokens": 9, "response_tokens_est": 34, "n_tiles": 6, "output_text": "A skier is navigating through a snowy landscape, wearing a blue jacket and a helmet, with ski poles in hand, as they carve through the snow.", "error": null, "sys_before": {"cpu_percent": 45.6, "ram_used_mb": 25572.1, "ram_available_mb": 100200.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 25572.1, "ram_available_mb": 100200.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.28, "peak": 14.81, "min": 13.72}, "VDD_GPU": {"avg": 30.98, "peak": 37.03, "min": 24.83}, "VIN": {"avg": 67.74, "peak": 107.23, "min": 51.55}, "VDD_CPU_SOC_MSS": {"avg": 14.93, "peak": 15.36, "min": 14.18}}, "power_watts_avg": 30.98, "energy_joules_est": 47.65, "sample_count": 11, "duration_seconds": 1.538}, "timestamp": "2026-01-16T15:18:39.642345"}
{"image_index": 53, "image_name": "000000005529.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005529.jpg", "image_width": 444, "image_height": 640, "image_resolution": "444x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1716.849, "latencies_ms": [1716.849], "images_per_second": 0.582, "prompt_tokens": 23, "response_tokens_est": 39, "n_tiles": 6, "output_text": "1. Person\n2. Ski poles\n3. Ski\n4. Snow\n5. Snowboard\n6. Snowboarder\n7. Snowboard\n8. Snowboarder", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25572.1, "ram_available_mb": 100200.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 25572.1, "ram_available_mb": 100200.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.53, "peak": 14.81, "min": 14.02}, "VDD_GPU": {"avg": 30.88, "peak": 38.21, "min": 24.04}, "VIN": {"avg": 73.52, "peak": 114.97, "min": 59.76}, "VDD_CPU_SOC_MSS": {"avg": 15.08, "peak": 15.36, "min": 14.57}}, "power_watts_avg": 30.88, "energy_joules_est": 53.03, "sample_count": 13, "duration_seconds": 1.717}, "timestamp": "2026-01-16T15:18:41.365832"}
{"image_index": 53, "image_name": "000000005529.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005529.jpg", "image_width": 444, "image_height": 640, "image_resolution": "444x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2725.695, "latencies_ms": [2725.695], "images_per_second": 0.367, "prompt_tokens": 27, "response_tokens_est": 75, "n_tiles": 6, "output_text": "The main objects in the image are two skiers, one on the left and one on the right. The skier on the left is closer to the foreground, while the skier on the right is further back. The skiers are positioned on a snowy slope, with the skier on the right appearing to be in motion, possibly skiing downhill.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25572.1, "ram_available_mb": 100200.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 25572.4, "ram_available_mb": 100199.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.68, "peak": 15.01, "min": 13.92}, "VDD_GPU": {"avg": 27.86, "peak": 37.82, "min": 22.86}, "VIN": {"avg": 67.05, "peak": 113.05, "min": 47.01}, "VDD_CPU_SOC_MSS": {"avg": 15.22, "peak": 15.36, "min": 14.57}}, "power_watts_avg": 27.86, "energy_joules_est": 75.95, "sample_count": 21, "duration_seconds": 2.726}, "timestamp": "2026-01-16T15:18:44.097767"}
{"image_index": 53, "image_name": "000000005529.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005529.jpg", "image_width": 444, "image_height": 640, "image_resolution": "444x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2859.143, "latencies_ms": [2859.143], "images_per_second": 0.35, "prompt_tokens": 21, "response_tokens_est": 80, "n_tiles": 6, "output_text": "The image depicts a snowy mountainous landscape with a person skiing down a snow-covered slope. The individual is wearing a blue jacket, black pants, and a helmet, and is using ski poles to navigate the snowy terrain. The scene is set in a winter environment, with snow-covered trees and a clear sky, creating a picturesque and serene winter sports setting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25572.4, "ram_available_mb": 100199.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 25572.6, "ram_available_mb": 100199.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.74, "peak": 15.01, "min": 14.02}, "VDD_GPU": {"avg": 27.6, "peak": 37.82, "min": 22.86}, "VIN": {"avg": 65.82, "peak": 97.39, "min": 56.54}, "VDD_CPU_SOC_MSS": {"avg": 15.16, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 27.6, "energy_joules_est": 78.92, "sample_count": 22, "duration_seconds": 2.86}, "timestamp": "2026-01-16T15:18:46.963071"}
{"image_index": 53, "image_name": "000000005529.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005529.jpg", "image_width": 444, "image_height": 640, "image_resolution": "444x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2186.667, "latencies_ms": [2186.667], "images_per_second": 0.457, "prompt_tokens": 19, "response_tokens_est": 56, "n_tiles": 6, "output_text": "The image depicts a snowy landscape with a person skiing. The individual is wearing a blue jacket and a white helmet, and is using yellow ski poles. The snow is white and powdery, and the trees are covered in snow, indicating a winter setting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25572.6, "ram_available_mb": 100199.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 25572.4, "ram_available_mb": 100199.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.66, "peak": 14.91, "min": 14.02}, "VDD_GPU": {"avg": 28.92, "peak": 37.82, "min": 23.25}, "VIN": {"avg": 68.81, "peak": 94.62, "min": 56.01}, "VDD_CPU_SOC_MSS": {"avg": 15.19, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 28.92, "energy_joules_est": 63.26, "sample_count": 17, "duration_seconds": 2.187}, "timestamp": "2026-01-16T15:18:49.156281"}
{"image_index": 54, "image_name": "000000005586.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005586.jpg", "image_width": 320, "image_height": 240, "image_resolution": "320x240", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2189.843, "latencies_ms": [2189.843], "images_per_second": 0.457, "prompt_tokens": 9, "response_tokens_est": 23, "n_tiles": 12, "output_text": "A tennis player is preparing to serve on a blue court, with a crowd of spectators in the background.", "error": null, "sys_before": {"cpu_percent": 42.6, "ram_used_mb": 25572.4, "ram_available_mb": 100199.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 2.6, "ram_used_mb": 25572.4, "ram_available_mb": 100199.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.17, "peak": 15.95, "min": 14.32}, "VDD_GPU": {"avg": 35.79, "peak": 40.18, "min": 28.36}, "VIN": {"avg": 79.29, "peak": 111.9, "min": 58.4}, "VDD_CPU_SOC_MSS": {"avg": 15.9, "peak": 16.92, "min": 14.96}}, "power_watts_avg": 35.79, "energy_joules_est": 78.39, "sample_count": 15, "duration_seconds": 2.19}, "timestamp": "2026-01-16T15:18:51.434607"}
{"image_index": 54, "image_name": "000000005586.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005586.jpg", "image_width": 320, "image_height": 240, "image_resolution": "320x240", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2883.748, "latencies_ms": [2883.748], "images_per_second": 0.347, "prompt_tokens": 23, "response_tokens_est": 40, "n_tiles": 12, "output_text": "1. Tennis player\n2. Tennis racket\n3. Tennis court\n4. Spectator\n5. Advertisements\n6. Stadium\n7. Ball\n8. Ball court", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25572.4, "ram_available_mb": 100199.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 25572.4, "ram_available_mb": 100199.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.0, "peak": 15.54, "min": 14.43}, "VDD_GPU": {"avg": 34.88, "peak": 42.94, "min": 26.79}, "VIN": {"avg": 74.74, "peak": 114.64, "min": 57.77}, "VDD_CPU_SOC_MSS": {"avg": 15.64, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 34.88, "energy_joules_est": 100.6, "sample_count": 22, "duration_seconds": 2.884}, "timestamp": "2026-01-16T15:18:54.324561"}
{"image_index": 54, "image_name": "000000005586.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005586.jpg", "image_width": 320, "image_height": 240, "image_resolution": "320x240", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3351.993, "latencies_ms": [3351.993], "images_per_second": 0.298, "prompt_tokens": 27, "response_tokens_est": 54, "n_tiles": 12, "output_text": "The main object in the foreground is a tennis player wearing a yellow shirt and black shorts. The player is standing on a blue tennis court with a white boundary line. In the background, there is a scoreboard displaying various advertisements and a person in an orange shirt.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25572.4, "ram_available_mb": 100199.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25572.4, "ram_available_mb": 100199.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.99, "peak": 15.54, "min": 14.43}, "VDD_GPU": {"avg": 33.54, "peak": 42.54, "min": 26.39}, "VIN": {"avg": 75.81, "peak": 133.54, "min": 48.63}, "VDD_CPU_SOC_MSS": {"avg": 15.57, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 33.54, "energy_joules_est": 112.44, "sample_count": 26, "duration_seconds": 3.352}, "timestamp": "2026-01-16T15:18:57.684008"}
{"image_index": 54, "image_name": "000000005586.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005586.jpg", "image_width": 320, "image_height": 240, "image_resolution": "320x240", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3061.114, "latencies_ms": [3061.114], "images_per_second": 0.327, "prompt_tokens": 21, "response_tokens_est": 45, "n_tiles": 12, "output_text": "The image depicts a scene from a tennis match taking place on a blue court with a crowd of spectators in the background. The player in the foreground is preparing to serve, while another player is observing the action.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25572.4, "ram_available_mb": 100199.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 25572.4, "ram_available_mb": 100199.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.96, "peak": 15.65, "min": 14.43}, "VDD_GPU": {"avg": 33.84, "peak": 42.15, "min": 26.0}, "VIN": {"avg": 76.14, "peak": 121.03, "min": 56.0}, "VDD_CPU_SOC_MSS": {"avg": 15.52, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 33.84, "energy_joules_est": 103.6, "sample_count": 24, "duration_seconds": 3.062}, "timestamp": "2026-01-16T15:19:00.752028"}
{"image_index": 54, "image_name": "000000005586.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005586.jpg", "image_width": 320, "image_height": 240, "image_resolution": "320x240", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2544.275, "latencies_ms": [2544.275], "images_per_second": 0.393, "prompt_tokens": 19, "response_tokens_est": 30, "n_tiles": 12, "output_text": "The image shows a blue tennis court with white boundary lines and a blue surface. The lighting is bright, indicating an indoor setting with artificial lighting.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25572.4, "ram_available_mb": 100199.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 25572.4, "ram_available_mb": 100199.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.7, "peak": 15.24, "min": 14.02}, "VDD_GPU": {"avg": 35.74, "peak": 42.15, "min": 27.18}, "VIN": {"avg": 73.7, "peak": 121.57, "min": 57.05}, "VDD_CPU_SOC_MSS": {"avg": 15.33, "peak": 16.14, "min": 14.56}}, "power_watts_avg": 35.74, "energy_joules_est": 90.95, "sample_count": 19, "duration_seconds": 2.545}, "timestamp": "2026-01-16T15:19:03.302713"}
{"image_index": 55, "image_name": "000000005600.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005600.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1160.14, "latencies_ms": [1160.14], "images_per_second": 0.862, "prompt_tokens": 9, "response_tokens_est": 44, "n_tiles": 2, "output_text": "The image shows a plate with a bowl of sweet, orange-colored fruits, possibly apricots, and a small bowl of a dark, savory-looking stew or curry, all placed on a tablecloth.", "error": null, "sys_before": {"cpu_percent": 38.1, "ram_used_mb": 25572.4, "ram_available_mb": 100199.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25572.6, "ram_available_mb": 100199.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5055.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.53, "peak": 15.22, "min": 13.69}, "VDD_GPU": {"avg": 26.22, "peak": 34.66, "min": 21.67}, "VIN": {"avg": 65.29, "peak": 86.88, "min": 60.57}, "VDD_CPU_SOC_MSS": {"avg": 15.23, "peak": 16.14, "min": 14.18}}, "power_watts_avg": 26.22, "energy_joules_est": 30.44, "sample_count": 9, "duration_seconds": 1.161}, "timestamp": "2026-01-16T15:19:04.495198"}
{"image_index": 55, "image_name": "000000005600.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005600.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1394.279, "latencies_ms": [1394.279], "images_per_second": 0.717, "prompt_tokens": 23, "response_tokens_est": 54, "n_tiles": 2, "output_text": "- plate: 1\n- bowl: 2\n- spoon: 1\n- plate: 1\n- bowl: 1\n- bowl: 1\n- bowl: 1\n- bowl: 1\n- bowl: 1", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25572.6, "ram_available_mb": 100199.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25571.6, "ram_available_mb": 100200.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5064.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.01, "peak": 15.52, "min": 14.3}, "VDD_GPU": {"avg": 24.9, "peak": 31.53, "min": 21.27}, "VIN": {"avg": 66.84, "peak": 103.78, "min": 61.41}, "VDD_CPU_SOC_MSS": {"avg": 15.79, "peak": 16.16, "min": 14.96}}, "power_watts_avg": 24.9, "energy_joules_est": 34.73, "sample_count": 10, "duration_seconds": 1.395}, "timestamp": "2026-01-16T15:19:05.895385"}
{"image_index": 55, "image_name": "000000005600.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005600.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1628.315, "latencies_ms": [1628.315], "images_per_second": 0.614, "prompt_tokens": 27, "response_tokens_est": 64, "n_tiles": 2, "output_text": "The main objects in the image are a plate with a bowl of meat and a bowl of peaches. The plate is placed on a tablecloth, and the bowl of meat is positioned to the left of the plate. The bowl of peaches is in the foreground, slightly to the right of the plate.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25571.6, "ram_available_mb": 100200.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25571.3, "ram_available_mb": 100200.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5067.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.16, "peak": 15.62, "min": 14.51}, "VDD_GPU": {"avg": 23.97, "peak": 30.33, "min": 20.88}, "VIN": {"avg": 67.01, "peak": 99.09, "min": 59.12}, "VDD_CPU_SOC_MSS": {"avg": 15.78, "peak": 16.14, "min": 15.36}}, "power_watts_avg": 23.97, "energy_joules_est": 39.05, "sample_count": 12, "duration_seconds": 1.629}, "timestamp": "2026-01-16T15:19:07.530224"}
{"image_index": 55, "image_name": "000000005600.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005600.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1894.807, "latencies_ms": [1894.807], "images_per_second": 0.528, "prompt_tokens": 21, "response_tokens_est": 76, "n_tiles": 2, "output_text": "The image depicts a dining setting with a focus on a plate of food. The plate contains a variety of items, including a bowl of meat, a bowl of peaches, and a bowl of what appears to be a sweet dish. The setting is likely a restaurant or a dining area, and the food items are being served on a table covered with a dark cloth.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25571.3, "ram_available_mb": 100200.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25571.3, "ram_available_mb": 100200.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5063.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.24, "peak": 15.62, "min": 14.61}, "VDD_GPU": {"avg": 23.05, "peak": 29.94, "min": 20.88}, "VIN": {"avg": 62.88, "peak": 70.75, "min": 55.66}, "VDD_CPU_SOC_MSS": {"avg": 15.84, "peak": 16.15, "min": 14.96}}, "power_watts_avg": 23.05, "energy_joules_est": 43.68, "sample_count": 14, "duration_seconds": 1.895}, "timestamp": "2026-01-16T15:19:09.430753"}
{"image_index": 55, "image_name": "000000005600.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005600.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1258.414, "latencies_ms": [1258.414], "images_per_second": 0.795, "prompt_tokens": 19, "response_tokens_est": 48, "n_tiles": 2, "output_text": "The image features a plate with a dark blue rim and a white center, containing a bowl of red-orange meat and a bowl of pale orange fruits. The lighting is soft and warm, casting gentle shadows on the tablecloth.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25571.3, "ram_available_mb": 100200.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25571.3, "ram_available_mb": 100200.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5062.6, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.17, "peak": 15.52, "min": 14.61}, "VDD_GPU": {"avg": 25.0, "peak": 31.53, "min": 21.67}, "VIN": {"avg": 65.73, "peak": 79.18, "min": 58.5}, "VDD_CPU_SOC_MSS": {"avg": 15.62, "peak": 16.14, "min": 14.96}}, "power_watts_avg": 25.0, "energy_joules_est": 31.47, "sample_count": 9, "duration_seconds": 1.259}, "timestamp": "2026-01-16T15:19:10.694935"}
{"image_index": 56, "image_name": "000000005992.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005992.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1255.344, "latencies_ms": [1255.344], "images_per_second": 0.797, "prompt_tokens": 9, "response_tokens_est": 25, "n_tiles": 6, "output_text": "A group of sheep are standing in a grassy area, with one sheep in the foreground looking directly at the camera.", "error": null, "sys_before": {"cpu_percent": 42.6, "ram_used_mb": 25571.3, "ram_available_mb": 100200.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25571.6, "ram_available_mb": 100200.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.47, "peak": 14.81, "min": 13.92}, "VDD_GPU": {"avg": 31.82, "peak": 36.24, "min": 26.79}, "VIN": {"avg": 70.04, "peak": 112.08, "min": 58.02}, "VDD_CPU_SOC_MSS": {"avg": 15.18, "peak": 15.36, "min": 14.57}}, "power_watts_avg": 31.82, "energy_joules_est": 39.95, "sample_count": 9, "duration_seconds": 1.256}, "timestamp": "2026-01-16T15:19:11.993930"}
{"image_index": 56, "image_name": "000000005992.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005992.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1741.014, "latencies_ms": [1741.014], "images_per_second": 0.574, "prompt_tokens": 23, "response_tokens_est": 40, "n_tiles": 6, "output_text": "object: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25571.6, "ram_available_mb": 100200.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 25571.3, "ram_available_mb": 100200.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.58, "peak": 14.91, "min": 14.02}, "VDD_GPU": {"avg": 31.09, "peak": 39.0, "min": 24.03}, "VIN": {"avg": 69.37, "peak": 108.53, "min": 61.09}, "VDD_CPU_SOC_MSS": {"avg": 15.11, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 31.09, "energy_joules_est": 54.14, "sample_count": 13, "duration_seconds": 1.742}, "timestamp": "2026-01-16T15:19:13.741120"}
{"image_index": 56, "image_name": "000000005992.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005992.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2106.371, "latencies_ms": [2106.371], "images_per_second": 0.475, "prompt_tokens": 27, "response_tokens_est": 53, "n_tiles": 6, "output_text": "The main objects in the image are sheep, with the closest sheep in the foreground and the others in the background. The sheep in the foreground are standing on a grassy area, while the sheep in the background are further away, suggesting depth in the scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25571.3, "ram_available_mb": 100200.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 25571.3, "ram_available_mb": 100200.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.6, "peak": 14.91, "min": 13.92}, "VDD_GPU": {"avg": 29.5, "peak": 38.21, "min": 23.25}, "VIN": {"avg": 68.56, "peak": 104.3, "min": 59.12}, "VDD_CPU_SOC_MSS": {"avg": 15.13, "peak": 15.36, "min": 14.57}}, "power_watts_avg": 29.5, "energy_joules_est": 62.15, "sample_count": 16, "duration_seconds": 2.107}, "timestamp": "2026-01-16T15:19:15.853543"}
{"image_index": 56, "image_name": "000000005992.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005992.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2547.172, "latencies_ms": [2547.172], "images_per_second": 0.393, "prompt_tokens": 21, "response_tokens_est": 69, "n_tiles": 6, "output_text": "The image depicts a group of sheep in a pastoral setting, likely a farm or a rural area. The sheep are standing on a grassy field with a brick wall in the background, and there is a playground visible in the distance. The sheep appear to be calm and relaxed, with some of them looking directly at the camera.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25571.3, "ram_available_mb": 100200.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 25571.3, "ram_available_mb": 100200.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.7, "peak": 15.01, "min": 14.02}, "VDD_GPU": {"avg": 28.11, "peak": 37.82, "min": 22.86}, "VIN": {"avg": 71.35, "peak": 120.1, "min": 62.46}, "VDD_CPU_SOC_MSS": {"avg": 15.22, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 28.11, "energy_joules_est": 71.61, "sample_count": 20, "duration_seconds": 2.548}, "timestamp": "2026-01-16T15:19:18.406907"}
{"image_index": 56, "image_name": "000000005992.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000005992.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2326.295, "latencies_ms": [2326.295], "images_per_second": 0.43, "prompt_tokens": 19, "response_tokens_est": 61, "n_tiles": 6, "output_text": "The sheep in the image have a thick, woolly coat that appears to be a mix of brown and gray colors. The lighting is bright and natural, suggesting it is daytime with ample sunlight. The weather seems to be clear and sunny, as the sheep are comfortably standing in a grassy area.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25571.3, "ram_available_mb": 100200.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 25571.3, "ram_available_mb": 100200.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.67, "peak": 14.91, "min": 14.02}, "VDD_GPU": {"avg": 28.59, "peak": 38.21, "min": 23.25}, "VIN": {"avg": 67.53, "peak": 105.83, "min": 54.18}, "VDD_CPU_SOC_MSS": {"avg": 15.22, "peak": 15.36, "min": 14.57}}, "power_watts_avg": 28.59, "energy_joules_est": 66.52, "sample_count": 18, "duration_seconds": 2.327}, "timestamp": "2026-01-16T15:19:20.739496"}
{"image_index": 57, "image_name": "000000006012.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006012.jpg", "image_width": 640, "image_height": 531, "image_resolution": "640x531", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2312.091, "latencies_ms": [2312.091], "images_per_second": 0.433, "prompt_tokens": 9, "response_tokens_est": 27, "n_tiles": 12, "output_text": "The image features a bunch of ripe bananas and a red apple, both resting on a fabric surface with a blue floral pattern.", "error": null, "sys_before": {"cpu_percent": 48.4, "ram_used_mb": 25571.3, "ram_available_mb": 100200.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 25571.3, "ram_available_mb": 100200.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.05, "peak": 16.05, "min": 13.69}, "VDD_GPU": {"avg": 34.93, "peak": 40.57, "min": 27.59}, "VIN": {"avg": 76.36, "peak": 117.54, "min": 59.12}, "VDD_CPU_SOC_MSS": {"avg": 15.84, "peak": 16.93, "min": 14.96}}, "power_watts_avg": 34.93, "energy_joules_est": 80.77, "sample_count": 18, "duration_seconds": 2.312}, "timestamp": "2026-01-16T15:19:23.143065"}
{"image_index": 57, "image_name": "000000006012.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006012.jpg", "image_width": 640, "image_height": 531, "image_resolution": "640x531", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2851.397, "latencies_ms": [2851.397], "images_per_second": 0.351, "prompt_tokens": 23, "response_tokens_est": 39, "n_tiles": 12, "output_text": "1. Banana\n2. Apple\n3. Banana\n4. Banana\n5. Banana\n6. Banana\n7. Banana\n8. Banana", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25571.3, "ram_available_mb": 100200.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 25571.3, "ram_available_mb": 100200.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.0, "peak": 15.54, "min": 14.43}, "VDD_GPU": {"avg": 34.9, "peak": 42.94, "min": 26.8}, "VIN": {"avg": 73.17, "peak": 109.43, "min": 47.7}, "VDD_CPU_SOC_MSS": {"avg": 15.59, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 34.9, "energy_joules_est": 99.53, "sample_count": 22, "duration_seconds": 2.852}, "timestamp": "2026-01-16T15:19:26.001942"}
{"image_index": 57, "image_name": "000000006012.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006012.jpg", "image_width": 640, "image_height": 531, "image_resolution": "640x531", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3850.548, "latencies_ms": [3850.548], "images_per_second": 0.26, "prompt_tokens": 27, "response_tokens_est": 69, "n_tiles": 12, "output_text": "The main objects in the image are a bunch of bananas and a red apple. The bananas are positioned in the foreground, with one partially cut and lying on its side. The apple is in the background, slightly to the right. The bananas are near the bottom of the image, while the apple is closer to the top.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25571.3, "ram_available_mb": 100200.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25570.4, "ram_available_mb": 100201.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.03, "peak": 15.65, "min": 14.43}, "VDD_GPU": {"avg": 32.47, "peak": 42.15, "min": 26.0}, "VIN": {"avg": 77.74, "peak": 128.88, "min": 58.42}, "VDD_CPU_SOC_MSS": {"avg": 15.58, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 32.47, "energy_joules_est": 125.04, "sample_count": 30, "duration_seconds": 3.851}, "timestamp": "2026-01-16T15:19:29.859228"}
{"image_index": 57, "image_name": "000000006012.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006012.jpg", "image_width": 640, "image_height": 531, "image_resolution": "640x531", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3883.234, "latencies_ms": [3883.234], "images_per_second": 0.258, "prompt_tokens": 21, "response_tokens_est": 70, "n_tiles": 12, "output_text": "The image features a bunch of ripe bananas and a red apple resting on a patterned blue fabric surface. The bananas are green with some brown spots, while the apple has a vibrant red and yellow color. The setting appears to be indoors, possibly in a kitchen or dining area, with the fabric providing a soft and textured background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25570.4, "ram_available_mb": 100201.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25570.4, "ram_available_mb": 100201.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.04, "peak": 15.54, "min": 14.53}, "VDD_GPU": {"avg": 32.49, "peak": 42.54, "min": 26.01}, "VIN": {"avg": 72.58, "peak": 113.82, "min": 57.96}, "VDD_CPU_SOC_MSS": {"avg": 15.56, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 32.49, "energy_joules_est": 126.18, "sample_count": 30, "duration_seconds": 3.884}, "timestamp": "2026-01-16T15:19:33.748801"}
{"image_index": 57, "image_name": "000000006012.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006012.jpg", "image_width": 640, "image_height": 531, "image_resolution": "640x531", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3011.84, "latencies_ms": [3011.84], "images_per_second": 0.332, "prompt_tokens": 19, "response_tokens_est": 44, "n_tiles": 12, "output_text": "The image features a bunch of ripe bananas and a red apple, both resting on a fabric surface with a blue floral pattern. The lighting is natural, casting soft shadows and highlighting the vibrant colors of the fruits.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25570.4, "ram_available_mb": 100201.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 25570.1, "ram_available_mb": 100202.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.0, "peak": 15.54, "min": 14.53}, "VDD_GPU": {"avg": 34.41, "peak": 42.54, "min": 26.4}, "VIN": {"avg": 71.91, "peak": 115.67, "min": 57.91}, "VDD_CPU_SOC_MSS": {"avg": 15.54, "peak": 16.14, "min": 14.96}}, "power_watts_avg": 34.41, "energy_joules_est": 103.65, "sample_count": 23, "duration_seconds": 3.012}, "timestamp": "2026-01-16T15:19:36.766821"}
{"image_index": 58, "image_name": "000000006040.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006040.jpg", "image_width": 640, "image_height": 351, "image_resolution": "640x351", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1114.16, "latencies_ms": [1114.16], "images_per_second": 0.898, "prompt_tokens": 9, "response_tokens_est": 42, "n_tiles": 2, "output_text": "A modern tram with blue and white color scheme is moving on the tracks, with the number 2 displayed on its front, and the scene is set against a backdrop of clear blue sky and green trees.", "error": null, "sys_before": {"cpu_percent": 31.6, "ram_used_mb": 25570.1, "ram_available_mb": 100202.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25570.6, "ram_available_mb": 100201.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5055.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.64, "peak": 15.11, "min": 14.0}, "VDD_GPU": {"avg": 26.55, "peak": 33.88, "min": 22.46}, "VIN": {"avg": 67.63, "peak": 93.91, "min": 62.16}, "VDD_CPU_SOC_MSS": {"avg": 15.21, "peak": 16.14, "min": 14.57}}, "power_watts_avg": 26.55, "energy_joules_est": 29.6, "sample_count": 8, "duration_seconds": 1.115}, "timestamp": "2026-01-16T15:19:37.912635"}
{"image_index": 58, "image_name": "000000006040.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006040.jpg", "image_width": 640, "image_height": 351, "image_resolution": "640x351", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1082.948, "latencies_ms": [1082.948], "images_per_second": 0.923, "prompt_tokens": 23, "response_tokens_est": 40, "n_tiles": 2, "output_text": "object: 1\nobject: 2\nobject: 3\nobject: 4\nobject: 5\nobject: 6\nobject: 7\nobject: 8", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25570.6, "ram_available_mb": 100201.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25570.6, "ram_available_mb": 100201.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5064.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.96, "peak": 15.32, "min": 14.4}, "VDD_GPU": {"avg": 25.66, "peak": 31.51, "min": 22.07}, "VIN": {"avg": 68.13, "peak": 104.68, "min": 53.0}, "VDD_CPU_SOC_MSS": {"avg": 15.6, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 25.66, "energy_joules_est": 27.8, "sample_count": 8, "duration_seconds": 1.083}, "timestamp": "2026-01-16T15:19:39.002076"}
{"image_index": 58, "image_name": "000000006040.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006040.jpg", "image_width": 640, "image_height": 351, "image_resolution": "640x351", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2446.559, "latencies_ms": [2446.559], "images_per_second": 0.409, "prompt_tokens": 27, "response_tokens_est": 101, "n_tiles": 2, "output_text": "The main object in the image is a blue and white tram, which is positioned in the foreground on the right side of the frame. The tram is moving along the tracks, indicating its role as a mode of transportation. In the background, there is a parked vehicle, possibly a van, and some trees, suggesting the tram is in an urban or suburban setting. The clear blue sky and the absence of other significant objects in the immediate vicinity further emphasize the tram as the central focus of the image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25570.6, "ram_available_mb": 100201.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25570.9, "ram_available_mb": 100201.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5067.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.29, "peak": 15.72, "min": 14.71}, "VDD_GPU": {"avg": 23.09, "peak": 31.92, "min": 20.88}, "VIN": {"avg": 64.84, "peak": 102.08, "min": 57.76}, "VDD_CPU_SOC_MSS": {"avg": 15.88, "peak": 16.14, "min": 15.35}}, "power_watts_avg": 23.09, "energy_joules_est": 56.5, "sample_count": 18, "duration_seconds": 2.447}, "timestamp": "2026-01-16T15:19:41.454231"}
{"image_index": 58, "image_name": "000000006040.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006040.jpg", "image_width": 640, "image_height": 351, "image_resolution": "640x351", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1856.747, "latencies_ms": [1856.747], "images_per_second": 0.539, "prompt_tokens": 21, "response_tokens_est": 75, "n_tiles": 2, "output_text": "The image depicts a modern tram in motion on a city street, with clear blue skies overhead. The tram is painted in a combination of white and blue, and it has the number \"2\" displayed on its front. The tram is moving along a set of tracks, and there are other vehicles and trees visible in the background, suggesting a bustling urban environment.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25570.9, "ram_available_mb": 100201.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25570.6, "ram_available_mb": 100201.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5063.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.29, "peak": 15.62, "min": 14.81}, "VDD_GPU": {"avg": 23.41, "peak": 30.73, "min": 20.89}, "VIN": {"avg": 65.29, "peak": 99.62, "min": 56.69}, "VDD_CPU_SOC_MSS": {"avg": 15.86, "peak": 16.15, "min": 15.35}}, "power_watts_avg": 23.41, "energy_joules_est": 43.48, "sample_count": 14, "duration_seconds": 1.857}, "timestamp": "2026-01-16T15:19:43.317589"}
{"image_index": 58, "image_name": "000000006040.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006040.jpg", "image_width": 640, "image_height": 351, "image_resolution": "640x351", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1112.529, "latencies_ms": [1112.529], "images_per_second": 0.899, "prompt_tokens": 19, "response_tokens_est": 42, "n_tiles": 2, "output_text": "The tram is painted in a striking blue and white color scheme, with a modern design featuring sleek lines and a streamlined shape. The lighting is bright and natural, suggesting it is daytime with clear skies.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25570.6, "ram_available_mb": 100201.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25570.6, "ram_available_mb": 100201.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5062.6, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.1, "peak": 15.32, "min": 14.81}, "VDD_GPU": {"avg": 25.46, "peak": 30.73, "min": 22.06}, "VIN": {"avg": 70.22, "peak": 104.06, "min": 62.1}, "VDD_CPU_SOC_MSS": {"avg": 15.75, "peak": 16.15, "min": 15.35}}, "power_watts_avg": 25.46, "energy_joules_est": 28.34, "sample_count": 8, "duration_seconds": 1.113}, "timestamp": "2026-01-16T15:19:44.436002"}
{"image_index": 59, "image_name": "000000006213.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006213.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1476.556, "latencies_ms": [1476.556], "images_per_second": 0.677, "prompt_tokens": 9, "response_tokens_est": 34, "n_tiles": 6, "output_text": "The image shows a well-lit bathroom with a wooden vanity, a white bathtub, a red patterned curtain, and a large mirror with gold fixtures.", "error": null, "sys_before": {"cpu_percent": 50.0, "ram_used_mb": 25570.6, "ram_available_mb": 100201.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 25570.6, "ram_available_mb": 100201.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.58, "peak": 15.11, "min": 13.92}, "VDD_GPU": {"avg": 30.73, "peak": 36.24, "min": 25.22}, "VIN": {"avg": 66.77, "peak": 74.74, "min": 60.06}, "VDD_CPU_SOC_MSS": {"avg": 15.5, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 30.73, "energy_joules_est": 45.38, "sample_count": 11, "duration_seconds": 1.477}, "timestamp": "2026-01-16T15:19:45.959539"}
{"image_index": 59, "image_name": "000000006213.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006213.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1688.462, "latencies_ms": [1688.462], "images_per_second": 0.592, "prompt_tokens": 23, "response_tokens_est": 40, "n_tiles": 6, "output_text": "object: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25570.6, "ram_available_mb": 100201.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 25571.5, "ram_available_mb": 100200.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.78, "peak": 15.11, "min": 14.22}, "VDD_GPU": {"avg": 31.75, "peak": 38.21, "min": 25.22}, "VIN": {"avg": 70.67, "peak": 96.06, "min": 52.7}, "VDD_CPU_SOC_MSS": {"avg": 15.35, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 31.75, "energy_joules_est": 53.62, "sample_count": 12, "duration_seconds": 1.689}, "timestamp": "2026-01-16T15:19:47.658093"}
{"image_index": 59, "image_name": "000000006213.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006213.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3335.344, "latencies_ms": [3335.344], "images_per_second": 0.3, "prompt_tokens": 27, "response_tokens_est": 101, "n_tiles": 6, "output_text": "The main objects in the image are a wooden vanity with a sink, a white bathtub, and a red patterned curtain. The vanity is positioned in the foreground, with the sink and faucet on the right side. The bathtub is located to the left of the vanity, and the red curtain is partially covering the bathtub. The vanity is situated near the bathtub, and the curtain is near the bathtub, creating a sense of depth in the image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25571.5, "ram_available_mb": 100200.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 25570.6, "ram_available_mb": 100201.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.98, "peak": 15.22, "min": 14.22}, "VDD_GPU": {"avg": 27.49, "peak": 38.21, "min": 23.25}, "VIN": {"avg": 70.42, "peak": 114.56, "min": 57.95}, "VDD_CPU_SOC_MSS": {"avg": 15.61, "peak": 16.14, "min": 14.96}}, "power_watts_avg": 27.49, "energy_joules_est": 91.71, "sample_count": 26, "duration_seconds": 3.336}, "timestamp": "2026-01-16T15:19:51.000420"}
{"image_index": 59, "image_name": "000000006213.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006213.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3035.2, "latencies_ms": [3035.2], "images_per_second": 0.329, "prompt_tokens": 21, "response_tokens_est": 90, "n_tiles": 6, "output_text": "The image depicts a well-lit bathroom with a modern and elegant design. The room features a wooden vanity with a white countertop, two sinks with gold faucets, and a red patterned shower curtain. The walls are painted in a light beige color, and there is a white bathtub with a towel draped over it. The bathroom appears to be clean and tidy, with a focus on a classic and luxurious aesthetic.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25570.6, "ram_available_mb": 100201.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 25570.8, "ram_available_mb": 100201.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.99, "peak": 15.22, "min": 14.22}, "VDD_GPU": {"avg": 27.73, "peak": 37.82, "min": 23.64}, "VIN": {"avg": 70.43, "peak": 114.14, "min": 56.33}, "VDD_CPU_SOC_MSS": {"avg": 15.51, "peak": 15.75, "min": 14.57}}, "power_watts_avg": 27.73, "energy_joules_est": 84.17, "sample_count": 23, "duration_seconds": 3.035}, "timestamp": "2026-01-16T15:19:54.041450"}
{"image_index": 59, "image_name": "000000006213.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006213.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2042.844, "latencies_ms": [2042.844], "images_per_second": 0.49, "prompt_tokens": 19, "response_tokens_est": 53, "n_tiles": 6, "output_text": "The bathroom features a warm, neutral color palette with beige walls and a wooden vanity. The lighting is soft and ambient, with recessed ceiling lights providing a gentle glow. The shower curtain is red, adding a pop of color to the otherwise neutral space.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25570.8, "ram_available_mb": 100201.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 25570.8, "ram_available_mb": 100201.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.84, "peak": 15.22, "min": 14.22}, "VDD_GPU": {"avg": 29.72, "peak": 37.82, "min": 24.04}, "VIN": {"avg": 68.77, "peak": 119.1, "min": 58.3}, "VDD_CPU_SOC_MSS": {"avg": 15.38, "peak": 15.75, "min": 14.57}}, "power_watts_avg": 29.72, "energy_joules_est": 60.73, "sample_count": 16, "duration_seconds": 2.043}, "timestamp": "2026-01-16T15:19:56.090296"}
{"image_index": 60, "image_name": "000000006460.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006460.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 991.322, "latencies_ms": [991.322], "images_per_second": 1.009, "prompt_tokens": 9, "response_tokens_est": 15, "n_tiles": 6, "output_text": "A surfer is performing a trick on a wave in the ocean.", "error": null, "sys_before": {"cpu_percent": 34.5, "ram_used_mb": 25570.6, "ram_available_mb": 100201.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 25570.0, "ram_available_mb": 100202.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.26, "peak": 14.43, "min": 13.92}, "VDD_GPU": {"avg": 33.77, "peak": 37.03, "min": 29.95}, "VIN": {"avg": 74.76, "peak": 112.75, "min": 51.31}, "VDD_CPU_SOC_MSS": {"avg": 14.85, "peak": 14.96, "min": 14.57}}, "power_watts_avg": 33.77, "energy_joules_est": 33.5, "sample_count": 7, "duration_seconds": 0.992}, "timestamp": "2026-01-16T15:19:57.130359"}
{"image_index": 60, "image_name": "000000006460.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006460.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1742.285, "latencies_ms": [1742.285], "images_per_second": 0.574, "prompt_tokens": 23, "response_tokens_est": 40, "n_tiles": 6, "output_text": "object: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25570.0, "ram_available_mb": 100202.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 25570.0, "ram_available_mb": 100202.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.5, "peak": 14.91, "min": 13.92}, "VDD_GPU": {"avg": 31.4, "peak": 39.78, "min": 24.04}, "VIN": {"avg": 72.0, "peak": 103.46, "min": 61.57}, "VDD_CPU_SOC_MSS": {"avg": 15.08, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 31.4, "energy_joules_est": 54.72, "sample_count": 13, "duration_seconds": 1.743}, "timestamp": "2026-01-16T15:19:58.878448"}
{"image_index": 60, "image_name": "000000006460.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006460.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2667.545, "latencies_ms": [2667.545], "images_per_second": 0.375, "prompt_tokens": 27, "response_tokens_est": 73, "n_tiles": 6, "output_text": "The main object in the foreground is a surfer performing a trick on a wave. The surfer is positioned near the center of the image, slightly to the left. The background features the ocean, with waves crashing towards the right side of the image. The surfer is in the middle of the wave, which is the most prominent feature in the scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25570.0, "ram_available_mb": 100202.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 25570.0, "ram_available_mb": 100202.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.7, "peak": 14.91, "min": 14.02}, "VDD_GPU": {"avg": 28.19, "peak": 38.21, "min": 22.86}, "VIN": {"avg": 67.7, "peak": 106.67, "min": 50.2}, "VDD_CPU_SOC_MSS": {"avg": 15.22, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 28.19, "energy_joules_est": 75.21, "sample_count": 20, "duration_seconds": 2.668}, "timestamp": "2026-01-16T15:20:01.555583"}
{"image_index": 60, "image_name": "000000006460.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006460.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2522.158, "latencies_ms": [2522.158], "images_per_second": 0.396, "prompt_tokens": 21, "response_tokens_est": 68, "n_tiles": 6, "output_text": "The image captures a dynamic scene of a surfer riding a large wave in the ocean. The surfer is in mid-air, performing a trick, with the wave's crest reaching up to their waist. The ocean is rough, with white foam and splashes, indicating the surfer's skill and the power of the wave.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25570.0, "ram_available_mb": 100202.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 25569.8, "ram_available_mb": 100202.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.67, "peak": 14.91, "min": 14.02}, "VDD_GPU": {"avg": 28.28, "peak": 37.82, "min": 22.86}, "VIN": {"avg": 65.52, "peak": 106.66, "min": 57.47}, "VDD_CPU_SOC_MSS": {"avg": 15.15, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 28.28, "energy_joules_est": 71.34, "sample_count": 19, "duration_seconds": 2.523}, "timestamp": "2026-01-16T15:20:04.084049"}
{"image_index": 60, "image_name": "000000006460.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006460.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2807.712, "latencies_ms": [2807.712], "images_per_second": 0.356, "prompt_tokens": 19, "response_tokens_est": 78, "n_tiles": 6, "output_text": "The image is a black and white photograph capturing a dynamic scene of a surfer riding a large wave. The surfer is in mid-air, performing a trick, and the wave is crashing powerfully, creating a dramatic and intense atmosphere. The lighting is natural, with sunlight illuminating the scene, highlighting the contrast between the surfer's silhouette and the white foam of the wave.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25569.8, "ram_available_mb": 100202.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 25569.8, "ram_available_mb": 100202.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.72, "peak": 15.01, "min": 14.02}, "VDD_GPU": {"avg": 27.71, "peak": 37.82, "min": 22.86}, "VIN": {"avg": 68.53, "peak": 95.65, "min": 59.19}, "VDD_CPU_SOC_MSS": {"avg": 15.24, "peak": 15.75, "min": 14.57}}, "power_watts_avg": 27.71, "energy_joules_est": 77.82, "sample_count": 21, "duration_seconds": 2.808}, "timestamp": "2026-01-16T15:20:06.898263"}
{"image_index": 61, "image_name": "000000006471.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006471.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1578.413, "latencies_ms": [1578.413], "images_per_second": 0.634, "prompt_tokens": 9, "response_tokens_est": 37, "n_tiles": 6, "output_text": "A baseball player in a white uniform with the number 10 is preparing to swing at a pitch, while a catcher and umpire are positioned behind him on the field.", "error": null, "sys_before": {"cpu_percent": 47.2, "ram_used_mb": 25569.8, "ram_available_mb": 100202.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25573.8, "ram_available_mb": 100198.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.4, "peak": 15.01, "min": 13.82}, "VDD_GPU": {"avg": 30.34, "peak": 36.24, "min": 24.83}, "VIN": {"avg": 65.83, "peak": 110.36, "min": 55.07}, "VDD_CPU_SOC_MSS": {"avg": 15.16, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 30.34, "energy_joules_est": 47.9, "sample_count": 12, "duration_seconds": 1.579}, "timestamp": "2026-01-16T15:20:08.532910"}
{"image_index": 61, "image_name": "000000006471.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006471.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1753.663, "latencies_ms": [1753.663], "images_per_second": 0.57, "prompt_tokens": 23, "response_tokens_est": 42, "n_tiles": 6, "output_text": "baseball player: 1\ncatcher: 1\numpire: 1\nhome plate: 1\npitcher: 1\nbatter: 1\nball: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25573.8, "ram_available_mb": 100198.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 25574.0, "ram_available_mb": 100198.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.74, "peak": 15.11, "min": 14.22}, "VDD_GPU": {"avg": 31.37, "peak": 38.21, "min": 24.83}, "VIN": {"avg": 73.33, "peak": 104.24, "min": 57.95}, "VDD_CPU_SOC_MSS": {"avg": 15.29, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 31.37, "energy_joules_est": 55.03, "sample_count": 13, "duration_seconds": 1.754}, "timestamp": "2026-01-16T15:20:10.292460"}
{"image_index": 61, "image_name": "000000006471.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006471.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3661.813, "latencies_ms": [3661.813], "images_per_second": 0.273, "prompt_tokens": 27, "response_tokens_est": 113, "n_tiles": 6, "output_text": "The main objects in the image are a baseball player, a catcher, and an umpire. The baseball player is positioned in the foreground, wearing a white uniform with the number 10, holding a bat, and standing ready to bat. The catcher is in the background, crouched behind the home plate, wearing a black uniform with a red and white design, and a protective helmet. The umpire is to the left of the catcher, wearing a blue shirt and gray pants, crouched down and observing the play.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25574.0, "ram_available_mb": 100198.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 25574.2, "ram_available_mb": 100197.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.02, "peak": 15.22, "min": 14.22}, "VDD_GPU": {"avg": 27.08, "peak": 38.21, "min": 23.64}, "VIN": {"avg": 71.23, "peak": 124.29, "min": 62.07}, "VDD_CPU_SOC_MSS": {"avg": 15.56, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 27.08, "energy_joules_est": 99.17, "sample_count": 29, "duration_seconds": 3.662}, "timestamp": "2026-01-16T15:20:13.960483"}
{"image_index": 61, "image_name": "000000006471.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006471.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2415.631, "latencies_ms": [2415.631], "images_per_second": 0.414, "prompt_tokens": 21, "response_tokens_est": 67, "n_tiles": 6, "output_text": "The image captures a moment during a baseball game, with a batter in a white uniform preparing to swing at a pitch. The scene takes place on a well-maintained baseball field, with a catcher crouched behind home plate, ready to catch the ball, and an umpire standing nearby, observing the play.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 25574.2, "ram_available_mb": 100197.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 25574.0, "ram_available_mb": 100198.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.91, "peak": 15.22, "min": 14.22}, "VDD_GPU": {"avg": 29.02, "peak": 37.82, "min": 23.64}, "VIN": {"avg": 72.27, "peak": 115.16, "min": 60.38}, "VDD_CPU_SOC_MSS": {"avg": 15.44, "peak": 15.75, "min": 14.57}}, "power_watts_avg": 29.02, "energy_joules_est": 70.12, "sample_count": 18, "duration_seconds": 2.416}, "timestamp": "2026-01-16T15:20:16.382626"}
{"image_index": 61, "image_name": "000000006471.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006471.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2195.761, "latencies_ms": [2195.761], "images_per_second": 0.455, "prompt_tokens": 19, "response_tokens_est": 59, "n_tiles": 6, "output_text": "The baseball player is wearing a white uniform with black and orange accents, and he is holding a bat. The catcher is wearing a black uniform with red and white accents, and he is crouched behind home plate. The lighting is bright, and the weather appears to be clear.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25574.0, "ram_available_mb": 100198.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 25576.2, "ram_available_mb": 100196.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.88, "peak": 15.22, "min": 14.22}, "VDD_GPU": {"avg": 29.34, "peak": 37.82, "min": 23.64}, "VIN": {"avg": 67.84, "peak": 117.03, "min": 56.3}, "VDD_CPU_SOC_MSS": {"avg": 15.4, "peak": 15.75, "min": 14.57}}, "power_watts_avg": 29.34, "energy_joules_est": 64.43, "sample_count": 17, "duration_seconds": 2.196}, "timestamp": "2026-01-16T15:20:18.584087"}
{"image_index": 62, "image_name": "000000006614.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006614.jpg", "image_width": 500, "image_height": 396, "image_resolution": "500x396", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2545.06, "latencies_ms": [2545.06], "images_per_second": 0.393, "prompt_tokens": 9, "response_tokens_est": 34, "n_tiles": 12, "output_text": "The image depicts a collection of various fruits, including apples, pears, and possibly peaches, arranged in a somewhat haphazard manner on a surface.", "error": null, "sys_before": {"cpu_percent": 42.6, "ram_used_mb": 25576.2, "ram_available_mb": 100196.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 25575.9, "ram_available_mb": 100196.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.08, "peak": 16.05, "min": 13.9}, "VDD_GPU": {"avg": 34.48, "peak": 40.57, "min": 27.18}, "VIN": {"avg": 78.17, "peak": 109.92, "min": 58.18}, "VDD_CPU_SOC_MSS": {"avg": 15.83, "peak": 16.92, "min": 14.96}}, "power_watts_avg": 34.48, "energy_joules_est": 87.77, "sample_count": 19, "duration_seconds": 2.546}, "timestamp": "2026-01-16T15:20:21.223774"}
{"image_index": 62, "image_name": "000000006614.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006614.jpg", "image_width": 500, "image_height": 396, "image_resolution": "500x396", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 5853.48, "latencies_ms": [5853.48], "images_per_second": 0.171, "prompt_tokens": 23, "response_tokens_est": 128, "n_tiles": 12, "output_text": "apple: 2\npear: 3\npeach: 1\nkiwi: 1\nkiwi: 1\nkiwi: 1\nkiwi: 1\nkiwi: 1\nkiwi: 1\nkiwi: 1\nkiwi: 1\nkiwi: 1\nkiwi: 1\nkiwi: 1\nkiwi: 1\nkiwi: 1\nkiwi: 1\nkiwi: 1\nkiwi: 1\nkiwi: 1\nkiwi: 1\nkiwi:", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25575.9, "ram_available_mb": 100196.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 25575.9, "ram_available_mb": 100196.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.05, "peak": 15.65, "min": 14.43}, "VDD_GPU": {"avg": 30.16, "peak": 42.15, "min": 26.0}, "VIN": {"avg": 70.81, "peak": 109.6, "min": 58.0}, "VDD_CPU_SOC_MSS": {"avg": 15.47, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 30.16, "energy_joules_est": 176.55, "sample_count": 46, "duration_seconds": 5.854}, "timestamp": "2026-01-16T15:20:27.083619"}
{"image_index": 62, "image_name": "000000006614.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006614.jpg", "image_width": 500, "image_height": 396, "image_resolution": "500x396", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2949.923, "latencies_ms": [2949.923], "images_per_second": 0.339, "prompt_tokens": 27, "response_tokens_est": 42, "n_tiles": 12, "output_text": "The main objects in the image are a cluster of peanuts and a single apple. The peanuts are scattered in the foreground, while the apple is positioned slightly to the right in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25575.9, "ram_available_mb": 100196.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 25576.2, "ram_available_mb": 100195.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.03, "peak": 15.65, "min": 14.43}, "VDD_GPU": {"avg": 34.15, "peak": 41.76, "min": 26.4}, "VIN": {"avg": 75.68, "peak": 117.17, "min": 56.78}, "VDD_CPU_SOC_MSS": {"avg": 15.58, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 34.15, "energy_joules_est": 100.76, "sample_count": 23, "duration_seconds": 2.95}, "timestamp": "2026-01-16T15:20:30.039449"}
{"image_index": 62, "image_name": "000000006614.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006614.jpg", "image_width": 500, "image_height": 396, "image_resolution": "500x396", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3214.508, "latencies_ms": [3214.508], "images_per_second": 0.311, "prompt_tokens": 21, "response_tokens_est": 50, "n_tiles": 12, "output_text": "The image depicts a collection of peanuts scattered on a surface, with a few peanuts partially buried among them. The setting appears to be a plain, light-colored background, providing a neutral backdrop that highlights the peanuts.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25576.2, "ram_available_mb": 100195.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 25576.2, "ram_available_mb": 100195.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.03, "peak": 15.54, "min": 14.43}, "VDD_GPU": {"avg": 33.71, "peak": 41.76, "min": 26.39}, "VIN": {"avg": 71.06, "peak": 101.5, "min": 57.98}, "VDD_CPU_SOC_MSS": {"avg": 15.53, "peak": 16.14, "min": 14.96}}, "power_watts_avg": 33.71, "energy_joules_est": 108.38, "sample_count": 24, "duration_seconds": 3.215}, "timestamp": "2026-01-16T15:20:33.260215"}
{"image_index": 62, "image_name": "000000006614.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006614.jpg", "image_width": 500, "image_height": 396, "image_resolution": "500x396", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3448.443, "latencies_ms": [3448.443], "images_per_second": 0.29, "prompt_tokens": 19, "response_tokens_est": 57, "n_tiles": 12, "output_text": "The image depicts a black and white photograph of a cluster of apples and peaches. The apples and peaches are depicted with a high level of detail, showcasing their textures and shapes. The lighting is soft and even, highlighting the natural colors and textures of the fruits.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25576.2, "ram_available_mb": 100195.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 25576.2, "ram_available_mb": 100195.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.03, "peak": 15.54, "min": 14.43}, "VDD_GPU": {"avg": 33.29, "peak": 42.15, "min": 26.01}, "VIN": {"avg": 73.44, "peak": 111.02, "min": 56.71}, "VDD_CPU_SOC_MSS": {"avg": 15.52, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 33.29, "energy_joules_est": 114.81, "sample_count": 26, "duration_seconds": 3.449}, "timestamp": "2026-01-16T15:20:36.715095"}
{"image_index": 63, "image_name": "000000006723.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006723.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1184.809, "latencies_ms": [1184.809], "images_per_second": 0.844, "prompt_tokens": 9, "response_tokens_est": 45, "n_tiles": 2, "output_text": "The image depicts a modern urban street scene with a curved road, a sidewalk, and a row of multi-story buildings on one side, while the other side features a green area with some vegetation and a bus stop.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25576.2, "ram_available_mb": 100195.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25576.0, "ram_available_mb": 100196.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5055.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.8, "peak": 15.42, "min": 14.0}, "VDD_GPU": {"avg": 26.09, "peak": 34.28, "min": 22.06}, "VIN": {"avg": 66.33, "peak": 75.72, "min": 60.52}, "VDD_CPU_SOC_MSS": {"avg": 15.4, "peak": 16.14, "min": 14.56}}, "power_watts_avg": 26.09, "energy_joules_est": 30.93, "sample_count": 9, "duration_seconds": 1.186}, "timestamp": "2026-01-16T15:20:37.930461"}
{"image_index": 63, "image_name": "000000006723.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006723.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 847.871, "latencies_ms": [847.871], "images_per_second": 1.179, "prompt_tokens": 23, "response_tokens_est": 30, "n_tiles": 2, "output_text": "- Street\n- Cars\n- Buildings\n- Sidewalk\n- Greenery\n- Trees\n- Utility poles\n- Streetlights", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25576.0, "ram_available_mb": 100196.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 25576.0, "ram_available_mb": 100196.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5064.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.93, "peak": 15.11, "min": 14.71}, "VDD_GPU": {"avg": 26.92, "peak": 31.51, "min": 23.25}, "VIN": {"avg": 69.92, "peak": 99.53, "min": 61.57}, "VDD_CPU_SOC_MSS": {"avg": 15.48, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 26.92, "energy_joules_est": 22.84, "sample_count": 6, "duration_seconds": 0.848}, "timestamp": "2026-01-16T15:20:38.783988"}
{"image_index": 63, "image_name": "000000006723.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006723.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1736.953, "latencies_ms": [1736.953], "images_per_second": 0.576, "prompt_tokens": 27, "response_tokens_est": 69, "n_tiles": 2, "output_text": "The main objects in the image are a street, a sidewalk, and a building. The street is on the left side of the image, with a sidewalk running parallel to it. The building is located on the right side of the image, near the sidewalk. The street curves gently to the right, and the sidewalk curves to the left.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25576.0, "ram_available_mb": 100196.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25576.0, "ram_available_mb": 100196.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5067.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.17, "peak": 15.62, "min": 14.4}, "VDD_GPU": {"avg": 24.19, "peak": 31.91, "min": 20.89}, "VIN": {"avg": 65.47, "peak": 104.6, "min": 57.44}, "VDD_CPU_SOC_MSS": {"avg": 15.75, "peak": 16.15, "min": 14.96}}, "power_watts_avg": 24.19, "energy_joules_est": 42.03, "sample_count": 13, "duration_seconds": 1.738}, "timestamp": "2026-01-16T15:20:40.527284"}
{"image_index": 63, "image_name": "000000006723.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006723.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1555.669, "latencies_ms": [1555.669], "images_per_second": 0.643, "prompt_tokens": 21, "response_tokens_est": 61, "n_tiles": 2, "output_text": "The image depicts a modern urban street scene with a mix of residential and commercial buildings lining the street. The road is curved, and there are parked cars and a bus visible. The sky is partly cloudy, and the area appears to be well-maintained with greenery and sidewalks.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25576.0, "ram_available_mb": 100196.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25576.2, "ram_available_mb": 100196.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5063.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.19, "peak": 15.62, "min": 14.61}, "VDD_GPU": {"avg": 23.8, "peak": 30.73, "min": 20.88}, "VIN": {"avg": 65.27, "peak": 98.35, "min": 55.5}, "VDD_CPU_SOC_MSS": {"avg": 15.75, "peak": 16.16, "min": 14.96}}, "power_watts_avg": 23.8, "energy_joules_est": 37.04, "sample_count": 12, "duration_seconds": 1.556}, "timestamp": "2026-01-16T15:20:42.089796"}
{"image_index": 63, "image_name": "000000006723.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006723.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1467.024, "latencies_ms": [1467.024], "images_per_second": 0.682, "prompt_tokens": 19, "response_tokens_est": 57, "n_tiles": 2, "output_text": "The image depicts a city street scene with a mix of modern and older buildings. The street is lined with a mix of concrete and brick structures, and there are green plants and trees along the sidewalk. The sky is partly cloudy, providing a soft, diffused light.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25576.2, "ram_available_mb": 100196.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25576.0, "ram_available_mb": 100196.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5062.6, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.18, "peak": 15.72, "min": 14.71}, "VDD_GPU": {"avg": 24.28, "peak": 31.51, "min": 21.27}, "VIN": {"avg": 64.52, "peak": 75.38, "min": 61.05}, "VDD_CPU_SOC_MSS": {"avg": 15.64, "peak": 16.14, "min": 14.96}}, "power_watts_avg": 24.28, "energy_joules_est": 35.63, "sample_count": 11, "duration_seconds": 1.467}, "timestamp": "2026-01-16T15:20:43.562443"}
{"image_index": 64, "image_name": "000000006763.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006763.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2677.588, "latencies_ms": [2677.588], "images_per_second": 0.373, "prompt_tokens": 9, "response_tokens_est": 38, "n_tiles": 12, "output_text": "The image depicts a group of three individuals seated at a table in what appears to be a restaurant, with one person holding a smartphone and another person wearing glasses and a blue shirt.", "error": null, "sys_before": {"cpu_percent": 51.1, "ram_used_mb": 25576.0, "ram_available_mb": 100196.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 25575.9, "ram_available_mb": 100196.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.19, "peak": 16.05, "min": 14.4}, "VDD_GPU": {"avg": 33.58, "peak": 40.18, "min": 26.79}, "VIN": {"avg": 72.77, "peak": 108.73, "min": 57.16}, "VDD_CPU_SOC_MSS": {"avg": 15.82, "peak": 16.92, "min": 15.35}}, "power_watts_avg": 33.58, "energy_joules_est": 89.93, "sample_count": 21, "duration_seconds": 2.678}, "timestamp": "2026-01-16T15:20:46.307060"}
{"image_index": 64, "image_name": "000000006763.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006763.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 5852.123, "latencies_ms": [5852.123], "images_per_second": 0.171, "prompt_tokens": 23, "response_tokens_est": 129, "n_tiles": 12, "output_text": "- television: 1\n- television set: 1\n- man: 1\n- woman: 1\n- camera: 1\n- camera: 1\n- table: 1\n- table: 1\n- wall: 1\n- wall: 1\n- wall: 1\n- wall: 1\n- wall: 1\n- wall: 1\n- wall: 1\n- wall: 1\n- wall: 1\n- wall: 1\n- wall: 1\n- wall: 1\n- wall: 1\n-", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25575.9, "ram_available_mb": 100196.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 25576.0, "ram_available_mb": 100196.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.05, "peak": 15.54, "min": 14.43}, "VDD_GPU": {"avg": 30.21, "peak": 42.54, "min": 26.0}, "VIN": {"avg": 72.71, "peak": 130.01, "min": 58.24}, "VDD_CPU_SOC_MSS": {"avg": 15.6, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 30.21, "energy_joules_est": 176.81, "sample_count": 46, "duration_seconds": 5.853}, "timestamp": "2026-01-16T15:20:52.166004"}
{"image_index": 64, "image_name": "000000006763.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006763.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3184.287, "latencies_ms": [3184.287], "images_per_second": 0.314, "prompt_tokens": 27, "response_tokens_est": 49, "n_tiles": 12, "output_text": "The main objects in the image are a man and a woman. The man is in the foreground, while the woman is slightly behind him. The man is holding a phone in his right hand, and the woman is standing close to him.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25576.0, "ram_available_mb": 100196.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25576.0, "ram_available_mb": 100196.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.03, "peak": 15.54, "min": 14.53}, "VDD_GPU": {"avg": 33.77, "peak": 42.54, "min": 26.4}, "VIN": {"avg": 73.86, "peak": 129.16, "min": 57.89}, "VDD_CPU_SOC_MSS": {"avg": 15.59, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 33.77, "energy_joules_est": 107.54, "sample_count": 25, "duration_seconds": 3.185}, "timestamp": "2026-01-16T15:20:55.356333"}
{"image_index": 64, "image_name": "000000006763.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006763.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 4466.312, "latencies_ms": [4466.312], "images_per_second": 0.224, "prompt_tokens": 21, "response_tokens_est": 87, "n_tiles": 12, "output_text": "The image depicts a cozy indoor setting, likely a restaurant or bar, with a warm and inviting ambiance. The scene includes a man and a woman posing for a photo, both smiling and looking directly at the camera. The man is wearing glasses and a blue shirt, while the woman is dressed in a white top. The background features a television screen displaying a scene, and there are framed pictures and posters on the walls.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25576.0, "ram_available_mb": 100196.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 25576.5, "ram_available_mb": 100195.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.04, "peak": 15.65, "min": 14.43}, "VDD_GPU": {"avg": 31.58, "peak": 42.54, "min": 26.0}, "VIN": {"avg": 76.74, "peak": 121.39, "min": 56.58}, "VDD_CPU_SOC_MSS": {"avg": 15.61, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 31.58, "energy_joules_est": 141.06, "sample_count": 35, "duration_seconds": 4.467}, "timestamp": "2026-01-16T15:20:59.829193"}
{"image_index": 64, "image_name": "000000006763.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006763.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3335.875, "latencies_ms": [3335.875], "images_per_second": 0.3, "prompt_tokens": 19, "response_tokens_est": 53, "n_tiles": 12, "output_text": "The image features a man and a woman seated at a table in a dimly lit room. The man is wearing a blue shirt, and the woman is in a white top. The lighting is subdued, with a warm glow illuminating the scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25576.5, "ram_available_mb": 100195.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 25576.3, "ram_available_mb": 100195.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.71, "peak": 15.24, "min": 14.12}, "VDD_GPU": {"avg": 33.38, "peak": 42.15, "min": 25.6}, "VIN": {"avg": 72.64, "peak": 113.76, "min": 58.13}, "VDD_CPU_SOC_MSS": {"avg": 15.28, "peak": 16.14, "min": 14.56}}, "power_watts_avg": 33.38, "energy_joules_est": 111.37, "sample_count": 26, "duration_seconds": 3.336}, "timestamp": "2026-01-16T15:21:03.171697"}
{"image_index": 65, "image_name": "000000006771.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006771.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1128.107, "latencies_ms": [1128.107], "images_per_second": 0.886, "prompt_tokens": 9, "response_tokens_est": 20, "n_tiles": 6, "output_text": "A woman in a costume is talking on her phone while surrounded by other people at an event.", "error": null, "sys_before": {"cpu_percent": 39.7, "ram_used_mb": 25576.3, "ram_available_mb": 100195.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 25576.3, "ram_available_mb": 100195.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.1, "peak": 14.61, "min": 13.72}, "VDD_GPU": {"avg": 33.53, "peak": 37.42, "min": 28.36}, "VIN": {"avg": 77.32, "peak": 117.12, "min": 50.83}, "VDD_CPU_SOC_MSS": {"avg": 14.71, "peak": 14.96, "min": 14.17}}, "power_watts_avg": 33.53, "energy_joules_est": 37.84, "sample_count": 8, "duration_seconds": 1.128}, "timestamp": "2026-01-16T15:21:04.354132"}
{"image_index": 65, "image_name": "000000006771.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006771.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1484.91, "latencies_ms": [1484.91], "images_per_second": 0.673, "prompt_tokens": 23, "response_tokens_est": 32, "n_tiles": 6, "output_text": "1. Woman\n2. Man\n3. Woman\n4. Man\n5. Woman\n6. Man\n7. Woman\n8. Man", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25576.3, "ram_available_mb": 100195.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 25576.3, "ram_available_mb": 100195.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.6, "peak": 15.01, "min": 14.12}, "VDD_GPU": {"avg": 33.2, "peak": 39.39, "min": 26.01}, "VIN": {"avg": 68.01, "peak": 100.39, "min": 55.1}, "VDD_CPU_SOC_MSS": {"avg": 15.17, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 33.2, "energy_joules_est": 49.31, "sample_count": 11, "duration_seconds": 1.485}, "timestamp": "2026-01-16T15:21:05.844950"}
{"image_index": 65, "image_name": "000000006771.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006771.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2901.139, "latencies_ms": [2901.139], "images_per_second": 0.345, "prompt_tokens": 27, "response_tokens_est": 85, "n_tiles": 6, "output_text": "The main object in the foreground is a woman wearing a costume with gold accents and a helmet. She is holding a phone to her ear and appears to be engaged in a conversation. In the background, there are several other people, including a man wearing a white cap and another man with a beard. The woman is positioned near the center of the image, while the other people are slightly out of focus in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25576.3, "ram_available_mb": 100195.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 25577.1, "ram_available_mb": 100195.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.96, "peak": 15.32, "min": 14.22}, "VDD_GPU": {"avg": 28.35, "peak": 38.6, "min": 23.64}, "VIN": {"avg": 70.81, "peak": 110.25, "min": 59.03}, "VDD_CPU_SOC_MSS": {"avg": 15.55, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 28.35, "energy_joules_est": 82.26, "sample_count": 22, "duration_seconds": 2.902}, "timestamp": "2026-01-16T15:21:08.752141"}
{"image_index": 65, "image_name": "000000006771.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006771.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2902.089, "latencies_ms": [2902.089], "images_per_second": 0.345, "prompt_tokens": 21, "response_tokens_est": 85, "n_tiles": 6, "output_text": "The image depicts a lively outdoor gathering, possibly a festival or a convention, with a diverse crowd of people. The setting appears to be in a public area with buildings in the background, and the atmosphere is bustling with activity. The focus is on a woman in the foreground who is engaged in a phone conversation, while other attendees are seen in the background, some wearing costumes and accessories, suggesting a themed event.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25577.1, "ram_available_mb": 100195.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 25577.0, "ram_available_mb": 100195.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.96, "peak": 15.22, "min": 14.22}, "VDD_GPU": {"avg": 27.82, "peak": 37.42, "min": 23.64}, "VIN": {"avg": 69.22, "peak": 111.75, "min": 55.82}, "VDD_CPU_SOC_MSS": {"avg": 15.54, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 27.82, "energy_joules_est": 80.75, "sample_count": 23, "duration_seconds": 2.903}, "timestamp": "2026-01-16T15:21:11.660766"}
{"image_index": 65, "image_name": "000000006771.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006771.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1565.187, "latencies_ms": [1565.187], "images_per_second": 0.639, "prompt_tokens": 19, "response_tokens_est": 35, "n_tiles": 6, "output_text": "The woman in the image is wearing a black and gold costume with intricate designs and accessories. The lighting is bright, and the scene appears to be outdoors during the daytime.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25577.0, "ram_available_mb": 100195.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 25576.7, "ram_available_mb": 100195.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.77, "peak": 15.01, "min": 14.22}, "VDD_GPU": {"avg": 31.29, "peak": 37.42, "min": 24.83}, "VIN": {"avg": 69.12, "peak": 100.23, "min": 58.23}, "VDD_CPU_SOC_MSS": {"avg": 15.32, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 31.29, "energy_joules_est": 48.99, "sample_count": 12, "duration_seconds": 1.566}, "timestamp": "2026-01-16T15:21:13.231974"}
{"image_index": 66, "image_name": "000000006818.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006818.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1762.77, "latencies_ms": [1762.77], "images_per_second": 0.567, "prompt_tokens": 9, "response_tokens_est": 44, "n_tiles": 6, "output_text": "The image shows a small, white tiled bathroom with a toilet, a green bucket, and a red bucket on the floor, along with a white toilet paper holder and a white shower head mounted on the wall.", "error": null, "sys_before": {"cpu_percent": 46.0, "ram_used_mb": 25576.7, "ram_available_mb": 100195.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 25576.7, "ram_available_mb": 100195.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.55, "peak": 15.11, "min": 13.82}, "VDD_GPU": {"avg": 30.36, "peak": 37.42, "min": 24.42}, "VIN": {"avg": 71.89, "peak": 111.29, "min": 51.92}, "VDD_CPU_SOC_MSS": {"avg": 15.2, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 30.36, "energy_joules_est": 53.53, "sample_count": 13, "duration_seconds": 1.763}, "timestamp": "2026-01-16T15:21:15.043951"}
{"image_index": 66, "image_name": "000000006818.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006818.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1792.814, "latencies_ms": [1792.814], "images_per_second": 0.558, "prompt_tokens": 23, "response_tokens_est": 42, "n_tiles": 6, "output_text": "1. Toilet\n2. Water heater\n3. Showerhead\n4. Tiled floor\n5. Tiled walls\n6. Green bucket\n7. Red bucket\n8. White pipe", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25576.7, "ram_available_mb": 100195.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 25576.7, "ram_available_mb": 100195.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.63, "peak": 14.91, "min": 14.02}, "VDD_GPU": {"avg": 30.82, "peak": 38.21, "min": 24.03}, "VIN": {"avg": 71.01, "peak": 113.11, "min": 55.0}, "VDD_CPU_SOC_MSS": {"avg": 15.17, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 30.82, "energy_joules_est": 55.27, "sample_count": 13, "duration_seconds": 1.793}, "timestamp": "2026-01-16T15:21:16.843674"}
{"image_index": 66, "image_name": "000000006818.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006818.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3004.681, "latencies_ms": [3004.681], "images_per_second": 0.333, "prompt_tokens": 27, "response_tokens_est": 85, "n_tiles": 6, "output_text": "The main objects in the image are a toilet, a green bucket, and a red bucket. The toilet is located in the foreground, with the green bucket and red bucket positioned near it. The toilet is situated on the left side of the image, while the green and red buckets are on the right side. The white pipes and the tiled walls are in the background, providing a sense of depth to the scene.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25576.7, "ram_available_mb": 100195.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 25576.9, "ram_available_mb": 100195.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.74, "peak": 15.01, "min": 14.02}, "VDD_GPU": {"avg": 27.48, "peak": 37.82, "min": 22.86}, "VIN": {"avg": 67.68, "peak": 117.23, "min": 60.5}, "VDD_CPU_SOC_MSS": {"avg": 15.34, "peak": 15.75, "min": 14.57}}, "power_watts_avg": 27.48, "energy_joules_est": 82.58, "sample_count": 23, "duration_seconds": 3.005}, "timestamp": "2026-01-16T15:21:19.854424"}
{"image_index": 66, "image_name": "000000006818.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006818.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3110.85, "latencies_ms": [3110.85], "images_per_second": 0.321, "prompt_tokens": 21, "response_tokens_est": 89, "n_tiles": 6, "output_text": "The image depicts a small, utilitarian bathroom with white tiled walls and floor. The room features a white toilet with a black circular seat, a green bucket, and a red bucket on the floor. There is a white toilet paper holder mounted on the wall, and a white showerhead is attached to the wall above the toilet. The room appears to be in a residential or small commercial setting, with no visible people or animals.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25576.9, "ram_available_mb": 100195.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 25576.9, "ram_available_mb": 100195.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.76, "peak": 15.01, "min": 14.02}, "VDD_GPU": {"avg": 27.1, "peak": 37.82, "min": 22.85}, "VIN": {"avg": 66.87, "peak": 115.75, "min": 53.11}, "VDD_CPU_SOC_MSS": {"avg": 15.31, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 27.1, "energy_joules_est": 84.32, "sample_count": 24, "duration_seconds": 3.111}, "timestamp": "2026-01-16T15:21:22.971593"}
{"image_index": 66, "image_name": "000000006818.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006818.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1629.757, "latencies_ms": [1629.757], "images_per_second": 0.614, "prompt_tokens": 19, "response_tokens_est": 36, "n_tiles": 6, "output_text": "The image depicts a small, white tiled bathroom with a single white toilet and a green bucket. The lighting is dim, and the walls are tiled in white.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25576.9, "ram_available_mb": 100195.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25576.9, "ram_available_mb": 100195.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.54, "peak": 14.81, "min": 14.02}, "VDD_GPU": {"avg": 31.18, "peak": 37.82, "min": 24.42}, "VIN": {"avg": 70.37, "peak": 96.56, "min": 54.52}, "VDD_CPU_SOC_MSS": {"avg": 15.13, "peak": 15.36, "min": 14.57}}, "power_watts_avg": 31.18, "energy_joules_est": 50.83, "sample_count": 12, "duration_seconds": 1.63}, "timestamp": "2026-01-16T15:21:24.607486"}
{"image_index": 67, "image_name": "000000006894.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006894.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2145.276, "latencies_ms": [2145.276], "images_per_second": 0.466, "prompt_tokens": 9, "response_tokens_est": 22, "n_tiles": 12, "output_text": "A man is laughing while standing next to an elephant, with the elephant's trunk resting on his shoulder.", "error": null, "sys_before": {"cpu_percent": 45.0, "ram_used_mb": 25576.9, "ram_available_mb": 100195.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 2.5, "ram_used_mb": 25576.2, "ram_available_mb": 100195.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.08, "peak": 16.05, "min": 13.79}, "VDD_GPU": {"avg": 35.75, "peak": 40.97, "min": 28.77}, "VIN": {"avg": 77.35, "peak": 114.57, "min": 58.91}, "VDD_CPU_SOC_MSS": {"avg": 15.87, "peak": 16.92, "min": 14.96}}, "power_watts_avg": 35.75, "energy_joules_est": 76.71, "sample_count": 16, "duration_seconds": 2.146}, "timestamp": "2026-01-16T15:21:26.837389"}
{"image_index": 67, "image_name": "000000006894.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006894.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2698.846, "latencies_ms": [2698.846], "images_per_second": 0.371, "prompt_tokens": 23, "response_tokens_est": 35, "n_tiles": 12, "output_text": "1. Elephant\n2. Man\n3. Glasses\n4. T-shirt\n5. Ear\n6. Neck\n7. Face\n8. Hair", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25576.2, "ram_available_mb": 100195.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 25580.5, "ram_available_mb": 100191.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.07, "peak": 15.65, "min": 14.43}, "VDD_GPU": {"avg": 35.44, "peak": 42.94, "min": 27.18}, "VIN": {"avg": 77.27, "peak": 116.69, "min": 58.36}, "VDD_CPU_SOC_MSS": {"avg": 15.63, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 35.44, "energy_joules_est": 95.66, "sample_count": 21, "duration_seconds": 2.699}, "timestamp": "2026-01-16T15:21:29.542502"}
{"image_index": 67, "image_name": "000000006894.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006894.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3083.28, "latencies_ms": [3083.28], "images_per_second": 0.324, "prompt_tokens": 27, "response_tokens_est": 46, "n_tiles": 12, "output_text": "The main object in the foreground is a person wearing a gray t-shirt. The person is interacting with an elephant, which is positioned to the left of the person. The elephant is in the background, slightly out of focus.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25580.5, "ram_available_mb": 100191.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 25580.5, "ram_available_mb": 100191.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.03, "peak": 15.54, "min": 14.53}, "VDD_GPU": {"avg": 34.31, "peak": 42.94, "min": 26.4}, "VIN": {"avg": 72.8, "peak": 111.53, "min": 49.95}, "VDD_CPU_SOC_MSS": {"avg": 15.6, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 34.31, "energy_joules_est": 105.8, "sample_count": 24, "duration_seconds": 3.084}, "timestamp": "2026-01-16T15:21:32.632682"}
{"image_index": 67, "image_name": "000000006894.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006894.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3785.156, "latencies_ms": [3785.156], "images_per_second": 0.264, "prompt_tokens": 21, "response_tokens_est": 67, "n_tiles": 12, "output_text": "The image depicts a man in a natural setting, likely a zoo or wildlife sanctuary, interacting with a group of elephants. The man is wearing a light-colored shirt and is seen laughing or smiling broadly, seemingly enjoying the moment with the elephants. The background features lush greenery and hills, indicating a serene and peaceful environment.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25580.5, "ram_available_mb": 100191.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25580.5, "ram_available_mb": 100191.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.05, "peak": 15.65, "min": 14.43}, "VDD_GPU": {"avg": 32.74, "peak": 42.54, "min": 26.01}, "VIN": {"avg": 72.48, "peak": 112.12, "min": 57.73}, "VDD_CPU_SOC_MSS": {"avg": 15.54, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 32.74, "energy_joules_est": 123.94, "sample_count": 29, "duration_seconds": 3.785}, "timestamp": "2026-01-16T15:21:36.424040"}
{"image_index": 67, "image_name": "000000006894.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006894.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3480.478, "latencies_ms": [3480.478], "images_per_second": 0.287, "prompt_tokens": 19, "response_tokens_est": 58, "n_tiles": 12, "output_text": "The image features a man wearing glasses and a light-colored shirt, standing next to a large elephant. The elephant has a dark, wrinkled skin, and the man is wearing a dark sweater. The lighting is natural, suggesting it is daytime, and the weather appears to be clear.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25580.5, "ram_available_mb": 100191.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25584.5, "ram_available_mb": 100187.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.04, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 33.13, "peak": 42.54, "min": 26.0}, "VIN": {"avg": 75.02, "peak": 112.15, "min": 56.41}, "VDD_CPU_SOC_MSS": {"avg": 15.57, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 33.13, "energy_joules_est": 115.32, "sample_count": 27, "duration_seconds": 3.481}, "timestamp": "2026-01-16T15:21:39.910936"}
{"image_index": 68, "image_name": "000000006954.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006954.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2751.926, "latencies_ms": [2751.926], "images_per_second": 0.363, "prompt_tokens": 9, "response_tokens_est": 40, "n_tiles": 12, "output_text": "The image depicts a group of four children sitting on the grass, each holding a white frisbee with a black and red logo, and they appear to be enjoying a casual outdoor activity.", "error": null, "sys_before": {"cpu_percent": 47.2, "ram_used_mb": 25584.5, "ram_available_mb": 100187.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 25584.9, "ram_available_mb": 100187.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.06, "peak": 16.15, "min": 13.79}, "VDD_GPU": {"avg": 33.99, "peak": 41.36, "min": 26.79}, "VIN": {"avg": 73.8, "peak": 116.78, "min": 58.12}, "VDD_CPU_SOC_MSS": {"avg": 15.76, "peak": 17.32, "min": 14.96}}, "power_watts_avg": 33.99, "energy_joules_est": 93.55, "sample_count": 21, "duration_seconds": 2.752}, "timestamp": "2026-01-16T15:21:42.755194"}
{"image_index": 68, "image_name": "000000006954.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006954.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2773.425, "latencies_ms": [2773.425], "images_per_second": 0.361, "prompt_tokens": 23, "response_tokens_est": 37, "n_tiles": 12, "output_text": "1. Frisbee: 2\n2. Children: 3\n3. Grass: 1\n4. Tree: 1\n5. Background: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25584.9, "ram_available_mb": 100187.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 25584.9, "ram_available_mb": 100187.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.06, "peak": 15.65, "min": 14.43}, "VDD_GPU": {"avg": 35.02, "peak": 42.15, "min": 26.8}, "VIN": {"avg": 76.84, "peak": 117.53, "min": 53.92}, "VDD_CPU_SOC_MSS": {"avg": 15.73, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 35.02, "energy_joules_est": 97.14, "sample_count": 21, "duration_seconds": 2.774}, "timestamp": "2026-01-16T15:21:45.535035"}
{"image_index": 68, "image_name": "000000006954.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006954.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3996.227, "latencies_ms": [3996.227], "images_per_second": 0.25, "prompt_tokens": 27, "response_tokens_est": 73, "n_tiles": 12, "output_text": "The main objects in the image are three children sitting on the grass. The child on the left is holding a frisbee, the child in the middle is also holding a frisbee, and the child on the right is holding a frisbee as well. The background consists of trees and foliage, indicating that the scene takes place outdoors.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25584.9, "ram_available_mb": 100187.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 25584.9, "ram_available_mb": 100187.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.06, "peak": 15.54, "min": 14.43}, "VDD_GPU": {"avg": 32.21, "peak": 41.76, "min": 26.0}, "VIN": {"avg": 71.16, "peak": 100.17, "min": 55.9}, "VDD_CPU_SOC_MSS": {"avg": 15.79, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 32.21, "energy_joules_est": 128.73, "sample_count": 31, "duration_seconds": 3.997}, "timestamp": "2026-01-16T15:21:49.539065"}
{"image_index": 68, "image_name": "000000006954.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006954.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3419.036, "latencies_ms": [3419.036], "images_per_second": 0.292, "prompt_tokens": 21, "response_tokens_est": 56, "n_tiles": 12, "output_text": "The scene depicts a group of children playing outdoors on a grassy field. They are holding white frisbees and appear to be enjoying their time together. The setting is a natural environment with trees and greenery in the background, suggesting a peaceful and relaxed atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25584.9, "ram_available_mb": 100187.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 25584.9, "ram_available_mb": 100187.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.06, "peak": 15.65, "min": 14.43}, "VDD_GPU": {"avg": 33.36, "peak": 41.36, "min": 26.0}, "VIN": {"avg": 76.3, "peak": 113.23, "min": 57.94}, "VDD_CPU_SOC_MSS": {"avg": 15.61, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 33.36, "energy_joules_est": 114.07, "sample_count": 26, "duration_seconds": 3.419}, "timestamp": "2026-01-16T15:21:52.964430"}
{"image_index": 68, "image_name": "000000006954.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000006954.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2979.537, "latencies_ms": [2979.537], "images_per_second": 0.336, "prompt_tokens": 19, "response_tokens_est": 43, "n_tiles": 12, "output_text": "The image depicts a group of children playing outdoors on a sunny day. The children are wearing casual clothing, and the grass is green and lush. The lighting is bright and natural, indicating it is daytime.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25584.9, "ram_available_mb": 100187.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 25584.9, "ram_available_mb": 100187.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.05, "peak": 15.65, "min": 14.43}, "VDD_GPU": {"avg": 34.58, "peak": 41.76, "min": 26.8}, "VIN": {"avg": 80.76, "peak": 137.14, "min": 57.18}, "VDD_CPU_SOC_MSS": {"avg": 15.64, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 34.58, "energy_joules_est": 103.04, "sample_count": 22, "duration_seconds": 2.98}, "timestamp": "2026-01-16T15:21:55.950311"}
{"image_index": 69, "image_name": "000000007088.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007088.jpg", "image_width": 478, "image_height": 640, "image_resolution": "478x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2187.606, "latencies_ms": [2187.606], "images_per_second": 0.457, "prompt_tokens": 9, "response_tokens_est": 23, "n_tiles": 12, "output_text": "A young child is holding a black umbrella with a pattern, standing on a wet sidewalk next to a hedge.", "error": null, "sys_before": {"cpu_percent": 40.3, "ram_used_mb": 25584.9, "ram_available_mb": 100187.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 2.6, "ram_used_mb": 25585.2, "ram_available_mb": 100187.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.04, "peak": 16.05, "min": 13.69}, "VDD_GPU": {"avg": 35.68, "peak": 41.36, "min": 28.36}, "VIN": {"avg": 74.44, "peak": 117.07, "min": 58.31}, "VDD_CPU_SOC_MSS": {"avg": 15.79, "peak": 16.92, "min": 14.57}}, "power_watts_avg": 35.68, "energy_joules_est": 78.07, "sample_count": 17, "duration_seconds": 2.188}, "timestamp": "2026-01-16T15:21:58.240202"}
{"image_index": 69, "image_name": "000000007088.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007088.jpg", "image_width": 478, "image_height": 640, "image_resolution": "478x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 3107.437, "latencies_ms": [3107.437], "images_per_second": 0.322, "prompt_tokens": 23, "response_tokens_est": 46, "n_tiles": 12, "output_text": "umbrella: 1\nchild: 1\ncoat: 1\nsweater: 1\nsocks: 1\nshoes: 1\nhouse: 1\ntree: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25585.2, "ram_available_mb": 100187.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 25588.7, "ram_available_mb": 100183.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.91, "peak": 15.54, "min": 14.53}, "VDD_GPU": {"avg": 34.19, "peak": 43.33, "min": 26.0}, "VIN": {"avg": 73.4, "peak": 116.17, "min": 57.86}, "VDD_CPU_SOC_MSS": {"avg": 15.48, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 34.19, "energy_joules_est": 106.26, "sample_count": 24, "duration_seconds": 3.108}, "timestamp": "2026-01-16T15:22:01.354144"}
{"image_index": 69, "image_name": "000000007088.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007088.jpg", "image_width": 478, "image_height": 640, "image_resolution": "478x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3390.439, "latencies_ms": [3390.439], "images_per_second": 0.295, "prompt_tokens": 27, "response_tokens_est": 55, "n_tiles": 12, "output_text": "The main object in the foreground is a young child holding a black umbrella with a pattern. The child is standing on a wet sidewalk, with a bush and a parked car visible in the background. The child is near the bush, and the car is parked further back.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25588.7, "ram_available_mb": 100183.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25588.7, "ram_available_mb": 100183.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.75, "peak": 15.34, "min": 14.02}, "VDD_GPU": {"avg": 33.38, "peak": 42.54, "min": 26.0}, "VIN": {"avg": 72.2, "peak": 112.68, "min": 58.24}, "VDD_CPU_SOC_MSS": {"avg": 15.26, "peak": 16.14, "min": 14.56}}, "power_watts_avg": 33.38, "energy_joules_est": 113.2, "sample_count": 26, "duration_seconds": 3.391}, "timestamp": "2026-01-16T15:22:04.751961"}
{"image_index": 69, "image_name": "000000007088.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007088.jpg", "image_width": 478, "image_height": 640, "image_resolution": "478x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3779.343, "latencies_ms": [3779.343], "images_per_second": 0.265, "prompt_tokens": 21, "response_tokens_est": 66, "n_tiles": 12, "output_text": "The image depicts a young child standing under a black umbrella in a residential area during a rainy day. The child is dressed in a red coat and blue jeans, holding the umbrella with both hands. The background shows a wet sidewalk, a parked car, and a neatly trimmed hedge, indicating a typical suburban setting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25588.7, "ram_available_mb": 100183.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25588.7, "ram_available_mb": 100183.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.71, "peak": 15.24, "min": 14.12}, "VDD_GPU": {"avg": 32.42, "peak": 42.54, "min": 25.6}, "VIN": {"avg": 71.57, "peak": 114.47, "min": 58.31}, "VDD_CPU_SOC_MSS": {"avg": 15.24, "peak": 16.14, "min": 14.56}}, "power_watts_avg": 32.42, "energy_joules_est": 122.54, "sample_count": 29, "duration_seconds": 3.78}, "timestamp": "2026-01-16T15:22:08.538023"}
{"image_index": 69, "image_name": "000000007088.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007088.jpg", "image_width": 478, "image_height": 640, "image_resolution": "478x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3776.682, "latencies_ms": [3776.682], "images_per_second": 0.265, "prompt_tokens": 19, "response_tokens_est": 66, "n_tiles": 12, "output_text": "The image depicts a young child holding a black umbrella with a pink pattern, standing on a wet sidewalk. The child is wearing a red coat and dark pants, and the umbrella provides some protection from the rain. The lighting is soft and diffused, suggesting an overcast day, and the wet ground reflects the surroundings.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25588.7, "ram_available_mb": 100183.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25590.8, "ram_available_mb": 100181.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.7, "peak": 15.24, "min": 14.02}, "VDD_GPU": {"avg": 32.3, "peak": 41.76, "min": 25.6}, "VIN": {"avg": 70.81, "peak": 117.54, "min": 56.45}, "VDD_CPU_SOC_MSS": {"avg": 15.18, "peak": 16.14, "min": 14.56}}, "power_watts_avg": 32.3, "energy_joules_est": 122.0, "sample_count": 30, "duration_seconds": 3.777}, "timestamp": "2026-01-16T15:22:12.321082"}
{"image_index": 70, "image_name": "000000007108.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007108.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1658.167, "latencies_ms": [1658.167], "images_per_second": 0.603, "prompt_tokens": 9, "response_tokens_est": 40, "n_tiles": 6, "output_text": "The image captures a group of elephants in a dusty, outdoor environment, with one elephant prominently in the foreground, its trunk raised as if it is either greeting or communicating with the others.", "error": null, "sys_before": {"cpu_percent": 37.2, "ram_used_mb": 25589.7, "ram_available_mb": 100182.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 25589.8, "ram_available_mb": 100182.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.38, "peak": 15.01, "min": 13.72}, "VDD_GPU": {"avg": 30.96, "peak": 37.82, "min": 24.83}, "VIN": {"avg": 71.69, "peak": 120.18, "min": 59.68}, "VDD_CPU_SOC_MSS": {"avg": 15.06, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 30.96, "energy_joules_est": 51.35, "sample_count": 12, "duration_seconds": 1.659}, "timestamp": "2026-01-16T15:22:14.044626"}
{"image_index": 70, "image_name": "000000007108.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007108.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1098.176, "latencies_ms": [1098.176], "images_per_second": 0.911, "prompt_tokens": 23, "response_tokens_est": 17, "n_tiles": 6, "output_text": "elephant: 2\nwater: 1\ntrees: 1", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25589.8, "ram_available_mb": 100182.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 25589.8, "ram_available_mb": 100182.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.46, "peak": 14.63, "min": 14.02}, "VDD_GPU": {"avg": 34.42, "peak": 38.21, "min": 28.76}, "VIN": {"avg": 76.2, "peak": 112.95, "min": 60.89}, "VDD_CPU_SOC_MSS": {"avg": 15.06, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 34.42, "energy_joules_est": 37.82, "sample_count": 8, "duration_seconds": 1.099}, "timestamp": "2026-01-16T15:22:15.148985"}
{"image_index": 70, "image_name": "000000007108.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007108.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2378.072, "latencies_ms": [2378.072], "images_per_second": 0.421, "prompt_tokens": 27, "response_tokens_est": 63, "n_tiles": 6, "output_text": "The main object in the foreground is an elephant, which is positioned near the center of the image. The background features another elephant, slightly out of focus, indicating that the primary focus is on the elephant in the foreground. The elephant in the background is further away, creating a sense of depth in the image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25589.8, "ram_available_mb": 100182.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 25589.8, "ram_available_mb": 100182.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.68, "peak": 15.01, "min": 13.92}, "VDD_GPU": {"avg": 29.54, "peak": 39.39, "min": 23.24}, "VIN": {"avg": 71.21, "peak": 117.96, "min": 56.23}, "VDD_CPU_SOC_MSS": {"avg": 15.29, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 29.54, "energy_joules_est": 70.26, "sample_count": 18, "duration_seconds": 2.378}, "timestamp": "2026-01-16T15:22:17.532702"}
{"image_index": 70, "image_name": "000000007108.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007108.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2354.231, "latencies_ms": [2354.231], "images_per_second": 0.425, "prompt_tokens": 21, "response_tokens_est": 62, "n_tiles": 6, "output_text": "The image captures a group of elephants in a natural, outdoor setting. The elephants are standing on a dirt path, with one elephant in the foreground appearing to be in motion, possibly walking or running. The background shows more elephants, some of which are also in motion, creating a dynamic and lively scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25589.8, "ram_available_mb": 100182.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 25589.8, "ram_available_mb": 100182.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.71, "peak": 15.01, "min": 14.02}, "VDD_GPU": {"avg": 28.6, "peak": 38.21, "min": 23.24}, "VIN": {"avg": 71.8, "peak": 112.33, "min": 52.96}, "VDD_CPU_SOC_MSS": {"avg": 15.25, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 28.6, "energy_joules_est": 67.34, "sample_count": 18, "duration_seconds": 2.355}, "timestamp": "2026-01-16T15:22:19.893032"}
{"image_index": 70, "image_name": "000000007108.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007108.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2268.543, "latencies_ms": [2268.543], "images_per_second": 0.441, "prompt_tokens": 19, "response_tokens_est": 59, "n_tiles": 6, "output_text": "The image features a group of elephants in a dusty, reddish-brown environment. The elephants have rough, wrinkled skin, and their tusks are visible. The lighting is natural, with a soft, diffused quality, suggesting it might be early morning or late afternoon.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25589.8, "ram_available_mb": 100182.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 25589.8, "ram_available_mb": 100182.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.7, "peak": 15.01, "min": 14.02}, "VDD_GPU": {"avg": 28.9, "peak": 37.82, "min": 23.25}, "VIN": {"avg": 64.51, "peak": 84.96, "min": 55.24}, "VDD_CPU_SOC_MSS": {"avg": 15.24, "peak": 15.75, "min": 14.57}}, "power_watts_avg": 28.9, "energy_joules_est": 65.57, "sample_count": 17, "duration_seconds": 2.269}, "timestamp": "2026-01-16T15:22:22.168617"}
{"image_index": 71, "image_name": "000000007278.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007278.jpg", "image_width": 640, "image_height": 482, "image_resolution": "640x482", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2080.668, "latencies_ms": [2080.668], "images_per_second": 0.481, "prompt_tokens": 9, "response_tokens_est": 20, "n_tiles": 12, "output_text": "A surfer is riding a wave in the ocean, wearing a red shirt and black shorts.", "error": null, "sys_before": {"cpu_percent": 42.9, "ram_used_mb": 25589.8, "ram_available_mb": 100182.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 2.4, "ram_used_mb": 25589.8, "ram_available_mb": 100182.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.08, "peak": 16.15, "min": 13.79}, "VDD_GPU": {"avg": 35.72, "peak": 40.97, "min": 28.76}, "VIN": {"avg": 78.21, "peak": 119.73, "min": 58.24}, "VDD_CPU_SOC_MSS": {"avg": 15.87, "peak": 16.92, "min": 14.96}}, "power_watts_avg": 35.72, "energy_joules_est": 74.33, "sample_count": 16, "duration_seconds": 2.081}, "timestamp": "2026-01-16T15:22:24.338108"}
{"image_index": 71, "image_name": "000000007278.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007278.jpg", "image_width": 640, "image_height": 482, "image_resolution": "640x482", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2880.233, "latencies_ms": [2880.233], "images_per_second": 0.347, "prompt_tokens": 23, "response_tokens_est": 40, "n_tiles": 12, "output_text": "object: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25589.8, "ram_available_mb": 100182.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 25590.0, "ram_available_mb": 100182.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.03, "peak": 15.54, "min": 14.43}, "VDD_GPU": {"avg": 35.11, "peak": 43.33, "min": 26.79}, "VIN": {"avg": 71.12, "peak": 97.67, "min": 56.0}, "VDD_CPU_SOC_MSS": {"avg": 15.55, "peak": 16.14, "min": 14.96}}, "power_watts_avg": 35.11, "energy_joules_est": 101.15, "sample_count": 22, "duration_seconds": 2.881}, "timestamp": "2026-01-16T15:22:27.225160"}
{"image_index": 71, "image_name": "000000007278.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007278.jpg", "image_width": 640, "image_height": 482, "image_resolution": "640x482", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2630.423, "latencies_ms": [2630.423], "images_per_second": 0.38, "prompt_tokens": 27, "response_tokens_est": 33, "n_tiles": 12, "output_text": "The main object in the foreground is a surfer riding a wave. The wave is in the background, and the surfer is near the water's surface.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25590.0, "ram_available_mb": 100182.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 25590.0, "ram_available_mb": 100182.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.08, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 35.65, "peak": 42.94, "min": 27.57}, "VIN": {"avg": 81.84, "peak": 118.03, "min": 64.82}, "VDD_CPU_SOC_MSS": {"avg": 15.69, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 35.65, "energy_joules_est": 93.78, "sample_count": 20, "duration_seconds": 2.631}, "timestamp": "2026-01-16T15:22:29.862015"}
{"image_index": 71, "image_name": "000000007278.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007278.jpg", "image_width": 640, "image_height": 482, "image_resolution": "640x482", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 4216.624, "latencies_ms": [4216.624], "images_per_second": 0.237, "prompt_tokens": 21, "response_tokens_est": 80, "n_tiles": 12, "output_text": "The image captures a dynamic scene of a surfer riding a wave in the ocean. The surfer is skillfully maneuvering on a white surfboard, with the water splashing around him, creating a sense of movement and excitement. The setting is an ocean with a turbulent wave, and the surfer is dressed in a red and black wetsuit, indicating the cold water conditions.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25590.0, "ram_available_mb": 100182.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 25591.1, "ram_available_mb": 100181.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.1, "peak": 15.65, "min": 14.43}, "VDD_GPU": {"avg": 31.96, "peak": 42.54, "min": 26.0}, "VIN": {"avg": 75.98, "peak": 119.87, "min": 58.02}, "VDD_CPU_SOC_MSS": {"avg": 15.68, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 31.96, "energy_joules_est": 134.77, "sample_count": 33, "duration_seconds": 4.217}, "timestamp": "2026-01-16T15:22:34.085137"}
{"image_index": 71, "image_name": "000000007278.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007278.jpg", "image_width": 640, "image_height": 482, "image_resolution": "640x482", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3504.79, "latencies_ms": [3504.79], "images_per_second": 0.285, "prompt_tokens": 19, "response_tokens_est": 59, "n_tiles": 12, "output_text": "The image captures a surfer skillfully riding a wave, with the surfer dressed in a red and black wetsuit. The wave is a vibrant green, indicating a sunny day with clear skies. The lighting is bright and natural, casting a clear reflection on the water's surface.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25591.1, "ram_available_mb": 100181.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 25590.9, "ram_available_mb": 100181.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.07, "peak": 15.65, "min": 14.43}, "VDD_GPU": {"avg": 33.15, "peak": 42.15, "min": 26.39}, "VIN": {"avg": 71.55, "peak": 121.88, "min": 58.13}, "VDD_CPU_SOC_MSS": {"avg": 15.6, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 33.15, "energy_joules_est": 116.2, "sample_count": 27, "duration_seconds": 3.505}, "timestamp": "2026-01-16T15:22:37.597029"}
{"image_index": 72, "image_name": "000000007281.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007281.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 793.373, "latencies_ms": [793.373], "images_per_second": 1.26, "prompt_tokens": 9, "response_tokens_est": 27, "n_tiles": 2, "output_text": "Two people are riding horses on a sandy beach, with one person holding a stick, while the other is holding a flag.", "error": null, "sys_before": {"cpu_percent": 31.4, "ram_used_mb": 25590.9, "ram_available_mb": 100181.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 25591.1, "ram_available_mb": 100181.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5055.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.32, "peak": 14.81, "min": 13.9}, "VDD_GPU": {"avg": 27.97, "peak": 33.88, "min": 23.64}, "VIN": {"avg": 68.46, "peak": 106.61, "min": 55.49}, "VDD_CPU_SOC_MSS": {"avg": 15.03, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 27.97, "energy_joules_est": 22.21, "sample_count": 6, "duration_seconds": 0.794}, "timestamp": "2026-01-16T15:22:38.420662"}
{"image_index": 72, "image_name": "000000007281.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007281.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1535.326, "latencies_ms": [1535.326], "images_per_second": 0.651, "prompt_tokens": 23, "response_tokens_est": 60, "n_tiles": 2, "output_text": "1. Horse: 1\n2. Rider: 2\n3. Horseback rider: 1\n4. Horse: 1\n5. Horseback rider: 1\n6. Horse: 1\n7. Horse: 1\n8. Horse: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25591.1, "ram_available_mb": 100181.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25590.9, "ram_available_mb": 100181.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5064.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.92, "peak": 15.22, "min": 14.51}, "VDD_GPU": {"avg": 24.79, "peak": 33.09, "min": 21.27}, "VIN": {"avg": 66.56, "peak": 102.34, "min": 54.97}, "VDD_CPU_SOC_MSS": {"avg": 15.82, "peak": 16.15, "min": 14.96}}, "power_watts_avg": 24.79, "energy_joules_est": 38.07, "sample_count": 11, "duration_seconds": 1.536}, "timestamp": "2026-01-16T15:22:39.961895"}
{"image_index": 72, "image_name": "000000007281.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007281.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2781.06, "latencies_ms": [2781.06], "images_per_second": 0.36, "prompt_tokens": 27, "response_tokens_est": 114, "n_tiles": 2, "output_text": "The main objects in the image are two people riding horses on a sandy beach. The person on the left is wearing a white shirt and a hat, while the person on the right is wearing a white shirt and a hat as well. The horses are in the foreground, with the person on the left riding a white horse and the person on the right riding a brown horse. In the background, there are more people on the beach, some near the water and others further away. The sky is partly cloudy, and the ocean is visible in the distance.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25590.9, "ram_available_mb": 100181.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25591.5, "ram_available_mb": 100180.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5067.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.01, "peak": 15.22, "min": 14.4}, "VDD_GPU": {"avg": 22.65, "peak": 31.12, "min": 20.49}, "VIN": {"avg": 64.94, "peak": 103.96, "min": 58.54}, "VDD_CPU_SOC_MSS": {"avg": 16.14, "peak": 16.54, "min": 15.36}}, "power_watts_avg": 22.65, "energy_joules_est": 63.0, "sample_count": 21, "duration_seconds": 2.781}, "timestamp": "2026-01-16T15:22:42.749046"}
{"image_index": 72, "image_name": "000000007281.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007281.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1920.853, "latencies_ms": [1920.853], "images_per_second": 0.521, "prompt_tokens": 21, "response_tokens_est": 77, "n_tiles": 2, "output_text": "The image depicts a lively beach scene with two individuals riding horses. One person is on a white horse, while the other is on a brown horse. They are both wearing traditional attire and appear to be enjoying a moment of leisure on the sandy beach. The background shows a calm ocean with a few people swimming and enjoying the beach, under a partly cloudy sky.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25591.5, "ram_available_mb": 100180.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25591.0, "ram_available_mb": 100181.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5063.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.14, "peak": 15.42, "min": 14.51}, "VDD_GPU": {"avg": 23.53, "peak": 30.74, "min": 20.88}, "VIN": {"avg": 66.03, "peak": 106.76, "min": 57.68}, "VDD_CPU_SOC_MSS": {"avg": 15.89, "peak": 16.15, "min": 15.36}}, "power_watts_avg": 23.53, "energy_joules_est": 45.21, "sample_count": 14, "duration_seconds": 1.921}, "timestamp": "2026-01-16T15:22:44.680368"}
{"image_index": 72, "image_name": "000000007281.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007281.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1081.816, "latencies_ms": [1081.816], "images_per_second": 0.924, "prompt_tokens": 19, "response_tokens_est": 40, "n_tiles": 2, "output_text": "The image depicts a sunny beach scene with clear blue skies and scattered clouds. The sandy beach is populated with people, a horse, and a rider, all bathed in bright sunlight.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25591.0, "ram_available_mb": 100181.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25590.8, "ram_available_mb": 100181.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5062.6, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.06, "peak": 15.42, "min": 14.71}, "VDD_GPU": {"avg": 25.41, "peak": 31.12, "min": 21.67}, "VIN": {"avg": 65.47, "peak": 86.22, "min": 58.57}, "VDD_CPU_SOC_MSS": {"avg": 15.7, "peak": 16.14, "min": 14.96}}, "power_watts_avg": 25.41, "energy_joules_est": 27.5, "sample_count": 8, "duration_seconds": 1.082}, "timestamp": "2026-01-16T15:22:45.768386"}
{"image_index": 73, "image_name": "000000007386.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007386.jpg", "image_width": 600, "image_height": 400, "image_resolution": "600x400", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1445.586, "latencies_ms": [1445.586], "images_per_second": 0.692, "prompt_tokens": 9, "response_tokens_est": 32, "n_tiles": 6, "output_text": "A small dog is standing near a red tractor with a black seat, which is parked on a gravel surface in front of a building with a bicycle rack.", "error": null, "sys_before": {"cpu_percent": 43.6, "ram_used_mb": 25590.8, "ram_available_mb": 100181.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 25590.8, "ram_available_mb": 100181.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.56, "peak": 14.91, "min": 14.02}, "VDD_GPU": {"avg": 30.87, "peak": 37.03, "min": 25.22}, "VIN": {"avg": 72.18, "peak": 105.86, "min": 63.07}, "VDD_CPU_SOC_MSS": {"avg": 15.21, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 30.87, "energy_joules_est": 44.64, "sample_count": 11, "duration_seconds": 1.446}, "timestamp": "2026-01-16T15:22:47.257936"}
{"image_index": 73, "image_name": "000000007386.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007386.jpg", "image_width": 600, "image_height": 400, "image_resolution": "600x400", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1733.86, "latencies_ms": [1733.86], "images_per_second": 0.577, "prompt_tokens": 23, "response_tokens_est": 41, "n_tiles": 6, "output_text": "1. Motorcycle\n2. Lawn mower\n3. Dog\n4. Bicycle\n5. Car\n6. Tire\n7. Wheel\n8. Lawn mower", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25590.8, "ram_available_mb": 100181.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25590.8, "ram_available_mb": 100181.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.73, "peak": 15.11, "min": 14.22}, "VDD_GPU": {"avg": 31.21, "peak": 38.21, "min": 24.83}, "VIN": {"avg": 78.52, "peak": 119.11, "min": 58.81}, "VDD_CPU_SOC_MSS": {"avg": 15.35, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 31.21, "energy_joules_est": 54.12, "sample_count": 13, "duration_seconds": 1.734}, "timestamp": "2026-01-16T15:22:49.001904"}
{"image_index": 73, "image_name": "000000007386.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007386.jpg", "image_width": 600, "image_height": 400, "image_resolution": "600x400", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2642.723, "latencies_ms": [2642.723], "images_per_second": 0.378, "prompt_tokens": 27, "response_tokens_est": 72, "n_tiles": 6, "output_text": "The main object in the foreground is a red tractor with a black seat. The tractor is positioned on a gravel surface, with a small dog standing near it. In the background, there is a parked silver car and a bicycle leaning against a building. The setting appears to be a rural or semi-rural area with greenery and trees in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25590.8, "ram_available_mb": 100181.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 25591.0, "ram_available_mb": 100181.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.72, "peak": 15.01, "min": 14.02}, "VDD_GPU": {"avg": 28.36, "peak": 38.21, "min": 22.86}, "VIN": {"avg": 69.58, "peak": 95.22, "min": 53.27}, "VDD_CPU_SOC_MSS": {"avg": 15.22, "peak": 15.36, "min": 14.57}}, "power_watts_avg": 28.36, "energy_joules_est": 74.96, "sample_count": 20, "duration_seconds": 2.643}, "timestamp": "2026-01-16T15:22:51.651725"}
{"image_index": 73, "image_name": "000000007386.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007386.jpg", "image_width": 600, "image_height": 400, "image_resolution": "600x400", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1744.528, "latencies_ms": [1744.528], "images_per_second": 0.573, "prompt_tokens": 21, "response_tokens_est": 40, "n_tiles": 6, "output_text": "The image depicts a rustic outdoor scene with a red tractor and a small dog standing beside it. The setting appears to be a rural area with greenery and a clear sky in the background.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 25591.0, "ram_available_mb": 100181.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 25590.8, "ram_available_mb": 100181.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.57, "peak": 14.91, "min": 14.02}, "VDD_GPU": {"avg": 30.42, "peak": 38.21, "min": 24.04}, "VIN": {"avg": 69.54, "peak": 81.96, "min": 62.8}, "VDD_CPU_SOC_MSS": {"avg": 15.23, "peak": 15.36, "min": 14.57}}, "power_watts_avg": 30.42, "energy_joules_est": 53.08, "sample_count": 13, "duration_seconds": 1.745}, "timestamp": "2026-01-16T15:22:53.404662"}
{"image_index": 73, "image_name": "000000007386.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007386.jpg", "image_width": 600, "image_height": 400, "image_resolution": "600x400", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2188.037, "latencies_ms": [2188.037], "images_per_second": 0.457, "prompt_tokens": 19, "response_tokens_est": 56, "n_tiles": 6, "output_text": "The image features a red tractor with a black seat, positioned on a gravel surface. The tractor is equipped with a metal ladder, and there is a bicycle leaning against a building in the background. The lighting is natural, suggesting daytime, and the weather appears to be clear.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25590.8, "ram_available_mb": 100181.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 25591.0, "ram_available_mb": 100181.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.65, "peak": 14.91, "min": 14.02}, "VDD_GPU": {"avg": 29.06, "peak": 38.21, "min": 23.24}, "VIN": {"avg": 71.41, "peak": 109.29, "min": 58.18}, "VDD_CPU_SOC_MSS": {"avg": 15.22, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 29.06, "energy_joules_est": 63.6, "sample_count": 17, "duration_seconds": 2.188}, "timestamp": "2026-01-16T15:22:55.598898"}
{"image_index": 74, "image_name": "000000007511.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007511.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2454.478, "latencies_ms": [2454.478], "images_per_second": 0.407, "prompt_tokens": 9, "response_tokens_est": 31, "n_tiles": 12, "output_text": "A man is standing on a sandy beach, holding a kite in his hands, with a few people around him, enjoying the sunny day.", "error": null, "sys_before": {"cpu_percent": 49.5, "ram_used_mb": 25590.7, "ram_available_mb": 100181.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 25591.2, "ram_available_mb": 100180.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.07, "peak": 16.05, "min": 13.79}, "VDD_GPU": {"avg": 34.6, "peak": 40.97, "min": 27.18}, "VIN": {"avg": 75.5, "peak": 99.04, "min": 58.39}, "VDD_CPU_SOC_MSS": {"avg": 15.75, "peak": 16.92, "min": 14.96}}, "power_watts_avg": 34.6, "energy_joules_est": 84.94, "sample_count": 19, "duration_seconds": 2.455}, "timestamp": "2026-01-16T15:22:58.133747"}
{"image_index": 74, "image_name": "000000007511.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007511.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2902.863, "latencies_ms": [2902.863], "images_per_second": 0.344, "prompt_tokens": 23, "response_tokens_est": 40, "n_tiles": 12, "output_text": "object: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25591.2, "ram_available_mb": 100180.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 25591.2, "ram_available_mb": 100180.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.95, "peak": 15.54, "min": 14.53}, "VDD_GPU": {"avg": 34.77, "peak": 42.94, "min": 26.39}, "VIN": {"avg": 75.87, "peak": 140.9, "min": 58.24}, "VDD_CPU_SOC_MSS": {"avg": 15.51, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 34.77, "energy_joules_est": 100.94, "sample_count": 22, "duration_seconds": 2.903}, "timestamp": "2026-01-16T15:23:01.047097"}
{"image_index": 74, "image_name": "000000007511.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007511.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 4349.385, "latencies_ms": [4349.385], "images_per_second": 0.23, "prompt_tokens": 27, "response_tokens_est": 83, "n_tiles": 12, "output_text": "The main objects in the image are a person standing on the sandy beach, a person flying a kite in the sky, and a person walking on the beach. The person on the beach is positioned in the foreground, while the person flying the kite is in the background. The person walking on the beach is near the foreground, and the person flying the kite is further away in the sky.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25591.2, "ram_available_mb": 100180.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 25589.9, "ram_available_mb": 100182.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.73, "peak": 15.34, "min": 14.02}, "VDD_GPU": {"avg": 31.67, "peak": 42.54, "min": 25.6}, "VIN": {"avg": 71.1, "peak": 116.19, "min": 50.73}, "VDD_CPU_SOC_MSS": {"avg": 15.2, "peak": 16.14, "min": 14.56}}, "power_watts_avg": 31.67, "energy_joules_est": 137.76, "sample_count": 34, "duration_seconds": 4.35}, "timestamp": "2026-01-16T15:23:05.403504"}
{"image_index": 74, "image_name": "000000007511.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007511.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3088.378, "latencies_ms": [3088.378], "images_per_second": 0.324, "prompt_tokens": 21, "response_tokens_est": 46, "n_tiles": 12, "output_text": "The image depicts a sunny beach scene with a man standing on the sand, holding a kite in his hands. The beach is populated with people, some walking and others sitting on the sand, enjoying the pleasant weather.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25589.9, "ram_available_mb": 100182.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 25589.6, "ram_available_mb": 100182.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.72, "peak": 15.34, "min": 14.12}, "VDD_GPU": {"avg": 33.88, "peak": 42.54, "min": 26.0}, "VIN": {"avg": 75.35, "peak": 116.25, "min": 50.28}, "VDD_CPU_SOC_MSS": {"avg": 15.29, "peak": 16.14, "min": 14.56}}, "power_watts_avg": 33.88, "energy_joules_est": 104.65, "sample_count": 24, "duration_seconds": 3.089}, "timestamp": "2026-01-16T15:23:08.498367"}
{"image_index": 74, "image_name": "000000007511.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007511.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2883.079, "latencies_ms": [2883.079], "images_per_second": 0.347, "prompt_tokens": 19, "response_tokens_est": 40, "n_tiles": 12, "output_text": "The image depicts a sunny day at a beach with a clear blue sky and scattered clouds. The sandy beach is populated with people, some walking and others sitting, enjoying the pleasant weather.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25589.6, "ram_available_mb": 100182.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 25589.6, "ram_available_mb": 100182.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.71, "peak": 15.34, "min": 14.12}, "VDD_GPU": {"avg": 34.46, "peak": 42.15, "min": 26.39}, "VIN": {"avg": 74.0, "peak": 107.9, "min": 58.34}, "VDD_CPU_SOC_MSS": {"avg": 15.29, "peak": 16.14, "min": 14.57}}, "power_watts_avg": 34.46, "energy_joules_est": 99.37, "sample_count": 23, "duration_seconds": 2.884}, "timestamp": "2026-01-16T15:23:11.388430"}
{"image_index": 75, "image_name": "000000007574.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007574.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2681.721, "latencies_ms": [2681.721], "images_per_second": 0.373, "prompt_tokens": 9, "response_tokens_est": 38, "n_tiles": 12, "output_text": "The image depicts a cluttered kitchen counter with various items scattered around, including a green bottle, a red gift, a blue bottle, a yellow notebook, and a silver knife.", "error": null, "sys_before": {"cpu_percent": 44.9, "ram_used_mb": 25589.6, "ram_available_mb": 100182.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 25589.7, "ram_available_mb": 100182.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.03, "peak": 16.15, "min": 13.59}, "VDD_GPU": {"avg": 34.25, "peak": 41.36, "min": 26.79}, "VIN": {"avg": 74.78, "peak": 113.47, "min": 58.37}, "VDD_CPU_SOC_MSS": {"avg": 15.75, "peak": 17.32, "min": 14.57}}, "power_watts_avg": 34.25, "energy_joules_est": 91.86, "sample_count": 20, "duration_seconds": 2.682}, "timestamp": "2026-01-16T15:23:14.174109"}
{"image_index": 75, "image_name": "000000007574.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007574.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 5856.261, "latencies_ms": [5856.261], "images_per_second": 0.171, "prompt_tokens": 23, "response_tokens_est": 128, "n_tiles": 12, "output_text": "object: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject:", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25589.7, "ram_available_mb": 100182.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 25589.7, "ram_available_mb": 100182.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.06, "peak": 15.54, "min": 14.43}, "VDD_GPU": {"avg": 30.29, "peak": 42.94, "min": 26.0}, "VIN": {"avg": 71.48, "peak": 130.73, "min": 57.62}, "VDD_CPU_SOC_MSS": {"avg": 15.54, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 30.29, "energy_joules_est": 177.4, "sample_count": 46, "duration_seconds": 5.857}, "timestamp": "2026-01-16T15:23:20.037394"}
{"image_index": 75, "image_name": "000000007574.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007574.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 4943.508, "latencies_ms": [4943.508], "images_per_second": 0.202, "prompt_tokens": 27, "response_tokens_est": 101, "n_tiles": 12, "output_text": "The main objects in the image are located in the kitchen, with the sink and countertop in the foreground, and various kitchen items and appliances in the background. The sink is positioned near the countertop, with a green bottle and a red gift placed on the countertop. The kitchen cabinets are on the left side of the image, and the appliances, such as the refrigerator and microwave, are on the right side. The tiled backsplash and the wall tiles are also visible in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25589.7, "ram_available_mb": 100182.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 25588.9, "ram_available_mb": 100183.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.07, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 31.02, "peak": 42.54, "min": 26.0}, "VIN": {"avg": 73.52, "peak": 122.24, "min": 56.5}, "VDD_CPU_SOC_MSS": {"avg": 15.57, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 31.02, "energy_joules_est": 153.36, "sample_count": 39, "duration_seconds": 4.944}, "timestamp": "2026-01-16T15:23:24.986976"}
{"image_index": 75, "image_name": "000000007574.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007574.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3652.505, "latencies_ms": [3652.505], "images_per_second": 0.274, "prompt_tokens": 21, "response_tokens_est": 63, "n_tiles": 12, "output_text": "The image depicts a cluttered kitchen counter with various items scattered around. The counter is cluttered with kitchen utensils, containers, and a green bottle, while the surrounding cabinets and countertops are visible. The setting appears to be a kitchen with a mix of modern and somewhat dated appliances and decor.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25588.9, "ram_available_mb": 100183.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25588.9, "ram_available_mb": 100183.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.06, "peak": 15.65, "min": 14.43}, "VDD_GPU": {"avg": 32.81, "peak": 42.54, "min": 26.0}, "VIN": {"avg": 71.92, "peak": 107.96, "min": 58.21}, "VDD_CPU_SOC_MSS": {"avg": 15.62, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 32.81, "energy_joules_est": 119.85, "sample_count": 28, "duration_seconds": 3.653}, "timestamp": "2026-01-16T15:23:28.647234"}
{"image_index": 75, "image_name": "000000007574.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007574.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3215.494, "latencies_ms": [3215.494], "images_per_second": 0.311, "prompt_tokens": 19, "response_tokens_est": 50, "n_tiles": 12, "output_text": "The kitchen is well-lit with natural light streaming in from a window, creating a bright and inviting atmosphere. The wooden cabinets and countertops provide a warm and cozy feel, while the stainless steel refrigerator and black countertop add a modern touch.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25588.9, "ram_available_mb": 100183.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 25588.9, "ram_available_mb": 100183.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.03, "peak": 15.54, "min": 14.43}, "VDD_GPU": {"avg": 33.8, "peak": 42.54, "min": 26.4}, "VIN": {"avg": 73.28, "peak": 117.62, "min": 45.1}, "VDD_CPU_SOC_MSS": {"avg": 15.59, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 33.8, "energy_joules_est": 108.69, "sample_count": 25, "duration_seconds": 3.216}, "timestamp": "2026-01-16T15:23:31.869877"}
{"image_index": 76, "image_name": "000000007784.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007784.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2084.886, "latencies_ms": [2084.886], "images_per_second": 0.48, "prompt_tokens": 9, "response_tokens_est": 20, "n_tiles": 12, "output_text": "A kite with a red and white color scheme is flying high in the clear blue sky.", "error": null, "sys_before": {"cpu_percent": 48.7, "ram_used_mb": 25588.9, "ram_available_mb": 100183.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 2.5, "ram_used_mb": 25588.9, "ram_available_mb": 100183.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.1, "peak": 16.15, "min": 13.79}, "VDD_GPU": {"avg": 36.17, "peak": 41.76, "min": 29.15}, "VIN": {"avg": 77.34, "peak": 112.67, "min": 58.17}, "VDD_CPU_SOC_MSS": {"avg": 15.82, "peak": 16.92, "min": 14.57}}, "power_watts_avg": 36.17, "energy_joules_est": 75.42, "sample_count": 16, "duration_seconds": 2.085}, "timestamp": "2026-01-16T15:23:34.040227"}
{"image_index": 76, "image_name": "000000007784.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007784.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2873.32, "latencies_ms": [2873.32], "images_per_second": 0.348, "prompt_tokens": 23, "response_tokens_est": 40, "n_tiles": 12, "output_text": "object: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25588.9, "ram_available_mb": 100183.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 25588.9, "ram_available_mb": 100183.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.05, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 35.27, "peak": 43.33, "min": 26.79}, "VIN": {"avg": 73.79, "peak": 105.06, "min": 58.46}, "VDD_CPU_SOC_MSS": {"avg": 15.66, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 35.27, "energy_joules_est": 101.36, "sample_count": 22, "duration_seconds": 2.874}, "timestamp": "2026-01-16T15:23:36.920081"}
{"image_index": 76, "image_name": "000000007784.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007784.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3720.483, "latencies_ms": [3720.483], "images_per_second": 0.269, "prompt_tokens": 27, "response_tokens_est": 65, "n_tiles": 12, "output_text": "The main object in the image is a kite, which is prominently positioned in the foreground. The kite is flying high in the sky, with its tail extending towards the right side of the image. The background features a clear blue sky, providing a contrasting backdrop to the vibrant colors of the kite.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25588.9, "ram_available_mb": 100183.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25589.1, "ram_available_mb": 100183.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.07, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 32.7, "peak": 42.94, "min": 26.0}, "VIN": {"avg": 77.09, "peak": 129.78, "min": 58.33}, "VDD_CPU_SOC_MSS": {"avg": 15.58, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 32.7, "energy_joules_est": 121.68, "sample_count": 29, "duration_seconds": 3.721}, "timestamp": "2026-01-16T15:23:40.651379"}
{"image_index": 76, "image_name": "000000007784.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007784.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 4148.696, "latencies_ms": [4148.696], "images_per_second": 0.241, "prompt_tokens": 21, "response_tokens_est": 78, "n_tiles": 12, "output_text": "The image depicts a vibrant scene of a kite flying in the clear blue sky. The kite, with its striking red and white color scheme, is captured in mid-flight, with its tail and tail fin visible against the backdrop of the sky. The scene suggests a sunny day, with the kite's design and the clear sky indicating a pleasant and enjoyable outdoor activity.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25589.1, "ram_available_mb": 100183.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25588.9, "ram_available_mb": 100183.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.07, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 32.09, "peak": 42.54, "min": 26.01}, "VIN": {"avg": 74.08, "peak": 130.56, "min": 58.58}, "VDD_CPU_SOC_MSS": {"avg": 15.56, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 32.09, "energy_joules_est": 133.14, "sample_count": 32, "duration_seconds": 4.149}, "timestamp": "2026-01-16T15:23:44.806459"}
{"image_index": 76, "image_name": "000000007784.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007784.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2771.94, "latencies_ms": [2771.94], "images_per_second": 0.361, "prompt_tokens": 19, "response_tokens_est": 37, "n_tiles": 12, "output_text": "The kite in the image is predominantly white with a striking red and black design. It is flying high against a clear blue sky, indicating a sunny day with good weather conditions.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25588.9, "ram_available_mb": 100183.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 25588.8, "ram_available_mb": 100183.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.1, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 35.0, "peak": 42.15, "min": 26.79}, "VIN": {"avg": 75.42, "peak": 109.44, "min": 56.53}, "VDD_CPU_SOC_MSS": {"avg": 15.63, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 35.0, "energy_joules_est": 97.03, "sample_count": 21, "duration_seconds": 2.772}, "timestamp": "2026-01-16T15:23:47.585377"}
{"image_index": 77, "image_name": "000000007795.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007795.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1883.892, "latencies_ms": [1883.892], "images_per_second": 0.531, "prompt_tokens": 9, "response_tokens_est": 48, "n_tiles": 6, "output_text": "The image shows a well-lit, modern bedroom with a large window that offers a view of trees outside, a bed with white and black pillows, a nightstand with a lamp, and a doorway leading to another room.", "error": null, "sys_before": {"cpu_percent": 44.6, "ram_used_mb": 25588.8, "ram_available_mb": 100183.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 25588.8, "ram_available_mb": 100183.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.52, "peak": 15.11, "min": 13.82}, "VDD_GPU": {"avg": 30.05, "peak": 37.42, "min": 24.04}, "VIN": {"avg": 70.66, "peak": 110.57, "min": 50.82}, "VDD_CPU_SOC_MSS": {"avg": 15.16, "peak": 15.75, "min": 14.57}}, "power_watts_avg": 30.05, "energy_joules_est": 56.62, "sample_count": 14, "duration_seconds": 1.884}, "timestamp": "2026-01-16T15:23:49.533342"}
{"image_index": 77, "image_name": "000000007795.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007795.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1868.042, "latencies_ms": [1868.042], "images_per_second": 0.535, "prompt_tokens": 23, "response_tokens_est": 46, "n_tiles": 6, "output_text": "bed: 2\npillows: 4\npillow covers: 2\nnightstand: 1\ntable lamp: 1\nwindow: 1\ndoor: 1\nwall art: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25588.8, "ram_available_mb": 100183.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 25588.8, "ram_available_mb": 100183.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.81, "peak": 15.11, "min": 14.22}, "VDD_GPU": {"avg": 30.45, "peak": 38.21, "min": 24.43}, "VIN": {"avg": 71.27, "peak": 110.32, "min": 55.86}, "VDD_CPU_SOC_MSS": {"avg": 15.36, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 30.45, "energy_joules_est": 56.9, "sample_count": 14, "duration_seconds": 1.869}, "timestamp": "2026-01-16T15:23:51.407426"}
{"image_index": 77, "image_name": "000000007795.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007795.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2300.341, "latencies_ms": [2300.341], "images_per_second": 0.435, "prompt_tokens": 27, "response_tokens_est": 62, "n_tiles": 6, "output_text": "The main objects in the image are a bed and a nightstand. The bed is positioned in the foreground, with its headboard and pillows visible. The nightstand is located to the left of the bed, near the window. The window is in the background, providing natural light to the room.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25588.8, "ram_available_mb": 100183.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 25589.5, "ram_available_mb": 100182.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.86, "peak": 15.11, "min": 14.22}, "VDD_GPU": {"avg": 29.43, "peak": 38.21, "min": 23.64}, "VIN": {"avg": 72.63, "peak": 115.49, "min": 60.72}, "VDD_CPU_SOC_MSS": {"avg": 15.45, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 29.43, "energy_joules_est": 67.71, "sample_count": 17, "duration_seconds": 2.301}, "timestamp": "2026-01-16T15:23:53.717792"}
{"image_index": 77, "image_name": "000000007795.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007795.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1892.921, "latencies_ms": [1892.921], "images_per_second": 0.528, "prompt_tokens": 21, "response_tokens_est": 47, "n_tiles": 6, "output_text": "The image depicts a well-lit, modern bedroom with a large window that offers a view of trees outside. The room is furnished with a bed, a nightstand, and a lamp, creating a cozy and inviting atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25589.5, "ram_available_mb": 100182.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 25589.5, "ram_available_mb": 100182.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.77, "peak": 15.11, "min": 14.22}, "VDD_GPU": {"avg": 30.39, "peak": 37.82, "min": 24.42}, "VIN": {"avg": 67.0, "peak": 86.26, "min": 59.75}, "VDD_CPU_SOC_MSS": {"avg": 15.38, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 30.39, "energy_joules_est": 57.54, "sample_count": 14, "duration_seconds": 1.893}, "timestamp": "2026-01-16T15:23:55.617252"}
{"image_index": 77, "image_name": "000000007795.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007795.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1835.751, "latencies_ms": [1835.751], "images_per_second": 0.545, "prompt_tokens": 19, "response_tokens_est": 45, "n_tiles": 6, "output_text": "The room features a warm, yellowish-green wall color, complemented by a wooden ceiling with a metal grid. The lighting is soft and ambient, with a warm glow from a bedside lamp casting a cozy atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25589.5, "ram_available_mb": 100182.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 25589.8, "ram_available_mb": 100182.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.81, "peak": 15.11, "min": 14.22}, "VDD_GPU": {"avg": 30.58, "peak": 38.21, "min": 24.42}, "VIN": {"avg": 73.2, "peak": 110.41, "min": 62.58}, "VDD_CPU_SOC_MSS": {"avg": 15.44, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 30.58, "energy_joules_est": 56.15, "sample_count": 14, "duration_seconds": 1.836}, "timestamp": "2026-01-16T15:23:57.459024"}
{"image_index": 78, "image_name": "000000007816.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007816.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1687.031, "latencies_ms": [1687.031], "images_per_second": 0.593, "prompt_tokens": 9, "response_tokens_est": 41, "n_tiles": 6, "output_text": "A motorcyclist wearing a white helmet and a white and green jacket is riding a white motorcycle with black accents, while a group of people stands behind a fence on the side of a road.", "error": null, "sys_before": {"cpu_percent": 35.1, "ram_used_mb": 25589.8, "ram_available_mb": 100182.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 25589.8, "ram_available_mb": 100182.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.57, "peak": 15.11, "min": 13.92}, "VDD_GPU": {"avg": 30.27, "peak": 37.82, "min": 24.42}, "VIN": {"avg": 69.44, "peak": 83.44, "min": 62.09}, "VDD_CPU_SOC_MSS": {"avg": 15.29, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 30.27, "energy_joules_est": 51.08, "sample_count": 13, "duration_seconds": 1.687}, "timestamp": "2026-01-16T15:23:59.192380"}
{"image_index": 78, "image_name": "000000007816.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007816.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1693.133, "latencies_ms": [1693.133], "images_per_second": 0.591, "prompt_tokens": 23, "response_tokens_est": 38, "n_tiles": 6, "output_text": "1. Motorcycle\n2. Rider\n3. Helmet\n4. Gloves\n5. Motorcycle\n6. Rider\n7. Helmet\n8. Gloves", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25589.8, "ram_available_mb": 100182.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25589.8, "ram_available_mb": 100182.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.67, "peak": 14.91, "min": 14.22}, "VDD_GPU": {"avg": 30.85, "peak": 38.21, "min": 24.04}, "VIN": {"avg": 74.64, "peak": 113.42, "min": 53.78}, "VDD_CPU_SOC_MSS": {"avg": 15.32, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 30.85, "energy_joules_est": 52.24, "sample_count": 13, "duration_seconds": 1.693}, "timestamp": "2026-01-16T15:24:00.891670"}
{"image_index": 78, "image_name": "000000007816.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007816.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2425.662, "latencies_ms": [2425.662], "images_per_second": 0.412, "prompt_tokens": 27, "response_tokens_est": 64, "n_tiles": 6, "output_text": "The main object in the foreground is a white motorcycle with black accents, positioned on the right side of the road. The background features a group of people standing behind a wooden fence, with a person lying on the grass to the left. The road curves to the left, and the fence runs parallel to the road.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25589.8, "ram_available_mb": 100182.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 25590.8, "ram_available_mb": 100181.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.66, "peak": 14.91, "min": 14.02}, "VDD_GPU": {"avg": 28.74, "peak": 38.21, "min": 23.25}, "VIN": {"avg": 69.99, "peak": 116.75, "min": 49.14}, "VDD_CPU_SOC_MSS": {"avg": 15.22, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 28.74, "energy_joules_est": 69.72, "sample_count": 18, "duration_seconds": 2.426}, "timestamp": "2026-01-16T15:24:03.324538"}
{"image_index": 78, "image_name": "000000007816.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007816.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2873.335, "latencies_ms": [2873.335], "images_per_second": 0.348, "prompt_tokens": 21, "response_tokens_est": 80, "n_tiles": 6, "output_text": "The image captures a dynamic scene on a rural road, where a motorcyclist is riding a white and green motorcycle. The rider is wearing a white helmet and a white jacket with green accents, and is in motion, leaning into a turn. In the background, there are several people standing and watching the rider, and the road is bordered by a wooden fence and lush green grass.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25590.8, "ram_available_mb": 100181.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 25590.8, "ram_available_mb": 100181.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.72, "peak": 15.01, "min": 14.02}, "VDD_GPU": {"avg": 27.5, "peak": 37.82, "min": 22.86}, "VIN": {"avg": 64.52, "peak": 73.5, "min": 55.87}, "VDD_CPU_SOC_MSS": {"avg": 15.43, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 27.5, "energy_joules_est": 79.02, "sample_count": 22, "duration_seconds": 2.874}, "timestamp": "2026-01-16T15:24:06.204095"}
{"image_index": 78, "image_name": "000000007816.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007816.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1828.873, "latencies_ms": [1828.873], "images_per_second": 0.547, "prompt_tokens": 19, "response_tokens_est": 43, "n_tiles": 6, "output_text": "The motorcycle in the image is predominantly white with black accents, featuring a sleek design and a visible logo on the side. The lighting is natural, suggesting daytime, and the weather appears to be clear and sunny.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25590.8, "ram_available_mb": 100181.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25590.8, "ram_available_mb": 100181.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.6, "peak": 14.91, "min": 14.02}, "VDD_GPU": {"avg": 30.14, "peak": 38.21, "min": 23.64}, "VIN": {"avg": 70.87, "peak": 106.67, "min": 56.84}, "VDD_CPU_SOC_MSS": {"avg": 15.16, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 30.14, "energy_joules_est": 55.13, "sample_count": 14, "duration_seconds": 1.829}, "timestamp": "2026-01-16T15:24:08.038948"}
{"image_index": 79, "image_name": "000000007818.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007818.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1637.242, "latencies_ms": [1637.242], "images_per_second": 0.611, "prompt_tokens": 9, "response_tokens_est": 39, "n_tiles": 6, "output_text": "The image shows a table set for a dinner party with a white tablecloth, elegant glassware, and a vase with white flowers, all illuminated by soft, warm lighting.", "error": null, "sys_before": {"cpu_percent": 42.6, "ram_used_mb": 25590.8, "ram_available_mb": 100181.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25589.8, "ram_available_mb": 100182.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.45, "peak": 15.01, "min": 13.82}, "VDD_GPU": {"avg": 30.66, "peak": 37.03, "min": 24.82}, "VIN": {"avg": 74.29, "peak": 98.6, "min": 51.88}, "VDD_CPU_SOC_MSS": {"avg": 15.09, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 30.66, "energy_joules_est": 50.21, "sample_count": 12, "duration_seconds": 1.638}, "timestamp": "2026-01-16T15:24:09.728215"}
{"image_index": 79, "image_name": "000000007818.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007818.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1571.088, "latencies_ms": [1571.088], "images_per_second": 0.637, "prompt_tokens": 23, "response_tokens_est": 35, "n_tiles": 6, "output_text": "table: 1\ntablecloth: 1\nglass: 1\nvase: 1\nflowers: 1\ntableware: 1", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25589.8, "ram_available_mb": 100182.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 25590.0, "ram_available_mb": 100182.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.74, "peak": 15.01, "min": 14.22}, "VDD_GPU": {"avg": 31.64, "peak": 38.21, "min": 25.21}, "VIN": {"avg": 69.89, "peak": 109.34, "min": 55.03}, "VDD_CPU_SOC_MSS": {"avg": 15.32, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 31.64, "energy_joules_est": 49.72, "sample_count": 12, "duration_seconds": 1.571}, "timestamp": "2026-01-16T15:24:11.305303"}
{"image_index": 79, "image_name": "000000007818.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007818.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2463.553, "latencies_ms": [2463.553], "images_per_second": 0.406, "prompt_tokens": 27, "response_tokens_est": 68, "n_tiles": 6, "output_text": "The main objects in the image are a table and a vase with flowers. The table is in the foreground, with a white tablecloth covering it. The vase with flowers is placed near the center of the table, slightly to the left. In the background, there are other tables and chairs, suggesting a dining area.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25590.0, "ram_available_mb": 100182.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 25590.3, "ram_available_mb": 100181.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.88, "peak": 15.22, "min": 14.22}, "VDD_GPU": {"avg": 28.82, "peak": 38.21, "min": 23.64}, "VIN": {"avg": 64.61, "peak": 79.23, "min": 55.72}, "VDD_CPU_SOC_MSS": {"avg": 15.52, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 28.82, "energy_joules_est": 71.01, "sample_count": 19, "duration_seconds": 2.464}, "timestamp": "2026-01-16T15:24:13.775007"}
{"image_index": 79, "image_name": "000000007818.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007818.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2476.546, "latencies_ms": [2476.546], "images_per_second": 0.404, "prompt_tokens": 21, "response_tokens_est": 69, "n_tiles": 6, "output_text": "The image depicts a dimly lit dining area with a table set for a meal. The table is adorned with white tablecloths, and there are several clear glassware pieces, including wine glasses and stemware, placed on the table. The setting suggests a formal dining experience, possibly in a restaurant or a private event space.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25590.3, "ram_available_mb": 100181.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 25590.8, "ram_available_mb": 100181.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.92, "peak": 15.32, "min": 14.22}, "VDD_GPU": {"avg": 28.74, "peak": 37.82, "min": 23.64}, "VIN": {"avg": 73.93, "peak": 118.85, "min": 62.54}, "VDD_CPU_SOC_MSS": {"avg": 15.48, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 28.74, "energy_joules_est": 71.18, "sample_count": 19, "duration_seconds": 2.477}, "timestamp": "2026-01-16T15:24:16.259456"}
{"image_index": 79, "image_name": "000000007818.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007818.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1956.679, "latencies_ms": [1956.679], "images_per_second": 0.511, "prompt_tokens": 19, "response_tokens_est": 50, "n_tiles": 6, "output_text": "The image depicts a dimly lit dining table with a white tablecloth, illuminated by soft, warm lighting. The table is adorned with elegant glassware, including a clear vase with white flowers and a smaller glass bowl.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25590.8, "ram_available_mb": 100181.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 25590.7, "ram_available_mb": 100181.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.88, "peak": 15.22, "min": 14.22}, "VDD_GPU": {"avg": 30.07, "peak": 37.82, "min": 24.03}, "VIN": {"avg": 69.46, "peak": 106.53, "min": 59.13}, "VDD_CPU_SOC_MSS": {"avg": 15.54, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 30.07, "energy_joules_est": 58.85, "sample_count": 15, "duration_seconds": 1.957}, "timestamp": "2026-01-16T15:24:18.222322"}
{"image_index": 80, "image_name": "000000007888.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007888.jpg", "image_width": 638, "image_height": 640, "image_resolution": "638x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2531.209, "latencies_ms": [2531.209], "images_per_second": 0.395, "prompt_tokens": 9, "response_tokens_est": 83, "n_tiles": 4, "output_text": "The image features a black and white photograph of a classic street clock mounted on a pole, with its face displaying the time at approximately 10:10. The clock has a traditional design with Roman numerals for the hours and a smaller hand indicating the minutes. The background is a blurred natural landscape, possibly a field or grassy area, which adds a serene and timeless ambiance to the scene.", "error": null, "sys_before": {"cpu_percent": 35.4, "ram_used_mb": 25590.7, "ram_available_mb": 100181.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 25590.5, "ram_available_mb": 100181.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4606.7, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5444.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.77, "peak": 15.22, "min": 13.61}, "VDD_GPU": {"avg": 25.26, "peak": 34.27, "min": 21.67}, "VIN": {"avg": 64.63, "peak": 94.36, "min": 58.04}, "VDD_CPU_SOC_MSS": {"avg": 15.42, "peak": 15.75, "min": 14.17}}, "power_watts_avg": 25.26, "energy_joules_est": 63.95, "sample_count": 19, "duration_seconds": 2.531}, "timestamp": "2026-01-16T15:24:20.795970"}
{"image_index": 80, "image_name": "000000007888.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007888.jpg", "image_width": 638, "image_height": 640, "image_resolution": "638x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1281.979, "latencies_ms": [1281.979], "images_per_second": 0.78, "prompt_tokens": 23, "response_tokens_est": 35, "n_tiles": 4, "output_text": "1. Clock\n2. Clock face\n3. Clock tower\n4. Grass\n5. Sky\n6. Street light\n7. Person\n8. Bird", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25590.5, "ram_available_mb": 100181.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 25590.5, "ram_available_mb": 100181.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4606.7, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5455.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.54, "peak": 15.01, "min": 13.9}, "VDD_GPU": {"avg": 28.68, "peak": 34.27, "min": 23.64}, "VIN": {"avg": 63.3, "peak": 74.28, "min": 52.57}, "VDD_CPU_SOC_MSS": {"avg": 15.18, "peak": 15.75, "min": 14.57}}, "power_watts_avg": 28.68, "energy_joules_est": 36.78, "sample_count": 9, "duration_seconds": 1.282}, "timestamp": "2026-01-16T15:24:22.088184"}
{"image_index": 80, "image_name": "000000007888.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007888.jpg", "image_width": 638, "image_height": 640, "image_resolution": "638x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1748.969, "latencies_ms": [1748.969], "images_per_second": 0.572, "prompt_tokens": 27, "response_tokens_est": 53, "n_tiles": 4, "output_text": "The main object in the image is a clock, which is positioned in the foreground. The clock is mounted on a pole, which is also in the foreground. The background features a blurred natural landscape, suggesting that the clock is placed in an outdoor setting.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25590.5, "ram_available_mb": 100181.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 25589.3, "ram_available_mb": 100182.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4606.7, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5458.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.7, "peak": 15.22, "min": 13.92}, "VDD_GPU": {"avg": 27.15, "peak": 35.06, "min": 22.46}, "VIN": {"avg": 67.93, "peak": 100.78, "min": 61.58}, "VDD_CPU_SOC_MSS": {"avg": 15.36, "peak": 15.75, "min": 14.57}}, "power_watts_avg": 27.15, "energy_joules_est": 47.5, "sample_count": 13, "duration_seconds": 1.749}, "timestamp": "2026-01-16T15:24:23.843182"}
{"image_index": 80, "image_name": "000000007888.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007888.jpg", "image_width": 638, "image_height": 640, "image_resolution": "638x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1925.026, "latencies_ms": [1925.026], "images_per_second": 0.519, "prompt_tokens": 21, "response_tokens_est": 60, "n_tiles": 4, "output_text": "The image depicts a black and white photograph of a classic street clock mounted on a pole. The clock face is visible, showing the time as approximately 10:10. The background is blurred, but it appears to be an outdoor setting with a grassy field and a clear sky.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25589.3, "ram_available_mb": 100182.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 25589.2, "ram_available_mb": 100183.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4606.7, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5453.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.79, "peak": 15.22, "min": 14.02}, "VDD_GPU": {"avg": 26.47, "peak": 35.06, "min": 22.06}, "VIN": {"avg": 67.62, "peak": 107.54, "min": 55.38}, "VDD_CPU_SOC_MSS": {"avg": 15.41, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 26.47, "energy_joules_est": 50.96, "sample_count": 15, "duration_seconds": 1.925}, "timestamp": "2026-01-16T15:24:25.774377"}
{"image_index": 80, "image_name": "000000007888.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007888.jpg", "image_width": 638, "image_height": 640, "image_resolution": "638x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2261.329, "latencies_ms": [2261.329], "images_per_second": 0.442, "prompt_tokens": 19, "response_tokens_est": 73, "n_tiles": 4, "output_text": "The image features a black and white photograph of a classic street clock, which is prominently displayed against a blurred background of a grassy field. The clock has a traditional design with Roman numerals and a decorative border, and it is mounted on a pole. The lighting in the photograph is soft and diffused, creating a nostalgic and timeless atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25589.2, "ram_available_mb": 100183.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 25589.4, "ram_available_mb": 100182.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4606.7, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5451.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.83, "peak": 15.22, "min": 13.92}, "VDD_GPU": {"avg": 25.93, "peak": 35.06, "min": 22.07}, "VIN": {"avg": 67.45, "peak": 100.89, "min": 56.58}, "VDD_CPU_SOC_MSS": {"avg": 15.49, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 25.93, "energy_joules_est": 58.65, "sample_count": 17, "duration_seconds": 2.262}, "timestamp": "2026-01-16T15:24:28.042070"}
{"image_index": 81, "image_name": "000000007977.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007977.jpg", "image_width": 429, "image_height": 640, "image_resolution": "429x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1280.09, "latencies_ms": [1280.09], "images_per_second": 0.781, "prompt_tokens": 9, "response_tokens_est": 26, "n_tiles": 6, "output_text": "A young man is performing a trick on a skateboard in an outdoor setting with trees and a clear sky in the background.", "error": null, "sys_before": {"cpu_percent": 34.0, "ram_used_mb": 25589.2, "ram_available_mb": 100183.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 25589.2, "ram_available_mb": 100183.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.43, "peak": 14.81, "min": 13.92}, "VDD_GPU": {"avg": 31.95, "peak": 36.63, "min": 26.79}, "VIN": {"avg": 67.11, "peak": 77.68, "min": 56.58}, "VDD_CPU_SOC_MSS": {"avg": 15.18, "peak": 15.36, "min": 14.57}}, "power_watts_avg": 31.95, "energy_joules_est": 40.91, "sample_count": 9, "duration_seconds": 1.28}, "timestamp": "2026-01-16T15:24:29.368349"}
{"image_index": 81, "image_name": "000000007977.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007977.jpg", "image_width": 429, "image_height": 640, "image_resolution": "429x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1889.059, "latencies_ms": [1889.059], "images_per_second": 0.529, "prompt_tokens": 23, "response_tokens_est": 47, "n_tiles": 6, "output_text": "1. Skateboard\n2. Person\n3. Skateboarder\n4. Skateboard\n5. Skateboard\n6. Skateboard\n7. Skateboard\n8. Skateboard", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25589.2, "ram_available_mb": 100183.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 25589.4, "ram_available_mb": 100182.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.81, "peak": 15.22, "min": 14.12}, "VDD_GPU": {"avg": 30.76, "peak": 38.6, "min": 24.42}, "VIN": {"avg": 72.15, "peak": 112.8, "min": 55.29}, "VDD_CPU_SOC_MSS": {"avg": 15.47, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 30.76, "energy_joules_est": 58.12, "sample_count": 14, "duration_seconds": 1.89}, "timestamp": "2026-01-16T15:24:31.264230"}
{"image_index": 81, "image_name": "000000007977.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007977.jpg", "image_width": 429, "image_height": 640, "image_resolution": "429x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2659.25, "latencies_ms": [2659.25], "images_per_second": 0.376, "prompt_tokens": 27, "response_tokens_est": 76, "n_tiles": 6, "output_text": "The main object in the foreground is a young man skateboarding. He is positioned slightly to the left and in the foreground of the image. The background features a tree and a few other people, indicating that the skateboarder is in an outdoor setting. The skateboard itself is located near the bottom center of the image, slightly to the left of the skateboarder.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25589.7, "ram_available_mb": 100182.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 25591.3, "ram_available_mb": 100180.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.95, "peak": 15.32, "min": 14.22}, "VDD_GPU": {"avg": 28.56, "peak": 38.21, "min": 23.64}, "VIN": {"avg": 67.08, "peak": 92.78, "min": 56.89}, "VDD_CPU_SOC_MSS": {"avg": 15.53, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 28.56, "energy_joules_est": 75.96, "sample_count": 20, "duration_seconds": 2.66}, "timestamp": "2026-01-16T15:24:33.930435"}
{"image_index": 81, "image_name": "000000007977.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007977.jpg", "image_width": 429, "image_height": 640, "image_resolution": "429x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2444.967, "latencies_ms": [2444.967], "images_per_second": 0.409, "prompt_tokens": 21, "response_tokens_est": 68, "n_tiles": 6, "output_text": "The image depicts a young man skateboarding in an outdoor urban setting. He is captured mid-action, performing a trick on a skateboard. The skateboarder is wearing a dark-colored t-shirt, light-colored pants, and sneakers, and is positioned on a concrete surface with a tree and some greenery in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25591.3, "ram_available_mb": 100180.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 25591.2, "ram_available_mb": 100181.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.94, "peak": 15.22, "min": 14.22}, "VDD_GPU": {"avg": 28.84, "peak": 38.21, "min": 23.64}, "VIN": {"avg": 73.22, "peak": 110.32, "min": 61.09}, "VDD_CPU_SOC_MSS": {"avg": 15.44, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 28.84, "energy_joules_est": 70.52, "sample_count": 19, "duration_seconds": 2.445}, "timestamp": "2026-01-16T15:24:36.381287"}
{"image_index": 81, "image_name": "000000007977.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007977.jpg", "image_width": 429, "image_height": 640, "image_resolution": "429x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2579.568, "latencies_ms": [2579.568], "images_per_second": 0.388, "prompt_tokens": 19, "response_tokens_est": 73, "n_tiles": 6, "output_text": "The image depicts a young man skateboarding in an outdoor setting. The skateboarder is wearing a dark-colored t-shirt, light-colored pants, and a baseball cap. The scene is well-lit, with natural daylight illuminating the surroundings. The ground appears to be made of concrete, and there are trees and a clear sky visible in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25591.2, "ram_available_mb": 100181.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 25591.5, "ram_available_mb": 100180.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.94, "peak": 15.22, "min": 14.22}, "VDD_GPU": {"avg": 28.44, "peak": 37.82, "min": 23.64}, "VIN": {"avg": 70.48, "peak": 120.0, "min": 56.65}, "VDD_CPU_SOC_MSS": {"avg": 15.47, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 28.44, "energy_joules_est": 73.38, "sample_count": 20, "duration_seconds": 2.58}, "timestamp": "2026-01-16T15:24:38.966942"}
{"image_index": 82, "image_name": "000000007991.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007991.jpg", "image_width": 640, "image_height": 359, "image_resolution": "640x359", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 808.544, "latencies_ms": [808.544], "images_per_second": 1.237, "prompt_tokens": 9, "response_tokens_est": 28, "n_tiles": 2, "output_text": "The image shows a white plate filled with orange carrots, green peas, and purple beets, all arranged on a kitchen countertop.", "error": null, "sys_before": {"cpu_percent": 25.8, "ram_used_mb": 25591.2, "ram_available_mb": 100181.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25591.2, "ram_available_mb": 100181.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5055.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.67, "peak": 15.22, "min": 14.1}, "VDD_GPU": {"avg": 26.98, "peak": 32.3, "min": 23.24}, "VIN": {"avg": 70.69, "peak": 108.42, "min": 61.52}, "VDD_CPU_SOC_MSS": {"avg": 15.29, "peak": 15.75, "min": 14.57}}, "power_watts_avg": 26.98, "energy_joules_est": 21.83, "sample_count": 6, "duration_seconds": 0.809}, "timestamp": "2026-01-16T15:24:39.804496"}
{"image_index": 82, "image_name": "000000007991.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007991.jpg", "image_width": 640, "image_height": 359, "image_resolution": "640x359", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 696.689, "latencies_ms": [696.689], "images_per_second": 1.435, "prompt_tokens": 23, "response_tokens_est": 23, "n_tiles": 2, "output_text": "carrot: 8\npea: 1\npotato: 1\nknife: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25591.2, "ram_available_mb": 100181.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25591.2, "ram_available_mb": 100181.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5064.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.87, "peak": 15.22, "min": 14.61}, "VDD_GPU": {"avg": 28.12, "peak": 32.3, "min": 24.42}, "VIN": {"avg": 72.66, "peak": 98.26, "min": 63.16}, "VDD_CPU_SOC_MSS": {"avg": 15.43, "peak": 15.75, "min": 15.36}}, "power_watts_avg": 28.12, "energy_joules_est": 19.6, "sample_count": 5, "duration_seconds": 0.697}, "timestamp": "2026-01-16T15:24:40.506867"}
{"image_index": 82, "image_name": "000000007991.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007991.jpg", "image_width": 640, "image_height": 359, "image_resolution": "640x359", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1962.098, "latencies_ms": [1962.098], "images_per_second": 0.51, "prompt_tokens": 27, "response_tokens_est": 78, "n_tiles": 2, "output_text": "The main objects in the image are a white plate filled with orange carrots, a blue knife, and a bunch of green beans. The plate is placed on a countertop, with the green beans and the knife positioned near the plate. The background includes a white container and a white cup, while the foreground features a bunch of green beans and a bunch of purple and red beets.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25591.2, "ram_available_mb": 100181.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25590.7, "ram_available_mb": 100181.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5067.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.17, "peak": 15.52, "min": 14.51}, "VDD_GPU": {"avg": 23.98, "peak": 33.09, "min": 20.88}, "VIN": {"avg": 66.8, "peak": 107.01, "min": 59.26}, "VDD_CPU_SOC_MSS": {"avg": 16.01, "peak": 16.54, "min": 14.96}}, "power_watts_avg": 23.98, "energy_joules_est": 47.07, "sample_count": 15, "duration_seconds": 1.963}, "timestamp": "2026-01-16T15:24:42.475294"}
{"image_index": 82, "image_name": "000000007991.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007991.jpg", "image_width": 640, "image_height": 359, "image_resolution": "640x359", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1919.683, "latencies_ms": [1919.683], "images_per_second": 0.521, "prompt_tokens": 21, "response_tokens_est": 77, "n_tiles": 2, "output_text": "The image depicts a kitchen countertop with a white plate containing a mix of cooked carrots and green beans. To the right of the plate, there is a blue knife and a bunch of fresh vegetables, including a bunch of purple beets and some green leafy vegetables. The setting appears to be a kitchen, and the focus is on the food items on the countertop.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25590.7, "ram_available_mb": 100181.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25591.0, "ram_available_mb": 100181.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5063.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.16, "peak": 15.72, "min": 14.51}, "VDD_GPU": {"avg": 23.59, "peak": 31.53, "min": 20.88}, "VIN": {"avg": 63.83, "peak": 75.69, "min": 56.61}, "VDD_CPU_SOC_MSS": {"avg": 15.97, "peak": 16.14, "min": 15.36}}, "power_watts_avg": 23.59, "energy_joules_est": 45.3, "sample_count": 14, "duration_seconds": 1.92}, "timestamp": "2026-01-16T15:24:44.405836"}
{"image_index": 82, "image_name": "000000007991.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000007991.jpg", "image_width": 640, "image_height": 359, "image_resolution": "640x359", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1629.327, "latencies_ms": [1629.327], "images_per_second": 0.614, "prompt_tokens": 19, "response_tokens_est": 64, "n_tiles": 2, "output_text": "The image features a white plate filled with orange carrots, green peas, and purple beets. The carrots are brightly colored, contrasting with the muted tones of the plate and the surrounding kitchen items. The lighting is soft and natural, casting gentle shadows and highlighting the textures of the vegetables and the countertop.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25591.0, "ram_available_mb": 100181.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25590.7, "ram_available_mb": 100181.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5062.6, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.33, "peak": 15.72, "min": 14.61}, "VDD_GPU": {"avg": 24.03, "peak": 31.12, "min": 20.89}, "VIN": {"avg": 66.68, "peak": 101.7, "min": 61.68}, "VDD_CPU_SOC_MSS": {"avg": 15.72, "peak": 16.14, "min": 14.96}}, "power_watts_avg": 24.03, "energy_joules_est": 39.16, "sample_count": 12, "duration_seconds": 1.63}, "timestamp": "2026-01-16T15:24:46.041002"}
{"image_index": 83, "image_name": "000000008021.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008021.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2548.398, "latencies_ms": [2548.398], "images_per_second": 0.392, "prompt_tokens": 9, "response_tokens_est": 34, "n_tiles": 12, "output_text": "The image shows a man standing on a stage with a large screen behind him displaying a person in a suit, and a woman in the foreground looking at the screen.", "error": null, "sys_before": {"cpu_percent": 47.1, "ram_used_mb": 25590.7, "ram_available_mb": 100181.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 25590.9, "ram_available_mb": 100181.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.86, "peak": 15.85, "min": 14.0}, "VDD_GPU": {"avg": 34.21, "peak": 40.57, "min": 26.79}, "VIN": {"avg": 76.15, "peak": 118.26, "min": 60.95}, "VDD_CPU_SOC_MSS": {"avg": 15.64, "peak": 16.92, "min": 14.96}}, "power_watts_avg": 34.21, "energy_joules_est": 87.19, "sample_count": 19, "duration_seconds": 2.549}, "timestamp": "2026-01-16T15:24:48.678420"}
{"image_index": 83, "image_name": "000000008021.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008021.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2679.894, "latencies_ms": [2679.894], "images_per_second": 0.373, "prompt_tokens": 23, "response_tokens_est": 34, "n_tiles": 12, "output_text": "1. Man\n2. Suits\n3. Background\n4. Speaker\n5. Table\n6. Water bottles\n7. Table\n8. Speaker", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25590.9, "ram_available_mb": 100181.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 25590.9, "ram_available_mb": 100181.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.71, "peak": 15.34, "min": 14.02}, "VDD_GPU": {"avg": 35.23, "peak": 42.94, "min": 26.79}, "VIN": {"avg": 72.04, "peak": 117.21, "min": 54.78}, "VDD_CPU_SOC_MSS": {"avg": 15.28, "peak": 16.14, "min": 14.56}}, "power_watts_avg": 35.23, "energy_joules_est": 94.42, "sample_count": 21, "duration_seconds": 2.68}, "timestamp": "2026-01-16T15:24:51.365813"}
{"image_index": 83, "image_name": "000000008021.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008021.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3574.819, "latencies_ms": [3574.819], "images_per_second": 0.28, "prompt_tokens": 27, "response_tokens_est": 60, "n_tiles": 12, "output_text": "The main object in the foreground is a person with red hair, who is facing away from the camera. The main object in the background is a large screen displaying a man in a suit. The screen is positioned behind the person with red hair, creating a clear spatial relationship between the two objects.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25590.9, "ram_available_mb": 100181.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25590.6, "ram_available_mb": 100181.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.71, "peak": 15.24, "min": 14.12}, "VDD_GPU": {"avg": 32.85, "peak": 42.94, "min": 25.61}, "VIN": {"avg": 74.55, "peak": 118.05, "min": 58.27}, "VDD_CPU_SOC_MSS": {"avg": 15.18, "peak": 16.14, "min": 14.56}}, "power_watts_avg": 32.85, "energy_joules_est": 117.45, "sample_count": 28, "duration_seconds": 3.575}, "timestamp": "2026-01-16T15:24:54.947118"}
{"image_index": 83, "image_name": "000000008021.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008021.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3093.916, "latencies_ms": [3093.916], "images_per_second": 0.323, "prompt_tokens": 21, "response_tokens_est": 46, "n_tiles": 12, "output_text": "The image depicts a man standing on a stage, delivering a speech or presentation. The background features a large screen displaying a projected image of the same man, suggesting that the presentation is being delivered in front of an audience.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25590.6, "ram_available_mb": 100181.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 25590.6, "ram_available_mb": 100181.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.72, "peak": 15.24, "min": 14.12}, "VDD_GPU": {"avg": 34.05, "peak": 42.15, "min": 26.01}, "VIN": {"avg": 75.36, "peak": 122.28, "min": 58.78}, "VDD_CPU_SOC_MSS": {"avg": 15.25, "peak": 16.14, "min": 14.56}}, "power_watts_avg": 34.05, "energy_joules_est": 105.36, "sample_count": 23, "duration_seconds": 3.094}, "timestamp": "2026-01-16T15:24:58.051421"}
{"image_index": 83, "image_name": "000000008021.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008021.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3402.435, "latencies_ms": [3402.435], "images_per_second": 0.294, "prompt_tokens": 19, "response_tokens_est": 55, "n_tiles": 12, "output_text": "The image features a man standing on a stage with a large screen behind him displaying a colorful geometric pattern. The lighting is warm, casting a soft glow on the scene, and the colors are vibrant, with a mix of blues, oranges, and purples.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25590.6, "ram_available_mb": 100181.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25590.4, "ram_available_mb": 100181.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.7, "peak": 15.24, "min": 14.12}, "VDD_GPU": {"avg": 33.11, "peak": 42.54, "min": 26.0}, "VIN": {"avg": 75.65, "peak": 120.86, "min": 61.63}, "VDD_CPU_SOC_MSS": {"avg": 15.22, "peak": 16.14, "min": 14.56}}, "power_watts_avg": 33.11, "energy_joules_est": 112.67, "sample_count": 26, "duration_seconds": 3.403}, "timestamp": "2026-01-16T15:25:01.460598"}
{"image_index": 84, "image_name": "000000008211.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008211.jpg", "image_width": 640, "image_height": 459, "image_resolution": "640x459", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1921.961, "latencies_ms": [1921.961], "images_per_second": 0.52, "prompt_tokens": 9, "response_tokens_est": 15, "n_tiles": 12, "output_text": "Two workers are conversing near a scooter parked on the sidewalk.", "error": null, "sys_before": {"cpu_percent": 41.5, "ram_used_mb": 25590.4, "ram_available_mb": 100181.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 2.1, "ram_used_mb": 25590.4, "ram_available_mb": 100181.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.01, "peak": 16.05, "min": 13.49}, "VDD_GPU": {"avg": 36.94, "peak": 40.97, "min": 30.33}, "VIN": {"avg": 71.22, "peak": 104.69, "min": 53.09}, "VDD_CPU_SOC_MSS": {"avg": 15.88, "peak": 17.32, "min": 14.57}}, "power_watts_avg": 36.94, "energy_joules_est": 71.01, "sample_count": 14, "duration_seconds": 1.922}, "timestamp": "2026-01-16T15:25:03.481258"}
{"image_index": 84, "image_name": "000000008211.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008211.jpg", "image_width": 640, "image_height": 459, "image_resolution": "640x459", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2882.504, "latencies_ms": [2882.504], "images_per_second": 0.347, "prompt_tokens": 23, "response_tokens_est": 40, "n_tiles": 12, "output_text": "object: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25590.4, "ram_available_mb": 100181.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 25591.0, "ram_available_mb": 100181.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.02, "peak": 15.54, "min": 14.43}, "VDD_GPU": {"avg": 35.31, "peak": 43.73, "min": 26.79}, "VIN": {"avg": 73.23, "peak": 117.03, "min": 56.26}, "VDD_CPU_SOC_MSS": {"avg": 15.66, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 35.31, "energy_joules_est": 101.79, "sample_count": 22, "duration_seconds": 2.883}, "timestamp": "2026-01-16T15:25:06.371864"}
{"image_index": 84, "image_name": "000000008211.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008211.jpg", "image_width": 640, "image_height": 459, "image_resolution": "640x459", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3658.215, "latencies_ms": [3658.215], "images_per_second": 0.273, "prompt_tokens": 27, "response_tokens_est": 63, "n_tiles": 12, "output_text": "The main objects in the image are a parked scooter and a woman standing near it. The scooter is positioned in the foreground, while the woman is standing in the background. The scooter is near the curb, and the woman is standing close to the scooter, indicating a spatial relationship between them.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25591.0, "ram_available_mb": 100181.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25591.0, "ram_available_mb": 100181.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.05, "peak": 15.65, "min": 14.43}, "VDD_GPU": {"avg": 32.89, "peak": 42.54, "min": 26.0}, "VIN": {"avg": 72.61, "peak": 115.49, "min": 58.27}, "VDD_CPU_SOC_MSS": {"avg": 15.58, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 32.89, "energy_joules_est": 120.33, "sample_count": 28, "duration_seconds": 3.659}, "timestamp": "2026-01-16T15:25:10.036644"}
{"image_index": 84, "image_name": "000000008211.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008211.jpg", "image_width": 640, "image_height": 459, "image_resolution": "640x459", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 4513.14, "latencies_ms": [4513.14], "images_per_second": 0.222, "prompt_tokens": 21, "response_tokens_est": 89, "n_tiles": 12, "output_text": "The image depicts a street scene with a focus on a building and a parked scooter. The building has a blue and white sign with Chinese characters, and a blue and white advertisement is visible on the wall. Two individuals, one in a blue uniform and the other in casual clothing, are standing near the scooter, seemingly engaged in a conversation. The setting appears to be an urban area with a mix of commercial and residential buildings.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25591.0, "ram_available_mb": 100181.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 25592.2, "ram_available_mb": 100180.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.09, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 31.65, "peak": 42.54, "min": 26.0}, "VIN": {"avg": 71.85, "peak": 112.96, "min": 57.69}, "VDD_CPU_SOC_MSS": {"avg": 15.71, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 31.65, "energy_joules_est": 142.86, "sample_count": 35, "duration_seconds": 4.514}, "timestamp": "2026-01-16T15:25:14.560927"}
{"image_index": 84, "image_name": "000000008211.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008211.jpg", "image_width": 640, "image_height": 459, "image_resolution": "640x459", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3151.531, "latencies_ms": [3151.531], "images_per_second": 0.317, "prompt_tokens": 19, "response_tokens_est": 48, "n_tiles": 12, "output_text": "The image depicts a scene with a blue and white sign on a building, featuring Chinese characters. The sign is mounted on a gray pillar. The pavement is wet, indicating recent rain. The lighting is natural, suggesting daytime.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25592.2, "ram_available_mb": 100180.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 25591.9, "ram_available_mb": 100180.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.01, "peak": 15.54, "min": 14.43}, "VDD_GPU": {"avg": 34.04, "peak": 42.54, "min": 26.39}, "VIN": {"avg": 74.46, "peak": 119.37, "min": 50.69}, "VDD_CPU_SOC_MSS": {"avg": 15.58, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 34.04, "energy_joules_est": 107.3, "sample_count": 24, "duration_seconds": 3.152}, "timestamp": "2026-01-16T15:25:17.719103"}
{"image_index": 85, "image_name": "000000008277.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008277.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 911.802, "latencies_ms": [911.802], "images_per_second": 1.097, "prompt_tokens": 9, "response_tokens_est": 38, "n_tiles": 1, "output_text": "The image shows a plate of food, which includes a black casserole dish filled with a green and yellow vegetable mixture, and a generous portion of shredded chicken on the side.", "error": null, "sys_before": {"cpu_percent": 28.6, "ram_used_mb": 25591.9, "ram_available_mb": 100180.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 25592.1, "ram_available_mb": 100180.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 4845.7, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.05, "peak": 15.42, "min": 14.51}, "VDD_GPU": {"avg": 23.7, "peak": 27.18, "min": 21.27}, "VIN": {"avg": 62.54, "peak": 64.81, "min": 61.27}, "VDD_CPU_SOC_MSS": {"avg": 15.82, "peak": 16.14, "min": 15.36}}, "power_watts_avg": 23.7, "energy_joules_est": 21.62, "sample_count": 6, "duration_seconds": 0.912}, "timestamp": "2026-01-16T15:25:18.658998"}
{"image_index": 85, "image_name": "000000008277.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008277.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2844.312, "latencies_ms": [2844.312], "images_per_second": 0.352, "prompt_tokens": 23, "response_tokens_est": 128, "n_tiles": 1, "output_text": "object: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject:", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25592.1, "ram_available_mb": 100180.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25590.6, "ram_available_mb": 100181.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 4856.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.54, "peak": 15.62, "min": 15.11}, "VDD_GPU": {"avg": 20.78, "peak": 24.82, "min": 20.1}, "VIN": {"avg": 63.73, "peak": 68.23, "min": 61.32}, "VDD_CPU_SOC_MSS": {"avg": 16.48, "peak": 16.54, "min": 16.14}}, "power_watts_avg": 20.78, "energy_joules_est": 59.11, "sample_count": 22, "duration_seconds": 2.845}, "timestamp": "2026-01-16T15:25:21.509216"}
{"image_index": 85, "image_name": "000000008277.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008277.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1387.733, "latencies_ms": [1387.733], "images_per_second": 0.721, "prompt_tokens": 27, "response_tokens_est": 61, "n_tiles": 1, "output_text": "The main objects in the image are a black rectangular baking dish filled with a green and yellow vegetable stew, and a white plate holding shredded chicken. The dish is positioned in the foreground, while the plate is in the background. The chicken is located on the plate, slightly to the right side.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25590.6, "ram_available_mb": 100181.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25590.9, "ram_available_mb": 100181.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 4859.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.46, "peak": 15.62, "min": 15.11}, "VDD_GPU": {"avg": 21.4, "peak": 24.04, "min": 20.1}, "VIN": {"avg": 63.65, "peak": 66.45, "min": 57.75}, "VDD_CPU_SOC_MSS": {"avg": 16.38, "peak": 16.54, "min": 16.14}}, "power_watts_avg": 21.4, "energy_joules_est": 29.71, "sample_count": 10, "duration_seconds": 1.388}, "timestamp": "2026-01-16T15:25:22.902376"}
{"image_index": 85, "image_name": "000000008277.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008277.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1384.895, "latencies_ms": [1384.895], "images_per_second": 0.722, "prompt_tokens": 21, "response_tokens_est": 61, "n_tiles": 1, "output_text": "The image shows a white plate with a black, square-shaped dish containing a green and yellow vegetable stew, likely a type of soup or stew, and a portion of shredded chicken. The plate is placed on a beige carpeted floor, and there is a silver fork resting on the plate.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25590.9, "ram_available_mb": 100181.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25590.6, "ram_available_mb": 100181.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 4854.7, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.49, "peak": 15.62, "min": 15.22}, "VDD_GPU": {"avg": 21.36, "peak": 24.42, "min": 20.1}, "VIN": {"avg": 63.94, "peak": 67.77, "min": 60.61}, "VDD_CPU_SOC_MSS": {"avg": 16.46, "peak": 16.54, "min": 16.14}}, "power_watts_avg": 21.36, "energy_joules_est": 29.59, "sample_count": 10, "duration_seconds": 1.385}, "timestamp": "2026-01-16T15:25:24.292863"}
{"image_index": 85, "image_name": "000000008277.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008277.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1041.254, "latencies_ms": [1041.254], "images_per_second": 0.96, "prompt_tokens": 19, "response_tokens_est": 45, "n_tiles": 1, "output_text": "The image features a white plate with a black rectangular baking dish containing a green and yellow vegetable stew. The plate is placed on a beige carpeted floor, and the lighting is soft and natural, casting gentle shadows.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25590.6, "ram_available_mb": 100181.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25590.3, "ram_available_mb": 100181.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 4853.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.36, "peak": 15.52, "min": 15.01}, "VDD_GPU": {"avg": 21.79, "peak": 24.04, "min": 20.49}, "VIN": {"avg": 62.66, "peak": 67.55, "min": 55.3}, "VDD_CPU_SOC_MSS": {"avg": 16.42, "peak": 16.54, "min": 16.14}}, "power_watts_avg": 21.79, "energy_joules_est": 22.7, "sample_count": 7, "duration_seconds": 1.042}, "timestamp": "2026-01-16T15:25:25.339955"}
{"image_index": 86, "image_name": "000000008532.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008532.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1522.356, "latencies_ms": [1522.356], "images_per_second": 0.657, "prompt_tokens": 9, "response_tokens_est": 35, "n_tiles": 6, "output_text": "The image features a man wearing a blue shirt, a red tie, and a plaid flat cap, standing outdoors with a building and a clear sky in the background.", "error": null, "sys_before": {"cpu_percent": 47.7, "ram_used_mb": 25590.3, "ram_available_mb": 100181.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 25590.1, "ram_available_mb": 100182.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.58, "peak": 15.01, "min": 14.02}, "VDD_GPU": {"avg": 30.33, "peak": 36.24, "min": 25.22}, "VIN": {"avg": 75.2, "peak": 124.23, "min": 61.86}, "VDD_CPU_SOC_MSS": {"avg": 15.46, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 30.33, "energy_joules_est": 46.18, "sample_count": 11, "duration_seconds": 1.523}, "timestamp": "2026-01-16T15:25:26.913353"}
{"image_index": 86, "image_name": "000000008532.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008532.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1705.957, "latencies_ms": [1705.957], "images_per_second": 0.586, "prompt_tokens": 23, "response_tokens_est": 40, "n_tiles": 6, "output_text": "object: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25590.1, "ram_available_mb": 100182.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 25590.3, "ram_available_mb": 100181.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.74, "peak": 15.11, "min": 14.22}, "VDD_GPU": {"avg": 31.36, "peak": 38.6, "min": 24.82}, "VIN": {"avg": 67.32, "peak": 88.69, "min": 60.06}, "VDD_CPU_SOC_MSS": {"avg": 15.32, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 31.36, "energy_joules_est": 53.51, "sample_count": 13, "duration_seconds": 1.706}, "timestamp": "2026-01-16T15:25:28.625576"}
{"image_index": 86, "image_name": "000000008532.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008532.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2024.416, "latencies_ms": [2024.416], "images_per_second": 0.494, "prompt_tokens": 27, "response_tokens_est": 52, "n_tiles": 6, "output_text": "The main object in the foreground is a smiling man wearing a blue shirt and red tie. The background features a building with a green roof and a window. The man is positioned slightly to the right of the frame, with the building being the far background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25590.3, "ram_available_mb": 100181.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 25590.6, "ram_available_mb": 100181.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.81, "peak": 15.11, "min": 14.22}, "VDD_GPU": {"avg": 30.39, "peak": 38.6, "min": 24.04}, "VIN": {"avg": 68.0, "peak": 116.39, "min": 57.0}, "VDD_CPU_SOC_MSS": {"avg": 15.38, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 30.39, "energy_joules_est": 61.54, "sample_count": 15, "duration_seconds": 2.025}, "timestamp": "2026-01-16T15:25:30.656497"}
{"image_index": 86, "image_name": "000000008532.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008532.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1727.614, "latencies_ms": [1727.614], "images_per_second": 0.579, "prompt_tokens": 21, "response_tokens_est": 41, "n_tiles": 6, "output_text": "The image shows a man wearing a light blue shirt and a red tie, standing outdoors. He is smiling and appears to be in a relaxed setting with a building and a window visible in the background.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25590.6, "ram_available_mb": 100181.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 25590.6, "ram_available_mb": 100181.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.77, "peak": 15.11, "min": 14.22}, "VDD_GPU": {"avg": 31.09, "peak": 37.82, "min": 24.43}, "VIN": {"avg": 67.36, "peak": 107.89, "min": 56.69}, "VDD_CPU_SOC_MSS": {"avg": 15.38, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 31.09, "energy_joules_est": 53.72, "sample_count": 13, "duration_seconds": 1.728}, "timestamp": "2026-01-16T15:25:32.390691"}
{"image_index": 86, "image_name": "000000008532.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008532.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1621.718, "latencies_ms": [1621.718], "images_per_second": 0.617, "prompt_tokens": 19, "response_tokens_est": 37, "n_tiles": 6, "output_text": "The man in the image is wearing a light blue shirt and a red tie. The background is blurred, but it appears to be a bright, sunny day with clear lighting.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25590.6, "ram_available_mb": 100181.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 25590.6, "ram_available_mb": 100181.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.77, "peak": 15.11, "min": 14.22}, "VDD_GPU": {"avg": 31.38, "peak": 38.21, "min": 24.82}, "VIN": {"avg": 71.05, "peak": 101.3, "min": 55.03}, "VDD_CPU_SOC_MSS": {"avg": 15.32, "peak": 15.75, "min": 14.57}}, "power_watts_avg": 31.38, "energy_joules_est": 50.9, "sample_count": 12, "duration_seconds": 1.622}, "timestamp": "2026-01-16T15:25:34.018430"}
{"image_index": 87, "image_name": "000000008629.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008629.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1123.153, "latencies_ms": [1123.153], "images_per_second": 0.89, "prompt_tokens": 9, "response_tokens_est": 30, "n_tiles": 4, "output_text": "The image is a collage of four photographs showing various slices of pizza, each with different toppings and appearances, all placed on white plates.", "error": null, "sys_before": {"cpu_percent": 41.9, "ram_used_mb": 25590.6, "ram_available_mb": 100181.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 25590.6, "ram_available_mb": 100181.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4606.7, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5444.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.31, "peak": 14.91, "min": 13.72}, "VDD_GPU": {"avg": 29.59, "peak": 34.27, "min": 24.82}, "VIN": {"avg": 66.65, "peak": 91.58, "min": 55.65}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 29.59, "energy_joules_est": 33.25, "sample_count": 8, "duration_seconds": 1.124}, "timestamp": "2026-01-16T15:25:35.189912"}
{"image_index": 87, "image_name": "000000008629.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008629.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1213.131, "latencies_ms": [1213.131], "images_per_second": 0.824, "prompt_tokens": 23, "response_tokens_est": 32, "n_tiles": 4, "output_text": "1. Pizza\n2. Pizza\n3. Pizza\n4. Pizza\n5. Pizza\n6. Pizza\n7. Pizza\n8. Pizza", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25590.6, "ram_available_mb": 100181.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 25590.6, "ram_available_mb": 100181.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4606.7, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5455.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.49, "peak": 14.81, "min": 13.92}, "VDD_GPU": {"avg": 29.59, "peak": 35.85, "min": 24.03}, "VIN": {"avg": 65.36, "peak": 78.22, "min": 58.84}, "VDD_CPU_SOC_MSS": {"avg": 15.05, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 29.59, "energy_joules_est": 35.91, "sample_count": 9, "duration_seconds": 1.214}, "timestamp": "2026-01-16T15:25:36.409169"}
{"image_index": 87, "image_name": "000000008629.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008629.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3376.672, "latencies_ms": [3376.672], "images_per_second": 0.296, "prompt_tokens": 27, "response_tokens_est": 114, "n_tiles": 4, "output_text": "The main objects in the image are pizzas, and they are arranged in a way that showcases their spatial relationships. The pizzas are positioned in the center of the image, with the leftmost pizza being the closest to the viewer, while the rightmost pizza is the furthest away. The foreground pizza is prominently displayed, with its toppings and crust clearly visible. The background pizza is slightly out of focus, indicating that it is further away. The pizzas are placed on white plates, which help to highlight their colors and textures.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25590.6, "ram_available_mb": 100181.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 25590.6, "ram_available_mb": 100181.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4606.7, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5458.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.82, "peak": 15.11, "min": 13.92}, "VDD_GPU": {"avg": 24.43, "peak": 35.06, "min": 21.66}, "VIN": {"avg": 65.52, "peak": 91.25, "min": 58.84}, "VDD_CPU_SOC_MSS": {"avg": 15.51, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 24.43, "energy_joules_est": 82.5, "sample_count": 26, "duration_seconds": 3.377}, "timestamp": "2026-01-16T15:25:39.792063"}
{"image_index": 87, "image_name": "000000008629.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008629.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1945.828, "latencies_ms": [1945.828], "images_per_second": 0.514, "prompt_tokens": 21, "response_tokens_est": 60, "n_tiles": 4, "output_text": "The image is a collage of four photographs depicting various slices of pizza. The pizza appears to be on a white plate, and the slices are being eaten, with some pieces being cut and others being eaten whole. The setting suggests a casual dining environment, possibly a restaurant or a home kitchen.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25590.6, "ram_available_mb": 100181.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 25590.6, "ram_available_mb": 100181.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4606.7, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5453.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.68, "peak": 15.01, "min": 13.92}, "VDD_GPU": {"avg": 26.21, "peak": 34.66, "min": 22.07}, "VIN": {"avg": 65.28, "peak": 94.91, "min": 47.83}, "VDD_CPU_SOC_MSS": {"avg": 15.36, "peak": 15.75, "min": 14.57}}, "power_watts_avg": 26.21, "energy_joules_est": 51.01, "sample_count": 15, "duration_seconds": 1.946}, "timestamp": "2026-01-16T15:25:41.744096"}
{"image_index": 87, "image_name": "000000008629.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008629.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1839.299, "latencies_ms": [1839.299], "images_per_second": 0.544, "prompt_tokens": 19, "response_tokens_est": 56, "n_tiles": 4, "output_text": "The image displays a collage of four photographs featuring a pizza with various toppings. The pizza crust appears golden-brown, and the toppings include melted cheese, red tomato pieces, and green herbs. The lighting is bright, highlighting the textures and colors of the food.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25590.6, "ram_available_mb": 100181.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 25590.6, "ram_available_mb": 100181.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4606.7, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5451.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.68, "peak": 15.01, "min": 13.92}, "VDD_GPU": {"avg": 26.53, "peak": 35.06, "min": 22.06}, "VIN": {"avg": 68.38, "peak": 104.61, "min": 60.48}, "VDD_CPU_SOC_MSS": {"avg": 15.3, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 26.53, "energy_joules_est": 48.81, "sample_count": 14, "duration_seconds": 1.84}, "timestamp": "2026-01-16T15:25:43.589626"}
{"image_index": 88, "image_name": "000000008690.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008690.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2519.462, "latencies_ms": [2519.462], "images_per_second": 0.397, "prompt_tokens": 9, "response_tokens_est": 33, "n_tiles": 12, "output_text": "Two young girls are standing in front of a fence, with one of them holding a black goat's head, while the other girl is holding a pink object.", "error": null, "sys_before": {"cpu_percent": 46.8, "ram_used_mb": 25590.6, "ram_available_mb": 100181.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 25589.4, "ram_available_mb": 100182.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.07, "peak": 16.15, "min": 13.79}, "VDD_GPU": {"avg": 34.33, "peak": 40.18, "min": 27.18}, "VIN": {"avg": 71.51, "peak": 97.13, "min": 54.3}, "VDD_CPU_SOC_MSS": {"avg": 15.91, "peak": 17.32, "min": 15.35}}, "power_watts_avg": 34.33, "energy_joules_est": 86.51, "sample_count": 19, "duration_seconds": 2.52}, "timestamp": "2026-01-16T15:25:46.199696"}
{"image_index": 88, "image_name": "000000008690.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008690.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2946.944, "latencies_ms": [2946.944], "images_per_second": 0.339, "prompt_tokens": 23, "response_tokens_est": 42, "n_tiles": 12, "output_text": "- goat: 2\n- girl: 2\n- woman: 1\n- dog: 1\n- fence: 1\n- child: 1\n- pet: 1", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25589.4, "ram_available_mb": 100182.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 25589.4, "ram_available_mb": 100182.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.05, "peak": 15.54, "min": 14.53}, "VDD_GPU": {"avg": 34.58, "peak": 42.94, "min": 26.39}, "VIN": {"avg": 73.11, "peak": 118.79, "min": 49.92}, "VDD_CPU_SOC_MSS": {"avg": 15.64, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 34.58, "energy_joules_est": 101.92, "sample_count": 23, "duration_seconds": 2.947}, "timestamp": "2026-01-16T15:25:49.153063"}
{"image_index": 88, "image_name": "000000008690.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008690.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 4231.742, "latencies_ms": [4231.742], "images_per_second": 0.236, "prompt_tokens": 27, "response_tokens_est": 80, "n_tiles": 12, "output_text": "In the image, the main objects are a black goat and a young girl. The goat is in the foreground, while the girl is positioned slightly behind it. The girl is holding the goat's head, indicating a close interaction between them. The goat is also in the foreground, while the girl is in the background. The background features a fence and a house, providing context to the setting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25589.4, "ram_available_mb": 100182.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 25589.4, "ram_available_mb": 100182.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.07, "peak": 15.54, "min": 14.43}, "VDD_GPU": {"avg": 31.91, "peak": 41.76, "min": 26.0}, "VIN": {"avg": 72.37, "peak": 113.82, "min": 58.12}, "VDD_CPU_SOC_MSS": {"avg": 15.71, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 31.91, "energy_joules_est": 135.05, "sample_count": 33, "duration_seconds": 4.232}, "timestamp": "2026-01-16T15:25:53.391608"}
{"image_index": 88, "image_name": "000000008690.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008690.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 4123.504, "latencies_ms": [4123.504], "images_per_second": 0.243, "prompt_tokens": 21, "response_tokens_est": 77, "n_tiles": 12, "output_text": "The image depicts a scene in a petting zoo or a similar animal enclosure. In the foreground, a young girl is petting a black goat with a gentle smile on her face. Behind her, another girl is also present, holding a pink toy. The setting is outdoors, with a fence and greenery visible in the background, indicating a controlled environment for the animals.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25589.4, "ram_available_mb": 100182.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 25590.0, "ram_available_mb": 100182.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.07, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 32.15, "peak": 42.54, "min": 26.0}, "VIN": {"avg": 75.58, "peak": 113.84, "min": 60.06}, "VDD_CPU_SOC_MSS": {"avg": 15.6, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 32.15, "energy_joules_est": 132.59, "sample_count": 32, "duration_seconds": 4.124}, "timestamp": "2026-01-16T15:25:57.522611"}
{"image_index": 88, "image_name": "000000008690.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008690.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3267.927, "latencies_ms": [3267.927], "images_per_second": 0.306, "prompt_tokens": 19, "response_tokens_est": 51, "n_tiles": 12, "output_text": "The image features a bright, sunny day with clear blue skies. The lighting is natural, casting soft shadows on the ground. The scene includes a white goat with black patches, a black and white dog, and a girl in a colorful floral dress.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25590.0, "ram_available_mb": 100182.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 25590.0, "ram_available_mb": 100182.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.0, "peak": 15.65, "min": 14.43}, "VDD_GPU": {"avg": 33.66, "peak": 42.15, "min": 26.0}, "VIN": {"avg": 75.78, "peak": 118.54, "min": 58.33}, "VDD_CPU_SOC_MSS": {"avg": 15.59, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 33.66, "energy_joules_est": 110.01, "sample_count": 25, "duration_seconds": 3.268}, "timestamp": "2026-01-16T15:26:00.800974"}
{"image_index": 89, "image_name": "000000008762.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008762.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1363.582, "latencies_ms": [1363.582], "images_per_second": 0.733, "prompt_tokens": 9, "response_tokens_est": 29, "n_tiles": 6, "output_text": "The image depicts a nighttime scene of a road intersection with traffic lights showing green, and a sign indicating a nearby exit or entrance.", "error": null, "sys_before": {"cpu_percent": 37.7, "ram_used_mb": 25590.0, "ram_available_mb": 100182.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 25590.0, "ram_available_mb": 100182.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.26, "peak": 14.91, "min": 13.72}, "VDD_GPU": {"avg": 32.18, "peak": 37.42, "min": 26.39}, "VIN": {"avg": 77.1, "peak": 123.9, "min": 59.21}, "VDD_CPU_SOC_MSS": {"avg": 15.04, "peak": 15.75, "min": 14.57}}, "power_watts_avg": 32.18, "energy_joules_est": 43.9, "sample_count": 10, "duration_seconds": 1.364}, "timestamp": "2026-01-16T15:26:02.227600"}
{"image_index": 89, "image_name": "000000008762.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008762.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1727.813, "latencies_ms": [1727.813], "images_per_second": 0.579, "prompt_tokens": 23, "response_tokens_est": 41, "n_tiles": 6, "output_text": "- Traffic light: 2\n- Street light: 1\n- Road: 1\n- Buildings: 3\n- Power lines: 2\n- Road sign: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25590.0, "ram_available_mb": 100182.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 25590.0, "ram_available_mb": 100182.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.73, "peak": 15.11, "min": 14.22}, "VDD_GPU": {"avg": 31.45, "peak": 38.19, "min": 24.82}, "VIN": {"avg": 68.63, "peak": 115.37, "min": 57.43}, "VDD_CPU_SOC_MSS": {"avg": 15.38, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 31.45, "energy_joules_est": 54.35, "sample_count": 13, "duration_seconds": 1.728}, "timestamp": "2026-01-16T15:26:03.961930"}
{"image_index": 89, "image_name": "000000008762.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008762.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2512.151, "latencies_ms": [2512.151], "images_per_second": 0.398, "prompt_tokens": 27, "response_tokens_est": 70, "n_tiles": 6, "output_text": "The main objects in the image are the traffic lights and the road. The traffic lights are positioned in the foreground, with the green light illuminated, indicating that vehicles can proceed. The road is in the background, extending into the distance with other traffic lights and signs visible. The distant mountains and buildings provide a sense of depth to the scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25590.0, "ram_available_mb": 100182.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 25590.0, "ram_available_mb": 100182.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.9, "peak": 15.22, "min": 14.22}, "VDD_GPU": {"avg": 28.74, "peak": 38.21, "min": 23.64}, "VIN": {"avg": 72.08, "peak": 116.26, "min": 57.6}, "VDD_CPU_SOC_MSS": {"avg": 15.53, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 28.74, "energy_joules_est": 72.21, "sample_count": 20, "duration_seconds": 2.513}, "timestamp": "2026-01-16T15:26:06.480322"}
{"image_index": 89, "image_name": "000000008762.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008762.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2242.875, "latencies_ms": [2242.875], "images_per_second": 0.446, "prompt_tokens": 21, "response_tokens_est": 60, "n_tiles": 6, "output_text": "The image depicts a nighttime scene of a highway intersection during twilight. The sky is dark, and the streetlights are on, illuminating the scene. The traffic lights are green, indicating that vehicles can proceed. The road is empty, and there are no visible pedestrians or vehicles.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25590.0, "ram_available_mb": 100182.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 25590.0, "ram_available_mb": 100182.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.88, "peak": 15.22, "min": 14.22}, "VDD_GPU": {"avg": 29.47, "peak": 37.82, "min": 23.64}, "VIN": {"avg": 71.16, "peak": 110.5, "min": 55.8}, "VDD_CPU_SOC_MSS": {"avg": 15.48, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 29.47, "energy_joules_est": 66.11, "sample_count": 16, "duration_seconds": 2.243}, "timestamp": "2026-01-16T15:26:08.729236"}
{"image_index": 89, "image_name": "000000008762.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008762.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2076.521, "latencies_ms": [2076.521], "images_per_second": 0.482, "prompt_tokens": 19, "response_tokens_est": 54, "n_tiles": 6, "output_text": "The image depicts a nighttime scene with a dimly lit street scene. The sky is dark, and the street is illuminated by streetlights and vehicle headlights. The traffic lights are green, and the overall lighting suggests it is late evening or night.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25590.0, "ram_available_mb": 100182.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 25590.0, "ram_available_mb": 100182.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.86, "peak": 15.11, "min": 14.22}, "VDD_GPU": {"avg": 29.67, "peak": 37.8, "min": 24.03}, "VIN": {"avg": 69.77, "peak": 115.09, "min": 55.65}, "VDD_CPU_SOC_MSS": {"avg": 15.43, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 29.67, "energy_joules_est": 61.63, "sample_count": 16, "duration_seconds": 2.077}, "timestamp": "2026-01-16T15:26:10.812965"}
{"image_index": 90, "image_name": "000000008844.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008844.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1044.43, "latencies_ms": [1044.43], "images_per_second": 0.957, "prompt_tokens": 9, "response_tokens_est": 17, "n_tiles": 6, "output_text": "A smiling woman is standing in front of a table filled with ripe bananas.", "error": null, "sys_before": {"cpu_percent": 42.5, "ram_used_mb": 25590.0, "ram_available_mb": 100182.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25590.0, "ram_available_mb": 100182.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.23, "peak": 14.51, "min": 13.92}, "VDD_GPU": {"avg": 33.76, "peak": 37.42, "min": 29.54}, "VIN": {"avg": 76.1, "peak": 119.36, "min": 64.34}, "VDD_CPU_SOC_MSS": {"avg": 15.02, "peak": 15.35, "min": 14.57}}, "power_watts_avg": 33.76, "energy_joules_est": 35.27, "sample_count": 7, "duration_seconds": 1.045}, "timestamp": "2026-01-16T15:26:11.914873"}
{"image_index": 90, "image_name": "000000008844.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008844.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1058.568, "latencies_ms": [1058.568], "images_per_second": 0.945, "prompt_tokens": 23, "response_tokens_est": 16, "n_tiles": 6, "output_text": "bananas: 20\nwoman: 1\nshirt: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25590.0, "ram_available_mb": 100182.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 25590.0, "ram_available_mb": 100182.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.52, "peak": 14.73, "min": 14.12}, "VDD_GPU": {"avg": 35.5, "peak": 38.98, "min": 29.54}, "VIN": {"avg": 76.89, "peak": 106.9, "min": 52.33}, "VDD_CPU_SOC_MSS": {"avg": 15.1, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 35.5, "energy_joules_est": 37.59, "sample_count": 8, "duration_seconds": 1.059}, "timestamp": "2026-01-16T15:26:12.980111"}
{"image_index": 90, "image_name": "000000008844.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008844.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2053.69, "latencies_ms": [2053.69], "images_per_second": 0.487, "prompt_tokens": 27, "response_tokens_est": 51, "n_tiles": 6, "output_text": "The main objects in the image are bananas and a woman. The bananas are in the foreground, with a bunch of bananas in the center and more bananas scattered around. The woman is in the background, standing near the bananas.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25590.0, "ram_available_mb": 100182.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 25590.2, "ram_available_mb": 100181.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.6, "peak": 14.91, "min": 14.02}, "VDD_GPU": {"avg": 30.48, "peak": 39.38, "min": 23.64}, "VIN": {"avg": 70.88, "peak": 116.0, "min": 60.88}, "VDD_CPU_SOC_MSS": {"avg": 15.21, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 30.48, "energy_joules_est": 62.61, "sample_count": 16, "duration_seconds": 2.054}, "timestamp": "2026-01-16T15:26:15.039281"}
{"image_index": 90, "image_name": "000000008844.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008844.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2386.26, "latencies_ms": [2386.26], "images_per_second": 0.419, "prompt_tokens": 21, "response_tokens_est": 63, "n_tiles": 6, "output_text": "The image depicts a woman standing in front of a rustic wooden stall, surrounded by a variety of bananas. She is smiling and appears to be enjoying her time at the market. The setting is likely a local market or a small grocery store, with the woman engaging in a casual interaction with the produce.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25590.2, "ram_available_mb": 100181.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 25590.1, "ram_available_mb": 100182.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.69, "peak": 15.01, "min": 14.02}, "VDD_GPU": {"avg": 28.69, "peak": 38.21, "min": 23.25}, "VIN": {"avg": 70.38, "peak": 119.43, "min": 54.76}, "VDD_CPU_SOC_MSS": {"avg": 15.22, "peak": 15.36, "min": 14.57}}, "power_watts_avg": 28.69, "energy_joules_est": 68.47, "sample_count": 18, "duration_seconds": 2.387}, "timestamp": "2026-01-16T15:26:17.432876"}
{"image_index": 90, "image_name": "000000008844.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008844.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1763.995, "latencies_ms": [1763.995], "images_per_second": 0.567, "prompt_tokens": 19, "response_tokens_est": 41, "n_tiles": 6, "output_text": "The image features a woman with a warm smile, wearing a colorful, patterned shirt. The background includes a rustic wooden table with a bunch of ripe bananas, suggesting a warm, sunny day.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25590.1, "ram_available_mb": 100182.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25590.1, "ram_available_mb": 100182.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.62, "peak": 14.91, "min": 14.02}, "VDD_GPU": {"avg": 30.64, "peak": 37.82, "min": 24.03}, "VIN": {"avg": 77.92, "peak": 119.78, "min": 61.11}, "VDD_CPU_SOC_MSS": {"avg": 15.17, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 30.64, "energy_joules_est": 54.06, "sample_count": 13, "duration_seconds": 1.764}, "timestamp": "2026-01-16T15:26:19.203262"}
{"image_index": 91, "image_name": "000000008899.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008899.jpg", "image_width": 640, "image_height": 539, "image_resolution": "640x539", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2728.417, "latencies_ms": [2728.417], "images_per_second": 0.367, "prompt_tokens": 9, "response_tokens_est": 39, "n_tiles": 12, "output_text": "The image depicts a street scene featuring a closed metal shuttered shop with graffiti on its door and window, a rusty fire hydrant, and a bicycle leaning against a building.", "error": null, "sys_before": {"cpu_percent": 40.8, "ram_used_mb": 25590.1, "ram_available_mb": 100182.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 25590.3, "ram_available_mb": 100181.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.76, "peak": 15.75, "min": 13.69}, "VDD_GPU": {"avg": 33.8, "peak": 40.97, "min": 26.39}, "VIN": {"avg": 73.86, "peak": 125.82, "min": 58.86}, "VDD_CPU_SOC_MSS": {"avg": 15.45, "peak": 16.92, "min": 14.57}}, "power_watts_avg": 33.8, "energy_joules_est": 92.23, "sample_count": 21, "duration_seconds": 2.729}, "timestamp": "2026-01-16T15:26:22.024181"}
{"image_index": 91, "image_name": "000000008899.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008899.jpg", "image_width": 640, "image_height": 539, "image_resolution": "640x539", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2785.355, "latencies_ms": [2785.355], "images_per_second": 0.359, "prompt_tokens": 23, "response_tokens_est": 37, "n_tiles": 12, "output_text": "fire hydrant: 1\nbicycle: 1\nbuilding: 1\nwindow: 2\ngarage door: 2\ngraffiti: 4", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25590.3, "ram_available_mb": 100181.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 25590.1, "ram_available_mb": 100182.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.7, "peak": 15.24, "min": 14.12}, "VDD_GPU": {"avg": 34.9, "peak": 42.94, "min": 26.39}, "VIN": {"avg": 80.51, "peak": 121.84, "min": 58.34}, "VDD_CPU_SOC_MSS": {"avg": 15.3, "peak": 16.14, "min": 14.56}}, "power_watts_avg": 34.9, "energy_joules_est": 97.23, "sample_count": 22, "duration_seconds": 2.786}, "timestamp": "2026-01-16T15:26:24.816053"}
{"image_index": 91, "image_name": "000000008899.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008899.jpg", "image_width": 640, "image_height": 539, "image_resolution": "640x539", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 4124.981, "latencies_ms": [4124.981], "images_per_second": 0.242, "prompt_tokens": 27, "response_tokens_est": 77, "n_tiles": 12, "output_text": "The main objects in the image are a red brick building with a green metal fire escape, a closed metal shutter door, and a green door with graffiti. The fire escape is attached to the building, and the green door is located near the fire hydrant. The graffiti on the door and shutter is in the foreground, while the fire hydrant is in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25590.1, "ram_available_mb": 100182.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25591.2, "ram_available_mb": 100181.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.76, "peak": 15.24, "min": 14.12}, "VDD_GPU": {"avg": 32.09, "peak": 42.13, "min": 26.0}, "VIN": {"avg": 70.38, "peak": 110.62, "min": 58.34}, "VDD_CPU_SOC_MSS": {"avg": 15.26, "peak": 16.14, "min": 14.57}}, "power_watts_avg": 32.09, "energy_joules_est": 132.39, "sample_count": 32, "duration_seconds": 4.125}, "timestamp": "2026-01-16T15:26:28.951883"}
{"image_index": 91, "image_name": "000000008899.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008899.jpg", "image_width": 640, "image_height": 539, "image_resolution": "640x539", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3880.828, "latencies_ms": [3880.828], "images_per_second": 0.258, "prompt_tokens": 21, "response_tokens_est": 69, "n_tiles": 12, "output_text": "The image depicts an urban alleyway with a red brick building on the left and a green metal shuttered shop on the right. The shop has various graffiti tags on its shutters, and a fire hydrant is visible in the foreground. The scene suggests a quiet, possibly neglected area with a mix of commercial and residential structures.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25591.2, "ram_available_mb": 100181.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25591.2, "ram_available_mb": 100181.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.71, "peak": 15.24, "min": 14.12}, "VDD_GPU": {"avg": 32.21, "peak": 42.15, "min": 25.6}, "VIN": {"avg": 70.04, "peak": 119.2, "min": 58.17}, "VDD_CPU_SOC_MSS": {"avg": 15.26, "peak": 16.14, "min": 14.56}}, "power_watts_avg": 32.21, "energy_joules_est": 125.02, "sample_count": 30, "duration_seconds": 3.881}, "timestamp": "2026-01-16T15:26:32.839244"}
{"image_index": 91, "image_name": "000000008899.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000008899.jpg", "image_width": 640, "image_height": 539, "image_resolution": "640x539", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3887.901, "latencies_ms": [3887.901], "images_per_second": 0.257, "prompt_tokens": 19, "response_tokens_est": 69, "n_tiles": 12, "output_text": "The image depicts a weathered, urban scene with a red brick building featuring a green metal fire escape. The building has a weathered look with visible signs of wear and tear, including graffiti on the windows and doors. The lighting is dim, suggesting it might be early morning or late afternoon, with a subdued atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25591.2, "ram_available_mb": 100181.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25590.9, "ram_available_mb": 100181.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.71, "peak": 15.34, "min": 14.12}, "VDD_GPU": {"avg": 32.41, "peak": 42.54, "min": 25.6}, "VIN": {"avg": 72.33, "peak": 121.43, "min": 58.41}, "VDD_CPU_SOC_MSS": {"avg": 15.21, "peak": 16.14, "min": 14.56}}, "power_watts_avg": 32.41, "energy_joules_est": 126.02, "sample_count": 30, "duration_seconds": 3.888}, "timestamp": "2026-01-16T15:26:36.734312"}
{"image_index": 92, "image_name": "000000009378.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009378.jpg", "image_width": 600, "image_height": 400, "image_resolution": "600x400", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1506.649, "latencies_ms": [1506.649], "images_per_second": 0.664, "prompt_tokens": 9, "response_tokens_est": 34, "n_tiles": 6, "output_text": "A man is in the process of throwing a bright yellow frisbee, with his eyes wide open and mouth open, indicating he is fully engaged in the activity.", "error": null, "sys_before": {"cpu_percent": 41.6, "ram_used_mb": 25590.9, "ram_available_mb": 100181.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25590.9, "ram_available_mb": 100181.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.29, "peak": 14.91, "min": 13.72}, "VDD_GPU": {"avg": 31.51, "peak": 37.42, "min": 25.21}, "VIN": {"avg": 71.55, "peak": 116.76, "min": 55.78}, "VDD_CPU_SOC_MSS": {"avg": 14.89, "peak": 15.36, "min": 14.17}}, "power_watts_avg": 31.51, "energy_joules_est": 47.48, "sample_count": 11, "duration_seconds": 1.507}, "timestamp": "2026-01-16T15:26:38.305532"}
{"image_index": 92, "image_name": "000000009378.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009378.jpg", "image_width": 600, "image_height": 400, "image_resolution": "600x400", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1567.24, "latencies_ms": [1567.24], "images_per_second": 0.638, "prompt_tokens": 23, "response_tokens_est": 35, "n_tiles": 6, "output_text": "frisbee: 1\nman: 1\nhat: 1\nshirt: 1\nclothes: 1\nsunglasses: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25590.9, "ram_available_mb": 100181.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 25591.2, "ram_available_mb": 100181.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.72, "peak": 15.11, "min": 14.22}, "VDD_GPU": {"avg": 31.91, "peak": 38.21, "min": 25.21}, "VIN": {"avg": 65.39, "peak": 91.4, "min": 56.57}, "VDD_CPU_SOC_MSS": {"avg": 15.32, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 31.91, "energy_joules_est": 50.02, "sample_count": 12, "duration_seconds": 1.568}, "timestamp": "2026-01-16T15:26:39.881072"}
{"image_index": 92, "image_name": "000000009378.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009378.jpg", "image_width": 600, "image_height": 400, "image_resolution": "600x400", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2472.479, "latencies_ms": [2472.479], "images_per_second": 0.404, "prompt_tokens": 27, "response_tokens_est": 69, "n_tiles": 6, "output_text": "The main object in the foreground is a man holding a bright green frisbee. He is positioned slightly off-center to the left. The background is dimly lit and out of focus, with indistinct shapes and people. The man appears to be in the center of the frame, with the frisbee being the focal point.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25591.2, "ram_available_mb": 100181.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 25590.9, "ram_available_mb": 100181.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.91, "peak": 15.22, "min": 14.22}, "VDD_GPU": {"avg": 29.07, "peak": 38.21, "min": 23.64}, "VIN": {"avg": 74.36, "peak": 123.98, "min": 62.97}, "VDD_CPU_SOC_MSS": {"avg": 15.58, "peak": 16.14, "min": 14.96}}, "power_watts_avg": 29.07, "energy_joules_est": 71.89, "sample_count": 19, "duration_seconds": 2.473}, "timestamp": "2026-01-16T15:26:42.360493"}
{"image_index": 92, "image_name": "000000009378.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009378.jpg", "image_width": 600, "image_height": 400, "image_resolution": "600x400", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2930.964, "latencies_ms": [2930.964], "images_per_second": 0.341, "prompt_tokens": 21, "response_tokens_est": 86, "n_tiles": 6, "output_text": "The image depicts a man in a dark-colored shirt and a blue and white striped beanie, holding a bright yellow frisbee in his right hand. He appears to be in an indoor setting, possibly a recreational area or a sports facility, with a crowd of people in the background. The man seems to be engaged in a game or activity involving the frisbee, as he is focused on catching it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25590.9, "ram_available_mb": 100181.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 25590.7, "ram_available_mb": 100181.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.98, "peak": 15.32, "min": 14.22}, "VDD_GPU": {"avg": 27.87, "peak": 37.82, "min": 23.64}, "VIN": {"avg": 68.24, "peak": 105.46, "min": 62.57}, "VDD_CPU_SOC_MSS": {"avg": 15.56, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 27.87, "energy_joules_est": 81.7, "sample_count": 23, "duration_seconds": 2.931}, "timestamp": "2026-01-16T15:26:45.297727"}
{"image_index": 92, "image_name": "000000009378.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009378.jpg", "image_width": 600, "image_height": 400, "image_resolution": "600x400", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1532.802, "latencies_ms": [1532.802], "images_per_second": 0.652, "prompt_tokens": 19, "response_tokens_est": 34, "n_tiles": 6, "output_text": "The man in the image is wearing a black sleeveless top and a blue and white striped beanie. The lighting is dim, creating a moody atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25590.7, "ram_available_mb": 100181.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 25591.5, "ram_available_mb": 100180.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.77, "peak": 15.11, "min": 14.22}, "VDD_GPU": {"avg": 32.12, "peak": 37.82, "min": 25.61}, "VIN": {"avg": 79.93, "peak": 108.02, "min": 63.09}, "VDD_CPU_SOC_MSS": {"avg": 15.32, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 32.12, "energy_joules_est": 49.25, "sample_count": 11, "duration_seconds": 1.533}, "timestamp": "2026-01-16T15:26:46.836679"}
{"image_index": 93, "image_name": "000000009400.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009400.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2821.575, "latencies_ms": [2821.575], "images_per_second": 0.354, "prompt_tokens": 9, "response_tokens_est": 42, "n_tiles": 12, "output_text": "The image depicts a group of people sitting around a table, each engaged with their own laptop, with a variety of items scattered around, including a keyboard, a mouse, and a cup of coffee.", "error": null, "sys_before": {"cpu_percent": 44.4, "ram_used_mb": 25591.0, "ram_available_mb": 100181.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 25591.0, "ram_available_mb": 100181.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.83, "peak": 15.75, "min": 13.9}, "VDD_GPU": {"avg": 33.61, "peak": 41.36, "min": 26.0}, "VIN": {"avg": 72.34, "peak": 115.62, "min": 54.92}, "VDD_CPU_SOC_MSS": {"avg": 15.5, "peak": 16.92, "min": 14.96}}, "power_watts_avg": 33.61, "energy_joules_est": 94.85, "sample_count": 22, "duration_seconds": 2.822}, "timestamp": "2026-01-16T15:26:49.749128"}
{"image_index": 93, "image_name": "000000009400.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009400.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2612.026, "latencies_ms": [2612.026], "images_per_second": 0.383, "prompt_tokens": 23, "response_tokens_est": 32, "n_tiles": 12, "output_text": "1. Laptop\n2. Laptop\n3. Laptop\n4. Laptop\n5. Laptop\n6. Laptop\n7. Laptop\n8. Laptop", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25591.0, "ram_available_mb": 100181.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 25591.0, "ram_available_mb": 100181.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.72, "peak": 15.24, "min": 14.12}, "VDD_GPU": {"avg": 35.75, "peak": 42.54, "min": 27.18}, "VIN": {"avg": 73.53, "peak": 97.97, "min": 54.39}, "VDD_CPU_SOC_MSS": {"avg": 15.29, "peak": 16.14, "min": 14.56}}, "power_watts_avg": 35.75, "energy_joules_est": 93.4, "sample_count": 20, "duration_seconds": 2.613}, "timestamp": "2026-01-16T15:26:52.367729"}
{"image_index": 93, "image_name": "000000009400.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009400.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3713.481, "latencies_ms": [3713.481], "images_per_second": 0.269, "prompt_tokens": 27, "response_tokens_est": 64, "n_tiles": 12, "output_text": "The main objects in the image are a laptop, a keyboard, and a mouse. The laptop is positioned in the foreground, with the keyboard and mouse placed in front of it. The laptop screen is turned on, displaying some content. The keyboard and mouse are in the foreground, with the laptop in the background.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25591.0, "ram_available_mb": 100181.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25590.9, "ram_available_mb": 100181.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.71, "peak": 15.24, "min": 14.12}, "VDD_GPU": {"avg": 32.72, "peak": 42.54, "min": 25.6}, "VIN": {"avg": 74.44, "peak": 122.61, "min": 58.26}, "VDD_CPU_SOC_MSS": {"avg": 15.18, "peak": 16.14, "min": 14.56}}, "power_watts_avg": 32.72, "energy_joules_est": 121.52, "sample_count": 29, "duration_seconds": 3.714}, "timestamp": "2026-01-16T15:26:56.088433"}
{"image_index": 93, "image_name": "000000009400.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009400.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3952.447, "latencies_ms": [3952.447], "images_per_second": 0.253, "prompt_tokens": 21, "response_tokens_est": 71, "n_tiles": 12, "output_text": "The image depicts a dimly lit room filled with people engaged in various activities. The setting appears to be a casual gathering or meeting place, possibly a bar or a small restaurant, with a table cluttered with laptops, drinks, and other personal items. The individuals are focused on their screens, suggesting they are either working or browsing through content.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25590.9, "ram_available_mb": 100181.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25591.0, "ram_available_mb": 100181.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.73, "peak": 15.34, "min": 14.12}, "VDD_GPU": {"avg": 32.12, "peak": 42.13, "min": 25.61}, "VIN": {"avg": 74.13, "peak": 140.42, "min": 56.48}, "VDD_CPU_SOC_MSS": {"avg": 15.18, "peak": 16.14, "min": 14.56}}, "power_watts_avg": 32.12, "energy_joules_est": 126.97, "sample_count": 31, "duration_seconds": 3.953}, "timestamp": "2026-01-16T15:27:00.047392"}
{"image_index": 93, "image_name": "000000009400.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009400.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2920.005, "latencies_ms": [2920.005], "images_per_second": 0.342, "prompt_tokens": 19, "response_tokens_est": 41, "n_tiles": 12, "output_text": "The image depicts a dimly lit room with a warm, yellowish glow from a lamp. The lighting creates a cozy atmosphere, highlighting the cluttered desk with various electronic devices and personal items.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25591.0, "ram_available_mb": 100181.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 25591.0, "ram_available_mb": 100181.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.72, "peak": 15.34, "min": 14.12}, "VDD_GPU": {"avg": 34.31, "peak": 42.54, "min": 26.0}, "VIN": {"avg": 77.49, "peak": 121.4, "min": 58.39}, "VDD_CPU_SOC_MSS": {"avg": 15.28, "peak": 16.14, "min": 14.56}}, "power_watts_avg": 34.31, "energy_joules_est": 100.2, "sample_count": 23, "duration_seconds": 2.92}, "timestamp": "2026-01-16T15:27:02.974182"}
{"image_index": 94, "image_name": "000000009448.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009448.jpg", "image_width": 551, "image_height": 640, "image_resolution": "551x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2216.414, "latencies_ms": [2216.414], "images_per_second": 0.451, "prompt_tokens": 9, "response_tokens_est": 24, "n_tiles": 12, "output_text": "A young girl with blonde hair is holding a blue umbrella and wearing a pink jacket, standing on a gravel surface.", "error": null, "sys_before": {"cpu_percent": 40.3, "ram_used_mb": 25591.0, "ram_available_mb": 100181.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 2.6, "ram_used_mb": 25591.3, "ram_available_mb": 100180.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.05, "peak": 16.15, "min": 13.59}, "VDD_GPU": {"avg": 35.73, "peak": 41.36, "min": 28.36}, "VIN": {"avg": 79.44, "peak": 126.88, "min": 59.54}, "VDD_CPU_SOC_MSS": {"avg": 15.95, "peak": 17.32, "min": 14.96}}, "power_watts_avg": 35.73, "energy_joules_est": 79.21, "sample_count": 17, "duration_seconds": 2.217}, "timestamp": "2026-01-16T15:27:05.288386"}
{"image_index": 94, "image_name": "000000009448.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009448.jpg", "image_width": 551, "image_height": 640, "image_resolution": "551x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2849.761, "latencies_ms": [2849.761], "images_per_second": 0.351, "prompt_tokens": 23, "response_tokens_est": 39, "n_tiles": 12, "output_text": "1. Girl\n2. Blue umbrella\n3. Blue umbrella\n4. Blue umbrella\n5. Blue umbrella\n6. Blue umbrella\n7. Blue umbrella\n8. Blue umbrella", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25591.3, "ram_available_mb": 100180.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 25587.5, "ram_available_mb": 100184.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.03, "peak": 15.65, "min": 14.43}, "VDD_GPU": {"avg": 34.99, "peak": 43.33, "min": 26.79}, "VIN": {"avg": 74.16, "peak": 118.27, "min": 50.14}, "VDD_CPU_SOC_MSS": {"avg": 15.67, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 34.99, "energy_joules_est": 99.72, "sample_count": 22, "duration_seconds": 2.85}, "timestamp": "2026-01-16T15:27:08.148779"}
{"image_index": 94, "image_name": "000000009448.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009448.jpg", "image_width": 551, "image_height": 640, "image_resolution": "551x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2576.504, "latencies_ms": [2576.504], "images_per_second": 0.388, "prompt_tokens": 27, "response_tokens_est": 31, "n_tiles": 12, "output_text": "The main object in the foreground is a young girl holding a blue umbrella. The background features a blue umbrella, and the ground is covered with gravel.", "error": null, "sys_before": {"cpu_percent": 14.3, "ram_used_mb": 25587.5, "ram_available_mb": 100184.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 25587.5, "ram_available_mb": 100184.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.03, "peak": 15.54, "min": 14.32}, "VDD_GPU": {"avg": 35.59, "peak": 42.54, "min": 27.57}, "VIN": {"avg": 74.37, "peak": 112.51, "min": 58.66}, "VDD_CPU_SOC_MSS": {"avg": 15.73, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 35.59, "energy_joules_est": 91.72, "sample_count": 20, "duration_seconds": 2.577}, "timestamp": "2026-01-16T15:27:10.735003"}
{"image_index": 94, "image_name": "000000009448.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009448.jpg", "image_width": 551, "image_height": 640, "image_resolution": "551x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3144.696, "latencies_ms": [3144.696], "images_per_second": 0.318, "prompt_tokens": 21, "response_tokens_est": 48, "n_tiles": 12, "output_text": "The image depicts a young girl standing under a large blue umbrella, holding a brown object in her hand. The setting appears to be outdoors, possibly in a park or a public area, with a gravel or concrete ground beneath her.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25587.5, "ram_available_mb": 100184.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 25587.2, "ram_available_mb": 100184.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.08, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 34.05, "peak": 42.94, "min": 26.39}, "VIN": {"avg": 73.91, "peak": 118.16, "min": 57.88}, "VDD_CPU_SOC_MSS": {"avg": 15.65, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 34.05, "energy_joules_est": 107.09, "sample_count": 25, "duration_seconds": 3.145}, "timestamp": "2026-01-16T15:27:13.886116"}
{"image_index": 94, "image_name": "000000009448.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009448.jpg", "image_width": 551, "image_height": 640, "image_resolution": "551x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3038.527, "latencies_ms": [3038.527], "images_per_second": 0.329, "prompt_tokens": 19, "response_tokens_est": 45, "n_tiles": 12, "output_text": "The notable visual attributes of the image include a young girl with light blonde hair, wearing a pink jacket and a colorful scarf. The background features a blue umbrella, and the lighting suggests it is daytime with natural sunlight.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25587.2, "ram_available_mb": 100184.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 25587.5, "ram_available_mb": 100184.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.08, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 34.52, "peak": 42.94, "min": 26.39}, "VIN": {"avg": 76.12, "peak": 116.78, "min": 54.94}, "VDD_CPU_SOC_MSS": {"avg": 15.66, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 34.52, "energy_joules_est": 104.91, "sample_count": 23, "duration_seconds": 3.039}, "timestamp": "2026-01-16T15:27:16.931634"}
{"image_index": 95, "image_name": "000000009483.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009483.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2312.202, "latencies_ms": [2312.202], "images_per_second": 0.432, "prompt_tokens": 9, "response_tokens_est": 27, "n_tiles": 12, "output_text": "The image shows a man standing in an office with a computer setup on a desk, including a monitor, keyboard, and mouse.", "error": null, "sys_before": {"cpu_percent": 41.1, "ram_used_mb": 25587.5, "ram_available_mb": 100184.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 25589.2, "ram_available_mb": 100182.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.11, "peak": 16.05, "min": 13.9}, "VDD_GPU": {"avg": 35.32, "peak": 41.36, "min": 27.97}, "VIN": {"avg": 77.71, "peak": 106.17, "min": 58.49}, "VDD_CPU_SOC_MSS": {"avg": 15.81, "peak": 16.92, "min": 14.96}}, "power_watts_avg": 35.32, "energy_joules_est": 81.7, "sample_count": 18, "duration_seconds": 2.313}, "timestamp": "2026-01-16T15:27:19.329428"}
{"image_index": 95, "image_name": "000000009483.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009483.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2744.613, "latencies_ms": [2744.613], "images_per_second": 0.364, "prompt_tokens": 23, "response_tokens_est": 36, "n_tiles": 12, "output_text": "1. Computer monitor\n2. Computer mouse\n3. Keyboard\n4. Printer\n5. Desk\n6. Chair\n7. Cables\n8. Power outlet", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25589.2, "ram_available_mb": 100182.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 25589.5, "ram_available_mb": 100182.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.06, "peak": 15.65, "min": 14.43}, "VDD_GPU": {"avg": 35.19, "peak": 42.94, "min": 26.8}, "VIN": {"avg": 74.88, "peak": 115.64, "min": 58.92}, "VDD_CPU_SOC_MSS": {"avg": 15.65, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 35.19, "energy_joules_est": 96.59, "sample_count": 21, "duration_seconds": 2.745}, "timestamp": "2026-01-16T15:27:22.081872"}
{"image_index": 95, "image_name": "000000009483.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009483.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3171.044, "latencies_ms": [3171.044], "images_per_second": 0.315, "prompt_tokens": 27, "response_tokens_est": 49, "n_tiles": 12, "output_text": "The main objects in the image are a man in a suit and a computer setup. The man is positioned in the foreground, standing near the computer setup. The computer setup is located in the background, slightly to the right of the man.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 25589.5, "ram_available_mb": 100182.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 25588.4, "ram_available_mb": 100183.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.08, "peak": 15.65, "min": 14.43}, "VDD_GPU": {"avg": 34.24, "peak": 42.94, "min": 26.79}, "VIN": {"avg": 74.15, "peak": 107.23, "min": 57.93}, "VDD_CPU_SOC_MSS": {"avg": 15.73, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 34.24, "energy_joules_est": 108.59, "sample_count": 24, "duration_seconds": 3.171}, "timestamp": "2026-01-16T15:27:25.259243"}
{"image_index": 95, "image_name": "000000009483.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009483.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3876.593, "latencies_ms": [3876.593], "images_per_second": 0.258, "prompt_tokens": 21, "response_tokens_est": 70, "n_tiles": 12, "output_text": "The image depicts a man standing in an office or workspace, with a computer setup in front of him. The room appears to be well-lit, with a window providing natural light. The man is dressed in a dark suit and tie, and he seems to be engaged in a task, possibly working on a computer or using a device.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25588.4, "ram_available_mb": 100183.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25589.8, "ram_available_mb": 100182.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.11, "peak": 15.65, "min": 14.43}, "VDD_GPU": {"avg": 32.49, "peak": 42.15, "min": 26.0}, "VIN": {"avg": 71.53, "peak": 113.59, "min": 55.76}, "VDD_CPU_SOC_MSS": {"avg": 15.65, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 32.49, "energy_joules_est": 125.98, "sample_count": 30, "duration_seconds": 3.878}, "timestamp": "2026-01-16T15:27:29.143469"}
{"image_index": 95, "image_name": "000000009483.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009483.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2944.104, "latencies_ms": [2944.104], "images_per_second": 0.34, "prompt_tokens": 19, "response_tokens_est": 42, "n_tiles": 12, "output_text": "The image depicts a well-lit office environment with a white wall and a window with a partially drawn curtain. The desk is equipped with a computer monitor, keyboard, mouse, and various electronic devices.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25589.8, "ram_available_mb": 100182.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 25589.8, "ram_available_mb": 100182.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.06, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 34.44, "peak": 42.54, "min": 26.79}, "VIN": {"avg": 76.27, "peak": 119.33, "min": 54.6}, "VDD_CPU_SOC_MSS": {"avg": 15.69, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 34.44, "energy_joules_est": 101.41, "sample_count": 23, "duration_seconds": 2.945}, "timestamp": "2026-01-16T15:27:32.094228"}
{"image_index": 96, "image_name": "000000009590.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009590.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1555.315, "latencies_ms": [1555.315], "images_per_second": 0.643, "prompt_tokens": 9, "response_tokens_est": 36, "n_tiles": 6, "output_text": "The image depicts a group of four men sitting around a wooden dining table, engaged in a casual conversation, with various items on the table suggesting a relaxed and informal gathering.", "error": null, "sys_before": {"cpu_percent": 33.8, "ram_used_mb": 25589.8, "ram_available_mb": 100182.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 25589.8, "ram_available_mb": 100182.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.49, "peak": 15.01, "min": 13.82}, "VDD_GPU": {"avg": 31.32, "peak": 37.82, "min": 24.82}, "VIN": {"avg": 70.23, "peak": 123.51, "min": 56.95}, "VDD_CPU_SOC_MSS": {"avg": 15.26, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 31.32, "energy_joules_est": 48.74, "sample_count": 12, "duration_seconds": 1.556}, "timestamp": "2026-01-16T15:27:33.703243"}
{"image_index": 96, "image_name": "000000009590.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009590.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1512.837, "latencies_ms": [1512.837], "images_per_second": 0.661, "prompt_tokens": 23, "response_tokens_est": 33, "n_tiles": 6, "output_text": "1. Table\n2. People\n3. Coffee\n4. Cups\n5. Bowl\n6. Bottle\n7. Glass\n8. Plate", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25589.8, "ram_available_mb": 100182.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 25589.8, "ram_available_mb": 100182.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.72, "peak": 15.01, "min": 14.22}, "VDD_GPU": {"avg": 32.37, "peak": 38.19, "min": 25.6}, "VIN": {"avg": 71.84, "peak": 118.85, "min": 51.71}, "VDD_CPU_SOC_MSS": {"avg": 15.39, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 32.37, "energy_joules_est": 48.98, "sample_count": 11, "duration_seconds": 1.513}, "timestamp": "2026-01-16T15:27:35.223101"}
{"image_index": 96, "image_name": "000000009590.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009590.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2921.101, "latencies_ms": [2921.101], "images_per_second": 0.342, "prompt_tokens": 27, "response_tokens_est": 82, "n_tiles": 6, "output_text": "The main objects in the image are a group of people sitting around a wooden dining table. The table is the central focus, with various items placed on it, including a coffee maker, a bottle of sauce, and bowls of food. The people are positioned in the background, with the foreground showing the legs of one person and the table in the middle. The lighting is warm, creating a cozy atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25589.8, "ram_available_mb": 100182.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 25590.0, "ram_available_mb": 100182.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.78, "peak": 15.01, "min": 14.02}, "VDD_GPU": {"avg": 27.93, "peak": 38.6, "min": 23.24}, "VIN": {"avg": 69.09, "peak": 108.09, "min": 60.49}, "VDD_CPU_SOC_MSS": {"avg": 15.34, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 27.93, "energy_joules_est": 81.6, "sample_count": 22, "duration_seconds": 2.922}, "timestamp": "2026-01-16T15:27:38.150480"}
{"image_index": 96, "image_name": "000000009590.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009590.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2464.811, "latencies_ms": [2464.811], "images_per_second": 0.406, "prompt_tokens": 21, "response_tokens_est": 66, "n_tiles": 6, "output_text": "The image depicts a cozy indoor setting where four men are gathered around a wooden dining table. They appear to be enjoying a meal together, with some of them holding cups and plates, while others are engaged in conversation. The room is warmly lit by natural light coming through a window, creating a relaxed and intimate atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25590.0, "ram_available_mb": 100182.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 25589.8, "ram_available_mb": 100182.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.73, "peak": 15.01, "min": 14.02}, "VDD_GPU": {"avg": 28.36, "peak": 37.82, "min": 23.24}, "VIN": {"avg": 64.31, "peak": 73.21, "min": 55.89}, "VDD_CPU_SOC_MSS": {"avg": 15.27, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 28.36, "energy_joules_est": 69.92, "sample_count": 19, "duration_seconds": 2.465}, "timestamp": "2026-01-16T15:27:40.621694"}
{"image_index": 96, "image_name": "000000009590.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009590.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2413.415, "latencies_ms": [2413.415], "images_per_second": 0.414, "prompt_tokens": 19, "response_tokens_est": 64, "n_tiles": 6, "output_text": "The image depicts a cozy dining room with a wooden ceiling and walls, featuring natural light streaming in through large windows with light-colored curtains. The room is filled with various items on the wooden table, including a coffee maker, a bottle of sauce, and a bowl of fruit, creating a warm and inviting atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25589.8, "ram_available_mb": 100182.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 25589.8, "ram_available_mb": 100182.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.74, "peak": 15.01, "min": 14.02}, "VDD_GPU": {"avg": 28.54, "peak": 38.21, "min": 23.25}, "VIN": {"avg": 72.47, "peak": 103.81, "min": 59.5}, "VDD_CPU_SOC_MSS": {"avg": 15.25, "peak": 15.36, "min": 14.57}}, "power_watts_avg": 28.54, "energy_joules_est": 68.89, "sample_count": 18, "duration_seconds": 2.414}, "timestamp": "2026-01-16T15:27:43.041545"}
{"image_index": 97, "image_name": "000000009769.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009769.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2383.935, "latencies_ms": [2383.935], "images_per_second": 0.419, "prompt_tokens": 9, "response_tokens_est": 29, "n_tiles": 12, "output_text": "A red pickup truck is parked on a snowy street, with a snow-covered roof and a snow-covered house in the background.", "error": null, "sys_before": {"cpu_percent": 41.3, "ram_used_mb": 25589.8, "ram_available_mb": 100182.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 25589.8, "ram_available_mb": 100182.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.11, "peak": 16.05, "min": 13.9}, "VDD_GPU": {"avg": 34.79, "peak": 40.97, "min": 27.57}, "VIN": {"avg": 73.59, "peak": 112.75, "min": 64.03}, "VDD_CPU_SOC_MSS": {"avg": 15.81, "peak": 16.92, "min": 14.96}}, "power_watts_avg": 34.79, "energy_joules_est": 82.95, "sample_count": 18, "duration_seconds": 2.384}, "timestamp": "2026-01-16T15:27:45.503297"}
{"image_index": 97, "image_name": "000000009769.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009769.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 3311.829, "latencies_ms": [3311.829], "images_per_second": 0.302, "prompt_tokens": 23, "response_tokens_est": 53, "n_tiles": 12, "output_text": "1. Snow\n2. Snow-covered truck\n3. Snow-covered house\n4. Snow-covered trees\n5. Snow-covered driveway\n6. Snow-covered sidewalk\n7. Snow-covered car\n8. Snow-covered vehicle", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25589.8, "ram_available_mb": 100182.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 25590.8, "ram_available_mb": 100181.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.08, "peak": 15.65, "min": 14.43}, "VDD_GPU": {"avg": 33.72, "peak": 42.94, "min": 26.39}, "VIN": {"avg": 72.91, "peak": 121.48, "min": 58.45}, "VDD_CPU_SOC_MSS": {"avg": 15.67, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 33.72, "energy_joules_est": 111.7, "sample_count": 25, "duration_seconds": 3.313}, "timestamp": "2026-01-16T15:27:48.822513"}
{"image_index": 97, "image_name": "000000009769.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009769.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 4056.112, "latencies_ms": [4056.112], "images_per_second": 0.247, "prompt_tokens": 27, "response_tokens_est": 75, "n_tiles": 12, "output_text": "The main objects in the image are a red pickup truck and a snow-covered residential area. The truck is parked on the right side of the image, with its rear facing the viewer. The residential area is in the background, with houses and trees visible. The truck is positioned near the snow-covered sidewalk, and the houses are situated further back in the scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25590.8, "ram_available_mb": 100181.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25591.0, "ram_available_mb": 100181.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.1, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 32.15, "peak": 42.54, "min": 26.0}, "VIN": {"avg": 70.63, "peak": 113.59, "min": 58.03}, "VDD_CPU_SOC_MSS": {"avg": 15.62, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 32.15, "energy_joules_est": 130.43, "sample_count": 31, "duration_seconds": 4.057}, "timestamp": "2026-01-16T15:27:52.887488"}
{"image_index": 97, "image_name": "000000009769.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009769.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3755.821, "latencies_ms": [3755.821], "images_per_second": 0.266, "prompt_tokens": 21, "response_tokens_est": 66, "n_tiles": 12, "output_text": "The image depicts a snowy residential area with a red pickup truck parked on the side of a snow-covered street. A person is seen clearing snow from the truck's bed, indicating ongoing snow removal efforts. The scene is set in a typical winter environment with snow-covered trees, houses, and a residential street.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25591.0, "ram_available_mb": 100181.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25590.3, "ram_available_mb": 100181.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.08, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 32.68, "peak": 42.54, "min": 26.0}, "VIN": {"avg": 73.71, "peak": 106.08, "min": 58.59}, "VDD_CPU_SOC_MSS": {"avg": 15.58, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 32.68, "energy_joules_est": 122.76, "sample_count": 29, "duration_seconds": 3.756}, "timestamp": "2026-01-16T15:27:56.650143"}
{"image_index": 97, "image_name": "000000009769.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009769.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3965.65, "latencies_ms": [3965.65], "images_per_second": 0.252, "prompt_tokens": 19, "response_tokens_est": 72, "n_tiles": 12, "output_text": "The image depicts a snowy residential area with a red pickup truck parked on the side of a snow-covered street. The truck is covered in snow, and the surrounding area is blanketed in white snow, indicating a winter scene. The lighting is subdued, with the overcast sky casting a soft, diffused light over the scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25590.3, "ram_available_mb": 100181.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25593.9, "ram_available_mb": 100178.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.05, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 32.38, "peak": 42.54, "min": 26.0}, "VIN": {"avg": 72.28, "peak": 114.69, "min": 58.19}, "VDD_CPU_SOC_MSS": {"avg": 15.63, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 32.38, "energy_joules_est": 128.42, "sample_count": 30, "duration_seconds": 3.966}, "timestamp": "2026-01-16T15:28:00.623271"}
{"image_index": 98, "image_name": "000000009772.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009772.jpg", "image_width": 550, "image_height": 640, "image_resolution": "550x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2320.131, "latencies_ms": [2320.131], "images_per_second": 0.431, "prompt_tokens": 9, "response_tokens_est": 27, "n_tiles": 12, "output_text": "The image shows a luxurious bathroom with a marble countertop, gold faucets, and a mirror reflecting a person taking a photo.", "error": null, "sys_before": {"cpu_percent": 34.7, "ram_used_mb": 25593.6, "ram_available_mb": 100178.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 25593.8, "ram_available_mb": 100178.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.08, "peak": 16.15, "min": 13.69}, "VDD_GPU": {"avg": 35.28, "peak": 41.76, "min": 27.97}, "VIN": {"avg": 72.98, "peak": 105.85, "min": 58.56}, "VDD_CPU_SOC_MSS": {"avg": 15.92, "peak": 17.32, "min": 14.57}}, "power_watts_avg": 35.28, "energy_joules_est": 81.88, "sample_count": 18, "duration_seconds": 2.321}, "timestamp": "2026-01-16T15:28:03.022023"}
{"image_index": 98, "image_name": "000000009772.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009772.jpg", "image_width": 550, "image_height": 640, "image_resolution": "550x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2872.297, "latencies_ms": [2872.297], "images_per_second": 0.348, "prompt_tokens": 23, "response_tokens_est": 40, "n_tiles": 12, "output_text": "object: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25593.8, "ram_available_mb": 100178.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 25593.8, "ram_available_mb": 100178.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.07, "peak": 15.54, "min": 14.53}, "VDD_GPU": {"avg": 35.11, "peak": 43.31, "min": 26.79}, "VIN": {"avg": 72.51, "peak": 111.68, "min": 58.61}, "VDD_CPU_SOC_MSS": {"avg": 15.69, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 35.11, "energy_joules_est": 100.86, "sample_count": 22, "duration_seconds": 2.873}, "timestamp": "2026-01-16T15:28:05.901363"}
{"image_index": 98, "image_name": "000000009772.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009772.jpg", "image_width": 550, "image_height": 640, "image_resolution": "550x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3720.21, "latencies_ms": [3720.21], "images_per_second": 0.269, "prompt_tokens": 27, "response_tokens_est": 65, "n_tiles": 12, "output_text": "The main objects in the image are located in the foreground, with the bathroom sink and mirror being the most prominent features. The sink is situated on the left side of the image, while the mirror is positioned in the center. The bathroom is also partially visible in the background, with a doorway leading to another room.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25593.8, "ram_available_mb": 100178.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25597.8, "ram_available_mb": 100174.4, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.09, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 32.82, "peak": 42.15, "min": 26.01}, "VIN": {"avg": 75.93, "peak": 131.81, "min": 56.77}, "VDD_CPU_SOC_MSS": {"avg": 15.68, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 32.82, "energy_joules_est": 122.11, "sample_count": 29, "duration_seconds": 3.721}, "timestamp": "2026-01-16T15:28:09.628371"}
{"image_index": 98, "image_name": "000000009772.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009772.jpg", "image_width": 550, "image_height": 640, "image_resolution": "550x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2747.381, "latencies_ms": [2747.381], "images_per_second": 0.364, "prompt_tokens": 21, "response_tokens_est": 36, "n_tiles": 12, "output_text": "The image depicts a luxurious bathroom with a marble countertop and a large mirror. A man is taking a selfie in the mirror, capturing the bathroom's elegance.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25597.8, "ram_available_mb": 100174.4, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 25597.8, "ram_available_mb": 100174.4, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.05, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 35.25, "peak": 42.54, "min": 27.19}, "VIN": {"avg": 77.64, "peak": 121.13, "min": 53.4}, "VDD_CPU_SOC_MSS": {"avg": 15.65, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 35.25, "energy_joules_est": 96.86, "sample_count": 21, "duration_seconds": 2.748}, "timestamp": "2026-01-16T15:28:12.382376"}
{"image_index": 98, "image_name": "000000009772.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009772.jpg", "image_width": 550, "image_height": 640, "image_resolution": "550x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2603.588, "latencies_ms": [2603.588], "images_per_second": 0.384, "prompt_tokens": 19, "response_tokens_est": 32, "n_tiles": 12, "output_text": "The bathroom features a warm, inviting ambiance with beige walls and a marble countertop. The lighting is soft and warm, creating a cozy atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25597.8, "ram_available_mb": 100174.4, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 25597.6, "ram_available_mb": 100174.6, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.07, "peak": 15.65, "min": 14.43}, "VDD_GPU": {"avg": 35.57, "peak": 42.54, "min": 27.18}, "VIN": {"avg": 81.81, "peak": 136.09, "min": 58.46}, "VDD_CPU_SOC_MSS": {"avg": 15.67, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 35.57, "energy_joules_est": 92.62, "sample_count": 20, "duration_seconds": 2.604}, "timestamp": "2026-01-16T15:28:14.992721"}
{"image_index": 99, "image_name": "000000009891.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009891.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1991.63, "latencies_ms": [1991.63], "images_per_second": 0.502, "prompt_tokens": 9, "response_tokens_est": 17, "n_tiles": 12, "output_text": "A group of people are seen loading luggage into a white SUV at an airport.", "error": null, "sys_before": {"cpu_percent": 20.8, "ram_used_mb": 25597.6, "ram_available_mb": 100174.6, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 2.3, "ram_used_mb": 25597.6, "ram_available_mb": 100174.6, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.19, "peak": 16.15, "min": 14.12}, "VDD_GPU": {"avg": 37.03, "peak": 41.76, "min": 30.33}, "VIN": {"avg": 79.63, "peak": 113.62, "min": 58.45}, "VDD_CPU_SOC_MSS": {"avg": 15.88, "peak": 16.92, "min": 14.96}}, "power_watts_avg": 37.03, "energy_joules_est": 73.76, "sample_count": 15, "duration_seconds": 1.992}, "timestamp": "2026-01-16T15:28:17.041916"}
{"image_index": 99, "image_name": "000000009891.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009891.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2604.924, "latencies_ms": [2604.924], "images_per_second": 0.384, "prompt_tokens": 23, "response_tokens_est": 32, "n_tiles": 12, "output_text": "1. Car\n2. Car\n3. Car\n4. Car\n5. Car\n6. Car\n7. Car\n8. Car", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25597.6, "ram_available_mb": 100174.6, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 25597.6, "ram_available_mb": 100174.6, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.08, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 35.9, "peak": 42.94, "min": 27.18}, "VIN": {"avg": 79.13, "peak": 134.92, "min": 58.94}, "VDD_CPU_SOC_MSS": {"avg": 15.69, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 35.9, "energy_joules_est": 93.53, "sample_count": 20, "duration_seconds": 2.605}, "timestamp": "2026-01-16T15:28:19.653658"}
{"image_index": 99, "image_name": "000000009891.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009891.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3962.628, "latencies_ms": [3962.628], "images_per_second": 0.252, "prompt_tokens": 27, "response_tokens_est": 72, "n_tiles": 12, "output_text": "In the image, the main objects are a luggage cart, a man, and a white SUV. The luggage cart is positioned near the foreground, with its wheels visible. The man is standing to the right of the luggage cart, and the white SUV is in the background. The man is interacting with the luggage cart, possibly loading or unloading items.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25597.6, "ram_available_mb": 100174.6, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 25597.6, "ram_available_mb": 100174.6, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.07, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 32.56, "peak": 42.94, "min": 26.0}, "VIN": {"avg": 71.43, "peak": 101.69, "min": 53.68}, "VDD_CPU_SOC_MSS": {"avg": 15.67, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 32.56, "energy_joules_est": 129.03, "sample_count": 31, "duration_seconds": 3.963}, "timestamp": "2026-01-16T15:28:23.622774"}
{"image_index": 99, "image_name": "000000009891.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009891.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 4061.096, "latencies_ms": [4061.096], "images_per_second": 0.246, "prompt_tokens": 21, "response_tokens_est": 75, "n_tiles": 12, "output_text": "The image depicts a scene inside a car dealership, specifically at the entrance where a group of people is gathered. They appear to be preparing to enter a white SUV, with one person carrying a black suitcase and another holding a black bag. The setting is indoors, with a high ceiling and artificial lighting, and there are various other vehicles and people in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25597.6, "ram_available_mb": 100174.6, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25597.8, "ram_available_mb": 100174.4, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.07, "peak": 15.54, "min": 14.53}, "VDD_GPU": {"avg": 32.14, "peak": 42.54, "min": 26.0}, "VIN": {"avg": 72.11, "peak": 112.28, "min": 58.78}, "VDD_CPU_SOC_MSS": {"avg": 15.66, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 32.14, "energy_joules_est": 130.53, "sample_count": 32, "duration_seconds": 4.061}, "timestamp": "2026-01-16T15:28:27.690365"}
{"image_index": 99, "image_name": "000000009891.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009891.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3483.558, "latencies_ms": [3483.558], "images_per_second": 0.287, "prompt_tokens": 19, "response_tokens_est": 58, "n_tiles": 12, "output_text": "The image depicts a scene inside a car dealership with a white SUV parked in the background. The lighting is bright, and the environment appears to be indoors with artificial lighting. The materials used in the scene include metal for the car and luggage, and the flooring is made of concrete.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25597.8, "ram_available_mb": 100174.4, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25599.9, "ram_available_mb": 100172.3, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.08, "peak": 15.65, "min": 14.43}, "VDD_GPU": {"avg": 33.16, "peak": 42.54, "min": 26.4}, "VIN": {"avg": 78.37, "peak": 127.38, "min": 58.5}, "VDD_CPU_SOC_MSS": {"avg": 15.64, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 33.16, "energy_joules_est": 115.53, "sample_count": 27, "duration_seconds": 3.484}, "timestamp": "2026-01-16T15:28:31.180626"}
{"image_index": 100, "image_name": "000000009914.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009914.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2583.811, "latencies_ms": [2583.811], "images_per_second": 0.387, "prompt_tokens": 9, "response_tokens_est": 35, "n_tiles": 12, "output_text": "The image shows a plate of food consisting of a grilled chicken sandwich with sesame seeds on top, accompanied by a side of French fries and a small bowl of sauce.", "error": null, "sys_before": {"cpu_percent": 44.3, "ram_used_mb": 25599.9, "ram_available_mb": 100172.3, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 25599.7, "ram_available_mb": 100172.5, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.11, "peak": 16.15, "min": 13.9}, "VDD_GPU": {"avg": 34.65, "peak": 41.36, "min": 27.19}, "VIN": {"avg": 73.57, "peak": 112.01, "min": 57.16}, "VDD_CPU_SOC_MSS": {"avg": 15.85, "peak": 17.32, "min": 14.96}}, "power_watts_avg": 34.65, "energy_joules_est": 89.54, "sample_count": 19, "duration_seconds": 2.584}, "timestamp": "2026-01-16T15:28:33.859180"}
{"image_index": 100, "image_name": "000000009914.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009914.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 3048.879, "latencies_ms": [3048.879], "images_per_second": 0.328, "prompt_tokens": 23, "response_tokens_est": 45, "n_tiles": 12, "output_text": "1. Bun\n2. Potato chips\n3. Cheese\n4. Sausage\n5. Sausage\n6. Sausage\n7. Sausage\n8. Sausage", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25599.7, "ram_available_mb": 100172.5, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 25599.9, "ram_available_mb": 100172.3, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.05, "peak": 15.65, "min": 14.43}, "VDD_GPU": {"avg": 34.44, "peak": 42.94, "min": 26.79}, "VIN": {"avg": 75.06, "peak": 111.57, "min": 59.01}, "VDD_CPU_SOC_MSS": {"avg": 15.66, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 34.44, "energy_joules_est": 105.02, "sample_count": 23, "duration_seconds": 3.049}, "timestamp": "2026-01-16T15:28:36.914876"}
{"image_index": 100, "image_name": "000000009914.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009914.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 4024.721, "latencies_ms": [4024.721], "images_per_second": 0.248, "prompt_tokens": 27, "response_tokens_est": 74, "n_tiles": 12, "output_text": "The main objects in the image are a hamburger and a side of fries. The hamburger is positioned in the foreground, with a sesame seed bun and a grilled patty. The fries are in the background, slightly out of focus. The hamburger is placed on a white plate, and there is a small blue bowl containing a sauce next to it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25599.9, "ram_available_mb": 100172.3, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25598.4, "ram_available_mb": 100173.8, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.08, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 32.42, "peak": 42.54, "min": 26.0}, "VIN": {"avg": 73.44, "peak": 111.47, "min": 56.38}, "VDD_CPU_SOC_MSS": {"avg": 15.7, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 32.42, "energy_joules_est": 130.5, "sample_count": 31, "duration_seconds": 4.025}, "timestamp": "2026-01-16T15:28:40.945767"}
{"image_index": 100, "image_name": "000000009914.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009914.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3249.344, "latencies_ms": [3249.344], "images_per_second": 0.308, "prompt_tokens": 21, "response_tokens_est": 51, "n_tiles": 12, "output_text": "The image depicts a plate of food, specifically a hamburger and fries, accompanied by a side of ketchup. The setting appears to be a casual dining environment, possibly a fast-food restaurant, with a focus on the food items.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25598.4, "ram_available_mb": 100173.8, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 25598.4, "ram_available_mb": 100173.8, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.08, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 33.79, "peak": 42.53, "min": 26.39}, "VIN": {"avg": 75.5, "peak": 108.15, "min": 58.45}, "VDD_CPU_SOC_MSS": {"avg": 15.67, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 33.79, "energy_joules_est": 109.82, "sample_count": 25, "duration_seconds": 3.25}, "timestamp": "2026-01-16T15:28:44.202206"}
{"image_index": 100, "image_name": "000000009914.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000009914.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3579.175, "latencies_ms": [3579.175], "images_per_second": 0.279, "prompt_tokens": 19, "response_tokens_est": 61, "n_tiles": 12, "output_text": "The image features a plate of food, including a grilled chicken sandwich with sesame seeds, a side of fries, and a small bowl of sauce. The lighting is bright, and the colors are vibrant, with the golden-brown fries contrasting against the white plate and the dark blue tabletop.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25598.4, "ram_available_mb": 100173.8, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 25598.6, "ram_available_mb": 100173.6, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.08, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 33.26, "peak": 42.15, "min": 26.39}, "VIN": {"avg": 75.51, "peak": 119.51, "min": 58.56}, "VDD_CPU_SOC_MSS": {"avg": 15.62, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 33.26, "energy_joules_est": 119.07, "sample_count": 27, "duration_seconds": 3.58}, "timestamp": "2026-01-16T15:28:47.789569"}
{"image_index": 101, "image_name": "000000010092.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000010092.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1923.073, "latencies_ms": [1923.073], "images_per_second": 0.52, "prompt_tokens": 9, "response_tokens_est": 50, "n_tiles": 6, "output_text": "The image depicts a cozy, well-lit room with a large bed covered by a green mosquito net, a small wooden table with a candle, and a wooden chair, all set against a backdrop of yellow walls and large windows with curtains.", "error": null, "sys_before": {"cpu_percent": 16.3, "ram_used_mb": 25598.6, "ram_available_mb": 100173.6, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 25598.4, "ram_available_mb": 100173.8, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.64, "peak": 15.22, "min": 13.92}, "VDD_GPU": {"avg": 30.13, "peak": 38.21, "min": 24.42}, "VIN": {"avg": 75.72, "peak": 117.29, "min": 60.3}, "VDD_CPU_SOC_MSS": {"avg": 15.27, "peak": 15.75, "min": 14.57}}, "power_watts_avg": 30.13, "energy_joules_est": 57.95, "sample_count": 14, "duration_seconds": 1.923}, "timestamp": "2026-01-16T15:28:49.751610"}
{"image_index": 101, "image_name": "000000010092.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000010092.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1646.513, "latencies_ms": [1646.513], "images_per_second": 0.607, "prompt_tokens": 23, "response_tokens_est": 38, "n_tiles": 6, "output_text": "bed: 1\ntable: 1\nchair: 1\ntable lamp: 1\nwindow: 2\ncurtain: 2\nwall art: 2", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25598.4, "ram_available_mb": 100173.8, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 25598.4, "ram_available_mb": 100173.8, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.79, "peak": 15.11, "min": 14.22}, "VDD_GPU": {"avg": 31.64, "peak": 37.82, "min": 25.21}, "VIN": {"avg": 68.59, "peak": 111.78, "min": 55.53}, "VDD_CPU_SOC_MSS": {"avg": 15.32, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 31.64, "energy_joules_est": 52.11, "sample_count": 12, "duration_seconds": 1.647}, "timestamp": "2026-01-16T15:28:51.406469"}
{"image_index": 101, "image_name": "000000010092.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000010092.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3167.365, "latencies_ms": [3167.365], "images_per_second": 0.316, "prompt_tokens": 27, "response_tokens_est": 95, "n_tiles": 6, "output_text": "The main object in the foreground is a bed with a green canopy, which is positioned near the center of the image. The bed is surrounded by a wooden frame and has a green blanket on it. In the background, there is a wooden table with a lamp on it, and a painting hanging on the wall. The painting is located to the right of the table. The room has a tiled floor and large windows with curtains, allowing natural light to enter.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25598.4, "ram_available_mb": 100173.8, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 25598.6, "ram_available_mb": 100173.6, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.02, "peak": 15.32, "min": 14.22}, "VDD_GPU": {"avg": 27.87, "peak": 38.21, "min": 23.64}, "VIN": {"avg": 68.55, "peak": 123.55, "min": 56.79}, "VDD_CPU_SOC_MSS": {"avg": 15.55, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 27.87, "energy_joules_est": 88.29, "sample_count": 24, "duration_seconds": 3.168}, "timestamp": "2026-01-16T15:28:54.580111"}
{"image_index": 101, "image_name": "000000010092.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000010092.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2338.021, "latencies_ms": [2338.021], "images_per_second": 0.428, "prompt_tokens": 21, "response_tokens_est": 64, "n_tiles": 6, "output_text": "The image depicts a cozy, well-lit room with a large bed covered in a green mosquito net. The room features wooden furniture, including a table and chairs, and is decorated with paintings and curtains. The setting appears to be a bedroom or a guest room, with natural light streaming in through large windows.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25598.6, "ram_available_mb": 100173.6, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 25598.4, "ram_available_mb": 100173.8, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.02, "peak": 15.32, "min": 14.32}, "VDD_GPU": {"avg": 28.16, "peak": 38.19, "min": 23.64}, "VIN": {"avg": 68.26, "peak": 111.1, "min": 61.06}, "VDD_CPU_SOC_MSS": {"avg": 15.53, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 28.16, "energy_joules_est": 65.85, "sample_count": 16, "duration_seconds": 2.338}, "timestamp": "2026-01-16T15:28:56.924371"}
{"image_index": 101, "image_name": "000000010092.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000010092.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1803.008, "latencies_ms": [1803.008], "images_per_second": 0.555, "prompt_tokens": 19, "response_tokens_est": 44, "n_tiles": 6, "output_text": "The room is brightly lit with natural light streaming through large windows adorned with sheer curtains. The walls are painted in a warm, earthy tone, complemented by wooden furniture and a green canopy bed.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25598.4, "ram_available_mb": 100173.8, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 25597.9, "ram_available_mb": 100174.3, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.88, "peak": 15.22, "min": 14.22}, "VDD_GPU": {"avg": 30.03, "peak": 37.8, "min": 24.42}, "VIN": {"avg": 66.95, "peak": 98.74, "min": 52.75}, "VDD_CPU_SOC_MSS": {"avg": 15.42, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 30.03, "energy_joules_est": 54.16, "sample_count": 12, "duration_seconds": 1.803}, "timestamp": "2026-01-16T15:28:58.733752"}
{"image_index": 102, "image_name": "000000010363.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000010363.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 691.421, "latencies_ms": [691.421], "images_per_second": 1.446, "prompt_tokens": 9, "response_tokens_est": 23, "n_tiles": 2, "output_text": "A gray and white cat is perched on the hood of a black car, seemingly curious about the surroundings.", "error": null, "sys_before": {"cpu_percent": 26.1, "ram_used_mb": 25597.9, "ram_available_mb": 100174.3, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25598.1, "ram_available_mb": 100174.1, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5055.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.59, "peak": 15.01, "min": 14.2}, "VDD_GPU": {"avg": 28.21, "peak": 33.48, "min": 24.43}, "VIN": {"avg": 64.91, "peak": 75.46, "min": 55.74}, "VDD_CPU_SOC_MSS": {"avg": 15.12, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 28.21, "energy_joules_est": 19.52, "sample_count": 5, "duration_seconds": 0.692}, "timestamp": "2026-01-16T15:28:59.444350"}
{"image_index": 102, "image_name": "000000010363.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000010363.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 968.616, "latencies_ms": [968.616], "images_per_second": 1.032, "prompt_tokens": 23, "response_tokens_est": 34, "n_tiles": 2, "output_text": "1. Cat\n2. Car\n3. Lamp\n4. Floor\n5. Floor lamp\n6. Table\n7. Table lamp\n8. Box", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25598.1, "ram_available_mb": 100174.1, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25597.2, "ram_available_mb": 100175.0, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5064.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.97, "peak": 15.52, "min": 14.4}, "VDD_GPU": {"avg": 27.01, "peak": 33.09, "min": 22.85}, "VIN": {"avg": 69.45, "peak": 101.64, "min": 60.37}, "VDD_CPU_SOC_MSS": {"avg": 15.52, "peak": 16.14, "min": 14.96}}, "power_watts_avg": 27.01, "energy_joules_est": 26.17, "sample_count": 7, "duration_seconds": 0.969}, "timestamp": "2026-01-16T15:29:00.418765"}
{"image_index": 102, "image_name": "000000010363.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000010363.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2374.977, "latencies_ms": [2374.977], "images_per_second": 0.421, "prompt_tokens": 27, "response_tokens_est": 94, "n_tiles": 2, "output_text": "In the image, there is a black car parked in the foreground, with a cat standing on its roof. The cat is positioned near the front of the car, near the windshield. In the background, there is a cluttered kitchen counter with various items, including a lamp, a box, and a bicycle. The kitchen counter is situated near the car, indicating a spatial relationship where the car is in the foreground and the kitchen counter is in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25597.2, "ram_available_mb": 100175.0, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25597.2, "ram_available_mb": 100175.0, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5067.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.18, "peak": 15.42, "min": 14.71}, "VDD_GPU": {"avg": 23.01, "peak": 31.5, "min": 20.49}, "VIN": {"avg": 65.35, "peak": 101.59, "min": 55.55}, "VDD_CPU_SOC_MSS": {"avg": 16.1, "peak": 16.54, "min": 15.35}}, "power_watts_avg": 23.01, "energy_joules_est": 54.67, "sample_count": 18, "duration_seconds": 2.376}, "timestamp": "2026-01-16T15:29:02.801125"}
{"image_index": 102, "image_name": "000000010363.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000010363.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1165.007, "latencies_ms": [1165.007], "images_per_second": 0.858, "prompt_tokens": 21, "response_tokens_est": 42, "n_tiles": 2, "output_text": "The image depicts a cozy, cluttered kitchen with various items on the countertops and shelves. A tabby cat is perched on the hood of a black car, seemingly curious about the surroundings.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25597.2, "ram_available_mb": 100175.0, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25597.2, "ram_available_mb": 100175.0, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5063.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.01, "peak": 15.42, "min": 14.51}, "VDD_GPU": {"avg": 24.25, "peak": 28.36, "min": 21.66}, "VIN": {"avg": 62.92, "peak": 64.21, "min": 60.32}, "VDD_CPU_SOC_MSS": {"avg": 15.81, "peak": 16.14, "min": 15.36}}, "power_watts_avg": 24.25, "energy_joules_est": 28.26, "sample_count": 7, "duration_seconds": 1.165}, "timestamp": "2026-01-16T15:29:03.972328"}
{"image_index": 102, "image_name": "000000010363.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000010363.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1286.283, "latencies_ms": [1286.283], "images_per_second": 0.777, "prompt_tokens": 19, "response_tokens_est": 47, "n_tiles": 2, "output_text": "The image features a black car with a reflective surface, and a gray and white striped cat is perched on the roof. The lighting is bright, and the scene appears to be indoors, possibly in a garage or workshop.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25597.2, "ram_available_mb": 100175.0, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25597.2, "ram_available_mb": 100175.0, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5062.6, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.09, "peak": 15.42, "min": 14.61}, "VDD_GPU": {"avg": 24.9, "peak": 31.51, "min": 21.27}, "VIN": {"avg": 65.76, "peak": 85.15, "min": 61.23}, "VDD_CPU_SOC_MSS": {"avg": 15.75, "peak": 16.14, "min": 15.36}}, "power_watts_avg": 24.9, "energy_joules_est": 32.04, "sample_count": 9, "duration_seconds": 1.287}, "timestamp": "2026-01-16T15:29:05.264472"}
{"image_index": 103, "image_name": "000000010583.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000010583.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1259.997, "latencies_ms": [1259.997], "images_per_second": 0.794, "prompt_tokens": 9, "response_tokens_est": 55, "n_tiles": 1, "output_text": "The image shows a plate of food consisting of a hearty meat dish, possibly a beef stew or shepherd's pie, topped with a layer of mashed potatoes and a generous amount of gravy, garnished with chopped herbs and a few slices of roasted tomato.", "error": null, "sys_before": {"cpu_percent": 5.6, "ram_used_mb": 25597.2, "ram_available_mb": 100175.0, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25597.2, "ram_available_mb": 100175.0, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 4845.7, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.59, "peak": 15.82, "min": 15.11}, "VDD_GPU": {"avg": 21.8, "peak": 24.82, "min": 20.1}, "VIN": {"avg": 62.74, "peak": 66.21, "min": 60.68}, "VDD_CPU_SOC_MSS": {"avg": 16.23, "peak": 16.54, "min": 15.74}}, "power_watts_avg": 21.8, "energy_joules_est": 27.48, "sample_count": 9, "duration_seconds": 1.261}, "timestamp": "2026-01-16T15:29:06.542664"}
{"image_index": 103, "image_name": "000000010583.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000010583.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2853.036, "latencies_ms": [2853.036], "images_per_second": 0.351, "prompt_tokens": 23, "response_tokens_est": 128, "n_tiles": 1, "output_text": "object: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject:", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25597.2, "ram_available_mb": 100175.0, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25597.4, "ram_available_mb": 100174.7, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 4856.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.75, "peak": 15.93, "min": 15.52}, "VDD_GPU": {"avg": 20.42, "peak": 22.86, "min": 19.7}, "VIN": {"avg": 63.48, "peak": 69.68, "min": 56.94}, "VDD_CPU_SOC_MSS": {"avg": 16.52, "peak": 16.54, "min": 16.14}}, "power_watts_avg": 20.42, "energy_joules_est": 58.26, "sample_count": 21, "duration_seconds": 2.853}, "timestamp": "2026-01-16T15:29:09.401520"}
{"image_index": 103, "image_name": "000000010583.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000010583.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1239.453, "latencies_ms": [1239.453], "images_per_second": 0.807, "prompt_tokens": 27, "response_tokens_est": 54, "n_tiles": 1, "output_text": "The main object in the foreground is a plate of food, which includes a beef dish topped with a creamy sauce and garnished with herbs. The plate is placed on a metal table, and there is a small portion of food in the background on a different plate.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25597.4, "ram_available_mb": 100174.7, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 25598.3, "ram_available_mb": 100173.9, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 4859.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.61, "peak": 15.72, "min": 15.42}, "VDD_GPU": {"avg": 21.0, "peak": 22.07, "min": 20.09}, "VIN": {"avg": 65.5, "peak": 68.57, "min": 63.55}, "VDD_CPU_SOC_MSS": {"avg": 16.54, "peak": 16.93, "min": 16.14}}, "power_watts_avg": 21.0, "energy_joules_est": 26.04, "sample_count": 7, "duration_seconds": 1.24}, "timestamp": "2026-01-16T15:29:10.651227"}
{"image_index": 103, "image_name": "000000010583.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000010583.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1325.589, "latencies_ms": [1325.589], "images_per_second": 0.754, "prompt_tokens": 21, "response_tokens_est": 58, "n_tiles": 1, "output_text": "The image depicts a plate of food, specifically a savory dish consisting of a meatloaf topped with a layer of mashed potatoes and a generous amount of gravy. The plate is placed on a metal table, and there is a side of fries visible in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25598.3, "ram_available_mb": 100173.9, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25598.3, "ram_available_mb": 100173.9, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 4854.7, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.51, "peak": 15.72, "min": 15.11}, "VDD_GPU": {"avg": 21.27, "peak": 24.02, "min": 20.09}, "VIN": {"avg": 64.93, "peak": 72.11, "min": 61.04}, "VDD_CPU_SOC_MSS": {"avg": 16.46, "peak": 16.54, "min": 16.15}}, "power_watts_avg": 21.27, "energy_joules_est": 28.21, "sample_count": 10, "duration_seconds": 1.326}, "timestamp": "2026-01-16T15:29:11.983190"}
{"image_index": 103, "image_name": "000000010583.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000010583.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1235.247, "latencies_ms": [1235.247], "images_per_second": 0.81, "prompt_tokens": 19, "response_tokens_est": 54, "n_tiles": 1, "output_text": "The plate is white, with a glossy finish, and is placed on a metal table. The food consists of a beef dish with a creamy sauce, topped with roasted vegetables and herbs. The lighting is bright, enhancing the colors of the food and the table.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25598.3, "ram_available_mb": 100173.9, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25598.5, "ram_available_mb": 100173.6, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 4853.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.62, "peak": 15.82, "min": 15.32}, "VDD_GPU": {"avg": 21.45, "peak": 24.04, "min": 20.1}, "VIN": {"avg": 63.1, "peak": 68.01, "min": 60.63}, "VDD_CPU_SOC_MSS": {"avg": 16.45, "peak": 16.54, "min": 16.14}}, "power_watts_avg": 21.45, "energy_joules_est": 26.51, "sample_count": 9, "duration_seconds": 1.236}, "timestamp": "2026-01-16T15:29:13.225229"}
{"image_index": 104, "image_name": "000000010707.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000010707.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2852.396, "latencies_ms": [2852.396], "images_per_second": 0.351, "prompt_tokens": 9, "response_tokens_est": 43, "n_tiles": 12, "output_text": "The image depicts a group of people gathered around a table in a cozy living room, with a man in a white t-shirt holding a controller and another man in a blue t-shirt playing a video game.", "error": null, "sys_before": {"cpu_percent": 24.5, "ram_used_mb": 25598.5, "ram_available_mb": 100173.6, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 25598.3, "ram_available_mb": 100173.9, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.26, "peak": 16.15, "min": 14.81}, "VDD_GPU": {"avg": 33.3, "peak": 40.57, "min": 26.4}, "VIN": {"avg": 74.4, "peak": 114.16, "min": 54.6}, "VDD_CPU_SOC_MSS": {"avg": 15.94, "peak": 17.32, "min": 15.35}}, "power_watts_avg": 33.3, "energy_joules_est": 95.0, "sample_count": 22, "duration_seconds": 2.853}, "timestamp": "2026-01-16T15:29:16.122226"}
{"image_index": 104, "image_name": "000000010707.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000010707.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2673.707, "latencies_ms": [2673.707], "images_per_second": 0.374, "prompt_tokens": 23, "response_tokens_est": 34, "n_tiles": 12, "output_text": "1. Couch\n2. Table\n3. People\n4. Laptop\n5. Bottle\n6. Beverage\n7. Bottle\n8. Beverage", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25598.3, "ram_available_mb": 100173.9, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 25598.3, "ram_available_mb": 100173.9, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.06, "peak": 15.65, "min": 14.43}, "VDD_GPU": {"avg": 35.55, "peak": 42.94, "min": 27.18}, "VIN": {"avg": 76.58, "peak": 111.58, "min": 58.76}, "VDD_CPU_SOC_MSS": {"avg": 15.69, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 35.55, "energy_joules_est": 95.07, "sample_count": 20, "duration_seconds": 2.674}, "timestamp": "2026-01-16T15:29:18.803021"}
{"image_index": 104, "image_name": "000000010707.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000010707.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 4328.606, "latencies_ms": [4328.606], "images_per_second": 0.231, "prompt_tokens": 27, "response_tokens_est": 83, "n_tiles": 12, "output_text": "The main objects in the image are a group of people and various items on a table. The people are seated on a couch, with one person holding a bottle of beer. The table is situated in the foreground, with various items such as cans, a remote control, and a bottle of beer on it. The background features a couch and a lamp, while the table is the central focus of the scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25598.3, "ram_available_mb": 100173.9, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25597.1, "ram_available_mb": 100175.1, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.08, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 32.02, "peak": 42.94, "min": 26.0}, "VIN": {"avg": 73.09, "peak": 110.36, "min": 58.9}, "VDD_CPU_SOC_MSS": {"avg": 15.6, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 32.02, "energy_joules_est": 138.61, "sample_count": 33, "duration_seconds": 4.329}, "timestamp": "2026-01-16T15:29:23.139694"}
{"image_index": 104, "image_name": "000000010707.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000010707.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3784.485, "latencies_ms": [3784.485], "images_per_second": 0.264, "prompt_tokens": 21, "response_tokens_est": 67, "n_tiles": 12, "output_text": "The image depicts a cozy living room scene with a group of people gathered around a table. The room is warmly lit, and various items such as a laptop, a bottle, and a cup are scattered on the table. The individuals appear to be engaged in a casual, social gathering, possibly enjoying drinks and snacks together.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25597.1, "ram_available_mb": 100175.1, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25597.1, "ram_available_mb": 100175.1, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.08, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 32.67, "peak": 42.54, "min": 26.01}, "VIN": {"avg": 74.11, "peak": 133.51, "min": 56.82}, "VDD_CPU_SOC_MSS": {"avg": 15.56, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 32.67, "energy_joules_est": 123.65, "sample_count": 29, "duration_seconds": 3.785}, "timestamp": "2026-01-16T15:29:26.930990"}
{"image_index": 104, "image_name": "000000010707.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000010707.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2977.058, "latencies_ms": [2977.058], "images_per_second": 0.336, "prompt_tokens": 19, "response_tokens_est": 43, "n_tiles": 12, "output_text": "The room is warmly lit with a yellowish hue, creating a cozy atmosphere. The furniture includes a red couch, a pink table, and a white sofa, all with a casual, lived-in feel.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25597.1, "ram_available_mb": 100175.1, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 25596.8, "ram_available_mb": 100175.3, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.07, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 34.39, "peak": 42.54, "min": 26.79}, "VIN": {"avg": 78.44, "peak": 118.61, "min": 56.92}, "VDD_CPU_SOC_MSS": {"avg": 15.64, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 34.39, "energy_joules_est": 102.4, "sample_count": 23, "duration_seconds": 2.978}, "timestamp": "2026-01-16T15:29:29.914550"}
{"image_index": 105, "image_name": "000000010764.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000010764.jpg", "image_width": 640, "image_height": 424, "image_resolution": "640x424", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1071.976, "latencies_ms": [1071.976], "images_per_second": 0.933, "prompt_tokens": 9, "response_tokens_est": 18, "n_tiles": 6, "output_text": "A baseball player is crouched on the field, ready to catch a ball.", "error": null, "sys_before": {"cpu_percent": 16.3, "ram_used_mb": 25596.8, "ram_available_mb": 100175.3, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 25598.1, "ram_available_mb": 100174.1, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.37, "peak": 14.71, "min": 13.92}, "VDD_GPU": {"avg": 34.07, "peak": 38.21, "min": 28.76}, "VIN": {"avg": 73.08, "peak": 111.67, "min": 51.09}, "VDD_CPU_SOC_MSS": {"avg": 15.01, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 34.07, "energy_joules_est": 36.54, "sample_count": 8, "duration_seconds": 1.072}, "timestamp": "2026-01-16T15:29:31.026074"}
{"image_index": 105, "image_name": "000000010764.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000010764.jpg", "image_width": 640, "image_height": 424, "image_resolution": "640x424", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2211.315, "latencies_ms": [2211.315], "images_per_second": 0.452, "prompt_tokens": 23, "response_tokens_est": 57, "n_tiles": 6, "output_text": "baseball player: 1\ncatcher's gear: 1\nbaseball bat: 1\nbaseball glove: 1\nbaseball uniform: 1\nbaseball helmet: 1\nbaseball bat: 1\nbaseball glove: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25598.1, "ram_available_mb": 100174.1, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 25597.9, "ram_available_mb": 100174.2, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.67, "peak": 15.01, "min": 13.92}, "VDD_GPU": {"avg": 29.94, "peak": 39.39, "min": 23.24}, "VIN": {"avg": 70.16, "peak": 127.66, "min": 56.11}, "VDD_CPU_SOC_MSS": {"avg": 15.17, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 29.94, "energy_joules_est": 66.23, "sample_count": 17, "duration_seconds": 2.212}, "timestamp": "2026-01-16T15:29:33.244019"}
{"image_index": 105, "image_name": "000000010764.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000010764.jpg", "image_width": 640, "image_height": 424, "image_resolution": "640x424", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2269.978, "latencies_ms": [2269.978], "images_per_second": 0.441, "prompt_tokens": 27, "response_tokens_est": 59, "n_tiles": 6, "output_text": "The main object in the foreground is a baseball player crouched near the home plate, wearing a black and white uniform. The player is holding a baseball glove near their left hand. In the background, there is a well-maintained green field with white lines marking the baseball diamond.", "error": null, "sys_before": {"cpu_percent": 50.0, "ram_used_mb": 25597.9, "ram_available_mb": 100174.2, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 25598.2, "ram_available_mb": 100174.0, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.71, "peak": 15.01, "min": 14.02}, "VDD_GPU": {"avg": 29.08, "peak": 37.82, "min": 23.24}, "VIN": {"avg": 72.04, "peak": 116.99, "min": 56.95}, "VDD_CPU_SOC_MSS": {"avg": 15.24, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 29.08, "energy_joules_est": 66.02, "sample_count": 17, "duration_seconds": 2.27}, "timestamp": "2026-01-16T15:29:35.520415"}
{"image_index": 105, "image_name": "000000010764.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000010764.jpg", "image_width": 640, "image_height": 424, "image_resolution": "640x424", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2774.686, "latencies_ms": [2774.686], "images_per_second": 0.36, "prompt_tokens": 21, "response_tokens_est": 77, "n_tiles": 6, "output_text": "The image captures a baseball player in a white and black uniform crouched on the dirt infield of a baseball field. The player is wearing a protective helmet, gloves, and cleats, indicating that he is ready to catch a ball. The field is well-maintained with white lines marking the bases, and the grass is green, suggesting it is a sunny day.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25598.2, "ram_available_mb": 100174.0, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 25598.2, "ram_available_mb": 100174.0, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.77, "peak": 15.01, "min": 14.02}, "VDD_GPU": {"avg": 27.93, "peak": 38.21, "min": 23.25}, "VIN": {"avg": 68.03, "peak": 108.89, "min": 55.81}, "VDD_CPU_SOC_MSS": {"avg": 15.24, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 27.93, "energy_joules_est": 77.51, "sample_count": 21, "duration_seconds": 2.775}, "timestamp": "2026-01-16T15:29:38.301627"}
{"image_index": 105, "image_name": "000000010764.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000010764.jpg", "image_width": 640, "image_height": 424, "image_resolution": "640x424", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2076.387, "latencies_ms": [2076.387], "images_per_second": 0.482, "prompt_tokens": 19, "response_tokens_est": 52, "n_tiles": 6, "output_text": "The baseball player is wearing a black and white uniform, complete with a helmet, chest protector, and batting gloves. The field is well-maintained with white chalk lines marking the playing area, and the lighting suggests it is daytime with clear skies.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25598.2, "ram_available_mb": 100174.0, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 25598.2, "ram_available_mb": 100174.0, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.7, "peak": 15.01, "min": 14.02}, "VDD_GPU": {"avg": 29.42, "peak": 38.21, "min": 23.64}, "VIN": {"avg": 71.48, "peak": 114.42, "min": 56.66}, "VDD_CPU_SOC_MSS": {"avg": 15.21, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 29.42, "energy_joules_est": 61.1, "sample_count": 16, "duration_seconds": 2.077}, "timestamp": "2026-01-16T15:29:40.384322"}
{"image_index": 106, "image_name": "000000010977.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000010977.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2826.538, "latencies_ms": [2826.538], "images_per_second": 0.354, "prompt_tokens": 9, "response_tokens_est": 42, "n_tiles": 12, "output_text": "The image shows a bathroom with a wooden vanity, a white toilet, a pink tiled wall, a white towel hanging on the wall, a glass shower door, and a green mat on the floor.", "error": null, "sys_before": {"cpu_percent": 51.8, "ram_used_mb": 25597.9, "ram_available_mb": 100174.3, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 25597.2, "ram_available_mb": 100175.0, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.08, "peak": 16.15, "min": 13.79}, "VDD_GPU": {"avg": 33.46, "peak": 41.36, "min": 26.39}, "VIN": {"avg": 77.55, "peak": 120.55, "min": 58.39}, "VDD_CPU_SOC_MSS": {"avg": 15.69, "peak": 17.32, "min": 14.57}}, "power_watts_avg": 33.46, "energy_joules_est": 94.59, "sample_count": 22, "duration_seconds": 2.827}, "timestamp": "2026-01-16T15:29:43.294519"}
{"image_index": 106, "image_name": "000000010977.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000010977.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 3381.885, "latencies_ms": [3381.885], "images_per_second": 0.296, "prompt_tokens": 23, "response_tokens_est": 55, "n_tiles": 12, "output_text": "toilet: 1\nbathroom sink: 1\nbathroom mirror: 1\nbathroom curtain: 1\nbathroom door: 1\nbathroom floor tiles: 1\nbathroom rug: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25597.2, "ram_available_mb": 100175.0, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 25597.1, "ram_available_mb": 100175.0, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.06, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 33.48, "peak": 42.54, "min": 26.39}, "VIN": {"avg": 74.09, "peak": 110.25, "min": 55.43}, "VDD_CPU_SOC_MSS": {"avg": 15.58, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 33.48, "energy_joules_est": 113.24, "sample_count": 26, "duration_seconds": 3.382}, "timestamp": "2026-01-16T15:29:46.682999"}
{"image_index": 106, "image_name": "000000010977.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000010977.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 5715.596, "latencies_ms": [5715.596], "images_per_second": 0.175, "prompt_tokens": 27, "response_tokens_est": 124, "n_tiles": 12, "output_text": "The main objects in the image are a toilet and a bathtub. The toilet is located in the foreground, while the bathtub is positioned to the left of the toilet. The bathtub is partially visible, with its edge and part of the interior visible. The toilet is situated near the bathtub, with its lid and part of the tank visible. The bathroom has a wooden cabinet to the right of the toilet, and a mirror above the bathtub. The walls are painted in a light blue color, and there is a white curtain partially visible on the right side of the image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25597.1, "ram_available_mb": 100175.0, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 25597.1, "ram_available_mb": 100175.0, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.1, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 30.37, "peak": 42.94, "min": 26.0}, "VIN": {"avg": 74.05, "peak": 121.36, "min": 57.16}, "VDD_CPU_SOC_MSS": {"avg": 15.66, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 30.37, "energy_joules_est": 173.6, "sample_count": 45, "duration_seconds": 5.716}, "timestamp": "2026-01-16T15:29:52.407869"}
{"image_index": 106, "image_name": "000000010977.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000010977.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2577.069, "latencies_ms": [2577.069], "images_per_second": 0.388, "prompt_tokens": 21, "response_tokens_est": 31, "n_tiles": 12, "output_text": "The image shows a bathroom with a wooden vanity and a white toilet. The room is well-lit, and there is a shower door partially open.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25597.1, "ram_available_mb": 100175.0, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 25597.1, "ram_available_mb": 100175.1, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.05, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 35.59, "peak": 42.54, "min": 27.57}, "VIN": {"avg": 73.44, "peak": 108.75, "min": 45.14}, "VDD_CPU_SOC_MSS": {"avg": 15.73, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 35.59, "energy_joules_est": 91.73, "sample_count": 20, "duration_seconds": 2.577}, "timestamp": "2026-01-16T15:29:54.991281"}
{"image_index": 106, "image_name": "000000010977.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000010977.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2942.509, "latencies_ms": [2942.509], "images_per_second": 0.34, "prompt_tokens": 19, "response_tokens_est": 42, "n_tiles": 12, "output_text": "The bathroom features a pinkish-beige tiled wall, a white toilet, and a wooden vanity with a light brown finish. The lighting is bright, and the overall color scheme is warm and inviting.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25597.1, "ram_available_mb": 100175.1, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 25597.1, "ram_available_mb": 100175.1, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.07, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 34.92, "peak": 42.94, "min": 26.79}, "VIN": {"avg": 78.6, "peak": 114.41, "min": 59.35}, "VDD_CPU_SOC_MSS": {"avg": 15.76, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 34.92, "energy_joules_est": 102.76, "sample_count": 22, "duration_seconds": 2.943}, "timestamp": "2026-01-16T15:29:57.944007"}
{"image_index": 107, "image_name": "000000010995.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000010995.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2370.698, "latencies_ms": [2370.698], "images_per_second": 0.422, "prompt_tokens": 9, "response_tokens_est": 28, "n_tiles": 12, "output_text": "The image shows a neatly made bed with a plaid bedspread, a window with curtains, and a lamp on the wall.", "error": null, "sys_before": {"cpu_percent": 41.2, "ram_used_mb": 25597.1, "ram_available_mb": 100175.1, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 25597.1, "ram_available_mb": 100175.1, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.02, "peak": 16.05, "min": 13.59}, "VDD_GPU": {"avg": 35.26, "peak": 41.36, "min": 27.59}, "VIN": {"avg": 74.88, "peak": 112.17, "min": 58.3}, "VDD_CPU_SOC_MSS": {"avg": 15.74, "peak": 16.92, "min": 14.57}}, "power_watts_avg": 35.26, "energy_joules_est": 83.6, "sample_count": 18, "duration_seconds": 2.371}, "timestamp": "2026-01-16T15:30:00.417446"}
{"image_index": 107, "image_name": "000000010995.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000010995.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2424.226, "latencies_ms": [2424.226], "images_per_second": 0.413, "prompt_tokens": 23, "response_tokens_est": 27, "n_tiles": 12, "output_text": "bed: 1\ncurtain: 1\nlamp: 1\nwindow: 1\nwall: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25597.1, "ram_available_mb": 100175.1, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 2.6, "ram_used_mb": 25598.1, "ram_available_mb": 100174.1, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.78, "peak": 15.24, "min": 14.12}, "VDD_GPU": {"avg": 36.74, "peak": 42.54, "min": 27.97}, "VIN": {"avg": 75.22, "peak": 101.65, "min": 58.44}, "VDD_CPU_SOC_MSS": {"avg": 15.44, "peak": 16.14, "min": 14.56}}, "power_watts_avg": 36.74, "energy_joules_est": 89.08, "sample_count": 18, "duration_seconds": 2.425}, "timestamp": "2026-01-16T15:30:02.848174"}
{"image_index": 107, "image_name": "000000010995.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000010995.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 4058.017, "latencies_ms": [4058.017], "images_per_second": 0.246, "prompt_tokens": 27, "response_tokens_est": 74, "n_tiles": 12, "output_text": "The image shows a room with a window on the left side, through which daylight is entering. The window is framed by a dark curtain. The main focus of the image is a plaid bedspread in the foreground, with a yellow pillow and a dark-colored headboard visible. The bed is positioned near the window, allowing natural light to illuminate the room.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25598.1, "ram_available_mb": 100174.1, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25597.9, "ram_available_mb": 100174.3, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.73, "peak": 15.34, "min": 14.12}, "VDD_GPU": {"avg": 32.13, "peak": 42.94, "min": 25.6}, "VIN": {"avg": 72.55, "peak": 118.54, "min": 58.34}, "VDD_CPU_SOC_MSS": {"avg": 15.23, "peak": 16.14, "min": 14.56}}, "power_watts_avg": 32.13, "energy_joules_est": 130.39, "sample_count": 32, "duration_seconds": 4.058}, "timestamp": "2026-01-16T15:30:06.912580"}
{"image_index": 107, "image_name": "000000010995.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000010995.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3400.498, "latencies_ms": [3400.498], "images_per_second": 0.294, "prompt_tokens": 21, "response_tokens_est": 55, "n_tiles": 12, "output_text": "The image depicts a cozy, dimly lit bedroom with a plaid bedspread and a window allowing natural light to filter in. The room appears to be in a residential setting, possibly a bedroom, with a dark-colored wall and a small lamp on the wall.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25597.9, "ram_available_mb": 100174.3, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 25597.8, "ram_available_mb": 100174.3, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.76, "peak": 15.34, "min": 14.12}, "VDD_GPU": {"avg": 33.12, "peak": 41.76, "min": 26.0}, "VIN": {"avg": 71.43, "peak": 110.35, "min": 58.43}, "VDD_CPU_SOC_MSS": {"avg": 15.25, "peak": 16.14, "min": 14.56}}, "power_watts_avg": 33.12, "energy_joules_est": 112.64, "sample_count": 26, "duration_seconds": 3.401}, "timestamp": "2026-01-16T15:30:10.319534"}
{"image_index": 107, "image_name": "000000010995.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000010995.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3160.17, "latencies_ms": [3160.17], "images_per_second": 0.316, "prompt_tokens": 19, "response_tokens_est": 48, "n_tiles": 12, "output_text": "The image shows a plaid bedspread with a yellow and green color scheme, featuring a checkered pattern. The room is dimly lit, with natural light coming through a window with white trim, creating a cozy and warm atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25597.8, "ram_available_mb": 100174.3, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 25597.8, "ram_available_mb": 100174.4, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.73, "peak": 15.34, "min": 14.12}, "VDD_GPU": {"avg": 33.7, "peak": 42.15, "min": 26.01}, "VIN": {"avg": 73.19, "peak": 113.16, "min": 58.36}, "VDD_CPU_SOC_MSS": {"avg": 15.25, "peak": 16.14, "min": 14.56}}, "power_watts_avg": 33.7, "energy_joules_est": 106.51, "sample_count": 24, "duration_seconds": 3.161}, "timestamp": "2026-01-16T15:30:13.490321"}
{"image_index": 108, "image_name": "000000011051.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011051.jpg", "image_width": 640, "image_height": 536, "image_resolution": "640x536", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2461.764, "latencies_ms": [2461.764], "images_per_second": 0.406, "prompt_tokens": 9, "response_tokens_est": 31, "n_tiles": 12, "output_text": "A young couple is posing for a photo at a formal event, with the man in a black suit and the woman in a sparkling black dress.", "error": null, "sys_before": {"cpu_percent": 46.8, "ram_used_mb": 25597.8, "ram_available_mb": 100174.4, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 25597.8, "ram_available_mb": 100174.4, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.0, "peak": 16.15, "min": 13.39}, "VDD_GPU": {"avg": 34.69, "peak": 40.97, "min": 27.18}, "VIN": {"avg": 71.14, "peak": 101.01, "min": 57.92}, "VDD_CPU_SOC_MSS": {"avg": 15.68, "peak": 17.32, "min": 14.18}}, "power_watts_avg": 34.69, "energy_joules_est": 85.41, "sample_count": 19, "duration_seconds": 2.462}, "timestamp": "2026-01-16T15:30:16.052278"}
{"image_index": 108, "image_name": "000000011051.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011051.jpg", "image_width": 640, "image_height": 536, "image_resolution": "640x536", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2679.368, "latencies_ms": [2679.368], "images_per_second": 0.373, "prompt_tokens": 23, "response_tokens_est": 34, "n_tiles": 12, "output_text": "1. Woman\n2. Man\n3. Rose\n4. Necklace\n5. Dress\n6. Bangle\n7. Ring\n8. Background", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25597.8, "ram_available_mb": 100174.4, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 25598.1, "ram_available_mb": 100174.1, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.06, "peak": 15.65, "min": 14.43}, "VDD_GPU": {"avg": 35.3, "peak": 42.94, "min": 27.19}, "VIN": {"avg": 79.53, "peak": 118.6, "min": 58.03}, "VDD_CPU_SOC_MSS": {"avg": 15.63, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 35.3, "energy_joules_est": 94.6, "sample_count": 21, "duration_seconds": 2.68}, "timestamp": "2026-01-16T15:30:18.739195"}
{"image_index": 108, "image_name": "000000011051.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011051.jpg", "image_width": 640, "image_height": 536, "image_resolution": "640x536", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3891.299, "latencies_ms": [3891.299], "images_per_second": 0.257, "prompt_tokens": 27, "response_tokens_est": 70, "n_tiles": 12, "output_text": "The main objects in the image are a man and a woman. The man is in the foreground, wearing a black suit and tie, while the woman is in the background, dressed in a black dress with a beige belt. The man is holding a white rose in his right hand, and the woman is smiling and looking towards the man.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25598.1, "ram_available_mb": 100174.1, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25597.7, "ram_available_mb": 100174.5, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.08, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 32.65, "peak": 42.94, "min": 26.0}, "VIN": {"avg": 75.4, "peak": 117.59, "min": 58.42}, "VDD_CPU_SOC_MSS": {"avg": 15.71, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 32.65, "energy_joules_est": 127.07, "sample_count": 30, "duration_seconds": 3.892}, "timestamp": "2026-01-16T15:30:22.637173"}
{"image_index": 108, "image_name": "000000011051.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011051.jpg", "image_width": 640, "image_height": 536, "image_resolution": "640x536", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 4458.12, "latencies_ms": [4458.12], "images_per_second": 0.224, "prompt_tokens": 21, "response_tokens_est": 87, "n_tiles": 12, "output_text": "The image depicts a young couple dressed in formal attire, likely at a wedding or a similar event. The man is wearing a black suit and tie, while the woman is dressed in a black dress with a sequined top and a beige belt. They are both smiling and appear to be enjoying the moment, with the man holding a white rose. The background is softly lit, suggesting an indoor setting with warm lighting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25597.7, "ram_available_mb": 100174.5, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 25597.9, "ram_available_mb": 100174.2, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.09, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 31.65, "peak": 42.53, "min": 26.0}, "VIN": {"avg": 72.18, "peak": 108.56, "min": 58.53}, "VDD_CPU_SOC_MSS": {"avg": 15.72, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 31.65, "energy_joules_est": 141.11, "sample_count": 35, "duration_seconds": 4.458}, "timestamp": "2026-01-16T15:30:27.101709"}
{"image_index": 108, "image_name": "000000011051.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011051.jpg", "image_width": 640, "image_height": 536, "image_resolution": "640x536", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3417.698, "latencies_ms": [3417.698], "images_per_second": 0.293, "prompt_tokens": 19, "response_tokens_est": 56, "n_tiles": 12, "output_text": "The image features a young woman with dark hair styled in a bun, wearing a black dress with a sequined top and a beige belt. The lighting is warm, casting a soft glow on her face, and the background is neutral, ensuring the focus remains on her.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25597.9, "ram_available_mb": 100174.2, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 25597.9, "ram_available_mb": 100174.2, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.07, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 33.56, "peak": 42.54, "min": 26.4}, "VIN": {"avg": 72.31, "peak": 113.17, "min": 56.69}, "VDD_CPU_SOC_MSS": {"avg": 15.62, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 33.56, "energy_joules_est": 114.71, "sample_count": 26, "duration_seconds": 3.418}, "timestamp": "2026-01-16T15:30:30.526760"}
{"image_index": 109, "image_name": "000000011122.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011122.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2689.778, "latencies_ms": [2689.778], "images_per_second": 0.372, "prompt_tokens": 9, "response_tokens_est": 38, "n_tiles": 12, "output_text": "The image shows a chain-link fence enclosing a grassy area with a stop sign attached to it, and behind the fence, there are palm trees and a building with balconies.", "error": null, "sys_before": {"cpu_percent": 43.5, "ram_used_mb": 25597.9, "ram_available_mb": 100174.2, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 25598.8, "ram_available_mb": 100173.4, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.06, "peak": 16.05, "min": 13.59}, "VDD_GPU": {"avg": 34.16, "peak": 41.76, "min": 26.79}, "VIN": {"avg": 76.14, "peak": 114.53, "min": 58.69}, "VDD_CPU_SOC_MSS": {"avg": 15.88, "peak": 17.32, "min": 14.57}}, "power_watts_avg": 34.16, "energy_joules_est": 91.89, "sample_count": 21, "duration_seconds": 2.69}, "timestamp": "2026-01-16T15:30:33.313149"}
{"image_index": 109, "image_name": "000000011122.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011122.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 3111.892, "latencies_ms": [3111.892], "images_per_second": 0.321, "prompt_tokens": 23, "response_tokens_est": 47, "n_tiles": 12, "output_text": "- Stop sign: 1\n- Fence: 1\n- Bushes: 1\n- Greenery: 1\n- House: 1\n- Palm trees: 1\n- Building: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25598.8, "ram_available_mb": 100173.4, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 25598.8, "ram_available_mb": 100173.4, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.07, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 34.2, "peak": 42.94, "min": 26.39}, "VIN": {"avg": 79.56, "peak": 127.82, "min": 58.3}, "VDD_CPU_SOC_MSS": {"avg": 15.65, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 34.2, "energy_joules_est": 106.44, "sample_count": 24, "duration_seconds": 3.112}, "timestamp": "2026-01-16T15:30:36.431608"}
{"image_index": 109, "image_name": "000000011122.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011122.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3958.106, "latencies_ms": [3958.106], "images_per_second": 0.253, "prompt_tokens": 27, "response_tokens_est": 72, "n_tiles": 12, "output_text": "The main objects in the image are a chain-link fence, a stop sign, and a bush. The stop sign is positioned in the foreground, near the bottom of the image, while the bush is situated to the right of the fence. The background features a building and palm trees, indicating that the scene is likely set in a suburban or urban area.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25598.8, "ram_available_mb": 100173.4, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25598.5, "ram_available_mb": 100173.7, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.09, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 32.44, "peak": 42.94, "min": 26.0}, "VIN": {"avg": 80.44, "peak": 136.65, "min": 61.58}, "VDD_CPU_SOC_MSS": {"avg": 15.72, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 32.44, "energy_joules_est": 128.41, "sample_count": 31, "duration_seconds": 3.958}, "timestamp": "2026-01-16T15:30:40.396240"}
{"image_index": 109, "image_name": "000000011122.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011122.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3315.874, "latencies_ms": [3315.874], "images_per_second": 0.302, "prompt_tokens": 21, "response_tokens_est": 53, "n_tiles": 12, "output_text": "The image depicts a chain-link fence enclosing a grassy area with a few scattered objects and plants. The setting appears to be an outdoor area, possibly a park or a residential area, with a clear sky and some palm trees visible in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25598.5, "ram_available_mb": 100173.7, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 25597.0, "ram_available_mb": 100175.2, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.08, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 33.61, "peak": 42.54, "min": 26.39}, "VIN": {"avg": 76.77, "peak": 124.9, "min": 56.99}, "VDD_CPU_SOC_MSS": {"avg": 15.67, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 33.61, "energy_joules_est": 111.46, "sample_count": 25, "duration_seconds": 3.316}, "timestamp": "2026-01-16T15:30:43.718645"}
{"image_index": 109, "image_name": "000000011122.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011122.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3417.037, "latencies_ms": [3417.037], "images_per_second": 0.293, "prompt_tokens": 19, "response_tokens_est": 56, "n_tiles": 12, "output_text": "The image shows a chain-link fence with a red stop sign attached to it. The stop sign is prominently displayed in the foreground, surrounded by a green metal fence. The lighting is bright and natural, suggesting it is daytime. The weather appears to be clear and sunny.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25597.0, "ram_available_mb": 100175.2, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 25597.2, "ram_available_mb": 100175.0, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.06, "peak": 15.54, "min": 14.43}, "VDD_GPU": {"avg": 33.48, "peak": 42.94, "min": 26.39}, "VIN": {"avg": 72.95, "peak": 115.62, "min": 58.46}, "VDD_CPU_SOC_MSS": {"avg": 15.63, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 33.48, "energy_joules_est": 114.42, "sample_count": 27, "duration_seconds": 3.418}, "timestamp": "2026-01-16T15:30:47.142374"}
{"image_index": 110, "image_name": "000000011149.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011149.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2514.879, "latencies_ms": [2514.879], "images_per_second": 0.398, "prompt_tokens": 9, "response_tokens_est": 33, "n_tiles": 12, "output_text": "A man wearing a gray t-shirt and brown shorts is standing next to a bicycle with a basket, while another man in a black jacket is riding a motorcycle.", "error": null, "sys_before": {"cpu_percent": 45.0, "ram_used_mb": 25597.2, "ram_available_mb": 100175.0, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 25597.2, "ram_available_mb": 100175.0, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.17, "peak": 16.15, "min": 14.0}, "VDD_GPU": {"avg": 34.79, "peak": 41.76, "min": 27.18}, "VIN": {"avg": 84.77, "peak": 120.93, "min": 58.5}, "VDD_CPU_SOC_MSS": {"avg": 16.1, "peak": 17.32, "min": 14.96}}, "power_watts_avg": 34.79, "energy_joules_est": 87.5, "sample_count": 19, "duration_seconds": 2.515}, "timestamp": "2026-01-16T15:30:49.738579"}
{"image_index": 110, "image_name": "000000011149.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011149.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2882.108, "latencies_ms": [2882.108], "images_per_second": 0.347, "prompt_tokens": 23, "response_tokens_est": 40, "n_tiles": 12, "output_text": "1. Bicycle\n2. Motorcycle\n3. Bicycle\n4. Bicycle\n5. Bicycle\n6. Bicycle\n7. Bicycle\n8. Bicycle", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 25597.2, "ram_available_mb": 100175.0, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 25597.0, "ram_available_mb": 100175.1, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.07, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 35.11, "peak": 42.94, "min": 26.79}, "VIN": {"avg": 74.89, "peak": 120.55, "min": 49.14}, "VDD_CPU_SOC_MSS": {"avg": 15.67, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 35.11, "energy_joules_est": 101.21, "sample_count": 22, "duration_seconds": 2.883}, "timestamp": "2026-01-16T15:30:52.627277"}
{"image_index": 110, "image_name": "000000011149.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011149.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3455.842, "latencies_ms": [3455.842], "images_per_second": 0.289, "prompt_tokens": 27, "response_tokens_est": 57, "n_tiles": 12, "output_text": "The main objects in the image are a motorcycle and a bicycle. The motorcycle is positioned in the foreground, while the bicycle is in the background. The bicycle is parked near the motorcycle, indicating a spatial relationship where the motorcycle is closer to the foreground and the bicycle is further back.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25597.0, "ram_available_mb": 100175.1, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25597.0, "ram_available_mb": 100175.1, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.08, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 33.33, "peak": 42.94, "min": 26.39}, "VIN": {"avg": 76.44, "peak": 127.95, "min": 58.48}, "VDD_CPU_SOC_MSS": {"avg": 15.74, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 33.33, "energy_joules_est": 115.2, "sample_count": 27, "duration_seconds": 3.456}, "timestamp": "2026-01-16T15:30:56.090006"}
{"image_index": 110, "image_name": "000000011149.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011149.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3556.213, "latencies_ms": [3556.213], "images_per_second": 0.281, "prompt_tokens": 21, "response_tokens_est": 60, "n_tiles": 12, "output_text": "The scene depicts a casual outdoor setting with a man standing next to a bicycle. The man is wearing a light-colored t-shirt and dark shorts, and he is holding a brown bag. The bicycle is parked on the side of a paved path, surrounded by greenery and a fence.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25597.0, "ram_available_mb": 100175.1, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25597.0, "ram_available_mb": 100175.2, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.08, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 33.12, "peak": 42.94, "min": 26.39}, "VIN": {"avg": 76.77, "peak": 117.92, "min": 58.52}, "VDD_CPU_SOC_MSS": {"avg": 15.62, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 33.12, "energy_joules_est": 117.8, "sample_count": 28, "duration_seconds": 3.557}, "timestamp": "2026-01-16T15:30:59.652982"}
{"image_index": 110, "image_name": "000000011149.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011149.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3025.012, "latencies_ms": [3025.012], "images_per_second": 0.331, "prompt_tokens": 19, "response_tokens_est": 44, "n_tiles": 12, "output_text": "The image features a person riding a yellow bicycle, which stands out against the green foliage and concrete ground. The lighting is bright, indicating it is daytime, and the weather appears to be clear and sunny.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25597.0, "ram_available_mb": 100175.2, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 25597.8, "ram_available_mb": 100174.4, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.79, "peak": 15.34, "min": 14.12}, "VDD_GPU": {"avg": 34.44, "peak": 42.54, "min": 26.0}, "VIN": {"avg": 74.37, "peak": 116.93, "min": 58.4}, "VDD_CPU_SOC_MSS": {"avg": 15.4, "peak": 16.14, "min": 14.56}}, "power_watts_avg": 34.44, "energy_joules_est": 104.19, "sample_count": 23, "duration_seconds": 3.025}, "timestamp": "2026-01-16T15:31:02.688770"}
{"image_index": 111, "image_name": "000000011197.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011197.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1369.586, "latencies_ms": [1369.586], "images_per_second": 0.73, "prompt_tokens": 9, "response_tokens_est": 29, "n_tiles": 6, "output_text": "A man in a blue shirt is standing on a sidewalk next to a black street lamp, while a red car is driving on the street.", "error": null, "sys_before": {"cpu_percent": 39.5, "ram_used_mb": 25597.5, "ram_available_mb": 100174.6, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25597.5, "ram_available_mb": 100174.7, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.24, "peak": 14.91, "min": 13.69}, "VDD_GPU": {"avg": 32.14, "peak": 37.42, "min": 26.0}, "VIN": {"avg": 69.38, "peak": 104.18, "min": 57.51}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 32.14, "energy_joules_est": 44.03, "sample_count": 10, "duration_seconds": 1.37}, "timestamp": "2026-01-16T15:31:04.126328"}
{"image_index": 111, "image_name": "000000011197.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011197.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2300.363, "latencies_ms": [2300.363], "images_per_second": 0.435, "prompt_tokens": 23, "response_tokens_est": 60, "n_tiles": 6, "output_text": "- Pedestrian: 1\n- Car: 1\n- Street light: 1\n- Building: 1\n- Traffic sign: 2\n- Street sign: 2\n- Bicycle: 1\n- Person: 1\n- Street: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25597.5, "ram_available_mb": 100174.7, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 25597.8, "ram_available_mb": 100174.4, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.7, "peak": 15.01, "min": 14.02}, "VDD_GPU": {"avg": 29.17, "peak": 38.6, "min": 23.24}, "VIN": {"avg": 72.96, "peak": 119.43, "min": 58.48}, "VDD_CPU_SOC_MSS": {"avg": 15.24, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 29.17, "energy_joules_est": 67.11, "sample_count": 18, "duration_seconds": 2.301}, "timestamp": "2026-01-16T15:31:06.433025"}
{"image_index": 111, "image_name": "000000011197.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011197.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2273.398, "latencies_ms": [2273.398], "images_per_second": 0.44, "prompt_tokens": 27, "response_tokens_est": 59, "n_tiles": 6, "output_text": "The main objects in the image are a black street lamp, a red car, and a person standing near the sidewalk. The street lamp is positioned in the foreground, while the red car is in the background. The person is standing near the sidewalk, which is located in the middle ground.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25597.8, "ram_available_mb": 100174.4, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 25598.1, "ram_available_mb": 100174.1, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.72, "peak": 15.01, "min": 14.02}, "VDD_GPU": {"avg": 29.13, "peak": 38.21, "min": 23.24}, "VIN": {"avg": 67.77, "peak": 111.03, "min": 54.03}, "VDD_CPU_SOC_MSS": {"avg": 15.22, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 29.13, "energy_joules_est": 66.23, "sample_count": 17, "duration_seconds": 2.274}, "timestamp": "2026-01-16T15:31:08.712689"}
{"image_index": 111, "image_name": "000000011197.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011197.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3111.094, "latencies_ms": [3111.094], "images_per_second": 0.321, "prompt_tokens": 21, "response_tokens_est": 89, "n_tiles": 6, "output_text": "The image depicts an urban street scene with a pedestrian crossing sign indicating \"Procter & Gamble\" and \"Procter & Gamble\" directions. A man in a blue shirt is standing near the crossing, while a woman in a black jacket is walking across the street. The street is lined with buildings, trees, and a red car is visible in the background. The setting appears to be a city intersection during daytime.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25598.1, "ram_available_mb": 100174.1, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 25597.1, "ram_available_mb": 100175.1, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.8, "peak": 15.01, "min": 14.02}, "VDD_GPU": {"avg": 27.36, "peak": 37.8, "min": 23.24}, "VIN": {"avg": 70.11, "peak": 119.6, "min": 61.71}, "VDD_CPU_SOC_MSS": {"avg": 15.4, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 27.36, "energy_joules_est": 85.13, "sample_count": 24, "duration_seconds": 3.112}, "timestamp": "2026-01-16T15:31:11.830146"}
{"image_index": 111, "image_name": "000000011197.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011197.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2325.823, "latencies_ms": [2325.823], "images_per_second": 0.43, "prompt_tokens": 19, "response_tokens_est": 61, "n_tiles": 6, "output_text": "The image depicts a city street scene with a mix of urban and natural elements. Notable visual attributes include the red brick sidewalk, black metal street lamp, and a red car on the road. The lighting is natural, likely from the overcast sky, and the weather appears to be clear.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25597.1, "ram_available_mb": 100175.1, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 25596.3, "ram_available_mb": 100175.9, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.75, "peak": 15.01, "min": 14.12}, "VDD_GPU": {"avg": 28.74, "peak": 38.21, "min": 23.24}, "VIN": {"avg": 70.07, "peak": 110.8, "min": 60.8}, "VDD_CPU_SOC_MSS": {"avg": 15.29, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 28.74, "energy_joules_est": 66.85, "sample_count": 18, "duration_seconds": 2.326}, "timestamp": "2026-01-16T15:31:14.161779"}
{"image_index": 112, "image_name": "000000011511.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011511.jpg", "image_width": 640, "image_height": 464, "image_resolution": "640x464", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2186.027, "latencies_ms": [2186.027], "images_per_second": 0.457, "prompt_tokens": 9, "response_tokens_est": 23, "n_tiles": 12, "output_text": "The image shows a bronze statue of two women sitting on a bench, with one of them holding a bag.", "error": null, "sys_before": {"cpu_percent": 47.7, "ram_used_mb": 25596.3, "ram_available_mb": 100175.9, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 2.6, "ram_used_mb": 25596.3, "ram_available_mb": 100175.9, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.09, "peak": 16.05, "min": 13.79}, "VDD_GPU": {"avg": 35.31, "peak": 40.57, "min": 28.36}, "VIN": {"avg": 74.12, "peak": 111.22, "min": 55.34}, "VDD_CPU_SOC_MSS": {"avg": 15.91, "peak": 17.32, "min": 14.96}}, "power_watts_avg": 35.31, "energy_joules_est": 77.2, "sample_count": 17, "duration_seconds": 2.186}, "timestamp": "2026-01-16T15:31:16.429055"}
{"image_index": 112, "image_name": "000000011511.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011511.jpg", "image_width": 640, "image_height": 464, "image_resolution": "640x464", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2811.366, "latencies_ms": [2811.366], "images_per_second": 0.356, "prompt_tokens": 23, "response_tokens_est": 38, "n_tiles": 12, "output_text": "1. Statue\n2. People\n3. Shoe\n4. Bag\n5. Shoe\n6. Shoe\n7. Shoe\n8. Shoe", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25596.3, "ram_available_mb": 100175.9, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 25596.3, "ram_available_mb": 100175.9, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.06, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 35.08, "peak": 43.33, "min": 26.79}, "VIN": {"avg": 78.65, "peak": 112.92, "min": 58.47}, "VDD_CPU_SOC_MSS": {"avg": 15.71, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 35.08, "energy_joules_est": 98.63, "sample_count": 22, "duration_seconds": 2.812}, "timestamp": "2026-01-16T15:31:19.247162"}
{"image_index": 112, "image_name": "000000011511.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011511.jpg", "image_width": 640, "image_height": 464, "image_resolution": "640x464", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 4026.431, "latencies_ms": [4026.431], "images_per_second": 0.248, "prompt_tokens": 27, "response_tokens_est": 74, "n_tiles": 12, "output_text": "The main object in the foreground is a bronze statue of a woman sitting on a bench. The statue is positioned near the center of the image, with a concrete bench and a metal trash can nearby. In the background, there are two people standing near a closed garage door. The statue and the bench are closer to the camera, while the people are further away.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25596.3, "ram_available_mb": 100175.9, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25597.8, "ram_available_mb": 100174.4, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.07, "peak": 15.65, "min": 14.43}, "VDD_GPU": {"avg": 32.31, "peak": 42.53, "min": 26.0}, "VIN": {"avg": 72.54, "peak": 116.55, "min": 56.33}, "VDD_CPU_SOC_MSS": {"avg": 15.7, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 32.31, "energy_joules_est": 130.11, "sample_count": 32, "duration_seconds": 4.027}, "timestamp": "2026-01-16T15:31:23.280533"}
{"image_index": 112, "image_name": "000000011511.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011511.jpg", "image_width": 640, "image_height": 464, "image_resolution": "640x464", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3180.267, "latencies_ms": [3180.267], "images_per_second": 0.314, "prompt_tokens": 21, "response_tokens_est": 49, "n_tiles": 12, "output_text": "The scene depicts a public square with a statue of two women sitting on a bench. The setting appears to be outdoors during the daytime, with a dark, shadowy area in the background and a clear, bright area in the foreground.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25597.8, "ram_available_mb": 100174.4, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 25597.7, "ram_available_mb": 100174.4, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.08, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 33.82, "peak": 42.54, "min": 26.39}, "VIN": {"avg": 71.87, "peak": 106.25, "min": 46.3}, "VDD_CPU_SOC_MSS": {"avg": 15.73, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 33.82, "energy_joules_est": 107.57, "sample_count": 25, "duration_seconds": 3.181}, "timestamp": "2026-01-16T15:31:26.467717"}
{"image_index": 112, "image_name": "000000011511.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011511.jpg", "image_width": 640, "image_height": 464, "image_resolution": "640x464", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2979.628, "latencies_ms": [2979.628], "images_per_second": 0.336, "prompt_tokens": 19, "response_tokens_est": 43, "n_tiles": 12, "output_text": "The bronze statue of two women is notable for its weathered appearance, with visible signs of wear and tear. The lighting in the image is natural, casting shadows on the ground, indicating that it is daytime.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25597.7, "ram_available_mb": 100174.4, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 25597.7, "ram_available_mb": 100174.4, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.05, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 34.6, "peak": 42.94, "min": 26.8}, "VIN": {"avg": 74.74, "peak": 114.04, "min": 58.51}, "VDD_CPU_SOC_MSS": {"avg": 15.68, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 34.6, "energy_joules_est": 103.11, "sample_count": 23, "duration_seconds": 2.98}, "timestamp": "2026-01-16T15:31:29.453934"}
{"image_index": 113, "image_name": "000000011615.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011615.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2894.971, "latencies_ms": [2894.971], "images_per_second": 0.345, "prompt_tokens": 9, "response_tokens_est": 44, "n_tiles": 12, "output_text": "The image shows a set of road signs, including a blue directional arrow indicating a left turn, a yellow bus icon, and a green directional arrow indicating a right turn, all mounted on a metal pole.", "error": null, "sys_before": {"cpu_percent": 41.6, "ram_used_mb": 25597.7, "ram_available_mb": 100174.4, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 25597.7, "ram_available_mb": 100174.4, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.1, "peak": 16.15, "min": 13.79}, "VDD_GPU": {"avg": 33.82, "peak": 41.36, "min": 26.4}, "VIN": {"avg": 74.91, "peak": 104.88, "min": 61.26}, "VDD_CPU_SOC_MSS": {"avg": 15.8, "peak": 17.32, "min": 14.56}}, "power_watts_avg": 33.82, "energy_joules_est": 97.92, "sample_count": 22, "duration_seconds": 2.895}, "timestamp": "2026-01-16T15:31:32.436088"}
{"image_index": 113, "image_name": "000000011615.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011615.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2603.775, "latencies_ms": [2603.775], "images_per_second": 0.384, "prompt_tokens": 23, "response_tokens_est": 32, "n_tiles": 12, "output_text": "1. Sign\n2. Sign\n3. Sign\n4. Sign\n5. Sign\n6. Sign\n7. Sign\n8. Sign", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25597.7, "ram_available_mb": 100174.4, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 25597.2, "ram_available_mb": 100174.9, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.07, "peak": 15.54, "min": 14.53}, "VDD_GPU": {"avg": 35.73, "peak": 42.94, "min": 27.57}, "VIN": {"avg": 73.44, "peak": 104.41, "min": 58.47}, "VDD_CPU_SOC_MSS": {"avg": 15.69, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 35.73, "energy_joules_est": 93.05, "sample_count": 20, "duration_seconds": 2.604}, "timestamp": "2026-01-16T15:31:35.046761"}
{"image_index": 113, "image_name": "000000011615.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011615.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3753.618, "latencies_ms": [3753.618], "images_per_second": 0.266, "prompt_tokens": 27, "response_tokens_est": 66, "n_tiles": 12, "output_text": "The main objects in the image are a series of road signs. The signs are arranged in a way that the sign on the left is closer to the foreground, while the sign on the right is closer to the background. The sign on the left is closer to the viewer, while the sign on the right is further away.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25597.2, "ram_available_mb": 100174.9, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25597.2, "ram_available_mb": 100174.9, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.1, "peak": 15.65, "min": 14.43}, "VDD_GPU": {"avg": 32.83, "peak": 42.94, "min": 26.0}, "VIN": {"avg": 73.2, "peak": 118.51, "min": 56.36}, "VDD_CPU_SOC_MSS": {"avg": 15.65, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 32.83, "energy_joules_est": 123.26, "sample_count": 29, "duration_seconds": 3.754}, "timestamp": "2026-01-16T15:31:38.807606"}
{"image_index": 113, "image_name": "000000011615.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011615.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3147.916, "latencies_ms": [3147.916], "images_per_second": 0.318, "prompt_tokens": 21, "response_tokens_est": 49, "n_tiles": 12, "output_text": "The image depicts a collection of road signs, including directional arrows and parking signs, against a backdrop of trees and an overcast sky. The signs are mounted on a pole and provide guidance for drivers, indicating directions and parking areas.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25597.2, "ram_available_mb": 100174.9, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 25597.2, "ram_available_mb": 100175.0, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.16, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 34.26, "peak": 41.76, "min": 26.79}, "VIN": {"avg": 78.93, "peak": 119.82, "min": 58.4}, "VDD_CPU_SOC_MSS": {"avg": 15.79, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 34.26, "energy_joules_est": 107.86, "sample_count": 24, "duration_seconds": 3.148}, "timestamp": "2026-01-16T15:31:41.962841"}
{"image_index": 113, "image_name": "000000011615.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011615.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3651.269, "latencies_ms": [3651.269], "images_per_second": 0.274, "prompt_tokens": 19, "response_tokens_est": 63, "n_tiles": 12, "output_text": "The image features a collection of road signs, including a blue and white sign with an airplane icon, a green and white sign with a bus icon, and a white sign with a parking symbol. The signs are mounted on a brown pole and are illuminated by natural daylight, suggesting a clear and sunny day.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25597.2, "ram_available_mb": 100175.0, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25596.5, "ram_available_mb": 100175.7, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.09, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 32.98, "peak": 42.15, "min": 26.39}, "VIN": {"avg": 75.02, "peak": 127.18, "min": 58.88}, "VDD_CPU_SOC_MSS": {"avg": 15.6, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 32.98, "energy_joules_est": 120.44, "sample_count": 28, "duration_seconds": 3.652}, "timestamp": "2026-01-16T15:31:45.621084"}
{"image_index": 114, "image_name": "000000011699.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011699.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2092.952, "latencies_ms": [2092.952], "images_per_second": 0.478, "prompt_tokens": 9, "response_tokens_est": 20, "n_tiles": 12, "output_text": "A woman and a young girl are standing next to a suitcase, both wearing backpacks.", "error": null, "sys_before": {"cpu_percent": 28.4, "ram_used_mb": 25596.5, "ram_available_mb": 100175.7, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 2.4, "ram_used_mb": 25596.5, "ram_available_mb": 100175.7, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.17, "peak": 16.15, "min": 14.02}, "VDD_GPU": {"avg": 36.34, "peak": 42.15, "min": 29.15}, "VIN": {"avg": 72.97, "peak": 97.19, "min": 57.43}, "VDD_CPU_SOC_MSS": {"avg": 16.04, "peak": 17.32, "min": 14.96}}, "power_watts_avg": 36.34, "energy_joules_est": 76.07, "sample_count": 16, "duration_seconds": 2.093}, "timestamp": "2026-01-16T15:31:47.774386"}
{"image_index": 114, "image_name": "000000011699.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011699.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2682.161, "latencies_ms": [2682.161], "images_per_second": 0.373, "prompt_tokens": 23, "response_tokens_est": 33, "n_tiles": 12, "output_text": "1. Woman\n2. Girl\n3. Bag\n4. Suitcase\n5. Bag\n6. Woman\n7. Bag\n8. Bag", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25596.5, "ram_available_mb": 100175.7, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 25596.5, "ram_available_mb": 100175.7, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.06, "peak": 15.54, "min": 14.53}, "VDD_GPU": {"avg": 35.69, "peak": 42.94, "min": 27.18}, "VIN": {"avg": 72.63, "peak": 116.43, "min": 58.79}, "VDD_CPU_SOC_MSS": {"avg": 15.75, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 35.69, "energy_joules_est": 95.74, "sample_count": 20, "duration_seconds": 2.683}, "timestamp": "2026-01-16T15:31:50.463626"}
{"image_index": 114, "image_name": "000000011699.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011699.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 4283.147, "latencies_ms": [4283.147], "images_per_second": 0.233, "prompt_tokens": 27, "response_tokens_est": 83, "n_tiles": 12, "output_text": "The main objects in the image are a woman and a young girl standing next to a suitcase. The woman is wearing a red shirt and has a black backpack, while the girl is wearing a light blue shirt and has a colorful backpack. The suitcase is positioned in the foreground, with the woman and girl standing near it. The background features a yellow tactile paving strip and a red ceiling structure.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25596.5, "ram_available_mb": 100175.7, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 25596.8, "ram_available_mb": 100175.3, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.17, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 32.1, "peak": 42.94, "min": 26.39}, "VIN": {"avg": 73.3, "peak": 112.81, "min": 58.48}, "VDD_CPU_SOC_MSS": {"avg": 15.79, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 32.1, "energy_joules_est": 137.5, "sample_count": 33, "duration_seconds": 4.283}, "timestamp": "2026-01-16T15:31:54.753723"}
{"image_index": 114, "image_name": "000000011699.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011699.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 4918.099, "latencies_ms": [4918.099], "images_per_second": 0.203, "prompt_tokens": 21, "response_tokens_est": 102, "n_tiles": 12, "output_text": "The image depicts a scene inside a subway station where a woman and a young girl are standing next to a black suitcase. The woman is wearing a red shirt and has a backpack, while the girl is dressed in a light blue shirt and is also carrying a backpack. They appear to be waiting for their train, with the woman holding onto the suitcase handle and the girl making a peace sign with her hand. The setting is a subway station with yellow tactile paving along the platform edge.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25596.8, "ram_available_mb": 100175.3, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25596.8, "ram_available_mb": 100175.3, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.17, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 31.36, "peak": 42.94, "min": 26.39}, "VIN": {"avg": 72.9, "peak": 113.52, "min": 50.03}, "VDD_CPU_SOC_MSS": {"avg": 15.77, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 31.36, "energy_joules_est": 154.24, "sample_count": 38, "duration_seconds": 4.918}, "timestamp": "2026-01-16T15:31:59.680715"}
{"image_index": 114, "image_name": "000000011699.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011699.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3737.242, "latencies_ms": [3737.242], "images_per_second": 0.268, "prompt_tokens": 19, "response_tokens_est": 65, "n_tiles": 12, "output_text": "The image features a woman and a young girl standing next to a black suitcase. The woman is wearing a red shirt and a black backpack, while the girl is dressed in a light blue shirt and a colorful backpack. The lighting is bright, and the scene appears to be indoors, possibly in a public transportation station.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25596.8, "ram_available_mb": 100175.3, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25596.8, "ram_available_mb": 100175.3, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.79, "peak": 15.34, "min": 14.12}, "VDD_GPU": {"avg": 32.72, "peak": 42.15, "min": 25.6}, "VIN": {"avg": 73.84, "peak": 107.23, "min": 58.17}, "VDD_CPU_SOC_MSS": {"avg": 15.33, "peak": 16.14, "min": 14.56}}, "power_watts_avg": 32.72, "energy_joules_est": 122.29, "sample_count": 29, "duration_seconds": 3.738}, "timestamp": "2026-01-16T15:32:03.424547"}
{"image_index": 115, "image_name": "000000011760.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011760.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1415.4, "latencies_ms": [1415.4], "images_per_second": 0.707, "prompt_tokens": 9, "response_tokens_est": 31, "n_tiles": 6, "output_text": "The image depicts a zebra standing in a natural environment with trees and bushes in the background, showcasing its distinctive black and white stripes.", "error": null, "sys_before": {"cpu_percent": 37.8, "ram_used_mb": 25596.8, "ram_available_mb": 100175.3, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25598.1, "ram_available_mb": 100174.1, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.38, "peak": 15.01, "min": 13.82}, "VDD_GPU": {"avg": 31.69, "peak": 37.42, "min": 25.6}, "VIN": {"avg": 71.65, "peak": 113.75, "min": 58.56}, "VDD_CPU_SOC_MSS": {"avg": 15.1, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 31.69, "energy_joules_est": 44.88, "sample_count": 11, "duration_seconds": 1.416}, "timestamp": "2026-01-16T15:32:04.898074"}
{"image_index": 115, "image_name": "000000011760.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011760.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1352.039, "latencies_ms": [1352.039], "images_per_second": 0.74, "prompt_tokens": 23, "response_tokens_est": 27, "n_tiles": 6, "output_text": "zebra: 2\ntree: 1\nbush: 1\nlog: 1\nground: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25598.1, "ram_available_mb": 100174.1, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25598.1, "ram_available_mb": 100174.1, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.71, "peak": 15.01, "min": 14.22}, "VDD_GPU": {"avg": 33.52, "peak": 38.6, "min": 26.79}, "VIN": {"avg": 79.13, "peak": 110.16, "min": 62.14}, "VDD_CPU_SOC_MSS": {"avg": 15.31, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 33.52, "energy_joules_est": 45.34, "sample_count": 10, "duration_seconds": 1.352}, "timestamp": "2026-01-16T15:32:06.256430"}
{"image_index": 115, "image_name": "000000011760.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011760.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2687.358, "latencies_ms": [2687.358], "images_per_second": 0.372, "prompt_tokens": 27, "response_tokens_est": 77, "n_tiles": 6, "output_text": "The main objects in the image are two zebras standing in the foreground. The foreground is the most prominent area, with the zebras being the central focus. The background consists of a dense forest with trees and bushes, creating a natural and serene setting. The foreground is closer to the camera, while the background is further away, providing depth to the image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25598.1, "ram_available_mb": 100174.1, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 25598.1, "ram_available_mb": 100174.1, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.98, "peak": 15.32, "min": 14.22}, "VDD_GPU": {"avg": 28.82, "peak": 39.0, "min": 23.64}, "VIN": {"avg": 69.47, "peak": 112.58, "min": 57.01}, "VDD_CPU_SOC_MSS": {"avg": 15.6, "peak": 16.14, "min": 14.96}}, "power_watts_avg": 28.82, "energy_joules_est": 77.46, "sample_count": 21, "duration_seconds": 2.688}, "timestamp": "2026-01-16T15:32:08.950152"}
{"image_index": 115, "image_name": "000000011760.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011760.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2629.353, "latencies_ms": [2629.353], "images_per_second": 0.38, "prompt_tokens": 21, "response_tokens_est": 75, "n_tiles": 6, "output_text": "The image depicts a scene in a natural habitat, likely a savanna or grassland, where two zebras are walking. The zebras are surrounded by a mix of dry grass and scattered trees, with some purple flowers visible in the background. The zebras appear to be in a peaceful walk, with their distinctive black and white stripes clearly visible.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25598.1, "ram_available_mb": 100174.1, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 25598.1, "ram_available_mb": 100174.1, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.99, "peak": 15.32, "min": 14.32}, "VDD_GPU": {"avg": 28.62, "peak": 38.21, "min": 23.64}, "VIN": {"avg": 70.21, "peak": 109.05, "min": 61.04}, "VDD_CPU_SOC_MSS": {"avg": 15.55, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 28.62, "energy_joules_est": 75.27, "sample_count": 20, "duration_seconds": 2.63}, "timestamp": "2026-01-16T15:32:11.589868"}
{"image_index": 115, "image_name": "000000011760.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011760.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2204.597, "latencies_ms": [2204.597], "images_per_second": 0.454, "prompt_tokens": 19, "response_tokens_est": 59, "n_tiles": 6, "output_text": "The image features two zebras standing in a natural environment. The zebras have distinctive black and white stripes on their bodies, and their fur appears to be a mix of brown and white. The lighting is natural, suggesting it is daytime, and the ground is covered in dirt.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25598.1, "ram_available_mb": 100174.1, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 25598.1, "ram_available_mb": 100174.1, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.9, "peak": 15.22, "min": 14.22}, "VDD_GPU": {"avg": 29.57, "peak": 38.6, "min": 24.03}, "VIN": {"avg": 71.74, "peak": 95.72, "min": 62.4}, "VDD_CPU_SOC_MSS": {"avg": 15.47, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 29.57, "energy_joules_est": 65.21, "sample_count": 17, "duration_seconds": 2.205}, "timestamp": "2026-01-16T15:32:13.801005"}
{"image_index": 116, "image_name": "000000011813.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011813.jpg", "image_width": 333, "image_height": 500, "image_resolution": "333x500", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1440.087, "latencies_ms": [1440.087], "images_per_second": 0.694, "prompt_tokens": 9, "response_tokens_est": 32, "n_tiles": 6, "output_text": "The image shows a professional video camera mounted on a tripod, with a laptop placed on a folding chair nearby, indicating a setup for filming or recording.", "error": null, "sys_before": {"cpu_percent": 43.3, "ram_used_mb": 25598.1, "ram_available_mb": 100174.1, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25598.4, "ram_available_mb": 100173.8, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.53, "peak": 15.01, "min": 13.92}, "VDD_GPU": {"avg": 31.41, "peak": 37.42, "min": 25.61}, "VIN": {"avg": 65.81, "peak": 77.4, "min": 57.2}, "VDD_CPU_SOC_MSS": {"avg": 15.28, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 31.41, "energy_joules_est": 45.25, "sample_count": 11, "duration_seconds": 1.441}, "timestamp": "2026-01-16T15:32:15.286841"}
{"image_index": 116, "image_name": "000000011813.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011813.jpg", "image_width": 333, "image_height": 500, "image_resolution": "333x500", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1539.673, "latencies_ms": [1539.673], "images_per_second": 0.649, "prompt_tokens": 23, "response_tokens_est": 34, "n_tiles": 6, "output_text": "1. Laptop\n2. Chair\n3. Tripod\n4. Camera\n5. Microphone\n6. Wall\n7. Floor\n8. Table", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25598.1, "ram_available_mb": 100174.0, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 25598.1, "ram_available_mb": 100174.0, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.75, "peak": 15.11, "min": 14.22}, "VDD_GPU": {"avg": 32.27, "peak": 38.6, "min": 25.6}, "VIN": {"avg": 73.26, "peak": 117.5, "min": 55.33}, "VDD_CPU_SOC_MSS": {"avg": 15.35, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 32.27, "energy_joules_est": 49.7, "sample_count": 11, "duration_seconds": 1.54}, "timestamp": "2026-01-16T15:32:16.832588"}
{"image_index": 116, "image_name": "000000011813.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011813.jpg", "image_width": 333, "image_height": 500, "image_resolution": "333x500", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1836.889, "latencies_ms": [1836.889], "images_per_second": 0.544, "prompt_tokens": 27, "response_tokens_est": 45, "n_tiles": 6, "output_text": "The main objects in the image are a laptop and a tripod. The laptop is positioned on a folding chair, which is located in the foreground. The tripod is positioned in the background, slightly to the right.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25598.1, "ram_available_mb": 100174.0, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25598.1, "ram_available_mb": 100174.0, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.83, "peak": 15.22, "min": 14.22}, "VDD_GPU": {"avg": 30.98, "peak": 38.21, "min": 24.42}, "VIN": {"avg": 77.13, "peak": 114.3, "min": 62.24}, "VDD_CPU_SOC_MSS": {"avg": 15.41, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 30.98, "energy_joules_est": 56.92, "sample_count": 14, "duration_seconds": 1.837}, "timestamp": "2026-01-16T15:32:18.675442"}
{"image_index": 116, "image_name": "000000011813.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011813.jpg", "image_width": 333, "image_height": 500, "image_resolution": "333x500", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1963.808, "latencies_ms": [1963.808], "images_per_second": 0.509, "prompt_tokens": 21, "response_tokens_est": 50, "n_tiles": 6, "output_text": "The image depicts a professional video production setup in a well-lit room. A camera is mounted on a tripod, and a laptop is placed on a folding chair, indicating that the scene is likely part of a filming or recording session.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25598.1, "ram_available_mb": 100174.0, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 25598.1, "ram_available_mb": 100174.0, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.87, "peak": 15.22, "min": 14.22}, "VDD_GPU": {"avg": 30.33, "peak": 38.21, "min": 24.43}, "VIN": {"avg": 69.79, "peak": 104.39, "min": 62.35}, "VDD_CPU_SOC_MSS": {"avg": 15.43, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 30.33, "energy_joules_est": 59.58, "sample_count": 15, "duration_seconds": 1.964}, "timestamp": "2026-01-16T15:32:20.645534"}
{"image_index": 116, "image_name": "000000011813.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000011813.jpg", "image_width": 333, "image_height": 500, "image_resolution": "333x500", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1669.951, "latencies_ms": [1669.951], "images_per_second": 0.599, "prompt_tokens": 19, "response_tokens_est": 39, "n_tiles": 6, "output_text": "The image features a well-lit indoor setting with a white wall and a black laptop on a folding chair. The lighting is bright, casting clear shadows and highlighting the objects in the scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25598.1, "ram_available_mb": 100174.0, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 25598.4, "ram_available_mb": 100173.8, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.84, "peak": 15.22, "min": 14.32}, "VDD_GPU": {"avg": 31.21, "peak": 38.21, "min": 24.83}, "VIN": {"avg": 72.16, "peak": 117.22, "min": 56.4}, "VDD_CPU_SOC_MSS": {"avg": 15.41, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 31.21, "energy_joules_est": 52.13, "sample_count": 13, "duration_seconds": 1.67}, "timestamp": "2026-01-16T15:32:22.321546"}
{"image_index": 117, "image_name": "000000012062.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012062.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1469.127, "latencies_ms": [1469.127], "images_per_second": 0.681, "prompt_tokens": 9, "response_tokens_est": 33, "n_tiles": 6, "output_text": "A flock of sheep is being kept in a metal pen, with some sheep lying on the ground and others standing, all surrounded by a thick layer of wool.", "error": null, "sys_before": {"cpu_percent": 34.5, "ram_used_mb": 25598.4, "ram_available_mb": 100173.8, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25598.1, "ram_available_mb": 100174.1, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.49, "peak": 15.01, "min": 13.92}, "VDD_GPU": {"avg": 31.69, "peak": 37.42, "min": 25.6}, "VIN": {"avg": 77.29, "peak": 123.33, "min": 54.99}, "VDD_CPU_SOC_MSS": {"avg": 15.28, "peak": 15.75, "min": 14.57}}, "power_watts_avg": 31.69, "energy_joules_est": 46.58, "sample_count": 11, "duration_seconds": 1.47}, "timestamp": "2026-01-16T15:32:23.839099"}
{"image_index": 117, "image_name": "000000012062.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012062.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1747.785, "latencies_ms": [1747.785], "images_per_second": 0.572, "prompt_tokens": 23, "response_tokens_est": 40, "n_tiles": 6, "output_text": "1. Sheep\n2. Sheep\n3. Sheep\n4. Sheep\n5. Sheep\n6. Sheep\n7. Sheep\n8. Sheep", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25598.1, "ram_available_mb": 100174.1, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25598.3, "ram_available_mb": 100173.9, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.69, "peak": 14.91, "min": 14.22}, "VDD_GPU": {"avg": 31.15, "peak": 38.19, "min": 24.43}, "VIN": {"avg": 74.94, "peak": 115.67, "min": 61.41}, "VDD_CPU_SOC_MSS": {"avg": 15.26, "peak": 15.36, "min": 14.96}}, "power_watts_avg": 31.15, "energy_joules_est": 54.45, "sample_count": 13, "duration_seconds": 1.748}, "timestamp": "2026-01-16T15:32:25.593417"}
{"image_index": 117, "image_name": "000000012062.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012062.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2473.259, "latencies_ms": [2473.259], "images_per_second": 0.404, "prompt_tokens": 27, "response_tokens_est": 66, "n_tiles": 6, "output_text": "The main objects in the image are sheep, which are located in the foreground. The sheep are surrounded by a metal wire fence, and there is a pile of sheep's wool in the foreground. The background features a paved surface and a small, white object, possibly a ball, which is not part of the sheep.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25598.3, "ram_available_mb": 100173.9, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 25598.3, "ram_available_mb": 100173.9, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.74, "peak": 15.01, "min": 14.02}, "VDD_GPU": {"avg": 28.72, "peak": 38.21, "min": 23.25}, "VIN": {"avg": 69.29, "peak": 119.58, "min": 58.97}, "VDD_CPU_SOC_MSS": {"avg": 15.37, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 28.72, "energy_joules_est": 71.04, "sample_count": 19, "duration_seconds": 2.474}, "timestamp": "2026-01-16T15:32:28.073856"}
{"image_index": 117, "image_name": "000000012062.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012062.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2527.557, "latencies_ms": [2527.557], "images_per_second": 0.396, "prompt_tokens": 21, "response_tokens_est": 68, "n_tiles": 6, "output_text": "The image depicts a sheep in a pen, surrounded by a variety of sheep's wool. The sheep appears to be in a stable or barn, with a metal fence enclosing the area. The sheep's wool is visible, and there is a soccer ball nearby, suggesting that the setting might be a farm or a livestock facility.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25598.3, "ram_available_mb": 100173.9, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 25598.5, "ram_available_mb": 100173.7, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.71, "peak": 15.01, "min": 14.02}, "VDD_GPU": {"avg": 28.45, "peak": 37.82, "min": 23.24}, "VIN": {"avg": 70.18, "peak": 107.21, "min": 58.54}, "VDD_CPU_SOC_MSS": {"avg": 15.35, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 28.45, "energy_joules_est": 71.92, "sample_count": 19, "duration_seconds": 2.528}, "timestamp": "2026-01-16T15:32:30.607639"}
{"image_index": 117, "image_name": "000000012062.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012062.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2102.027, "latencies_ms": [2102.027], "images_per_second": 0.476, "prompt_tokens": 19, "response_tokens_est": 53, "n_tiles": 6, "output_text": "The image shows a sheep in a wire pen, surrounded by a variety of sheep's wool. The sheep's wool is predominantly white, with some areas appearing slightly darker. The lighting is natural, suggesting it is daytime, and the weather appears to be clear.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25598.5, "ram_available_mb": 100173.7, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 25598.3, "ram_available_mb": 100173.9, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.72, "peak": 15.01, "min": 14.02}, "VDD_GPU": {"avg": 29.42, "peak": 38.6, "min": 23.25}, "VIN": {"avg": 74.34, "peak": 114.76, "min": 61.55}, "VDD_CPU_SOC_MSS": {"avg": 15.28, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 29.42, "energy_joules_est": 61.85, "sample_count": 16, "duration_seconds": 2.102}, "timestamp": "2026-01-16T15:32:32.716279"}
{"image_index": 118, "image_name": "000000012120.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012120.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1688.53, "latencies_ms": [1688.53], "images_per_second": 0.592, "prompt_tokens": 9, "response_tokens_est": 41, "n_tiles": 6, "output_text": "A female tennis player in a purple outfit is preparing to hit a tennis ball on a blue court, while a man in a red shirt and another in a blue shirt are capturing the moment with cameras.", "error": null, "sys_before": {"cpu_percent": 35.6, "ram_used_mb": 25598.3, "ram_available_mb": 100173.9, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25598.5, "ram_available_mb": 100173.7, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.51, "peak": 15.01, "min": 13.92}, "VDD_GPU": {"avg": 30.56, "peak": 37.03, "min": 24.82}, "VIN": {"avg": 68.25, "peak": 114.46, "min": 54.52}, "VDD_CPU_SOC_MSS": {"avg": 15.22, "peak": 15.75, "min": 14.57}}, "power_watts_avg": 30.56, "energy_joules_est": 51.62, "sample_count": 12, "duration_seconds": 1.689}, "timestamp": "2026-01-16T15:32:34.453091"}
{"image_index": 118, "image_name": "000000012120.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012120.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1839.815, "latencies_ms": [1839.815], "images_per_second": 0.544, "prompt_tokens": 23, "response_tokens_est": 45, "n_tiles": 6, "output_text": "1. Tennis player\n2. Tennis racket\n3. Tennis ball\n4. Blue tennis court\n5. Blue net\n6. Blue advertising boards\n7. Blue seating area\n8. Blue spectators", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25598.5, "ram_available_mb": 100173.7, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 25598.4, "ram_available_mb": 100173.8, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.82, "peak": 15.11, "min": 14.22}, "VDD_GPU": {"avg": 30.64, "peak": 37.8, "min": 24.42}, "VIN": {"avg": 66.29, "peak": 85.95, "min": 56.02}, "VDD_CPU_SOC_MSS": {"avg": 15.35, "peak": 15.75, "min": 14.57}}, "power_watts_avg": 30.64, "energy_joules_est": 56.39, "sample_count": 14, "duration_seconds": 1.84}, "timestamp": "2026-01-16T15:32:36.299303"}
{"image_index": 118, "image_name": "000000012120.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012120.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3096.103, "latencies_ms": [3096.103], "images_per_second": 0.323, "prompt_tokens": 27, "response_tokens_est": 92, "n_tiles": 6, "output_text": "The main object in the foreground is a tennis player in a purple outfit, preparing to hit a tennis ball. The player is positioned near the net, which is the central focus of the image. In the background, there is a scoreboard displaying the names of the players and the score, with a crowd of spectators seated in the stands. The scoreboard is mounted on a blue wall, and there are advertisements and logos on the wall as well.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25598.4, "ram_available_mb": 100173.8, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 25598.6, "ram_available_mb": 100173.6, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.0, "peak": 15.32, "min": 14.32}, "VDD_GPU": {"avg": 27.74, "peak": 37.82, "min": 23.64}, "VIN": {"avg": 65.67, "peak": 76.67, "min": 54.4}, "VDD_CPU_SOC_MSS": {"avg": 15.55, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 27.74, "energy_joules_est": 85.9, "sample_count": 24, "duration_seconds": 3.097}, "timestamp": "2026-01-16T15:32:39.401690"}
{"image_index": 118, "image_name": "000000012120.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012120.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2389.295, "latencies_ms": [2389.295], "images_per_second": 0.419, "prompt_tokens": 21, "response_tokens_est": 66, "n_tiles": 6, "output_text": "The image depicts a professional tennis match taking place on a blue hard court. The court is surrounded by a blue net, and there are several people in the background, including a referee and a cameraman. The match is being broadcasted, as indicated by the presence of a Sony Ericsson logo on the court.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25598.6, "ram_available_mb": 100173.6, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 25598.6, "ram_available_mb": 100173.6, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.95, "peak": 15.32, "min": 14.22}, "VDD_GPU": {"avg": 28.93, "peak": 38.21, "min": 23.64}, "VIN": {"avg": 66.58, "peak": 82.96, "min": 56.3}, "VDD_CPU_SOC_MSS": {"avg": 15.48, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 28.93, "energy_joules_est": 69.13, "sample_count": 19, "duration_seconds": 2.39}, "timestamp": "2026-01-16T15:32:41.797648"}
{"image_index": 118, "image_name": "000000012120.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012120.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1590.296, "latencies_ms": [1590.296], "images_per_second": 0.629, "prompt_tokens": 19, "response_tokens_est": 36, "n_tiles": 6, "output_text": "The image depicts a brightly lit blue tennis court with a vibrant blue surface. The court is surrounded by a blue net, and there are several advertisements on the walls.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25598.6, "ram_available_mb": 100173.6, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25598.6, "ram_available_mb": 100173.6, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.8, "peak": 15.11, "min": 14.22}, "VDD_GPU": {"avg": 31.55, "peak": 38.21, "min": 25.21}, "VIN": {"avg": 65.31, "peak": 81.12, "min": 56.08}, "VDD_CPU_SOC_MSS": {"avg": 15.39, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 31.55, "energy_joules_est": 50.18, "sample_count": 12, "duration_seconds": 1.591}, "timestamp": "2026-01-16T15:32:43.394369"}
{"image_index": 119, "image_name": "000000012280.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012280.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2216.613, "latencies_ms": [2216.613], "images_per_second": 0.451, "prompt_tokens": 9, "response_tokens_est": 24, "n_tiles": 12, "output_text": "A man is walking with a suitcase in an airport terminal, while a signboard with directions is visible overhead.", "error": null, "sys_before": {"cpu_percent": 41.2, "ram_used_mb": 25598.3, "ram_available_mb": 100173.8, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 25598.7, "ram_available_mb": 100173.5, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.16, "peak": 16.05, "min": 14.1}, "VDD_GPU": {"avg": 35.75, "peak": 41.36, "min": 28.36}, "VIN": {"avg": 80.82, "peak": 124.67, "min": 58.33}, "VDD_CPU_SOC_MSS": {"avg": 15.91, "peak": 16.92, "min": 15.35}}, "power_watts_avg": 35.75, "energy_joules_est": 79.27, "sample_count": 17, "duration_seconds": 2.217}, "timestamp": "2026-01-16T15:32:45.685710"}
{"image_index": 119, "image_name": "000000012280.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012280.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 5855.214, "latencies_ms": [5855.214], "images_per_second": 0.171, "prompt_tokens": 23, "response_tokens_est": 129, "n_tiles": 12, "output_text": "- Exit sign: 1\n- Exit sign: 1\n- Exit sign: 1\n- Exit sign: 1\n- Exit sign: 1\n- Exit sign: 1\n- Exit sign: 1\n- Exit sign: 1\n- Exit sign: 1\n- Exit sign: 1\n- Exit sign: 1\n- Exit sign: 1\n- Exit sign: 1\n- Exit sign: 1\n- Exit sign: 1\n- Exit sign: 1\n- Exit sign: 1\n- Exit sign: 1\n- Exit", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25598.7, "ram_available_mb": 100173.5, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 25598.5, "ram_available_mb": 100173.7, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.09, "peak": 15.65, "min": 14.43}, "VDD_GPU": {"avg": 30.51, "peak": 43.33, "min": 26.0}, "VIN": {"avg": 71.94, "peak": 116.27, "min": 58.49}, "VDD_CPU_SOC_MSS": {"avg": 15.69, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 30.51, "energy_joules_est": 178.66, "sample_count": 46, "duration_seconds": 5.856}, "timestamp": "2026-01-16T15:32:51.547506"}
{"image_index": 119, "image_name": "000000012280.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012280.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3922.912, "latencies_ms": [3922.912], "images_per_second": 0.255, "prompt_tokens": 27, "response_tokens_est": 71, "n_tiles": 12, "output_text": "The main objects in the image are a man in a suit carrying a suitcase and a glass door. The man is positioned in the foreground, slightly to the left, while the glass door is located in the background, to the right. The man's suitcase is near the glass door, indicating that he is either entering or exiting the building.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25598.5, "ram_available_mb": 100173.7, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25598.6, "ram_available_mb": 100173.6, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.09, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 32.49, "peak": 42.54, "min": 26.0}, "VIN": {"avg": 77.6, "peak": 120.24, "min": 58.48}, "VDD_CPU_SOC_MSS": {"avg": 15.59, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 32.49, "energy_joules_est": 127.47, "sample_count": 31, "duration_seconds": 3.923}, "timestamp": "2026-01-16T15:32:55.476727"}
{"image_index": 119, "image_name": "000000012280.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012280.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 4050.704, "latencies_ms": [4050.704], "images_per_second": 0.247, "prompt_tokens": 21, "response_tokens_est": 75, "n_tiles": 12, "output_text": "The image depicts an indoor setting, likely a train station or airport terminal, characterized by modern architecture and clean lines. A man is seen walking with a suitcase, suggesting he is either arriving or departing from the location. The scene is well-lit, with overhead lights and a clear view of the surroundings, including a staircase and a sign indicating directions.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25598.6, "ram_available_mb": 100173.6, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25598.3, "ram_available_mb": 100173.9, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.1, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 32.18, "peak": 42.13, "min": 26.39}, "VIN": {"avg": 74.85, "peak": 122.2, "min": 58.37}, "VDD_CPU_SOC_MSS": {"avg": 15.66, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 32.18, "energy_joules_est": 130.37, "sample_count": 32, "duration_seconds": 4.051}, "timestamp": "2026-01-16T15:32:59.534461"}
{"image_index": 119, "image_name": "000000012280.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012280.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2781.524, "latencies_ms": [2781.524], "images_per_second": 0.36, "prompt_tokens": 19, "response_tokens_est": 37, "n_tiles": 12, "output_text": "The image depicts a modern, spacious indoor setting with a sleek, metallic floor and ceiling. The lighting is bright and evenly distributed, creating a clean and well-lit environment.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25598.3, "ram_available_mb": 100173.9, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 25598.3, "ram_available_mb": 100173.9, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.81, "peak": 15.34, "min": 14.12}, "VDD_GPU": {"avg": 35.09, "peak": 41.76, "min": 26.79}, "VIN": {"avg": 77.57, "peak": 114.82, "min": 58.29}, "VDD_CPU_SOC_MSS": {"avg": 15.43, "peak": 16.14, "min": 14.56}}, "power_watts_avg": 35.09, "energy_joules_est": 97.62, "sample_count": 21, "duration_seconds": 2.782}, "timestamp": "2026-01-16T15:33:02.322838"}
{"image_index": 120, "image_name": "000000012576.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012576.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2716.077, "latencies_ms": [2716.077], "images_per_second": 0.368, "prompt_tokens": 9, "response_tokens_est": 39, "n_tiles": 12, "output_text": "The image shows a table with a pizza box, a glass of water, a glass of red wine, and a plate of pizza, all set on a colorful checkered tablecloth.", "error": null, "sys_before": {"cpu_percent": 42.9, "ram_used_mb": 25598.3, "ram_available_mb": 100173.9, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 25598.3, "ram_available_mb": 100173.9, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.07, "peak": 16.15, "min": 13.49}, "VDD_GPU": {"avg": 33.91, "peak": 41.36, "min": 26.79}, "VIN": {"avg": 74.04, "peak": 117.75, "min": 55.88}, "VDD_CPU_SOC_MSS": {"avg": 15.9, "peak": 17.32, "min": 14.57}}, "power_watts_avg": 33.91, "energy_joules_est": 92.12, "sample_count": 21, "duration_seconds": 2.717}, "timestamp": "2026-01-16T15:33:05.140269"}
{"image_index": 120, "image_name": "000000012576.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012576.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 5844.218, "latencies_ms": [5844.218], "images_per_second": 0.171, "prompt_tokens": 23, "response_tokens_est": 128, "n_tiles": 12, "output_text": "object: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject:", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25598.3, "ram_available_mb": 100173.9, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 25596.2, "ram_available_mb": 100176.0, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.12, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 30.36, "peak": 42.92, "min": 26.0}, "VIN": {"avg": 70.72, "peak": 115.29, "min": 55.5}, "VDD_CPU_SOC_MSS": {"avg": 15.75, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 30.36, "energy_joules_est": 177.45, "sample_count": 46, "duration_seconds": 5.845}, "timestamp": "2026-01-16T15:33:10.992479"}
{"image_index": 120, "image_name": "000000012576.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012576.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3557.797, "latencies_ms": [3557.797], "images_per_second": 0.281, "prompt_tokens": 27, "response_tokens_est": 61, "n_tiles": 12, "output_text": "The main objects in the image are a pizza and a glass of water. The pizza is placed on a pizza box in the foreground, while the glass of water is positioned near the pizza box. The background features a television and various other items, including a can of soda and a plate with food.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25596.2, "ram_available_mb": 100176.0, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25596.2, "ram_available_mb": 100176.0, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.13, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 33.21, "peak": 41.76, "min": 26.39}, "VIN": {"avg": 73.9, "peak": 118.59, "min": 58.2}, "VDD_CPU_SOC_MSS": {"avg": 15.76, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 33.21, "energy_joules_est": 118.17, "sample_count": 27, "duration_seconds": 3.558}, "timestamp": "2026-01-16T15:33:14.556351"}
{"image_index": 120, "image_name": "000000012576.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012576.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3885.365, "latencies_ms": [3885.365], "images_per_second": 0.257, "prompt_tokens": 21, "response_tokens_est": 71, "n_tiles": 12, "output_text": "The image depicts a dining scene with a focus on a pizza box and a glass of water on a table. In the background, there is a television set on a stand, and various items such as a soda can, a glass of water, and a plate with food are visible. The setting appears to be a home kitchen or dining area.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25596.2, "ram_available_mb": 100176.0, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25597.4, "ram_available_mb": 100174.8, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.15, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 32.63, "peak": 42.54, "min": 26.39}, "VIN": {"avg": 73.52, "peak": 113.76, "min": 58.59}, "VDD_CPU_SOC_MSS": {"avg": 15.73, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 32.63, "energy_joules_est": 126.79, "sample_count": 30, "duration_seconds": 3.886}, "timestamp": "2026-01-16T15:33:18.448103"}
{"image_index": 120, "image_name": "000000012576.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012576.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3385.992, "latencies_ms": [3385.992], "images_per_second": 0.295, "prompt_tokens": 19, "response_tokens_est": 56, "n_tiles": 12, "output_text": "The image depicts a dining table with a pizza and a glass of water. The pizza has a golden-brown crust and a red tomato sauce, while the glass of water is clear and contains a few bubbles. The lighting is soft and warm, creating a cozy atmosphere.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25597.4, "ram_available_mb": 100174.8, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 25597.6, "ram_available_mb": 100174.6, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.14, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 33.61, "peak": 42.94, "min": 26.39}, "VIN": {"avg": 73.08, "peak": 105.22, "min": 57.1}, "VDD_CPU_SOC_MSS": {"avg": 15.75, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 33.61, "energy_joules_est": 113.82, "sample_count": 27, "duration_seconds": 3.386}, "timestamp": "2026-01-16T15:33:21.840511"}
{"image_index": 121, "image_name": "000000012639.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012639.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2341.342, "latencies_ms": [2341.342], "images_per_second": 0.427, "prompt_tokens": 9, "response_tokens_est": 28, "n_tiles": 12, "output_text": "A young boy is crouched down, wearing a protective helmet and a red shirt, preparing to catch a baseball with his mitt.", "error": null, "sys_before": {"cpu_percent": 39.5, "ram_used_mb": 25597.6, "ram_available_mb": 100174.6, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 25598.1, "ram_available_mb": 100174.1, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.11, "peak": 16.05, "min": 13.69}, "VDD_GPU": {"avg": 35.5, "peak": 41.36, "min": 27.97}, "VIN": {"avg": 77.74, "peak": 119.37, "min": 61.63}, "VDD_CPU_SOC_MSS": {"avg": 15.96, "peak": 17.32, "min": 14.57}}, "power_watts_avg": 35.5, "energy_joules_est": 83.13, "sample_count": 18, "duration_seconds": 2.342}, "timestamp": "2026-01-16T15:33:24.282489"}
{"image_index": 121, "image_name": "000000012639.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012639.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 5855.249, "latencies_ms": [5855.249], "images_per_second": 0.171, "prompt_tokens": 23, "response_tokens_est": 128, "n_tiles": 12, "output_text": "baseball bat: 1\nbaseball glove: 1\ncatcher's mask: 1\ncatcher's uniform: 1\nbaseball uniform: 1\nbaseball uniform: 1\nbaseball uniform: 1\nbaseball uniform: 1\nbaseball uniform: 1\nbaseball uniform: 1\nbaseball uniform: 1\nbaseball uniform: 1\nbaseball uniform: 1\nbaseball uniform: 1\nbaseball uniform: 1\nbaseball uniform: 1\nbaseball uniform: 1\nbaseball uniform: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25598.1, "ram_available_mb": 100174.1, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 25598.0, "ram_available_mb": 100174.2, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.11, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 30.36, "peak": 42.94, "min": 26.0}, "VIN": {"avg": 72.23, "peak": 115.05, "min": 58.34}, "VDD_CPU_SOC_MSS": {"avg": 15.76, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 30.36, "energy_joules_est": 177.78, "sample_count": 46, "duration_seconds": 5.856}, "timestamp": "2026-01-16T15:33:30.144226"}
{"image_index": 121, "image_name": "000000012639.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012639.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 4553.69, "latencies_ms": [4553.69], "images_per_second": 0.22, "prompt_tokens": 27, "response_tokens_est": 90, "n_tiles": 12, "output_text": "In the image, the baseball player is positioned in the foreground, crouched down near the dirt area. The baseball bat is held by the player, and the catcher is positioned behind him, wearing a protective helmet and mitt. The background features a group of people sitting on the grass, with a few standing and watching the game. The setting is a park with trees and a clear sky, providing a serene atmosphere for the game.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25598.0, "ram_available_mb": 100174.2, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 25598.2, "ram_available_mb": 100174.0, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.13, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 31.66, "peak": 42.54, "min": 26.0}, "VIN": {"avg": 71.79, "peak": 115.39, "min": 58.55}, "VDD_CPU_SOC_MSS": {"avg": 15.72, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 31.66, "energy_joules_est": 144.18, "sample_count": 35, "duration_seconds": 4.554}, "timestamp": "2026-01-16T15:33:34.708129"}
{"image_index": 121, "image_name": "000000012639.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012639.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 4159.191, "latencies_ms": [4159.191], "images_per_second": 0.24, "prompt_tokens": 21, "response_tokens_est": 78, "n_tiles": 12, "output_text": "The image depicts a lively outdoor scene at a park where a group of people is gathered, likely enjoying a sunny day. In the foreground, a young baseball player is crouched down, preparing to catch a ball, while a group of spectators watches from the sidelines. The setting is a grassy area with trees providing shade, and the atmosphere is casual and relaxed.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25598.2, "ram_available_mb": 100174.0, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25599.0, "ram_available_mb": 100173.2, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.08, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 32.05, "peak": 42.54, "min": 26.01}, "VIN": {"avg": 72.26, "peak": 114.12, "min": 57.97}, "VDD_CPU_SOC_MSS": {"avg": 15.58, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 32.05, "energy_joules_est": 133.32, "sample_count": 33, "duration_seconds": 4.16}, "timestamp": "2026-01-16T15:33:38.874279"}
{"image_index": 121, "image_name": "000000012639.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012639.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3756.295, "latencies_ms": [3756.295], "images_per_second": 0.266, "prompt_tokens": 19, "response_tokens_est": 66, "n_tiles": 12, "output_text": "The image depicts a sunny day at a park where a young baseball player is preparing to bat. The player is wearing a black helmet, a gray t-shirt, gray pants, and black shoes. The ground is covered with a layer of dirt, and there are several people sitting and standing around, enjoying the sunny weather.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25599.0, "ram_available_mb": 100173.2, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25597.8, "ram_available_mb": 100174.4, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.09, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 32.67, "peak": 42.15, "min": 26.01}, "VIN": {"avg": 71.62, "peak": 104.52, "min": 58.53}, "VDD_CPU_SOC_MSS": {"avg": 15.64, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 32.67, "energy_joules_est": 122.73, "sample_count": 29, "duration_seconds": 3.757}, "timestamp": "2026-01-16T15:33:42.636856"}
{"image_index": 122, "image_name": "000000012667.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012667.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2425.229, "latencies_ms": [2425.229], "images_per_second": 0.412, "prompt_tokens": 9, "response_tokens_est": 30, "n_tiles": 12, "output_text": "The image shows a black telephone with a banana placed on top of it, along with a calculator and a piece of paper with handwritten notes.", "error": null, "sys_before": {"cpu_percent": 42.6, "ram_used_mb": 25597.8, "ram_available_mb": 100174.4, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 25597.8, "ram_available_mb": 100174.4, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.08, "peak": 16.15, "min": 13.59}, "VDD_GPU": {"avg": 35.12, "peak": 41.36, "min": 27.57}, "VIN": {"avg": 81.47, "peak": 116.83, "min": 59.18}, "VDD_CPU_SOC_MSS": {"avg": 15.79, "peak": 17.32, "min": 14.57}}, "power_watts_avg": 35.12, "energy_joules_est": 85.19, "sample_count": 18, "duration_seconds": 2.426}, "timestamp": "2026-01-16T15:33:45.164484"}
{"image_index": 122, "image_name": "000000012667.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012667.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2314.113, "latencies_ms": [2314.113], "images_per_second": 0.432, "prompt_tokens": 23, "response_tokens_est": 23, "n_tiles": 12, "output_text": "- Phone\n- Banana\n- Calculator\n- Notebook\n- Cable\n- Keyboard\n- Paper", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25597.8, "ram_available_mb": 100174.4, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 2.6, "ram_used_mb": 25598.1, "ram_available_mb": 100174.1, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.06, "peak": 15.65, "min": 14.43}, "VDD_GPU": {"avg": 36.79, "peak": 42.94, "min": 28.76}, "VIN": {"avg": 75.54, "peak": 107.16, "min": 57.17}, "VDD_CPU_SOC_MSS": {"avg": 15.72, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 36.79, "energy_joules_est": 85.15, "sample_count": 18, "duration_seconds": 2.314}, "timestamp": "2026-01-16T15:33:47.484903"}
{"image_index": 122, "image_name": "000000012667.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012667.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 4597.955, "latencies_ms": [4597.955], "images_per_second": 0.217, "prompt_tokens": 27, "response_tokens_est": 91, "n_tiles": 12, "output_text": "The main objects in the image are a banana, a calculator, and a piece of paper. The banana is positioned in the foreground, slightly to the right, with its stem pointing towards the right side of the image. The calculator is placed in the background, slightly to the left, with its screen facing the left side of the image. The piece of paper is located near the bottom right corner of the image, partially obscured by the calculator.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25598.1, "ram_available_mb": 100174.1, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 25597.8, "ram_available_mb": 100174.4, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.1, "peak": 15.65, "min": 14.43}, "VDD_GPU": {"avg": 31.79, "peak": 43.33, "min": 26.0}, "VIN": {"avg": 72.64, "peak": 114.64, "min": 59.01}, "VDD_CPU_SOC_MSS": {"avg": 15.65, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 31.79, "energy_joules_est": 146.18, "sample_count": 35, "duration_seconds": 4.598}, "timestamp": "2026-01-16T15:33:52.089329"}
{"image_index": 122, "image_name": "000000012667.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012667.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 4290.0, "latencies_ms": [4290.0], "images_per_second": 0.233, "prompt_tokens": 21, "response_tokens_est": 82, "n_tiles": 12, "output_text": "The image depicts a cluttered desk with various items scattered around. The desk surface is white, and there are several objects present, including a black telephone with a cord, a yellow banana, a calculator, a piece of paper with handwritten text, and a small electronic device. The overall setting appears to be a workspace or study area, with the objects suggesting a mix of personal and professional activities.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25597.8, "ram_available_mb": 100174.4, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 25598.1, "ram_available_mb": 100174.1, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.1, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 31.99, "peak": 42.54, "min": 26.0}, "VIN": {"avg": 73.3, "peak": 121.63, "min": 58.64}, "VDD_CPU_SOC_MSS": {"avg": 15.67, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 31.99, "energy_joules_est": 137.25, "sample_count": 33, "duration_seconds": 4.29}, "timestamp": "2026-01-16T15:33:56.390762"}
{"image_index": 122, "image_name": "000000012667.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012667.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3014.935, "latencies_ms": [3014.935], "images_per_second": 0.332, "prompt_tokens": 19, "response_tokens_est": 44, "n_tiles": 12, "output_text": "The image features a white desk with a yellow banana placed on it. The lighting is bright, casting soft shadows on the desk. The banana's vibrant color contrasts with the white surface, making it stand out.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25598.1, "ram_available_mb": 100174.1, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25598.0, "ram_available_mb": 100174.1, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.07, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 34.39, "peak": 42.15, "min": 26.79}, "VIN": {"avg": 77.73, "peak": 130.05, "min": 59.24}, "VDD_CPU_SOC_MSS": {"avg": 15.76, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 34.39, "energy_joules_est": 103.7, "sample_count": 23, "duration_seconds": 3.015}, "timestamp": "2026-01-16T15:33:59.412061"}
{"image_index": 123, "image_name": "000000012670.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012670.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2732.446, "latencies_ms": [2732.446], "images_per_second": 0.366, "prompt_tokens": 9, "response_tokens_est": 77, "n_tiles": 6, "output_text": "The image depicts a bustling outdoor scene with a crowd of people, including young individuals and adults, gathered in a public space. Among the crowd, there are several people holding stuffed animals, adding a playful element to the atmosphere. The setting appears to be a park or a similar public area, with greenery and trees in the background, creating a lively and vibrant environment.", "error": null, "sys_before": {"cpu_percent": 42.9, "ram_used_mb": 25598.0, "ram_available_mb": 100174.1, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 25598.3, "ram_available_mb": 100173.9, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.66, "peak": 15.01, "min": 13.82}, "VDD_GPU": {"avg": 27.83, "peak": 38.21, "min": 23.24}, "VIN": {"avg": 68.15, "peak": 120.78, "min": 55.61}, "VDD_CPU_SOC_MSS": {"avg": 15.39, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 27.83, "energy_joules_est": 76.07, "sample_count": 20, "duration_seconds": 2.733}, "timestamp": "2026-01-16T15:34:02.208703"}
{"image_index": 123, "image_name": "000000012670.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012670.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1655.49, "latencies_ms": [1655.49], "images_per_second": 0.604, "prompt_tokens": 23, "response_tokens_est": 37, "n_tiles": 6, "output_text": "1. People\n2. Teddy bear\n3. Paper\n4. Bag\n5. Backpack\n6. Clothing\n7. Green bag\n8. Green bag", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25598.3, "ram_available_mb": 100173.9, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 25598.0, "ram_available_mb": 100174.2, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.64, "peak": 14.91, "min": 14.12}, "VDD_GPU": {"avg": 31.48, "peak": 38.6, "min": 24.82}, "VIN": {"avg": 72.86, "peak": 111.88, "min": 62.94}, "VDD_CPU_SOC_MSS": {"avg": 15.19, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 31.48, "energy_joules_est": 52.13, "sample_count": 12, "duration_seconds": 1.656}, "timestamp": "2026-01-16T15:34:03.871593"}
{"image_index": 123, "image_name": "000000012670.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012670.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2606.614, "latencies_ms": [2606.614], "images_per_second": 0.384, "prompt_tokens": 27, "response_tokens_est": 71, "n_tiles": 6, "output_text": "The main objects in the image are a group of people, with the foreground and background objects being more distant from the camera. The foreground includes a group of young women and girls, while the background features a crowd of people. The central focus is on the young woman in the red sweater, who is standing out due to her position in the foreground.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25598.0, "ram_available_mb": 100174.2, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 25598.0, "ram_available_mb": 100174.2, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.77, "peak": 15.11, "min": 14.02}, "VDD_GPU": {"avg": 28.32, "peak": 38.59, "min": 23.24}, "VIN": {"avg": 70.22, "peak": 101.05, "min": 55.66}, "VDD_CPU_SOC_MSS": {"avg": 15.35, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 28.32, "energy_joules_est": 73.83, "sample_count": 20, "duration_seconds": 2.607}, "timestamp": "2026-01-16T15:34:06.484010"}
{"image_index": 123, "image_name": "000000012670.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012670.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3135.834, "latencies_ms": [3135.834], "images_per_second": 0.319, "prompt_tokens": 21, "response_tokens_est": 90, "n_tiles": 6, "output_text": "The image depicts a bustling outdoor scene with a large crowd of people gathered in what appears to be a public square or park. The setting is likely during the daytime, as indicated by the natural light and shadows. The crowd consists of various individuals, some of whom are engaged in conversation, while others are looking around or holding items such as books or bags. The atmosphere seems lively and dynamic, with people moving and interacting with one another.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25598.0, "ram_available_mb": 100174.2, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 25598.3, "ram_available_mb": 100173.9, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.81, "peak": 15.11, "min": 14.02}, "VDD_GPU": {"avg": 27.36, "peak": 37.8, "min": 23.24}, "VIN": {"avg": 68.24, "peak": 116.46, "min": 57.92}, "VDD_CPU_SOC_MSS": {"avg": 15.45, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 27.36, "energy_joules_est": 85.81, "sample_count": 24, "duration_seconds": 3.136}, "timestamp": "2026-01-16T15:34:09.625994"}
{"image_index": 123, "image_name": "000000012670.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012670.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2768.416, "latencies_ms": [2768.416], "images_per_second": 0.361, "prompt_tokens": 19, "response_tokens_est": 77, "n_tiles": 6, "output_text": "The image depicts a bustling outdoor scene with a diverse crowd of people. Notable visual attributes include the vibrant colors of the clothing worn by the individuals, such as red, blue, green, and black, which stand out against the overcast sky. The lighting is soft and diffused, likely due to the cloudy weather, casting a gentle glow over the scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25598.3, "ram_available_mb": 100173.9, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 25598.2, "ram_available_mb": 100173.9, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.78, "peak": 15.11, "min": 14.02}, "VDD_GPU": {"avg": 28.01, "peak": 38.21, "min": 23.24}, "VIN": {"avg": 69.21, "peak": 112.49, "min": 55.59}, "VDD_CPU_SOC_MSS": {"avg": 15.37, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 28.01, "energy_joules_est": 77.55, "sample_count": 21, "duration_seconds": 2.769}, "timestamp": "2026-01-16T15:34:12.404541"}
{"image_index": 124, "image_name": "000000012748.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012748.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2089.606, "latencies_ms": [2089.606], "images_per_second": 0.479, "prompt_tokens": 9, "response_tokens_est": 20, "n_tiles": 12, "output_text": "A man is holding a baby while standing next to a horse, both looking at each other.", "error": null, "sys_before": {"cpu_percent": 46.5, "ram_used_mb": 25598.2, "ram_available_mb": 100173.9, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 2.6, "ram_used_mb": 25598.2, "ram_available_mb": 100173.9, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.1, "peak": 16.15, "min": 13.69}, "VDD_GPU": {"avg": 35.87, "peak": 40.97, "min": 29.15}, "VIN": {"avg": 80.07, "peak": 111.05, "min": 59.06}, "VDD_CPU_SOC_MSS": {"avg": 15.87, "peak": 17.32, "min": 14.57}}, "power_watts_avg": 35.87, "energy_joules_est": 74.97, "sample_count": 16, "duration_seconds": 2.09}, "timestamp": "2026-01-16T15:34:14.589390"}
{"image_index": 124, "image_name": "000000012748.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012748.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2612.477, "latencies_ms": [2612.477], "images_per_second": 0.383, "prompt_tokens": 23, "response_tokens_est": 32, "n_tiles": 12, "output_text": "1. Man\n2. Baby\n3. Horse\n4. Building\n5. Door\n6. Window\n7. Floor\n8. Chain", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25598.2, "ram_available_mb": 100173.9, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 25598.2, "ram_available_mb": 100173.9, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.06, "peak": 15.65, "min": 14.43}, "VDD_GPU": {"avg": 35.98, "peak": 43.73, "min": 27.57}, "VIN": {"avg": 79.26, "peak": 134.07, "min": 54.07}, "VDD_CPU_SOC_MSS": {"avg": 15.69, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 35.98, "energy_joules_est": 94.01, "sample_count": 20, "duration_seconds": 2.613}, "timestamp": "2026-01-16T15:34:17.208138"}
{"image_index": 124, "image_name": "000000012748.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012748.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3519.306, "latencies_ms": [3519.306], "images_per_second": 0.284, "prompt_tokens": 27, "response_tokens_est": 59, "n_tiles": 12, "output_text": "The main objects in the image are a man and a baby. The man is holding the baby in his arms. The baby is standing near the man, with the man's left arm extended towards the baby. The baby is positioned in the foreground, while the man is in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25598.2, "ram_available_mb": 100173.9, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 25598.5, "ram_available_mb": 100173.7, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.09, "peak": 15.54, "min": 14.53}, "VDD_GPU": {"avg": 33.16, "peak": 42.54, "min": 26.39}, "VIN": {"avg": 76.14, "peak": 133.99, "min": 58.2}, "VDD_CPU_SOC_MSS": {"avg": 15.72, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 33.16, "energy_joules_est": 116.72, "sample_count": 27, "duration_seconds": 3.52}, "timestamp": "2026-01-16T15:34:20.734731"}
{"image_index": 124, "image_name": "000000012748.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012748.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3483.331, "latencies_ms": [3483.331], "images_per_second": 0.287, "prompt_tokens": 21, "response_tokens_est": 58, "n_tiles": 12, "output_text": "The scene depicts a man and a young child standing in an outdoor area with a horse. The man is holding the child, and both are looking at the horse. The setting appears to be a stable or a barn, with a stone wall and wooden beams visible in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25598.5, "ram_available_mb": 100173.7, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 25598.2, "ram_available_mb": 100173.9, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.1, "peak": 15.75, "min": 14.53}, "VDD_GPU": {"avg": 33.19, "peak": 42.54, "min": 26.39}, "VIN": {"avg": 76.74, "peak": 116.27, "min": 58.24}, "VDD_CPU_SOC_MSS": {"avg": 15.73, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 33.19, "energy_joules_est": 115.63, "sample_count": 27, "duration_seconds": 3.484}, "timestamp": "2026-01-16T15:34:24.224789"}
{"image_index": 124, "image_name": "000000012748.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000012748.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3380.466, "latencies_ms": [3380.466], "images_per_second": 0.296, "prompt_tokens": 19, "response_tokens_est": 55, "n_tiles": 12, "output_text": "The image features a man and a baby in a rustic, stone-walled enclosure. The man is wearing a red shirt, and the baby is dressed in a light blue shirt. The lighting is natural, suggesting it is daytime, and the weather appears to be clear.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25598.2, "ram_available_mb": 100173.9, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 25598.2, "ram_available_mb": 100174.0, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.08, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 33.5, "peak": 42.53, "min": 26.39}, "VIN": {"avg": 71.97, "peak": 104.35, "min": 56.0}, "VDD_CPU_SOC_MSS": {"avg": 15.75, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 33.5, "energy_joules_est": 113.26, "sample_count": 26, "duration_seconds": 3.381}, "timestamp": "2026-01-16T15:34:27.611857"}
{"image_index": 125, "image_name": "000000013004.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013004.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2050.416, "latencies_ms": [2050.416], "images_per_second": 0.488, "prompt_tokens": 9, "response_tokens_est": 19, "n_tiles": 12, "output_text": "The image shows a white plate with a banana on it, resting on a wooden surface.", "error": null, "sys_before": {"cpu_percent": 43.5, "ram_used_mb": 25598.2, "ram_available_mb": 100174.0, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 2.6, "ram_used_mb": 25598.2, "ram_available_mb": 100174.0, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.16, "peak": 16.15, "min": 13.92}, "VDD_GPU": {"avg": 36.77, "peak": 41.36, "min": 30.33}, "VIN": {"avg": 74.11, "peak": 113.45, "min": 58.32}, "VDD_CPU_SOC_MSS": {"avg": 15.88, "peak": 17.32, "min": 14.57}}, "power_watts_avg": 36.77, "energy_joules_est": 75.41, "sample_count": 15, "duration_seconds": 2.051}, "timestamp": "2026-01-16T15:34:29.745252"}
{"image_index": 125, "image_name": "000000013004.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013004.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2673.18, "latencies_ms": [2673.18], "images_per_second": 0.374, "prompt_tokens": 23, "response_tokens_est": 34, "n_tiles": 12, "output_text": "1. plate\n2. banana\n3. bowl\n4. banana peel\n5. bowl\n6. banana\n7. bowl\n8. banana peel", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25598.2, "ram_available_mb": 100174.0, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 25598.2, "ram_available_mb": 100174.0, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.11, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 35.92, "peak": 42.94, "min": 27.18}, "VIN": {"avg": 78.4, "peak": 142.11, "min": 52.87}, "VDD_CPU_SOC_MSS": {"avg": 15.71, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 35.92, "energy_joules_est": 96.03, "sample_count": 20, "duration_seconds": 2.673}, "timestamp": "2026-01-16T15:34:32.424853"}
{"image_index": 125, "image_name": "000000013004.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013004.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3217.657, "latencies_ms": [3217.657], "images_per_second": 0.311, "prompt_tokens": 27, "response_tokens_est": 50, "n_tiles": 12, "output_text": "The main object in the image is a white plate with a banana on it. The banana is placed on the plate, which is positioned on a wooden surface. The banana is the closest object to the viewer, while the plate is further away.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25598.2, "ram_available_mb": 100174.0, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25598.5, "ram_available_mb": 100173.7, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.1, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 33.83, "peak": 42.94, "min": 26.39}, "VIN": {"avg": 75.57, "peak": 120.35, "min": 58.76}, "VDD_CPU_SOC_MSS": {"avg": 15.71, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 33.83, "energy_joules_est": 108.87, "sample_count": 25, "duration_seconds": 3.218}, "timestamp": "2026-01-16T15:34:35.650050"}
{"image_index": 125, "image_name": "000000013004.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013004.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2679.723, "latencies_ms": [2679.723], "images_per_second": 0.373, "prompt_tokens": 21, "response_tokens_est": 34, "n_tiles": 12, "output_text": "The image shows a white plate with a banana on it, placed on a wooden table. The banana appears to be fresh and ripe, with a slightly curved shape.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25598.5, "ram_available_mb": 100173.7, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 25598.7, "ram_available_mb": 100173.5, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.07, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 35.59, "peak": 42.54, "min": 27.59}, "VIN": {"avg": 75.99, "peak": 108.37, "min": 58.4}, "VDD_CPU_SOC_MSS": {"avg": 15.72, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 35.59, "energy_joules_est": 95.4, "sample_count": 20, "duration_seconds": 2.68}, "timestamp": "2026-01-16T15:34:38.336477"}
{"image_index": 125, "image_name": "000000013004.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013004.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2903.833, "latencies_ms": [2903.833], "images_per_second": 0.344, "prompt_tokens": 19, "response_tokens_est": 41, "n_tiles": 12, "output_text": "The image features a white plate with a light brown rim, placed on a wooden table. The lighting is soft and natural, casting a gentle glow on the plate and creating a warm, inviting atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25598.7, "ram_available_mb": 100173.5, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25598.5, "ram_available_mb": 100173.7, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.08, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 34.95, "peak": 42.94, "min": 26.79}, "VIN": {"avg": 74.97, "peak": 116.32, "min": 58.82}, "VDD_CPU_SOC_MSS": {"avg": 15.84, "peak": 16.53, "min": 15.35}}, "power_watts_avg": 34.95, "energy_joules_est": 101.5, "sample_count": 22, "duration_seconds": 2.904}, "timestamp": "2026-01-16T15:34:41.250644"}
{"image_index": 126, "image_name": "000000013177.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013177.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1282.461, "latencies_ms": [1282.461], "images_per_second": 0.78, "prompt_tokens": 9, "response_tokens_est": 26, "n_tiles": 6, "output_text": "A man is kneeling on the ground, working on a metal wheel with a spoke wrench, while another person stands nearby.", "error": null, "sys_before": {"cpu_percent": 41.6, "ram_used_mb": 25598.4, "ram_available_mb": 100173.7, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 25598.4, "ram_available_mb": 100173.7, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.33, "peak": 14.91, "min": 13.82}, "VDD_GPU": {"avg": 33.22, "peak": 37.82, "min": 27.57}, "VIN": {"avg": 76.12, "peak": 111.97, "min": 64.03}, "VDD_CPU_SOC_MSS": {"avg": 15.0, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 33.22, "energy_joules_est": 42.62, "sample_count": 9, "duration_seconds": 1.283}, "timestamp": "2026-01-16T15:34:42.591665"}
{"image_index": 126, "image_name": "000000013177.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013177.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1487.656, "latencies_ms": [1487.656], "images_per_second": 0.672, "prompt_tokens": 23, "response_tokens_est": 32, "n_tiles": 6, "output_text": "1. Wheel\n2. Wheel\n3. Wheel\n4. Wheel\n5. Wheel\n6. Wheel\n7. Wheel\n8. Wheel", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25598.4, "ram_available_mb": 100173.7, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 25598.4, "ram_available_mb": 100173.7, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.72, "peak": 15.01, "min": 14.22}, "VDD_GPU": {"avg": 32.87, "peak": 38.59, "min": 26.0}, "VIN": {"avg": 65.06, "peak": 71.85, "min": 55.52}, "VDD_CPU_SOC_MSS": {"avg": 15.35, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 32.87, "energy_joules_est": 48.91, "sample_count": 11, "duration_seconds": 1.488}, "timestamp": "2026-01-16T15:34:44.085388"}
{"image_index": 126, "image_name": "000000013177.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013177.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2887.291, "latencies_ms": [2887.291], "images_per_second": 0.346, "prompt_tokens": 27, "response_tokens_est": 81, "n_tiles": 6, "output_text": "The main object in the foreground is a man wearing glasses and a green shirt, who is crouched down and working on a metal wheel. The wheel is placed on a flat surface, possibly a workshop floor. In the background, there is a motorcycle with a blue and red design, partially visible. The motorcycle is parked on a brick surface, and there is another motorcycle partially visible behind it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25598.4, "ram_available_mb": 100173.7, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 25598.4, "ram_available_mb": 100173.7, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.83, "peak": 15.11, "min": 14.12}, "VDD_GPU": {"avg": 27.93, "peak": 38.6, "min": 23.24}, "VIN": {"avg": 67.81, "peak": 112.8, "min": 58.61}, "VDD_CPU_SOC_MSS": {"avg": 15.49, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 27.93, "energy_joules_est": 80.65, "sample_count": 23, "duration_seconds": 2.888}, "timestamp": "2026-01-16T15:34:46.978871"}
{"image_index": 126, "image_name": "000000013177.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013177.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2415.971, "latencies_ms": [2415.971], "images_per_second": 0.414, "prompt_tokens": 21, "response_tokens_est": 64, "n_tiles": 6, "output_text": "The image depicts a man working on a motorcycle in a workshop or garage setting. He is focused on cleaning or repairing the front wheel of the motorcycle, which is supported by a metal stand. The man is wearing casual clothing and sandals, and the workshop environment is cluttered with various tools and parts.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25598.4, "ram_available_mb": 100173.7, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 25598.4, "ram_available_mb": 100173.7, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.74, "peak": 15.01, "min": 14.12}, "VDD_GPU": {"avg": 28.65, "peak": 38.19, "min": 23.25}, "VIN": {"avg": 67.61, "peak": 101.26, "min": 55.38}, "VDD_CPU_SOC_MSS": {"avg": 15.38, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 28.65, "energy_joules_est": 69.23, "sample_count": 18, "duration_seconds": 2.416}, "timestamp": "2026-01-16T15:34:49.401077"}
{"image_index": 126, "image_name": "000000013177.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013177.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1932.614, "latencies_ms": [1932.614], "images_per_second": 0.517, "prompt_tokens": 19, "response_tokens_est": 47, "n_tiles": 6, "output_text": "The image features a man wearing a green shirt and dark pants, kneeling on a wet ground. He is working on a metal wheel with a circular pattern. The lighting is dim, and the weather appears to be overcast.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25598.4, "ram_available_mb": 100173.7, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 25598.4, "ram_available_mb": 100173.7, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.7, "peak": 15.01, "min": 14.02}, "VDD_GPU": {"avg": 30.07, "peak": 38.21, "min": 23.64}, "VIN": {"avg": 73.19, "peak": 113.33, "min": 56.83}, "VDD_CPU_SOC_MSS": {"avg": 15.27, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 30.07, "energy_joules_est": 58.12, "sample_count": 15, "duration_seconds": 1.933}, "timestamp": "2026-01-16T15:34:51.339819"}
{"image_index": 127, "image_name": "000000013201.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013201.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1254.651, "latencies_ms": [1254.651], "images_per_second": 0.797, "prompt_tokens": 9, "response_tokens_est": 25, "n_tiles": 6, "output_text": "A skateboarder is performing a trick on a ramp in a park, with trees and a fence in the background.", "error": null, "sys_before": {"cpu_percent": 47.6, "ram_used_mb": 25598.4, "ram_available_mb": 100173.7, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25598.4, "ram_available_mb": 100173.7, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.34, "peak": 14.81, "min": 13.92}, "VDD_GPU": {"avg": 32.48, "peak": 37.82, "min": 26.79}, "VIN": {"avg": 73.02, "peak": 103.62, "min": 62.74}, "VDD_CPU_SOC_MSS": {"avg": 15.09, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 32.48, "energy_joules_est": 40.76, "sample_count": 9, "duration_seconds": 1.255}, "timestamp": "2026-01-16T15:34:52.655926"}
{"image_index": 127, "image_name": "000000013201.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013201.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2172.548, "latencies_ms": [2172.548], "images_per_second": 0.46, "prompt_tokens": 23, "response_tokens_est": 58, "n_tiles": 6, "output_text": "skateboard: 1\nskateboarder: 1\nskateboard: 1\nskateboarder: 1\nskateboard: 1\nskateboard: 1\nskateboard: 1\nskateboard: 1", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25598.4, "ram_available_mb": 100173.7, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 25598.4, "ram_available_mb": 100173.8, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.88, "peak": 15.22, "min": 14.22}, "VDD_GPU": {"avg": 30.21, "peak": 38.59, "min": 24.04}, "VIN": {"avg": 70.4, "peak": 110.27, "min": 57.58}, "VDD_CPU_SOC_MSS": {"avg": 15.5, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 30.21, "energy_joules_est": 65.65, "sample_count": 16, "duration_seconds": 2.173}, "timestamp": "2026-01-16T15:34:54.838858"}
{"image_index": 127, "image_name": "000000013201.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013201.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2234.791, "latencies_ms": [2234.791], "images_per_second": 0.447, "prompt_tokens": 27, "response_tokens_est": 60, "n_tiles": 6, "output_text": "The main subject, a skateboarder, is in the foreground, performing a trick on a ramp. The skateboarder is positioned near the center of the image, with the ramp and the background elements slightly blurred. The background features a fence and a building, indicating an outdoor setting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25598.4, "ram_available_mb": 100173.8, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 25598.4, "ram_available_mb": 100173.8, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.93, "peak": 15.32, "min": 14.22}, "VDD_GPU": {"avg": 29.57, "peak": 38.21, "min": 24.03}, "VIN": {"avg": 74.12, "peak": 114.16, "min": 62.75}, "VDD_CPU_SOC_MSS": {"avg": 15.49, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 29.57, "energy_joules_est": 66.1, "sample_count": 17, "duration_seconds": 2.235}, "timestamp": "2026-01-16T15:34:57.079981"}
{"image_index": 127, "image_name": "000000013201.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013201.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2576.319, "latencies_ms": [2576.319], "images_per_second": 0.388, "prompt_tokens": 21, "response_tokens_est": 73, "n_tiles": 6, "output_text": "The image captures a dynamic moment of a skateboarder performing a trick on a concrete ramp in an outdoor setting. The skateboarder, dressed in a black t-shirt and black pants, is captured mid-air, with their body angled towards the ground and their arms extended. The background features a fence and greenery, suggesting a park or recreational area.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25598.4, "ram_available_mb": 100173.8, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 25598.4, "ram_available_mb": 100173.8, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.01, "peak": 15.32, "min": 14.32}, "VDD_GPU": {"avg": 28.76, "peak": 38.21, "min": 23.64}, "VIN": {"avg": 75.13, "peak": 116.68, "min": 61.18}, "VDD_CPU_SOC_MSS": {"avg": 15.53, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 28.76, "energy_joules_est": 74.1, "sample_count": 20, "duration_seconds": 2.577}, "timestamp": "2026-01-16T15:34:59.662003"}
{"image_index": 127, "image_name": "000000013201.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013201.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1710.56, "latencies_ms": [1710.56], "images_per_second": 0.585, "prompt_tokens": 19, "response_tokens_est": 39, "n_tiles": 6, "output_text": "The notable visual attributes of the image include the vibrant green grass, the clear blue sky, and the bright sunlight. The lighting is natural and bright, casting a warm glow on the scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25598.4, "ram_available_mb": 100173.8, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 25598.4, "ram_available_mb": 100173.8, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.67, "peak": 14.94, "min": 14.12}, "VDD_GPU": {"avg": 31.41, "peak": 39.0, "min": 24.42}, "VIN": {"avg": 67.66, "peak": 101.32, "min": 57.03}, "VDD_CPU_SOC_MSS": {"avg": 15.29, "peak": 15.74, "min": 14.96}}, "power_watts_avg": 31.41, "energy_joules_est": 53.74, "sample_count": 12, "duration_seconds": 1.711}, "timestamp": "2026-01-16T15:35:01.378762"}
{"image_index": 128, "image_name": "000000013291.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013291.jpg", "image_width": 500, "image_height": 335, "image_resolution": "500x335", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1387.632, "latencies_ms": [1387.632], "images_per_second": 0.721, "prompt_tokens": 9, "response_tokens_est": 30, "n_tiles": 6, "output_text": "The image depicts a group of people playing frisbee outdoors during sunset, with the sky displaying a gradient of colors from blue to orange.", "error": null, "sys_before": {"cpu_percent": 41.5, "ram_used_mb": 25598.4, "ram_available_mb": 100173.8, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 25598.0, "ram_available_mb": 100174.2, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.38, "peak": 14.91, "min": 13.82}, "VDD_GPU": {"avg": 32.47, "peak": 37.03, "min": 26.39}, "VIN": {"avg": 78.06, "peak": 116.25, "min": 64.26}, "VDD_CPU_SOC_MSS": {"avg": 15.18, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 32.47, "energy_joules_est": 45.08, "sample_count": 9, "duration_seconds": 1.388}, "timestamp": "2026-01-16T15:35:02.818884"}
{"image_index": 128, "image_name": "000000013291.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013291.jpg", "image_width": 500, "image_height": 335, "image_resolution": "500x335", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1910.269, "latencies_ms": [1910.269], "images_per_second": 0.523, "prompt_tokens": 23, "response_tokens_est": 48, "n_tiles": 6, "output_text": "frisbee: 3\npeople: 4\nshorts: 2\nsocks: 2\nshoes: 2\nt-shirt: 2\nglasses: 1\ntrunks: 1", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25598.0, "ram_available_mb": 100174.2, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 25597.4, "ram_available_mb": 100174.8, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.85, "peak": 15.22, "min": 14.22}, "VDD_GPU": {"avg": 30.67, "peak": 38.6, "min": 24.42}, "VIN": {"avg": 72.62, "peak": 118.17, "min": 63.46}, "VDD_CPU_SOC_MSS": {"avg": 15.49, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 30.67, "energy_joules_est": 58.6, "sample_count": 15, "duration_seconds": 1.911}, "timestamp": "2026-01-16T15:35:04.736144"}
{"image_index": 128, "image_name": "000000013291.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013291.jpg", "image_width": 500, "image_height": 335, "image_resolution": "500x335", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2579.214, "latencies_ms": [2579.214], "images_per_second": 0.388, "prompt_tokens": 27, "response_tokens_est": 73, "n_tiles": 6, "output_text": "The main objects in the image are a group of people playing with frisbees. The individuals are positioned in the foreground, with the man in the center holding a white frisbee. The background features a clear sky and a distant tree line. The man on the right is kneeling on the grass, while the man on the left is standing.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25597.4, "ram_available_mb": 100174.8, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 25597.4, "ram_available_mb": 100174.8, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.96, "peak": 15.32, "min": 14.32}, "VDD_GPU": {"avg": 28.72, "peak": 38.21, "min": 23.64}, "VIN": {"avg": 67.91, "peak": 88.91, "min": 61.14}, "VDD_CPU_SOC_MSS": {"avg": 15.53, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 28.72, "energy_joules_est": 74.09, "sample_count": 20, "duration_seconds": 2.58}, "timestamp": "2026-01-16T15:35:07.321171"}
{"image_index": 128, "image_name": "000000013291.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013291.jpg", "image_width": 500, "image_height": 335, "image_resolution": "500x335", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1935.776, "latencies_ms": [1935.776], "images_per_second": 0.517, "prompt_tokens": 21, "response_tokens_est": 49, "n_tiles": 6, "output_text": "The scene is set in an outdoor park during twilight, with a group of people gathered on a grassy field. They are holding and displaying various colored Frisbees, suggesting they are engaged in a game or a casual gathering.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25597.4, "ram_available_mb": 100174.8, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 25597.4, "ram_available_mb": 100174.8, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.89, "peak": 15.22, "min": 14.22}, "VDD_GPU": {"avg": 30.33, "peak": 37.8, "min": 24.43}, "VIN": {"avg": 74.82, "peak": 113.14, "min": 61.93}, "VDD_CPU_SOC_MSS": {"avg": 15.51, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 30.33, "energy_joules_est": 58.72, "sample_count": 15, "duration_seconds": 1.936}, "timestamp": "2026-01-16T15:35:09.263009"}
{"image_index": 128, "image_name": "000000013291.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013291.jpg", "image_width": 500, "image_height": 335, "image_resolution": "500x335", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2278.71, "latencies_ms": [2278.71], "images_per_second": 0.439, "prompt_tokens": 19, "response_tokens_est": 62, "n_tiles": 6, "output_text": "The image depicts a group of people playing frisbee outdoors during sunset. The sky is clear with a gradient of blue hues, and the lighting is warm, casting a golden glow on the scene. The grass is green, and the individuals are dressed in casual attire suitable for warm weather.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25597.4, "ram_available_mb": 100174.8, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25597.6, "ram_available_mb": 100174.5, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.95, "peak": 15.32, "min": 14.22}, "VDD_GPU": {"avg": 29.64, "peak": 38.6, "min": 24.03}, "VIN": {"avg": 70.8, "peak": 117.26, "min": 57.21}, "VDD_CPU_SOC_MSS": {"avg": 15.61, "peak": 16.14, "min": 14.96}}, "power_watts_avg": 29.64, "energy_joules_est": 67.56, "sample_count": 17, "duration_seconds": 2.279}, "timestamp": "2026-01-16T15:35:11.548316"}
{"image_index": 129, "image_name": "000000013348.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013348.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1570.59, "latencies_ms": [1570.59], "images_per_second": 0.637, "prompt_tokens": 9, "response_tokens_est": 37, "n_tiles": 6, "output_text": "The image shows a large, white airplane with red accents parked on a tarmac, with various ground support vehicles and personnel nearby, indicating that it is likely at an airport.", "error": null, "sys_before": {"cpu_percent": 43.8, "ram_used_mb": 25597.4, "ram_available_mb": 100174.8, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25597.6, "ram_available_mb": 100174.6, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.6, "peak": 15.11, "min": 13.92}, "VDD_GPU": {"avg": 30.73, "peak": 37.03, "min": 24.82}, "VIN": {"avg": 72.73, "peak": 113.57, "min": 60.29}, "VDD_CPU_SOC_MSS": {"avg": 15.32, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 30.73, "energy_joules_est": 48.28, "sample_count": 12, "duration_seconds": 1.571}, "timestamp": "2026-01-16T15:35:13.165636"}
{"image_index": 129, "image_name": "000000013348.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013348.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 4205.229, "latencies_ms": [4205.229], "images_per_second": 0.238, "prompt_tokens": 23, "response_tokens_est": 128, "n_tiles": 6, "output_text": "object: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject:", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25597.6, "ram_available_mb": 100174.6, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 25597.6, "ram_available_mb": 100174.6, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.89, "peak": 15.11, "min": 14.12}, "VDD_GPU": {"avg": 26.47, "peak": 38.59, "min": 22.85}, "VIN": {"avg": 67.89, "peak": 104.7, "min": 61.09}, "VDD_CPU_SOC_MSS": {"avg": 15.37, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 26.47, "energy_joules_est": 111.32, "sample_count": 33, "duration_seconds": 4.206}, "timestamp": "2026-01-16T15:35:17.377050"}
{"image_index": 129, "image_name": "000000013348.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013348.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3000.832, "latencies_ms": [3000.832], "images_per_second": 0.333, "prompt_tokens": 27, "response_tokens_est": 85, "n_tiles": 6, "output_text": "The main object in the foreground is a large airplane with the \"Japan Endless Discovery\" logo on its fuselage. The airplane is parked on the tarmac, with its landing gear extended. In the background, there are other aircraft and ground support vehicles, indicating that this is an airport setting. The airplane is positioned near the tarmac, with other planes and ground equipment visible in the distance.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25597.6, "ram_available_mb": 100174.6, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 25597.6, "ram_available_mb": 100174.6, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.8, "peak": 15.11, "min": 14.02}, "VDD_GPU": {"avg": 27.54, "peak": 38.21, "min": 23.24}, "VIN": {"avg": 67.48, "peak": 116.49, "min": 55.63}, "VDD_CPU_SOC_MSS": {"avg": 15.37, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 27.54, "energy_joules_est": 82.65, "sample_count": 23, "duration_seconds": 3.001}, "timestamp": "2026-01-16T15:35:20.383684"}
{"image_index": 129, "image_name": "000000013348.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013348.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2385.318, "latencies_ms": [2385.318], "images_per_second": 0.419, "prompt_tokens": 21, "response_tokens_est": 63, "n_tiles": 6, "output_text": "The image depicts a large commercial airplane parked on a tarmac at an airport. The scene is set under a partly cloudy sky, with various airport ground support vehicles and personnel present. The airplane is adorned with the airline's logo and the word \"Japan\" on its fuselage.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25597.6, "ram_available_mb": 100174.6, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 25597.6, "ram_available_mb": 100174.6, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.73, "peak": 15.01, "min": 14.02}, "VDD_GPU": {"avg": 28.67, "peak": 37.82, "min": 23.25}, "VIN": {"avg": 67.81, "peak": 114.16, "min": 56.55}, "VDD_CPU_SOC_MSS": {"avg": 15.2, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 28.67, "energy_joules_est": 68.4, "sample_count": 18, "duration_seconds": 2.386}, "timestamp": "2026-01-16T15:35:22.775015"}
{"image_index": 129, "image_name": "000000013348.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013348.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2355.486, "latencies_ms": [2355.486], "images_per_second": 0.425, "prompt_tokens": 19, "response_tokens_est": 62, "n_tiles": 6, "output_text": "The notable visual attributes of the image include a large, white airplane with red accents on its tail and fuselage, parked on a tarmac under a clear blue sky with scattered white clouds. The lighting is bright and natural, indicating daytime, and the weather appears to be clear and sunny.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25597.6, "ram_available_mb": 100174.6, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 25597.4, "ram_available_mb": 100174.8, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.7, "peak": 15.01, "min": 14.02}, "VDD_GPU": {"avg": 28.89, "peak": 38.6, "min": 23.24}, "VIN": {"avg": 70.59, "peak": 114.85, "min": 50.32}, "VDD_CPU_SOC_MSS": {"avg": 15.22, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 28.89, "energy_joules_est": 68.07, "sample_count": 18, "duration_seconds": 2.356}, "timestamp": "2026-01-16T15:35:25.137263"}
{"image_index": 130, "image_name": "000000013546.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013546.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1332.848, "latencies_ms": [1332.848], "images_per_second": 0.75, "prompt_tokens": 9, "response_tokens_est": 28, "n_tiles": 6, "output_text": "A young person is performing a skateboard trick on a concrete ledge in a park, surrounded by green grass and other park amenities.", "error": null, "sys_before": {"cpu_percent": 41.3, "ram_used_mb": 25597.4, "ram_available_mb": 100174.8, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25597.4, "ram_available_mb": 100174.8, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.42, "peak": 14.91, "min": 13.92}, "VDD_GPU": {"avg": 31.79, "peak": 37.03, "min": 26.0}, "VIN": {"avg": 70.29, "peak": 105.51, "min": 59.86}, "VDD_CPU_SOC_MSS": {"avg": 15.12, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 31.79, "energy_joules_est": 42.4, "sample_count": 10, "duration_seconds": 1.334}, "timestamp": "2026-01-16T15:35:26.518759"}
{"image_index": 130, "image_name": "000000013546.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013546.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1757.801, "latencies_ms": [1757.801], "images_per_second": 0.569, "prompt_tokens": 23, "response_tokens_est": 42, "n_tiles": 6, "output_text": "1. Person\n2. Skateboard\n3. Park bench\n4. Sidewalk\n5. Street\n6. Green fence\n7. Green trash can\n8. Green metal structure", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25597.4, "ram_available_mb": 100174.8, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 25597.3, "ram_available_mb": 100174.8, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.77, "peak": 15.11, "min": 14.22}, "VDD_GPU": {"avg": 31.36, "peak": 38.6, "min": 24.82}, "VIN": {"avg": 68.29, "peak": 106.47, "min": 55.93}, "VDD_CPU_SOC_MSS": {"avg": 15.32, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 31.36, "energy_joules_est": 55.13, "sample_count": 13, "duration_seconds": 1.758}, "timestamp": "2026-01-16T15:35:28.282267"}
{"image_index": 130, "image_name": "000000013546.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013546.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3090.846, "latencies_ms": [3090.846], "images_per_second": 0.324, "prompt_tokens": 27, "response_tokens_est": 92, "n_tiles": 6, "output_text": "The main object in the foreground is a young person skateboarding on a concrete ledge. The person is positioned near the edge of the ledge, with their body leaning forward and arms outstretched for balance. In the background, there is a green metal bench and a trash can, as well as a street with a parked car and a person walking. The scene is set in a park or skate park with green grass and trees in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25597.3, "ram_available_mb": 100174.8, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 25597.3, "ram_available_mb": 100174.8, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.01, "peak": 15.32, "min": 14.22}, "VDD_GPU": {"avg": 27.82, "peak": 38.21, "min": 23.64}, "VIN": {"avg": 69.82, "peak": 120.34, "min": 57.93}, "VDD_CPU_SOC_MSS": {"avg": 15.55, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 27.82, "energy_joules_est": 86.0, "sample_count": 24, "duration_seconds": 3.091}, "timestamp": "2026-01-16T15:35:31.379359"}
{"image_index": 130, "image_name": "000000013546.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013546.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2309.278, "latencies_ms": [2309.278], "images_per_second": 0.433, "prompt_tokens": 21, "response_tokens_est": 63, "n_tiles": 6, "output_text": "The image depicts a young person skateboarding in a park-like setting. The skateboarder is performing a trick on a concrete ledge, surrounded by green grass and a few benches. The scene is set in a public park with a paved path, trees, and a fence in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25597.3, "ram_available_mb": 100174.8, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 25586.0, "ram_available_mb": 100186.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.93, "peak": 15.22, "min": 14.22}, "VDD_GPU": {"avg": 29.5, "peak": 38.21, "min": 23.64}, "VIN": {"avg": 71.4, "peak": 115.22, "min": 58.03}, "VDD_CPU_SOC_MSS": {"avg": 15.52, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 29.5, "energy_joules_est": 68.14, "sample_count": 17, "duration_seconds": 2.31}, "timestamp": "2026-01-16T15:35:33.695238"}
{"image_index": 130, "image_name": "000000013546.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013546.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2687.989, "latencies_ms": [2687.989], "images_per_second": 0.372, "prompt_tokens": 19, "response_tokens_est": 77, "n_tiles": 6, "output_text": "The image depicts a young person skateboarding on a concrete ramp in a park. The skateboarder is wearing a yellow t-shirt, black pants, and a black cap, and is captured mid-action with their arms outstretched for balance. The scene is bathed in natural sunlight, casting long shadows on the ground, indicating it is likely a bright, sunny day.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25586.0, "ram_available_mb": 100186.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 25585.8, "ram_available_mb": 100186.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.98, "peak": 15.32, "min": 14.32}, "VDD_GPU": {"avg": 28.58, "peak": 37.8, "min": 23.64}, "VIN": {"avg": 67.23, "peak": 111.22, "min": 56.13}, "VDD_CPU_SOC_MSS": {"avg": 15.51, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 28.58, "energy_joules_est": 76.84, "sample_count": 20, "duration_seconds": 2.689}, "timestamp": "2026-01-16T15:35:36.389914"}
{"image_index": 131, "image_name": "000000013597.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013597.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1276.566, "latencies_ms": [1276.566], "images_per_second": 0.783, "prompt_tokens": 9, "response_tokens_est": 26, "n_tiles": 6, "output_text": "The image shows a close-up of a chocolate cake with a dollop of cream on a plate adorned with floral patterns.", "error": null, "sys_before": {"cpu_percent": 41.0, "ram_used_mb": 25585.8, "ram_available_mb": 100186.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25586.2, "ram_available_mb": 100185.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.43, "peak": 14.81, "min": 13.92}, "VDD_GPU": {"avg": 32.43, "peak": 37.03, "min": 27.18}, "VIN": {"avg": 68.23, "peak": 110.29, "min": 56.73}, "VDD_CPU_SOC_MSS": {"avg": 15.18, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 32.43, "energy_joules_est": 41.41, "sample_count": 9, "duration_seconds": 1.277}, "timestamp": "2026-01-16T15:35:37.714193"}
{"image_index": 131, "image_name": "000000013597.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013597.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1702.982, "latencies_ms": [1702.982], "images_per_second": 0.587, "prompt_tokens": 23, "response_tokens_est": 40, "n_tiles": 6, "output_text": "object: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25586.2, "ram_available_mb": 100185.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 25586.9, "ram_available_mb": 100185.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.78, "peak": 15.11, "min": 14.22}, "VDD_GPU": {"avg": 31.66, "peak": 38.6, "min": 24.82}, "VIN": {"avg": 79.61, "peak": 116.16, "min": 62.46}, "VDD_CPU_SOC_MSS": {"avg": 15.32, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 31.66, "energy_joules_est": 53.93, "sample_count": 13, "duration_seconds": 1.703}, "timestamp": "2026-01-16T15:35:39.423274"}
{"image_index": 131, "image_name": "000000013597.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013597.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2100.086, "latencies_ms": [2100.086], "images_per_second": 0.476, "prompt_tokens": 27, "response_tokens_est": 55, "n_tiles": 6, "output_text": "The main object in the image is a chocolate cake, which is positioned on a white plate with a floral pattern. The cake is in the foreground, and the plate is placed on a table. The cake is being cut with a knife, which is in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25586.9, "ram_available_mb": 100185.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 25587.1, "ram_available_mb": 100185.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.88, "peak": 15.22, "min": 14.22}, "VDD_GPU": {"avg": 30.06, "peak": 38.6, "min": 24.03}, "VIN": {"avg": 74.65, "peak": 115.86, "min": 56.46}, "VDD_CPU_SOC_MSS": {"avg": 15.45, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 30.06, "energy_joules_est": 63.15, "sample_count": 16, "duration_seconds": 2.101}, "timestamp": "2026-01-16T15:35:41.530346"}
{"image_index": 131, "image_name": "000000013597.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013597.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2185.347, "latencies_ms": [2185.347], "images_per_second": 0.458, "prompt_tokens": 21, "response_tokens_est": 58, "n_tiles": 6, "output_text": "The image depicts a close-up view of a chocolate cake with a dollop of cream on a white plate adorned with gold floral patterns. The cake is being cut with a knife, and the cream is being poured onto the cake, creating a visually appealing and appetizing scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25587.1, "ram_available_mb": 100185.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 25587.4, "ram_available_mb": 100184.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.91, "peak": 15.22, "min": 14.32}, "VDD_GPU": {"avg": 29.5, "peak": 38.21, "min": 24.03}, "VIN": {"avg": 67.5, "peak": 90.71, "min": 49.41}, "VDD_CPU_SOC_MSS": {"avg": 15.49, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 29.5, "energy_joules_est": 64.48, "sample_count": 17, "duration_seconds": 2.186}, "timestamp": "2026-01-16T15:35:43.721964"}
{"image_index": 131, "image_name": "000000013597.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013597.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2093.14, "latencies_ms": [2093.14], "images_per_second": 0.478, "prompt_tokens": 19, "response_tokens_est": 55, "n_tiles": 6, "output_text": "The image features a chocolate cake with a glossy, shiny chocolate glaze and a golden-brown frosting. The cake is placed on a white plate with a floral pattern, and the lighting is warm and soft, highlighting the rich colors and textures of the cake.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25587.4, "ram_available_mb": 100184.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 25587.4, "ram_available_mb": 100184.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.9, "peak": 15.22, "min": 14.32}, "VDD_GPU": {"avg": 29.86, "peak": 37.82, "min": 24.03}, "VIN": {"avg": 70.22, "peak": 99.36, "min": 58.99}, "VDD_CPU_SOC_MSS": {"avg": 15.48, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 29.86, "energy_joules_est": 62.51, "sample_count": 16, "duration_seconds": 2.093}, "timestamp": "2026-01-16T15:35:45.821103"}
{"image_index": 132, "image_name": "000000013659.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013659.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2645.75, "latencies_ms": [2645.75], "images_per_second": 0.378, "prompt_tokens": 9, "response_tokens_est": 37, "n_tiles": 12, "output_text": "The image depicts a cluttered office space with various items scattered across the floor and tables, including a laptop, a person working on it, and other office supplies and equipment.", "error": null, "sys_before": {"cpu_percent": 41.5, "ram_used_mb": 25587.4, "ram_available_mb": 100184.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 25587.4, "ram_available_mb": 100184.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.17, "peak": 16.15, "min": 14.1}, "VDD_GPU": {"avg": 34.21, "peak": 40.97, "min": 26.79}, "VIN": {"avg": 73.44, "peak": 100.21, "min": 58.29}, "VDD_CPU_SOC_MSS": {"avg": 15.84, "peak": 17.32, "min": 15.35}}, "power_watts_avg": 34.21, "energy_joules_est": 90.53, "sample_count": 20, "duration_seconds": 2.646}, "timestamp": "2026-01-16T15:35:48.548894"}
{"image_index": 132, "image_name": "000000013659.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013659.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2638.04, "latencies_ms": [2638.04], "images_per_second": 0.379, "prompt_tokens": 23, "response_tokens_est": 33, "n_tiles": 12, "output_text": "1. Laptop\n2. Computer monitor\n3. Paper\n4. Chair\n5. Box\n6. Box\n7. Box\n8. Box", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25587.4, "ram_available_mb": 100184.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 25588.5, "ram_available_mb": 100183.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.07, "peak": 15.65, "min": 14.43}, "VDD_GPU": {"avg": 35.75, "peak": 42.53, "min": 27.18}, "VIN": {"avg": 77.56, "peak": 116.66, "min": 58.47}, "VDD_CPU_SOC_MSS": {"avg": 15.73, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 35.75, "energy_joules_est": 94.32, "sample_count": 20, "duration_seconds": 2.638}, "timestamp": "2026-01-16T15:35:51.193018"}
{"image_index": 132, "image_name": "000000013659.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013659.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 4661.405, "latencies_ms": [4661.405], "images_per_second": 0.215, "prompt_tokens": 27, "response_tokens_est": 93, "n_tiles": 12, "output_text": "In the image, the main objects are arranged in a somewhat cluttered workspace. The foreground features a man seated at a desk with a laptop, surrounded by various electronic components and tools. To his left, there is a red box and a black bag. In the background, there are more desks with people working, a red chair, and a computer monitor. The overall spatial arrangement suggests a busy and active work environment with multiple individuals engaged in different tasks.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25588.5, "ram_available_mb": 100183.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 25588.5, "ram_available_mb": 100183.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.1, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 31.59, "peak": 42.94, "min": 26.0}, "VIN": {"avg": 70.92, "peak": 110.26, "min": 56.21}, "VDD_CPU_SOC_MSS": {"avg": 15.55, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 31.59, "energy_joules_est": 147.26, "sample_count": 36, "duration_seconds": 4.662}, "timestamp": "2026-01-16T15:35:55.860776"}
{"image_index": 132, "image_name": "000000013659.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013659.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3810.071, "latencies_ms": [3810.071], "images_per_second": 0.262, "prompt_tokens": 21, "response_tokens_est": 68, "n_tiles": 12, "output_text": "The image depicts a cluttered office or workshop environment with various items scattered across the floor and tables. A group of people is engaged in work, with some seated at desks and others standing. The setting appears to be a workspace with a mix of electronic devices, papers, and tools, suggesting a busy and active work environment.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25588.5, "ram_available_mb": 100183.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25588.4, "ram_available_mb": 100183.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.1, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 32.83, "peak": 42.15, "min": 26.39}, "VIN": {"avg": 75.17, "peak": 122.46, "min": 57.91}, "VDD_CPU_SOC_MSS": {"avg": 15.62, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 32.83, "energy_joules_est": 125.1, "sample_count": 29, "duration_seconds": 3.811}, "timestamp": "2026-01-16T15:35:59.677340"}
{"image_index": 132, "image_name": "000000013659.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013659.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2640.223, "latencies_ms": [2640.223], "images_per_second": 0.379, "prompt_tokens": 19, "response_tokens_est": 33, "n_tiles": 12, "output_text": "The image depicts a cluttered office space with a yellowish floor and walls. The lighting is dim, with a mix of natural and artificial light sources.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25588.4, "ram_available_mb": 100183.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 25588.1, "ram_available_mb": 100184.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.82, "peak": 15.34, "min": 14.12}, "VDD_GPU": {"avg": 35.59, "peak": 42.54, "min": 26.79}, "VIN": {"avg": 78.44, "peak": 137.14, "min": 58.77}, "VDD_CPU_SOC_MSS": {"avg": 15.47, "peak": 16.14, "min": 14.56}}, "power_watts_avg": 35.59, "energy_joules_est": 93.98, "sample_count": 20, "duration_seconds": 2.641}, "timestamp": "2026-01-16T15:36:02.323975"}
{"image_index": 133, "image_name": "000000013729.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013729.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2521.426, "latencies_ms": [2521.426], "images_per_second": 0.397, "prompt_tokens": 9, "response_tokens_est": 33, "n_tiles": 12, "output_text": "The image depicts a group of people playing a video game in a cozy living room, with one person standing on the floor and others standing on a couch.", "error": null, "sys_before": {"cpu_percent": 45.1, "ram_used_mb": 25588.1, "ram_available_mb": 100184.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 25588.3, "ram_available_mb": 100183.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.03, "peak": 16.15, "min": 13.39}, "VDD_GPU": {"avg": 34.91, "peak": 42.15, "min": 27.57}, "VIN": {"avg": 78.09, "peak": 119.45, "min": 58.53}, "VDD_CPU_SOC_MSS": {"avg": 15.93, "peak": 17.32, "min": 14.57}}, "power_watts_avg": 34.91, "energy_joules_est": 88.03, "sample_count": 19, "duration_seconds": 2.522}, "timestamp": "2026-01-16T15:36:04.947907"}
{"image_index": 133, "image_name": "000000013729.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013729.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 3221.349, "latencies_ms": [3221.349], "images_per_second": 0.31, "prompt_tokens": 23, "response_tokens_est": 50, "n_tiles": 12, "output_text": "- woman: 1\n- man: 3\n- game controller: 1\n- couch: 1\n- table: 1\n- bottle: 1\n- basket: 1\n- framed picture: 1", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25588.3, "ram_available_mb": 100183.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 25588.1, "ram_available_mb": 100184.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.06, "peak": 15.65, "min": 14.43}, "VDD_GPU": {"avg": 33.94, "peak": 42.94, "min": 26.39}, "VIN": {"avg": 71.78, "peak": 95.77, "min": 58.48}, "VDD_CPU_SOC_MSS": {"avg": 15.65, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 33.94, "energy_joules_est": 109.35, "sample_count": 25, "duration_seconds": 3.222}, "timestamp": "2026-01-16T15:36:08.176055"}
{"image_index": 133, "image_name": "000000013729.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013729.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3996.195, "latencies_ms": [3996.195], "images_per_second": 0.25, "prompt_tokens": 27, "response_tokens_est": 73, "n_tiles": 12, "output_text": "The main objects in the image are a group of people playing a video game in a living room. The individuals are positioned in the foreground, with the woman in the center foreground and the two men on either side. The background features a couch, a wooden cabinet, and a television stand, while a basket and a small table are also visible in the foreground.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25588.1, "ram_available_mb": 100184.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 25586.6, "ram_available_mb": 100185.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.08, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 32.43, "peak": 42.94, "min": 26.01}, "VIN": {"avg": 73.03, "peak": 113.37, "min": 48.46}, "VDD_CPU_SOC_MSS": {"avg": 15.75, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 32.43, "energy_joules_est": 129.61, "sample_count": 31, "duration_seconds": 3.997}, "timestamp": "2026-01-16T15:36:12.178117"}
{"image_index": 133, "image_name": "000000013729.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013729.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 4800.511, "latencies_ms": [4800.511], "images_per_second": 0.208, "prompt_tokens": 21, "response_tokens_est": 97, "n_tiles": 12, "output_text": "The image depicts a casual indoor scene where three individuals are engaged in a video game. The setting appears to be a living room with a carpeted floor, a couch, a wooden table, and various personal items scattered around. The individuals are casually dressed, with one person in a white tank top and shorts, another in a striped shirt and shorts, and the third in a green shirt and shorts. They are all barefoot, suggesting a relaxed and informal atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25586.6, "ram_available_mb": 100185.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 25586.8, "ram_available_mb": 100185.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.09, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 31.24, "peak": 42.54, "min": 26.0}, "VIN": {"avg": 74.55, "peak": 110.76, "min": 55.54}, "VDD_CPU_SOC_MSS": {"avg": 15.71, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 31.24, "energy_joules_est": 149.98, "sample_count": 38, "duration_seconds": 4.801}, "timestamp": "2026-01-16T15:36:16.987093"}
{"image_index": 133, "image_name": "000000013729.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013729.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2612.05, "latencies_ms": [2612.05], "images_per_second": 0.383, "prompt_tokens": 19, "response_tokens_est": 32, "n_tiles": 12, "output_text": "The image depicts a cozy living room with a beige carpeted floor. The room is well-lit, with a warm and inviting ambiance.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25586.8, "ram_available_mb": 100185.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 25587.0, "ram_available_mb": 100185.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.06, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 35.71, "peak": 42.54, "min": 27.57}, "VIN": {"avg": 81.43, "peak": 119.33, "min": 58.44}, "VDD_CPU_SOC_MSS": {"avg": 15.69, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 35.71, "energy_joules_est": 93.29, "sample_count": 20, "duration_seconds": 2.613}, "timestamp": "2026-01-16T15:36:19.609729"}
{"image_index": 134, "image_name": "000000013774.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013774.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2061.454, "latencies_ms": [2061.454], "images_per_second": 0.485, "prompt_tokens": 9, "response_tokens_est": 19, "n_tiles": 12, "output_text": "A lone person stands on a snowy beach, watching the sun set over the ocean.", "error": null, "sys_before": {"cpu_percent": 40.9, "ram_used_mb": 25586.5, "ram_available_mb": 100185.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 2.5, "ram_used_mb": 25586.6, "ram_available_mb": 100185.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.1, "peak": 16.15, "min": 13.69}, "VDD_GPU": {"avg": 36.74, "peak": 41.36, "min": 29.94}, "VIN": {"avg": 79.12, "peak": 114.72, "min": 60.01}, "VDD_CPU_SOC_MSS": {"avg": 16.03, "peak": 17.32, "min": 14.57}}, "power_watts_avg": 36.74, "energy_joules_est": 75.75, "sample_count": 15, "duration_seconds": 2.062}, "timestamp": "2026-01-16T15:36:21.754437"}
{"image_index": 134, "image_name": "000000013774.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013774.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2876.457, "latencies_ms": [2876.457], "images_per_second": 0.348, "prompt_tokens": 23, "response_tokens_est": 40, "n_tiles": 12, "output_text": "object: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25586.6, "ram_available_mb": 100185.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 25586.4, "ram_available_mb": 100185.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.08, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 35.06, "peak": 42.94, "min": 26.79}, "VIN": {"avg": 75.25, "peak": 110.84, "min": 54.3}, "VDD_CPU_SOC_MSS": {"avg": 15.69, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 35.06, "energy_joules_est": 100.87, "sample_count": 22, "duration_seconds": 2.877}, "timestamp": "2026-01-16T15:36:24.637640"}
{"image_index": 134, "image_name": "000000013774.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013774.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3319.67, "latencies_ms": [3319.67], "images_per_second": 0.301, "prompt_tokens": 27, "response_tokens_est": 53, "n_tiles": 12, "output_text": "The main objects in the image are the sun, the person, and the beach. The person is standing near the water's edge, with the sun positioned in the background. The beach is the foreground, with the person and the sun being the main subjects.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25586.4, "ram_available_mb": 100185.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 25586.6, "ram_available_mb": 100185.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.08, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 33.65, "peak": 42.53, "min": 26.39}, "VIN": {"avg": 73.95, "peak": 117.16, "min": 58.61}, "VDD_CPU_SOC_MSS": {"avg": 15.66, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 33.65, "energy_joules_est": 111.72, "sample_count": 26, "duration_seconds": 3.32}, "timestamp": "2026-01-16T15:36:27.964629"}
{"image_index": 134, "image_name": "000000013774.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013774.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3350.221, "latencies_ms": [3350.221], "images_per_second": 0.298, "prompt_tokens": 21, "response_tokens_est": 54, "n_tiles": 12, "output_text": "The image captures a serene beach scene during sunset, with the sun low on the horizon casting a warm glow over the water and the sand. A lone person is standing on the shore, gazing at the sun, while the waves gently lap at the shore.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25586.6, "ram_available_mb": 100185.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 25588.6, "ram_available_mb": 100183.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.07, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 33.64, "peak": 42.53, "min": 26.39}, "VIN": {"avg": 74.87, "peak": 107.91, "min": 58.75}, "VDD_CPU_SOC_MSS": {"avg": 15.66, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 33.64, "energy_joules_est": 112.71, "sample_count": 26, "duration_seconds": 3.351}, "timestamp": "2026-01-16T15:36:31.321351"}
{"image_index": 134, "image_name": "000000013774.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013774.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 4293.848, "latencies_ms": [4293.848], "images_per_second": 0.233, "prompt_tokens": 19, "response_tokens_est": 82, "n_tiles": 12, "output_text": "The image captures a serene beach scene during sunset, with the sun low on the horizon casting a warm, golden glow across the sky and reflecting off the water. The sky is a gradient of warm colors, transitioning from a deep blue to a soft orange near the horizon. The beach is covered in a thin layer of snow, and the water is calm with gentle waves lapping at the shore.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25588.6, "ram_available_mb": 100183.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25588.6, "ram_available_mb": 100183.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.08, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 31.82, "peak": 42.54, "min": 26.0}, "VIN": {"avg": 73.5, "peak": 131.44, "min": 53.72}, "VDD_CPU_SOC_MSS": {"avg": 15.59, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 31.82, "energy_joules_est": 136.64, "sample_count": 34, "duration_seconds": 4.294}, "timestamp": "2026-01-16T15:36:35.621664"}
{"image_index": 135, "image_name": "000000013923.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013923.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2828.921, "latencies_ms": [2828.921], "images_per_second": 0.353, "prompt_tokens": 9, "response_tokens_est": 84, "n_tiles": 6, "output_text": "The image depicts a cozy living room with a modern and stylish interior design, featuring a white sofa adorned with colorful pillows, a round coffee table with a vase of flowers, and a red chair with a white cushion. The room is well-lit by natural light coming through large windows, and the walls are decorated with various wall art pieces, including a circular wall clock and abstract wall hangings.", "error": null, "sys_before": {"cpu_percent": 40.5, "ram_used_mb": 25588.6, "ram_available_mb": 100183.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 25588.6, "ram_available_mb": 100183.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.83, "peak": 15.32, "min": 13.82}, "VDD_GPU": {"avg": 27.99, "peak": 37.82, "min": 23.64}, "VIN": {"avg": 69.17, "peak": 112.06, "min": 56.72}, "VDD_CPU_SOC_MSS": {"avg": 15.45, "peak": 15.75, "min": 14.57}}, "power_watts_avg": 27.99, "energy_joules_est": 79.19, "sample_count": 22, "duration_seconds": 2.829}, "timestamp": "2026-01-16T15:36:38.515295"}
{"image_index": 135, "image_name": "000000013923.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013923.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1649.264, "latencies_ms": [1649.264], "images_per_second": 0.606, "prompt_tokens": 23, "response_tokens_est": 38, "n_tiles": 6, "output_text": "1. Living room\n2. Sofa\n3. Coffee table\n4. Rug\n5. Chairs\n6. TV stand\n7. Decorative plates\n8. Plants", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25588.6, "ram_available_mb": 100183.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 25588.3, "ram_available_mb": 100183.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.79, "peak": 15.11, "min": 14.32}, "VDD_GPU": {"avg": 31.55, "peak": 38.21, "min": 25.21}, "VIN": {"avg": 70.64, "peak": 119.27, "min": 59.04}, "VDD_CPU_SOC_MSS": {"avg": 15.35, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 31.55, "energy_joules_est": 52.06, "sample_count": 12, "duration_seconds": 1.65}, "timestamp": "2026-01-16T15:36:40.175057"}
{"image_index": 135, "image_name": "000000013923.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013923.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3562.18, "latencies_ms": [3562.18], "images_per_second": 0.281, "prompt_tokens": 27, "response_tokens_est": 105, "n_tiles": 6, "output_text": "The main objects in the image are a living room and a dining area. The living room features a white sofa with yellow and striped pillows, a black and white patterned rug, a coffee table with a vase of flowers, and a television on a stand. The dining area includes a round white table with a red chair, a black and white patterned rug, and a small side table with a vase. The living room is positioned in the background, while the dining area is in the foreground.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25588.3, "ram_available_mb": 100183.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 25588.3, "ram_available_mb": 100183.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.84, "peak": 15.01, "min": 14.12}, "VDD_GPU": {"avg": 27.0, "peak": 38.21, "min": 22.85}, "VIN": {"avg": 69.03, "peak": 108.16, "min": 61.81}, "VDD_CPU_SOC_MSS": {"avg": 15.43, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 27.0, "energy_joules_est": 96.19, "sample_count": 28, "duration_seconds": 3.562}, "timestamp": "2026-01-16T15:36:43.743383"}
{"image_index": 135, "image_name": "000000013923.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013923.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3333.966, "latencies_ms": [3333.966], "images_per_second": 0.3, "prompt_tokens": 21, "response_tokens_est": 97, "n_tiles": 6, "output_text": "The image depicts a cozy, well-lit living room with a modern and stylish interior. The room features a white sofa adorned with colorful pillows, a black and white patterned rug, and a round white coffee table with a vase of flowers. The space is decorated with various decorative items, including a television on a stand, a plant, and wall art. The room is furnished with red chairs and a small round table, creating a warm and inviting atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25588.3, "ram_available_mb": 100183.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 25588.3, "ram_available_mb": 100183.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.83, "peak": 15.11, "min": 14.02}, "VDD_GPU": {"avg": 27.09, "peak": 38.21, "min": 23.24}, "VIN": {"avg": 69.18, "peak": 101.37, "min": 59.78}, "VDD_CPU_SOC_MSS": {"avg": 15.42, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 27.09, "energy_joules_est": 90.33, "sample_count": 26, "duration_seconds": 3.334}, "timestamp": "2026-01-16T15:36:47.083447"}
{"image_index": 135, "image_name": "000000013923.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000013923.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2596.068, "latencies_ms": [2596.068], "images_per_second": 0.385, "prompt_tokens": 19, "response_tokens_est": 71, "n_tiles": 6, "output_text": "The room features a warm and inviting ambiance with a combination of natural and artificial lighting. The walls are painted in a light color, and the furniture includes a white sofa, red chairs, and a black and white patterned rug. The room is well-lit, with natural light streaming in through the windows and additional artificial lighting from the ceiling.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25588.3, "ram_available_mb": 100183.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 25588.3, "ram_available_mb": 100183.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.77, "peak": 15.11, "min": 14.02}, "VDD_GPU": {"avg": 28.29, "peak": 38.21, "min": 23.24}, "VIN": {"avg": 65.11, "peak": 91.54, "min": 55.16}, "VDD_CPU_SOC_MSS": {"avg": 15.32, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 28.29, "energy_joules_est": 73.45, "sample_count": 20, "duration_seconds": 2.596}, "timestamp": "2026-01-16T15:36:49.686125"}
{"image_index": 136, "image_name": "000000014007.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014007.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 961.54, "latencies_ms": [961.54], "images_per_second": 1.04, "prompt_tokens": 9, "response_tokens_est": 14, "n_tiles": 6, "output_text": "A cat is sitting on top of a refrigerator, looking around.", "error": null, "sys_before": {"cpu_percent": 32.3, "ram_used_mb": 25588.3, "ram_available_mb": 100183.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 25588.3, "ram_available_mb": 100183.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.25, "peak": 14.51, "min": 13.92}, "VDD_GPU": {"avg": 33.65, "peak": 37.03, "min": 29.94}, "VIN": {"avg": 92.36, "peak": 127.24, "min": 63.63}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 33.65, "energy_joules_est": 32.37, "sample_count": 7, "duration_seconds": 0.962}, "timestamp": "2026-01-16T15:36:50.701070"}
{"image_index": 136, "image_name": "000000014007.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014007.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1694.822, "latencies_ms": [1694.822], "images_per_second": 0.59, "prompt_tokens": 23, "response_tokens_est": 40, "n_tiles": 6, "output_text": "1. Cat\n2. Refrigerator\n3. Toy\n4. Toy box\n5. Toy shelf\n6. Toy box\n7. Toy box\n8. Toy box", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25588.3, "ram_available_mb": 100183.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 25588.3, "ram_available_mb": 100183.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.74, "peak": 15.22, "min": 14.12}, "VDD_GPU": {"avg": 32.21, "peak": 39.39, "min": 24.82}, "VIN": {"avg": 69.53, "peak": 115.23, "min": 56.82}, "VDD_CPU_SOC_MSS": {"avg": 15.29, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 32.21, "energy_joules_est": 54.6, "sample_count": 13, "duration_seconds": 1.695}, "timestamp": "2026-01-16T15:36:52.401967"}
{"image_index": 136, "image_name": "000000014007.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014007.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1857.333, "latencies_ms": [1857.333], "images_per_second": 0.538, "prompt_tokens": 27, "response_tokens_est": 46, "n_tiles": 6, "output_text": "The main object in the foreground is a cat sitting on top of a refrigerator. The refrigerator is located to the right of the cat. The background includes a wall and a ceiling, which are not directly visible in the image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25588.3, "ram_available_mb": 100183.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 25588.6, "ram_available_mb": 100183.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.88, "peak": 15.22, "min": 14.32}, "VDD_GPU": {"avg": 30.67, "peak": 38.19, "min": 24.43}, "VIN": {"avg": 71.71, "peak": 104.5, "min": 61.14}, "VDD_CPU_SOC_MSS": {"avg": 15.47, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 30.67, "energy_joules_est": 56.97, "sample_count": 14, "duration_seconds": 1.858}, "timestamp": "2026-01-16T15:36:54.265093"}
{"image_index": 136, "image_name": "000000014007.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014007.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1928.225, "latencies_ms": [1928.225], "images_per_second": 0.519, "prompt_tokens": 21, "response_tokens_est": 49, "n_tiles": 6, "output_text": "The image depicts a domestic scene featuring a cat sitting on top of a refrigerator. The cat appears curious and is looking towards the camera. The setting is a kitchen, with a beige wall and a ceiling light illuminating the area.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25588.6, "ram_available_mb": 100183.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 25588.6, "ram_available_mb": 100183.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.93, "peak": 15.32, "min": 14.32}, "VDD_GPU": {"avg": 30.31, "peak": 38.21, "min": 24.43}, "VIN": {"avg": 69.71, "peak": 110.46, "min": 56.67}, "VDD_CPU_SOC_MSS": {"avg": 15.51, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 30.31, "energy_joules_est": 58.45, "sample_count": 15, "duration_seconds": 1.929}, "timestamp": "2026-01-16T15:36:56.199114"}
{"image_index": 136, "image_name": "000000014007.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014007.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2397.368, "latencies_ms": [2397.368], "images_per_second": 0.417, "prompt_tokens": 19, "response_tokens_est": 64, "n_tiles": 6, "output_text": "The image features a cat sitting on top of a refrigerator, which is painted in a deep blue color. The lighting in the room is soft and warm, casting a gentle glow on the cat and the refrigerator. The background is neutral, with a beige wall and a ceiling that has a subtle, rounded design.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25588.6, "ram_available_mb": 100183.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 25588.6, "ram_available_mb": 100183.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.8, "peak": 15.11, "min": 14.12}, "VDD_GPU": {"avg": 28.93, "peak": 38.6, "min": 23.24}, "VIN": {"avg": 66.12, "peak": 95.33, "min": 58.4}, "VDD_CPU_SOC_MSS": {"avg": 15.33, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 28.93, "energy_joules_est": 69.36, "sample_count": 18, "duration_seconds": 2.398}, "timestamp": "2026-01-16T15:36:58.602848"}
{"image_index": 137, "image_name": "000000014038.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014038.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1814.015, "latencies_ms": [1814.015], "images_per_second": 0.551, "prompt_tokens": 9, "response_tokens_est": 45, "n_tiles": 6, "output_text": "The image depicts a cozy, well-organized living room with a yellow balloon floating in the air, a blue balloon on the refrigerator, and a yellow balloon on a table, all contributing to a festive atmosphere.", "error": null, "sys_before": {"cpu_percent": 42.9, "ram_used_mb": 25588.6, "ram_available_mb": 100183.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 25588.3, "ram_available_mb": 100183.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.56, "peak": 15.01, "min": 13.82}, "VDD_GPU": {"avg": 29.74, "peak": 36.63, "min": 24.03}, "VIN": {"avg": 71.03, "peak": 126.49, "min": 60.0}, "VDD_CPU_SOC_MSS": {"avg": 15.27, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 29.74, "energy_joules_est": 53.96, "sample_count": 14, "duration_seconds": 1.814}, "timestamp": "2026-01-16T15:37:00.476549"}
{"image_index": 137, "image_name": "000000014038.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014038.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2044.509, "latencies_ms": [2044.509], "images_per_second": 0.489, "prompt_tokens": 23, "response_tokens_est": 51, "n_tiles": 6, "output_text": "- refrigerator: 1\n- microwave: 1\n- table: 1\n- chair: 1\n- bookshelf: 1\n- lamp: 1\n- wall art: 1\n- balloons: 3", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25588.3, "ram_available_mb": 100183.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 25588.3, "ram_available_mb": 100183.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.7, "peak": 15.01, "min": 14.02}, "VDD_GPU": {"avg": 29.69, "peak": 38.21, "min": 23.64}, "VIN": {"avg": 69.98, "peak": 111.88, "min": 57.77}, "VDD_CPU_SOC_MSS": {"avg": 15.3, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 29.69, "energy_joules_est": 60.72, "sample_count": 16, "duration_seconds": 2.045}, "timestamp": "2026-01-16T15:37:02.528298"}
{"image_index": 137, "image_name": "000000014038.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014038.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2947.143, "latencies_ms": [2947.143], "images_per_second": 0.339, "prompt_tokens": 27, "response_tokens_est": 83, "n_tiles": 6, "output_text": "The main objects in the image are a refrigerator, a bed, and a desk. The refrigerator is located on the left side of the image, near the foreground. The bed is in the background, slightly to the right. The desk is located in the middle of the image, near the right side. The yellow balloons are placed on the desk and the bed, adding a playful touch to the room.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 25588.3, "ram_available_mb": 100183.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 25589.3, "ram_available_mb": 100182.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.79, "peak": 15.01, "min": 14.02}, "VDD_GPU": {"avg": 27.66, "peak": 38.21, "min": 23.24}, "VIN": {"avg": 66.48, "peak": 85.23, "min": 61.77}, "VDD_CPU_SOC_MSS": {"avg": 15.44, "peak": 15.75, "min": 14.57}}, "power_watts_avg": 27.66, "energy_joules_est": 81.53, "sample_count": 23, "duration_seconds": 2.947}, "timestamp": "2026-01-16T15:37:05.481424"}
{"image_index": 137, "image_name": "000000014038.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014038.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3952.405, "latencies_ms": [3952.405], "images_per_second": 0.253, "prompt_tokens": 21, "response_tokens_est": 119, "n_tiles": 6, "output_text": "The image depicts a cozy, well-organized living room with a mix of personal and decorative items. The room is furnished with a wooden coffee table, a bed with a yellow balloon, a white refrigerator with various magnets and notes, a wooden bookshelf filled with books and decorative items, a black speaker, and a yellow balloon on a table. The walls are adorned with framed pictures and a yellow wall decoration, and there is a small green balloon floating in the air. The overall atmosphere is casual and inviting, suggesting a personal space that is used for relaxation and entertainment.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25589.3, "ram_available_mb": 100182.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 25589.5, "ram_available_mb": 100182.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.87, "peak": 15.11, "min": 14.02}, "VDD_GPU": {"avg": 26.48, "peak": 38.21, "min": 22.86}, "VIN": {"avg": 66.71, "peak": 100.86, "min": 57.87}, "VDD_CPU_SOC_MSS": {"avg": 15.51, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 26.48, "energy_joules_est": 104.67, "sample_count": 31, "duration_seconds": 3.953}, "timestamp": "2026-01-16T15:37:09.440025"}
{"image_index": 137, "image_name": "000000014038.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014038.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2126.275, "latencies_ms": [2126.275], "images_per_second": 0.47, "prompt_tokens": 19, "response_tokens_est": 54, "n_tiles": 6, "output_text": "The room is brightly lit with a modern chandelier hanging from the ceiling, casting a soft glow over the space. The walls are painted in a light, neutral color, complementing the wooden flooring and the various colorful balloons adorning the room.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25589.5, "ram_available_mb": 100182.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 25589.5, "ram_available_mb": 100182.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.7, "peak": 15.01, "min": 14.02}, "VDD_GPU": {"avg": 29.59, "peak": 38.21, "min": 23.64}, "VIN": {"avg": 69.45, "peak": 112.35, "min": 56.22}, "VDD_CPU_SOC_MSS": {"avg": 15.35, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 29.59, "energy_joules_est": 62.92, "sample_count": 16, "duration_seconds": 2.127}, "timestamp": "2026-01-16T15:37:11.576491"}
{"image_index": 138, "image_name": "000000014226.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014226.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2112.323, "latencies_ms": [2112.323], "images_per_second": 0.473, "prompt_tokens": 9, "response_tokens_est": 21, "n_tiles": 12, "output_text": "A man is sitting in a train, using a laptop on a table, with a focused expression.", "error": null, "sys_before": {"cpu_percent": 43.0, "ram_used_mb": 25589.5, "ram_available_mb": 100182.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 2.5, "ram_used_mb": 25589.8, "ram_available_mb": 100182.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.14, "peak": 16.15, "min": 13.9}, "VDD_GPU": {"avg": 36.02, "peak": 41.36, "min": 29.16}, "VIN": {"avg": 84.98, "peak": 132.28, "min": 58.57}, "VDD_CPU_SOC_MSS": {"avg": 15.94, "peak": 17.32, "min": 14.96}}, "power_watts_avg": 36.02, "energy_joules_est": 76.1, "sample_count": 16, "duration_seconds": 2.113}, "timestamp": "2026-01-16T15:37:13.766190"}
{"image_index": 138, "image_name": "000000014226.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014226.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2649.133, "latencies_ms": [2649.133], "images_per_second": 0.377, "prompt_tokens": 23, "response_tokens_est": 33, "n_tiles": 12, "output_text": "1. Laptop\n2. Man\n3. Headphones\n4. Window\n5. Chair\n6. Table\n7. Keyboard\n8. Mouse", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25589.8, "ram_available_mb": 100182.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 25589.5, "ram_available_mb": 100182.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.04, "peak": 15.54, "min": 14.43}, "VDD_GPU": {"avg": 35.86, "peak": 43.73, "min": 27.18}, "VIN": {"avg": 72.32, "peak": 123.21, "min": 58.25}, "VDD_CPU_SOC_MSS": {"avg": 15.67, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 35.86, "energy_joules_est": 95.03, "sample_count": 20, "duration_seconds": 2.65}, "timestamp": "2026-01-16T15:37:16.422076"}
{"image_index": 138, "image_name": "000000014226.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014226.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3289.908, "latencies_ms": [3289.908], "images_per_second": 0.304, "prompt_tokens": 27, "response_tokens_est": 52, "n_tiles": 12, "output_text": "The main object in the foreground is a man sitting on a train seat, holding a laptop. The laptop is placed on a small table or bench near the man. The background features blurred train tracks and a window, indicating the train is in motion.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25589.5, "ram_available_mb": 100182.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 25589.3, "ram_available_mb": 100182.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.06, "peak": 15.65, "min": 14.43}, "VDD_GPU": {"avg": 33.92, "peak": 42.94, "min": 26.39}, "VIN": {"avg": 75.14, "peak": 111.85, "min": 58.44}, "VDD_CPU_SOC_MSS": {"avg": 15.61, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 33.92, "energy_joules_est": 111.6, "sample_count": 25, "duration_seconds": 3.29}, "timestamp": "2026-01-16T15:37:19.717897"}
{"image_index": 138, "image_name": "000000014226.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014226.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3316.033, "latencies_ms": [3316.033], "images_per_second": 0.302, "prompt_tokens": 21, "response_tokens_est": 53, "n_tiles": 12, "output_text": "The image depicts a man sitting in a train carriage, engrossed in using a laptop. The setting is inside a train, with the man seated on a bench, and the train's interior is visible, including a window and a cushioned seat.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25589.3, "ram_available_mb": 100182.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 25589.2, "ram_available_mb": 100182.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.08, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 33.62, "peak": 42.54, "min": 26.39}, "VIN": {"avg": 77.11, "peak": 115.06, "min": 58.73}, "VDD_CPU_SOC_MSS": {"avg": 15.66, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 33.62, "energy_joules_est": 111.49, "sample_count": 26, "duration_seconds": 3.316}, "timestamp": "2026-01-16T15:37:23.040149"}
{"image_index": 138, "image_name": "000000014226.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014226.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3483.93, "latencies_ms": [3483.93], "images_per_second": 0.287, "prompt_tokens": 19, "response_tokens_est": 58, "n_tiles": 12, "output_text": "The image features a man sitting in a train, holding a silver laptop. The laptop is placed on a light-colored table, and the man is wearing a dark jacket. The lighting in the train is dim, with natural light coming through the windows, creating a cozy and relaxed atmosphere.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25589.2, "ram_available_mb": 100182.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 25589.2, "ram_available_mb": 100182.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.07, "peak": 15.65, "min": 14.43}, "VDD_GPU": {"avg": 33.29, "peak": 42.94, "min": 26.39}, "VIN": {"avg": 73.3, "peak": 112.33, "min": 58.33}, "VDD_CPU_SOC_MSS": {"avg": 15.64, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 33.29, "energy_joules_est": 115.99, "sample_count": 27, "duration_seconds": 3.484}, "timestamp": "2026-01-16T15:37:26.530338"}
{"image_index": 139, "image_name": "000000014380.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014380.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1679.477, "latencies_ms": [1679.477], "images_per_second": 0.595, "prompt_tokens": 9, "response_tokens_est": 41, "n_tiles": 6, "output_text": "The image depicts a modern train station with multiple tracks and a sleek, silver train moving along them, set against a backdrop of a cloudy sky and a cityscape with buildings in the distance.", "error": null, "sys_before": {"cpu_percent": 37.3, "ram_used_mb": 25589.2, "ram_available_mb": 100182.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 25589.2, "ram_available_mb": 100182.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.52, "peak": 15.11, "min": 13.82}, "VDD_GPU": {"avg": 30.76, "peak": 37.42, "min": 24.83}, "VIN": {"avg": 71.43, "peak": 117.66, "min": 62.01}, "VDD_CPU_SOC_MSS": {"avg": 15.2, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 30.76, "energy_joules_est": 51.67, "sample_count": 13, "duration_seconds": 1.68}, "timestamp": "2026-01-16T15:37:28.274491"}
{"image_index": 139, "image_name": "000000014380.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014380.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1696.246, "latencies_ms": [1696.246], "images_per_second": 0.59, "prompt_tokens": 23, "response_tokens_est": 40, "n_tiles": 6, "output_text": "object: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25589.2, "ram_available_mb": 100182.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 25589.2, "ram_available_mb": 100182.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.84, "peak": 15.22, "min": 14.22}, "VDD_GPU": {"avg": 31.46, "peak": 38.6, "min": 24.83}, "VIN": {"avg": 71.86, "peak": 114.9, "min": 57.24}, "VDD_CPU_SOC_MSS": {"avg": 15.41, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 31.46, "energy_joules_est": 53.38, "sample_count": 13, "duration_seconds": 1.697}, "timestamp": "2026-01-16T15:37:29.977097"}
{"image_index": 139, "image_name": "000000014380.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014380.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2487.98, "latencies_ms": [2487.98], "images_per_second": 0.402, "prompt_tokens": 27, "response_tokens_est": 67, "n_tiles": 6, "output_text": "The main objects in the image are a train and a bridge. The train is positioned on the tracks, with its front end closer to the foreground and its rear end further back. The bridge spans across the tracks, connecting the two sides of the railway. The train is near the tracks, while the bridge is in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25589.2, "ram_available_mb": 100182.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 25593.2, "ram_available_mb": 100178.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.8, "peak": 15.11, "min": 14.12}, "VDD_GPU": {"avg": 28.88, "peak": 38.6, "min": 23.24}, "VIN": {"avg": 70.15, "peak": 116.88, "min": 55.56}, "VDD_CPU_SOC_MSS": {"avg": 15.37, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 28.88, "energy_joules_est": 71.86, "sample_count": 19, "duration_seconds": 2.488}, "timestamp": "2026-01-16T15:37:32.470834"}
{"image_index": 139, "image_name": "000000014380.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014380.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1764.209, "latencies_ms": [1764.209], "images_per_second": 0.567, "prompt_tokens": 21, "response_tokens_est": 41, "n_tiles": 6, "output_text": "The image depicts a modern train station with a sleek, silver train moving along the tracks. The station is surrounded by a network of overhead electrical lines and platforms, with a clear blue sky overhead.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25593.2, "ram_available_mb": 100178.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25593.7, "ram_available_mb": 100178.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.64, "peak": 14.91, "min": 14.02}, "VDD_GPU": {"avg": 30.94, "peak": 38.21, "min": 24.42}, "VIN": {"avg": 71.53, "peak": 112.86, "min": 58.51}, "VDD_CPU_SOC_MSS": {"avg": 15.17, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 30.94, "energy_joules_est": 54.61, "sample_count": 13, "duration_seconds": 1.765}, "timestamp": "2026-01-16T15:37:34.245337"}
{"image_index": 139, "image_name": "000000014380.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014380.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2493.651, "latencies_ms": [2493.651], "images_per_second": 0.401, "prompt_tokens": 19, "response_tokens_est": 67, "n_tiles": 6, "output_text": "The image depicts a modern train station with a sleek, silver train in motion on the tracks. The station is surrounded by a clear blue sky with scattered white clouds, indicating a bright and sunny day. The station's infrastructure, including the metal beams and platforms, is well-maintained and reflects a clean, urban environment.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25593.7, "ram_available_mb": 100178.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 25593.5, "ram_available_mb": 100178.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.73, "peak": 15.01, "min": 14.02}, "VDD_GPU": {"avg": 28.61, "peak": 38.21, "min": 23.24}, "VIN": {"avg": 69.93, "peak": 96.67, "min": 60.39}, "VDD_CPU_SOC_MSS": {"avg": 15.25, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 28.61, "energy_joules_est": 71.35, "sample_count": 19, "duration_seconds": 2.494}, "timestamp": "2026-01-16T15:37:36.747023"}
{"image_index": 140, "image_name": "000000014439.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014439.jpg", "image_width": 640, "image_height": 404, "image_resolution": "640x404", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1422.796, "latencies_ms": [1422.796], "images_per_second": 0.703, "prompt_tokens": 9, "response_tokens_est": 31, "n_tiles": 6, "output_text": "A group of people are gathered in a park, with a colorful kite being flown by a woman and a child, while others sit and watch.", "error": null, "sys_before": {"cpu_percent": 43.4, "ram_used_mb": 25593.5, "ram_available_mb": 100178.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 25593.7, "ram_available_mb": 100178.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.41, "peak": 15.01, "min": 13.82}, "VDD_GPU": {"avg": 31.16, "peak": 36.63, "min": 25.61}, "VIN": {"avg": 71.86, "peak": 95.38, "min": 58.31}, "VDD_CPU_SOC_MSS": {"avg": 15.07, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 31.16, "energy_joules_est": 44.34, "sample_count": 11, "duration_seconds": 1.423}, "timestamp": "2026-01-16T15:37:38.230239"}
{"image_index": 140, "image_name": "000000014439.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014439.jpg", "image_width": 640, "image_height": 404, "image_resolution": "640x404", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2156.62, "latencies_ms": [2156.62], "images_per_second": 0.464, "prompt_tokens": 23, "response_tokens_est": 57, "n_tiles": 6, "output_text": "1. Kite: 1\n2. Person: 3\n3. Person: 2\n4. Person: 1\n5. Person: 1\n6. Person: 1\n7. Person: 1\n8. Person: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25593.7, "ram_available_mb": 100178.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 25593.5, "ram_available_mb": 100178.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.87, "peak": 15.22, "min": 14.22}, "VDD_GPU": {"avg": 29.82, "peak": 39.0, "min": 24.03}, "VIN": {"avg": 71.7, "peak": 116.86, "min": 45.53}, "VDD_CPU_SOC_MSS": {"avg": 15.45, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 29.82, "energy_joules_est": 64.33, "sample_count": 17, "duration_seconds": 2.157}, "timestamp": "2026-01-16T15:37:40.393194"}
{"image_index": 140, "image_name": "000000014439.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014439.jpg", "image_width": 640, "image_height": 404, "image_resolution": "640x404", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3200.854, "latencies_ms": [3200.854], "images_per_second": 0.312, "prompt_tokens": 27, "response_tokens_est": 96, "n_tiles": 6, "output_text": "The main object in the foreground is a colorful kite with a purple body and a yellow tail, which is being flown by a person. The kite is positioned near the center of the image, drawing attention to its vibrant colors. In the background, there are several people scattered across the grassy field, with some standing and others sitting. The kite is the focal point of the image, with the people serving as a secondary element, providing context to the scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25593.5, "ram_available_mb": 100178.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 25593.5, "ram_available_mb": 100178.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.02, "peak": 15.32, "min": 14.32}, "VDD_GPU": {"avg": 27.72, "peak": 37.82, "min": 23.64}, "VIN": {"avg": 66.47, "peak": 76.82, "min": 56.27}, "VDD_CPU_SOC_MSS": {"avg": 15.73, "peak": 16.14, "min": 14.96}}, "power_watts_avg": 27.72, "energy_joules_est": 88.74, "sample_count": 24, "duration_seconds": 3.201}, "timestamp": "2026-01-16T15:37:43.600129"}
{"image_index": 140, "image_name": "000000014439.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014439.jpg", "image_width": 640, "image_height": 404, "image_resolution": "640x404", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2416.71, "latencies_ms": [2416.71], "images_per_second": 0.414, "prompt_tokens": 21, "response_tokens_est": 67, "n_tiles": 6, "output_text": "The image depicts a vibrant outdoor scene in a park where a group of people is gathered to fly kites. The setting is a lush green field with a few scattered chairs and a soccer goal in the background. The individuals are engaged in the activity of flying colorful kites, with some children and adults participating in the fun.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25593.5, "ram_available_mb": 100178.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 25593.5, "ram_available_mb": 100178.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.96, "peak": 15.32, "min": 14.22}, "VDD_GPU": {"avg": 28.94, "peak": 38.19, "min": 23.64}, "VIN": {"avg": 67.21, "peak": 89.04, "min": 57.36}, "VDD_CPU_SOC_MSS": {"avg": 15.54, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 28.94, "energy_joules_est": 69.95, "sample_count": 19, "duration_seconds": 2.417}, "timestamp": "2026-01-16T15:37:46.022726"}
{"image_index": 140, "image_name": "000000014439.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014439.jpg", "image_width": 640, "image_height": 404, "image_resolution": "640x404", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2200.532, "latencies_ms": [2200.532], "images_per_second": 0.454, "prompt_tokens": 19, "response_tokens_est": 59, "n_tiles": 6, "output_text": "The kite in the image is vibrant and colorful, featuring a combination of blue, purple, yellow, and orange hues. It is being flown by a person in a light-colored shirt, and the kite is illuminated by the sunlight, casting a warm glow on the scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25593.5, "ram_available_mb": 100178.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 25593.7, "ram_available_mb": 100178.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.93, "peak": 15.32, "min": 14.32}, "VDD_GPU": {"avg": 29.52, "peak": 38.19, "min": 24.03}, "VIN": {"avg": 65.52, "peak": 82.52, "min": 55.58}, "VDD_CPU_SOC_MSS": {"avg": 15.52, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 29.52, "energy_joules_est": 64.98, "sample_count": 17, "duration_seconds": 2.201}, "timestamp": "2026-01-16T15:37:48.229909"}
{"image_index": 141, "image_name": "000000014473.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014473.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1124.251, "latencies_ms": [1124.251], "images_per_second": 0.889, "prompt_tokens": 9, "response_tokens_est": 20, "n_tiles": 6, "output_text": "A model of a red and black train is seen on tracks with workers in orange uniforms nearby.", "error": null, "sys_before": {"cpu_percent": 44.6, "ram_used_mb": 25593.7, "ram_available_mb": 100178.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25593.7, "ram_available_mb": 100178.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.38, "peak": 14.71, "min": 13.92}, "VDD_GPU": {"avg": 33.29, "peak": 37.82, "min": 28.36}, "VIN": {"avg": 69.54, "peak": 93.11, "min": 50.29}, "VDD_CPU_SOC_MSS": {"avg": 15.15, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 33.29, "energy_joules_est": 37.44, "sample_count": 8, "duration_seconds": 1.125}, "timestamp": "2026-01-16T15:37:49.403292"}
{"image_index": 141, "image_name": "000000014473.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014473.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1700.876, "latencies_ms": [1700.876], "images_per_second": 0.588, "prompt_tokens": 23, "response_tokens_est": 40, "n_tiles": 6, "output_text": "object: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25593.7, "ram_available_mb": 100178.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25593.7, "ram_available_mb": 100178.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.75, "peak": 15.11, "min": 14.22}, "VDD_GPU": {"avg": 31.87, "peak": 38.98, "min": 24.82}, "VIN": {"avg": 74.65, "peak": 112.58, "min": 53.8}, "VDD_CPU_SOC_MSS": {"avg": 15.32, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 31.87, "energy_joules_est": 54.22, "sample_count": 13, "duration_seconds": 1.701}, "timestamp": "2026-01-16T15:37:51.110110"}
{"image_index": 141, "image_name": "000000014473.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014473.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2220.597, "latencies_ms": [2220.597], "images_per_second": 0.45, "prompt_tokens": 27, "response_tokens_est": 57, "n_tiles": 6, "output_text": "The main objects in the image are a model train and a group of railway workers. The model train is positioned in the foreground, while the railway workers are in the background. The workers are near the tracks, working on the ground, while the model train is on the tracks.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25593.7, "ram_available_mb": 100178.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 25593.7, "ram_available_mb": 100178.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.72, "peak": 15.01, "min": 14.12}, "VDD_GPU": {"avg": 29.45, "peak": 38.19, "min": 23.25}, "VIN": {"avg": 67.07, "peak": 95.19, "min": 55.53}, "VDD_CPU_SOC_MSS": {"avg": 15.26, "peak": 15.75, "min": 14.57}}, "power_watts_avg": 29.45, "energy_joules_est": 65.41, "sample_count": 17, "duration_seconds": 2.221}, "timestamp": "2026-01-16T15:37:53.336619"}
{"image_index": 141, "image_name": "000000014473.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014473.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1993.661, "latencies_ms": [1993.661], "images_per_second": 0.502, "prompt_tokens": 21, "response_tokens_est": 49, "n_tiles": 6, "output_text": "The image depicts a miniature model of a train on railway tracks, with a group of workers in orange uniforms nearby. The setting appears to be a rural or semi-rural area, with greenery and a fence visible in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25593.7, "ram_available_mb": 100178.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 25593.7, "ram_available_mb": 100178.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.7, "peak": 15.01, "min": 14.02}, "VDD_GPU": {"avg": 29.83, "peak": 38.21, "min": 23.64}, "VIN": {"avg": 76.25, "peak": 117.13, "min": 59.52}, "VDD_CPU_SOC_MSS": {"avg": 15.22, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 29.83, "energy_joules_est": 59.48, "sample_count": 15, "duration_seconds": 1.994}, "timestamp": "2026-01-16T15:37:55.336274"}
{"image_index": 141, "image_name": "000000014473.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014473.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1877.671, "latencies_ms": [1877.671], "images_per_second": 0.533, "prompt_tokens": 19, "response_tokens_est": 45, "n_tiles": 6, "output_text": "The image features a red and black train with the Virgin logo on its side, traveling on tracks surrounded by greenery. The scene is well-lit, with natural sunlight casting shadows on the train and the surrounding landscape.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25593.7, "ram_available_mb": 100178.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 25593.7, "ram_available_mb": 100178.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.65, "peak": 15.01, "min": 14.02}, "VDD_GPU": {"avg": 30.27, "peak": 37.82, "min": 24.03}, "VIN": {"avg": 69.45, "peak": 119.77, "min": 53.55}, "VDD_CPU_SOC_MSS": {"avg": 15.21, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 30.27, "energy_joules_est": 56.85, "sample_count": 14, "duration_seconds": 1.878}, "timestamp": "2026-01-16T15:37:57.220180"}
{"image_index": 142, "image_name": "000000014831.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014831.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2273.282, "latencies_ms": [2273.282], "images_per_second": 0.44, "prompt_tokens": 9, "response_tokens_est": 26, "n_tiles": 12, "output_text": "The image shows a close-up of a cat's fur, which appears to be a mix of light brown and white colors.", "error": null, "sys_before": {"cpu_percent": 45.4, "ram_used_mb": 25593.7, "ram_available_mb": 100178.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 25593.7, "ram_available_mb": 100178.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.84, "peak": 15.85, "min": 13.79}, "VDD_GPU": {"avg": 35.52, "peak": 40.97, "min": 27.97}, "VIN": {"avg": 77.74, "peak": 108.82, "min": 58.49}, "VDD_CPU_SOC_MSS": {"avg": 15.63, "peak": 16.93, "min": 14.96}}, "power_watts_avg": 35.52, "energy_joules_est": 80.76, "sample_count": 17, "duration_seconds": 2.274}, "timestamp": "2026-01-16T15:37:59.583603"}
{"image_index": 142, "image_name": "000000014831.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014831.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1895.555, "latencies_ms": [1895.555], "images_per_second": 0.528, "prompt_tokens": 23, "response_tokens_est": 11, "n_tiles": 12, "output_text": "cat: 1\nblanket: 1", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25593.7, "ram_available_mb": 100178.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 2.1, "ram_used_mb": 25593.7, "ram_available_mb": 100178.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.8, "peak": 15.24, "min": 14.12}, "VDD_GPU": {"avg": 38.74, "peak": 42.94, "min": 32.3}, "VIN": {"avg": 81.08, "peak": 121.01, "min": 64.53}, "VDD_CPU_SOC_MSS": {"avg": 15.43, "peak": 16.14, "min": 14.56}}, "power_watts_avg": 38.74, "energy_joules_est": 73.45, "sample_count": 14, "duration_seconds": 1.896}, "timestamp": "2026-01-16T15:38:01.485887"}
{"image_index": 142, "image_name": "000000014831.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014831.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3304.155, "latencies_ms": [3304.155], "images_per_second": 0.303, "prompt_tokens": 27, "response_tokens_est": 52, "n_tiles": 12, "output_text": "The main object in the image is a cat's fur, which occupies the foreground. The fur is predominantly light brown with some white areas. The background is a textured fabric, possibly a couch or a blanket, which is slightly out of focus.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25593.7, "ram_available_mb": 100178.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 25593.7, "ram_available_mb": 100178.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.7, "peak": 15.34, "min": 14.02}, "VDD_GPU": {"avg": 34.09, "peak": 44.12, "min": 26.0}, "VIN": {"avg": 77.6, "peak": 118.78, "min": 57.31}, "VDD_CPU_SOC_MSS": {"avg": 15.22, "peak": 16.14, "min": 14.56}}, "power_watts_avg": 34.09, "energy_joules_est": 112.65, "sample_count": 26, "duration_seconds": 3.304}, "timestamp": "2026-01-16T15:38:04.796664"}
{"image_index": 142, "image_name": "000000014831.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014831.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 4025.817, "latencies_ms": [4025.817], "images_per_second": 0.248, "prompt_tokens": 21, "response_tokens_est": 73, "n_tiles": 12, "output_text": "The image shows a close-up of a cat's fur, which appears to be a mix of light brown and white colors. The cat is lying down on a textured surface, possibly a bed or a couch, with its head resting on its front paws. The overall setting suggests a cozy and comfortable environment, with the cat appearing relaxed and at ease.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25593.7, "ram_available_mb": 100178.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25593.7, "ram_available_mb": 100178.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.73, "peak": 15.34, "min": 14.12}, "VDD_GPU": {"avg": 32.17, "peak": 42.54, "min": 25.61}, "VIN": {"avg": 72.38, "peak": 120.84, "min": 58.17}, "VDD_CPU_SOC_MSS": {"avg": 15.19, "peak": 16.14, "min": 14.56}}, "power_watts_avg": 32.17, "energy_joules_est": 129.53, "sample_count": 31, "duration_seconds": 4.026}, "timestamp": "2026-01-16T15:38:08.828826"}
{"image_index": 142, "image_name": "000000014831.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014831.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3711.773, "latencies_ms": [3711.773], "images_per_second": 0.269, "prompt_tokens": 19, "response_tokens_est": 64, "n_tiles": 12, "output_text": "The image shows a close-up of a cat's fur with a mix of light brown and white colors. The lighting is soft and diffused, creating gentle shadows and highlights on the fur. The background appears to be a textured fabric, possibly a blanket or a piece of clothing, with a neutral color palette.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25593.7, "ram_available_mb": 100178.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25593.9, "ram_available_mb": 100178.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.73, "peak": 15.24, "min": 14.12}, "VDD_GPU": {"avg": 32.63, "peak": 42.54, "min": 25.6}, "VIN": {"avg": 71.36, "peak": 106.63, "min": 58.35}, "VDD_CPU_SOC_MSS": {"avg": 15.22, "peak": 16.14, "min": 14.56}}, "power_watts_avg": 32.63, "energy_joules_est": 121.13, "sample_count": 29, "duration_seconds": 3.712}, "timestamp": "2026-01-16T15:38:12.547174"}
{"image_index": 143, "image_name": "000000014888.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014888.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2815.545, "latencies_ms": [2815.545], "images_per_second": 0.355, "prompt_tokens": 9, "response_tokens_est": 42, "n_tiles": 12, "output_text": "The image shows a close-up of a black and white cow's head, with its tongue visible and a red and white bottle nearby, indicating that the cow might be in a barn or a farm setting.", "error": null, "sys_before": {"cpu_percent": 40.6, "ram_used_mb": 25593.9, "ram_available_mb": 100178.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 25593.7, "ram_available_mb": 100178.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.13, "peak": 16.15, "min": 13.9}, "VDD_GPU": {"avg": 33.99, "peak": 41.36, "min": 26.79}, "VIN": {"avg": 77.97, "peak": 116.97, "min": 58.35}, "VDD_CPU_SOC_MSS": {"avg": 15.95, "peak": 17.32, "min": 14.96}}, "power_watts_avg": 33.99, "energy_joules_est": 95.71, "sample_count": 21, "duration_seconds": 2.816}, "timestamp": "2026-01-16T15:38:15.445156"}
{"image_index": 143, "image_name": "000000014888.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014888.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 3182.146, "latencies_ms": [3182.146], "images_per_second": 0.314, "prompt_tokens": 23, "response_tokens_est": 49, "n_tiles": 12, "output_text": "1. Dog\n2. Dog's ear\n3. Dog's paw\n4. Dog's nose\n5. Dog's mouth\n6. Dog's tail\n7. Dog's paw\n8. Dog's paw", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25593.7, "ram_available_mb": 100178.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 25593.6, "ram_available_mb": 100178.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.07, "peak": 15.65, "min": 14.43}, "VDD_GPU": {"avg": 33.94, "peak": 42.94, "min": 26.39}, "VIN": {"avg": 73.22, "peak": 115.6, "min": 58.26}, "VDD_CPU_SOC_MSS": {"avg": 15.65, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 33.94, "energy_joules_est": 108.02, "sample_count": 25, "duration_seconds": 3.183}, "timestamp": "2026-01-16T15:38:18.633943"}
{"image_index": 143, "image_name": "000000014888.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014888.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 4538.055, "latencies_ms": [4538.055], "images_per_second": 0.22, "prompt_tokens": 27, "response_tokens_est": 89, "n_tiles": 12, "output_text": "The main object in the foreground is a black and white cow, which is positioned near the center of the image. The cow is interacting with a red and white plastic bottle, which is placed to the right of the cow. The bottle is situated on a dark surface, possibly a floor or a table, and is in close proximity to the cow. The background is out of focus, emphasizing the cow and the bottle in the foreground.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25593.6, "ram_available_mb": 100178.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 25593.6, "ram_available_mb": 100178.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.1, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 31.58, "peak": 42.94, "min": 26.0}, "VIN": {"avg": 71.69, "peak": 108.71, "min": 58.49}, "VDD_CPU_SOC_MSS": {"avg": 15.69, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 31.58, "energy_joules_est": 143.32, "sample_count": 36, "duration_seconds": 4.538}, "timestamp": "2026-01-16T15:38:23.177641"}
{"image_index": 143, "image_name": "000000014888.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014888.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 4194.862, "latencies_ms": [4194.862], "images_per_second": 0.238, "prompt_tokens": 21, "response_tokens_est": 79, "n_tiles": 12, "output_text": "The image depicts a close-up view of a black and white cow's head, with its nose and mouth visible. The cow is standing on a concrete surface, surrounded by various objects and debris, including a black container, a red and white container, and a yellow label. The setting appears to be a farm or livestock area, with the cow possibly being in a pen or stall.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25593.6, "ram_available_mb": 100178.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25593.6, "ram_available_mb": 100178.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.1, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 31.97, "peak": 42.15, "min": 26.0}, "VIN": {"avg": 74.66, "peak": 121.86, "min": 58.38}, "VDD_CPU_SOC_MSS": {"avg": 15.63, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 31.97, "energy_joules_est": 134.12, "sample_count": 33, "duration_seconds": 4.195}, "timestamp": "2026-01-16T15:38:27.378579"}
{"image_index": 143, "image_name": "000000014888.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000014888.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3079.872, "latencies_ms": [3079.872], "images_per_second": 0.325, "prompt_tokens": 19, "response_tokens_est": 46, "n_tiles": 12, "output_text": "The image shows a black and white cow with a shiny, wet coat, indicating recent rain or exposure to water. The lighting is dim, with a soft, natural glow, suggesting an outdoor setting or a shaded area.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25593.6, "ram_available_mb": 100178.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 25593.6, "ram_available_mb": 100178.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.06, "peak": 15.65, "min": 14.43}, "VDD_GPU": {"avg": 34.21, "peak": 42.54, "min": 26.79}, "VIN": {"avg": 73.39, "peak": 114.77, "min": 57.94}, "VDD_CPU_SOC_MSS": {"avg": 15.63, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 34.21, "energy_joules_est": 105.39, "sample_count": 24, "duration_seconds": 3.081}, "timestamp": "2026-01-16T15:38:30.465298"}
{"image_index": 144, "image_name": "000000015079.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015079.jpg", "image_width": 640, "image_height": 424, "image_resolution": "640x424", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1493.902, "latencies_ms": [1493.902], "images_per_second": 0.669, "prompt_tokens": 9, "response_tokens_est": 34, "n_tiles": 6, "output_text": "The image shows a plate of savory sandwiches with a side of pickles, placed on a patterned tablecloth, with a fork resting on the plate.", "error": null, "sys_before": {"cpu_percent": 38.2, "ram_used_mb": 25593.6, "ram_available_mb": 100178.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 25593.7, "ram_available_mb": 100178.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.48, "peak": 15.01, "min": 13.92}, "VDD_GPU": {"avg": 31.8, "peak": 38.21, "min": 25.61}, "VIN": {"avg": 74.97, "peak": 110.34, "min": 58.87}, "VDD_CPU_SOC_MSS": {"avg": 15.1, "peak": 15.75, "min": 14.57}}, "power_watts_avg": 31.8, "energy_joules_est": 47.52, "sample_count": 11, "duration_seconds": 1.494}, "timestamp": "2026-01-16T15:38:32.012378"}
{"image_index": 144, "image_name": "000000015079.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015079.jpg", "image_width": 640, "image_height": 424, "image_resolution": "640x424", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1736.271, "latencies_ms": [1736.271], "images_per_second": 0.576, "prompt_tokens": 23, "response_tokens_est": 40, "n_tiles": 6, "output_text": "object: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25593.7, "ram_available_mb": 100178.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 25593.6, "ram_available_mb": 100178.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.66, "peak": 14.91, "min": 14.12}, "VDD_GPU": {"avg": 31.21, "peak": 38.59, "min": 24.42}, "VIN": {"avg": 71.37, "peak": 124.86, "min": 55.53}, "VDD_CPU_SOC_MSS": {"avg": 15.17, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 31.21, "energy_joules_est": 54.2, "sample_count": 13, "duration_seconds": 1.737}, "timestamp": "2026-01-16T15:38:33.754877"}
{"image_index": 144, "image_name": "000000015079.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015079.jpg", "image_width": 640, "image_height": 424, "image_resolution": "640x424", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2385.96, "latencies_ms": [2385.96], "images_per_second": 0.419, "prompt_tokens": 27, "response_tokens_est": 63, "n_tiles": 6, "output_text": "The main objects in the image are a plate with a sandwich and a knife. The sandwich is positioned in the foreground, while the knife is placed in the background. The plate is placed on a tablecloth with a green pattern, and the background is dark, which helps to highlight the sandwich and knife.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25593.6, "ram_available_mb": 100178.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 25593.9, "ram_available_mb": 100178.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.72, "peak": 15.01, "min": 14.02}, "VDD_GPU": {"avg": 29.04, "peak": 38.6, "min": 23.24}, "VIN": {"avg": 69.46, "peak": 92.97, "min": 59.31}, "VDD_CPU_SOC_MSS": {"avg": 15.24, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 29.04, "energy_joules_est": 69.3, "sample_count": 18, "duration_seconds": 2.386}, "timestamp": "2026-01-16T15:38:36.146935"}
{"image_index": 144, "image_name": "000000015079.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015079.jpg", "image_width": 640, "image_height": 424, "image_resolution": "640x424", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2521.639, "latencies_ms": [2521.639], "images_per_second": 0.397, "prompt_tokens": 21, "response_tokens_est": 68, "n_tiles": 6, "output_text": "The image depicts a plate of sandwiches placed on a patterned tablecloth. The sandwiches are cut in half, revealing their contents, which appear to be a combination of bread and a filling, possibly a meat or cheese mixture. The setting suggests a cozy, intimate dining environment, possibly at home or in a relaxed dining establishment.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25593.9, "ram_available_mb": 100178.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 25593.6, "ram_available_mb": 100178.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.77, "peak": 15.01, "min": 14.12}, "VDD_GPU": {"avg": 28.57, "peak": 38.21, "min": 23.25}, "VIN": {"avg": 69.21, "peak": 103.85, "min": 54.97}, "VDD_CPU_SOC_MSS": {"avg": 15.25, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 28.57, "energy_joules_est": 72.05, "sample_count": 19, "duration_seconds": 2.522}, "timestamp": "2026-01-16T15:38:38.678457"}
{"image_index": 144, "image_name": "000000015079.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015079.jpg", "image_width": 640, "image_height": 424, "image_resolution": "640x424", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1938.121, "latencies_ms": [1938.121], "images_per_second": 0.516, "prompt_tokens": 19, "response_tokens_est": 48, "n_tiles": 6, "output_text": "The image features a plate of food with a light-colored bread and a dark, possibly red or purple filling. The plate is adorned with a decorative pattern, and the lighting is warm and soft, creating a cozy and inviting atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25593.6, "ram_available_mb": 100178.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 25593.9, "ram_available_mb": 100178.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.72, "peak": 15.11, "min": 14.12}, "VDD_GPU": {"avg": 30.3, "peak": 38.21, "min": 24.04}, "VIN": {"avg": 69.79, "peak": 119.3, "min": 60.91}, "VDD_CPU_SOC_MSS": {"avg": 15.24, "peak": 15.36, "min": 14.57}}, "power_watts_avg": 30.3, "energy_joules_est": 58.73, "sample_count": 14, "duration_seconds": 1.938}, "timestamp": "2026-01-16T15:38:40.623364"}
{"image_index": 145, "image_name": "000000015254.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015254.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1877.828, "latencies_ms": [1877.828], "images_per_second": 0.533, "prompt_tokens": 9, "response_tokens_est": 49, "n_tiles": 6, "output_text": "The image shows a colorful assortment of food items arranged in various colored containers, including a red container with spaghetti and meatballs, a green container with carrots, a blue container with grapes, and a purple container with a salad.", "error": null, "sys_before": {"cpu_percent": 43.8, "ram_used_mb": 25593.6, "ram_available_mb": 100178.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 25593.6, "ram_available_mb": 100178.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.67, "peak": 15.32, "min": 13.92}, "VDD_GPU": {"avg": 30.05, "peak": 37.42, "min": 24.43}, "VIN": {"avg": 73.6, "peak": 118.88, "min": 51.89}, "VDD_CPU_SOC_MSS": {"avg": 15.32, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 30.05, "energy_joules_est": 56.44, "sample_count": 14, "duration_seconds": 1.878}, "timestamp": "2026-01-16T15:38:42.561473"}
{"image_index": 145, "image_name": "000000015254.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015254.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 4013.503, "latencies_ms": [4013.503], "images_per_second": 0.249, "prompt_tokens": 23, "response_tokens_est": 128, "n_tiles": 6, "output_text": "object: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject:", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25593.6, "ram_available_mb": 100178.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 25593.6, "ram_available_mb": 100178.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.18, "peak": 15.42, "min": 14.32}, "VDD_GPU": {"avg": 27.04, "peak": 38.21, "min": 23.64}, "VIN": {"avg": 70.64, "peak": 111.71, "min": 61.75}, "VDD_CPU_SOC_MSS": {"avg": 15.71, "peak": 16.14, "min": 14.96}}, "power_watts_avg": 27.04, "energy_joules_est": 108.54, "sample_count": 31, "duration_seconds": 4.014}, "timestamp": "2026-01-16T15:38:46.583367"}
{"image_index": 145, "image_name": "000000015254.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015254.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2746.832, "latencies_ms": [2746.832], "images_per_second": 0.364, "prompt_tokens": 27, "response_tokens_est": 79, "n_tiles": 6, "output_text": "The main objects in the image are arranged in a visually appealing manner. The leftmost object, a red rectangular container, is placed in the foreground. The rightmost object, a blue rectangular container, is also in the foreground. The background features a purple rectangular container, which is slightly out of focus. The foreground objects are closer to the viewer, while the background objects are further away.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25593.6, "ram_available_mb": 100178.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 25593.9, "ram_available_mb": 100178.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.97, "peak": 15.32, "min": 14.22}, "VDD_GPU": {"avg": 28.29, "peak": 37.82, "min": 23.64}, "VIN": {"avg": 66.59, "peak": 79.86, "min": 55.91}, "VDD_CPU_SOC_MSS": {"avg": 15.54, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 28.29, "energy_joules_est": 77.72, "sample_count": 21, "duration_seconds": 2.747}, "timestamp": "2026-01-16T15:38:49.336592"}
{"image_index": 145, "image_name": "000000015254.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015254.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2530.002, "latencies_ms": [2530.002], "images_per_second": 0.395, "prompt_tokens": 21, "response_tokens_est": 71, "n_tiles": 6, "output_text": "The image shows a colorful assortment of food items arranged neatly in various colored trays. The scene appears to be set on a table or countertop, with the food items including a pasta dish, a salad, grapes, and carrots. The setting suggests a meal or snack time, with the food items being prepared and ready to be eaten.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25593.9, "ram_available_mb": 100178.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 25593.9, "ram_available_mb": 100178.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.96, "peak": 15.22, "min": 14.32}, "VDD_GPU": {"avg": 28.64, "peak": 37.82, "min": 23.64}, "VIN": {"avg": 69.25, "peak": 121.83, "min": 57.79}, "VDD_CPU_SOC_MSS": {"avg": 15.55, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 28.64, "energy_joules_est": 72.47, "sample_count": 20, "duration_seconds": 2.53}, "timestamp": "2026-01-16T15:38:51.872632"}
{"image_index": 145, "image_name": "000000015254.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015254.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1967.201, "latencies_ms": [1967.201], "images_per_second": 0.508, "prompt_tokens": 19, "response_tokens_est": 50, "n_tiles": 6, "output_text": "The image features a colorful assortment of food items arranged in various colored containers. The containers are vibrant and include a mix of red, green, and blue hues. The lighting is bright, highlighting the freshness and freshness of the ingredients.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25593.9, "ram_available_mb": 100178.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 25593.9, "ram_available_mb": 100178.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.86, "peak": 15.22, "min": 14.22}, "VDD_GPU": {"avg": 30.25, "peak": 38.21, "min": 24.03}, "VIN": {"avg": 76.85, "peak": 126.74, "min": 62.07}, "VDD_CPU_SOC_MSS": {"avg": 15.43, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 30.25, "energy_joules_est": 59.52, "sample_count": 15, "duration_seconds": 1.968}, "timestamp": "2026-01-16T15:38:53.845804"}
{"image_index": 146, "image_name": "000000015272.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015272.jpg", "image_width": 424, "image_height": 640, "image_resolution": "424x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1765.812, "latencies_ms": [1765.812], "images_per_second": 0.566, "prompt_tokens": 9, "response_tokens_est": 44, "n_tiles": 6, "output_text": "The image captures a vibrant scene of cherry blossoms in full bloom, with the blossoms forming a dense canopy that partially obscures the view of a nearby building, creating a picturesque and serene atmosphere.", "error": null, "sys_before": {"cpu_percent": 38.7, "ram_used_mb": 25593.9, "ram_available_mb": 100178.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 25593.9, "ram_available_mb": 100178.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.55, "peak": 15.11, "min": 13.92}, "VDD_GPU": {"avg": 30.36, "peak": 37.03, "min": 24.43}, "VIN": {"avg": 68.1, "peak": 90.9, "min": 54.87}, "VDD_CPU_SOC_MSS": {"avg": 15.2, "peak": 15.75, "min": 14.57}}, "power_watts_avg": 30.36, "energy_joules_est": 53.62, "sample_count": 13, "duration_seconds": 1.766}, "timestamp": "2026-01-16T15:38:55.672089"}
{"image_index": 146, "image_name": "000000015272.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015272.jpg", "image_width": 424, "image_height": 640, "image_resolution": "424x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1699.791, "latencies_ms": [1699.791], "images_per_second": 0.588, "prompt_tokens": 23, "response_tokens_est": 40, "n_tiles": 6, "output_text": "object: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25593.9, "ram_available_mb": 100178.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 25593.9, "ram_available_mb": 100178.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.8, "peak": 15.11, "min": 14.22}, "VDD_GPU": {"avg": 31.24, "peak": 37.82, "min": 24.83}, "VIN": {"avg": 73.19, "peak": 119.86, "min": 58.06}, "VDD_CPU_SOC_MSS": {"avg": 15.35, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 31.24, "energy_joules_est": 53.12, "sample_count": 13, "duration_seconds": 1.7}, "timestamp": "2026-01-16T15:38:57.377948"}
{"image_index": 146, "image_name": "000000015272.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015272.jpg", "image_width": 424, "image_height": 640, "image_resolution": "424x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2452.448, "latencies_ms": [2452.448], "images_per_second": 0.408, "prompt_tokens": 27, "response_tokens_est": 68, "n_tiles": 6, "output_text": "The main objects in the image are the cherry blossoms in full bloom, which are in the foreground. The background features a traffic light, which is slightly out of focus. The cherry blossoms are densely packed, creating a lush and vibrant foreground, while the traffic light is positioned in the middle ground, slightly out of focus.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25593.9, "ram_available_mb": 100178.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 25594.1, "ram_available_mb": 100178.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.94, "peak": 15.22, "min": 14.22}, "VDD_GPU": {"avg": 29.07, "peak": 38.21, "min": 23.64}, "VIN": {"avg": 74.39, "peak": 117.06, "min": 60.1}, "VDD_CPU_SOC_MSS": {"avg": 15.5, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 29.07, "energy_joules_est": 71.3, "sample_count": 19, "duration_seconds": 2.453}, "timestamp": "2026-01-16T15:38:59.836180"}
{"image_index": 146, "image_name": "000000015272.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015272.jpg", "image_width": 424, "image_height": 640, "image_resolution": "424x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2048.625, "latencies_ms": [2048.625], "images_per_second": 0.488, "prompt_tokens": 21, "response_tokens_est": 51, "n_tiles": 6, "output_text": "The image captures a vibrant scene of cherry blossoms in full bloom, with their delicate pink petals swaying gently in the wind. The blossoms are densely packed on the branches of a tree, creating a picturesque and serene atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25594.1, "ram_available_mb": 100178.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 25594.6, "ram_available_mb": 100177.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.72, "peak": 15.01, "min": 14.12}, "VDD_GPU": {"avg": 29.59, "peak": 38.19, "min": 23.64}, "VIN": {"avg": 70.21, "peak": 111.67, "min": 55.8}, "VDD_CPU_SOC_MSS": {"avg": 15.25, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 29.59, "energy_joules_est": 60.63, "sample_count": 16, "duration_seconds": 2.049}, "timestamp": "2026-01-16T15:39:01.891094"}
{"image_index": 146, "image_name": "000000015272.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015272.jpg", "image_width": 424, "image_height": 640, "image_resolution": "424x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2210.033, "latencies_ms": [2210.033], "images_per_second": 0.452, "prompt_tokens": 19, "response_tokens_est": 57, "n_tiles": 6, "output_text": "The image showcases a tree in full bloom with a profusion of pink cherry blossoms, creating a stunning display of natural beauty. The lighting is soft and diffused, likely due to an overcast sky, which enhances the delicate colors and intricate details of the blossoms.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25594.6, "ram_available_mb": 100177.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 25594.3, "ram_available_mb": 100177.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.7, "peak": 15.01, "min": 14.02}, "VDD_GPU": {"avg": 29.24, "peak": 38.21, "min": 23.24}, "VIN": {"avg": 72.36, "peak": 113.95, "min": 61.12}, "VDD_CPU_SOC_MSS": {"avg": 15.26, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 29.24, "energy_joules_est": 64.64, "sample_count": 17, "duration_seconds": 2.211}, "timestamp": "2026-01-16T15:39:04.107393"}
{"image_index": 147, "image_name": "000000015278.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015278.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2314.994, "latencies_ms": [2314.994], "images_per_second": 0.432, "prompt_tokens": 9, "response_tokens_est": 27, "n_tiles": 12, "output_text": "The image shows a plate of broccoli and other vegetables, including pieces of potato, with a piece of grilled chicken on top.", "error": null, "sys_before": {"cpu_percent": 41.9, "ram_used_mb": 25594.3, "ram_available_mb": 100177.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 25594.3, "ram_available_mb": 100177.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.11, "peak": 16.05, "min": 13.9}, "VDD_GPU": {"avg": 34.99, "peak": 40.97, "min": 27.57}, "VIN": {"avg": 77.26, "peak": 121.58, "min": 58.84}, "VDD_CPU_SOC_MSS": {"avg": 15.94, "peak": 17.32, "min": 14.96}}, "power_watts_avg": 34.99, "energy_joules_est": 81.02, "sample_count": 18, "duration_seconds": 2.315}, "timestamp": "2026-01-16T15:39:06.502770"}
{"image_index": 147, "image_name": "000000015278.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015278.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2710.77, "latencies_ms": [2710.77], "images_per_second": 0.369, "prompt_tokens": 23, "response_tokens_est": 35, "n_tiles": 12, "output_text": "broccoli: 10\nonion: 1\ngarlic: 1\nchili: 1\npepper: 1\nrice: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25594.3, "ram_available_mb": 100177.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 25594.6, "ram_available_mb": 100177.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.04, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 35.53, "peak": 43.33, "min": 27.19}, "VIN": {"avg": 76.19, "peak": 119.41, "min": 47.07}, "VDD_CPU_SOC_MSS": {"avg": 15.73, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 35.53, "energy_joules_est": 96.33, "sample_count": 21, "duration_seconds": 2.711}, "timestamp": "2026-01-16T15:39:09.220978"}
{"image_index": 147, "image_name": "000000015278.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015278.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3790.852, "latencies_ms": [3790.852], "images_per_second": 0.264, "prompt_tokens": 27, "response_tokens_est": 67, "n_tiles": 12, "output_text": "The main object in the foreground is a plate of broccoli and other vegetables. The broccoli is the most prominent vegetable on the plate, with other vegetables such as carrots and possibly onions or garlic scattered around it. The background is slightly blurred, indicating that the focus is on the broccoli and other vegetables in the foreground.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25594.8, "ram_available_mb": 100177.4, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 25594.6, "ram_available_mb": 100177.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.08, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 32.8, "peak": 42.94, "min": 26.0}, "VIN": {"avg": 71.33, "peak": 107.42, "min": 58.45}, "VDD_CPU_SOC_MSS": {"avg": 15.74, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 32.8, "energy_joules_est": 124.35, "sample_count": 30, "duration_seconds": 3.791}, "timestamp": "2026-01-16T15:39:13.018214"}
{"image_index": 147, "image_name": "000000015278.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015278.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3416.026, "latencies_ms": [3416.026], "images_per_second": 0.293, "prompt_tokens": 21, "response_tokens_est": 56, "n_tiles": 12, "output_text": "The image depicts a plate of cooked broccoli, which appears to be seasoned with a mix of spices and possibly some form of sauce. The broccoli is placed on a white plate, and the dish is presented in a way that suggests it is ready to be eaten.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25594.6, "ram_available_mb": 100177.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25594.6, "ram_available_mb": 100177.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.08, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 33.53, "peak": 42.15, "min": 26.39}, "VIN": {"avg": 73.6, "peak": 108.91, "min": 59.14}, "VDD_CPU_SOC_MSS": {"avg": 15.72, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 33.53, "energy_joules_est": 114.56, "sample_count": 26, "duration_seconds": 3.417}, "timestamp": "2026-01-16T15:39:16.440752"}
{"image_index": 147, "image_name": "000000015278.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015278.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3808.514, "latencies_ms": [3808.514], "images_per_second": 0.263, "prompt_tokens": 19, "response_tokens_est": 68, "n_tiles": 12, "output_text": "The image showcases a plate of broccoli and other vegetables, with the broccoli being the most prominent color due to its vibrant green hue. The lighting is bright and even, highlighting the textures and colors of the food. The plate itself is white, providing a clean and neutral background that makes the colors of the food stand out.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25594.6, "ram_available_mb": 100177.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25594.6, "ram_available_mb": 100177.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.11, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 32.66, "peak": 42.54, "min": 26.39}, "VIN": {"avg": 77.39, "peak": 120.66, "min": 58.63}, "VDD_CPU_SOC_MSS": {"avg": 15.58, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 32.66, "energy_joules_est": 124.4, "sample_count": 30, "duration_seconds": 3.809}, "timestamp": "2026-01-16T15:39:20.255712"}
{"image_index": 148, "image_name": "000000015335.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015335.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2807.214, "latencies_ms": [2807.214], "images_per_second": 0.356, "prompt_tokens": 9, "response_tokens_est": 42, "n_tiles": 12, "output_text": "The image depicts a dimly lit indoor setting with a group of people seated around a table, some of whom are engaged in conversation, while others appear to be focused on their phones or other devices.", "error": null, "sys_before": {"cpu_percent": 42.4, "ram_used_mb": 25594.6, "ram_available_mb": 100177.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 25594.6, "ram_available_mb": 100177.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.15, "peak": 16.05, "min": 13.9}, "VDD_GPU": {"avg": 33.93, "peak": 41.36, "min": 26.79}, "VIN": {"avg": 78.16, "peak": 135.7, "min": 58.42}, "VDD_CPU_SOC_MSS": {"avg": 15.87, "peak": 16.92, "min": 14.96}}, "power_watts_avg": 33.93, "energy_joules_est": 95.26, "sample_count": 22, "duration_seconds": 2.808}, "timestamp": "2026-01-16T15:39:23.148548"}
{"image_index": 148, "image_name": "000000015335.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015335.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2605.553, "latencies_ms": [2605.553], "images_per_second": 0.384, "prompt_tokens": 23, "response_tokens_est": 32, "n_tiles": 12, "output_text": "1. Person\n2. Person\n3. Person\n4. Person\n5. Person\n6. Person\n7. Person\n8. Person", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25594.5, "ram_available_mb": 100177.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 25594.0, "ram_available_mb": 100178.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.09, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 35.75, "peak": 42.15, "min": 27.57}, "VIN": {"avg": 84.31, "peak": 132.67, "min": 58.2}, "VDD_CPU_SOC_MSS": {"avg": 15.71, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 35.75, "energy_joules_est": 93.16, "sample_count": 20, "duration_seconds": 2.606}, "timestamp": "2026-01-16T15:39:25.760554"}
{"image_index": 148, "image_name": "000000015335.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015335.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3452.122, "latencies_ms": [3452.122], "images_per_second": 0.29, "prompt_tokens": 27, "response_tokens_est": 57, "n_tiles": 12, "output_text": "The main objects in the image are a group of people seated at a table. The person in the foreground is holding a piece of food, while the person in the background is partially visible. The table is situated in the center of the image, with the people seated around it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25594.0, "ram_available_mb": 100178.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25594.0, "ram_available_mb": 100178.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.08, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 33.53, "peak": 42.94, "min": 26.39}, "VIN": {"avg": 72.29, "peak": 107.56, "min": 58.46}, "VDD_CPU_SOC_MSS": {"avg": 15.66, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 33.53, "energy_joules_est": 115.76, "sample_count": 27, "duration_seconds": 3.453}, "timestamp": "2026-01-16T15:39:29.218514"}
{"image_index": 148, "image_name": "000000015335.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015335.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3481.412, "latencies_ms": [3481.412], "images_per_second": 0.287, "prompt_tokens": 21, "response_tokens_est": 58, "n_tiles": 12, "output_text": "The image depicts a dimly lit indoor setting, likely a restaurant or bar, with patrons seated at tables. The atmosphere is cozy and intimate, with warm lighting creating a relaxed ambiance. The focus is on a young boy eating, while other patrons are visible in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25594.0, "ram_available_mb": 100178.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 25594.0, "ram_available_mb": 100178.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.09, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 33.25, "peak": 42.54, "min": 26.39}, "VIN": {"avg": 72.99, "peak": 106.85, "min": 58.02}, "VDD_CPU_SOC_MSS": {"avg": 15.66, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 33.25, "energy_joules_est": 115.77, "sample_count": 27, "duration_seconds": 3.482}, "timestamp": "2026-01-16T15:39:32.706306"}
{"image_index": 148, "image_name": "000000015335.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015335.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3402.106, "latencies_ms": [3402.106], "images_per_second": 0.294, "prompt_tokens": 19, "response_tokens_est": 56, "n_tiles": 12, "output_text": "The image depicts a dimly lit indoor setting with warm, orange lighting. The individuals are seated at a table, with some wearing dark clothing and others in lighter attire. The lighting creates a cozy and intimate atmosphere, with shadows and highlights adding depth to the scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25594.0, "ram_available_mb": 100178.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 25594.0, "ram_available_mb": 100178.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.11, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 33.44, "peak": 42.15, "min": 26.39}, "VIN": {"avg": 74.03, "peak": 121.03, "min": 58.62}, "VDD_CPU_SOC_MSS": {"avg": 15.62, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 33.44, "energy_joules_est": 113.78, "sample_count": 26, "duration_seconds": 3.402}, "timestamp": "2026-01-16T15:39:36.114898"}
{"image_index": 149, "image_name": "000000015338.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015338.jpg", "image_width": 640, "image_height": 424, "image_resolution": "640x424", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1225.075, "latencies_ms": [1225.075], "images_per_second": 0.816, "prompt_tokens": 9, "response_tokens_est": 24, "n_tiles": 6, "output_text": "A yellow bus is parked on the side of a street, with a white van and other vehicles in the background.", "error": null, "sys_before": {"cpu_percent": 41.7, "ram_used_mb": 25593.8, "ram_available_mb": 100178.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 25593.8, "ram_available_mb": 100178.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.35, "peak": 14.81, "min": 13.82}, "VDD_GPU": {"avg": 33.13, "peak": 37.82, "min": 27.18}, "VIN": {"avg": 79.69, "peak": 115.37, "min": 61.43}, "VDD_CPU_SOC_MSS": {"avg": 15.09, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 33.13, "energy_joules_est": 40.6, "sample_count": 9, "duration_seconds": 1.225}, "timestamp": "2026-01-16T15:39:37.402208"}
{"image_index": 149, "image_name": "000000015338.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015338.jpg", "image_width": 640, "image_height": 424, "image_resolution": "640x424", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1690.826, "latencies_ms": [1690.826], "images_per_second": 0.591, "prompt_tokens": 23, "response_tokens_est": 40, "n_tiles": 6, "output_text": "object: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25593.8, "ram_available_mb": 100178.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 25594.0, "ram_available_mb": 100178.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.81, "peak": 15.22, "min": 14.22}, "VDD_GPU": {"avg": 31.97, "peak": 39.0, "min": 24.83}, "VIN": {"avg": 72.5, "peak": 101.62, "min": 59.08}, "VDD_CPU_SOC_MSS": {"avg": 15.38, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 31.97, "energy_joules_est": 54.07, "sample_count": 13, "duration_seconds": 1.691}, "timestamp": "2026-01-16T15:39:39.098912"}
{"image_index": 149, "image_name": "000000015338.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015338.jpg", "image_width": 640, "image_height": 424, "image_resolution": "640x424", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2171.402, "latencies_ms": [2171.402], "images_per_second": 0.461, "prompt_tokens": 27, "response_tokens_est": 56, "n_tiles": 6, "output_text": "The main objects in the image are a bus and a building. The bus is positioned in the foreground, slightly to the right, and is closer to the viewer. The building is in the background, towering over the bus, and is situated further back in the scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25594.0, "ram_available_mb": 100178.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 25594.0, "ram_available_mb": 100178.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.81, "peak": 15.11, "min": 14.12}, "VDD_GPU": {"avg": 29.59, "peak": 38.6, "min": 23.64}, "VIN": {"avg": 71.51, "peak": 97.88, "min": 62.67}, "VDD_CPU_SOC_MSS": {"avg": 15.38, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 29.59, "energy_joules_est": 64.26, "sample_count": 17, "duration_seconds": 2.172}, "timestamp": "2026-01-16T15:39:41.276313"}
{"image_index": 149, "image_name": "000000015338.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015338.jpg", "image_width": 640, "image_height": 424, "image_resolution": "640x424", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2347.007, "latencies_ms": [2347.007], "images_per_second": 0.426, "prompt_tokens": 21, "response_tokens_est": 62, "n_tiles": 6, "output_text": "The image depicts a city street scene with a yellow bus in the foreground, moving along a curved road. The setting appears to be a modern urban area with a mix of buildings, trees, and a sidewalk. The bus is the main focus, and the scene is captured during daylight with clear skies.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25594.3, "ram_available_mb": 100177.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 25594.3, "ram_available_mb": 100177.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.78, "peak": 15.01, "min": 14.12}, "VDD_GPU": {"avg": 28.71, "peak": 37.8, "min": 23.25}, "VIN": {"avg": 69.06, "peak": 108.28, "min": 55.05}, "VDD_CPU_SOC_MSS": {"avg": 15.33, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 28.71, "energy_joules_est": 67.39, "sample_count": 18, "duration_seconds": 2.347}, "timestamp": "2026-01-16T15:39:43.629286"}
{"image_index": 149, "image_name": "000000015338.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015338.jpg", "image_width": 640, "image_height": 424, "image_resolution": "640x424", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1676.937, "latencies_ms": [1676.937], "images_per_second": 0.596, "prompt_tokens": 19, "response_tokens_est": 38, "n_tiles": 6, "output_text": "The image depicts a sunny day with clear blue skies. The scene features a yellow bus on the road, surrounded by a modern building with reflective windows and a tree-lined sidewalk.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25594.3, "ram_available_mb": 100177.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25594.0, "ram_available_mb": 100178.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.65, "peak": 15.01, "min": 14.02}, "VDD_GPU": {"avg": 30.91, "peak": 37.82, "min": 24.43}, "VIN": {"avg": 71.42, "peak": 106.96, "min": 61.87}, "VDD_CPU_SOC_MSS": {"avg": 15.23, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 30.91, "energy_joules_est": 51.85, "sample_count": 13, "duration_seconds": 1.677}, "timestamp": "2026-01-16T15:39:45.312213"}
{"image_index": 150, "image_name": "000000015440.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015440.jpg", "image_width": 404, "image_height": 640, "image_resolution": "404x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1518.846, "latencies_ms": [1518.846], "images_per_second": 0.658, "prompt_tokens": 9, "response_tokens_est": 35, "n_tiles": 6, "output_text": "The image depicts a stop sign mounted on a metal pole, with a blue car visible in the background, and a clear, sunny sky casting shadows on the ground.", "error": null, "sys_before": {"cpu_percent": 37.9, "ram_used_mb": 25594.0, "ram_available_mb": 100178.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 25594.3, "ram_available_mb": 100177.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.54, "peak": 15.01, "min": 13.92}, "VDD_GPU": {"avg": 31.55, "peak": 37.42, "min": 25.6}, "VIN": {"avg": 79.6, "peak": 116.66, "min": 61.44}, "VDD_CPU_SOC_MSS": {"avg": 15.24, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 31.55, "energy_joules_est": 47.93, "sample_count": 11, "duration_seconds": 1.519}, "timestamp": "2026-01-16T15:39:46.882093"}
{"image_index": 150, "image_name": "000000015440.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015440.jpg", "image_width": 404, "image_height": 640, "image_resolution": "404x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1704.566, "latencies_ms": [1704.566], "images_per_second": 0.587, "prompt_tokens": 23, "response_tokens_est": 40, "n_tiles": 6, "output_text": "object: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25594.3, "ram_available_mb": 100177.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 25594.3, "ram_available_mb": 100177.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.78, "peak": 15.11, "min": 14.22}, "VDD_GPU": {"avg": 31.3, "peak": 38.21, "min": 24.83}, "VIN": {"avg": 65.17, "peak": 71.68, "min": 55.51}, "VDD_CPU_SOC_MSS": {"avg": 15.35, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 31.3, "energy_joules_est": 53.36, "sample_count": 13, "duration_seconds": 1.705}, "timestamp": "2026-01-16T15:39:48.594893"}
{"image_index": 150, "image_name": "000000015440.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015440.jpg", "image_width": 404, "image_height": 640, "image_resolution": "404x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2526.795, "latencies_ms": [2526.795], "images_per_second": 0.396, "prompt_tokens": 27, "response_tokens_est": 71, "n_tiles": 6, "output_text": "The main objects in the image are a stop sign and a metal fence. The stop sign is positioned in the foreground, near the bottom left corner of the image. The metal fence is situated in the background, to the right of the stop sign. The fence is partially obscured by the stop sign, creating a sense of depth in the scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25594.3, "ram_available_mb": 100177.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 25594.3, "ram_available_mb": 100177.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.94, "peak": 15.32, "min": 14.22}, "VDD_GPU": {"avg": 29.0, "peak": 38.6, "min": 23.64}, "VIN": {"avg": 67.25, "peak": 93.85, "min": 60.23}, "VDD_CPU_SOC_MSS": {"avg": 15.48, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 29.0, "energy_joules_est": 73.29, "sample_count": 19, "duration_seconds": 2.527}, "timestamp": "2026-01-16T15:39:51.127788"}
{"image_index": 150, "image_name": "000000015440.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015440.jpg", "image_width": 404, "image_height": 640, "image_resolution": "404x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2714.481, "latencies_ms": [2714.481], "images_per_second": 0.368, "prompt_tokens": 21, "response_tokens_est": 78, "n_tiles": 6, "output_text": "The image depicts a scene in an urban setting during what appears to be either sunrise or sunset, given the warm, golden light bathing the surroundings. A stop sign is prominently displayed on a metal pole, with a blue car visible in the background. The sign is illuminated in red, and the surrounding area includes a sidewalk, a curb, and some greenery.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25594.3, "ram_available_mb": 100177.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 25594.0, "ram_available_mb": 100178.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.0, "peak": 15.32, "min": 14.32}, "VDD_GPU": {"avg": 28.29, "peak": 37.82, "min": 23.64}, "VIN": {"avg": 68.33, "peak": 105.32, "min": 56.8}, "VDD_CPU_SOC_MSS": {"avg": 15.52, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 28.29, "energy_joules_est": 76.8, "sample_count": 21, "duration_seconds": 2.715}, "timestamp": "2026-01-16T15:39:53.848272"}
{"image_index": 150, "image_name": "000000015440.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015440.jpg", "image_width": 404, "image_height": 640, "image_resolution": "404x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2869.19, "latencies_ms": [2869.19], "images_per_second": 0.349, "prompt_tokens": 19, "response_tokens_est": 84, "n_tiles": 6, "output_text": "The image features a stop sign with a red octagonal shape and white lettering, mounted on a metal pole. The sign is illuminated by the warm, golden light of a setting or rising sun, casting long shadows and creating a serene atmosphere. The surrounding area includes a paved road, a metal fence with arches, and some greenery, all bathed in the soft glow of the sun.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25594.2, "ram_available_mb": 100177.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 25594.2, "ram_available_mb": 100177.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.02, "peak": 15.32, "min": 14.32}, "VDD_GPU": {"avg": 28.08, "peak": 37.82, "min": 23.64}, "VIN": {"avg": 70.32, "peak": 118.97, "min": 58.46}, "VDD_CPU_SOC_MSS": {"avg": 15.53, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 28.08, "energy_joules_est": 80.58, "sample_count": 22, "duration_seconds": 2.869}, "timestamp": "2026-01-16T15:39:56.723498"}
{"image_index": 151, "image_name": "000000015497.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015497.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2144.895, "latencies_ms": [2144.895], "images_per_second": 0.466, "prompt_tokens": 9, "response_tokens_est": 22, "n_tiles": 12, "output_text": "A cat is lying on a black surface, with its paws wrapped around a white and black cord.", "error": null, "sys_before": {"cpu_percent": 47.1, "ram_used_mb": 25594.2, "ram_available_mb": 100177.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 2.6, "ram_used_mb": 25594.2, "ram_available_mb": 100177.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.19, "peak": 16.15, "min": 14.1}, "VDD_GPU": {"avg": 35.61, "peak": 40.97, "min": 28.76}, "VIN": {"avg": 75.18, "peak": 105.2, "min": 58.89}, "VDD_CPU_SOC_MSS": {"avg": 16.05, "peak": 17.32, "min": 15.36}}, "power_watts_avg": 35.61, "energy_joules_est": 76.39, "sample_count": 17, "duration_seconds": 2.145}, "timestamp": "2026-01-16T15:39:58.948566"}
{"image_index": 151, "image_name": "000000015497.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015497.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1974.3, "latencies_ms": [1974.3], "images_per_second": 0.507, "prompt_tokens": 23, "response_tokens_est": 13, "n_tiles": 12, "output_text": "1. Cat\n2. Mouse\n3. Power cord", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25594.2, "ram_available_mb": 100177.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 2.1, "ram_used_mb": 25594.2, "ram_available_mb": 100177.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.98, "peak": 15.44, "min": 14.22}, "VDD_GPU": {"avg": 38.18, "peak": 43.31, "min": 31.12}, "VIN": {"avg": 82.31, "peak": 115.51, "min": 59.55}, "VDD_CPU_SOC_MSS": {"avg": 15.74, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 38.18, "energy_joules_est": 75.39, "sample_count": 15, "duration_seconds": 1.975}, "timestamp": "2026-01-16T15:40:00.931505"}
{"image_index": 151, "image_name": "000000015497.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015497.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3337.809, "latencies_ms": [3337.809], "images_per_second": 0.3, "prompt_tokens": 27, "response_tokens_est": 53, "n_tiles": 12, "output_text": "The main object in the foreground is a white cat with black stripes, lying on a black surface. The cat's paws are holding a white cord. In the background, there is a black object, possibly a piece of furniture or clothing, partially visible.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25594.2, "ram_available_mb": 100177.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 25594.5, "ram_available_mb": 100177.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.72, "peak": 15.34, "min": 14.12}, "VDD_GPU": {"avg": 34.0, "peak": 44.12, "min": 26.0}, "VIN": {"avg": 70.48, "peak": 98.05, "min": 58.25}, "VDD_CPU_SOC_MSS": {"avg": 15.21, "peak": 16.14, "min": 14.56}}, "power_watts_avg": 34.0, "energy_joules_est": 113.5, "sample_count": 25, "duration_seconds": 3.338}, "timestamp": "2026-01-16T15:40:04.275650"}
{"image_index": 151, "image_name": "000000015497.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015497.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3917.944, "latencies_ms": [3917.944], "images_per_second": 0.255, "prompt_tokens": 21, "response_tokens_est": 70, "n_tiles": 12, "output_text": "The image depicts a domestic cat lying on a dark surface, possibly a bed or couch, with its paws curled around a white and gray mouse. The cat's eyes are wide open, and it appears to be looking directly at the camera. The setting is indoors, and the cat seems to be relaxed and comfortable in its environment.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25594.5, "ram_available_mb": 100177.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25594.5, "ram_available_mb": 100177.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.75, "peak": 15.34, "min": 14.12}, "VDD_GPU": {"avg": 32.38, "peak": 42.15, "min": 25.6}, "VIN": {"avg": 76.11, "peak": 126.72, "min": 54.42}, "VDD_CPU_SOC_MSS": {"avg": 15.19, "peak": 16.14, "min": 14.56}}, "power_watts_avg": 32.38, "energy_joules_est": 126.88, "sample_count": 29, "duration_seconds": 3.918}, "timestamp": "2026-01-16T15:40:08.200212"}
{"image_index": 151, "image_name": "000000015497.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015497.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3360.986, "latencies_ms": [3360.986], "images_per_second": 0.298, "prompt_tokens": 19, "response_tokens_est": 54, "n_tiles": 12, "output_text": "The cat in the image has a striking appearance with a mix of white, brown, and black fur. The lighting is dim, casting a soft glow on the cat's fur, and the background is dark, providing a stark contrast that highlights the cat's features.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25594.5, "ram_available_mb": 100177.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 25594.2, "ram_available_mb": 100177.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.76, "peak": 15.34, "min": 14.12}, "VDD_GPU": {"avg": 33.31, "peak": 42.54, "min": 26.0}, "VIN": {"avg": 77.31, "peak": 130.17, "min": 57.08}, "VDD_CPU_SOC_MSS": {"avg": 15.26, "peak": 16.14, "min": 14.56}}, "power_watts_avg": 33.31, "energy_joules_est": 111.97, "sample_count": 26, "duration_seconds": 3.361}, "timestamp": "2026-01-16T15:40:11.568380"}
{"image_index": 152, "image_name": "000000015517.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015517.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2447.509, "latencies_ms": [2447.509], "images_per_second": 0.409, "prompt_tokens": 9, "response_tokens_est": 31, "n_tiles": 12, "output_text": "The image depicts a bustling urban scene with a variety of buses and vehicles navigating through a busy city street, surrounded by modern buildings and infrastructure.", "error": null, "sys_before": {"cpu_percent": 41.7, "ram_used_mb": 25594.2, "ram_available_mb": 100177.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 25594.5, "ram_available_mb": 100177.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.08, "peak": 16.05, "min": 13.59}, "VDD_GPU": {"avg": 34.97, "peak": 41.36, "min": 27.57}, "VIN": {"avg": 76.66, "peak": 116.75, "min": 58.9}, "VDD_CPU_SOC_MSS": {"avg": 15.77, "peak": 16.92, "min": 14.57}}, "power_watts_avg": 34.97, "energy_joules_est": 85.61, "sample_count": 19, "duration_seconds": 2.448}, "timestamp": "2026-01-16T15:40:14.107352"}
{"image_index": 152, "image_name": "000000015517.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015517.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2245.536, "latencies_ms": [2245.536], "images_per_second": 0.445, "prompt_tokens": 23, "response_tokens_est": 21, "n_tiles": 12, "output_text": "bus: 5\nbuilding: 1\ntrees: 1\nsign: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25594.5, "ram_available_mb": 100177.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 2.5, "ram_used_mb": 25594.2, "ram_available_mb": 100178.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.07, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 37.28, "peak": 42.94, "min": 29.15}, "VIN": {"avg": 75.37, "peak": 97.44, "min": 44.92}, "VDD_CPU_SOC_MSS": {"avg": 15.72, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 37.28, "energy_joules_est": 83.73, "sample_count": 17, "duration_seconds": 2.246}, "timestamp": "2026-01-16T15:40:16.359676"}
{"image_index": 152, "image_name": "000000015517.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015517.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3897.685, "latencies_ms": [3897.685], "images_per_second": 0.257, "prompt_tokens": 27, "response_tokens_est": 70, "n_tiles": 12, "output_text": "The main objects in the image are a bus and a cityscape. The bus is in the foreground, near the bottom left corner, while the cityscape is in the background, stretching across the entire image. The bus is parked on the street, and the cityscape features various buildings, including a tall tower with a red and white antenna.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25594.2, "ram_available_mb": 100178.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25594.0, "ram_available_mb": 100178.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.07, "peak": 15.65, "min": 14.43}, "VDD_GPU": {"avg": 32.73, "peak": 42.92, "min": 26.01}, "VIN": {"avg": 77.65, "peak": 143.86, "min": 60.33}, "VDD_CPU_SOC_MSS": {"avg": 15.59, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 32.73, "energy_joules_est": 127.59, "sample_count": 30, "duration_seconds": 3.898}, "timestamp": "2026-01-16T15:40:20.263909"}
{"image_index": 152, "image_name": "000000015517.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015517.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3423.71, "latencies_ms": [3423.71], "images_per_second": 0.292, "prompt_tokens": 21, "response_tokens_est": 57, "n_tiles": 12, "output_text": "The image depicts a bustling urban scene with a busy street filled with various buses and vehicles. The setting appears to be a city with modern buildings and infrastructure, including a bridge and a roadway. The weather is overcast, and the overall atmosphere is busy and dynamic.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25594.0, "ram_available_mb": 100178.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25594.0, "ram_available_mb": 100178.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.15, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 33.5, "peak": 42.54, "min": 26.39}, "VIN": {"avg": 73.13, "peak": 119.71, "min": 58.32}, "VDD_CPU_SOC_MSS": {"avg": 15.76, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 33.5, "energy_joules_est": 114.71, "sample_count": 26, "duration_seconds": 3.424}, "timestamp": "2026-01-16T15:40:23.694657"}
{"image_index": 152, "image_name": "000000015517.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015517.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3004.855, "latencies_ms": [3004.855], "images_per_second": 0.333, "prompt_tokens": 19, "response_tokens_est": 44, "n_tiles": 12, "output_text": "The image depicts a bustling urban scene with a mix of modern and traditional architectural styles. The sky is partly cloudy, providing a soft, diffused light that enhances the colors and textures of the scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25594.0, "ram_available_mb": 100178.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 25594.0, "ram_available_mb": 100178.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.11, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 34.48, "peak": 42.94, "min": 26.79}, "VIN": {"avg": 77.54, "peak": 115.83, "min": 58.54}, "VDD_CPU_SOC_MSS": {"avg": 15.71, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 34.48, "energy_joules_est": 103.62, "sample_count": 24, "duration_seconds": 3.005}, "timestamp": "2026-01-16T15:40:26.706899"}
{"image_index": 153, "image_name": "000000015597.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015597.jpg", "image_width": 433, "image_height": 640, "image_resolution": "433x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1253.142, "latencies_ms": [1253.142], "images_per_second": 0.798, "prompt_tokens": 9, "response_tokens_est": 25, "n_tiles": 6, "output_text": "A young man is performing a skateboard trick on a ramp, with a large tent and other equipment in the background.", "error": null, "sys_before": {"cpu_percent": 38.2, "ram_used_mb": 25594.0, "ram_available_mb": 100178.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25594.9, "ram_available_mb": 100177.2, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.35, "peak": 14.81, "min": 13.92}, "VDD_GPU": {"avg": 33.18, "peak": 37.82, "min": 27.57}, "VIN": {"avg": 68.4, "peak": 97.05, "min": 53.93}, "VDD_CPU_SOC_MSS": {"avg": 15.0, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 33.18, "energy_joules_est": 41.59, "sample_count": 9, "duration_seconds": 1.253}, "timestamp": "2026-01-16T15:40:28.019952"}
{"image_index": 153, "image_name": "000000015597.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015597.jpg", "image_width": 433, "image_height": 640, "image_resolution": "433x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2204.753, "latencies_ms": [2204.753], "images_per_second": 0.454, "prompt_tokens": 23, "response_tokens_est": 57, "n_tiles": 6, "output_text": "skateboard: 1\nskateboarder: 1\nskateboard: 1\nskateboard: 1\nskateboard: 1\nskateboard: 1\nskateboard: 1\nskateboard: 1", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25594.9, "ram_available_mb": 100177.2, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 25595.2, "ram_available_mb": 100177.0, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.72, "peak": 15.01, "min": 14.02}, "VDD_GPU": {"avg": 29.85, "peak": 38.98, "min": 23.64}, "VIN": {"avg": 70.45, "peak": 97.2, "min": 62.6}, "VDD_CPU_SOC_MSS": {"avg": 15.21, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 29.85, "energy_joules_est": 65.82, "sample_count": 17, "duration_seconds": 2.205}, "timestamp": "2026-01-16T15:40:30.231020"}
{"image_index": 153, "image_name": "000000015597.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015597.jpg", "image_width": 433, "image_height": 640, "image_resolution": "433x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3108.966, "latencies_ms": [3108.966], "images_per_second": 0.322, "prompt_tokens": 27, "response_tokens_est": 89, "n_tiles": 6, "output_text": "The main object in the foreground is a skateboarder performing a trick on a ramp. The skateboarder is positioned near the center of the image, with their skateboard and feet clearly visible. In the background, there are several large, open tents, likely serving as a makeshift skate park or event space. The tents are positioned to the right of the skateboarder, creating a sense of depth in the image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.2, "ram_available_mb": 100177.0, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 25595.2, "ram_available_mb": 100177.0, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.83, "peak": 15.01, "min": 14.12}, "VDD_GPU": {"avg": 27.53, "peak": 38.21, "min": 23.24}, "VIN": {"avg": 68.6, "peak": 107.78, "min": 55.45}, "VDD_CPU_SOC_MSS": {"avg": 15.42, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 27.53, "energy_joules_est": 85.6, "sample_count": 24, "duration_seconds": 3.109}, "timestamp": "2026-01-16T15:40:33.345519"}
{"image_index": 153, "image_name": "000000015597.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015597.jpg", "image_width": 433, "image_height": 640, "image_resolution": "433x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2514.13, "latencies_ms": [2514.13], "images_per_second": 0.398, "prompt_tokens": 21, "response_tokens_est": 68, "n_tiles": 6, "output_text": "The image captures a skateboarder performing a trick on a concrete ramp at an outdoor skate park. The skate park is surrounded by large, tarp-covered structures, and there are other skateboards and bicycles scattered around. The setting is sunny, and the skateboarder is wearing a hat and shorts, indicating warm weather.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.2, "ram_available_mb": 100177.0, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 25595.2, "ram_available_mb": 100177.0, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.77, "peak": 15.01, "min": 14.12}, "VDD_GPU": {"avg": 28.57, "peak": 37.82, "min": 23.24}, "VIN": {"avg": 68.16, "peak": 103.83, "min": 56.46}, "VDD_CPU_SOC_MSS": {"avg": 15.33, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 28.57, "energy_joules_est": 71.84, "sample_count": 19, "duration_seconds": 2.514}, "timestamp": "2026-01-16T15:40:35.866366"}
{"image_index": 153, "image_name": "000000015597.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015597.jpg", "image_width": 433, "image_height": 640, "image_resolution": "433x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2293.648, "latencies_ms": [2293.648], "images_per_second": 0.436, "prompt_tokens": 19, "response_tokens_est": 60, "n_tiles": 6, "output_text": "The image features a skateboarder performing a trick on a concrete ramp, with a clear blue sky overhead. The skateboarder is wearing a white hat and black shorts, and the ramp is covered in a light-colored material. The lighting is bright and natural, casting shadows on the ramp.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.2, "ram_available_mb": 100177.0, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 25595.2, "ram_available_mb": 100177.0, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.73, "peak": 15.01, "min": 14.02}, "VDD_GPU": {"avg": 29.11, "peak": 37.82, "min": 23.25}, "VIN": {"avg": 69.56, "peak": 114.43, "min": 57.36}, "VDD_CPU_SOC_MSS": {"avg": 15.24, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 29.11, "energy_joules_est": 66.78, "sample_count": 17, "duration_seconds": 2.294}, "timestamp": "2026-01-16T15:40:38.165908"}
{"image_index": 154, "image_name": "000000015660.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015660.jpg", "image_width": 640, "image_height": 348, "image_resolution": "640x348", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 720.295, "latencies_ms": [720.295], "images_per_second": 1.388, "prompt_tokens": 9, "response_tokens_est": 24, "n_tiles": 2, "output_text": "A person is windsurfing on the ocean, holding onto a sail and maneuvering a board with a sail.", "error": null, "sys_before": {"cpu_percent": 35.5, "ram_used_mb": 25595.2, "ram_available_mb": 100177.0, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25595.2, "ram_available_mb": 100177.0, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5055.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.55, "peak": 15.01, "min": 14.1}, "VDD_GPU": {"avg": 27.58, "peak": 32.7, "min": 24.04}, "VIN": {"avg": 64.97, "peak": 68.42, "min": 63.44}, "VDD_CPU_SOC_MSS": {"avg": 15.04, "peak": 15.74, "min": 14.56}}, "power_watts_avg": 27.58, "energy_joules_est": 19.89, "sample_count": 5, "duration_seconds": 0.721}, "timestamp": "2026-01-16T15:40:38.913140"}
{"image_index": 154, "image_name": "000000015660.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015660.jpg", "image_width": 640, "image_height": 348, "image_resolution": "640x348", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1090.197, "latencies_ms": [1090.197], "images_per_second": 0.917, "prompt_tokens": 23, "response_tokens_est": 40, "n_tiles": 2, "output_text": "object: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.2, "ram_available_mb": 100177.0, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25595.4, "ram_available_mb": 100176.8, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5064.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.0, "peak": 15.32, "min": 14.4}, "VDD_GPU": {"avg": 26.56, "peak": 33.09, "min": 22.45}, "VIN": {"avg": 64.74, "peak": 76.31, "min": 61.25}, "VDD_CPU_SOC_MSS": {"avg": 15.47, "peak": 16.14, "min": 14.96}}, "power_watts_avg": 26.56, "energy_joules_est": 28.96, "sample_count": 7, "duration_seconds": 1.09}, "timestamp": "2026-01-16T15:40:40.009526"}
{"image_index": 154, "image_name": "000000015660.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015660.jpg", "image_width": 640, "image_height": 348, "image_resolution": "640x348", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1698.18, "latencies_ms": [1698.18], "images_per_second": 0.589, "prompt_tokens": 27, "response_tokens_est": 67, "n_tiles": 2, "output_text": "The main objects in the image are a person kiteboarding and several kites flying in the sky. The person is positioned in the foreground, closer to the viewer, while the kites are in the background, further away. The kites are near the person, indicating that they are likely being used for kiteboarding.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.4, "ram_available_mb": 100176.8, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25595.4, "ram_available_mb": 100176.8, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5067.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.36, "peak": 15.82, "min": 14.81}, "VDD_GPU": {"avg": 24.15, "peak": 31.5, "min": 20.88}, "VIN": {"avg": 66.2, "peak": 105.87, "min": 53.91}, "VDD_CPU_SOC_MSS": {"avg": 15.95, "peak": 16.55, "min": 15.35}}, "power_watts_avg": 24.15, "energy_joules_est": 41.02, "sample_count": 12, "duration_seconds": 1.698}, "timestamp": "2026-01-16T15:40:41.713507"}
{"image_index": 154, "image_name": "000000015660.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015660.jpg", "image_width": 640, "image_height": 348, "image_resolution": "640x348", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2029.183, "latencies_ms": [2029.183], "images_per_second": 0.493, "prompt_tokens": 21, "response_tokens_est": 81, "n_tiles": 2, "output_text": "The image captures a serene beach scene with a person windsurfing on choppy waters. The individual is wearing a wetsuit and is actively maneuvering a windsurf board, with several kites in the sky, some of which are also windsurfing. The setting is a sunny day with clear blue skies and a calm sea, providing a perfect backdrop for the windsurfing activity.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.4, "ram_available_mb": 100176.8, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25595.4, "ram_available_mb": 100176.8, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5063.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.3, "peak": 15.82, "min": 14.71}, "VDD_GPU": {"avg": 23.3, "peak": 31.5, "min": 20.87}, "VIN": {"avg": 64.88, "peak": 80.13, "min": 61.24}, "VDD_CPU_SOC_MSS": {"avg": 15.85, "peak": 16.15, "min": 14.96}}, "power_watts_avg": 23.3, "energy_joules_est": 47.28, "sample_count": 15, "duration_seconds": 2.029}, "timestamp": "2026-01-16T15:40:43.752336"}
{"image_index": 154, "image_name": "000000015660.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015660.jpg", "image_width": 640, "image_height": 348, "image_resolution": "640x348", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1040.302, "latencies_ms": [1040.302], "images_per_second": 0.961, "prompt_tokens": 19, "response_tokens_est": 38, "n_tiles": 2, "output_text": "The image depicts a vibrant scene at the beach with clear blue skies and a few scattered clouds. The ocean waves are gently crashing onto the shore, creating a dynamic and lively atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.4, "ram_available_mb": 100176.8, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25595.4, "ram_available_mb": 100176.8, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5062.6, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.05, "peak": 15.42, "min": 14.71}, "VDD_GPU": {"avg": 25.46, "peak": 30.71, "min": 22.06}, "VIN": {"avg": 66.28, "peak": 97.7, "min": 58.02}, "VDD_CPU_SOC_MSS": {"avg": 15.7, "peak": 16.14, "min": 15.36}}, "power_watts_avg": 25.46, "energy_joules_est": 26.5, "sample_count": 8, "duration_seconds": 1.041}, "timestamp": "2026-01-16T15:40:44.798477"}
{"image_index": 155, "image_name": "000000015746.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015746.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1494.823, "latencies_ms": [1494.823], "images_per_second": 0.669, "prompt_tokens": 9, "response_tokens_est": 34, "n_tiles": 6, "output_text": "The image shows a vibrant red fire hydrant situated on a grassy area, with a backdrop of trees and a house, and a few flowers in the foreground.", "error": null, "sys_before": {"cpu_percent": 48.6, "ram_used_mb": 25595.4, "ram_available_mb": 100176.8, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 25595.4, "ram_available_mb": 100176.8, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.52, "peak": 15.01, "min": 13.92}, "VDD_GPU": {"avg": 31.19, "peak": 36.63, "min": 25.6}, "VIN": {"avg": 70.74, "peak": 113.85, "min": 55.7}, "VDD_CPU_SOC_MSS": {"avg": 15.25, "peak": 15.75, "min": 14.57}}, "power_watts_avg": 31.19, "energy_joules_est": 46.63, "sample_count": 11, "duration_seconds": 1.495}, "timestamp": "2026-01-16T15:40:46.347342"}
{"image_index": 155, "image_name": "000000015746.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015746.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1697.218, "latencies_ms": [1697.218], "images_per_second": 0.589, "prompt_tokens": 23, "response_tokens_est": 40, "n_tiles": 6, "output_text": "object: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.4, "ram_available_mb": 100176.8, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25595.6, "ram_available_mb": 100176.5, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.79, "peak": 15.11, "min": 14.22}, "VDD_GPU": {"avg": 31.77, "peak": 38.19, "min": 25.22}, "VIN": {"avg": 77.26, "peak": 126.06, "min": 50.69}, "VDD_CPU_SOC_MSS": {"avg": 15.35, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 31.77, "energy_joules_est": 53.93, "sample_count": 12, "duration_seconds": 1.697}, "timestamp": "2026-01-16T15:40:48.051180"}
{"image_index": 155, "image_name": "000000015746.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015746.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2721.826, "latencies_ms": [2721.826], "images_per_second": 0.367, "prompt_tokens": 27, "response_tokens_est": 75, "n_tiles": 6, "output_text": "The main object in the foreground is a red fire hydrant, which is situated on the grass. The hydrant is positioned near the center of the image, slightly to the left. In the background, there is a house with a white exterior and a window with purple flowers. The house is slightly out of focus, indicating that the focus is on the hydrant.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.6, "ram_available_mb": 100176.5, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 25595.4, "ram_available_mb": 100176.8, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.8, "peak": 15.01, "min": 14.12}, "VDD_GPU": {"avg": 28.31, "peak": 38.6, "min": 23.25}, "VIN": {"avg": 68.9, "peak": 107.87, "min": 58.15}, "VDD_CPU_SOC_MSS": {"avg": 15.37, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 28.31, "energy_joules_est": 77.06, "sample_count": 20, "duration_seconds": 2.722}, "timestamp": "2026-01-16T15:40:50.779137"}
{"image_index": 155, "image_name": "000000015746.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015746.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2300.359, "latencies_ms": [2300.359], "images_per_second": 0.435, "prompt_tokens": 21, "response_tokens_est": 60, "n_tiles": 6, "output_text": "The image depicts a vibrant red fire hydrant situated in a lush, green grassy area with a backdrop of a residential building and trees. The hydrant appears weathered, with some rust and dirt, and is surrounded by various plants and flowers, creating a serene and natural setting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.4, "ram_available_mb": 100176.8, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 25595.1, "ram_available_mb": 100177.0, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.74, "peak": 15.01, "min": 14.12}, "VDD_GPU": {"avg": 29.02, "peak": 38.21, "min": 23.25}, "VIN": {"avg": 68.53, "peak": 121.47, "min": 57.47}, "VDD_CPU_SOC_MSS": {"avg": 15.21, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 29.02, "energy_joules_est": 66.77, "sample_count": 17, "duration_seconds": 2.301}, "timestamp": "2026-01-16T15:40:53.086539"}
{"image_index": 155, "image_name": "000000015746.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015746.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2130.729, "latencies_ms": [2130.729], "images_per_second": 0.469, "prompt_tokens": 19, "response_tokens_est": 54, "n_tiles": 6, "output_text": "The notable visual attributes of the image include a vivid red fire hydrant with a black cap and a black hose attachment, set against a lush green grassy background. The lighting is natural, suggesting it is daytime, and the weather appears to be clear and sunny.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.1, "ram_available_mb": 100177.0, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 25595.1, "ram_available_mb": 100177.0, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.7, "peak": 15.01, "min": 14.02}, "VDD_GPU": {"avg": 29.5, "peak": 38.21, "min": 23.64}, "VIN": {"avg": 70.34, "peak": 101.75, "min": 50.44}, "VDD_CPU_SOC_MSS": {"avg": 15.23, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 29.5, "energy_joules_est": 62.88, "sample_count": 16, "duration_seconds": 2.131}, "timestamp": "2026-01-16T15:40:55.223622"}
{"image_index": 156, "image_name": "000000015751.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015751.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1387.65, "latencies_ms": [1387.65], "images_per_second": 0.721, "prompt_tokens": 9, "response_tokens_est": 30, "n_tiles": 6, "output_text": "The image shows a close-up view of a blue wooden surface with some birds perched on it, and a small fish swimming near the birds.", "error": null, "sys_before": {"cpu_percent": 41.7, "ram_used_mb": 25595.1, "ram_available_mb": 100177.0, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 25595.1, "ram_available_mb": 100177.0, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.46, "peak": 15.01, "min": 13.92}, "VDD_GPU": {"avg": 31.63, "peak": 37.42, "min": 26.0}, "VIN": {"avg": 71.23, "peak": 115.01, "min": 53.33}, "VDD_CPU_SOC_MSS": {"avg": 15.16, "peak": 15.74, "min": 14.56}}, "power_watts_avg": 31.63, "energy_joules_est": 43.91, "sample_count": 10, "duration_seconds": 1.388}, "timestamp": "2026-01-16T15:40:56.658205"}
{"image_index": 156, "image_name": "000000015751.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015751.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1915.076, "latencies_ms": [1915.076], "images_per_second": 0.522, "prompt_tokens": 23, "response_tokens_est": 48, "n_tiles": 6, "output_text": "birds: 3\nbirds: 2\nbirds: 1\nbirds: 1\nbirds: 1\nbirds: 1\nbirds: 1\nbirds: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.1, "ram_available_mb": 100177.0, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 25594.9, "ram_available_mb": 100177.3, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.85, "peak": 15.22, "min": 14.22}, "VDD_GPU": {"avg": 30.59, "peak": 38.6, "min": 24.03}, "VIN": {"avg": 75.59, "peak": 120.24, "min": 61.99}, "VDD_CPU_SOC_MSS": {"avg": 15.43, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 30.59, "energy_joules_est": 58.6, "sample_count": 15, "duration_seconds": 1.916}, "timestamp": "2026-01-16T15:40:58.579566"}
{"image_index": 156, "image_name": "000000015751.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015751.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2275.927, "latencies_ms": [2275.927], "images_per_second": 0.439, "prompt_tokens": 27, "response_tokens_est": 60, "n_tiles": 6, "output_text": "The main objects in the image are a pair of birds and a piece of wood. The birds are positioned near the bottom left corner of the image, while the wood is in the background. The birds are in close proximity to the wood, suggesting they may be perched or resting on it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25594.9, "ram_available_mb": 100177.3, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 25594.9, "ram_available_mb": 100177.3, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.87, "peak": 15.11, "min": 14.22}, "VDD_GPU": {"avg": 29.22, "peak": 38.6, "min": 23.25}, "VIN": {"avg": 68.71, "peak": 113.68, "min": 57.97}, "VDD_CPU_SOC_MSS": {"avg": 15.57, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 29.22, "energy_joules_est": 66.52, "sample_count": 18, "duration_seconds": 2.276}, "timestamp": "2026-01-16T15:41:00.861694"}
{"image_index": 156, "image_name": "000000015751.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015751.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2633.907, "latencies_ms": [2633.907], "images_per_second": 0.38, "prompt_tokens": 21, "response_tokens_est": 72, "n_tiles": 6, "output_text": "The image depicts a close-up view of a weathered wooden surface with a blue paint finish. Birds are perched on the surface, with one bird in the foreground appearing to be in the process of eating or catching something. The setting suggests an outdoor or rustic environment, possibly a park or a backyard, where the birds have found a suitable perch.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25594.9, "ram_available_mb": 100177.3, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 25594.9, "ram_available_mb": 100177.3, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.76, "peak": 15.01, "min": 14.02}, "VDD_GPU": {"avg": 28.17, "peak": 38.21, "min": 23.25}, "VIN": {"avg": 70.26, "peak": 114.53, "min": 61.99}, "VDD_CPU_SOC_MSS": {"avg": 15.3, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 28.17, "energy_joules_est": 74.21, "sample_count": 20, "duration_seconds": 2.634}, "timestamp": "2026-01-16T15:41:03.502598"}
{"image_index": 156, "image_name": "000000015751.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015751.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1733.156, "latencies_ms": [1733.156], "images_per_second": 0.577, "prompt_tokens": 19, "response_tokens_est": 40, "n_tiles": 6, "output_text": "The image features a weathered, teal-colored wooden surface with visible signs of wear and age. The lighting is soft and diffused, casting gentle shadows and highlighting the texture of the wood.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25594.9, "ram_available_mb": 100177.3, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 25594.9, "ram_available_mb": 100177.3, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.62, "peak": 14.91, "min": 14.02}, "VDD_GPU": {"avg": 30.88, "peak": 38.21, "min": 24.43}, "VIN": {"avg": 72.13, "peak": 111.67, "min": 62.63}, "VDD_CPU_SOC_MSS": {"avg": 15.2, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 30.88, "energy_joules_est": 53.53, "sample_count": 13, "duration_seconds": 1.734}, "timestamp": "2026-01-16T15:41:05.241879"}
{"image_index": 157, "image_name": "000000015956.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015956.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1982.724, "latencies_ms": [1982.724], "images_per_second": 0.504, "prompt_tokens": 9, "response_tokens_est": 17, "n_tiles": 12, "output_text": "A woman is walking through a rustic barn with a brown horse in the background.", "error": null, "sys_before": {"cpu_percent": 44.8, "ram_used_mb": 25594.9, "ram_available_mb": 100177.3, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 2.2, "ram_used_mb": 25594.9, "ram_available_mb": 100177.3, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.15, "peak": 16.05, "min": 13.9}, "VDD_GPU": {"avg": 36.61, "peak": 41.36, "min": 29.15}, "VIN": {"avg": 82.36, "peak": 114.65, "min": 59.42}, "VDD_CPU_SOC_MSS": {"avg": 15.95, "peak": 16.92, "min": 14.96}}, "power_watts_avg": 36.61, "energy_joules_est": 72.6, "sample_count": 15, "duration_seconds": 1.983}, "timestamp": "2026-01-16T15:41:07.310648"}
{"image_index": 157, "image_name": "000000015956.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015956.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2679.494, "latencies_ms": [2679.494], "images_per_second": 0.373, "prompt_tokens": 23, "response_tokens_est": 34, "n_tiles": 12, "output_text": "1. Horse\n2. Barn\n3. Woman\n4. Door\n5. Bucket\n6. Stool\n7. Shed\n8. Floor", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25594.9, "ram_available_mb": 100177.3, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 25595.1, "ram_available_mb": 100177.1, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.03, "peak": 15.65, "min": 14.32}, "VDD_GPU": {"avg": 35.7, "peak": 43.33, "min": 27.18}, "VIN": {"avg": 75.98, "peak": 114.82, "min": 58.13}, "VDD_CPU_SOC_MSS": {"avg": 15.71, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 35.7, "energy_joules_est": 95.69, "sample_count": 21, "duration_seconds": 2.68}, "timestamp": "2026-01-16T15:41:09.997553"}
{"image_index": 157, "image_name": "000000015956.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015956.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3620.38, "latencies_ms": [3620.38], "images_per_second": 0.276, "prompt_tokens": 27, "response_tokens_est": 62, "n_tiles": 12, "output_text": "The main object in the foreground is a brown horse with a red halter. The horse is standing on a dirt ground, with a wooden fence and a red door in the background. The background features a wooden barn with a window, a woman in blue jeans, and various items on shelves and tables.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.1, "ram_available_mb": 100177.1, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25594.9, "ram_available_mb": 100177.3, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.08, "peak": 15.54, "min": 14.53}, "VDD_GPU": {"avg": 33.1, "peak": 42.94, "min": 26.39}, "VIN": {"avg": 77.05, "peak": 116.7, "min": 58.42}, "VDD_CPU_SOC_MSS": {"avg": 15.7, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 33.1, "energy_joules_est": 119.84, "sample_count": 28, "duration_seconds": 3.621}, "timestamp": "2026-01-16T15:41:13.625183"}
{"image_index": 157, "image_name": "000000015956.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015956.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2945.726, "latencies_ms": [2945.726], "images_per_second": 0.339, "prompt_tokens": 21, "response_tokens_est": 42, "n_tiles": 12, "output_text": "The image depicts a rustic, rural barn interior with a dirt floor and wooden walls. A woman is walking through the barn, while a brown horse with a red halter is seen in the background.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 25594.9, "ram_available_mb": 100177.3, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 25595.3, "ram_available_mb": 100176.8, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.07, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 34.47, "peak": 42.54, "min": 26.79}, "VIN": {"avg": 75.28, "peak": 121.16, "min": 58.21}, "VDD_CPU_SOC_MSS": {"avg": 15.69, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 34.47, "energy_joules_est": 101.56, "sample_count": 23, "duration_seconds": 2.946}, "timestamp": "2026-01-16T15:41:16.577532"}
{"image_index": 157, "image_name": "000000015956.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000015956.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2738.126, "latencies_ms": [2738.126], "images_per_second": 0.365, "prompt_tokens": 19, "response_tokens_est": 36, "n_tiles": 12, "output_text": "The image depicts a rustic barn with a dirt floor and wooden walls. The lighting is natural, with sunlight streaming in through a window, creating a warm and inviting atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.3, "ram_available_mb": 100176.8, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 25595.1, "ram_available_mb": 100177.1, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.08, "peak": 15.65, "min": 14.63}, "VDD_GPU": {"avg": 35.36, "peak": 42.94, "min": 27.18}, "VIN": {"avg": 74.29, "peak": 102.67, "min": 56.59}, "VDD_CPU_SOC_MSS": {"avg": 15.75, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 35.36, "energy_joules_est": 96.83, "sample_count": 21, "duration_seconds": 2.738}, "timestamp": "2026-01-16T15:41:19.322123"}
{"image_index": 158, "image_name": "000000016010.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016010.jpg", "image_width": 640, "image_height": 471, "image_resolution": "640x471", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2790.686, "latencies_ms": [2790.686], "images_per_second": 0.358, "prompt_tokens": 9, "response_tokens_est": 41, "n_tiles": 12, "output_text": "The image depicts a serene and lush green meadow with a variety of wildlife, including a zebra grazing and a herd of elephants and rhinos peacefully coexisting in the background.", "error": null, "sys_before": {"cpu_percent": 45.3, "ram_used_mb": 25595.1, "ram_available_mb": 100177.1, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 25595.3, "ram_available_mb": 100176.8, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.08, "peak": 16.05, "min": 13.59}, "VDD_GPU": {"avg": 34.08, "peak": 41.76, "min": 26.79}, "VIN": {"avg": 77.26, "peak": 106.99, "min": 67.27}, "VDD_CPU_SOC_MSS": {"avg": 15.86, "peak": 17.32, "min": 14.57}}, "power_watts_avg": 34.08, "energy_joules_est": 95.12, "sample_count": 21, "duration_seconds": 2.791}, "timestamp": "2026-01-16T15:41:22.212801"}
{"image_index": 158, "image_name": "000000016010.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016010.jpg", "image_width": 640, "image_height": 471, "image_resolution": "640x471", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 3687.324, "latencies_ms": [3687.324], "images_per_second": 0.271, "prompt_tokens": 23, "response_tokens_est": 64, "n_tiles": 12, "output_text": "1. Rhino: 1\n2. Elephant: 1\n3. Zebra: 1\n4. Buffalo: 1\n5. Giraffe: 1\n6. Antelope: 1\n7. Deer: 1\n8. Cattle: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.3, "ram_available_mb": 100176.8, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25594.8, "ram_available_mb": 100177.3, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.09, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 32.98, "peak": 43.33, "min": 26.39}, "VIN": {"avg": 76.86, "peak": 121.09, "min": 58.61}, "VDD_CPU_SOC_MSS": {"avg": 15.64, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 32.98, "energy_joules_est": 121.62, "sample_count": 29, "duration_seconds": 3.688}, "timestamp": "2026-01-16T15:41:25.906489"}
{"image_index": 158, "image_name": "000000016010.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016010.jpg", "image_width": 640, "image_height": 471, "image_resolution": "640x471", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3885.206, "latencies_ms": [3885.206], "images_per_second": 0.257, "prompt_tokens": 27, "response_tokens_est": 70, "n_tiles": 12, "output_text": "The main objects in the image include a herd of elephants, zebras, and a few other animals. The elephants are in the background, while the zebras are in the foreground. The zebras are grazing on the grass, and there is a rope in the foreground that appears to be part of a fence or boundary.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25594.8, "ram_available_mb": 100177.3, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25594.8, "ram_available_mb": 100177.3, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.11, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 32.63, "peak": 42.15, "min": 26.0}, "VIN": {"avg": 73.93, "peak": 104.52, "min": 58.51}, "VDD_CPU_SOC_MSS": {"avg": 15.68, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 32.63, "energy_joules_est": 126.79, "sample_count": 30, "duration_seconds": 3.886}, "timestamp": "2026-01-16T15:41:29.797511"}
{"image_index": 158, "image_name": "000000016010.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016010.jpg", "image_width": 640, "image_height": 471, "image_resolution": "640x471", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3409.053, "latencies_ms": [3409.053], "images_per_second": 0.293, "prompt_tokens": 21, "response_tokens_est": 56, "n_tiles": 12, "output_text": "The image depicts a serene and lush green meadow with a variety of wildlife, including a herd of zebras grazing peacefully. The meadow is bordered by a stone wall and is surrounded by tall trees, creating a tranquil and natural environment.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25594.8, "ram_available_mb": 100177.3, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 25594.8, "ram_available_mb": 100177.3, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.09, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 33.55, "peak": 42.15, "min": 26.39}, "VIN": {"avg": 72.55, "peak": 133.79, "min": 53.33}, "VDD_CPU_SOC_MSS": {"avg": 15.72, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 33.55, "energy_joules_est": 114.38, "sample_count": 26, "duration_seconds": 3.409}, "timestamp": "2026-01-16T15:41:33.212881"}
{"image_index": 158, "image_name": "000000016010.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016010.jpg", "image_width": 640, "image_height": 471, "image_resolution": "640x471", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2713.122, "latencies_ms": [2713.122], "images_per_second": 0.369, "prompt_tokens": 19, "response_tokens_est": 35, "n_tiles": 12, "output_text": "The image depicts a lush, green meadow with a variety of trees in the background. The sky is partly cloudy, providing a mix of sunlight and shade.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25594.8, "ram_available_mb": 100177.3, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 25594.8, "ram_available_mb": 100177.3, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.08, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 35.23, "peak": 42.94, "min": 27.18}, "VIN": {"avg": 80.85, "peak": 121.31, "min": 58.73}, "VDD_CPU_SOC_MSS": {"avg": 15.8, "peak": 16.53, "min": 15.35}}, "power_watts_avg": 35.23, "energy_joules_est": 95.6, "sample_count": 21, "duration_seconds": 2.714}, "timestamp": "2026-01-16T15:41:35.932352"}
{"image_index": 159, "image_name": "000000016228.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016228.jpg", "image_width": 640, "image_height": 440, "image_resolution": "640x440", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1519.869, "latencies_ms": [1519.869], "images_per_second": 0.658, "prompt_tokens": 9, "response_tokens_est": 35, "n_tiles": 6, "output_text": "A horse-drawn carriage is being pulled by a white horse, with a crowd of people gathered around it, in what appears to be a park or public square.", "error": null, "sys_before": {"cpu_percent": 40.7, "ram_used_mb": 25594.8, "ram_available_mb": 100177.3, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 25595.1, "ram_available_mb": 100177.1, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.44, "peak": 15.01, "min": 13.82}, "VDD_GPU": {"avg": 31.91, "peak": 37.82, "min": 25.61}, "VIN": {"avg": 66.9, "peak": 110.38, "min": 49.87}, "VDD_CPU_SOC_MSS": {"avg": 15.14, "peak": 15.75, "min": 14.57}}, "power_watts_avg": 31.91, "energy_joules_est": 48.51, "sample_count": 11, "duration_seconds": 1.52}, "timestamp": "2026-01-16T15:41:37.519244"}
{"image_index": 159, "image_name": "000000016228.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016228.jpg", "image_width": 640, "image_height": 440, "image_resolution": "640x440", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1699.503, "latencies_ms": [1699.503], "images_per_second": 0.588, "prompt_tokens": 23, "response_tokens_est": 40, "n_tiles": 6, "output_text": "object: 1\nobject: 2\nobject: 3\nobject: 4\nobject: 5\nobject: 6\nobject: 7\nobject: 8", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25595.1, "ram_available_mb": 100177.1, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25594.8, "ram_available_mb": 100177.4, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.8, "peak": 15.22, "min": 14.22}, "VDD_GPU": {"avg": 31.45, "peak": 38.21, "min": 24.82}, "VIN": {"avg": 68.22, "peak": 91.45, "min": 57.8}, "VDD_CPU_SOC_MSS": {"avg": 15.48, "peak": 16.14, "min": 14.96}}, "power_watts_avg": 31.45, "energy_joules_est": 53.46, "sample_count": 13, "duration_seconds": 1.7}, "timestamp": "2026-01-16T15:41:39.226833"}
{"image_index": 159, "image_name": "000000016228.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016228.jpg", "image_width": 640, "image_height": 440, "image_resolution": "640x440", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2985.572, "latencies_ms": [2985.572], "images_per_second": 0.335, "prompt_tokens": 27, "response_tokens_est": 88, "n_tiles": 6, "output_text": "The main object in the foreground is a white horse being pulled by a carriage. The carriage is decorated with green and gold colors and has the word \"Disneyland\" on it. In the background, there is a crowd of people and a trolley. The trolley is moving along the street, and there is a yellow and white umbrella nearby. The scene appears to be set in a park or a public area.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25594.8, "ram_available_mb": 100177.4, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 25594.8, "ram_available_mb": 100177.4, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.01, "peak": 15.32, "min": 14.22}, "VDD_GPU": {"avg": 28.09, "peak": 38.21, "min": 23.64}, "VIN": {"avg": 66.46, "peak": 103.12, "min": 56.92}, "VDD_CPU_SOC_MSS": {"avg": 15.65, "peak": 16.14, "min": 14.96}}, "power_watts_avg": 28.09, "energy_joules_est": 83.88, "sample_count": 23, "duration_seconds": 2.986}, "timestamp": "2026-01-16T15:41:42.218328"}
{"image_index": 159, "image_name": "000000016228.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016228.jpg", "image_width": 640, "image_height": 440, "image_resolution": "640x440", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2471.812, "latencies_ms": [2471.812], "images_per_second": 0.405, "prompt_tokens": 21, "response_tokens_est": 69, "n_tiles": 6, "output_text": "The image depicts a scene from a parade or public event, likely in a park or open area, with a white horse being pulled by a carriage. The carriage is adorned with colorful decorations and has the word \"Disneyland\" on it. People are walking around, and there are trees and other park features in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25594.8, "ram_available_mb": 100177.4, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 25595.1, "ram_available_mb": 100177.1, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.99, "peak": 15.32, "min": 14.32}, "VDD_GPU": {"avg": 28.74, "peak": 37.82, "min": 23.64}, "VIN": {"avg": 68.91, "peak": 95.04, "min": 54.23}, "VDD_CPU_SOC_MSS": {"avg": 15.71, "peak": 16.14, "min": 14.96}}, "power_watts_avg": 28.74, "energy_joules_est": 71.06, "sample_count": 19, "duration_seconds": 2.472}, "timestamp": "2026-01-16T15:41:44.697036"}
{"image_index": 159, "image_name": "000000016228.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016228.jpg", "image_width": 640, "image_height": 440, "image_resolution": "640x440", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2577.301, "latencies_ms": [2577.301], "images_per_second": 0.388, "prompt_tokens": 19, "response_tokens_est": 73, "n_tiles": 6, "output_text": "The image depicts a vintage scene with a horse-drawn carriage, featuring a white horse and a green carriage with gold trim. The carriage is adorned with a yellow and white striped canopy, and the horse is wearing a harness. The scene is set outdoors under clear, bright daylight, with trees and a park-like setting in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.1, "ram_available_mb": 100177.1, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 25594.8, "ram_available_mb": 100177.4, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.97, "peak": 15.32, "min": 14.22}, "VDD_GPU": {"avg": 28.86, "peak": 38.21, "min": 23.64}, "VIN": {"avg": 71.88, "peak": 112.18, "min": 62.03}, "VDD_CPU_SOC_MSS": {"avg": 15.62, "peak": 16.14, "min": 14.96}}, "power_watts_avg": 28.86, "energy_joules_est": 74.39, "sample_count": 19, "duration_seconds": 2.578}, "timestamp": "2026-01-16T15:41:47.280989"}
{"image_index": 160, "image_name": "000000016249.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016249.jpg", "image_width": 500, "image_height": 365, "image_resolution": "500x365", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2319.943, "latencies_ms": [2319.943], "images_per_second": 0.431, "prompt_tokens": 9, "response_tokens_est": 27, "n_tiles": 12, "output_text": "The image depicts a group of elderly men sitting on benches in a public square, each engrossed in reading newspapers.", "error": null, "sys_before": {"cpu_percent": 41.7, "ram_used_mb": 25594.8, "ram_available_mb": 100177.4, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 25594.8, "ram_available_mb": 100177.4, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.14, "peak": 16.15, "min": 13.9}, "VDD_GPU": {"avg": 35.15, "peak": 40.97, "min": 27.96}, "VIN": {"avg": 77.21, "peak": 103.02, "min": 59.36}, "VDD_CPU_SOC_MSS": {"avg": 15.93, "peak": 17.32, "min": 14.96}}, "power_watts_avg": 35.15, "energy_joules_est": 81.56, "sample_count": 17, "duration_seconds": 2.32}, "timestamp": "2026-01-16T15:41:49.692017"}
{"image_index": 160, "image_name": "000000016249.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016249.jpg", "image_width": 500, "image_height": 365, "image_resolution": "500x365", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 3420.686, "latencies_ms": [3420.686], "images_per_second": 0.292, "prompt_tokens": 23, "response_tokens_est": 56, "n_tiles": 12, "output_text": "1. Man: 2\n2. Man: 2\n3. Man: 2\n4. Man: 2\n5. Man: 2\n6. Man: 2\n7. Man: 2\n8. Man: 2", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25594.8, "ram_available_mb": 100177.4, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 25594.8, "ram_available_mb": 100177.4, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.06, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 33.68, "peak": 43.33, "min": 26.39}, "VIN": {"avg": 73.61, "peak": 118.36, "min": 58.48}, "VDD_CPU_SOC_MSS": {"avg": 15.64, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 33.68, "energy_joules_est": 115.22, "sample_count": 26, "duration_seconds": 3.421}, "timestamp": "2026-01-16T15:41:53.119104"}
{"image_index": 160, "image_name": "000000016249.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016249.jpg", "image_width": 500, "image_height": 365, "image_resolution": "500x365", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3389.857, "latencies_ms": [3389.857], "images_per_second": 0.295, "prompt_tokens": 27, "response_tokens_est": 55, "n_tiles": 12, "output_text": "The main objects in the image are a group of elderly men sitting on benches. The benches are positioned in the foreground, with the men seated on them. The background features a building with a large sign, and a few other people are visible in the distance.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25594.8, "ram_available_mb": 100177.4, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 25594.8, "ram_available_mb": 100177.4, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.08, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 33.39, "peak": 41.74, "min": 26.39}, "VIN": {"avg": 73.74, "peak": 118.77, "min": 56.37}, "VDD_CPU_SOC_MSS": {"avg": 15.78, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 33.39, "energy_joules_est": 113.2, "sample_count": 26, "duration_seconds": 3.39}, "timestamp": "2026-01-16T15:41:56.519739"}
{"image_index": 160, "image_name": "000000016249.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016249.jpg", "image_width": 500, "image_height": 365, "image_resolution": "500x365", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3593.035, "latencies_ms": [3593.035], "images_per_second": 0.278, "prompt_tokens": 21, "response_tokens_est": 61, "n_tiles": 12, "output_text": "The image depicts an outdoor scene with several elderly individuals seated on benches in a public space. The setting appears to be a city square or park, with a mix of modern and traditional architectural elements. The individuals are engaged in various activities, such as reading newspapers, conversing, and relaxing.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25594.8, "ram_available_mb": 100177.4, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25594.8, "ram_available_mb": 100177.4, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.08, "peak": 15.65, "min": 14.43}, "VDD_GPU": {"avg": 33.04, "peak": 42.15, "min": 26.39}, "VIN": {"avg": 75.28, "peak": 132.47, "min": 54.43}, "VDD_CPU_SOC_MSS": {"avg": 15.76, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 33.04, "energy_joules_est": 118.73, "sample_count": 27, "duration_seconds": 3.593}, "timestamp": "2026-01-16T15:42:00.119446"}
{"image_index": 160, "image_name": "000000016249.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016249.jpg", "image_width": 500, "image_height": 365, "image_resolution": "500x365", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3885.682, "latencies_ms": [3885.682], "images_per_second": 0.257, "prompt_tokens": 19, "response_tokens_est": 69, "n_tiles": 12, "output_text": "The image depicts a sunny day with clear blue skies and bright sunlight. The scene is characterized by the vibrant colors of the buildings, the green metal benches, and the red and white signage. The lighting is natural, casting shadows on the ground, and the weather appears to be pleasant with no signs of rain or extreme heat.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25594.8, "ram_available_mb": 100177.4, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25594.8, "ram_available_mb": 100177.4, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.76, "peak": 15.34, "min": 14.12}, "VDD_GPU": {"avg": 32.45, "peak": 42.54, "min": 25.6}, "VIN": {"avg": 73.92, "peak": 117.31, "min": 56.56}, "VDD_CPU_SOC_MSS": {"avg": 15.29, "peak": 16.14, "min": 14.56}}, "power_watts_avg": 32.45, "energy_joules_est": 126.11, "sample_count": 30, "duration_seconds": 3.886}, "timestamp": "2026-01-16T15:42:04.015767"}
{"image_index": 161, "image_name": "000000016439.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016439.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2617.202, "latencies_ms": [2617.202], "images_per_second": 0.382, "prompt_tokens": 9, "response_tokens_est": 36, "n_tiles": 12, "output_text": "The image shows a well-organized desk with a laptop, a glass of orange juice, a phone, and a lamp, all set against a warmly lit background.", "error": null, "sys_before": {"cpu_percent": 38.8, "ram_used_mb": 25594.8, "ram_available_mb": 100177.4, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 25594.8, "ram_available_mb": 100177.4, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.07, "peak": 16.05, "min": 13.59}, "VDD_GPU": {"avg": 34.21, "peak": 40.97, "min": 26.79}, "VIN": {"avg": 69.29, "peak": 93.62, "min": 58.57}, "VDD_CPU_SOC_MSS": {"avg": 15.9, "peak": 17.32, "min": 14.57}}, "power_watts_avg": 34.21, "energy_joules_est": 89.55, "sample_count": 20, "duration_seconds": 2.618}, "timestamp": "2026-01-16T15:42:06.722447"}
{"image_index": 161, "image_name": "000000016439.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016439.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 3185.494, "latencies_ms": [3185.494], "images_per_second": 0.314, "prompt_tokens": 23, "response_tokens_est": 49, "n_tiles": 12, "output_text": "- Laptop: 1\n- Phone: 1\n- Glass: 1\n- Lamp: 1\n- Desk: 1\n- Magazine: 1\n- Book: 1\n- Candle: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25594.8, "ram_available_mb": 100177.4, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 25595.0, "ram_available_mb": 100177.1, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.07, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 34.03, "peak": 42.94, "min": 26.39}, "VIN": {"avg": 73.68, "peak": 102.7, "min": 57.93}, "VDD_CPU_SOC_MSS": {"avg": 15.64, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 34.03, "energy_joules_est": 108.42, "sample_count": 25, "duration_seconds": 3.186}, "timestamp": "2026-01-16T15:42:09.914404"}
{"image_index": 161, "image_name": "000000016439.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016439.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 4061.224, "latencies_ms": [4061.224], "images_per_second": 0.246, "prompt_tokens": 27, "response_tokens_est": 75, "n_tiles": 12, "output_text": "The main objects in the image are a desk, a laptop, a lamp, and a cup of orange juice. The desk is positioned in the foreground, with the laptop and the lamp placed on top of it. The cup of orange juice is placed on the desk, near the lamp. The lamp is positioned in the background, slightly to the right of the desk.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.0, "ram_available_mb": 100177.1, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25594.8, "ram_available_mb": 100177.4, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.09, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 32.28, "peak": 42.92, "min": 26.0}, "VIN": {"avg": 73.03, "peak": 118.12, "min": 55.78}, "VDD_CPU_SOC_MSS": {"avg": 15.64, "peak": 16.53, "min": 15.35}}, "power_watts_avg": 32.28, "energy_joules_est": 131.12, "sample_count": 32, "duration_seconds": 4.062}, "timestamp": "2026-01-16T15:42:13.982232"}
{"image_index": 161, "image_name": "000000016439.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016439.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3992.755, "latencies_ms": [3992.755], "images_per_second": 0.25, "prompt_tokens": 21, "response_tokens_est": 73, "n_tiles": 12, "output_text": "The image depicts a well-organized workspace with a desk featuring a laptop, a phone, and a glass of orange juice. The desk is set against a warmly lit wall, and there is a decorative lamp with a classic design on the desk. The overall scene suggests a calm and productive environment, possibly in a home office or a study room.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25594.8, "ram_available_mb": 100177.4, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25594.8, "ram_available_mb": 100177.4, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.09, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 32.43, "peak": 42.53, "min": 26.39}, "VIN": {"avg": 77.79, "peak": 121.01, "min": 56.99}, "VDD_CPU_SOC_MSS": {"avg": 15.57, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 32.43, "energy_joules_est": 129.51, "sample_count": 31, "duration_seconds": 3.994}, "timestamp": "2026-01-16T15:42:17.985969"}
{"image_index": 161, "image_name": "000000016439.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016439.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3689.118, "latencies_ms": [3689.118], "images_per_second": 0.271, "prompt_tokens": 19, "response_tokens_est": 64, "n_tiles": 12, "output_text": "The image depicts a well-lit, cozy office desk with a warm yellowish light illuminating the scene. The desk features a black laptop, a glass of orange juice, a phone, and various papers and books. The desk is made of dark wood, and the lighting creates a cozy and inviting atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25594.8, "ram_available_mb": 100177.4, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25594.8, "ram_available_mb": 100177.4, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.07, "peak": 15.65, "min": 14.43}, "VDD_GPU": {"avg": 32.82, "peak": 42.54, "min": 26.0}, "VIN": {"avg": 74.32, "peak": 140.15, "min": 58.47}, "VDD_CPU_SOC_MSS": {"avg": 15.56, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 32.82, "energy_joules_est": 121.09, "sample_count": 29, "duration_seconds": 3.689}, "timestamp": "2026-01-16T15:42:21.681353"}
{"image_index": 162, "image_name": "000000016451.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016451.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1006.071, "latencies_ms": [1006.071], "images_per_second": 0.994, "prompt_tokens": 9, "response_tokens_est": 43, "n_tiles": 1, "output_text": "The image depicts a beach scene with various beach gear and equipment laid out on the sand, including a striped beach towel, a blue surfboard, a red bag, and a green and white umbrella.", "error": null, "sys_before": {"cpu_percent": 19.4, "ram_used_mb": 25594.8, "ram_available_mb": 100177.4, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25594.8, "ram_available_mb": 100177.4, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 4845.7, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.22, "peak": 15.62, "min": 14.61}, "VDD_GPU": {"avg": 23.58, "peak": 27.57, "min": 21.27}, "VIN": {"avg": 63.49, "peak": 66.05, "min": 58.34}, "VDD_CPU_SOC_MSS": {"avg": 15.86, "peak": 16.54, "min": 14.96}}, "power_watts_avg": 23.58, "energy_joules_est": 23.74, "sample_count": 7, "duration_seconds": 1.007}, "timestamp": "2026-01-16T15:42:22.714232"}
{"image_index": 162, "image_name": "000000016451.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016451.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2814.617, "latencies_ms": [2814.617], "images_per_second": 0.355, "prompt_tokens": 23, "response_tokens_est": 128, "n_tiles": 1, "output_text": "object: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject:", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25594.8, "ram_available_mb": 100177.4, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25594.8, "ram_available_mb": 100177.4, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 4856.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.64, "peak": 15.72, "min": 15.22}, "VDD_GPU": {"avg": 20.86, "peak": 24.82, "min": 20.09}, "VIN": {"avg": 62.07, "peak": 71.11, "min": 54.53}, "VDD_CPU_SOC_MSS": {"avg": 16.5, "peak": 16.54, "min": 16.14}}, "power_watts_avg": 20.86, "energy_joules_est": 58.72, "sample_count": 22, "duration_seconds": 2.815}, "timestamp": "2026-01-16T15:42:25.535090"}
{"image_index": 162, "image_name": "000000016451.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016451.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1927.959, "latencies_ms": [1927.959], "images_per_second": 0.519, "prompt_tokens": 27, "response_tokens_est": 87, "n_tiles": 1, "output_text": "The main objects in the image are located in the foreground, with the beach towel, surfboards, and bags positioned near the sandy shore. The surfboards are placed on the sand, while the beach towel is spread out on the sand. In the background, there is a beach umbrella and a person standing near the water. The ocean is visible in the distance, and a person can be seen further out on the water.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25594.8, "ram_available_mb": 100177.4, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25594.8, "ram_available_mb": 100177.4, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 4859.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.7, "peak": 15.82, "min": 15.32}, "VDD_GPU": {"avg": 21.04, "peak": 24.43, "min": 20.09}, "VIN": {"avg": 63.49, "peak": 74.21, "min": 56.13}, "VDD_CPU_SOC_MSS": {"avg": 16.46, "peak": 16.54, "min": 16.14}}, "power_watts_avg": 21.04, "energy_joules_est": 40.57, "sample_count": 15, "duration_seconds": 1.928}, "timestamp": "2026-01-16T15:42:27.468682"}
{"image_index": 162, "image_name": "000000016451.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016451.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1696.387, "latencies_ms": [1696.387], "images_per_second": 0.589, "prompt_tokens": 21, "response_tokens_est": 76, "n_tiles": 1, "output_text": "The image depicts a serene beach scene with a clear blue sky overhead. The beach is adorned with a variety of beach gear, including a striped beach towel, a blue surfboard, a red bag, and a green and white beach umbrella. In the background, the ocean is calm with gentle waves, and a few people can be seen enjoying the beach.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25594.8, "ram_available_mb": 100177.4, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25594.8, "ram_available_mb": 100177.4, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 4854.7, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.62, "peak": 15.72, "min": 15.32}, "VDD_GPU": {"avg": 21.19, "peak": 24.42, "min": 20.1}, "VIN": {"avg": 64.06, "peak": 74.07, "min": 60.06}, "VDD_CPU_SOC_MSS": {"avg": 16.48, "peak": 16.54, "min": 16.14}}, "power_watts_avg": 21.19, "energy_joules_est": 35.96, "sample_count": 13, "duration_seconds": 1.697}, "timestamp": "2026-01-16T15:42:29.170886"}
{"image_index": 162, "image_name": "000000016451.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016451.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1525.07, "latencies_ms": [1525.07], "images_per_second": 0.656, "prompt_tokens": 19, "response_tokens_est": 68, "n_tiles": 1, "output_text": "The image depicts a sunny beach scene with clear blue skies and calm ocean waters. The sandy beach is adorned with various beach gear, including a striped towel, a blue surfboard, a red bag, and a green and white umbrella. The lighting is bright and natural, indicating a clear day with minimal cloud cover.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25594.8, "ram_available_mb": 100177.4, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25594.7, "ram_available_mb": 100177.4, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 4853.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.61, "peak": 15.82, "min": 15.22}, "VDD_GPU": {"avg": 21.38, "peak": 24.42, "min": 20.09}, "VIN": {"avg": 65.27, "peak": 70.07, "min": 61.54}, "VDD_CPU_SOC_MSS": {"avg": 16.46, "peak": 16.54, "min": 16.14}}, "power_watts_avg": 21.38, "energy_joules_est": 32.62, "sample_count": 11, "duration_seconds": 1.525}, "timestamp": "2026-01-16T15:42:30.701668"}
{"image_index": 163, "image_name": "000000016502.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016502.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1979.921, "latencies_ms": [1979.921], "images_per_second": 0.505, "prompt_tokens": 9, "response_tokens_est": 17, "n_tiles": 12, "output_text": "A sheep stands on a rocky outcrop with a blue sky in the background.", "error": null, "sys_before": {"cpu_percent": 45.7, "ram_used_mb": 25594.7, "ram_available_mb": 100177.4, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 2.4, "ram_used_mb": 25594.7, "ram_available_mb": 100177.4, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.33, "peak": 16.15, "min": 14.61}, "VDD_GPU": {"avg": 35.95, "peak": 40.18, "min": 27.97}, "VIN": {"avg": 83.19, "peak": 121.89, "min": 57.92}, "VDD_CPU_SOC_MSS": {"avg": 16.24, "peak": 17.32, "min": 15.35}}, "power_watts_avg": 35.95, "energy_joules_est": 71.21, "sample_count": 15, "duration_seconds": 1.981}, "timestamp": "2026-01-16T15:42:32.744188"}
{"image_index": 163, "image_name": "000000016502.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016502.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2879.349, "latencies_ms": [2879.349], "images_per_second": 0.347, "prompt_tokens": 23, "response_tokens_est": 40, "n_tiles": 12, "output_text": "object: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25594.7, "ram_available_mb": 100177.4, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 25594.7, "ram_available_mb": 100177.4, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.08, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 34.99, "peak": 43.73, "min": 26.79}, "VIN": {"avg": 79.18, "peak": 117.54, "min": 58.3}, "VDD_CPU_SOC_MSS": {"avg": 15.73, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 34.99, "energy_joules_est": 100.76, "sample_count": 23, "duration_seconds": 2.88}, "timestamp": "2026-01-16T15:42:35.629758"}
{"image_index": 163, "image_name": "000000016502.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016502.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3923.994, "latencies_ms": [3923.994], "images_per_second": 0.255, "prompt_tokens": 27, "response_tokens_est": 71, "n_tiles": 12, "output_text": "The main objects in the image are a sheep and a rocky landscape. The sheep is positioned on the left side of the image, standing on a small mound of grass. The rocky landscape is in the foreground, with large, dark rocks and patches of green grass. The sheep is near the rocks, and the sky is visible in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25594.7, "ram_available_mb": 100177.4, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25595.0, "ram_available_mb": 100177.2, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.09, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 32.47, "peak": 42.15, "min": 26.01}, "VIN": {"avg": 73.31, "peak": 114.11, "min": 58.16}, "VDD_CPU_SOC_MSS": {"avg": 15.72, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 32.47, "energy_joules_est": 127.42, "sample_count": 30, "duration_seconds": 3.924}, "timestamp": "2026-01-16T15:42:39.559682"}
{"image_index": 163, "image_name": "000000016502.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016502.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3180.812, "latencies_ms": [3180.812], "images_per_second": 0.314, "prompt_tokens": 21, "response_tokens_est": 49, "n_tiles": 12, "output_text": "The image depicts a serene landscape featuring a sheep standing on a rocky outcrop under a clear blue sky. The sheep appears to be grazing on the grassy area, surrounded by large, rugged rocks and patches of greenery.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.0, "ram_available_mb": 100177.2, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 25595.0, "ram_available_mb": 100177.2, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.08, "peak": 15.65, "min": 14.43}, "VDD_GPU": {"avg": 33.96, "peak": 42.15, "min": 26.39}, "VIN": {"avg": 71.74, "peak": 97.08, "min": 58.09}, "VDD_CPU_SOC_MSS": {"avg": 15.78, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 33.96, "energy_joules_est": 108.03, "sample_count": 24, "duration_seconds": 3.181}, "timestamp": "2026-01-16T15:42:42.751965"}
{"image_index": 163, "image_name": "000000016502.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016502.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3417.738, "latencies_ms": [3417.738], "images_per_second": 0.293, "prompt_tokens": 19, "response_tokens_est": 56, "n_tiles": 12, "output_text": "The image features a serene landscape with a clear blue sky adorned with fluffy white clouds. The ground is covered in lush green grass, and the rocky terrain is rugged and textured. The lighting is bright and natural, suggesting a sunny day with minimal cloud cover.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.0, "ram_available_mb": 100177.2, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 25594.7, "ram_available_mb": 100177.4, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.06, "peak": 15.65, "min": 14.43}, "VDD_GPU": {"avg": 33.29, "peak": 42.54, "min": 26.39}, "VIN": {"avg": 76.61, "peak": 129.65, "min": 58.34}, "VDD_CPU_SOC_MSS": {"avg": 15.7, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 33.29, "energy_joules_est": 113.79, "sample_count": 27, "duration_seconds": 3.418}, "timestamp": "2026-01-16T15:42:46.176203"}
{"image_index": 164, "image_name": "000000016598.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016598.jpg", "image_width": 478, "image_height": 640, "image_resolution": "478x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2192.083, "latencies_ms": [2192.083], "images_per_second": 0.456, "prompt_tokens": 9, "response_tokens_est": 23, "n_tiles": 12, "output_text": "The image shows a person with blue hair and a blue shirt, holding a smartphone and taking a selfie.", "error": null, "sys_before": {"cpu_percent": 44.4, "ram_used_mb": 25594.7, "ram_available_mb": 100177.5, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 2.6, "ram_used_mb": 25594.7, "ram_available_mb": 100177.5, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.08, "peak": 16.05, "min": 13.59}, "VDD_GPU": {"avg": 35.85, "peak": 41.76, "min": 28.76}, "VIN": {"avg": 76.38, "peak": 114.47, "min": 58.35}, "VDD_CPU_SOC_MSS": {"avg": 15.79, "peak": 16.92, "min": 14.57}}, "power_watts_avg": 35.85, "energy_joules_est": 78.6, "sample_count": 17, "duration_seconds": 2.193}, "timestamp": "2026-01-16T15:42:48.468570"}
{"image_index": 164, "image_name": "000000016598.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016598.jpg", "image_width": 478, "image_height": 640, "image_resolution": "478x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2679.469, "latencies_ms": [2679.469], "images_per_second": 0.373, "prompt_tokens": 23, "response_tokens_est": 34, "n_tiles": 12, "output_text": "1. Woman\n2. Blue hair\n3. Blue shirt\n4. Phone\n5. Ring\n6. Background\n7. Wall\n8. Light", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25594.7, "ram_available_mb": 100177.5, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 25594.7, "ram_available_mb": 100177.5, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.07, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 35.68, "peak": 42.94, "min": 27.18}, "VIN": {"avg": 73.02, "peak": 104.41, "min": 58.45}, "VDD_CPU_SOC_MSS": {"avg": 15.6, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 35.68, "energy_joules_est": 95.62, "sample_count": 21, "duration_seconds": 2.68}, "timestamp": "2026-01-16T15:42:51.154399"}
{"image_index": 164, "image_name": "000000016598.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016598.jpg", "image_width": 478, "image_height": 640, "image_resolution": "478x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3219.479, "latencies_ms": [3219.479], "images_per_second": 0.311, "prompt_tokens": 27, "response_tokens_est": 50, "n_tiles": 12, "output_text": "The main object in the image is a person holding a smartphone. The person is positioned in the foreground, with the smartphone held up to their face. The background is out of focus, indicating that the focus is on the person and the smartphone.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25594.7, "ram_available_mb": 100177.5, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 25594.7, "ram_available_mb": 100177.5, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.08, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 33.97, "peak": 42.94, "min": 26.39}, "VIN": {"avg": 73.04, "peak": 105.13, "min": 58.74}, "VDD_CPU_SOC_MSS": {"avg": 15.75, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 33.97, "energy_joules_est": 109.38, "sample_count": 25, "duration_seconds": 3.22}, "timestamp": "2026-01-16T15:42:54.380067"}
{"image_index": 164, "image_name": "000000016598.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016598.jpg", "image_width": 478, "image_height": 640, "image_resolution": "478x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2712.586, "latencies_ms": [2712.586], "images_per_second": 0.369, "prompt_tokens": 21, "response_tokens_est": 35, "n_tiles": 12, "output_text": "The image depicts a person with blue hair and a blue shirt standing in front of a light-colored wall. The person is holding a smartphone and taking a selfie.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25594.7, "ram_available_mb": 100177.5, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 25594.7, "ram_available_mb": 100177.5, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.08, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 35.21, "peak": 42.54, "min": 27.18}, "VIN": {"avg": 77.26, "peak": 120.68, "min": 57.71}, "VDD_CPU_SOC_MSS": {"avg": 15.73, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 35.21, "energy_joules_est": 95.52, "sample_count": 21, "duration_seconds": 2.713}, "timestamp": "2026-01-16T15:42:57.100005"}
{"image_index": 164, "image_name": "000000016598.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016598.jpg", "image_width": 478, "image_height": 640, "image_resolution": "478x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 4180.089, "latencies_ms": [4180.089], "images_per_second": 0.239, "prompt_tokens": 19, "response_tokens_est": 78, "n_tiles": 12, "output_text": "The notable visual attributes of the image include the striking blue hair of the person, which stands out against the neutral background. The lighting is soft and diffused, casting a gentle glow on the person's face. The material of the person's clothing appears to be a light blue denim shirt, and the weather seems to be overcast, as the overall lighting is subdued.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 25594.7, "ram_available_mb": 100177.5, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 25594.7, "ram_available_mb": 100177.5, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.01, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 32.08, "peak": 42.94, "min": 25.6}, "VIN": {"avg": 70.4, "peak": 109.6, "min": 56.86}, "VDD_CPU_SOC_MSS": {"avg": 15.65, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 32.08, "energy_joules_est": 134.11, "sample_count": 32, "duration_seconds": 4.181}, "timestamp": "2026-01-16T15:43:01.290477"}
{"image_index": 165, "image_name": "000000016958.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016958.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2026.587, "latencies_ms": [2026.587], "images_per_second": 0.493, "prompt_tokens": 9, "response_tokens_est": 54, "n_tiles": 6, "output_text": "The image depicts a classic, well-appointed room with a large, ornate fireplace, a wooden cabinet, a wooden chair, and a round wooden table, all set against a backdrop of a wall adorned with framed pictures and a decorative vase.", "error": null, "sys_before": {"cpu_percent": 37.3, "ram_used_mb": 25594.7, "ram_available_mb": 100177.5, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 25594.7, "ram_available_mb": 100177.5, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.6, "peak": 15.32, "min": 13.61}, "VDD_GPU": {"avg": 29.6, "peak": 37.82, "min": 24.04}, "VIN": {"avg": 71.61, "peak": 121.21, "min": 58.79}, "VDD_CPU_SOC_MSS": {"avg": 15.28, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 29.6, "energy_joules_est": 60.0, "sample_count": 15, "duration_seconds": 2.027}, "timestamp": "2026-01-16T15:43:03.384119"}
{"image_index": 165, "image_name": "000000016958.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016958.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1688.918, "latencies_ms": [1688.918], "images_per_second": 0.592, "prompt_tokens": 23, "response_tokens_est": 40, "n_tiles": 6, "output_text": "object: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25594.7, "ram_available_mb": 100177.5, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 25595.7, "ram_available_mb": 100176.5, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.87, "peak": 15.22, "min": 14.32}, "VDD_GPU": {"avg": 31.64, "peak": 38.21, "min": 25.21}, "VIN": {"avg": 74.67, "peak": 109.56, "min": 62.28}, "VDD_CPU_SOC_MSS": {"avg": 15.55, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 31.64, "energy_joules_est": 53.45, "sample_count": 12, "duration_seconds": 1.689}, "timestamp": "2026-01-16T15:43:05.078964"}
{"image_index": 165, "image_name": "000000016958.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016958.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2864.338, "latencies_ms": [2864.338], "images_per_second": 0.349, "prompt_tokens": 27, "response_tokens_est": 80, "n_tiles": 6, "output_text": "The main objects in the image are a fireplace, a wooden cabinet, a chair, and a small round table. The fireplace is positioned in the foreground, with a chair and a small round table placed near it. The wooden cabinet is located to the right of the fireplace, and the chair is positioned to the right of the cabinet. The small round table is placed in front of the cabinet.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.7, "ram_available_mb": 100176.5, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 25595.4, "ram_available_mb": 100176.7, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.82, "peak": 15.01, "min": 14.12}, "VDD_GPU": {"avg": 27.93, "peak": 38.6, "min": 23.25}, "VIN": {"avg": 64.68, "peak": 76.89, "min": 52.39}, "VDD_CPU_SOC_MSS": {"avg": 15.5, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 27.93, "energy_joules_est": 80.01, "sample_count": 22, "duration_seconds": 2.865}, "timestamp": "2026-01-16T15:43:07.949541"}
{"image_index": 165, "image_name": "000000016958.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016958.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2189.374, "latencies_ms": [2189.374], "images_per_second": 0.457, "prompt_tokens": 21, "response_tokens_est": 56, "n_tiles": 6, "output_text": "The image depicts an old-fashioned living room with a classic fireplace, a wooden cabinet, and a patterned carpet. The room is furnished with a wooden chair, a small wooden table, and a dark leather chair, suggesting a setting that is both elegant and cozy.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.4, "ram_available_mb": 100176.7, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 25595.7, "ram_available_mb": 100176.5, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.73, "peak": 15.01, "min": 14.02}, "VDD_GPU": {"avg": 29.1, "peak": 38.21, "min": 23.25}, "VIN": {"avg": 72.07, "peak": 109.7, "min": 54.78}, "VDD_CPU_SOC_MSS": {"avg": 15.24, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 29.1, "energy_joules_est": 63.72, "sample_count": 17, "duration_seconds": 2.19}, "timestamp": "2026-01-16T15:43:10.145036"}
{"image_index": 165, "image_name": "000000016958.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000016958.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2357.095, "latencies_ms": [2357.095], "images_per_second": 0.424, "prompt_tokens": 19, "response_tokens_est": 62, "n_tiles": 6, "output_text": "The room features a classic, well-kept fireplace with a brick surround and a black mantel. The walls are painted white, and there is a wooden cabinet with glass doors. The room is well-lit with soft, ambient lighting, and the furniture is made of dark wood with leather accents.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.7, "ram_available_mb": 100176.5, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 25595.4, "ram_available_mb": 100176.8, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.74, "peak": 15.01, "min": 14.12}, "VDD_GPU": {"avg": 28.73, "peak": 38.21, "min": 23.24}, "VIN": {"avg": 69.14, "peak": 105.85, "min": 53.61}, "VDD_CPU_SOC_MSS": {"avg": 15.33, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 28.73, "energy_joules_est": 67.73, "sample_count": 18, "duration_seconds": 2.358}, "timestamp": "2026-01-16T15:43:12.508294"}
{"image_index": 166, "image_name": "000000017029.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017029.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1460.377, "latencies_ms": [1460.377], "images_per_second": 0.685, "prompt_tokens": 9, "response_tokens_est": 44, "n_tiles": 4, "output_text": "A brown dog is energetically jumping in the air, attempting to catch a red frisbee with its mouth wide open, while a black car is parked in the background on a well-maintained lawn.", "error": null, "sys_before": {"cpu_percent": 30.8, "ram_used_mb": 25595.4, "ram_available_mb": 100176.8, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 25595.4, "ram_available_mb": 100176.8, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4606.7, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5444.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.47, "peak": 15.11, "min": 13.61}, "VDD_GPU": {"avg": 27.65, "peak": 33.88, "min": 23.25}, "VIN": {"avg": 66.97, "peak": 83.2, "min": 59.28}, "VDD_CPU_SOC_MSS": {"avg": 15.18, "peak": 15.75, "min": 14.17}}, "power_watts_avg": 27.65, "energy_joules_est": 40.39, "sample_count": 11, "duration_seconds": 1.461}, "timestamp": "2026-01-16T15:43:14.019560"}
{"image_index": 166, "image_name": "000000017029.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017029.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1341.328, "latencies_ms": [1341.328], "images_per_second": 0.746, "prompt_tokens": 23, "response_tokens_est": 39, "n_tiles": 4, "output_text": "dog: 1\nfrisbee: 1\ncar: 1\ntree: 1\ngrass: 1\nbushes: 1\nhouse: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.4, "ram_available_mb": 100176.8, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 25595.7, "ram_available_mb": 100176.5, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4606.7, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5455.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.68, "peak": 15.11, "min": 14.12}, "VDD_GPU": {"avg": 28.95, "peak": 35.45, "min": 23.64}, "VIN": {"avg": 70.04, "peak": 102.44, "min": 57.19}, "VDD_CPU_SOC_MSS": {"avg": 15.35, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 28.95, "energy_joules_est": 38.84, "sample_count": 10, "duration_seconds": 1.342}, "timestamp": "2026-01-16T15:43:15.366637"}
{"image_index": 166, "image_name": "000000017029.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017029.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3034.09, "latencies_ms": [3034.09], "images_per_second": 0.33, "prompt_tokens": 27, "response_tokens_est": 107, "n_tiles": 4, "output_text": "In the image, the main object is a dog in the foreground, positioned near the center of the frame. The dog is facing to the right, with its body slightly turned towards the camera. In the background, there is a black car parked on the left side of the image, slightly behind the dog. The car is positioned near the edge of the frame, with its headlights visible. The dog is surrounded by a well-maintained lawn, and there is a hedge and a tree to the left of the dog.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.7, "ram_available_mb": 100176.5, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 25595.4, "ram_available_mb": 100176.8, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4606.7, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5458.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.02, "peak": 15.32, "min": 14.02}, "VDD_GPU": {"avg": 25.27, "peak": 35.06, "min": 22.45}, "VIN": {"avg": 66.34, "peak": 110.25, "min": 59.73}, "VDD_CPU_SOC_MSS": {"avg": 15.66, "peak": 16.14, "min": 14.57}}, "power_watts_avg": 25.27, "energy_joules_est": 76.68, "sample_count": 23, "duration_seconds": 3.034}, "timestamp": "2026-01-16T15:43:18.406648"}
{"image_index": 166, "image_name": "000000017029.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017029.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1839.69, "latencies_ms": [1839.69], "images_per_second": 0.544, "prompt_tokens": 21, "response_tokens_est": 59, "n_tiles": 4, "output_text": "The image captures a lively scene in a well-maintained backyard, featuring a brown dog mid-leap, seemingly in the midst of a playful game of fetch with a red frisbee. The lush green lawn and the presence of a black car in the background suggest a suburban setting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.4, "ram_available_mb": 100176.8, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 25595.4, "ram_available_mb": 100176.8, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4606.7, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5453.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.83, "peak": 15.32, "min": 14.12}, "VDD_GPU": {"avg": 26.87, "peak": 34.66, "min": 22.45}, "VIN": {"avg": 65.93, "peak": 94.18, "min": 55.34}, "VDD_CPU_SOC_MSS": {"avg": 15.49, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 26.87, "energy_joules_est": 49.44, "sample_count": 14, "duration_seconds": 1.84}, "timestamp": "2026-01-16T15:43:20.253137"}
{"image_index": 166, "image_name": "000000017029.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017029.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1782.763, "latencies_ms": [1782.763], "images_per_second": 0.561, "prompt_tokens": 19, "response_tokens_est": 57, "n_tiles": 4, "output_text": "The image features a dog with a dark brown coat, wearing a pink collar, mid-jump in a lush green yard. The dog's fur appears fluffy and well-groomed, and the background includes a black car and a brick wall, with a clear sky overhead.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.4, "ram_available_mb": 100176.8, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 25595.4, "ram_available_mb": 100176.8, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4606.7, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5451.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.84, "peak": 15.22, "min": 14.12}, "VDD_GPU": {"avg": 27.3, "peak": 35.06, "min": 22.85}, "VIN": {"avg": 63.25, "peak": 68.2, "min": 47.78}, "VDD_CPU_SOC_MSS": {"avg": 15.5, "peak": 16.14, "min": 14.96}}, "power_watts_avg": 27.3, "energy_joules_est": 48.68, "sample_count": 13, "duration_seconds": 1.783}, "timestamp": "2026-01-16T15:43:22.041587"}
{"image_index": 167, "image_name": "000000017031.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017031.jpg", "image_width": 500, "image_height": 334, "image_resolution": "500x334", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1614.16, "latencies_ms": [1614.16], "images_per_second": 0.62, "prompt_tokens": 9, "response_tokens_est": 37, "n_tiles": 6, "output_text": "The image features a close-up of a giraffe's face, with its distinctive brown and white spotted pattern, and its large, expressive eyes gazing directly at the viewer.", "error": null, "sys_before": {"cpu_percent": 38.0, "ram_used_mb": 25595.4, "ram_available_mb": 100176.8, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 25595.4, "ram_available_mb": 100176.8, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.43, "peak": 14.91, "min": 13.82}, "VDD_GPU": {"avg": 30.33, "peak": 37.03, "min": 24.42}, "VIN": {"avg": 72.13, "peak": 127.11, "min": 58.85}, "VDD_CPU_SOC_MSS": {"avg": 15.19, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 30.33, "energy_joules_est": 48.97, "sample_count": 12, "duration_seconds": 1.615}, "timestamp": "2026-01-16T15:43:23.711602"}
{"image_index": 167, "image_name": "000000017031.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017031.jpg", "image_width": 500, "image_height": 334, "image_resolution": "500x334", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 958.015, "latencies_ms": [958.015], "images_per_second": 1.044, "prompt_tokens": 23, "response_tokens_est": 12, "n_tiles": 6, "output_text": "giraffe: 1\ntree: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.4, "ram_available_mb": 100176.8, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 25595.4, "ram_available_mb": 100176.8, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.45, "peak": 14.63, "min": 14.02}, "VDD_GPU": {"avg": 35.11, "peak": 38.21, "min": 31.12}, "VIN": {"avg": 76.34, "peak": 111.37, "min": 61.16}, "VDD_CPU_SOC_MSS": {"avg": 15.07, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 35.11, "energy_joules_est": 33.65, "sample_count": 7, "duration_seconds": 0.958}, "timestamp": "2026-01-16T15:43:24.675633"}
{"image_index": 167, "image_name": "000000017031.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017031.jpg", "image_width": 500, "image_height": 334, "image_resolution": "500x334", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2861.25, "latencies_ms": [2861.25], "images_per_second": 0.349, "prompt_tokens": 27, "response_tokens_est": 80, "n_tiles": 6, "output_text": "The main object in the foreground is a giraffe with a brown and white spotted coat. The giraffe is positioned near the center of the image, slightly off-center to the left. In the background, there are blurred greenery, indicating the presence of trees or bushes. The giraffe is not directly near any other objects, but its head is slightly off-center to the left.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.4, "ram_available_mb": 100176.8, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 25595.2, "ram_available_mb": 100177.0, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.73, "peak": 15.01, "min": 13.92}, "VDD_GPU": {"avg": 28.6, "peak": 39.77, "min": 23.24}, "VIN": {"avg": 68.09, "peak": 96.28, "min": 55.5}, "VDD_CPU_SOC_MSS": {"avg": 15.18, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 28.6, "energy_joules_est": 81.85, "sample_count": 22, "duration_seconds": 2.862}, "timestamp": "2026-01-16T15:43:27.544434"}
{"image_index": 167, "image_name": "000000017031.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017031.jpg", "image_width": 500, "image_height": 334, "image_resolution": "500x334", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2495.893, "latencies_ms": [2495.893], "images_per_second": 0.401, "prompt_tokens": 21, "response_tokens_est": 67, "n_tiles": 6, "output_text": "The image captures a giraffe standing in a lush, green environment, likely a savanna or a similar habitat. The giraffe is facing the camera, with its head slightly tilted, and its long neck and legs are prominently visible. The background is filled with various shades of green, indicating a dense, natural setting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.2, "ram_available_mb": 100177.0, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 25595.4, "ram_available_mb": 100176.8, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.75, "peak": 15.01, "min": 14.02}, "VDD_GPU": {"avg": 28.36, "peak": 37.82, "min": 23.24}, "VIN": {"avg": 65.74, "peak": 93.57, "min": 55.15}, "VDD_CPU_SOC_MSS": {"avg": 15.33, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 28.36, "energy_joules_est": 70.8, "sample_count": 19, "duration_seconds": 2.496}, "timestamp": "2026-01-16T15:43:30.046327"}
{"image_index": 167, "image_name": "000000017031.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017031.jpg", "image_width": 500, "image_height": 334, "image_resolution": "500x334", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1459.3, "latencies_ms": [1459.3], "images_per_second": 0.685, "prompt_tokens": 19, "response_tokens_est": 30, "n_tiles": 6, "output_text": "The giraffe in the image has a brown and white coat with distinctive black spots. The lighting is bright and natural, suggesting it is daytime.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.4, "ram_available_mb": 100176.8, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25595.1, "ram_available_mb": 100177.0, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.59, "peak": 14.91, "min": 14.02}, "VDD_GPU": {"avg": 31.9, "peak": 38.19, "min": 25.21}, "VIN": {"avg": 73.55, "peak": 117.99, "min": 55.87}, "VDD_CPU_SOC_MSS": {"avg": 15.21, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 31.9, "energy_joules_est": 46.57, "sample_count": 11, "duration_seconds": 1.46}, "timestamp": "2026-01-16T15:43:31.511765"}
{"image_index": 168, "image_name": "000000017115.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017115.jpg", "image_width": 443, "image_height": 640, "image_resolution": "443x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1394.076, "latencies_ms": [1394.076], "images_per_second": 0.717, "prompt_tokens": 9, "response_tokens_est": 30, "n_tiles": 6, "output_text": "Two zebras stand in a pen, their black and white stripes clearly visible against the backdrop of a dirt ground and a chain-link fence.", "error": null, "sys_before": {"cpu_percent": 36.0, "ram_used_mb": 25595.1, "ram_available_mb": 100177.0, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25595.4, "ram_available_mb": 100176.8, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.29, "peak": 14.91, "min": 13.72}, "VDD_GPU": {"avg": 32.11, "peak": 37.03, "min": 26.39}, "VIN": {"avg": 72.87, "peak": 117.94, "min": 60.29}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 32.11, "energy_joules_est": 44.78, "sample_count": 10, "duration_seconds": 1.395}, "timestamp": "2026-01-16T15:43:32.976005"}
{"image_index": 168, "image_name": "000000017115.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017115.jpg", "image_width": 443, "image_height": 640, "image_resolution": "443x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1099.3, "latencies_ms": [1099.3], "images_per_second": 0.91, "prompt_tokens": 23, "response_tokens_est": 17, "n_tiles": 6, "output_text": "zebra: 2\nfence: 1\nrock: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.4, "ram_available_mb": 100176.8, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 25595.1, "ram_available_mb": 100177.0, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.47, "peak": 14.63, "min": 14.02}, "VDD_GPU": {"avg": 34.76, "peak": 39.0, "min": 28.76}, "VIN": {"avg": 71.86, "peak": 94.49, "min": 55.63}, "VDD_CPU_SOC_MSS": {"avg": 15.11, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 34.76, "energy_joules_est": 38.23, "sample_count": 8, "duration_seconds": 1.1}, "timestamp": "2026-01-16T15:43:34.081271"}
{"image_index": 168, "image_name": "000000017115.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017115.jpg", "image_width": 443, "image_height": 640, "image_resolution": "443x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2547.142, "latencies_ms": [2547.142], "images_per_second": 0.393, "prompt_tokens": 27, "response_tokens_est": 69, "n_tiles": 6, "output_text": "The main objects in the image are two zebras standing in a zoo enclosure. The foreground features the backs of the zebras, with their distinctive black and white stripes. The background shows a chain-link fence, indicating the enclosure's boundary. The zebras are positioned near the fence, with their heads turned slightly towards it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.1, "ram_available_mb": 100177.0, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 25595.1, "ram_available_mb": 100177.0, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.75, "peak": 15.11, "min": 14.02}, "VDD_GPU": {"avg": 29.09, "peak": 39.39, "min": 23.24}, "VIN": {"avg": 67.72, "peak": 86.09, "min": 60.57}, "VDD_CPU_SOC_MSS": {"avg": 15.23, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 29.09, "energy_joules_est": 74.11, "sample_count": 19, "duration_seconds": 2.547}, "timestamp": "2026-01-16T15:43:36.635476"}
{"image_index": 168, "image_name": "000000017115.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017115.jpg", "image_width": 443, "image_height": 640, "image_resolution": "443x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2555.155, "latencies_ms": [2555.155], "images_per_second": 0.391, "prompt_tokens": 21, "response_tokens_est": 69, "n_tiles": 6, "output_text": "The image depicts a scene in a zoo or a wildlife sanctuary, where two zebras are standing in a fenced enclosure. The zebras are facing the camera, and their distinctive black and white stripes are clearly visible. The ground is covered with dirt and scattered rocks, and the enclosure is surrounded by a chain-link fence.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.1, "ram_available_mb": 100177.0, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 25595.1, "ram_available_mb": 100177.0, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.74, "peak": 15.01, "min": 14.02}, "VDD_GPU": {"avg": 28.28, "peak": 37.82, "min": 23.24}, "VIN": {"avg": 66.68, "peak": 75.22, "min": 62.33}, "VDD_CPU_SOC_MSS": {"avg": 15.23, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 28.28, "energy_joules_est": 72.27, "sample_count": 19, "duration_seconds": 2.556}, "timestamp": "2026-01-16T15:43:39.196873"}
{"image_index": 168, "image_name": "000000017115.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017115.jpg", "image_width": 443, "image_height": 640, "image_resolution": "443x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2160.961, "latencies_ms": [2160.961], "images_per_second": 0.463, "prompt_tokens": 19, "response_tokens_est": 55, "n_tiles": 6, "output_text": "The zebras in the image have black and white stripes, which are a notable visual attribute. The lighting is bright and natural, suggesting it is daytime. The ground is covered with dry leaves and patches of grass, indicating a dry season or a lack of rainfall.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.1, "ram_available_mb": 100177.0, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 25595.1, "ram_available_mb": 100177.1, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.72, "peak": 15.01, "min": 14.02}, "VDD_GPU": {"avg": 29.25, "peak": 37.82, "min": 23.25}, "VIN": {"avg": 70.36, "peak": 116.39, "min": 55.28}, "VDD_CPU_SOC_MSS": {"avg": 15.21, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 29.25, "energy_joules_est": 63.22, "sample_count": 16, "duration_seconds": 2.161}, "timestamp": "2026-01-16T15:43:41.363944"}
{"image_index": 169, "image_name": "000000017178.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017178.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1018.155, "latencies_ms": [1018.155], "images_per_second": 0.982, "prompt_tokens": 9, "response_tokens_est": 16, "n_tiles": 6, "output_text": "A group of horses stands in a field, with a car parked nearby.", "error": null, "sys_before": {"cpu_percent": 40.9, "ram_used_mb": 25595.1, "ram_available_mb": 100177.1, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 25595.1, "ram_available_mb": 100177.1, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.28, "peak": 14.51, "min": 13.92}, "VDD_GPU": {"avg": 33.71, "peak": 37.42, "min": 29.54}, "VIN": {"avg": 77.95, "peak": 96.48, "min": 60.84}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 33.71, "energy_joules_est": 34.33, "sample_count": 7, "duration_seconds": 1.018}, "timestamp": "2026-01-16T15:43:42.432241"}
{"image_index": 169, "image_name": "000000017178.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017178.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1481.106, "latencies_ms": [1481.106], "images_per_second": 0.675, "prompt_tokens": 23, "response_tokens_est": 32, "n_tiles": 6, "output_text": "1. Horse\n2. Horse\n3. Horse\n4. Horse\n5. Horse\n6. Horse\n7. Horse\n8. Car", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.1, "ram_available_mb": 100177.1, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 25595.1, "ram_available_mb": 100177.1, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.68, "peak": 15.11, "min": 14.22}, "VDD_GPU": {"avg": 33.05, "peak": 39.0, "min": 26.0}, "VIN": {"avg": 72.49, "peak": 98.86, "min": 59.88}, "VDD_CPU_SOC_MSS": {"avg": 15.25, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 33.05, "energy_joules_est": 48.96, "sample_count": 11, "duration_seconds": 1.481}, "timestamp": "2026-01-16T15:43:43.919237"}
{"image_index": 169, "image_name": "000000017178.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017178.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2051.247, "latencies_ms": [2051.247], "images_per_second": 0.488, "prompt_tokens": 27, "response_tokens_est": 51, "n_tiles": 6, "output_text": "The main objects in the image are a wooden fence, a road, and a car. The wooden fence is on the left side of the image, the road is in the foreground, and the car is on the right side, near the fence.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.1, "ram_available_mb": 100177.1, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 25595.4, "ram_available_mb": 100176.8, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.71, "peak": 15.01, "min": 14.12}, "VDD_GPU": {"avg": 30.25, "peak": 38.59, "min": 23.64}, "VIN": {"avg": 67.76, "peak": 107.01, "min": 60.52}, "VDD_CPU_SOC_MSS": {"avg": 15.22, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 30.25, "energy_joules_est": 62.07, "sample_count": 15, "duration_seconds": 2.052}, "timestamp": "2026-01-16T15:43:45.977490"}
{"image_index": 169, "image_name": "000000017178.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017178.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2412.445, "latencies_ms": [2412.445], "images_per_second": 0.415, "prompt_tokens": 21, "response_tokens_est": 64, "n_tiles": 6, "output_text": "The image depicts a serene rural scene with a dirt road flanked by a wooden fence on one side and lush green trees on the other. A dark-colored car is parked on the right side of the road, and a group of horses is standing in the middle of the road, appearing calm and peaceful.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.4, "ram_available_mb": 100176.8, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 25595.1, "ram_available_mb": 100177.1, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.72, "peak": 15.01, "min": 14.02}, "VDD_GPU": {"avg": 28.8, "peak": 37.82, "min": 23.24}, "VIN": {"avg": 69.2, "peak": 122.7, "min": 56.15}, "VDD_CPU_SOC_MSS": {"avg": 15.27, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 28.8, "energy_joules_est": 69.49, "sample_count": 18, "duration_seconds": 2.413}, "timestamp": "2026-01-16T15:43:48.396154"}
{"image_index": 169, "image_name": "000000017178.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017178.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1710.32, "latencies_ms": [1710.32], "images_per_second": 0.585, "prompt_tokens": 19, "response_tokens_est": 39, "n_tiles": 6, "output_text": "The image features a dark-colored car parked on the side of a paved road. The road is surrounded by a wooden fence and lush green trees, indicating a sunny day with clear skies.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.1, "ram_available_mb": 100177.1, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 25595.1, "ram_available_mb": 100177.1, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.62, "peak": 14.91, "min": 14.02}, "VDD_GPU": {"avg": 30.94, "peak": 37.82, "min": 24.42}, "VIN": {"avg": 69.18, "peak": 112.2, "min": 50.76}, "VDD_CPU_SOC_MSS": {"avg": 15.17, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 30.94, "energy_joules_est": 52.93, "sample_count": 13, "duration_seconds": 1.711}, "timestamp": "2026-01-16T15:43:50.112526"}
{"image_index": 170, "image_name": "000000017182.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017182.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1756.627, "latencies_ms": [1756.627], "images_per_second": 0.569, "prompt_tokens": 9, "response_tokens_est": 42, "n_tiles": 6, "output_text": "The image shows a wooden desk with a stack of hardcover books on top, a small green bottle, and a red apple on the desk, with a framed picture hanging on the wall to the right.", "error": null, "sys_before": {"cpu_percent": 42.9, "ram_used_mb": 25595.1, "ram_available_mb": 100177.1, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 25595.1, "ram_available_mb": 100177.1, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.43, "peak": 14.91, "min": 13.82}, "VDD_GPU": {"avg": 30.03, "peak": 37.03, "min": 24.03}, "VIN": {"avg": 67.41, "peak": 117.75, "min": 51.13}, "VDD_CPU_SOC_MSS": {"avg": 15.11, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 30.03, "energy_joules_est": 52.77, "sample_count": 13, "duration_seconds": 1.757}, "timestamp": "2026-01-16T15:43:51.929384"}
{"image_index": 170, "image_name": "000000017182.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017182.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1376.131, "latencies_ms": [1376.131], "images_per_second": 0.727, "prompt_tokens": 23, "response_tokens_est": 27, "n_tiles": 6, "output_text": "bookcase: 1\nbook: 1\napple: 1\nvase: 1\nchair: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.1, "ram_available_mb": 100177.1, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 25595.1, "ram_available_mb": 100177.1, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.54, "peak": 14.81, "min": 14.02}, "VDD_GPU": {"avg": 32.65, "peak": 38.19, "min": 26.0}, "VIN": {"avg": 68.6, "peak": 104.09, "min": 58.61}, "VDD_CPU_SOC_MSS": {"avg": 15.12, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 32.65, "energy_joules_est": 44.95, "sample_count": 10, "duration_seconds": 1.377}, "timestamp": "2026-01-16T15:43:53.311646"}
{"image_index": 170, "image_name": "000000017182.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017182.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2691.129, "latencies_ms": [2691.129], "images_per_second": 0.372, "prompt_tokens": 27, "response_tokens_est": 74, "n_tiles": 6, "output_text": "The main objects in the image are a wooden desk and a chair. The desk is positioned in the foreground, with a stack of books placed on top of it. The chair is located to the right of the desk, partially visible in the image. The background features a chalkboard, a framed picture on the wall, and a doorway leading to another room.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.1, "ram_available_mb": 100177.1, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 25595.3, "ram_available_mb": 100176.8, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.75, "peak": 15.01, "min": 14.02}, "VDD_GPU": {"avg": 28.46, "peak": 38.6, "min": 23.24}, "VIN": {"avg": 68.12, "peak": 118.69, "min": 58.47}, "VDD_CPU_SOC_MSS": {"avg": 15.28, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 28.46, "energy_joules_est": 76.6, "sample_count": 20, "duration_seconds": 2.692}, "timestamp": "2026-01-16T15:43:56.009479"}
{"image_index": 170, "image_name": "000000017182.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017182.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2492.844, "latencies_ms": [2492.844], "images_per_second": 0.401, "prompt_tokens": 21, "response_tokens_est": 67, "n_tiles": 6, "output_text": "The image depicts a cozy, vintage library setting. The room is filled with wooden bookshelves, a wooden desk, and a chair, creating a warm and inviting atmosphere. The books on the desk and shelves suggest a rich collection of literature, while the overall ambiance conveys a sense of tranquility and intellectual pursuit.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.3, "ram_available_mb": 100176.8, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 25595.1, "ram_available_mb": 100177.1, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.74, "peak": 15.01, "min": 14.02}, "VDD_GPU": {"avg": 28.59, "peak": 38.21, "min": 23.24}, "VIN": {"avg": 69.2, "peak": 108.75, "min": 58.13}, "VDD_CPU_SOC_MSS": {"avg": 15.21, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 28.59, "energy_joules_est": 71.28, "sample_count": 19, "duration_seconds": 2.493}, "timestamp": "2026-01-16T15:43:58.512493"}
{"image_index": 170, "image_name": "000000017182.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017182.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2237.595, "latencies_ms": [2237.595], "images_per_second": 0.447, "prompt_tokens": 19, "response_tokens_est": 58, "n_tiles": 6, "output_text": "The image depicts a room with a dark, textured wall and a wooden desk. The desk has a dark wooden finish and features a stack of blue books on top. The room is well-lit, with natural light coming from a window, creating a warm and inviting atmosphere.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25595.1, "ram_available_mb": 100177.1, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 25595.1, "ram_available_mb": 100177.1, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.7, "peak": 15.01, "min": 14.02}, "VDD_GPU": {"avg": 29.1, "peak": 37.82, "min": 23.25}, "VIN": {"avg": 70.48, "peak": 120.94, "min": 59.6}, "VDD_CPU_SOC_MSS": {"avg": 15.19, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 29.1, "energy_joules_est": 65.13, "sample_count": 17, "duration_seconds": 2.238}, "timestamp": "2026-01-16T15:44:00.756225"}
{"image_index": 171, "image_name": "000000017207.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017207.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2588.718, "latencies_ms": [2588.718], "images_per_second": 0.386, "prompt_tokens": 9, "response_tokens_est": 35, "n_tiles": 12, "output_text": "A yellow and white bus with the number 475 is seen in motion on a city street, with a white vehicle in the background and a person riding a motorcycle nearby.", "error": null, "sys_before": {"cpu_percent": 45.1, "ram_used_mb": 25595.1, "ram_available_mb": 100177.1, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 25595.1, "ram_available_mb": 100177.1, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.1, "peak": 16.05, "min": 13.69}, "VDD_GPU": {"avg": 34.25, "peak": 40.97, "min": 27.18}, "VIN": {"avg": 73.41, "peak": 100.18, "min": 58.13}, "VDD_CPU_SOC_MSS": {"avg": 15.71, "peak": 16.92, "min": 14.57}}, "power_watts_avg": 34.25, "energy_joules_est": 88.68, "sample_count": 20, "duration_seconds": 2.589}, "timestamp": "2026-01-16T15:44:03.438060"}
{"image_index": 171, "image_name": "000000017207.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017207.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 3547.356, "latencies_ms": [3547.356], "images_per_second": 0.282, "prompt_tokens": 23, "response_tokens_est": 60, "n_tiles": 12, "output_text": "1. Bus: 1\n2. Motorcycle: 1\n3. Car: 1\n4. Street: 1\n5. Road: 1\n6. Pedestrian: 1\n7. Building: 1\n8. Street sign: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.1, "ram_available_mb": 100177.1, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25595.1, "ram_available_mb": 100177.1, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.08, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 33.14, "peak": 42.94, "min": 26.39}, "VIN": {"avg": 74.02, "peak": 117.93, "min": 58.23}, "VDD_CPU_SOC_MSS": {"avg": 15.56, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 33.14, "energy_joules_est": 117.58, "sample_count": 28, "duration_seconds": 3.548}, "timestamp": "2026-01-16T15:44:06.992938"}
{"image_index": 171, "image_name": "000000017207.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017207.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3551.388, "latencies_ms": [3551.388], "images_per_second": 0.282, "prompt_tokens": 27, "response_tokens_est": 60, "n_tiles": 12, "output_text": "The image shows a yellow bus with the number \"475\" and some text in Hindi on its side. The bus is in motion, as indicated by the blurred background. The bus is positioned near the center of the frame, with the background slightly blurred, suggesting it is moving quickly.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.1, "ram_available_mb": 100177.1, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25595.1, "ram_available_mb": 100177.1, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.09, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 33.17, "peak": 42.53, "min": 26.39}, "VIN": {"avg": 71.34, "peak": 114.51, "min": 55.5}, "VDD_CPU_SOC_MSS": {"avg": 15.69, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 33.17, "energy_joules_est": 117.82, "sample_count": 28, "duration_seconds": 3.552}, "timestamp": "2026-01-16T15:44:10.550647"}
{"image_index": 171, "image_name": "000000017207.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017207.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3048.25, "latencies_ms": [3048.25], "images_per_second": 0.328, "prompt_tokens": 21, "response_tokens_est": 45, "n_tiles": 12, "output_text": "The image depicts a yellow and white bus in motion on a city street, with a whiteboard on the side displaying text in Hindi. The bus is moving along a road with other vehicles and buildings in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.1, "ram_available_mb": 100177.1, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 25595.1, "ram_available_mb": 100177.1, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.07, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 34.41, "peak": 42.53, "min": 26.39}, "VIN": {"avg": 78.32, "peak": 134.61, "min": 57.71}, "VDD_CPU_SOC_MSS": {"avg": 15.66, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 34.41, "energy_joules_est": 104.91, "sample_count": 23, "duration_seconds": 3.049}, "timestamp": "2026-01-16T15:44:13.605424"}
{"image_index": 171, "image_name": "000000017207.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017207.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2943.442, "latencies_ms": [2943.442], "images_per_second": 0.34, "prompt_tokens": 19, "response_tokens_est": 42, "n_tiles": 12, "output_text": "The image features a yellow and white bus with graffiti on its side, indicating it is likely from a region where such languages are prevalent. The bus is parked on a wet road, suggesting recent rain.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.1, "ram_available_mb": 100177.1, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 25595.1, "ram_available_mb": 100177.1, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.06, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 34.68, "peak": 42.94, "min": 26.79}, "VIN": {"avg": 78.79, "peak": 126.42, "min": 55.4}, "VDD_CPU_SOC_MSS": {"avg": 15.61, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 34.68, "energy_joules_est": 102.09, "sample_count": 23, "duration_seconds": 2.944}, "timestamp": "2026-01-16T15:44:16.555294"}
{"image_index": 172, "image_name": "000000017379.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017379.jpg", "image_width": 478, "image_height": 640, "image_resolution": "478x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2255.586, "latencies_ms": [2255.586], "images_per_second": 0.443, "prompt_tokens": 9, "response_tokens_est": 25, "n_tiles": 12, "output_text": "The image shows a bathroom with a sink, a mirror, and a toilet, all set against a tiled wall.", "error": null, "sys_before": {"cpu_percent": 42.5, "ram_used_mb": 25595.1, "ram_available_mb": 100177.1, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 25595.1, "ram_available_mb": 100177.1, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.09, "peak": 16.15, "min": 13.69}, "VDD_GPU": {"avg": 35.68, "peak": 41.76, "min": 28.36}, "VIN": {"avg": 75.7, "peak": 106.61, "min": 59.13}, "VDD_CPU_SOC_MSS": {"avg": 15.79, "peak": 17.32, "min": 14.57}}, "power_watts_avg": 35.68, "energy_joules_est": 80.49, "sample_count": 17, "duration_seconds": 2.256}, "timestamp": "2026-01-16T15:44:18.906183"}
{"image_index": 172, "image_name": "000000017379.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017379.jpg", "image_width": 478, "image_height": 640, "image_resolution": "478x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 3382.121, "latencies_ms": [3382.121], "images_per_second": 0.296, "prompt_tokens": 23, "response_tokens_est": 55, "n_tiles": 12, "output_text": "- TV: 1\n- TV stand: 1\n- TV screen: 1\n- Mirror: 1\n- Sink: 1\n- Faucet: 1\n- Trash can: 1\n- Wall tiles: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.1, "ram_available_mb": 100177.1, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25595.1, "ram_available_mb": 100177.1, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.09, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 33.77, "peak": 43.33, "min": 26.39}, "VIN": {"avg": 71.72, "peak": 114.88, "min": 49.41}, "VDD_CPU_SOC_MSS": {"avg": 15.72, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 33.77, "energy_joules_est": 114.23, "sample_count": 26, "duration_seconds": 3.383}, "timestamp": "2026-01-16T15:44:22.296170"}
{"image_index": 172, "image_name": "000000017379.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017379.jpg", "image_width": 478, "image_height": 640, "image_resolution": "478x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3721.868, "latencies_ms": [3721.868], "images_per_second": 0.269, "prompt_tokens": 27, "response_tokens_est": 65, "n_tiles": 12, "output_text": "The main objects in the image are a bathroom sink, a toilet paper holder, and a mirror. The sink is located in the foreground, with the toilet paper holder and mirror positioned near it. The toilet paper holder is situated to the right of the sink, and the mirror is positioned to the left of the sink.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.1, "ram_available_mb": 100177.1, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25595.1, "ram_available_mb": 100177.1, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.1, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 32.83, "peak": 42.54, "min": 26.39}, "VIN": {"avg": 73.25, "peak": 105.85, "min": 58.5}, "VDD_CPU_SOC_MSS": {"avg": 15.79, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 32.83, "energy_joules_est": 122.2, "sample_count": 28, "duration_seconds": 3.722}, "timestamp": "2026-01-16T15:44:26.024302"}
{"image_index": 172, "image_name": "000000017379.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017379.jpg", "image_width": 478, "image_height": 640, "image_resolution": "478x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3554.907, "latencies_ms": [3554.907], "images_per_second": 0.281, "prompt_tokens": 21, "response_tokens_est": 60, "n_tiles": 12, "output_text": "The image depicts a bathroom with a modern and clean design. The setting is a bathroom with a sink, a mirror, and a tiled wall. The sink is empty, and there is a toilet paper roll on the counter. The wall is tiled with beige and brown tiles.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25595.1, "ram_available_mb": 100177.1, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25595.0, "ram_available_mb": 100177.1, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.09, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 33.03, "peak": 42.94, "min": 26.39}, "VIN": {"avg": 76.75, "peak": 120.42, "min": 55.3}, "VDD_CPU_SOC_MSS": {"avg": 15.8, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 33.03, "energy_joules_est": 117.43, "sample_count": 27, "duration_seconds": 3.555}, "timestamp": "2026-01-16T15:44:29.587853"}
{"image_index": 172, "image_name": "000000017379.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017379.jpg", "image_width": 478, "image_height": 640, "image_resolution": "478x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3140.915, "latencies_ms": [3140.915], "images_per_second": 0.318, "prompt_tokens": 19, "response_tokens_est": 48, "n_tiles": 12, "output_text": "The image shows a bathroom with beige tiled walls and a brown door. The lighting is soft and ambient, creating a warm and inviting atmosphere. The materials used in the bathroom include granite countertops and a stainless steel faucet.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25595.0, "ram_available_mb": 100177.1, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 25595.0, "ram_available_mb": 100177.1, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.08, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 34.24, "peak": 42.94, "min": 26.39}, "VIN": {"avg": 73.69, "peak": 111.76, "min": 58.55}, "VDD_CPU_SOC_MSS": {"avg": 15.73, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 34.24, "energy_joules_est": 107.56, "sample_count": 24, "duration_seconds": 3.141}, "timestamp": "2026-01-16T15:44:32.739162"}
{"image_index": 173, "image_name": "000000017436.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017436.jpg", "image_width": 481, "image_height": 640, "image_resolution": "481x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2159.485, "latencies_ms": [2159.485], "images_per_second": 0.463, "prompt_tokens": 9, "response_tokens_est": 22, "n_tiles": 12, "output_text": "A man is sitting on a bench in a park, looking down and appearing to be lost in thought.", "error": null, "sys_before": {"cpu_percent": 43.7, "ram_used_mb": 25595.0, "ram_available_mb": 100177.1, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 2.5, "ram_used_mb": 25595.3, "ram_available_mb": 100176.9, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.1, "peak": 16.15, "min": 13.69}, "VDD_GPU": {"avg": 35.87, "peak": 40.97, "min": 28.76}, "VIN": {"avg": 74.78, "peak": 97.17, "min": 58.32}, "VDD_CPU_SOC_MSS": {"avg": 15.84, "peak": 17.32, "min": 14.96}}, "power_watts_avg": 35.87, "energy_joules_est": 77.47, "sample_count": 16, "duration_seconds": 2.16}, "timestamp": "2026-01-16T15:44:34.995107"}
{"image_index": 173, "image_name": "000000017436.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017436.jpg", "image_width": 481, "image_height": 640, "image_resolution": "481x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 3014.942, "latencies_ms": [3014.942], "images_per_second": 0.332, "prompt_tokens": 23, "response_tokens_est": 44, "n_tiles": 12, "output_text": "1. Church\n2. Church steeple\n3. Church building\n4. Church clock\n5. Church clock tower\n6. Church bell\n7. Church bell tower\n8. Church spire", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25595.3, "ram_available_mb": 100176.9, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 25595.3, "ram_available_mb": 100176.9, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.07, "peak": 15.65, "min": 14.43}, "VDD_GPU": {"avg": 34.82, "peak": 43.33, "min": 26.8}, "VIN": {"avg": 74.18, "peak": 113.87, "min": 58.29}, "VDD_CPU_SOC_MSS": {"avg": 15.71, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 34.82, "energy_joules_est": 105.0, "sample_count": 23, "duration_seconds": 3.015}, "timestamp": "2026-01-16T15:44:38.020579"}
{"image_index": 173, "image_name": "000000017436.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017436.jpg", "image_width": 481, "image_height": 640, "image_resolution": "481x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3991.403, "latencies_ms": [3991.403], "images_per_second": 0.251, "prompt_tokens": 27, "response_tokens_est": 73, "n_tiles": 12, "output_text": "The main objects in the image are a man sitting on a bench and a church tower. The man is positioned in the foreground, sitting on the bench, while the church tower is in the background, towering over the scene. The bench is located near the man, and the church tower is further away, creating a clear spatial relationship between the two objects.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25595.3, "ram_available_mb": 100176.9, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25595.0, "ram_available_mb": 100177.1, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.09, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 32.52, "peak": 42.94, "min": 26.0}, "VIN": {"avg": 72.87, "peak": 111.51, "min": 58.27}, "VDD_CPU_SOC_MSS": {"avg": 15.72, "peak": 16.53, "min": 15.35}}, "power_watts_avg": 32.52, "energy_joules_est": 129.82, "sample_count": 31, "duration_seconds": 3.992}, "timestamp": "2026-01-16T15:44:42.018173"}
{"image_index": 173, "image_name": "000000017436.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017436.jpg", "image_width": 481, "image_height": 640, "image_resolution": "481x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3993.981, "latencies_ms": [3993.981], "images_per_second": 0.25, "prompt_tokens": 21, "response_tokens_est": 73, "n_tiles": 12, "output_text": "The image depicts a serene, monochromatic scene of a park or garden with a man sitting on a bench. The setting appears to be a quiet, possibly urban area with well-maintained greenery and a small building in the background. The man seems to be resting or contemplating, while the surrounding environment is peaceful and orderly.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.0, "ram_available_mb": 100177.1, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25595.0, "ram_available_mb": 100177.2, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.09, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 32.3, "peak": 42.54, "min": 26.0}, "VIN": {"avg": 72.06, "peak": 93.03, "min": 58.17}, "VDD_CPU_SOC_MSS": {"avg": 15.73, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 32.3, "energy_joules_est": 129.03, "sample_count": 31, "duration_seconds": 3.995}, "timestamp": "2026-01-16T15:44:46.019663"}
{"image_index": 173, "image_name": "000000017436.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017436.jpg", "image_width": 481, "image_height": 640, "image_resolution": "481x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 4191.835, "latencies_ms": [4191.835], "images_per_second": 0.239, "prompt_tokens": 19, "response_tokens_est": 79, "n_tiles": 12, "output_text": "The image is a black and white photograph featuring a man sitting on a bench in a park-like setting. The man is dressed in casual attire, and the surrounding environment includes well-maintained bushes, trees, and a building with a prominent clock tower. The lighting is soft and diffused, likely due to an overcast sky, creating a serene and nostalgic atmosphere.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25595.0, "ram_available_mb": 100177.2, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25595.0, "ram_available_mb": 100177.2, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.09, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 32.12, "peak": 42.54, "min": 26.0}, "VIN": {"avg": 74.36, "peak": 123.93, "min": 58.27}, "VDD_CPU_SOC_MSS": {"avg": 15.72, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 32.12, "energy_joules_est": 134.66, "sample_count": 32, "duration_seconds": 4.192}, "timestamp": "2026-01-16T15:44:50.218083"}
{"image_index": 174, "image_name": "000000017627.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017627.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2059.924, "latencies_ms": [2059.924], "images_per_second": 0.485, "prompt_tokens": 9, "response_tokens_est": 19, "n_tiles": 12, "output_text": "A street scene with cars parked and a person walking by a building with a stone facade.", "error": null, "sys_before": {"cpu_percent": 45.4, "ram_used_mb": 25595.0, "ram_available_mb": 100177.2, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 2.6, "ram_used_mb": 25595.0, "ram_available_mb": 100177.2, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.09, "peak": 16.05, "min": 13.69}, "VDD_GPU": {"avg": 36.12, "peak": 41.36, "min": 29.15}, "VIN": {"avg": 79.39, "peak": 115.79, "min": 58.54}, "VDD_CPU_SOC_MSS": {"avg": 15.89, "peak": 17.32, "min": 14.57}}, "power_watts_avg": 36.12, "energy_joules_est": 74.42, "sample_count": 16, "duration_seconds": 2.06}, "timestamp": "2026-01-16T15:44:52.375080"}
{"image_index": 174, "image_name": "000000017627.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017627.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 3153.385, "latencies_ms": [3153.385], "images_per_second": 0.317, "prompt_tokens": 23, "response_tokens_est": 48, "n_tiles": 12, "output_text": "- Car: 5\n- Car: 2\n- Car: 1\n- Car: 1\n- Car: 1\n- Car: 1\n- Car: 1\n- Car: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.0, "ram_available_mb": 100177.2, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 25595.0, "ram_available_mb": 100177.2, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.07, "peak": 15.65, "min": 14.43}, "VDD_GPU": {"avg": 34.42, "peak": 42.94, "min": 26.4}, "VIN": {"avg": 77.07, "peak": 132.01, "min": 52.45}, "VDD_CPU_SOC_MSS": {"avg": 15.76, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 34.42, "energy_joules_est": 108.56, "sample_count": 24, "duration_seconds": 3.154}, "timestamp": "2026-01-16T15:44:55.535365"}
{"image_index": 174, "image_name": "000000017627.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017627.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3147.464, "latencies_ms": [3147.464], "images_per_second": 0.318, "prompt_tokens": 27, "response_tokens_est": 48, "n_tiles": 12, "output_text": "The main objects in the image are parked cars and a building. The cars are parked in the foreground, with the closest one being a white sedan. The building is in the background, with a stone facade and a sign on it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.0, "ram_available_mb": 100177.2, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 25595.3, "ram_available_mb": 100176.9, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.08, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 34.2, "peak": 42.13, "min": 26.39}, "VIN": {"avg": 75.02, "peak": 102.26, "min": 58.51}, "VDD_CPU_SOC_MSS": {"avg": 15.78, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 34.2, "energy_joules_est": 107.66, "sample_count": 24, "duration_seconds": 3.148}, "timestamp": "2026-01-16T15:44:58.689079"}
{"image_index": 174, "image_name": "000000017627.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017627.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 4031.787, "latencies_ms": [4031.787], "images_per_second": 0.248, "prompt_tokens": 21, "response_tokens_est": 73, "n_tiles": 12, "output_text": "The image depicts a bustling urban street scene, likely in a Middle Eastern city, given the architecture and the presence of a Jewish cemetery. The setting is characterized by a mix of modern and traditional elements, with parked cars, a bus, and a pedestrian crossing. The scene is lively, with people walking and vehicles parked along the side of the street.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.3, "ram_available_mb": 100176.9, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25595.2, "ram_available_mb": 100176.9, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.88, "peak": 15.65, "min": 14.32}, "VDD_GPU": {"avg": 32.12, "peak": 42.94, "min": 25.6}, "VIN": {"avg": 72.85, "peak": 111.25, "min": 58.35}, "VDD_CPU_SOC_MSS": {"avg": 15.41, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 32.12, "energy_joules_est": 129.52, "sample_count": 32, "duration_seconds": 4.032}, "timestamp": "2026-01-16T15:45:02.727409"}
{"image_index": 174, "image_name": "000000017627.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017627.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3705.806, "latencies_ms": [3705.806], "images_per_second": 0.27, "prompt_tokens": 19, "response_tokens_est": 64, "n_tiles": 12, "output_text": "The image depicts a sunny day with clear blue skies. The scene is characterized by a mix of modern and traditional elements, with a stone building in the background and a variety of vehicles parked along the street. The lighting is bright and natural, casting shadows on the ground, indicating that it is likely midday.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.2, "ram_available_mb": 100176.9, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25595.2, "ram_available_mb": 100176.9, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.74, "peak": 15.34, "min": 14.12}, "VDD_GPU": {"avg": 32.68, "peak": 42.54, "min": 26.01}, "VIN": {"avg": 71.83, "peak": 103.78, "min": 57.55}, "VDD_CPU_SOC_MSS": {"avg": 15.23, "peak": 16.14, "min": 14.56}}, "power_watts_avg": 32.68, "energy_joules_est": 121.11, "sample_count": 29, "duration_seconds": 3.706}, "timestamp": "2026-01-16T15:45:06.439986"}
{"image_index": 175, "image_name": "000000017714.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017714.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2647.609, "latencies_ms": [2647.609], "images_per_second": 0.378, "prompt_tokens": 9, "response_tokens_est": 37, "n_tiles": 12, "output_text": "The image shows a plate of food consisting of a slice of banana bread, a cup of coffee, and a bowl of watermelon slices, all placed on a wooden table.", "error": null, "sys_before": {"cpu_percent": 42.3, "ram_used_mb": 25595.2, "ram_available_mb": 100176.9, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 25595.2, "ram_available_mb": 100176.9, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.08, "peak": 16.05, "min": 13.69}, "VDD_GPU": {"avg": 34.35, "peak": 41.36, "min": 27.18}, "VIN": {"avg": 73.17, "peak": 113.98, "min": 58.52}, "VDD_CPU_SOC_MSS": {"avg": 15.96, "peak": 17.32, "min": 14.57}}, "power_watts_avg": 34.35, "energy_joules_est": 90.97, "sample_count": 20, "duration_seconds": 2.648}, "timestamp": "2026-01-16T15:45:09.172586"}
{"image_index": 175, "image_name": "000000017714.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017714.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2813.505, "latencies_ms": [2813.505], "images_per_second": 0.355, "prompt_tokens": 23, "response_tokens_est": 38, "n_tiles": 12, "output_text": "1. Plate\n2. Cereal\n3. Cup\n4. Cutting board\n5. Knife\n6. Fork\n7. Bananas\n8. Watermelon", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.2, "ram_available_mb": 100176.9, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 25595.2, "ram_available_mb": 100176.9, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.07, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 35.05, "peak": 42.94, "min": 26.79}, "VIN": {"avg": 75.98, "peak": 117.96, "min": 45.9}, "VDD_CPU_SOC_MSS": {"avg": 15.67, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 35.05, "energy_joules_est": 98.63, "sample_count": 22, "duration_seconds": 2.814}, "timestamp": "2026-01-16T15:45:11.992529"}
{"image_index": 175, "image_name": "000000017714.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017714.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 4186.147, "latencies_ms": [4186.147], "images_per_second": 0.239, "prompt_tokens": 27, "response_tokens_est": 79, "n_tiles": 12, "output_text": "The main objects in the image are a plate of food and a cup of coffee. The plate of food is positioned in the foreground, with a slice of banana and a piece of fruit on it. The cup of coffee is placed to the left of the plate, with a spoon inside it. The background features a wooden table and a tiled floor, with shadows cast by the objects.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.2, "ram_available_mb": 100176.9, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 25595.2, "ram_available_mb": 100176.9, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.12, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 32.23, "peak": 42.94, "min": 26.39}, "VIN": {"avg": 72.15, "peak": 111.17, "min": 58.42}, "VDD_CPU_SOC_MSS": {"avg": 15.77, "peak": 16.53, "min": 15.35}}, "power_watts_avg": 32.23, "energy_joules_est": 134.93, "sample_count": 33, "duration_seconds": 4.186}, "timestamp": "2026-01-16T15:45:16.184935"}
{"image_index": 175, "image_name": "000000017714.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017714.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3751.027, "latencies_ms": [3751.027], "images_per_second": 0.267, "prompt_tokens": 21, "response_tokens_est": 67, "n_tiles": 12, "output_text": "The image depicts a breakfast scene set on a wooden table with a tiled floor. The table is adorned with a plate of food, including a slice of pancake, a bowl of fruit, and a cup of coffee. The setting suggests a cozy and relaxed atmosphere, likely in a home or a casual dining environment.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.2, "ram_available_mb": 100176.9, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25595.2, "ram_available_mb": 100176.9, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.16, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 32.9, "peak": 42.15, "min": 26.39}, "VIN": {"avg": 74.52, "peak": 119.68, "min": 58.68}, "VDD_CPU_SOC_MSS": {"avg": 15.75, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 32.9, "energy_joules_est": 123.43, "sample_count": 29, "duration_seconds": 3.752}, "timestamp": "2026-01-16T15:45:19.942863"}
{"image_index": 175, "image_name": "000000017714.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017714.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3754.631, "latencies_ms": [3754.631], "images_per_second": 0.266, "prompt_tokens": 19, "response_tokens_est": 66, "n_tiles": 12, "output_text": "The image features a wooden table with a white plate containing a slice of banana bread and a cup of coffee. The lighting is natural, casting shadows on the table, indicating a sunny day. The colors are warm, with the brown of the table contrasting with the white of the plate and the red of the banana bread.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.2, "ram_available_mb": 100176.9, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25595.2, "ram_available_mb": 100177.0, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.1, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 32.74, "peak": 42.15, "min": 26.39}, "VIN": {"avg": 73.46, "peak": 107.61, "min": 58.46}, "VDD_CPU_SOC_MSS": {"avg": 15.62, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 32.74, "energy_joules_est": 122.94, "sample_count": 29, "duration_seconds": 3.755}, "timestamp": "2026-01-16T15:45:23.704123"}
{"image_index": 176, "image_name": "000000017899.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017899.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2217.321, "latencies_ms": [2217.321], "images_per_second": 0.451, "prompt_tokens": 9, "response_tokens_est": 24, "n_tiles": 12, "output_text": "An elderly woman is preparing a batch of homemade cookies on a wooden table, surrounded by various baking tools and ingredients.", "error": null, "sys_before": {"cpu_percent": 42.4, "ram_used_mb": 25595.2, "ram_available_mb": 100177.0, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 2.5, "ram_used_mb": 25595.2, "ram_available_mb": 100177.0, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.14, "peak": 16.15, "min": 13.9}, "VDD_GPU": {"avg": 35.71, "peak": 41.76, "min": 28.36}, "VIN": {"avg": 72.6, "peak": 92.96, "min": 57.63}, "VDD_CPU_SOC_MSS": {"avg": 15.93, "peak": 17.32, "min": 14.96}}, "power_watts_avg": 35.71, "energy_joules_est": 79.2, "sample_count": 17, "duration_seconds": 2.218}, "timestamp": "2026-01-16T15:45:26.007910"}
{"image_index": 176, "image_name": "000000017899.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017899.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 3376.19, "latencies_ms": [3376.19], "images_per_second": 0.296, "prompt_tokens": 23, "response_tokens_est": 55, "n_tiles": 12, "output_text": "- plate: 1\n- bowl: 1\n- knife: 1\n- cookies: 8\n- plate: 1\n- glass: 1\n- cup: 1\n- napkin: 1\n- table: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.2, "ram_available_mb": 100177.0, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 25595.2, "ram_available_mb": 100177.0, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.09, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 33.83, "peak": 42.54, "min": 26.4}, "VIN": {"avg": 72.53, "peak": 108.86, "min": 58.55}, "VDD_CPU_SOC_MSS": {"avg": 15.73, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 33.83, "energy_joules_est": 114.23, "sample_count": 26, "duration_seconds": 3.377}, "timestamp": "2026-01-16T15:45:29.390610"}
{"image_index": 176, "image_name": "000000017899.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017899.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 4602.667, "latencies_ms": [4602.667], "images_per_second": 0.217, "prompt_tokens": 27, "response_tokens_est": 91, "n_tiles": 12, "output_text": "The main objects in the image are a woman, a table, and various baking items. The woman is seated at the table, engaged in baking. The table is the central focus, with the baking items placed around it. The background includes a couch and a door, while the foreground features a glass of water and a book. The woman is positioned near the table, with the baking items in the foreground and the couch and door in the background.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25595.2, "ram_available_mb": 100177.0, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 25595.2, "ram_available_mb": 100177.0, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.1, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 31.6, "peak": 42.54, "min": 26.39}, "VIN": {"avg": 72.54, "peak": 116.73, "min": 58.56}, "VDD_CPU_SOC_MSS": {"avg": 15.7, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 31.6, "energy_joules_est": 145.45, "sample_count": 36, "duration_seconds": 4.603}, "timestamp": "2026-01-16T15:45:33.999555"}
{"image_index": 176, "image_name": "000000017899.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017899.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3782.549, "latencies_ms": [3782.549], "images_per_second": 0.264, "prompt_tokens": 21, "response_tokens_est": 67, "n_tiles": 12, "output_text": "The image depicts an indoor scene where an elderly woman is preparing food in a cozy kitchen. She is seen in the process of cutting dough into shapes on a wooden table, surrounded by various baking supplies and ingredients. The setting is a well-used kitchen with a wooden floor, a couch, and a window providing natural light.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.2, "ram_available_mb": 100177.0, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25595.2, "ram_available_mb": 100177.0, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.11, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 32.79, "peak": 43.33, "min": 26.39}, "VIN": {"avg": 76.27, "peak": 121.35, "min": 58.25}, "VDD_CPU_SOC_MSS": {"avg": 15.8, "peak": 16.53, "min": 15.35}}, "power_watts_avg": 32.79, "energy_joules_est": 124.05, "sample_count": 30, "duration_seconds": 3.783}, "timestamp": "2026-01-16T15:45:37.788724"}
{"image_index": 176, "image_name": "000000017899.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017899.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 4326.832, "latencies_ms": [4326.832], "images_per_second": 0.231, "prompt_tokens": 19, "response_tokens_est": 83, "n_tiles": 12, "output_text": "The image depicts a cozy, well-lit kitchen scene with a warm, inviting ambiance. The lighting is soft and natural, casting gentle shadows that enhance the textures and colors of the objects present. The kitchen features a wooden table, a white metal rack holding freshly baked cookies, a glass of water, and a colorful children's book on the table. The overall atmosphere is cheerful and homely.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.2, "ram_available_mb": 100177.0, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25595.2, "ram_available_mb": 100177.0, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.1, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 31.99, "peak": 42.94, "min": 26.39}, "VIN": {"avg": 72.47, "peak": 109.0, "min": 56.87}, "VDD_CPU_SOC_MSS": {"avg": 15.65, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 31.99, "energy_joules_est": 138.43, "sample_count": 34, "duration_seconds": 4.327}, "timestamp": "2026-01-16T15:45:42.122227"}
{"image_index": 177, "image_name": "000000017905.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017905.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2523.003, "latencies_ms": [2523.003], "images_per_second": 0.396, "prompt_tokens": 9, "response_tokens_est": 33, "n_tiles": 12, "output_text": "A man wearing a white t-shirt and khaki shorts stands in front of a traffic light, which is illuminated in red, indicating that vehicles should stop.", "error": null, "sys_before": {"cpu_percent": 44.9, "ram_used_mb": 25595.2, "ram_available_mb": 100177.0, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 25595.2, "ram_available_mb": 100177.0, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.12, "peak": 16.15, "min": 13.9}, "VDD_GPU": {"avg": 34.91, "peak": 41.36, "min": 27.18}, "VIN": {"avg": 76.9, "peak": 128.31, "min": 58.74}, "VDD_CPU_SOC_MSS": {"avg": 16.06, "peak": 17.32, "min": 14.96}}, "power_watts_avg": 34.91, "energy_joules_est": 88.09, "sample_count": 19, "duration_seconds": 2.523}, "timestamp": "2026-01-16T15:45:44.733139"}
{"image_index": 177, "image_name": "000000017905.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017905.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 3310.754, "latencies_ms": [3310.754], "images_per_second": 0.302, "prompt_tokens": 23, "response_tokens_est": 53, "n_tiles": 12, "output_text": "- man: 1\n- white t-shirt: 1\n- shorts: 1\n- sandals: 2\n- traffic light: 1\n- sign: 1\n- tree: 1\n- bushes: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.2, "ram_available_mb": 100177.0, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 25595.4, "ram_available_mb": 100176.7, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.1, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 33.96, "peak": 42.54, "min": 26.39}, "VIN": {"avg": 72.67, "peak": 110.52, "min": 57.95}, "VDD_CPU_SOC_MSS": {"avg": 15.76, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 33.96, "energy_joules_est": 112.45, "sample_count": 25, "duration_seconds": 3.311}, "timestamp": "2026-01-16T15:45:48.050396"}
{"image_index": 177, "image_name": "000000017905.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017905.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3492.806, "latencies_ms": [3492.806], "images_per_second": 0.286, "prompt_tokens": 27, "response_tokens_est": 58, "n_tiles": 12, "output_text": "The main object in the foreground is a man standing next to a traffic light. The traffic light is positioned to the right of the man. The background features lush green plants and a sign that reads \"AUSTRALIA TRAFFIC LIGHT.\" The sign is located near the man.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.4, "ram_available_mb": 100176.7, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25595.2, "ram_available_mb": 100177.0, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.09, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 33.39, "peak": 42.94, "min": 26.39}, "VIN": {"avg": 74.37, "peak": 118.99, "min": 58.51}, "VDD_CPU_SOC_MSS": {"avg": 15.75, "peak": 16.53, "min": 15.35}}, "power_watts_avg": 33.39, "energy_joules_est": 116.64, "sample_count": 27, "duration_seconds": 3.493}, "timestamp": "2026-01-16T15:45:51.549769"}
{"image_index": 177, "image_name": "000000017905.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017905.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3716.592, "latencies_ms": [3716.592], "images_per_second": 0.269, "prompt_tokens": 21, "response_tokens_est": 65, "n_tiles": 12, "output_text": "The image depicts a man standing in front of a traffic light, which is currently showing red. The setting appears to be outdoors, possibly in a park or garden area, with lush green plants and flowers in the background. The man is wearing a white t-shirt and light-colored shorts, and he is barefoot.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.2, "ram_available_mb": 100177.0, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25595.4, "ram_available_mb": 100176.7, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.1, "peak": 15.65, "min": 14.63}, "VDD_GPU": {"avg": 32.88, "peak": 42.54, "min": 26.0}, "VIN": {"avg": 72.07, "peak": 143.86, "min": 57.78}, "VDD_CPU_SOC_MSS": {"avg": 15.8, "peak": 16.53, "min": 15.35}}, "power_watts_avg": 32.88, "energy_joules_est": 122.22, "sample_count": 29, "duration_seconds": 3.717}, "timestamp": "2026-01-16T15:45:55.273877"}
{"image_index": 177, "image_name": "000000017905.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017905.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2811.69, "latencies_ms": [2811.69], "images_per_second": 0.356, "prompt_tokens": 19, "response_tokens_est": 38, "n_tiles": 12, "output_text": "The image features a brightly lit scene with a man standing in front of a traffic light. The traffic light is red, and the surrounding area is lush with green foliage.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.4, "ram_available_mb": 100176.7, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 25595.2, "ram_available_mb": 100177.0, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.09, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 35.09, "peak": 42.53, "min": 27.19}, "VIN": {"avg": 76.72, "peak": 119.22, "min": 66.15}, "VDD_CPU_SOC_MSS": {"avg": 15.82, "peak": 16.53, "min": 15.35}}, "power_watts_avg": 35.09, "energy_joules_est": 98.67, "sample_count": 21, "duration_seconds": 2.812}, "timestamp": "2026-01-16T15:45:58.091941"}
{"image_index": 178, "image_name": "000000017959.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017959.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1609.911, "latencies_ms": [1609.911], "images_per_second": 0.621, "prompt_tokens": 9, "response_tokens_est": 38, "n_tiles": 6, "output_text": "The image depicts a vibrant scene of a large group of people gathered in a park, where numerous colorful kites are flying high in the sky, creating a lively and festive atmosphere.", "error": null, "sys_before": {"cpu_percent": 41.7, "ram_used_mb": 25595.2, "ram_available_mb": 100177.0, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 25595.2, "ram_available_mb": 100177.0, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.48, "peak": 15.01, "min": 13.82}, "VDD_GPU": {"avg": 31.29, "peak": 37.82, "min": 24.83}, "VIN": {"avg": 67.97, "peak": 84.24, "min": 57.48}, "VDD_CPU_SOC_MSS": {"avg": 15.26, "peak": 15.75, "min": 14.57}}, "power_watts_avg": 31.29, "energy_joules_est": 50.39, "sample_count": 12, "duration_seconds": 1.611}, "timestamp": "2026-01-16T15:45:59.765262"}
{"image_index": 178, "image_name": "000000017959.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017959.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1740.132, "latencies_ms": [1740.132], "images_per_second": 0.575, "prompt_tokens": 23, "response_tokens_est": 40, "n_tiles": 6, "output_text": "object: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.2, "ram_available_mb": 100177.0, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 25595.2, "ram_available_mb": 100177.0, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.66, "peak": 14.91, "min": 14.12}, "VDD_GPU": {"avg": 31.12, "peak": 38.19, "min": 24.42}, "VIN": {"avg": 69.12, "peak": 95.01, "min": 58.89}, "VDD_CPU_SOC_MSS": {"avg": 15.2, "peak": 15.36, "min": 14.57}}, "power_watts_avg": 31.12, "energy_joules_est": 54.16, "sample_count": 13, "duration_seconds": 1.74}, "timestamp": "2026-01-16T15:46:01.511334"}
{"image_index": 178, "image_name": "000000017959.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017959.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3574.575, "latencies_ms": [3574.575], "images_per_second": 0.28, "prompt_tokens": 27, "response_tokens_est": 105, "n_tiles": 6, "output_text": "The main objects in the image are kites, with the most prominent one being a large, colorful fish kite in the foreground. The fish kite is positioned near the center of the image, with its tail extending towards the right side. In the background, there are other kites, including a red and black one and a blue and purple one, which are positioned further away from the fish kite. The kites are spread out across the field, with some closer to the foreground and others further back.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.2, "ram_available_mb": 100177.0, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 25595.2, "ram_available_mb": 100177.0, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.83, "peak": 15.11, "min": 14.02}, "VDD_GPU": {"avg": 26.83, "peak": 38.19, "min": 23.24}, "VIN": {"avg": 68.07, "peak": 112.95, "min": 56.97}, "VDD_CPU_SOC_MSS": {"avg": 15.47, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 26.83, "energy_joules_est": 95.91, "sample_count": 28, "duration_seconds": 3.575}, "timestamp": "2026-01-16T15:46:05.091386"}
{"image_index": 178, "image_name": "000000017959.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017959.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3073.817, "latencies_ms": [3073.817], "images_per_second": 0.325, "prompt_tokens": 21, "response_tokens_est": 89, "n_tiles": 6, "output_text": "The image depicts a vibrant outdoor scene at a public park where numerous people are gathered, enjoying a day of leisure and recreation. The park is filled with green grass, trees, and a few scattered buildings in the background. In the foreground, there are several colorful kites, including a large, whimsical fish-shaped kite that stands out prominently. The sky is partly cloudy, suggesting a pleasant day for outdoor activities.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.2, "ram_available_mb": 100177.0, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 25595.4, "ram_available_mb": 100176.8, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.9, "peak": 15.22, "min": 14.12}, "VDD_GPU": {"avg": 27.48, "peak": 38.21, "min": 23.24}, "VIN": {"avg": 67.08, "peak": 99.76, "min": 60.94}, "VDD_CPU_SOC_MSS": {"avg": 15.49, "peak": 15.75, "min": 14.57}}, "power_watts_avg": 27.48, "energy_joules_est": 84.48, "sample_count": 24, "duration_seconds": 3.074}, "timestamp": "2026-01-16T15:46:08.171738"}
{"image_index": 178, "image_name": "000000017959.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000017959.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2245.048, "latencies_ms": [2245.048], "images_per_second": 0.445, "prompt_tokens": 19, "response_tokens_est": 59, "n_tiles": 6, "output_text": "The image showcases a vibrant kite flying high in the sky, featuring a striking red fish with black and white stripes. The kite is illuminated by the bright sunlight, casting a warm glow on the scene. The sky is clear, indicating a sunny day with good weather conditions.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25595.4, "ram_available_mb": 100176.8, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 25595.4, "ram_available_mb": 100176.8, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.82, "peak": 15.11, "min": 14.12}, "VDD_GPU": {"avg": 29.24, "peak": 38.21, "min": 23.64}, "VIN": {"avg": 67.84, "peak": 75.61, "min": 59.53}, "VDD_CPU_SOC_MSS": {"avg": 15.35, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 29.24, "energy_joules_est": 65.66, "sample_count": 17, "duration_seconds": 2.245}, "timestamp": "2026-01-16T15:46:10.424150"}
{"image_index": 179, "image_name": "000000018150.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018150.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2347.092, "latencies_ms": [2347.092], "images_per_second": 0.426, "prompt_tokens": 9, "response_tokens_est": 28, "n_tiles": 12, "output_text": "A young boy is eating a slice of pizza from a box, while a man sits beside him, both in a cozy indoor setting.", "error": null, "sys_before": {"cpu_percent": 46.7, "ram_used_mb": 25595.4, "ram_available_mb": 100176.8, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 25595.7, "ram_available_mb": 100176.5, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.14, "peak": 16.05, "min": 13.9}, "VDD_GPU": {"avg": 35.1, "peak": 40.97, "min": 27.97}, "VIN": {"avg": 75.87, "peak": 120.03, "min": 58.56}, "VDD_CPU_SOC_MSS": {"avg": 15.96, "peak": 17.32, "min": 15.35}}, "power_watts_avg": 35.1, "energy_joules_est": 82.4, "sample_count": 18, "duration_seconds": 2.348}, "timestamp": "2026-01-16T15:46:12.861007"}
{"image_index": 179, "image_name": "000000018150.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018150.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2846.571, "latencies_ms": [2846.571], "images_per_second": 0.351, "prompt_tokens": 23, "response_tokens_est": 39, "n_tiles": 12, "output_text": "1. Pizza\n2. Pizza box\n3. Pizza slice\n4. Pizza box\n5. Pizza box\n6. Pizza box\n7. Pizza box\n8. Pizza box", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.7, "ram_available_mb": 100176.5, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 25595.4, "ram_available_mb": 100176.8, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.08, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 35.16, "peak": 42.94, "min": 27.18}, "VIN": {"avg": 80.83, "peak": 118.74, "min": 55.58}, "VDD_CPU_SOC_MSS": {"avg": 15.69, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 35.16, "energy_joules_est": 100.1, "sample_count": 22, "duration_seconds": 2.847}, "timestamp": "2026-01-16T15:46:15.713757"}
{"image_index": 179, "image_name": "000000018150.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018150.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 4739.768, "latencies_ms": [4739.768], "images_per_second": 0.211, "prompt_tokens": 27, "response_tokens_est": 95, "n_tiles": 12, "output_text": "The main objects in the image are a man and a young child. The man is seated on the left side of the image, while the child is on the right side. The man is holding a pizza slice in his right hand, and the child is holding a green toy in their left hand. The pizza slice is in the foreground, and the child is slightly behind it. The background includes a couch and a window with blinds, creating a cozy indoor setting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.4, "ram_available_mb": 100176.8, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 25595.4, "ram_available_mb": 100176.8, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.1, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 31.5, "peak": 42.94, "min": 26.0}, "VIN": {"avg": 73.61, "peak": 124.2, "min": 58.58}, "VDD_CPU_SOC_MSS": {"avg": 15.74, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 31.5, "energy_joules_est": 149.32, "sample_count": 37, "duration_seconds": 4.74}, "timestamp": "2026-01-16T15:46:20.460901"}
{"image_index": 179, "image_name": "000000018150.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018150.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3686.393, "latencies_ms": [3686.393], "images_per_second": 0.271, "prompt_tokens": 21, "response_tokens_est": 64, "n_tiles": 12, "output_text": "The image depicts a cozy indoor setting with a young boy and a man seated on a couch. The boy is holding a slice of pizza, while the man is eating from a box of pizza. The room has warm lighting, and there are various items on the couch, including a stuffed animal and a bottle.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 25595.4, "ram_available_mb": 100176.8, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25595.4, "ram_available_mb": 100176.8, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.1, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 32.83, "peak": 42.53, "min": 26.39}, "VIN": {"avg": 71.06, "peak": 113.2, "min": 45.6}, "VDD_CPU_SOC_MSS": {"avg": 15.68, "peak": 16.53, "min": 15.35}}, "power_watts_avg": 32.83, "energy_joules_est": 121.04, "sample_count": 29, "duration_seconds": 3.687}, "timestamp": "2026-01-16T15:46:24.153659"}
{"image_index": 179, "image_name": "000000018150.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018150.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2812.032, "latencies_ms": [2812.032], "images_per_second": 0.356, "prompt_tokens": 19, "response_tokens_est": 38, "n_tiles": 12, "output_text": "The image features a young boy with curly hair, wearing a blue hoodie, sitting on a red and brown striped blanket. The lighting is warm, creating a cozy atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.4, "ram_available_mb": 100176.8, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 25595.6, "ram_available_mb": 100176.5, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.07, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 34.88, "peak": 42.94, "min": 26.79}, "VIN": {"avg": 78.75, "peak": 118.86, "min": 60.14}, "VDD_CPU_SOC_MSS": {"avg": 15.67, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 34.88, "energy_joules_est": 98.1, "sample_count": 22, "duration_seconds": 2.813}, "timestamp": "2026-01-16T15:46:26.972147"}
{"image_index": 180, "image_name": "000000018193.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018193.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2159.429, "latencies_ms": [2159.429], "images_per_second": 0.463, "prompt_tokens": 9, "response_tokens_est": 22, "n_tiles": 12, "output_text": "A young woman is eating a hot dog from a sandwich, with a plate of nachos beside her.", "error": null, "sys_before": {"cpu_percent": 44.8, "ram_used_mb": 25595.4, "ram_available_mb": 100176.8, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 2.5, "ram_used_mb": 25595.4, "ram_available_mb": 100176.8, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.08, "peak": 16.05, "min": 13.59}, "VDD_GPU": {"avg": 36.01, "peak": 42.15, "min": 28.76}, "VIN": {"avg": 74.48, "peak": 111.34, "min": 59.46}, "VDD_CPU_SOC_MSS": {"avg": 15.95, "peak": 16.92, "min": 14.57}}, "power_watts_avg": 36.01, "energy_joules_est": 77.78, "sample_count": 17, "duration_seconds": 2.16}, "timestamp": "2026-01-16T15:46:29.229410"}
{"image_index": 180, "image_name": "000000018193.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018193.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2680.505, "latencies_ms": [2680.505], "images_per_second": 0.373, "prompt_tokens": 23, "response_tokens_est": 34, "n_tiles": 12, "output_text": "1. Plate\n2. Sandwich\n3. Bag\n4. Woman\n5. Chair\n6. Food\n7. Plate\n8. Sandwich", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.4, "ram_available_mb": 100176.8, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 25595.4, "ram_available_mb": 100176.8, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.09, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 35.62, "peak": 43.33, "min": 27.18}, "VIN": {"avg": 76.07, "peak": 102.84, "min": 58.19}, "VDD_CPU_SOC_MSS": {"avg": 15.71, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 35.62, "energy_joules_est": 95.5, "sample_count": 21, "duration_seconds": 2.681}, "timestamp": "2026-01-16T15:46:31.916421"}
{"image_index": 180, "image_name": "000000018193.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018193.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3289.158, "latencies_ms": [3289.158], "images_per_second": 0.304, "prompt_tokens": 27, "response_tokens_est": 52, "n_tiles": 12, "output_text": "The main object in the foreground is a plate with a few pieces of potato chips. The plate is placed on a person's lap, which is in the foreground. The background consists of a dark, possibly outdoor setting with a tree branch and some rocks.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.4, "ram_available_mb": 100176.8, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 25595.4, "ram_available_mb": 100176.8, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.08, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 33.82, "peak": 42.94, "min": 26.39}, "VIN": {"avg": 75.2, "peak": 119.63, "min": 58.52}, "VDD_CPU_SOC_MSS": {"avg": 15.72, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 33.82, "energy_joules_est": 111.25, "sample_count": 26, "duration_seconds": 3.29}, "timestamp": "2026-01-16T15:46:35.212793"}
{"image_index": 180, "image_name": "000000018193.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018193.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3043.618, "latencies_ms": [3043.618], "images_per_second": 0.329, "prompt_tokens": 21, "response_tokens_est": 45, "n_tiles": 12, "output_text": "The image depicts a young woman sitting on a blue chair, holding a sandwich in her hand. She is surrounded by a dark, outdoor setting with a small pile of potato chips on a plate in front of her.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.4, "ram_available_mb": 100176.8, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 25595.4, "ram_available_mb": 100176.8, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.09, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 34.16, "peak": 42.94, "min": 26.39}, "VIN": {"avg": 72.69, "peak": 109.74, "min": 58.18}, "VDD_CPU_SOC_MSS": {"avg": 15.78, "peak": 16.53, "min": 15.35}}, "power_watts_avg": 34.16, "energy_joules_est": 103.99, "sample_count": 24, "duration_seconds": 3.044}, "timestamp": "2026-01-16T15:46:38.263036"}
{"image_index": 180, "image_name": "000000018193.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018193.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3015.002, "latencies_ms": [3015.002], "images_per_second": 0.332, "prompt_tokens": 19, "response_tokens_est": 44, "n_tiles": 12, "output_text": "The image depicts a young woman with light skin and dark hair, wearing a striped shirt and holding a sandwich. The setting appears to be outdoors at night, with dim lighting and a dark, rocky ground.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.4, "ram_available_mb": 100176.8, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 25595.4, "ram_available_mb": 100176.8, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.08, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 34.56, "peak": 42.94, "min": 26.79}, "VIN": {"avg": 78.17, "peak": 116.17, "min": 58.48}, "VDD_CPU_SOC_MSS": {"avg": 15.75, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 34.56, "energy_joules_est": 104.21, "sample_count": 23, "duration_seconds": 3.015}, "timestamp": "2026-01-16T15:46:41.284487"}
{"image_index": 181, "image_name": "000000018380.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018380.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1285.43, "latencies_ms": [1285.43], "images_per_second": 0.778, "prompt_tokens": 9, "response_tokens_est": 26, "n_tiles": 6, "output_text": "A large family is gathered around a table, enjoying a meal together, with various dishes and drinks spread out on the table.", "error": null, "sys_before": {"cpu_percent": 40.4, "ram_used_mb": 25595.4, "ram_available_mb": 100176.8, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25595.4, "ram_available_mb": 100176.8, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.32, "peak": 14.81, "min": 13.82}, "VDD_GPU": {"avg": 33.09, "peak": 37.82, "min": 27.18}, "VIN": {"avg": 72.34, "peak": 105.04, "min": 50.33}, "VDD_CPU_SOC_MSS": {"avg": 15.05, "peak": 15.36, "min": 14.57}}, "power_watts_avg": 33.09, "energy_joules_est": 42.55, "sample_count": 9, "duration_seconds": 1.286}, "timestamp": "2026-01-16T15:46:42.635782"}
{"image_index": 181, "image_name": "000000018380.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018380.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 4072.499, "latencies_ms": [4072.499], "images_per_second": 0.246, "prompt_tokens": 23, "response_tokens_est": 128, "n_tiles": 6, "output_text": "- People: 8\n- Table: 1\n- Plate: 1\n- Glass: 1\n- Wine: 1\n- Bowl: 1\n- Cutlery: 1\n- Napkin: 1\n- Fork: 1\n- Knife: 1\n- Bread: 1\n- Meat: 1\n- Carrots: 1\n- Potatoes: 1\n- Chicken: 1\n- Dessert: 1\n- Beverage: 1\n- Wine Glass: 1\n- Plate: 1\n- Bowl:", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.4, "ram_available_mb": 100176.8, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 25595.4, "ram_available_mb": 100176.8, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.04, "peak": 15.22, "min": 14.22}, "VDD_GPU": {"avg": 26.99, "peak": 38.59, "min": 23.64}, "VIN": {"avg": 66.18, "peak": 114.97, "min": 55.97}, "VDD_CPU_SOC_MSS": {"avg": 15.61, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 26.99, "energy_joules_est": 109.92, "sample_count": 31, "duration_seconds": 4.073}, "timestamp": "2026-01-16T15:46:46.718525"}
{"image_index": 181, "image_name": "000000018380.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018380.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2911.226, "latencies_ms": [2911.226], "images_per_second": 0.343, "prompt_tokens": 27, "response_tokens_est": 85, "n_tiles": 6, "output_text": "The main objects in the image are a family gathered around a dining table. The table is covered with a purple tablecloth, and various dishes and glasses are placed on it. The family members are seated in the foreground, with the woman in the center and the children on either side. The background features a staircase and a doorway, with a decorative piece on the wall and a framed picture above the door.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.4, "ram_available_mb": 100176.8, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 25595.6, "ram_available_mb": 100176.6, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.0, "peak": 15.32, "min": 14.22}, "VDD_GPU": {"avg": 28.13, "peak": 37.82, "min": 23.64}, "VIN": {"avg": 72.82, "peak": 116.58, "min": 58.43}, "VDD_CPU_SOC_MSS": {"avg": 15.52, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 28.13, "energy_joules_est": 81.91, "sample_count": 22, "duration_seconds": 2.912}, "timestamp": "2026-01-16T15:46:49.636198"}
{"image_index": 181, "image_name": "000000018380.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018380.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2925.042, "latencies_ms": [2925.042], "images_per_second": 0.342, "prompt_tokens": 21, "response_tokens_est": 86, "n_tiles": 6, "output_text": "The image depicts a family gathering at a dining table in a warmly lit, homely kitchen. The family members are seated around the table, enjoying a meal together. The table is covered with a purple tablecloth, and various dishes, including a plate of food, a bowl of fruit, and glasses of wine, are visible. The family members are smiling and appear to be in a relaxed and happy mood.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25595.6, "ram_available_mb": 100176.6, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 25595.4, "ram_available_mb": 100176.8, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.04, "peak": 15.32, "min": 14.32}, "VDD_GPU": {"avg": 28.1, "peak": 38.21, "min": 23.64}, "VIN": {"avg": 70.81, "peak": 121.67, "min": 55.76}, "VDD_CPU_SOC_MSS": {"avg": 15.55, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 28.1, "energy_joules_est": 82.2, "sample_count": 22, "duration_seconds": 2.925}, "timestamp": "2026-01-16T15:46:52.567413"}
{"image_index": 181, "image_name": "000000018380.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018380.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2202.062, "latencies_ms": [2202.062], "images_per_second": 0.454, "prompt_tokens": 19, "response_tokens_est": 59, "n_tiles": 6, "output_text": "The image depicts a family gathering in a warmly lit, cozy kitchen. The table is covered with a purple tablecloth, and various dishes, including a plate of food and a bowl of fruit, are spread out. The lighting is soft and warm, creating a welcoming atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.4, "ram_available_mb": 100176.8, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 25595.6, "ram_available_mb": 100176.6, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.95, "peak": 15.22, "min": 14.32}, "VDD_GPU": {"avg": 29.72, "peak": 37.82, "min": 24.03}, "VIN": {"avg": 70.0, "peak": 104.0, "min": 60.75}, "VDD_CPU_SOC_MSS": {"avg": 15.48, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 29.72, "energy_joules_est": 65.46, "sample_count": 16, "duration_seconds": 2.203}, "timestamp": "2026-01-16T15:46:54.775809"}
{"image_index": 182, "image_name": "000000018491.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018491.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1580.741, "latencies_ms": [1580.741], "images_per_second": 0.633, "prompt_tokens": 9, "response_tokens_est": 37, "n_tiles": 6, "output_text": "A baseball game is in progress on a sunny day, with players in protective gear on the field, including a catcher and a batter, and spectators watching from the stands.", "error": null, "sys_before": {"cpu_percent": 41.8, "ram_used_mb": 25595.6, "ram_available_mb": 100176.6, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25595.6, "ram_available_mb": 100176.6, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.49, "peak": 15.01, "min": 13.92}, "VDD_GPU": {"avg": 31.05, "peak": 37.42, "min": 25.21}, "VIN": {"avg": 67.45, "peak": 83.76, "min": 53.27}, "VDD_CPU_SOC_MSS": {"avg": 15.18, "peak": 15.75, "min": 14.57}}, "power_watts_avg": 31.05, "energy_joules_est": 49.1, "sample_count": 11, "duration_seconds": 1.581}, "timestamp": "2026-01-16T15:46:56.416895"}
{"image_index": 182, "image_name": "000000018491.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018491.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2018.841, "latencies_ms": [2018.841], "images_per_second": 0.495, "prompt_tokens": 23, "response_tokens_est": 52, "n_tiles": 6, "output_text": "baseball player: 1\numpire: 1\ncatcher: 1\npitcher: 1\nbatter: 1\nhome plate: 1\ndirt infield: 1\ngrass outfield: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.6, "ram_available_mb": 100176.6, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 25595.8, "ram_available_mb": 100176.3, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.86, "peak": 15.22, "min": 14.22}, "VDD_GPU": {"avg": 30.46, "peak": 38.59, "min": 24.03}, "VIN": {"avg": 68.09, "peak": 105.29, "min": 56.1}, "VDD_CPU_SOC_MSS": {"avg": 15.43, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 30.46, "energy_joules_est": 61.52, "sample_count": 15, "duration_seconds": 2.02}, "timestamp": "2026-01-16T15:46:58.448454"}
{"image_index": 182, "image_name": "000000018491.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018491.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3782.047, "latencies_ms": [3782.047], "images_per_second": 0.264, "prompt_tokens": 27, "response_tokens_est": 114, "n_tiles": 6, "output_text": "In the image, the baseball field is the primary setting with a dirt infield and a grassy outfield. The main objects in the foreground include a batter and a catcher, both wearing protective gear. The catcher is crouched near the home plate, while the batter is positioned at the pitcher's mound. In the background, there are spectators seated on bleachers, and a chain-link fence encloses the field. The scene captures a moment of action during a baseball game, with the catcher and batter positioned near the home plate.", "error": null, "sys_before": {"cpu_percent": 40.0, "ram_used_mb": 25595.8, "ram_available_mb": 100176.3, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 25595.8, "ram_available_mb": 100176.3, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.98, "peak": 15.11, "min": 14.22}, "VDD_GPU": {"avg": 26.92, "peak": 38.21, "min": 23.24}, "VIN": {"avg": 69.7, "peak": 113.79, "min": 51.91}, "VDD_CPU_SOC_MSS": {"avg": 15.59, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 26.92, "energy_joules_est": 101.83, "sample_count": 29, "duration_seconds": 3.783}, "timestamp": "2026-01-16T15:47:02.237579"}
{"image_index": 182, "image_name": "000000018491.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018491.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2558.631, "latencies_ms": [2558.631], "images_per_second": 0.391, "prompt_tokens": 21, "response_tokens_est": 69, "n_tiles": 6, "output_text": "The image captures a moment during a baseball game, with players in action on the field. The scene is set in a baseball field with a dirt infield and grass surrounding it, and a chain-link fence in the background. The focus is on a player sliding into first base, while another player is in the process of throwing the ball.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.8, "ram_available_mb": 100176.3, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 25595.8, "ram_available_mb": 100176.4, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.75, "peak": 15.01, "min": 14.02}, "VDD_GPU": {"avg": 28.36, "peak": 38.21, "min": 23.24}, "VIN": {"avg": 69.31, "peak": 117.18, "min": 55.51}, "VDD_CPU_SOC_MSS": {"avg": 15.25, "peak": 15.75, "min": 14.57}}, "power_watts_avg": 28.36, "energy_joules_est": 72.57, "sample_count": 19, "duration_seconds": 2.559}, "timestamp": "2026-01-16T15:47:04.802637"}
{"image_index": 182, "image_name": "000000018491.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018491.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2050.966, "latencies_ms": [2050.966], "images_per_second": 0.488, "prompt_tokens": 19, "response_tokens_est": 51, "n_tiles": 6, "output_text": "The image captures a baseball game in action, with players in dark and light-colored uniforms, and a clear blue sky overhead. The lighting is bright, indicating it is daytime, and the field is well-maintained with green grass and brown dirt.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.8, "ram_available_mb": 100176.4, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 25595.8, "ram_available_mb": 100176.4, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.69, "peak": 15.01, "min": 14.12}, "VDD_GPU": {"avg": 29.75, "peak": 38.21, "min": 23.64}, "VIN": {"avg": 68.91, "peak": 100.79, "min": 56.32}, "VDD_CPU_SOC_MSS": {"avg": 15.25, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 29.75, "energy_joules_est": 61.03, "sample_count": 15, "duration_seconds": 2.051}, "timestamp": "2026-01-16T15:47:06.860029"}
{"image_index": 183, "image_name": "000000018519.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018519.jpg", "image_width": 515, "image_height": 640, "image_resolution": "515x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2150.06, "latencies_ms": [2150.06], "images_per_second": 0.465, "prompt_tokens": 9, "response_tokens_est": 22, "n_tiles": 12, "output_text": "A skateboarder is performing a trick on a concrete ramp, with his shadow visible on the ground.", "error": null, "sys_before": {"cpu_percent": 36.7, "ram_used_mb": 25595.8, "ram_available_mb": 100176.4, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25595.8, "ram_available_mb": 100176.4, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.14, "peak": 16.05, "min": 13.79}, "VDD_GPU": {"avg": 35.72, "peak": 40.97, "min": 28.77}, "VIN": {"avg": 83.43, "peak": 130.91, "min": 58.57}, "VDD_CPU_SOC_MSS": {"avg": 16.02, "peak": 17.32, "min": 14.96}}, "power_watts_avg": 35.72, "energy_joules_est": 76.81, "sample_count": 16, "duration_seconds": 2.15}, "timestamp": "2026-01-16T15:47:09.104884"}
{"image_index": 183, "image_name": "000000018519.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018519.jpg", "image_width": 515, "image_height": 640, "image_resolution": "515x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 3135.533, "latencies_ms": [3135.533], "images_per_second": 0.319, "prompt_tokens": 23, "response_tokens_est": 48, "n_tiles": 12, "output_text": "1. Skateboarder\n2. Helmet\n3. Gloves\n4. Skateboard\n5. Concrete wall\n6. Skateboard ramp\n7. Skateboard\n8. Skateboarder", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.8, "ram_available_mb": 100176.4, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25596.0, "ram_available_mb": 100176.1, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.11, "peak": 15.65, "min": 14.43}, "VDD_GPU": {"avg": 34.42, "peak": 42.94, "min": 26.79}, "VIN": {"avg": 74.77, "peak": 116.05, "min": 49.6}, "VDD_CPU_SOC_MSS": {"avg": 15.76, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 34.42, "energy_joules_est": 107.94, "sample_count": 24, "duration_seconds": 3.136}, "timestamp": "2026-01-16T15:47:12.247853"}
{"image_index": 183, "image_name": "000000018519.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018519.jpg", "image_width": 515, "image_height": 640, "image_resolution": "515x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 4998.086, "latencies_ms": [4998.086], "images_per_second": 0.2, "prompt_tokens": 27, "response_tokens_est": 103, "n_tiles": 12, "output_text": "The main object in the foreground is a skateboarder performing a trick on a concrete ramp. The skateboarder is positioned near the center of the image, with their skateboard and skateboarder's body extending towards the right side of the frame. In the background, there is a metal fence and a tree, which are further away from the skateboarder. The skateboarder's shadow is cast on the concrete surface, indicating that the light source is coming from the left side of the frame.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 25596.0, "ram_available_mb": 100176.1, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 25596.0, "ram_available_mb": 100176.2, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.11, "peak": 15.75, "min": 14.53}, "VDD_GPU": {"avg": 31.15, "peak": 42.94, "min": 26.0}, "VIN": {"avg": 73.43, "peak": 114.79, "min": 58.09}, "VDD_CPU_SOC_MSS": {"avg": 15.72, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 31.15, "energy_joules_est": 155.71, "sample_count": 39, "duration_seconds": 4.999}, "timestamp": "2026-01-16T15:47:17.252634"}
{"image_index": 183, "image_name": "000000018519.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018519.jpg", "image_width": 515, "image_height": 640, "image_resolution": "515x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3947.529, "latencies_ms": [3947.529], "images_per_second": 0.253, "prompt_tokens": 21, "response_tokens_est": 72, "n_tiles": 12, "output_text": "The image captures a skateboarder performing a trick on a concrete ramp in an outdoor setting. The skateboarder is wearing a black helmet, black gloves, and black shorts, and is in mid-air, showcasing a dynamic and athletic posture. The background features a metal fence, green grass, and a clear blue sky, indicating a sunny day.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.8, "ram_available_mb": 100176.4, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25595.8, "ram_available_mb": 100176.4, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.12, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 32.47, "peak": 42.54, "min": 26.01}, "VIN": {"avg": 76.58, "peak": 131.07, "min": 58.51}, "VDD_CPU_SOC_MSS": {"avg": 15.76, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 32.47, "energy_joules_est": 128.19, "sample_count": 30, "duration_seconds": 3.948}, "timestamp": "2026-01-16T15:47:21.206830"}
{"image_index": 183, "image_name": "000000018519.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018519.jpg", "image_width": 515, "image_height": 640, "image_resolution": "515x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3849.689, "latencies_ms": [3849.689], "images_per_second": 0.26, "prompt_tokens": 19, "response_tokens_est": 69, "n_tiles": 12, "output_text": "The image depicts a skateboarder performing a trick on a concrete ramp. The skateboarder is wearing a black helmet, black gloves, and black shorts, and is captured in mid-air against a bright, sunny day. The lighting is natural, casting shadows on the ground, and the skateboarder's shadow is clearly visible.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.8, "ram_available_mb": 100176.4, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25595.8, "ram_available_mb": 100176.4, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.08, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 32.66, "peak": 42.94, "min": 26.0}, "VIN": {"avg": 72.72, "peak": 109.81, "min": 58.82}, "VDD_CPU_SOC_MSS": {"avg": 15.58, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 32.66, "energy_joules_est": 125.75, "sample_count": 30, "duration_seconds": 3.85}, "timestamp": "2026-01-16T15:47:25.063050"}
{"image_index": 184, "image_name": "000000018575.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018575.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2785.726, "latencies_ms": [2785.726], "images_per_second": 0.359, "prompt_tokens": 9, "response_tokens_est": 41, "n_tiles": 12, "output_text": "The image shows a plate of food consisting of a slice of bread, a side of fries, a small bowl of pickles, a slice of tomato, and a small bowl of mayonnaise.", "error": null, "sys_before": {"cpu_percent": 42.5, "ram_used_mb": 25595.8, "ram_available_mb": 100176.4, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 25596.0, "ram_available_mb": 100176.2, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.1, "peak": 16.05, "min": 13.69}, "VDD_GPU": {"avg": 34.23, "peak": 41.36, "min": 26.79}, "VIN": {"avg": 74.63, "peak": 110.76, "min": 57.93}, "VDD_CPU_SOC_MSS": {"avg": 15.77, "peak": 16.92, "min": 14.57}}, "power_watts_avg": 34.23, "energy_joules_est": 95.37, "sample_count": 21, "duration_seconds": 2.786}, "timestamp": "2026-01-16T15:47:27.948970"}
{"image_index": 184, "image_name": "000000018575.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018575.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2940.197, "latencies_ms": [2940.197], "images_per_second": 0.34, "prompt_tokens": 23, "response_tokens_est": 42, "n_tiles": 12, "output_text": "- bread: 1\n- fries: 1\n- tomato: 1\n- pickle: 1\n- salad: 1\n- lemon: 1\n- dip: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25596.0, "ram_available_mb": 100176.2, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 25595.8, "ram_available_mb": 100176.4, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.1, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 34.97, "peak": 42.94, "min": 26.79}, "VIN": {"avg": 76.66, "peak": 114.38, "min": 58.89}, "VDD_CPU_SOC_MSS": {"avg": 15.73, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 34.97, "energy_joules_est": 102.83, "sample_count": 22, "duration_seconds": 2.94}, "timestamp": "2026-01-16T15:47:30.895230"}
{"image_index": 184, "image_name": "000000018575.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018575.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3828.991, "latencies_ms": [3828.991], "images_per_second": 0.261, "prompt_tokens": 27, "response_tokens_est": 68, "n_tiles": 12, "output_text": "The main objects in the image are a plate of food, a glass of water, and a napkin. The plate of food is placed in the foreground, with the glass of water and napkin positioned behind it. The glass of water is on the right side of the plate, while the napkin is on the left side.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.8, "ram_available_mb": 100176.4, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25596.0, "ram_available_mb": 100176.2, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.07, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 32.62, "peak": 42.54, "min": 26.0}, "VIN": {"avg": 73.82, "peak": 115.63, "min": 58.7}, "VDD_CPU_SOC_MSS": {"avg": 15.68, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 32.62, "energy_joules_est": 124.91, "sample_count": 30, "duration_seconds": 3.829}, "timestamp": "2026-01-16T15:47:34.730857"}
{"image_index": 184, "image_name": "000000018575.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018575.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 4801.322, "latencies_ms": [4801.322], "images_per_second": 0.208, "prompt_tokens": 21, "response_tokens_est": 97, "n_tiles": 12, "output_text": "The image depicts a well-arranged dining table with a variety of food items. The setting appears to be a restaurant or a dining area, as indicated by the presence of a tablecloth and a menu card. The food items on the table include a plate of French fries, a slice of bread, a pickle, a small bowl of salad, a glass of water, and a lemon. The overall scene suggests a casual dining experience, possibly in a restaurant setting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25596.0, "ram_available_mb": 100176.2, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25596.0, "ram_available_mb": 100176.2, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.1, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 31.28, "peak": 42.15, "min": 26.01}, "VIN": {"avg": 73.36, "peak": 113.57, "min": 56.11}, "VDD_CPU_SOC_MSS": {"avg": 15.67, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 31.28, "energy_joules_est": 150.2, "sample_count": 37, "duration_seconds": 4.802}, "timestamp": "2026-01-16T15:47:39.539731"}
{"image_index": 184, "image_name": "000000018575.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018575.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3648.252, "latencies_ms": [3648.252], "images_per_second": 0.274, "prompt_tokens": 19, "response_tokens_est": 63, "n_tiles": 12, "output_text": "The image features a plate of food with a white plate, including a slice of bread, a fried egg, a side of fries, a tomato, a pickle, and a lemon wedge. The lighting is bright, and the colors are vibrant, with the white plate contrasting against the colorful food items.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25596.0, "ram_available_mb": 100176.2, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 25596.0, "ram_available_mb": 100176.2, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.09, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 32.99, "peak": 42.15, "min": 26.39}, "VIN": {"avg": 72.87, "peak": 109.97, "min": 56.14}, "VDD_CPU_SOC_MSS": {"avg": 15.75, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 32.99, "energy_joules_est": 120.37, "sample_count": 28, "duration_seconds": 3.649}, "timestamp": "2026-01-16T15:47:43.194833"}
{"image_index": 185, "image_name": "000000018737.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018737.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1628.158, "latencies_ms": [1628.158], "images_per_second": 0.614, "prompt_tokens": 9, "response_tokens_est": 39, "n_tiles": 6, "output_text": "A classic red motorcycle with chrome accents is parked on a dusty road, with a wooden crate and a black seat visible on the bike, and a fence and sparse vegetation in the background.", "error": null, "sys_before": {"cpu_percent": 40.7, "ram_used_mb": 25596.0, "ram_available_mb": 100176.2, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 25596.2, "ram_available_mb": 100175.9, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.49, "peak": 15.11, "min": 13.82}, "VDD_GPU": {"avg": 31.28, "peak": 37.82, "min": 25.21}, "VIN": {"avg": 73.06, "peak": 123.09, "min": 52.19}, "VDD_CPU_SOC_MSS": {"avg": 15.26, "peak": 15.75, "min": 14.57}}, "power_watts_avg": 31.28, "energy_joules_est": 50.94, "sample_count": 12, "duration_seconds": 1.629}, "timestamp": "2026-01-16T15:47:44.883655"}
{"image_index": 185, "image_name": "000000018737.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018737.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2972.311, "latencies_ms": [2972.311], "images_per_second": 0.336, "prompt_tokens": 23, "response_tokens_est": 84, "n_tiles": 6, "output_text": "motorcycle: 1\nseat: 1\nsteering wheel: 1\nrear-view mirror: 1\nrear fender: 1\nrear tire: 1\nrear shock absorber: 1\nrear exhaust: 1\nrear bumper: 1\nrear light: 1\nrear license plate: 1\nrear storage box: 1\nrear seat: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25596.2, "ram_available_mb": 100175.9, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 25596.5, "ram_available_mb": 100175.7, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.82, "peak": 15.01, "min": 14.12}, "VDD_GPU": {"avg": 27.95, "peak": 38.6, "min": 23.24}, "VIN": {"avg": 71.76, "peak": 118.03, "min": 57.73}, "VDD_CPU_SOC_MSS": {"avg": 15.32, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 27.95, "energy_joules_est": 83.08, "sample_count": 23, "duration_seconds": 2.973}, "timestamp": "2026-01-16T15:47:47.866317"}
{"image_index": 185, "image_name": "000000018737.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018737.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2044.682, "latencies_ms": [2044.682], "images_per_second": 0.489, "prompt_tokens": 27, "response_tokens_est": 51, "n_tiles": 6, "output_text": "The main object in the foreground is a red motorcycle with a black seat and chrome details. The motorcycle is parked on a paved road, with a wooden fence visible in the background. The fence is made of vertical wooden posts connected by horizontal rails.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25596.5, "ram_available_mb": 100175.7, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 25596.1, "ram_available_mb": 100176.0, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.68, "peak": 15.01, "min": 14.02}, "VDD_GPU": {"avg": 29.59, "peak": 37.82, "min": 23.64}, "VIN": {"avg": 69.76, "peak": 111.3, "min": 62.5}, "VDD_CPU_SOC_MSS": {"avg": 15.23, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 29.59, "energy_joules_est": 60.52, "sample_count": 16, "duration_seconds": 2.045}, "timestamp": "2026-01-16T15:47:49.917571"}
{"image_index": 185, "image_name": "000000018737.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018737.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1929.904, "latencies_ms": [1929.904], "images_per_second": 0.518, "prompt_tokens": 21, "response_tokens_est": 47, "n_tiles": 6, "output_text": "The image depicts a red motorcycle parked on a dusty road with a clear blue sky in the background. The setting appears to be a desert or arid region, with sparse vegetation and a wooden fence visible in the distance.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25596.1, "ram_available_mb": 100176.0, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 25596.0, "ram_available_mb": 100176.2, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.69, "peak": 15.01, "min": 14.12}, "VDD_GPU": {"avg": 30.3, "peak": 38.21, "min": 24.04}, "VIN": {"avg": 69.38, "peak": 98.53, "min": 56.22}, "VDD_CPU_SOC_MSS": {"avg": 15.27, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 30.3, "energy_joules_est": 58.49, "sample_count": 14, "duration_seconds": 1.93}, "timestamp": "2026-01-16T15:47:51.854017"}
{"image_index": 185, "image_name": "000000018737.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018737.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1730.588, "latencies_ms": [1730.588], "images_per_second": 0.578, "prompt_tokens": 19, "response_tokens_est": 40, "n_tiles": 6, "output_text": "The motorcycle in the image is painted in a vibrant red color, with chrome accents and a shiny finish. The lighting is bright and sunny, casting a clear shadow of the motorcycle on the road.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25596.0, "ram_available_mb": 100176.2, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 25596.2, "ram_available_mb": 100175.9, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.69, "peak": 15.01, "min": 14.02}, "VDD_GPU": {"avg": 30.91, "peak": 38.21, "min": 24.42}, "VIN": {"avg": 74.85, "peak": 113.52, "min": 61.41}, "VDD_CPU_SOC_MSS": {"avg": 15.23, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 30.91, "energy_joules_est": 53.5, "sample_count": 13, "duration_seconds": 1.731}, "timestamp": "2026-01-16T15:47:53.590728"}
{"image_index": 186, "image_name": "000000018770.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018770.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1226.513, "latencies_ms": [1226.513], "images_per_second": 0.815, "prompt_tokens": 9, "response_tokens_est": 24, "n_tiles": 6, "output_text": "The image shows a close-up of a man wearing a dark suit with a white shirt and a patterned tie.", "error": null, "sys_before": {"cpu_percent": 33.9, "ram_used_mb": 25596.2, "ram_available_mb": 100175.9, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 25596.0, "ram_available_mb": 100176.2, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.42, "peak": 14.81, "min": 13.92}, "VDD_GPU": {"avg": 32.78, "peak": 37.42, "min": 27.18}, "VIN": {"avg": 73.08, "peak": 124.82, "min": 54.15}, "VDD_CPU_SOC_MSS": {"avg": 15.18, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 32.78, "energy_joules_est": 40.22, "sample_count": 9, "duration_seconds": 1.227}, "timestamp": "2026-01-16T15:47:54.865576"}
{"image_index": 186, "image_name": "000000018770.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018770.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1477.357, "latencies_ms": [1477.357], "images_per_second": 0.677, "prompt_tokens": 23, "response_tokens_est": 32, "n_tiles": 6, "output_text": "1. Man\n2. Suit\n3. Tie\n4. Shirt\n5. Background\n6. Wall\n7. Light\n8. Door", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25596.0, "ram_available_mb": 100176.2, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25596.0, "ram_available_mb": 100176.2, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.73, "peak": 15.11, "min": 14.22}, "VDD_GPU": {"avg": 33.16, "peak": 39.0, "min": 26.0}, "VIN": {"avg": 79.8, "peak": 113.24, "min": 61.71}, "VDD_CPU_SOC_MSS": {"avg": 15.35, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 33.16, "energy_joules_est": 49.0, "sample_count": 11, "duration_seconds": 1.478}, "timestamp": "2026-01-16T15:47:56.349121"}
{"image_index": 186, "image_name": "000000018770.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018770.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2091.947, "latencies_ms": [2091.947], "images_per_second": 0.478, "prompt_tokens": 27, "response_tokens_est": 55, "n_tiles": 6, "output_text": "The main object in the foreground is a man wearing a dark suit and tie. The background is blurred, making it difficult to determine the exact location. The man appears to be standing near a wall, as there is a white sign or object partially visible behind him.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25596.0, "ram_available_mb": 100176.2, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 25596.0, "ram_available_mb": 100176.2, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.95, "peak": 15.32, "min": 14.32}, "VDD_GPU": {"avg": 30.21, "peak": 38.6, "min": 24.03}, "VIN": {"avg": 70.15, "peak": 117.63, "min": 57.72}, "VDD_CPU_SOC_MSS": {"avg": 15.45, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 30.21, "energy_joules_est": 63.21, "sample_count": 16, "duration_seconds": 2.092}, "timestamp": "2026-01-16T15:47:58.447616"}
{"image_index": 186, "image_name": "000000018770.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018770.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1615.14, "latencies_ms": [1615.14], "images_per_second": 0.619, "prompt_tokens": 21, "response_tokens_est": 37, "n_tiles": 6, "output_text": "The image depicts a man dressed in a formal black suit and tie, standing in an indoor setting. The background is dark, and the man appears to be looking downwards.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25596.0, "ram_available_mb": 100176.2, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 25596.0, "ram_available_mb": 100176.2, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.82, "peak": 15.11, "min": 14.32}, "VDD_GPU": {"avg": 31.88, "peak": 38.21, "min": 25.22}, "VIN": {"avg": 77.72, "peak": 108.73, "min": 61.01}, "VDD_CPU_SOC_MSS": {"avg": 15.32, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 31.88, "energy_joules_est": 51.5, "sample_count": 12, "duration_seconds": 1.616}, "timestamp": "2026-01-16T15:48:00.068756"}
{"image_index": 186, "image_name": "000000018770.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018770.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2268.564, "latencies_ms": [2268.564], "images_per_second": 0.441, "prompt_tokens": 19, "response_tokens_est": 59, "n_tiles": 6, "output_text": "The man in the image is wearing a dark suit with a white shirt and a dark tie. The lighting is soft and even, highlighting the contours of his face and the texture of his suit. The background is dark, providing a stark contrast that makes the man's attire stand out.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25596.0, "ram_available_mb": 100176.2, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 25596.2, "ram_available_mb": 100176.0, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.75, "peak": 15.01, "min": 14.12}, "VDD_GPU": {"avg": 29.43, "peak": 38.6, "min": 23.25}, "VIN": {"avg": 72.77, "peak": 111.14, "min": 50.72}, "VDD_CPU_SOC_MSS": {"avg": 15.31, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 29.43, "energy_joules_est": 66.77, "sample_count": 17, "duration_seconds": 2.269}, "timestamp": "2026-01-16T15:48:02.348218"}
{"image_index": 187, "image_name": "000000018833.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018833.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1205.328, "latencies_ms": [1205.328], "images_per_second": 0.83, "prompt_tokens": 9, "response_tokens_est": 23, "n_tiles": 6, "output_text": "A cat is sleeping on a pair of sneakers, with its head resting on the toe of one shoe.", "error": null, "sys_before": {"cpu_percent": 42.5, "ram_used_mb": 25596.2, "ram_available_mb": 100176.0, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25596.0, "ram_available_mb": 100176.2, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.29, "peak": 14.81, "min": 13.82}, "VDD_GPU": {"avg": 32.21, "peak": 37.42, "min": 26.8}, "VIN": {"avg": 78.85, "peak": 117.08, "min": 60.14}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 32.21, "energy_joules_est": 38.83, "sample_count": 9, "duration_seconds": 1.206}, "timestamp": "2026-01-16T15:48:03.613814"}
{"image_index": 187, "image_name": "000000018833.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018833.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 924.881, "latencies_ms": [924.881], "images_per_second": 1.081, "prompt_tokens": 23, "response_tokens_est": 11, "n_tiles": 6, "output_text": "cat: 1\nshoe: 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25596.0, "ram_available_mb": 100176.2, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 25596.0, "ram_available_mb": 100176.2, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.58, "peak": 14.83, "min": 14.22}, "VDD_GPU": {"avg": 36.63, "peak": 39.0, "min": 33.88}, "VIN": {"avg": 77.83, "peak": 99.78, "min": 62.88}, "VDD_CPU_SOC_MSS": {"avg": 15.15, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 36.63, "energy_joules_est": 33.89, "sample_count": 6, "duration_seconds": 0.925}, "timestamp": "2026-01-16T15:48:04.544720"}
{"image_index": 187, "image_name": "000000018833.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018833.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1851.866, "latencies_ms": [1851.866], "images_per_second": 0.54, "prompt_tokens": 27, "response_tokens_est": 44, "n_tiles": 6, "output_text": "The main object in the foreground is a cat lying on a shoe. The shoe is positioned near the cat, with its laces visible. The background is out of focus, emphasizing the cat and the shoe.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25596.0, "ram_available_mb": 100176.2, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 25595.7, "ram_available_mb": 100176.5, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.58, "peak": 15.01, "min": 14.02}, "VDD_GPU": {"avg": 31.94, "peak": 40.16, "min": 24.04}, "VIN": {"avg": 68.03, "peak": 87.67, "min": 63.67}, "VDD_CPU_SOC_MSS": {"avg": 15.13, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 31.94, "energy_joules_est": 59.17, "sample_count": 14, "duration_seconds": 1.852}, "timestamp": "2026-01-16T15:48:06.403096"}
{"image_index": 187, "image_name": "000000018833.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018833.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2495.92, "latencies_ms": [2495.92], "images_per_second": 0.401, "prompt_tokens": 21, "response_tokens_est": 67, "n_tiles": 6, "output_text": "The image depicts a serene scene of a cat resting on a pair of sneakers. The cat appears to be sleeping peacefully, with its head resting on the white sole of the sneakers. The setting is indoors, likely in a home or a cozy space, with a soft, textured wall in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.7, "ram_available_mb": 100176.5, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 25595.7, "ram_available_mb": 100176.5, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.77, "peak": 15.01, "min": 14.02}, "VDD_GPU": {"avg": 28.47, "peak": 38.19, "min": 23.24}, "VIN": {"avg": 70.16, "peak": 103.84, "min": 56.67}, "VDD_CPU_SOC_MSS": {"avg": 15.37, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 28.47, "energy_joules_est": 71.07, "sample_count": 19, "duration_seconds": 2.496}, "timestamp": "2026-01-16T15:48:08.905227"}
{"image_index": 187, "image_name": "000000018833.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018833.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1850.863, "latencies_ms": [1850.863], "images_per_second": 0.54, "prompt_tokens": 19, "response_tokens_est": 44, "n_tiles": 6, "output_text": "The image features a gray and white cat lying on a pair of sneakers. The sneakers are light-colored with a black sole and white laces. The lighting is soft and natural, suggesting a daytime setting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.7, "ram_available_mb": 100176.5, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 25595.7, "ram_available_mb": 100176.5, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.64, "peak": 15.01, "min": 14.02}, "VDD_GPU": {"avg": 30.48, "peak": 38.21, "min": 24.04}, "VIN": {"avg": 71.74, "peak": 105.72, "min": 62.28}, "VDD_CPU_SOC_MSS": {"avg": 15.24, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 30.48, "energy_joules_est": 56.43, "sample_count": 14, "duration_seconds": 1.851}, "timestamp": "2026-01-16T15:48:10.763337"}
{"image_index": 188, "image_name": "000000018837.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018837.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2758.262, "latencies_ms": [2758.262], "images_per_second": 0.363, "prompt_tokens": 9, "response_tokens_est": 40, "n_tiles": 12, "output_text": "A green garbage truck is parked on the street, with two workers wearing safety vests and helmets, one of whom is standing next to the truck, and another is sitting inside the truck.", "error": null, "sys_before": {"cpu_percent": 44.0, "ram_used_mb": 25595.7, "ram_available_mb": 100176.5, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 25595.7, "ram_available_mb": 100176.5, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.1, "peak": 16.05, "min": 13.69}, "VDD_GPU": {"avg": 33.69, "peak": 40.97, "min": 26.79}, "VIN": {"avg": 77.85, "peak": 112.87, "min": 59.19}, "VDD_CPU_SOC_MSS": {"avg": 15.82, "peak": 17.32, "min": 14.57}}, "power_watts_avg": 33.69, "energy_joules_est": 92.94, "sample_count": 21, "duration_seconds": 2.759}, "timestamp": "2026-01-16T15:48:13.614885"}
{"image_index": 188, "image_name": "000000018837.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018837.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 3353.17, "latencies_ms": [3353.17], "images_per_second": 0.298, "prompt_tokens": 23, "response_tokens_est": 54, "n_tiles": 12, "output_text": "1. Green truck\n2. Person in green vest\n3. Person in green hat\n4. Person in green jacket\n5. Person in green jacket\n6. Person in green jacket\n7. Person in green jacket\n8. Person in green jacket", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.7, "ram_available_mb": 100176.5, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 25595.7, "ram_available_mb": 100176.5, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.07, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 33.73, "peak": 42.94, "min": 26.39}, "VIN": {"avg": 73.39, "peak": 119.22, "min": 58.88}, "VDD_CPU_SOC_MSS": {"avg": 15.62, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 33.73, "energy_joules_est": 113.12, "sample_count": 26, "duration_seconds": 3.354}, "timestamp": "2026-01-16T15:48:16.974648"}
{"image_index": 188, "image_name": "000000018837.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018837.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 4672.633, "latencies_ms": [4672.633], "images_per_second": 0.214, "prompt_tokens": 27, "response_tokens_est": 93, "n_tiles": 12, "output_text": "The main object in the foreground is a green truck with a person wearing a safety vest and a cap. The truck is parked on the street, with its front facing the camera. The person is standing near the truck's front wheel, possibly inspecting or working on it. In the background, there is a building with a sign that reads \"Power Exchange.\" The truck is parked on the street, and there are other vehicles and trees visible in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.7, "ram_available_mb": 100176.5, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 25595.7, "ram_available_mb": 100176.5, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.1, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 31.45, "peak": 42.54, "min": 26.0}, "VIN": {"avg": 72.08, "peak": 119.63, "min": 58.32}, "VDD_CPU_SOC_MSS": {"avg": 15.56, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 31.45, "energy_joules_est": 146.98, "sample_count": 37, "duration_seconds": 4.673}, "timestamp": "2026-01-16T15:48:21.653763"}
{"image_index": 188, "image_name": "000000018837.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018837.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3518.975, "latencies_ms": [3518.975], "images_per_second": 0.284, "prompt_tokens": 21, "response_tokens_est": 59, "n_tiles": 12, "output_text": "The image depicts a scene on a city street where a green garbage truck is parked. Two individuals, one wearing a green vest and the other in a yellow cap, are seen interacting with the truck. The setting appears to be a busy urban area with buildings and trees in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.7, "ram_available_mb": 100176.5, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25595.7, "ram_available_mb": 100176.5, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.08, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 33.26, "peak": 42.92, "min": 26.39}, "VIN": {"avg": 74.99, "peak": 117.89, "min": 58.5}, "VDD_CPU_SOC_MSS": {"avg": 15.57, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 33.26, "energy_joules_est": 117.06, "sample_count": 27, "duration_seconds": 3.519}, "timestamp": "2026-01-16T15:48:25.179405"}
{"image_index": 188, "image_name": "000000018837.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000018837.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2735.315, "latencies_ms": [2735.315], "images_per_second": 0.366, "prompt_tokens": 19, "response_tokens_est": 36, "n_tiles": 12, "output_text": "The image features a green garbage truck with a red and white striped front bumper. The truck is parked on a street with a clear sky overhead, indicating a sunny day.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.7, "ram_available_mb": 100176.5, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 25595.7, "ram_available_mb": 100176.5, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.09, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 35.32, "peak": 42.94, "min": 27.18}, "VIN": {"avg": 76.18, "peak": 112.43, "min": 58.31}, "VDD_CPU_SOC_MSS": {"avg": 15.61, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 35.32, "energy_joules_est": 96.63, "sample_count": 21, "duration_seconds": 2.736}, "timestamp": "2026-01-16T15:48:27.921126"}
{"image_index": 189, "image_name": "000000019042.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019042.jpg", "image_width": 640, "image_height": 371, "image_resolution": "640x371", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1151.508, "latencies_ms": [1151.508], "images_per_second": 0.868, "prompt_tokens": 9, "response_tokens_est": 21, "n_tiles": 6, "output_text": "A solitary bird stands on a rocky shoreline, surrounded by a calm river with gentle ripples.", "error": null, "sys_before": {"cpu_percent": 36.1, "ram_used_mb": 25595.7, "ram_available_mb": 100176.5, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25595.9, "ram_available_mb": 100176.3, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.32, "peak": 14.81, "min": 13.82}, "VDD_GPU": {"avg": 33.35, "peak": 37.82, "min": 27.57}, "VIN": {"avg": 73.91, "peak": 98.05, "min": 54.3}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 33.35, "energy_joules_est": 38.42, "sample_count": 9, "duration_seconds": 1.152}, "timestamp": "2026-01-16T15:48:29.136445"}
{"image_index": 189, "image_name": "000000019042.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019042.jpg", "image_width": 640, "image_height": 371, "image_resolution": "640x371", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1738.403, "latencies_ms": [1738.403], "images_per_second": 0.575, "prompt_tokens": 23, "response_tokens_est": 40, "n_tiles": 6, "output_text": "object: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.9, "ram_available_mb": 100176.3, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25596.2, "ram_available_mb": 100176.0, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.63, "peak": 14.91, "min": 14.02}, "VDD_GPU": {"avg": 31.6, "peak": 39.38, "min": 24.42}, "VIN": {"avg": 72.3, "peak": 104.58, "min": 62.23}, "VDD_CPU_SOC_MSS": {"avg": 15.17, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 31.6, "energy_joules_est": 54.95, "sample_count": 13, "duration_seconds": 1.739}, "timestamp": "2026-01-16T15:48:30.881041"}
{"image_index": 189, "image_name": "000000019042.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019042.jpg", "image_width": 640, "image_height": 371, "image_resolution": "640x371", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2209.453, "latencies_ms": [2209.453], "images_per_second": 0.453, "prompt_tokens": 27, "response_tokens_est": 57, "n_tiles": 6, "output_text": "The main objects in the image are a river, a bridge, and a bird standing on the riverbank. The bird is positioned near the riverbank, while the bridge is in the background. The foreground consists of the river and the bird, while the background features the bridge.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25596.2, "ram_available_mb": 100176.0, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 25596.2, "ram_available_mb": 100176.0, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.73, "peak": 15.11, "min": 14.02}, "VDD_GPU": {"avg": 29.4, "peak": 38.19, "min": 23.64}, "VIN": {"avg": 70.46, "peak": 99.82, "min": 58.23}, "VDD_CPU_SOC_MSS": {"avg": 15.31, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 29.4, "energy_joules_est": 64.97, "sample_count": 17, "duration_seconds": 2.21}, "timestamp": "2026-01-16T15:48:33.097824"}
{"image_index": 189, "image_name": "000000019042.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019042.jpg", "image_width": 640, "image_height": 371, "image_resolution": "640x371", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2069.64, "latencies_ms": [2069.64], "images_per_second": 0.483, "prompt_tokens": 21, "response_tokens_est": 52, "n_tiles": 6, "output_text": "The image depicts a serene river scene with a rocky shoreline in the foreground. A solitary bird is standing on the rocks, seemingly surveying its surroundings. The setting is peaceful, with the bird appearing to be at ease in its natural habitat.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25596.2, "ram_available_mb": 100176.0, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 25595.9, "ram_available_mb": 100176.3, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.73, "peak": 15.01, "min": 14.12}, "VDD_GPU": {"avg": 29.6, "peak": 37.82, "min": 23.64}, "VIN": {"avg": 63.77, "peak": 76.72, "min": 55.84}, "VDD_CPU_SOC_MSS": {"avg": 15.3, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 29.6, "energy_joules_est": 61.27, "sample_count": 16, "duration_seconds": 2.07}, "timestamp": "2026-01-16T15:48:35.173990"}
{"image_index": 189, "image_name": "000000019042.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019042.jpg", "image_width": 640, "image_height": 371, "image_resolution": "640x371", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2206.044, "latencies_ms": [2206.044], "images_per_second": 0.453, "prompt_tokens": 19, "response_tokens_est": 57, "n_tiles": 6, "output_text": "The image depicts a serene river scene with a clear blue sky overhead. The lighting is bright and natural, casting soft shadows on the rocky riverbed. The overall atmosphere is calm and peaceful, with the clear water reflecting the surrounding greenery and the bridge in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.9, "ram_available_mb": 100176.3, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 25595.9, "ram_available_mb": 100176.3, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.75, "peak": 15.11, "min": 14.02}, "VDD_GPU": {"avg": 29.2, "peak": 37.82, "min": 23.64}, "VIN": {"avg": 69.28, "peak": 117.24, "min": 57.21}, "VDD_CPU_SOC_MSS": {"avg": 15.33, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 29.2, "energy_joules_est": 64.43, "sample_count": 17, "duration_seconds": 2.207}, "timestamp": "2026-01-16T15:48:37.386018"}
{"image_index": 190, "image_name": "000000019109.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019109.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1439.267, "latencies_ms": [1439.267], "images_per_second": 0.695, "prompt_tokens": 9, "response_tokens_est": 32, "n_tiles": 6, "output_text": "A row of parked motorcycles is lined up in front of a building with a red awning, while a few people are seen walking around the area.", "error": null, "sys_before": {"cpu_percent": 41.5, "ram_used_mb": 25595.9, "ram_available_mb": 100176.3, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 25596.1, "ram_available_mb": 100176.0, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.52, "peak": 15.11, "min": 13.92}, "VDD_GPU": {"avg": 31.12, "peak": 37.42, "min": 25.6}, "VIN": {"avg": 67.89, "peak": 78.89, "min": 61.02}, "VDD_CPU_SOC_MSS": {"avg": 15.25, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 31.12, "energy_joules_est": 44.8, "sample_count": 11, "duration_seconds": 1.44}, "timestamp": "2026-01-16T15:48:38.875948"}
{"image_index": 190, "image_name": "000000019109.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019109.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2332.19, "latencies_ms": [2332.19], "images_per_second": 0.429, "prompt_tokens": 23, "response_tokens_est": 64, "n_tiles": 6, "output_text": "1. Motorcycles: 8\n2. Motorcycle: 1\n3. Motorcycle: 1\n4. Motorcycle: 1\n5. Motorcycle: 1\n6. Motorcycle: 1\n7. Motorcycle: 1\n8. Motorcycle: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25596.1, "ram_available_mb": 100176.0, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 25595.9, "ram_available_mb": 100176.3, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.98, "peak": 15.32, "min": 14.22}, "VDD_GPU": {"avg": 29.41, "peak": 38.59, "min": 24.03}, "VIN": {"avg": 75.25, "peak": 121.16, "min": 63.24}, "VDD_CPU_SOC_MSS": {"avg": 15.66, "peak": 16.14, "min": 14.96}}, "power_watts_avg": 29.41, "energy_joules_est": 68.6, "sample_count": 18, "duration_seconds": 2.333}, "timestamp": "2026-01-16T15:48:41.213971"}
{"image_index": 190, "image_name": "000000019109.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019109.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2174.461, "latencies_ms": [2174.461], "images_per_second": 0.46, "prompt_tokens": 27, "response_tokens_est": 58, "n_tiles": 6, "output_text": "The main objects in the image are a row of parked motorcycles and a building with a red awning. The motorcycles are parked in the foreground, with their fronts facing the camera. The building with the red awning is located in the background, slightly to the right.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.9, "ram_available_mb": 100176.3, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 25596.4, "ram_available_mb": 100175.8, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.98, "peak": 15.32, "min": 14.32}, "VDD_GPU": {"avg": 29.89, "peak": 38.19, "min": 24.03}, "VIN": {"avg": 72.96, "peak": 111.93, "min": 55.54}, "VDD_CPU_SOC_MSS": {"avg": 15.62, "peak": 16.14, "min": 14.96}}, "power_watts_avg": 29.89, "energy_joules_est": 65.0, "sample_count": 16, "duration_seconds": 2.175}, "timestamp": "2026-01-16T15:48:43.394328"}
{"image_index": 190, "image_name": "000000019109.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019109.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2758.909, "latencies_ms": [2758.909], "images_per_second": 0.362, "prompt_tokens": 21, "response_tokens_est": 80, "n_tiles": 6, "output_text": "The image depicts a street scene in a European city, likely Paris, given the presence of the word \"PARIS\" on a sign. A row of parked motorcycles is lined up along the curb, with a few people visible in the background. The setting appears to be a busy urban area with shops and cafes nearby, as indicated by the signage and the presence of people.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25596.4, "ram_available_mb": 100175.8, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 25596.1, "ram_available_mb": 100176.0, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.03, "peak": 15.32, "min": 14.32}, "VDD_GPU": {"avg": 28.46, "peak": 38.21, "min": 23.64}, "VIN": {"avg": 70.63, "peak": 101.11, "min": 63.07}, "VDD_CPU_SOC_MSS": {"avg": 15.62, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 28.46, "energy_joules_est": 78.53, "sample_count": 21, "duration_seconds": 2.759}, "timestamp": "2026-01-16T15:48:46.163345"}
{"image_index": 190, "image_name": "000000019109.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019109.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1990.127, "latencies_ms": [1990.127], "images_per_second": 0.502, "prompt_tokens": 19, "response_tokens_est": 51, "n_tiles": 6, "output_text": "The image depicts a row of parked motorcycles lined up against a backdrop of a historic building. The scene is illuminated by natural daylight, casting soft shadows on the pavement. The motorcycles are predominantly black, with some featuring reflective surfaces.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25596.1, "ram_available_mb": 100176.0, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 25596.4, "ram_available_mb": 100175.8, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.87, "peak": 15.22, "min": 14.22}, "VDD_GPU": {"avg": 30.31, "peak": 38.6, "min": 24.42}, "VIN": {"avg": 70.98, "peak": 96.91, "min": 63.24}, "VDD_CPU_SOC_MSS": {"avg": 15.43, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 30.31, "energy_joules_est": 60.34, "sample_count": 15, "duration_seconds": 1.991}, "timestamp": "2026-01-16T15:48:48.160533"}
{"image_index": 191, "image_name": "000000019221.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019221.jpg", "image_width": 640, "image_height": 478, "image_resolution": "640x478", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2512.208, "latencies_ms": [2512.208], "images_per_second": 0.398, "prompt_tokens": 9, "response_tokens_est": 33, "n_tiles": 12, "output_text": "The image shows a close-up of a person's hand holding a small, green, broccoli-like vegetable, with a blurred background suggesting an outdoor setting.", "error": null, "sys_before": {"cpu_percent": 42.3, "ram_used_mb": 25596.4, "ram_available_mb": 100175.8, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 25596.1, "ram_available_mb": 100176.1, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.17, "peak": 16.05, "min": 14.1}, "VDD_GPU": {"avg": 34.85, "peak": 40.97, "min": 27.57}, "VIN": {"avg": 76.55, "peak": 115.99, "min": 58.58}, "VDD_CPU_SOC_MSS": {"avg": 16.12, "peak": 17.32, "min": 15.36}}, "power_watts_avg": 34.85, "energy_joules_est": 87.56, "sample_count": 19, "duration_seconds": 2.513}, "timestamp": "2026-01-16T15:48:50.745798"}
{"image_index": 191, "image_name": "000000019221.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019221.jpg", "image_width": 640, "image_height": 478, "image_resolution": "640x478", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1901.816, "latencies_ms": [1901.816], "images_per_second": 0.526, "prompt_tokens": 23, "response_tokens_est": 11, "n_tiles": 12, "output_text": "broccoli: 1\npot: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25596.1, "ram_available_mb": 100176.1, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 2.0, "ram_used_mb": 25596.1, "ram_available_mb": 100176.1, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.14, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 38.24, "peak": 42.15, "min": 31.91}, "VIN": {"avg": 78.7, "peak": 116.15, "min": 58.56}, "VDD_CPU_SOC_MSS": {"avg": 15.89, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 38.24, "energy_joules_est": 72.74, "sample_count": 14, "duration_seconds": 1.902}, "timestamp": "2026-01-16T15:48:52.654385"}
{"image_index": 191, "image_name": "000000019221.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019221.jpg", "image_width": 640, "image_height": 478, "image_resolution": "640x478", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3803.604, "latencies_ms": [3803.604], "images_per_second": 0.263, "prompt_tokens": 27, "response_tokens_est": 68, "n_tiles": 12, "output_text": "The main object in the foreground is a person's hand holding a broccoli. The broccoli is positioned in the center of the image, with the person's hand slightly to the right and in the foreground. The background is blurred, with a dark, indistinct area that appears to be a wall or a dark surface.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 25596.1, "ram_available_mb": 100176.1, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25596.1, "ram_available_mb": 100176.1, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.11, "peak": 15.65, "min": 14.43}, "VDD_GPU": {"avg": 33.45, "peak": 44.12, "min": 26.38}, "VIN": {"avg": 74.47, "peak": 116.18, "min": 58.65}, "VDD_CPU_SOC_MSS": {"avg": 15.71, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 33.45, "energy_joules_est": 127.24, "sample_count": 29, "duration_seconds": 3.804}, "timestamp": "2026-01-16T15:48:56.469482"}
{"image_index": 191, "image_name": "000000019221.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019221.jpg", "image_width": 640, "image_height": 478, "image_resolution": "640x478", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3173.388, "latencies_ms": [3173.388], "images_per_second": 0.315, "prompt_tokens": 21, "response_tokens_est": 49, "n_tiles": 12, "output_text": "The image shows a close-up of a person's hand holding a small, green, broccoli-like vegetable. The background is blurred, but it appears to be an indoor setting with a dark surface, possibly a table or countertop.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25596.1, "ram_available_mb": 100176.1, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 25596.1, "ram_available_mb": 100176.1, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.11, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 34.01, "peak": 42.53, "min": 26.39}, "VIN": {"avg": 73.05, "peak": 113.38, "min": 53.86}, "VDD_CPU_SOC_MSS": {"avg": 15.68, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 34.01, "energy_joules_est": 107.94, "sample_count": 25, "duration_seconds": 3.174}, "timestamp": "2026-01-16T15:48:59.649206"}
{"image_index": 191, "image_name": "000000019221.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019221.jpg", "image_width": 640, "image_height": 478, "image_resolution": "640x478", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3711.697, "latencies_ms": [3711.697], "images_per_second": 0.269, "prompt_tokens": 19, "response_tokens_est": 64, "n_tiles": 12, "output_text": "The image features a close-up of a person's hand holding a green broccoli floret. The broccoli is bright green with small, tightly packed florets, and the hand is holding it with a slightly wrinkled skin texture. The lighting is natural, suggesting the photo was taken outdoors during the day.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25596.1, "ram_available_mb": 100176.1, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25596.1, "ram_available_mb": 100176.1, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.78, "peak": 15.34, "min": 14.12}, "VDD_GPU": {"avg": 32.68, "peak": 42.54, "min": 25.6}, "VIN": {"avg": 70.43, "peak": 96.57, "min": 58.96}, "VDD_CPU_SOC_MSS": {"avg": 15.31, "peak": 16.14, "min": 14.56}}, "power_watts_avg": 32.68, "energy_joules_est": 121.31, "sample_count": 29, "duration_seconds": 3.712}, "timestamp": "2026-01-16T15:49:03.367232"}
{"image_index": 192, "image_name": "000000019402.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019402.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2249.82, "latencies_ms": [2249.82], "images_per_second": 0.444, "prompt_tokens": 9, "response_tokens_est": 25, "n_tiles": 12, "output_text": "The image shows a person wearing a green jacket with a hood, and their face is partially obscured by the hood.", "error": null, "sys_before": {"cpu_percent": 42.7, "ram_used_mb": 25596.1, "ram_available_mb": 100176.1, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 2.6, "ram_used_mb": 25596.1, "ram_available_mb": 100176.1, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.09, "peak": 16.05, "min": 13.69}, "VDD_GPU": {"avg": 35.75, "peak": 41.36, "min": 28.36}, "VIN": {"avg": 74.38, "peak": 116.83, "min": 58.48}, "VDD_CPU_SOC_MSS": {"avg": 16.0, "peak": 17.32, "min": 14.57}}, "power_watts_avg": 35.75, "energy_joules_est": 80.44, "sample_count": 17, "duration_seconds": 2.25}, "timestamp": "2026-01-16T15:49:05.703520"}
{"image_index": 192, "image_name": "000000019402.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019402.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2610.86, "latencies_ms": [2610.86], "images_per_second": 0.383, "prompt_tokens": 23, "response_tokens_est": 32, "n_tiles": 12, "output_text": "1. Woman\n2. Hat\n3. Light\n4. Background\n5. Person\n6. Light\n7. Person\n8. Light", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25596.1, "ram_available_mb": 100176.1, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 25596.1, "ram_available_mb": 100176.1, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.06, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 35.91, "peak": 43.33, "min": 27.59}, "VIN": {"avg": 78.09, "peak": 117.15, "min": 58.49}, "VDD_CPU_SOC_MSS": {"avg": 15.71, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 35.91, "energy_joules_est": 93.77, "sample_count": 20, "duration_seconds": 2.611}, "timestamp": "2026-01-16T15:49:08.320754"}
{"image_index": 192, "image_name": "000000019402.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019402.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3149.897, "latencies_ms": [3149.897], "images_per_second": 0.317, "prompt_tokens": 27, "response_tokens_est": 48, "n_tiles": 12, "output_text": "The main object in the foreground is a person wearing a green hat. The person's face is partially visible, with their mouth open. The background is blurred, but there appears to be a lamp and some indistinct objects.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25596.1, "ram_available_mb": 100176.1, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 25596.1, "ram_available_mb": 100176.1, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.09, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 34.42, "peak": 42.92, "min": 26.39}, "VIN": {"avg": 73.87, "peak": 118.34, "min": 58.47}, "VDD_CPU_SOC_MSS": {"avg": 15.71, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 34.42, "energy_joules_est": 108.43, "sample_count": 24, "duration_seconds": 3.15}, "timestamp": "2026-01-16T15:49:11.477774"}
{"image_index": 192, "image_name": "000000019402.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019402.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2879.295, "latencies_ms": [2879.295], "images_per_second": 0.347, "prompt_tokens": 21, "response_tokens_est": 40, "n_tiles": 12, "output_text": "The image depicts a person wearing a green jacket, with their face partially obscured by the jacket. The background is blurred, but it appears to be an indoor setting with warm lighting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25596.1, "ram_available_mb": 100176.1, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 25596.1, "ram_available_mb": 100176.1, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.1, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 34.91, "peak": 42.92, "min": 26.79}, "VIN": {"avg": 75.41, "peak": 109.14, "min": 58.33}, "VDD_CPU_SOC_MSS": {"avg": 15.78, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 34.91, "energy_joules_est": 100.53, "sample_count": 22, "duration_seconds": 2.88}, "timestamp": "2026-01-16T15:49:14.363462"}
{"image_index": 192, "image_name": "000000019402.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019402.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3414.591, "latencies_ms": [3414.591], "images_per_second": 0.293, "prompt_tokens": 19, "response_tokens_est": 56, "n_tiles": 12, "output_text": "The image features a person wearing a green jacket with a fur-lined hood, creating a cozy and warm atmosphere. The lighting is dim, with a soft glow illuminating the person's face and the surrounding area, adding a sense of intimacy and mystery to the scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25596.1, "ram_available_mb": 100176.1, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 25596.1, "ram_available_mb": 100176.1, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.07, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 33.6, "peak": 42.94, "min": 26.39}, "VIN": {"avg": 74.48, "peak": 114.24, "min": 58.83}, "VDD_CPU_SOC_MSS": {"avg": 15.75, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 33.6, "energy_joules_est": 114.74, "sample_count": 26, "duration_seconds": 3.415}, "timestamp": "2026-01-16T15:49:17.784699"}
{"image_index": 193, "image_name": "000000019432.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019432.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1912.777, "latencies_ms": [1912.777], "images_per_second": 0.523, "prompt_tokens": 9, "response_tokens_est": 15, "n_tiles": 12, "output_text": "A tennis player is preparing to serve a ball on a blue court.", "error": null, "sys_before": {"cpu_percent": 44.0, "ram_used_mb": 25596.1, "ram_available_mb": 100176.1, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 2.2, "ram_used_mb": 25596.1, "ram_available_mb": 100176.1, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.14, "peak": 16.15, "min": 13.79}, "VDD_GPU": {"avg": 37.16, "peak": 41.36, "min": 30.73}, "VIN": {"avg": 85.44, "peak": 120.71, "min": 58.78}, "VDD_CPU_SOC_MSS": {"avg": 16.11, "peak": 17.32, "min": 14.96}}, "power_watts_avg": 37.16, "energy_joules_est": 71.09, "sample_count": 14, "duration_seconds": 1.913}, "timestamp": "2026-01-16T15:49:19.797440"}
{"image_index": 193, "image_name": "000000019432.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019432.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2846.494, "latencies_ms": [2846.494], "images_per_second": 0.351, "prompt_tokens": 23, "response_tokens_est": 39, "n_tiles": 12, "output_text": "1. Tennis player\n2. Tennis racket\n3. Tennis ball\n4. Court\n5. Chairs\n6. Stadium\n7. Blue surface\n8. White lines", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25596.1, "ram_available_mb": 100176.1, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 25596.3, "ram_available_mb": 100175.8, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.08, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 35.25, "peak": 43.31, "min": 26.79}, "VIN": {"avg": 75.46, "peak": 135.19, "min": 58.62}, "VDD_CPU_SOC_MSS": {"avg": 15.8, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 35.25, "energy_joules_est": 100.36, "sample_count": 22, "duration_seconds": 2.847}, "timestamp": "2026-01-16T15:49:22.650430"}
{"image_index": 193, "image_name": "000000019432.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019432.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3590.566, "latencies_ms": [3590.566], "images_per_second": 0.279, "prompt_tokens": 27, "response_tokens_est": 61, "n_tiles": 12, "output_text": "The main object in the foreground is a tennis player holding a tennis racket. The player is standing on a blue tennis court with white lines marking the playing area. In the background, there are empty white chairs arranged in a row, suggesting that the court is set up for an event or match.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25596.3, "ram_available_mb": 100175.8, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 25596.1, "ram_available_mb": 100176.1, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.11, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 33.11, "peak": 42.92, "min": 26.39}, "VIN": {"avg": 75.48, "peak": 110.95, "min": 58.74}, "VDD_CPU_SOC_MSS": {"avg": 15.7, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 33.11, "energy_joules_est": 118.9, "sample_count": 28, "duration_seconds": 3.591}, "timestamp": "2026-01-16T15:49:26.247580"}
{"image_index": 193, "image_name": "000000019432.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019432.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3552.964, "latencies_ms": [3552.964], "images_per_second": 0.281, "prompt_tokens": 21, "response_tokens_est": 60, "n_tiles": 12, "output_text": "The image depicts a tennis player on a blue court, preparing to serve the ball. The player is dressed in a white shirt and black shorts, with a tennis racket in hand. The court is surrounded by white chairs, indicating that this is a formal or semi-formal tennis match.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25596.1, "ram_available_mb": 100176.1, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25596.1, "ram_available_mb": 100176.1, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.1, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 33.17, "peak": 42.94, "min": 26.39}, "VIN": {"avg": 72.18, "peak": 100.04, "min": 58.61}, "VDD_CPU_SOC_MSS": {"avg": 15.72, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 33.17, "energy_joules_est": 117.87, "sample_count": 28, "duration_seconds": 3.553}, "timestamp": "2026-01-16T15:49:29.807206"}
{"image_index": 193, "image_name": "000000019432.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019432.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2509.53, "latencies_ms": [2509.53], "images_per_second": 0.398, "prompt_tokens": 19, "response_tokens_est": 29, "n_tiles": 12, "output_text": "The tennis court is a vibrant blue, with white lines marking the playing area. The lighting is bright and natural, suggesting it is daytime.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25596.1, "ram_available_mb": 100176.1, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 25596.1, "ram_available_mb": 100176.1, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.08, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 36.12, "peak": 42.54, "min": 27.98}, "VIN": {"avg": 80.35, "peak": 115.76, "min": 58.51}, "VDD_CPU_SOC_MSS": {"avg": 15.76, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 36.12, "energy_joules_est": 90.66, "sample_count": 19, "duration_seconds": 2.51}, "timestamp": "2026-01-16T15:49:32.323564"}
{"image_index": 194, "image_name": "000000019742.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019742.jpg", "image_width": 500, "image_height": 374, "image_resolution": "500x374", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2880.529, "latencies_ms": [2880.529], "images_per_second": 0.347, "prompt_tokens": 9, "response_tokens_est": 44, "n_tiles": 12, "output_text": "The image shows a candle with a lit wick on a white plate, placed on a wooden table, with a glass of amber-colored liquid beside it, and a string of white lights hanging above the table.", "error": null, "sys_before": {"cpu_percent": 42.1, "ram_used_mb": 25595.8, "ram_available_mb": 100176.4, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 25596.1, "ram_available_mb": 100176.1, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.15, "peak": 16.15, "min": 13.92}, "VDD_GPU": {"avg": 34.04, "peak": 41.76, "min": 26.79}, "VIN": {"avg": 77.12, "peak": 117.77, "min": 58.3}, "VDD_CPU_SOC_MSS": {"avg": 16.0, "peak": 17.32, "min": 14.96}}, "power_watts_avg": 34.04, "energy_joules_est": 98.08, "sample_count": 22, "duration_seconds": 2.881}, "timestamp": "2026-01-16T15:49:35.283568"}
{"image_index": 194, "image_name": "000000019742.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019742.jpg", "image_width": 500, "image_height": 374, "image_resolution": "500x374", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2877.385, "latencies_ms": [2877.385], "images_per_second": 0.348, "prompt_tokens": 23, "response_tokens_est": 40, "n_tiles": 12, "output_text": "object: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25596.1, "ram_available_mb": 100176.1, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 25595.8, "ram_available_mb": 100176.4, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.09, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 34.97, "peak": 42.94, "min": 26.79}, "VIN": {"avg": 75.63, "peak": 99.65, "min": 56.47}, "VDD_CPU_SOC_MSS": {"avg": 15.67, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 34.97, "energy_joules_est": 100.64, "sample_count": 22, "duration_seconds": 2.878}, "timestamp": "2026-01-16T15:49:38.167449"}
{"image_index": 194, "image_name": "000000019742.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019742.jpg", "image_width": 500, "image_height": 374, "image_resolution": "500x374", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 4220.065, "latencies_ms": [4220.065], "images_per_second": 0.237, "prompt_tokens": 27, "response_tokens_est": 80, "n_tiles": 12, "output_text": "The main objects in the image are a candle and a glass of amber-colored liquid. The candle is placed on a small, round, green plate near the left side of the image, while the glass of liquid is positioned in the foreground, slightly to the right. The candle and the glass are the central focus of the image, with the plate serving as a supporting element in the foreground.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.8, "ram_available_mb": 100176.4, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 25596.1, "ram_available_mb": 100176.1, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.11, "peak": 15.65, "min": 14.63}, "VDD_GPU": {"avg": 32.24, "peak": 42.54, "min": 26.0}, "VIN": {"avg": 72.56, "peak": 114.34, "min": 57.85}, "VDD_CPU_SOC_MSS": {"avg": 15.71, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 32.24, "energy_joules_est": 136.07, "sample_count": 33, "duration_seconds": 4.221}, "timestamp": "2026-01-16T15:49:42.395695"}
{"image_index": 194, "image_name": "000000019742.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019742.jpg", "image_width": 500, "image_height": 374, "image_resolution": "500x374", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3856.93, "latencies_ms": [3856.93], "images_per_second": 0.259, "prompt_tokens": 21, "response_tokens_est": 69, "n_tiles": 12, "output_text": "The image depicts a cozy indoor setting with a warm, inviting atmosphere. A glass of amber-colored liquid, possibly tea or a warm beverage, is placed on a small, round glass plate on a wooden table. The table is adorned with a string of small, white, string lights that add a festive touch to the scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25596.1, "ram_available_mb": 100176.1, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25595.8, "ram_available_mb": 100176.4, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.09, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 32.64, "peak": 42.94, "min": 26.39}, "VIN": {"avg": 74.06, "peak": 113.44, "min": 58.63}, "VDD_CPU_SOC_MSS": {"avg": 15.65, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 32.64, "energy_joules_est": 125.9, "sample_count": 30, "duration_seconds": 3.857}, "timestamp": "2026-01-16T15:49:46.259416"}
{"image_index": 194, "image_name": "000000019742.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019742.jpg", "image_width": 500, "image_height": 374, "image_resolution": "500x374", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3043.73, "latencies_ms": [3043.73], "images_per_second": 0.329, "prompt_tokens": 19, "response_tokens_est": 45, "n_tiles": 12, "output_text": "The image features a candle with a white base and a lit wick, placed on a green glass plate. The candle is surrounded by a string of small, white, string lights, creating a warm and cozy atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.8, "ram_available_mb": 100176.4, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 25596.1, "ram_available_mb": 100176.1, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.08, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 34.49, "peak": 42.53, "min": 26.79}, "VIN": {"avg": 71.34, "peak": 86.98, "min": 58.74}, "VDD_CPU_SOC_MSS": {"avg": 15.75, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 34.49, "energy_joules_est": 104.99, "sample_count": 23, "duration_seconds": 3.044}, "timestamp": "2026-01-16T15:49:49.313757"}
{"image_index": 195, "image_name": "000000019786.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019786.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1985.542, "latencies_ms": [1985.542], "images_per_second": 0.504, "prompt_tokens": 9, "response_tokens_est": 17, "n_tiles": 12, "output_text": "A man is playing a video game while sitting on a couch in a room.", "error": null, "sys_before": {"cpu_percent": 40.5, "ram_used_mb": 25595.8, "ram_available_mb": 100176.4, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 2.3, "ram_used_mb": 25595.8, "ram_available_mb": 100176.4, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.1, "peak": 16.15, "min": 13.69}, "VDD_GPU": {"avg": 36.76, "peak": 41.76, "min": 29.94}, "VIN": {"avg": 74.08, "peak": 106.0, "min": 58.54}, "VDD_CPU_SOC_MSS": {"avg": 16.03, "peak": 17.32, "min": 14.57}}, "power_watts_avg": 36.76, "energy_joules_est": 73.0, "sample_count": 15, "duration_seconds": 1.986}, "timestamp": "2026-01-16T15:49:51.395819"}
{"image_index": 195, "image_name": "000000019786.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019786.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2835.734, "latencies_ms": [2835.734], "images_per_second": 0.353, "prompt_tokens": 23, "response_tokens_est": 39, "n_tiles": 12, "output_text": "1. man\n2. game controller\n3. game console\n4. game controller\n5. game controller\n6. game controller\n7. game controller\n8. game controller", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25595.8, "ram_available_mb": 100176.4, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 25596.0, "ram_available_mb": 100176.1, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.11, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 35.58, "peak": 43.33, "min": 27.18}, "VIN": {"avg": 74.14, "peak": 119.36, "min": 58.73}, "VDD_CPU_SOC_MSS": {"avg": 15.84, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 35.58, "energy_joules_est": 100.91, "sample_count": 21, "duration_seconds": 2.836}, "timestamp": "2026-01-16T15:49:54.238897"}
{"image_index": 195, "image_name": "000000019786.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019786.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3306.733, "latencies_ms": [3306.733], "images_per_second": 0.302, "prompt_tokens": 27, "response_tokens_est": 53, "n_tiles": 12, "output_text": "The main object in the foreground is a black tripod stand with a white object on it. The white object appears to be a laptop. In the background, there is a person standing next to a couch. The couch is positioned behind the tripod stand.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25596.0, "ram_available_mb": 100176.1, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 25595.8, "ram_available_mb": 100176.4, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.11, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 33.84, "peak": 42.92, "min": 26.39}, "VIN": {"avg": 72.89, "peak": 116.05, "min": 58.17}, "VDD_CPU_SOC_MSS": {"avg": 15.79, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 33.84, "energy_joules_est": 111.91, "sample_count": 26, "duration_seconds": 3.307}, "timestamp": "2026-01-16T15:49:57.552706"}
{"image_index": 195, "image_name": "000000019786.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019786.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3839.146, "latencies_ms": [3839.146], "images_per_second": 0.26, "prompt_tokens": 21, "response_tokens_est": 68, "n_tiles": 12, "output_text": "The image depicts a man in a casual setting, possibly indoors, wearing a brown jacket and jeans. He is holding a white object in his hand and appears to be in motion, possibly walking or running. The background includes a couch, a table, and a piece of furniture, suggesting a living room or a similar indoor space.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.8, "ram_available_mb": 100176.4, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25595.8, "ram_available_mb": 100176.4, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.02, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 32.6, "peak": 42.94, "min": 26.0}, "VIN": {"avg": 78.73, "peak": 139.84, "min": 58.89}, "VDD_CPU_SOC_MSS": {"avg": 15.65, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 32.6, "energy_joules_est": 125.17, "sample_count": 30, "duration_seconds": 3.84}, "timestamp": "2026-01-16T15:50:01.398701"}
{"image_index": 195, "image_name": "000000019786.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019786.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3018.077, "latencies_ms": [3018.077], "images_per_second": 0.331, "prompt_tokens": 19, "response_tokens_est": 44, "n_tiles": 12, "output_text": "The image features a man in a gray shirt and dark pants standing in a room with a brown couch and a black tripod stand. The lighting is bright, and the overall atmosphere appears to be casual and comfortable.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.8, "ram_available_mb": 100176.4, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 25595.8, "ram_available_mb": 100176.4, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.76, "peak": 15.34, "min": 14.12}, "VDD_GPU": {"avg": 34.44, "peak": 42.54, "min": 26.39}, "VIN": {"avg": 72.82, "peak": 120.81, "min": 58.42}, "VDD_CPU_SOC_MSS": {"avg": 15.44, "peak": 16.14, "min": 14.96}}, "power_watts_avg": 34.44, "energy_joules_est": 103.96, "sample_count": 23, "duration_seconds": 3.019}, "timestamp": "2026-01-16T15:50:04.423919"}
{"image_index": 196, "image_name": "000000019924.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019924.jpg", "image_width": 458, "image_height": 500, "image_resolution": "458x500", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 979.567, "latencies_ms": [979.567], "images_per_second": 1.021, "prompt_tokens": 9, "response_tokens_est": 41, "n_tiles": 1, "output_text": "The image features a black-and-white photograph of a woman wearing a wide-brimmed hat, a striped tank top, and a tie, holding a cigarette in her right hand and smiling broadly.", "error": null, "sys_before": {"cpu_percent": 23.1, "ram_used_mb": 25595.8, "ram_available_mb": 100176.4, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 25595.8, "ram_available_mb": 100176.4, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 4845.7, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.0, "peak": 15.52, "min": 14.2}, "VDD_GPU": {"avg": 23.58, "peak": 27.57, "min": 21.27}, "VIN": {"avg": 63.95, "peak": 69.74, "min": 60.86}, "VDD_CPU_SOC_MSS": {"avg": 15.69, "peak": 16.54, "min": 14.96}}, "power_watts_avg": 23.58, "energy_joules_est": 23.11, "sample_count": 7, "duration_seconds": 0.98}, "timestamp": "2026-01-16T15:50:05.426829"}
{"image_index": 196, "image_name": "000000019924.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019924.jpg", "image_width": 458, "image_height": 500, "image_resolution": "458x500", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 912.55, "latencies_ms": [912.55], "images_per_second": 1.096, "prompt_tokens": 23, "response_tokens_est": 39, "n_tiles": 1, "output_text": "1. Woman\n2. Hat\n3. Cigarette\n4. Necktie\n5. Sunglasses\n6. Sunglasses\n7. Sunglasses\n8. Sunglasses", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25595.8, "ram_available_mb": 100176.4, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25596.0, "ram_available_mb": 100176.2, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 4856.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.49, "peak": 15.72, "min": 15.22}, "VDD_GPU": {"avg": 22.34, "peak": 24.82, "min": 20.88}, "VIN": {"avg": 63.14, "peak": 68.46, "min": 56.72}, "VDD_CPU_SOC_MSS": {"avg": 16.42, "peak": 16.54, "min": 16.14}}, "power_watts_avg": 22.34, "energy_joules_est": 20.4, "sample_count": 7, "duration_seconds": 0.913}, "timestamp": "2026-01-16T15:50:06.345085"}
{"image_index": 196, "image_name": "000000019924.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019924.jpg", "image_width": 458, "image_height": 500, "image_resolution": "458x500", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1217.689, "latencies_ms": [1217.689], "images_per_second": 0.821, "prompt_tokens": 27, "response_tokens_est": 53, "n_tiles": 1, "output_text": "The main object in the image is a woman wearing a wide-brimmed hat and a striped tank top. She is holding a cigarette in her right hand, which is near her mouth. The background is plain and does not contain any distinguishable features.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25596.0, "ram_available_mb": 100176.2, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25595.8, "ram_available_mb": 100176.4, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 4859.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.55, "peak": 15.72, "min": 15.22}, "VDD_GPU": {"avg": 21.8, "peak": 24.82, "min": 20.48}, "VIN": {"avg": 64.21, "peak": 72.82, "min": 59.87}, "VDD_CPU_SOC_MSS": {"avg": 16.62, "peak": 16.93, "min": 16.14}}, "power_watts_avg": 21.8, "energy_joules_est": 26.56, "sample_count": 9, "duration_seconds": 1.218}, "timestamp": "2026-01-16T15:50:07.568680"}
{"image_index": 196, "image_name": "000000019924.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019924.jpg", "image_width": 458, "image_height": 500, "image_resolution": "458x500", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1651.186, "latencies_ms": [1651.186], "images_per_second": 0.606, "prompt_tokens": 21, "response_tokens_est": 73, "n_tiles": 1, "output_text": "The image depicts a black-and-white photograph of a woman wearing a wide-brimmed hat and a striped tank top. She is holding a cigarette in her right hand and appears to be smiling while looking at the camera. The setting is not clearly defined, but the woman's attire and the cigarette suggest a casual, possibly outdoor or relaxed environment.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.8, "ram_available_mb": 100176.4, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 25595.8, "ram_available_mb": 100176.4, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 4854.7, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.63, "peak": 15.82, "min": 15.32}, "VDD_GPU": {"avg": 21.27, "peak": 24.42, "min": 20.09}, "VIN": {"avg": 66.59, "peak": 73.11, "min": 63.38}, "VDD_CPU_SOC_MSS": {"avg": 16.6, "peak": 16.93, "min": 16.14}}, "power_watts_avg": 21.27, "energy_joules_est": 35.13, "sample_count": 12, "duration_seconds": 1.652}, "timestamp": "2026-01-16T15:50:09.229996"}
{"image_index": 196, "image_name": "000000019924.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000019924.jpg", "image_width": 458, "image_height": 500, "image_resolution": "458x500", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1389.218, "latencies_ms": [1389.218], "images_per_second": 0.72, "prompt_tokens": 19, "response_tokens_est": 61, "n_tiles": 1, "output_text": "The notable visual attributes of the image include a black and white color scheme, a soft and diffused lighting that highlights the subject's features, and a woman wearing a wide-brimmed hat and a striped tank top. The hat casts a shadow over her face, adding depth to the image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.8, "ram_available_mb": 100176.4, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25596.0, "ram_available_mb": 100176.2, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 4853.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.65, "peak": 15.82, "min": 15.22}, "VDD_GPU": {"avg": 21.39, "peak": 24.03, "min": 20.1}, "VIN": {"avg": 63.94, "peak": 70.44, "min": 61.17}, "VDD_CPU_SOC_MSS": {"avg": 16.54, "peak": 16.93, "min": 16.14}}, "power_watts_avg": 21.39, "energy_joules_est": 29.73, "sample_count": 10, "duration_seconds": 1.39}, "timestamp": "2026-01-16T15:50:10.629222"}
{"image_index": 197, "image_name": "000000020059.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000020059.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 989.845, "latencies_ms": [989.845], "images_per_second": 1.01, "prompt_tokens": 9, "response_tokens_est": 15, "n_tiles": 6, "output_text": "A zebra is grazing on the grass in a natural habitat.", "error": null, "sys_before": {"cpu_percent": 40.4, "ram_used_mb": 25595.8, "ram_available_mb": 100176.4, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 25595.8, "ram_available_mb": 100176.4, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.5, "peak": 14.73, "min": 14.02}, "VDD_GPU": {"avg": 32.81, "peak": 36.24, "min": 29.15}, "VIN": {"avg": 73.87, "peak": 118.95, "min": 61.11}, "VDD_CPU_SOC_MSS": {"avg": 15.46, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 32.81, "energy_joules_est": 32.49, "sample_count": 7, "duration_seconds": 0.99}, "timestamp": "2026-01-16T15:50:11.661068"}
{"image_index": 197, "image_name": "000000020059.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000020059.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1194.227, "latencies_ms": [1194.227], "images_per_second": 0.837, "prompt_tokens": 23, "response_tokens_est": 21, "n_tiles": 6, "output_text": "zebra: 1\nrock: 1\ntree: 1\nwall: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.8, "ram_available_mb": 100176.4, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25596.0, "ram_available_mb": 100176.2, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.54, "peak": 14.91, "min": 14.12}, "VDD_GPU": {"avg": 35.01, "peak": 39.39, "min": 27.97}, "VIN": {"avg": 72.53, "peak": 120.11, "min": 60.71}, "VDD_CPU_SOC_MSS": {"avg": 15.13, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 35.01, "energy_joules_est": 41.82, "sample_count": 9, "duration_seconds": 1.195}, "timestamp": "2026-01-16T15:50:12.861194"}
{"image_index": 197, "image_name": "000000020059.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000020059.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2945.301, "latencies_ms": [2945.301], "images_per_second": 0.34, "prompt_tokens": 27, "response_tokens_est": 83, "n_tiles": 6, "output_text": "The main object in the foreground is a zebra, which is positioned near the center of the image. The zebra is the focal point, with its distinctive black and white stripes clearly visible. In the background, there is another zebra, slightly out of focus, which is positioned further away. The zebra in the background is also grazing, contributing to the overall scene of a zebra enclosure.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25596.0, "ram_available_mb": 100176.2, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 25596.0, "ram_available_mb": 100176.2, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.79, "peak": 15.11, "min": 14.02}, "VDD_GPU": {"avg": 28.18, "peak": 39.38, "min": 23.24}, "VIN": {"avg": 67.57, "peak": 106.2, "min": 53.11}, "VDD_CPU_SOC_MSS": {"avg": 15.3, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 28.18, "energy_joules_est": 83.01, "sample_count": 22, "duration_seconds": 2.946}, "timestamp": "2026-01-16T15:50:15.812468"}
{"image_index": 197, "image_name": "000000020059.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000020059.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2598.982, "latencies_ms": [2598.982], "images_per_second": 0.385, "prompt_tokens": 21, "response_tokens_est": 71, "n_tiles": 6, "output_text": "The image depicts a serene scene in a natural habitat, likely a zoo or wildlife reserve, where a zebra is grazing on the lush green grass. The zebra is the central focus, with its distinctive black and white stripes clearly visible. The background features a rocky terrain and a few trees, providing a natural backdrop to the scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25596.0, "ram_available_mb": 100176.2, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 25595.8, "ram_available_mb": 100176.4, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.78, "peak": 15.11, "min": 14.02}, "VDD_GPU": {"avg": 28.38, "peak": 38.21, "min": 23.24}, "VIN": {"avg": 65.58, "peak": 113.09, "min": 55.65}, "VDD_CPU_SOC_MSS": {"avg": 15.26, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 28.38, "energy_joules_est": 73.77, "sample_count": 20, "duration_seconds": 2.599}, "timestamp": "2026-01-16T15:50:18.417579"}
{"image_index": 197, "image_name": "000000020059.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000020059.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1791.293, "latencies_ms": [1791.293], "images_per_second": 0.558, "prompt_tokens": 19, "response_tokens_est": 42, "n_tiles": 6, "output_text": "The image features a zebra standing in a lush, green meadow. The zebra has a striking black and white striped pattern, and the lighting is bright and natural, suggesting it is daytime.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.8, "ram_available_mb": 100176.4, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 25596.0, "ram_available_mb": 100176.2, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.69, "peak": 15.01, "min": 14.12}, "VDD_GPU": {"avg": 30.39, "peak": 37.8, "min": 24.04}, "VIN": {"avg": 69.19, "peak": 116.53, "min": 55.95}, "VDD_CPU_SOC_MSS": {"avg": 15.18, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 30.39, "energy_joules_est": 54.45, "sample_count": 14, "duration_seconds": 1.792}, "timestamp": "2026-01-16T15:50:20.214804"}
{"image_index": 198, "image_name": "000000020107.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000020107.jpg", "image_width": 333, "image_height": 500, "image_resolution": "333x500", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1558.258, "latencies_ms": [1558.258], "images_per_second": 0.642, "prompt_tokens": 9, "response_tokens_est": 35, "n_tiles": 6, "output_text": "The image shows an old, rusty fire hydrant with a chain attached to it, situated on a concrete surface with some greenery and a wall in the background.", "error": null, "sys_before": {"cpu_percent": 41.3, "ram_used_mb": 25596.0, "ram_available_mb": 100176.2, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 25595.7, "ram_available_mb": 100176.4, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.42, "peak": 14.81, "min": 13.92}, "VDD_GPU": {"avg": 31.12, "peak": 37.82, "min": 24.82}, "VIN": {"avg": 71.72, "peak": 118.73, "min": 60.69}, "VDD_CPU_SOC_MSS": {"avg": 14.99, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 31.12, "energy_joules_est": 48.5, "sample_count": 11, "duration_seconds": 1.559}, "timestamp": "2026-01-16T15:50:21.820999"}
{"image_index": 198, "image_name": "000000020107.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000020107.jpg", "image_width": 333, "image_height": 500, "image_resolution": "333x500", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 4220.027, "latencies_ms": [4220.027], "images_per_second": 0.237, "prompt_tokens": 23, "response_tokens_est": 129, "n_tiles": 6, "output_text": "fire hydrant: 1\nchain: 1\npipe: 1\npipe cap: 1\npipe bolt: 1\npipe nut: 1\npipe flange: 1\npipe flange bolt: 1\npipe flange nut: 1\npipe flange bolt cap: 1\npipe flange bolt cap nut: 1\npipe flange bolt cap nut chain: 1\npipe flange bolt cap nut chain: 1\npipe flange bolt cap nut chain: 1\npipe flange bolt cap nut chain: 1\npipe flange bolt cap nut chain", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25595.7, "ram_available_mb": 100176.4, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 25595.7, "ram_available_mb": 100176.4, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.84, "peak": 15.01, "min": 14.02}, "VDD_GPU": {"avg": 26.28, "peak": 38.19, "min": 22.85}, "VIN": {"avg": 66.69, "peak": 113.04, "min": 52.47}, "VDD_CPU_SOC_MSS": {"avg": 15.27, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 26.28, "energy_joules_est": 110.91, "sample_count": 33, "duration_seconds": 4.22}, "timestamp": "2026-01-16T15:50:26.047232"}
{"image_index": 198, "image_name": "000000020107.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000020107.jpg", "image_width": 333, "image_height": 500, "image_resolution": "333x500", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2638.126, "latencies_ms": [2638.126], "images_per_second": 0.379, "prompt_tokens": 27, "response_tokens_est": 72, "n_tiles": 6, "output_text": "The main object in the foreground is a rusty, old fire hydrant with a chain attached to it. The hydrant is positioned on a concrete surface, with a chain lying nearby. In the background, there is a wall with a mural depicting green trees. The hydrant is situated near the wall, with the mural providing a backdrop.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25595.7, "ram_available_mb": 100176.4, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 25595.7, "ram_available_mb": 100176.4, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.76, "peak": 15.01, "min": 14.02}, "VDD_GPU": {"avg": 28.19, "peak": 38.21, "min": 23.25}, "VIN": {"avg": 66.79, "peak": 93.89, "min": 58.09}, "VDD_CPU_SOC_MSS": {"avg": 15.24, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 28.19, "energy_joules_est": 74.38, "sample_count": 20, "duration_seconds": 2.639}, "timestamp": "2026-01-16T15:50:28.691038"}
{"image_index": 198, "image_name": "000000020107.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000020107.jpg", "image_width": 333, "image_height": 500, "image_resolution": "333x500", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2498.821, "latencies_ms": [2498.821], "images_per_second": 0.4, "prompt_tokens": 21, "response_tokens_est": 67, "n_tiles": 6, "output_text": "The image depicts an old, rusty fire hydrant situated outdoors on a concrete surface. The hydrant is weathered and shows signs of wear, with a chain attached to it. The surrounding area appears to be a public space, possibly a park or a street, with some greenery and a wall in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.7, "ram_available_mb": 100176.4, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 25596.0, "ram_available_mb": 100176.2, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.75, "peak": 15.01, "min": 14.02}, "VDD_GPU": {"avg": 28.53, "peak": 37.82, "min": 23.24}, "VIN": {"avg": 69.4, "peak": 110.19, "min": 61.23}, "VDD_CPU_SOC_MSS": {"avg": 15.23, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 28.53, "energy_joules_est": 71.3, "sample_count": 19, "duration_seconds": 2.499}, "timestamp": "2026-01-16T15:50:31.196226"}
{"image_index": 198, "image_name": "000000020107.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000020107.jpg", "image_width": 333, "image_height": 500, "image_resolution": "333x500", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1682.988, "latencies_ms": [1682.988], "images_per_second": 0.594, "prompt_tokens": 19, "response_tokens_est": 38, "n_tiles": 6, "output_text": "The fire hydrant in the image has a rusty, peeling paint job, with a rusty metal cap and chain. The lighting is natural, suggesting it might be daytime.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25596.0, "ram_available_mb": 100176.2, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 25596.0, "ram_available_mb": 100176.2, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.63, "peak": 14.91, "min": 14.02}, "VDD_GPU": {"avg": 30.69, "peak": 37.82, "min": 24.03}, "VIN": {"avg": 73.32, "peak": 117.85, "min": 62.16}, "VDD_CPU_SOC_MSS": {"avg": 15.14, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 30.69, "energy_joules_est": 51.66, "sample_count": 13, "duration_seconds": 1.683}, "timestamp": "2026-01-16T15:50:32.885602"}
{"image_index": 199, "image_name": "000000020247.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000020247.jpg", "image_width": 640, "image_height": 457, "image_resolution": "640x457", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1979.657, "latencies_ms": [1979.657], "images_per_second": 0.505, "prompt_tokens": 9, "response_tokens_est": 17, "n_tiles": 12, "output_text": "A brown bear is standing on a dirt road, looking directly at the camera.", "error": null, "sys_before": {"cpu_percent": 42.3, "ram_used_mb": 25596.0, "ram_available_mb": 100176.2, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 2.3, "ram_used_mb": 25596.0, "ram_available_mb": 100176.2, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.14, "peak": 16.15, "min": 13.9}, "VDD_GPU": {"avg": 36.5, "peak": 41.36, "min": 29.15}, "VIN": {"avg": 77.39, "peak": 114.7, "min": 58.48}, "VDD_CPU_SOC_MSS": {"avg": 16.01, "peak": 17.32, "min": 14.96}}, "power_watts_avg": 36.5, "energy_joules_est": 72.29, "sample_count": 15, "duration_seconds": 1.98}, "timestamp": "2026-01-16T15:50:34.940005"}
{"image_index": 199, "image_name": "000000020247.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000020247.jpg", "image_width": 640, "image_height": 457, "image_resolution": "640x457", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1901.681, "latencies_ms": [1901.681], "images_per_second": 0.526, "prompt_tokens": 23, "response_tokens_est": 11, "n_tiles": 12, "output_text": "bear: 1\nground: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25596.0, "ram_available_mb": 100176.2, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 2.0, "ram_used_mb": 25596.0, "ram_available_mb": 100176.2, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.15, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 39.1, "peak": 42.94, "min": 33.48}, "VIN": {"avg": 79.96, "peak": 106.28, "min": 59.2}, "VDD_CPU_SOC_MSS": {"avg": 15.91, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 39.1, "energy_joules_est": 74.37, "sample_count": 14, "duration_seconds": 1.902}, "timestamp": "2026-01-16T15:50:36.848045"}
{"image_index": 199, "image_name": "000000020247.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000020247.jpg", "image_width": 640, "image_height": 457, "image_resolution": "640x457", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3959.233, "latencies_ms": [3959.233], "images_per_second": 0.253, "prompt_tokens": 27, "response_tokens_est": 72, "n_tiles": 12, "output_text": "The main object in the foreground is a brown bear, which is positioned near the center of the image. The bear is facing the camera and appears to be walking. In the background, there is a blurred brown bear, which is slightly out of focus. The blurred bear is located further away from the camera, indicating a depth of field effect.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25596.0, "ram_available_mb": 100176.2, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25596.0, "ram_available_mb": 100176.2, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.06, "peak": 15.65, "min": 14.32}, "VDD_GPU": {"avg": 32.97, "peak": 44.51, "min": 26.0}, "VIN": {"avg": 71.19, "peak": 102.02, "min": 45.77}, "VDD_CPU_SOC_MSS": {"avg": 15.7, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 32.97, "energy_joules_est": 130.55, "sample_count": 31, "duration_seconds": 3.96}, "timestamp": "2026-01-16T15:50:40.814177"}
{"image_index": 199, "image_name": "000000020247.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000020247.jpg", "image_width": 640, "image_height": 457, "image_resolution": "640x457", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3408.196, "latencies_ms": [3408.196], "images_per_second": 0.293, "prompt_tokens": 21, "response_tokens_est": 56, "n_tiles": 12, "output_text": "The image depicts a brown bear walking on a dirt path in a natural, outdoor setting. The bear appears to be in motion, with its fur slightly tousled by the wind. The background is blurred, emphasizing the bear as the focal point of the scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25596.0, "ram_available_mb": 100176.2, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25596.0, "ram_available_mb": 100176.2, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.11, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 33.6, "peak": 42.15, "min": 26.39}, "VIN": {"avg": 80.31, "peak": 143.28, "min": 59.11}, "VDD_CPU_SOC_MSS": {"avg": 15.73, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 33.6, "energy_joules_est": 114.53, "sample_count": 26, "duration_seconds": 3.409}, "timestamp": "2026-01-16T15:50:44.232824"}
{"image_index": 199, "image_name": "000000020247.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000020247.jpg", "image_width": 640, "image_height": 457, "image_resolution": "640x457", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3351.141, "latencies_ms": [3351.141], "images_per_second": 0.298, "prompt_tokens": 19, "response_tokens_est": 54, "n_tiles": 12, "output_text": "The image features a brown bear standing on a dusty, rocky ground. The lighting is natural, casting shadows on the ground, indicating it is daytime. The bear's fur appears thick and slightly matted, suggesting it is well-adapted to its environment.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25596.0, "ram_available_mb": 100176.2, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 25596.0, "ram_available_mb": 100176.2, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.07, "peak": 15.65, "min": 14.43}, "VDD_GPU": {"avg": 33.56, "peak": 42.54, "min": 26.39}, "VIN": {"avg": 77.12, "peak": 131.79, "min": 59.91}, "VDD_CPU_SOC_MSS": {"avg": 15.69, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 33.56, "energy_joules_est": 112.47, "sample_count": 26, "duration_seconds": 3.351}, "timestamp": "2026-01-16T15:50:47.590715"}
{"image_index": 200, "image_name": "000000020333.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000020333.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1514.36, "latencies_ms": [1514.36], "images_per_second": 0.66, "prompt_tokens": 9, "response_tokens_est": 35, "n_tiles": 6, "output_text": "A young child is kneeling on the ground, wearing a white shirt and a colorful tie, and is engaged in digging a hole in the soil with a metal bucket.", "error": null, "sys_before": {"cpu_percent": 38.1, "ram_used_mb": 25596.0, "ram_available_mb": 100176.2, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 25595.9, "ram_available_mb": 100176.2, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.45, "peak": 15.11, "min": 13.82}, "VDD_GPU": {"avg": 31.91, "peak": 37.82, "min": 25.6}, "VIN": {"avg": 71.22, "peak": 105.69, "min": 52.42}, "VDD_CPU_SOC_MSS": {"avg": 15.28, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 31.91, "energy_joules_est": 48.33, "sample_count": 11, "duration_seconds": 1.515}, "timestamp": "2026-01-16T15:50:49.170417"}
{"image_index": 200, "image_name": "000000020333.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000020333.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1802.934, "latencies_ms": [1802.934], "images_per_second": 0.555, "prompt_tokens": 23, "response_tokens_est": 43, "n_tiles": 6, "output_text": "1. Child\n2. Shovel\n3. Bucket\n4. Shovel handle\n5. Shovel blade\n6. Shovel base\n7. Shovel handle\n8. Shovel blade", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.9, "ram_available_mb": 100176.2, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 25595.9, "ram_available_mb": 100176.2, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.75, "peak": 15.11, "min": 14.12}, "VDD_GPU": {"avg": 31.27, "peak": 38.59, "min": 24.42}, "VIN": {"avg": 74.45, "peak": 114.37, "min": 60.81}, "VDD_CPU_SOC_MSS": {"avg": 15.23, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 31.27, "energy_joules_est": 56.39, "sample_count": 13, "duration_seconds": 1.803}, "timestamp": "2026-01-16T15:50:50.979623"}
{"image_index": 200, "image_name": "000000020333.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000020333.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2467.029, "latencies_ms": [2467.029], "images_per_second": 0.405, "prompt_tokens": 27, "response_tokens_est": 67, "n_tiles": 6, "output_text": "The main object in the foreground is a small child wearing a white shirt and a colorful tie. The child is kneeling on the ground, with their right hand reaching into a large metal bowl filled with dark soil. The background features a dark, leafy bush, providing a contrast to the bright colors of the child's clothing.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.9, "ram_available_mb": 100176.2, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 25595.9, "ram_available_mb": 100176.2, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.86, "peak": 15.22, "min": 14.12}, "VDD_GPU": {"avg": 28.92, "peak": 38.19, "min": 23.64}, "VIN": {"avg": 71.8, "peak": 112.65, "min": 61.77}, "VDD_CPU_SOC_MSS": {"avg": 15.44, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 28.92, "energy_joules_est": 71.36, "sample_count": 19, "duration_seconds": 2.467}, "timestamp": "2026-01-16T15:50:53.453137"}
{"image_index": 200, "image_name": "000000020333.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000020333.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2370.638, "latencies_ms": [2370.638], "images_per_second": 0.422, "prompt_tokens": 21, "response_tokens_est": 63, "n_tiles": 6, "output_text": "The image depicts a young child, likely a toddler, engaged in an outdoor activity. The child is kneeling on a patch of dirt, surrounded by a black and white photographic style. The child is wearing a white shirt and a colorful tie, and is interacting with a metal bucket filled with soil.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.9, "ram_available_mb": 100176.2, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 25595.9, "ram_available_mb": 100176.2, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.79, "peak": 15.01, "min": 14.12}, "VDD_GPU": {"avg": 28.84, "peak": 38.21, "min": 23.24}, "VIN": {"avg": 69.0, "peak": 107.78, "min": 54.39}, "VDD_CPU_SOC_MSS": {"avg": 15.42, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 28.84, "energy_joules_est": 68.38, "sample_count": 18, "duration_seconds": 2.371}, "timestamp": "2026-01-16T15:50:55.829948"}
{"image_index": 200, "image_name": "000000020333.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000020333.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1846.078, "latencies_ms": [1846.078], "images_per_second": 0.542, "prompt_tokens": 19, "response_tokens_est": 44, "n_tiles": 6, "output_text": "The notable visual attributes of the image include a child with light-colored hair, wearing a white shirt and a colorful tie, and a black and white photograph. The lighting is natural, with shadows indicating a sunny day.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.9, "ram_available_mb": 100176.2, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 25595.9, "ram_available_mb": 100176.2, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.68, "peak": 15.01, "min": 14.02}, "VDD_GPU": {"avg": 30.47, "peak": 38.19, "min": 24.03}, "VIN": {"avg": 76.31, "peak": 116.47, "min": 55.26}, "VDD_CPU_SOC_MSS": {"avg": 15.21, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 30.47, "energy_joules_est": 56.27, "sample_count": 14, "duration_seconds": 1.847}, "timestamp": "2026-01-16T15:50:57.682329"}
{"image_index": 201, "image_name": "000000020553.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000020553.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2488.915, "latencies_ms": [2488.915], "images_per_second": 0.402, "prompt_tokens": 9, "response_tokens_est": 32, "n_tiles": 12, "output_text": "A small table is placed on the ground with a few items on it, including a bottle of beer, a small plant, and a few other objects.", "error": null, "sys_before": {"cpu_percent": 46.0, "ram_used_mb": 25595.9, "ram_available_mb": 100176.2, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 25595.7, "ram_available_mb": 100176.5, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.12, "peak": 16.15, "min": 13.9}, "VDD_GPU": {"avg": 34.62, "peak": 40.97, "min": 27.18}, "VIN": {"avg": 82.26, "peak": 147.26, "min": 61.54}, "VDD_CPU_SOC_MSS": {"avg": 15.91, "peak": 17.32, "min": 14.96}}, "power_watts_avg": 34.62, "energy_joules_est": 86.18, "sample_count": 19, "duration_seconds": 2.489}, "timestamp": "2026-01-16T15:51:00.262156"}
{"image_index": 201, "image_name": "000000020553.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000020553.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2909.883, "latencies_ms": [2909.883], "images_per_second": 0.344, "prompt_tokens": 23, "response_tokens_est": 41, "n_tiles": 12, "output_text": "1. teddy bear\n2. stuffed animal\n3. stuffed animal\n4. stuffed animal\n5. stuffed animal\n6. stuffed animal\n7. stuffed animal\n8. stuffed animal", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.7, "ram_available_mb": 100176.5, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 25595.9, "ram_available_mb": 100176.2, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.8, "peak": 15.34, "min": 14.22}, "VDD_GPU": {"avg": 35.0, "peak": 42.92, "min": 26.79}, "VIN": {"avg": 77.04, "peak": 122.2, "min": 58.57}, "VDD_CPU_SOC_MSS": {"avg": 15.41, "peak": 16.14, "min": 14.56}}, "power_watts_avg": 35.0, "energy_joules_est": 101.87, "sample_count": 22, "duration_seconds": 2.911}, "timestamp": "2026-01-16T15:51:03.179022"}
{"image_index": 201, "image_name": "000000020553.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000020553.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 4154.146, "latencies_ms": [4154.146], "images_per_second": 0.241, "prompt_tokens": 27, "response_tokens_est": 77, "n_tiles": 12, "output_text": "The main objects in the image are a collection of stuffed animals and a small table. The stuffed animals are positioned on the table, with one bear sitting on top of the table and another bear standing on the table. The table is placed in the foreground, with the stuffed animals and other objects placed on it. The background features a barren, sandy landscape with sparse vegetation.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.9, "ram_available_mb": 100176.2, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25595.9, "ram_available_mb": 100176.3, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.78, "peak": 15.34, "min": 14.12}, "VDD_GPU": {"avg": 31.95, "peak": 42.92, "min": 25.6}, "VIN": {"avg": 75.2, "peak": 127.75, "min": 57.26}, "VDD_CPU_SOC_MSS": {"avg": 15.25, "peak": 16.14, "min": 14.56}}, "power_watts_avg": 31.95, "energy_joules_est": 132.74, "sample_count": 33, "duration_seconds": 4.155}, "timestamp": "2026-01-16T15:51:07.339937"}
{"image_index": 201, "image_name": "000000020553.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000020553.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3541.68, "latencies_ms": [3541.68], "images_per_second": 0.282, "prompt_tokens": 21, "response_tokens_est": 59, "n_tiles": 12, "output_text": "The image depicts a desert-like environment with a dirt ground and sparse vegetation. A small table is placed on the ground, holding various items including a bottle of beer, a plastic cup, and a small plant. A teddy bear and a stuffed animal are also present on the table.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.9, "ram_available_mb": 100176.3, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 25595.7, "ram_available_mb": 100176.5, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.74, "peak": 15.34, "min": 14.12}, "VDD_GPU": {"avg": 32.86, "peak": 42.53, "min": 26.0}, "VIN": {"avg": 77.73, "peak": 113.65, "min": 58.73}, "VDD_CPU_SOC_MSS": {"avg": 15.34, "peak": 16.14, "min": 14.56}}, "power_watts_avg": 32.86, "energy_joules_est": 116.39, "sample_count": 28, "duration_seconds": 3.542}, "timestamp": "2026-01-16T15:51:10.888077"}
{"image_index": 201, "image_name": "000000020553.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000020553.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3402.281, "latencies_ms": [3402.281], "images_per_second": 0.294, "prompt_tokens": 19, "response_tokens_est": 55, "n_tiles": 12, "output_text": "The image depicts a desert-like environment with a sandy ground and sparse vegetation. The lighting is bright and natural, casting shadows on the ground. The materials visible include a wooden table, a red plastic bottle, a green plastic bottle, and a yellow flower arrangement.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25595.7, "ram_available_mb": 100176.5, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 25595.7, "ram_available_mb": 100176.5, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.74, "peak": 15.34, "min": 14.12}, "VDD_GPU": {"avg": 33.4, "peak": 42.53, "min": 26.0}, "VIN": {"avg": 77.9, "peak": 124.71, "min": 58.96}, "VDD_CPU_SOC_MSS": {"avg": 15.29, "peak": 16.14, "min": 14.56}}, "power_watts_avg": 33.4, "energy_joules_est": 113.65, "sample_count": 26, "duration_seconds": 3.403}, "timestamp": "2026-01-16T15:51:14.296553"}
{"image_index": 202, "image_name": "000000020571.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000020571.jpg", "image_width": 539, "image_height": 640, "image_resolution": "539x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2995.858, "latencies_ms": [2995.858], "images_per_second": 0.334, "prompt_tokens": 9, "response_tokens_est": 47, "n_tiles": 12, "output_text": "The image depicts a harbor scene with a variety of boats and a calm body of water, featuring a rusty, weathered boat with a number \"19\" on its side, alongside other boats and a rocky shoreline.", "error": null, "sys_before": {"cpu_percent": 40.3, "ram_used_mb": 25595.7, "ram_available_mb": 100176.5, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 25595.7, "ram_available_mb": 100176.5, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.06, "peak": 16.05, "min": 13.39}, "VDD_GPU": {"avg": 33.48, "peak": 41.36, "min": 26.39}, "VIN": {"avg": 73.69, "peak": 97.8, "min": 54.77}, "VDD_CPU_SOC_MSS": {"avg": 15.73, "peak": 17.32, "min": 14.57}}, "power_watts_avg": 33.48, "energy_joules_est": 100.32, "sample_count": 23, "duration_seconds": 2.996}, "timestamp": "2026-01-16T15:51:17.393369"}
{"image_index": 202, "image_name": "000000020571.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000020571.jpg", "image_width": 539, "image_height": 640, "image_resolution": "539x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 5844.45, "latencies_ms": [5844.45], "images_per_second": 0.171, "prompt_tokens": 23, "response_tokens_est": 129, "n_tiles": 12, "output_text": "boat: 1\nbuoy: 1\nlifebuoy: 1\npole: 1\npole: 1\npole: 1\npole: 1\npole: 1\npole: 1\npole: 1\npole: 1\npole: 1\npole: 1\npole: 1\npole: 1\npole: 1\npole: 1\npole: 1\npole: 1\npole: 1\npole: 1\npole", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.7, "ram_available_mb": 100176.5, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 25595.7, "ram_available_mb": 100176.5, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.14, "peak": 15.65, "min": 14.63}, "VDD_GPU": {"avg": 30.44, "peak": 42.54, "min": 26.0}, "VIN": {"avg": 71.93, "peak": 112.45, "min": 57.08}, "VDD_CPU_SOC_MSS": {"avg": 15.71, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 30.44, "energy_joules_est": 177.92, "sample_count": 46, "duration_seconds": 5.845}, "timestamp": "2026-01-16T15:51:23.244778"}
{"image_index": 202, "image_name": "000000020571.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000020571.jpg", "image_width": 539, "image_height": 640, "image_resolution": "539x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3985.852, "latencies_ms": [3985.852], "images_per_second": 0.251, "prompt_tokens": 27, "response_tokens_est": 73, "n_tiles": 12, "output_text": "The main objects in the image are a weathered boat and a rusty metal sign. The boat is situated in the foreground, with its deck and various items like ropes and buoys visible. The sign is located near the boat, attached to a metal structure. The background features a calm body of water with several boats and a mountainous landscape.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25595.7, "ram_available_mb": 100176.5, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25595.6, "ram_available_mb": 100176.5, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.11, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 32.35, "peak": 41.76, "min": 26.39}, "VIN": {"avg": 72.33, "peak": 107.99, "min": 58.11}, "VDD_CPU_SOC_MSS": {"avg": 15.71, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 32.35, "energy_joules_est": 128.96, "sample_count": 31, "duration_seconds": 3.986}, "timestamp": "2026-01-16T15:51:27.236690"}
{"image_index": 202, "image_name": "000000020571.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000020571.jpg", "image_width": 539, "image_height": 640, "image_resolution": "539x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3349.417, "latencies_ms": [3349.417], "images_per_second": 0.299, "prompt_tokens": 21, "response_tokens_est": 54, "n_tiles": 12, "output_text": "The image depicts a harbor scene with a variety of boats and maritime equipment. The boats are moored in the water, and there are several buoys scattered around. In the background, there are hills and a few buildings, suggesting a coastal town setting.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25595.6, "ram_available_mb": 100176.5, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 25595.6, "ram_available_mb": 100176.5, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.09, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 33.4, "peak": 42.15, "min": 26.39}, "VIN": {"avg": 75.41, "peak": 132.9, "min": 58.75}, "VDD_CPU_SOC_MSS": {"avg": 15.67, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 33.4, "energy_joules_est": 111.88, "sample_count": 26, "duration_seconds": 3.35}, "timestamp": "2026-01-16T15:51:30.592341"}
{"image_index": 202, "image_name": "000000020571.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000020571.jpg", "image_width": 539, "image_height": 640, "image_resolution": "539x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3919.087, "latencies_ms": [3919.087], "images_per_second": 0.255, "prompt_tokens": 19, "response_tokens_est": 71, "n_tiles": 12, "output_text": "The image depicts a harbor scene with a variety of boats and a calm body of water. The boats are painted in different colors, including green, yellow, and red, and are equipped with various equipment such as lifebuoys and ropes. The lighting is soft and diffused, suggesting an overcast day, with no harsh shadows.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.6, "ram_available_mb": 100176.5, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25595.4, "ram_available_mb": 100176.8, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.09, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 32.46, "peak": 42.53, "min": 26.39}, "VIN": {"avg": 74.05, "peak": 109.1, "min": 58.43}, "VDD_CPU_SOC_MSS": {"avg": 15.73, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 32.46, "energy_joules_est": 127.22, "sample_count": 31, "duration_seconds": 3.919}, "timestamp": "2026-01-16T15:51:34.517867"}
{"image_index": 203, "image_name": "000000020992.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000020992.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1176.472, "latencies_ms": [1176.472], "images_per_second": 0.85, "prompt_tokens": 9, "response_tokens_est": 22, "n_tiles": 6, "output_text": "A woman is eating a hot dog with her mouth open, and she is wearing a black scarf.", "error": null, "sys_before": {"cpu_percent": 40.3, "ram_used_mb": 25595.4, "ram_available_mb": 100176.8, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 25595.7, "ram_available_mb": 100176.5, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.41, "peak": 14.81, "min": 13.92}, "VDD_GPU": {"avg": 33.22, "peak": 37.82, "min": 27.18}, "VIN": {"avg": 75.46, "peak": 116.89, "min": 54.83}, "VDD_CPU_SOC_MSS": {"avg": 15.09, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 33.22, "energy_joules_est": 39.1, "sample_count": 9, "duration_seconds": 1.177}, "timestamp": "2026-01-16T15:51:35.745812"}
{"image_index": 203, "image_name": "000000020992.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000020992.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1487.157, "latencies_ms": [1487.157], "images_per_second": 0.672, "prompt_tokens": 23, "response_tokens_est": 32, "n_tiles": 6, "output_text": "1. Woman\n2. Food\n3. Plate\n4. Bowl\n5. Bowl\n6. Bowl\n7. Bowl\n8. Bowl", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.7, "ram_available_mb": 100176.5, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 25595.4, "ram_available_mb": 100176.8, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.7, "peak": 15.11, "min": 14.22}, "VDD_GPU": {"avg": 33.12, "peak": 38.98, "min": 26.0}, "VIN": {"avg": 75.18, "peak": 104.46, "min": 59.31}, "VDD_CPU_SOC_MSS": {"avg": 15.35, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 33.12, "energy_joules_est": 49.27, "sample_count": 11, "duration_seconds": 1.488}, "timestamp": "2026-01-16T15:51:37.239184"}
{"image_index": 203, "image_name": "000000020992.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000020992.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2107.964, "latencies_ms": [2107.964], "images_per_second": 0.474, "prompt_tokens": 27, "response_tokens_est": 53, "n_tiles": 6, "output_text": "The main object in the foreground is a person holding a hot dog. The person is wearing a dark-colored scarf and has short hair. The background is blurred, but there appears to be a light source, possibly a lamp, illuminating the scene.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25595.4, "ram_available_mb": 100176.8, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 25595.4, "ram_available_mb": 100176.8, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.72, "peak": 15.01, "min": 14.12}, "VDD_GPU": {"avg": 29.86, "peak": 38.59, "min": 23.64}, "VIN": {"avg": 72.38, "peak": 115.01, "min": 61.22}, "VDD_CPU_SOC_MSS": {"avg": 15.31, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 29.86, "energy_joules_est": 62.95, "sample_count": 16, "duration_seconds": 2.108}, "timestamp": "2026-01-16T15:51:39.352692"}
{"image_index": 203, "image_name": "000000020992.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000020992.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2106.174, "latencies_ms": [2106.174], "images_per_second": 0.475, "prompt_tokens": 21, "response_tokens_est": 53, "n_tiles": 6, "output_text": "The image depicts a close-up of a person eating a hot dog, with their mouth open and tongue visible. The setting appears to be indoors, possibly in a dimly lit area, as suggested by the blurred background and the warm, soft lighting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.4, "ram_available_mb": 100176.8, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 25595.4, "ram_available_mb": 100176.8, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.7, "peak": 15.01, "min": 14.02}, "VDD_GPU": {"avg": 29.64, "peak": 38.19, "min": 23.64}, "VIN": {"avg": 67.09, "peak": 107.77, "min": 55.94}, "VDD_CPU_SOC_MSS": {"avg": 15.33, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 29.64, "energy_joules_est": 62.44, "sample_count": 16, "duration_seconds": 2.107}, "timestamp": "2026-01-16T15:51:41.464817"}
{"image_index": 203, "image_name": "000000020992.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000020992.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1542.514, "latencies_ms": [1542.514], "images_per_second": 0.648, "prompt_tokens": 19, "response_tokens_est": 33, "n_tiles": 6, "output_text": "The image features a person with short hair, wearing a dark-colored scarf. The lighting is dim, with a warm glow illuminating the subject's face.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.4, "ram_available_mb": 100176.8, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 25595.6, "ram_available_mb": 100176.5, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.61, "peak": 14.81, "min": 14.12}, "VDD_GPU": {"avg": 31.98, "peak": 37.82, "min": 25.21}, "VIN": {"avg": 69.54, "peak": 107.52, "min": 56.75}, "VDD_CPU_SOC_MSS": {"avg": 15.17, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 31.98, "energy_joules_est": 49.34, "sample_count": 11, "duration_seconds": 1.543}, "timestamp": "2026-01-16T15:51:43.017404"}
{"image_index": 204, "image_name": "000000021167.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000021167.jpg", "image_width": 426, "image_height": 640, "image_resolution": "426x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1391.815, "latencies_ms": [1391.815], "images_per_second": 0.718, "prompt_tokens": 9, "response_tokens_est": 29, "n_tiles": 6, "output_text": "A man and a woman are standing in a room, with the man holding a martini glass and the woman looking off to the side.", "error": null, "sys_before": {"cpu_percent": 40.6, "ram_used_mb": 25595.6, "ram_available_mb": 100176.5, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 25595.4, "ram_available_mb": 100176.8, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.31, "peak": 14.81, "min": 13.82}, "VDD_GPU": {"avg": 32.03, "peak": 37.82, "min": 25.6}, "VIN": {"avg": 79.59, "peak": 124.11, "min": 63.05}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 32.03, "energy_joules_est": 44.59, "sample_count": 10, "duration_seconds": 1.392}, "timestamp": "2026-01-16T15:51:44.458492"}
{"image_index": 204, "image_name": "000000021167.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000021167.jpg", "image_width": 426, "image_height": 640, "image_resolution": "426x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2528.667, "latencies_ms": [2528.667], "images_per_second": 0.395, "prompt_tokens": 23, "response_tokens_est": 68, "n_tiles": 6, "output_text": "1. Man: 1\n2. Woman: 1\n3. Man's suit: 1\n4. Woman's dress: 1\n5. Man's tie: 1\n6. Woman's dress: 1\n7. Man's jacket: 1\n8. Woman's dress: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.4, "ram_available_mb": 100176.8, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 25595.6, "ram_available_mb": 100176.5, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.73, "peak": 15.01, "min": 14.02}, "VDD_GPU": {"avg": 28.84, "peak": 38.6, "min": 23.24}, "VIN": {"avg": 70.17, "peak": 116.37, "min": 52.78}, "VDD_CPU_SOC_MSS": {"avg": 15.35, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 28.84, "energy_joules_est": 72.95, "sample_count": 19, "duration_seconds": 2.529}, "timestamp": "2026-01-16T15:51:46.993655"}
{"image_index": 204, "image_name": "000000021167.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000021167.jpg", "image_width": 426, "image_height": 640, "image_resolution": "426x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1964.494, "latencies_ms": [1964.494], "images_per_second": 0.509, "prompt_tokens": 27, "response_tokens_est": 48, "n_tiles": 6, "output_text": "The main objects in the image are a man and a woman standing close to each other. The man is in the foreground, while the woman is slightly behind him. The background features a doorway and a glimpse of a kitchen counter.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.6, "ram_available_mb": 100176.5, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 25595.6, "ram_available_mb": 100176.5, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.7, "peak": 15.01, "min": 14.02}, "VDD_GPU": {"avg": 29.83, "peak": 38.21, "min": 23.64}, "VIN": {"avg": 71.91, "peak": 115.85, "min": 52.37}, "VDD_CPU_SOC_MSS": {"avg": 15.22, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 29.83, "energy_joules_est": 58.61, "sample_count": 15, "duration_seconds": 1.965}, "timestamp": "2026-01-16T15:51:48.965327"}
{"image_index": 204, "image_name": "000000021167.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000021167.jpg", "image_width": 426, "image_height": 640, "image_resolution": "426x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2774.871, "latencies_ms": [2774.871], "images_per_second": 0.36, "prompt_tokens": 21, "response_tokens_est": 77, "n_tiles": 6, "output_text": "The image depicts a man and a woman standing in an indoor setting, possibly a hallway or a room. The man is dressed in a formal black suit and tie, holding a martini glass with a drink. The woman is wearing a dark, sleeveless dress and has her hair tied back. They appear to be engaged in a conversation or looking at something off-camera.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 25595.6, "ram_available_mb": 100176.5, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 25595.6, "ram_available_mb": 100176.5, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.78, "peak": 15.11, "min": 14.02}, "VDD_GPU": {"avg": 28.04, "peak": 37.82, "min": 23.24}, "VIN": {"avg": 69.18, "peak": 112.82, "min": 57.07}, "VDD_CPU_SOC_MSS": {"avg": 15.43, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 28.04, "energy_joules_est": 77.82, "sample_count": 21, "duration_seconds": 2.775}, "timestamp": "2026-01-16T15:51:51.750273"}
{"image_index": 204, "image_name": "000000021167.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000021167.jpg", "image_width": 426, "image_height": 640, "image_resolution": "426x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2324.729, "latencies_ms": [2324.729], "images_per_second": 0.43, "prompt_tokens": 19, "response_tokens_est": 61, "n_tiles": 6, "output_text": "The image features a man and a woman standing indoors. The man is dressed in a black suit with a white shirt and a dark tie, while the woman is wearing a dark, sleeveless dress. The lighting is dim, and the overall atmosphere appears to be indoors with a warm, soft glow.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.6, "ram_available_mb": 100176.5, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 25595.4, "ram_available_mb": 100176.8, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.71, "peak": 15.01, "min": 14.02}, "VDD_GPU": {"avg": 28.71, "peak": 38.21, "min": 23.24}, "VIN": {"avg": 67.08, "peak": 103.07, "min": 55.84}, "VDD_CPU_SOC_MSS": {"avg": 15.29, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 28.71, "energy_joules_est": 66.76, "sample_count": 18, "duration_seconds": 2.325}, "timestamp": "2026-01-16T15:51:54.081377"}
{"image_index": 205, "image_name": "000000021465.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000021465.jpg", "image_width": 500, "image_height": 281, "image_resolution": "500x281", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1373.094, "latencies_ms": [1373.094], "images_per_second": 0.728, "prompt_tokens": 9, "response_tokens_est": 53, "n_tiles": 2, "output_text": "The image shows a cluttered blue table with various items scattered on it, including a metal pitcher, a small plate, a basket, and a wooden chair, all set against a backdrop of a rustic wooden table and a green surface with a small figurine.", "error": null, "sys_before": {"cpu_percent": 31.4, "ram_used_mb": 25595.4, "ram_available_mb": 100176.8, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25595.4, "ram_available_mb": 100176.8, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5055.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.81, "peak": 15.42, "min": 14.1}, "VDD_GPU": {"avg": 24.78, "peak": 32.29, "min": 21.28}, "VIN": {"avg": 65.21, "peak": 75.1, "min": 58.46}, "VDD_CPU_SOC_MSS": {"avg": 15.43, "peak": 16.14, "min": 14.56}}, "power_watts_avg": 24.78, "energy_joules_est": 34.03, "sample_count": 10, "duration_seconds": 1.373}, "timestamp": "2026-01-16T15:51:55.483394"}
{"image_index": 205, "image_name": "000000021465.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000021465.jpg", "image_width": 500, "image_height": 281, "image_resolution": "500x281", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1091.729, "latencies_ms": [1091.729], "images_per_second": 0.916, "prompt_tokens": 23, "response_tokens_est": 40, "n_tiles": 2, "output_text": "object: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.4, "ram_available_mb": 100176.8, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25595.1, "ram_available_mb": 100177.1, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5064.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.03, "peak": 15.42, "min": 14.61}, "VDD_GPU": {"avg": 25.61, "peak": 31.13, "min": 22.06}, "VIN": {"avg": 68.22, "peak": 96.79, "min": 61.54}, "VDD_CPU_SOC_MSS": {"avg": 15.75, "peak": 16.54, "min": 14.95}}, "power_watts_avg": 25.61, "energy_joules_est": 27.97, "sample_count": 8, "duration_seconds": 1.092}, "timestamp": "2026-01-16T15:51:56.581819"}
{"image_index": 205, "image_name": "000000021465.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000021465.jpg", "image_width": 500, "image_height": 281, "image_resolution": "500x281", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1454.54, "latencies_ms": [1454.54], "images_per_second": 0.688, "prompt_tokens": 27, "response_tokens_est": 56, "n_tiles": 2, "output_text": "The main objects in the image are a blue table and a green table. The blue table is in the foreground, while the green table is in the background. The objects on the blue table are closer to the viewer, while the objects on the green table are farther away.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.1, "ram_available_mb": 100177.1, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25595.1, "ram_available_mb": 100177.1, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5067.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.1, "peak": 15.42, "min": 14.51}, "VDD_GPU": {"avg": 24.6, "peak": 31.5, "min": 21.27}, "VIN": {"avg": 67.27, "peak": 102.51, "min": 61.1}, "VDD_CPU_SOC_MSS": {"avg": 15.93, "peak": 16.54, "min": 15.36}}, "power_watts_avg": 24.6, "energy_joules_est": 35.79, "sample_count": 11, "duration_seconds": 1.455}, "timestamp": "2026-01-16T15:51:58.042281"}
{"image_index": 205, "image_name": "000000021465.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000021465.jpg", "image_width": 500, "image_height": 281, "image_resolution": "500x281", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2026.364, "latencies_ms": [2026.364], "images_per_second": 0.493, "prompt_tokens": 21, "response_tokens_est": 81, "n_tiles": 2, "output_text": "The image depicts an outdoor setting with a rustic, vintage-style table and chair. The table is painted blue and has various items on it, including a metal pitcher, a small plate, and a few cups. The chair is yellow and has a wooden seat. The table and chair are surrounded by a concrete floor, and there is a green tabletop with a white border visible in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.1, "ram_available_mb": 100177.1, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25595.4, "ram_available_mb": 100176.8, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5063.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.03, "peak": 15.42, "min": 14.51}, "VDD_GPU": {"avg": 23.67, "peak": 31.51, "min": 20.88}, "VIN": {"avg": 65.79, "peak": 102.16, "min": 59.02}, "VDD_CPU_SOC_MSS": {"avg": 16.12, "peak": 16.54, "min": 15.36}}, "power_watts_avg": 23.67, "energy_joules_est": 47.97, "sample_count": 15, "duration_seconds": 2.027}, "timestamp": "2026-01-16T15:52:00.074310"}
{"image_index": 205, "image_name": "000000021465.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000021465.jpg", "image_width": 500, "image_height": 281, "image_resolution": "500x281", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1608.738, "latencies_ms": [1608.738], "images_per_second": 0.622, "prompt_tokens": 19, "response_tokens_est": 61, "n_tiles": 2, "output_text": "The image showcases a vibrant blue table with a rustic, weathered look. The table is adorned with various metallic objects, including vases and cups, and is surrounded by a mix of wooden and metal furniture pieces. The lighting is soft and natural, casting a warm glow over the scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.4, "ram_available_mb": 100176.8, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25595.1, "ram_available_mb": 100177.1, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5062.6, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.18, "peak": 15.42, "min": 14.71}, "VDD_GPU": {"avg": 23.8, "peak": 30.73, "min": 20.89}, "VIN": {"avg": 65.98, "peak": 105.22, "min": 60.38}, "VDD_CPU_SOC_MSS": {"avg": 15.91, "peak": 16.14, "min": 15.36}}, "power_watts_avg": 23.8, "energy_joules_est": 38.29, "sample_count": 12, "duration_seconds": 1.609}, "timestamp": "2026-01-16T15:52:01.688690"}
{"image_index": 206, "image_name": "000000021503.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000021503.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2751.825, "latencies_ms": [2751.825], "images_per_second": 0.363, "prompt_tokens": 9, "response_tokens_est": 40, "n_tiles": 12, "output_text": "The image shows a close-up view of a plate with several pieces of a cheesy, golden-brown pastry, possibly a type of bread or pastry, with a creamy white topping.", "error": null, "sys_before": {"cpu_percent": 47.5, "ram_used_mb": 25595.1, "ram_available_mb": 100177.1, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 25595.1, "ram_available_mb": 100177.1, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.17, "peak": 16.15, "min": 14.1}, "VDD_GPU": {"avg": 33.51, "peak": 40.57, "min": 26.79}, "VIN": {"avg": 78.86, "peak": 118.09, "min": 58.5}, "VDD_CPU_SOC_MSS": {"avg": 15.88, "peak": 17.32, "min": 15.35}}, "power_watts_avg": 33.51, "energy_joules_est": 92.23, "sample_count": 21, "duration_seconds": 2.752}, "timestamp": "2026-01-16T15:52:04.518138"}
{"image_index": 206, "image_name": "000000021503.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000021503.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 3351.407, "latencies_ms": [3351.407], "images_per_second": 0.298, "prompt_tokens": 23, "response_tokens_est": 54, "n_tiles": 12, "output_text": "- 1 pizza\n- 1 slice of pizza\n- 1 slice of bread\n- 1 slice of bread\n- 1 slice of bread\n- 1 slice of bread\n- 1 slice of bread\n- 1 slice of bread", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.1, "ram_available_mb": 100177.1, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 25595.1, "ram_available_mb": 100177.1, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.07, "peak": 15.65, "min": 14.43}, "VDD_GPU": {"avg": 33.67, "peak": 42.94, "min": 26.39}, "VIN": {"avg": 74.15, "peak": 122.27, "min": 58.51}, "VDD_CPU_SOC_MSS": {"avg": 15.58, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 33.67, "energy_joules_est": 112.87, "sample_count": 26, "duration_seconds": 3.352}, "timestamp": "2026-01-16T15:52:07.877124"}
{"image_index": 206, "image_name": "000000021503.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000021503.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 4706.342, "latencies_ms": [4706.342], "images_per_second": 0.212, "prompt_tokens": 27, "response_tokens_est": 94, "n_tiles": 12, "output_text": "The main objects in the image are a plate of cheesy breadsticks. The cheesy breadsticks are positioned in the foreground, with the plate serving as the central point of focus. The background features a blurred computer keyboard and mouse, indicating that the cheesy breadsticks are likely placed on a desk or table. The cheesy breadsticks are near the center of the image, with the computer equipment slightly out of focus in the background.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 25595.1, "ram_available_mb": 100177.1, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 25595.1, "ram_available_mb": 100177.1, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.1, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 31.38, "peak": 42.54, "min": 26.0}, "VIN": {"avg": 76.54, "peak": 131.89, "min": 58.24}, "VDD_CPU_SOC_MSS": {"avg": 15.64, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 31.38, "energy_joules_est": 147.7, "sample_count": 37, "duration_seconds": 4.707}, "timestamp": "2026-01-16T15:52:12.590111"}
{"image_index": 206, "image_name": "000000021503.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000021503.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 4259.667, "latencies_ms": [4259.667], "images_per_second": 0.235, "prompt_tokens": 21, "response_tokens_est": 81, "n_tiles": 12, "output_text": "The image shows a close-up view of a plate with several pieces of a cheesy, golden-brown pastry. The pastry appears to be a type of bread or biscuit, with a creamy, white topping that is likely cheese. The setting seems to be indoors, possibly in a kitchen or dining area, with a blurred background featuring a computer mouse and other indistinct objects.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.1, "ram_available_mb": 100177.1, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25595.1, "ram_available_mb": 100177.1, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.09, "peak": 15.65, "min": 14.43}, "VDD_GPU": {"avg": 32.11, "peak": 42.94, "min": 26.0}, "VIN": {"avg": 73.99, "peak": 132.89, "min": 56.87}, "VDD_CPU_SOC_MSS": {"avg": 15.64, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 32.11, "energy_joules_est": 136.79, "sample_count": 33, "duration_seconds": 4.26}, "timestamp": "2026-01-16T15:52:16.856198"}
{"image_index": 206, "image_name": "000000021503.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000021503.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3854.33, "latencies_ms": [3854.33], "images_per_second": 0.259, "prompt_tokens": 19, "response_tokens_est": 69, "n_tiles": 12, "output_text": "The image features a close-up view of a plate with several pieces of a cheesy, golden-brown pastry. The lighting is warm and soft, casting gentle shadows and highlighting the textures of the pastry. The colors are predominantly warm, with the golden-brown pastry contrasting against the white plate and the blurred background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.1, "ram_available_mb": 100177.1, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25594.8, "ram_available_mb": 100177.3, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.08, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 32.7, "peak": 42.54, "min": 26.0}, "VIN": {"avg": 71.61, "peak": 110.95, "min": 56.68}, "VDD_CPU_SOC_MSS": {"avg": 15.67, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 32.7, "energy_joules_est": 126.05, "sample_count": 30, "duration_seconds": 3.855}, "timestamp": "2026-01-16T15:52:20.717597"}
{"image_index": 207, "image_name": "000000021604.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000021604.jpg", "image_width": 512, "image_height": 640, "image_resolution": "512x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2312.356, "latencies_ms": [2312.356], "images_per_second": 0.432, "prompt_tokens": 9, "response_tokens_est": 27, "n_tiles": 12, "output_text": "The image shows a man dressed in a formal black suit, white shirt, and a tie adorned with small, colorful lights.", "error": null, "sys_before": {"cpu_percent": 41.1, "ram_used_mb": 25594.8, "ram_available_mb": 100177.3, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 25595.1, "ram_available_mb": 100177.1, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.16, "peak": 16.15, "min": 14.0}, "VDD_GPU": {"avg": 35.41, "peak": 41.36, "min": 27.97}, "VIN": {"avg": 78.1, "peak": 109.44, "min": 58.63}, "VDD_CPU_SOC_MSS": {"avg": 16.05, "peak": 17.32, "min": 14.96}}, "power_watts_avg": 35.41, "energy_joules_est": 81.9, "sample_count": 18, "duration_seconds": 2.313}, "timestamp": "2026-01-16T15:52:23.113625"}
{"image_index": 207, "image_name": "000000021604.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000021604.jpg", "image_width": 512, "image_height": 640, "image_resolution": "512x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2679.387, "latencies_ms": [2679.387], "images_per_second": 0.373, "prompt_tokens": 23, "response_tokens_est": 34, "n_tiles": 12, "output_text": "1. Man\n2. Suits\n3. Tie\n4. Glasses\n5. Shirt\n6. Background\n7. Lighting\n8. Background", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.1, "ram_available_mb": 100177.1, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 25595.1, "ram_available_mb": 100177.1, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.07, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 35.47, "peak": 43.33, "min": 27.18}, "VIN": {"avg": 74.47, "peak": 100.87, "min": 58.34}, "VDD_CPU_SOC_MSS": {"avg": 15.8, "peak": 16.53, "min": 15.35}}, "power_watts_avg": 35.47, "energy_joules_est": 95.05, "sample_count": 21, "duration_seconds": 2.68}, "timestamp": "2026-01-16T15:52:25.799144"}
{"image_index": 207, "image_name": "000000021604.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000021604.jpg", "image_width": 512, "image_height": 640, "image_resolution": "512x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3320.144, "latencies_ms": [3320.144], "images_per_second": 0.301, "prompt_tokens": 27, "response_tokens_est": 53, "n_tiles": 12, "output_text": "The main object in the foreground is a man wearing a black suit and tie. He is standing with his left hand near his neck, adjusting his tie. The background is a plain, dark gray wall, providing a stark contrast to the man's attire.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.1, "ram_available_mb": 100177.1, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 25594.8, "ram_available_mb": 100177.3, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.07, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 33.66, "peak": 42.94, "min": 26.39}, "VIN": {"avg": 73.81, "peak": 121.01, "min": 45.41}, "VDD_CPU_SOC_MSS": {"avg": 15.7, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 33.66, "energy_joules_est": 111.78, "sample_count": 26, "duration_seconds": 3.321}, "timestamp": "2026-01-16T15:52:29.126576"}
{"image_index": 207, "image_name": "000000021604.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000021604.jpg", "image_width": 512, "image_height": 640, "image_resolution": "512x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3454.164, "latencies_ms": [3454.164], "images_per_second": 0.29, "prompt_tokens": 21, "response_tokens_est": 57, "n_tiles": 12, "output_text": "The image depicts a man dressed in a formal black suit and tie, standing against a dark, neutral background. He is adjusting his tie, with a focused expression on his face. The lighting is soft and focused on him, highlighting his attire and creating a dramatic effect.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25594.8, "ram_available_mb": 100177.3, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 25594.8, "ram_available_mb": 100177.3, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.08, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 33.4, "peak": 42.94, "min": 26.39}, "VIN": {"avg": 73.26, "peak": 107.44, "min": 58.65}, "VDD_CPU_SOC_MSS": {"avg": 15.74, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 33.4, "energy_joules_est": 115.39, "sample_count": 27, "duration_seconds": 3.455}, "timestamp": "2026-01-16T15:52:32.587556"}
{"image_index": 207, "image_name": "000000021604.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000021604.jpg", "image_width": 512, "image_height": 640, "image_resolution": "512x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3315.69, "latencies_ms": [3315.69], "images_per_second": 0.302, "prompt_tokens": 19, "response_tokens_est": 53, "n_tiles": 12, "output_text": "The man in the image is wearing a black suit jacket and a white shirt. The lighting is soft and focused on him, highlighting his facial features and the details of his attire. The background is dark, providing a stark contrast that emphasizes the subject.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25594.8, "ram_available_mb": 100177.3, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25594.6, "ram_available_mb": 100177.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.1, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 33.53, "peak": 42.53, "min": 26.4}, "VIN": {"avg": 74.63, "peak": 116.84, "min": 50.09}, "VDD_CPU_SOC_MSS": {"avg": 15.73, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 33.53, "energy_joules_est": 111.19, "sample_count": 26, "duration_seconds": 3.316}, "timestamp": "2026-01-16T15:52:35.909579"}
{"image_index": 208, "image_name": "000000021839.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000021839.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2326.155, "latencies_ms": [2326.155], "images_per_second": 0.43, "prompt_tokens": 9, "response_tokens_est": 27, "n_tiles": 12, "output_text": "A woman is walking down a street at night, passing by a building with a sign that reads \"TADU RIA.\"", "error": null, "sys_before": {"cpu_percent": 46.2, "ram_used_mb": 25594.6, "ram_available_mb": 100177.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 2.6, "ram_used_mb": 25594.6, "ram_available_mb": 100177.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.09, "peak": 16.05, "min": 13.69}, "VDD_GPU": {"avg": 35.39, "peak": 42.15, "min": 27.97}, "VIN": {"avg": 80.66, "peak": 108.19, "min": 67.01}, "VDD_CPU_SOC_MSS": {"avg": 15.86, "peak": 16.92, "min": 14.57}}, "power_watts_avg": 35.39, "energy_joules_est": 82.33, "sample_count": 18, "duration_seconds": 2.326}, "timestamp": "2026-01-16T15:52:38.331963"}
{"image_index": 208, "image_name": "000000021839.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000021839.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2510.904, "latencies_ms": [2510.904], "images_per_second": 0.398, "prompt_tokens": 23, "response_tokens_est": 29, "n_tiles": 12, "output_text": "- Street light\n- Street sign\n- Pedestrian\n- Woman\n- Car\n- Building\n- Street\n- Storefront", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25594.6, "ram_available_mb": 100177.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 25594.8, "ram_available_mb": 100177.4, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.07, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 36.45, "peak": 43.33, "min": 27.97}, "VIN": {"avg": 81.68, "peak": 145.06, "min": 58.61}, "VDD_CPU_SOC_MSS": {"avg": 15.72, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 36.45, "energy_joules_est": 91.54, "sample_count": 19, "duration_seconds": 2.511}, "timestamp": "2026-01-16T15:52:40.849266"}
{"image_index": 208, "image_name": "000000021839.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000021839.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 4597.04, "latencies_ms": [4597.04], "images_per_second": 0.218, "prompt_tokens": 27, "response_tokens_est": 91, "n_tiles": 12, "output_text": "The main objects in the image are a street scene at night, with a focus on a person walking on the sidewalk. The person is positioned in the foreground, slightly to the right, and is walking towards the camera. The background features a building with a lit sign reading \"TADU RIA,\" a streetlight, and a few other indistinct figures. The street is relatively empty, with no other people visible in the immediate vicinity.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25594.8, "ram_available_mb": 100177.4, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 25594.6, "ram_available_mb": 100177.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.11, "peak": 15.65, "min": 14.43}, "VDD_GPU": {"avg": 31.71, "peak": 42.94, "min": 26.39}, "VIN": {"avg": 70.28, "peak": 115.31, "min": 58.59}, "VDD_CPU_SOC_MSS": {"avg": 15.7, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 31.71, "energy_joules_est": 145.78, "sample_count": 36, "duration_seconds": 4.597}, "timestamp": "2026-01-16T15:52:45.452611"}
{"image_index": 208, "image_name": "000000021839.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000021839.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 4029.239, "latencies_ms": [4029.239], "images_per_second": 0.248, "prompt_tokens": 21, "response_tokens_est": 74, "n_tiles": 12, "output_text": "The image depicts a nighttime street scene in an urban area, likely in a city with a European architectural style. A woman is walking on the sidewalk, carrying a bag, while a streetlight and traffic signal are visible in the foreground. The building in the background has a sign that reads \"TADU RIA,\" indicating a possible business or establishment.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25594.6, "ram_available_mb": 100177.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25594.8, "ram_available_mb": 100177.4, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.1, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 32.26, "peak": 42.94, "min": 26.0}, "VIN": {"avg": 71.5, "peak": 102.54, "min": 51.36}, "VDD_CPU_SOC_MSS": {"avg": 15.72, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 32.26, "energy_joules_est": 130.0, "sample_count": 32, "duration_seconds": 4.03}, "timestamp": "2026-01-16T15:52:49.490490"}
{"image_index": 208, "image_name": "000000021839.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000021839.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3354.024, "latencies_ms": [3354.024], "images_per_second": 0.298, "prompt_tokens": 19, "response_tokens_est": 54, "n_tiles": 12, "output_text": "The image depicts a nighttime scene with a building illuminated by warm, yellow lights. The building's exterior is white, and it has multiple balconies with glass windows. The street is dark, and the sky is clear, indicating it is nighttime.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25594.8, "ram_available_mb": 100177.4, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 25594.8, "ram_available_mb": 100177.4, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.07, "peak": 15.65, "min": 14.43}, "VDD_GPU": {"avg": 33.53, "peak": 42.54, "min": 26.4}, "VIN": {"avg": 74.51, "peak": 116.61, "min": 57.48}, "VDD_CPU_SOC_MSS": {"avg": 15.73, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 33.53, "energy_joules_est": 112.47, "sample_count": 26, "duration_seconds": 3.354}, "timestamp": "2026-01-16T15:52:52.851074"}
{"image_index": 209, "image_name": "000000021879.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000021879.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1330.803, "latencies_ms": [1330.803], "images_per_second": 0.751, "prompt_tokens": 9, "response_tokens_est": 28, "n_tiles": 6, "output_text": "A young woman is surfing on a blue surfboard, balancing on the waves, while another person is also surfing on a blue board.", "error": null, "sys_before": {"cpu_percent": 35.5, "ram_used_mb": 25594.8, "ram_available_mb": 100177.4, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 25594.8, "ram_available_mb": 100177.4, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.42, "peak": 14.91, "min": 13.82}, "VDD_GPU": {"avg": 32.62, "peak": 37.82, "min": 26.39}, "VIN": {"avg": 73.34, "peak": 113.69, "min": 51.69}, "VDD_CPU_SOC_MSS": {"avg": 15.2, "peak": 15.75, "min": 14.57}}, "power_watts_avg": 32.62, "energy_joules_est": 43.42, "sample_count": 10, "duration_seconds": 1.331}, "timestamp": "2026-01-16T15:52:54.240793"}
{"image_index": 209, "image_name": "000000021879.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000021879.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1270.388, "latencies_ms": [1270.388], "images_per_second": 0.787, "prompt_tokens": 23, "response_tokens_est": 24, "n_tiles": 6, "output_text": "surfboard: 2\nwoman: 1\nman: 1\nwetsuit: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25594.8, "ram_available_mb": 100177.4, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25594.8, "ram_available_mb": 100177.4, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.68, "peak": 14.91, "min": 14.22}, "VDD_GPU": {"avg": 34.31, "peak": 38.59, "min": 27.97}, "VIN": {"avg": 81.62, "peak": 112.18, "min": 63.87}, "VDD_CPU_SOC_MSS": {"avg": 15.31, "peak": 15.74, "min": 14.96}}, "power_watts_avg": 34.31, "energy_joules_est": 43.6, "sample_count": 9, "duration_seconds": 1.271}, "timestamp": "2026-01-16T15:52:55.517474"}
{"image_index": 209, "image_name": "000000021879.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000021879.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1914.823, "latencies_ms": [1914.823], "images_per_second": 0.522, "prompt_tokens": 27, "response_tokens_est": 48, "n_tiles": 6, "output_text": "The main objects in the image are a blue surfboard and a person riding it. The person is in the foreground, riding the surfboard. The background features a person lying on a surfboard, further away from the main action.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25594.8, "ram_available_mb": 100177.4, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 25595.0, "ram_available_mb": 100177.1, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.83, "peak": 15.22, "min": 14.22}, "VDD_GPU": {"avg": 31.32, "peak": 39.0, "min": 24.43}, "VIN": {"avg": 73.53, "peak": 118.86, "min": 57.63}, "VDD_CPU_SOC_MSS": {"avg": 15.44, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 31.32, "energy_joules_est": 59.99, "sample_count": 14, "duration_seconds": 1.915}, "timestamp": "2026-01-16T15:52:57.438868"}
{"image_index": 209, "image_name": "000000021879.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000021879.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2916.597, "latencies_ms": [2916.597], "images_per_second": 0.343, "prompt_tokens": 21, "response_tokens_est": 85, "n_tiles": 6, "output_text": "The image captures a dynamic scene at the beach, where a young woman is surfing on a blue surfboard. She is skillfully riding a wave, with her body leaning forward and her arms outstretched for balance. In the background, another person is seen lying on a surfboard, seemingly relaxed and enjoying the ocean. The setting is a sunny day at the beach, with clear blue water and a calm atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.0, "ram_available_mb": 100177.1, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 25594.8, "ram_available_mb": 100177.4, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.02, "peak": 15.32, "min": 14.32}, "VDD_GPU": {"avg": 28.14, "peak": 38.19, "min": 23.24}, "VIN": {"avg": 71.37, "peak": 112.28, "min": 57.74}, "VDD_CPU_SOC_MSS": {"avg": 15.54, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 28.14, "energy_joules_est": 82.08, "sample_count": 23, "duration_seconds": 2.917}, "timestamp": "2026-01-16T15:53:00.361723"}
{"image_index": 209, "image_name": "000000021879.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000021879.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2235.222, "latencies_ms": [2235.222], "images_per_second": 0.447, "prompt_tokens": 19, "response_tokens_est": 58, "n_tiles": 6, "output_text": "The image captures a vibrant scene of a young woman surfing on a blue surfboard in the ocean. The water is a mix of blue and green hues, indicating a sunny day with clear skies. The lighting is bright and natural, casting a clear reflection on the water's surface.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25594.8, "ram_available_mb": 100177.4, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 25594.8, "ram_available_mb": 100177.4, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.77, "peak": 15.11, "min": 14.12}, "VDD_GPU": {"avg": 29.2, "peak": 38.21, "min": 23.64}, "VIN": {"avg": 71.84, "peak": 116.54, "min": 56.68}, "VDD_CPU_SOC_MSS": {"avg": 15.33, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 29.2, "energy_joules_est": 65.28, "sample_count": 17, "duration_seconds": 2.235}, "timestamp": "2026-01-16T15:53:02.603063"}
{"image_index": 210, "image_name": "000000021903.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000021903.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2551.901, "latencies_ms": [2551.901], "images_per_second": 0.392, "prompt_tokens": 9, "response_tokens_est": 34, "n_tiles": 12, "output_text": "A man is standing on a concrete platform, reaching out to touch the trunk of an elephant, which is standing on a grassy area with trees in the background.", "error": null, "sys_before": {"cpu_percent": 43.0, "ram_used_mb": 25594.8, "ram_available_mb": 100177.4, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 25594.8, "ram_available_mb": 100177.4, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.13, "peak": 16.15, "min": 13.79}, "VDD_GPU": {"avg": 34.4, "peak": 40.97, "min": 27.18}, "VIN": {"avg": 76.35, "peak": 132.43, "min": 58.37}, "VDD_CPU_SOC_MSS": {"avg": 15.88, "peak": 17.32, "min": 14.96}}, "power_watts_avg": 34.4, "energy_joules_est": 87.8, "sample_count": 20, "duration_seconds": 2.552}, "timestamp": "2026-01-16T15:53:05.246584"}
{"image_index": 210, "image_name": "000000021903.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000021903.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2644.852, "latencies_ms": [2644.852], "images_per_second": 0.378, "prompt_tokens": 23, "response_tokens_est": 33, "n_tiles": 12, "output_text": "1. Elephant\n2. Man\n3. Person\n4. Person\n5. Person\n6. Person\n7. Person\n8. Person", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25594.8, "ram_available_mb": 100177.4, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 25595.0, "ram_available_mb": 100177.2, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.1, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 35.53, "peak": 42.13, "min": 27.18}, "VIN": {"avg": 77.49, "peak": 146.81, "min": 58.56}, "VDD_CPU_SOC_MSS": {"avg": 15.72, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 35.53, "energy_joules_est": 93.99, "sample_count": 20, "duration_seconds": 2.645}, "timestamp": "2026-01-16T15:53:07.897999"}
{"image_index": 210, "image_name": "000000021903.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000021903.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3545.355, "latencies_ms": [3545.355], "images_per_second": 0.282, "prompt_tokens": 27, "response_tokens_est": 60, "n_tiles": 12, "output_text": "The main object in the foreground is a man standing near a fence, with his right hand extended towards an elephant. The elephant is positioned behind the fence, slightly to the left. The background features a dense green area with trees and foliage, indicating a natural or semi-natural environment.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.0, "ram_available_mb": 100177.2, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25594.8, "ram_available_mb": 100177.4, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.1, "peak": 15.65, "min": 14.43}, "VDD_GPU": {"avg": 33.37, "peak": 42.94, "min": 26.39}, "VIN": {"avg": 72.47, "peak": 110.79, "min": 51.64}, "VDD_CPU_SOC_MSS": {"avg": 15.73, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 33.37, "energy_joules_est": 118.33, "sample_count": 28, "duration_seconds": 3.546}, "timestamp": "2026-01-16T15:53:11.450413"}
{"image_index": 210, "image_name": "000000021903.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000021903.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3382.141, "latencies_ms": [3382.141], "images_per_second": 0.296, "prompt_tokens": 21, "response_tokens_est": 55, "n_tiles": 12, "output_text": "The image depicts an outdoor scene with a man in a white shirt and khaki pants standing on a concrete platform, holding onto a metal fence. In the background, there is a lush green area with trees and foliage, suggesting a park or garden setting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25594.8, "ram_available_mb": 100177.4, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25595.0, "ram_available_mb": 100177.2, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.1, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 33.55, "peak": 42.15, "min": 26.39}, "VIN": {"avg": 74.82, "peak": 114.17, "min": 58.27}, "VDD_CPU_SOC_MSS": {"avg": 15.75, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 33.55, "energy_joules_est": 113.48, "sample_count": 26, "duration_seconds": 3.382}, "timestamp": "2026-01-16T15:53:14.839673"}
{"image_index": 210, "image_name": "000000021903.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000021903.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2943.615, "latencies_ms": [2943.615], "images_per_second": 0.34, "prompt_tokens": 19, "response_tokens_est": 42, "n_tiles": 12, "output_text": "The image depicts a man in a white shirt and khaki pants standing next to a fence, with a large elephant in the background. The lighting is natural, and the weather appears to be clear.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.0, "ram_available_mb": 100177.2, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 25595.0, "ram_available_mb": 100177.2, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.09, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 34.51, "peak": 42.94, "min": 26.79}, "VIN": {"avg": 76.0, "peak": 113.49, "min": 58.47}, "VDD_CPU_SOC_MSS": {"avg": 15.71, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 34.51, "energy_joules_est": 101.6, "sample_count": 23, "duration_seconds": 2.944}, "timestamp": "2026-01-16T15:53:17.790816"}
{"image_index": 211, "image_name": "000000022192.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022192.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1071.779, "latencies_ms": [1071.779], "images_per_second": 0.933, "prompt_tokens": 9, "response_tokens_est": 18, "n_tiles": 6, "output_text": "A brown dog is sitting on a bed with various clothes and items scattered around it.", "error": null, "sys_before": {"cpu_percent": 42.5, "ram_used_mb": 25595.0, "ram_available_mb": 100177.2, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 25595.2, "ram_available_mb": 100176.9, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.22, "peak": 14.51, "min": 13.92}, "VDD_GPU": {"avg": 34.55, "peak": 38.19, "min": 29.94}, "VIN": {"avg": 71.31, "peak": 96.55, "min": 51.31}, "VDD_CPU_SOC_MSS": {"avg": 14.9, "peak": 14.96, "min": 14.56}}, "power_watts_avg": 34.55, "energy_joules_est": 37.04, "sample_count": 7, "duration_seconds": 1.072}, "timestamp": "2026-01-16T15:53:18.925607"}
{"image_index": 211, "image_name": "000000022192.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022192.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1513.011, "latencies_ms": [1513.011], "images_per_second": 0.661, "prompt_tokens": 23, "response_tokens_est": 33, "n_tiles": 6, "output_text": "dog: 1\npillow: 1\nblanket: 1\nclothes: 1\npaper: 1\nbox: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.2, "ram_available_mb": 100176.9, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25595.0, "ram_available_mb": 100177.2, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.67, "peak": 15.11, "min": 14.22}, "VDD_GPU": {"avg": 33.41, "peak": 39.39, "min": 26.0}, "VIN": {"avg": 67.21, "peak": 84.43, "min": 56.4}, "VDD_CPU_SOC_MSS": {"avg": 15.32, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 33.41, "energy_joules_est": 50.56, "sample_count": 11, "duration_seconds": 1.513}, "timestamp": "2026-01-16T15:53:20.444739"}
{"image_index": 211, "image_name": "000000022192.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022192.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2717.434, "latencies_ms": [2717.434], "images_per_second": 0.368, "prompt_tokens": 27, "response_tokens_est": 78, "n_tiles": 6, "output_text": "The main object in the foreground is a brown dog sitting on a bed. The dog is positioned near the center of the image, with its body facing the camera and its head turned slightly to the side. The bed is situated in the background, with a white sheet and a brown pillow visible. The dog's position and the bed's placement create a sense of depth in the image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.0, "ram_available_mb": 100177.2, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 25595.0, "ram_available_mb": 100177.2, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.99, "peak": 15.32, "min": 14.22}, "VDD_GPU": {"avg": 28.7, "peak": 38.59, "min": 23.64}, "VIN": {"avg": 71.05, "peak": 97.61, "min": 58.8}, "VDD_CPU_SOC_MSS": {"avg": 15.63, "peak": 16.14, "min": 14.96}}, "power_watts_avg": 28.7, "energy_joules_est": 78.0, "sample_count": 21, "duration_seconds": 2.718}, "timestamp": "2026-01-16T15:53:23.168483"}
{"image_index": 211, "image_name": "000000022192.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022192.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1829.375, "latencies_ms": [1829.375], "images_per_second": 0.547, "prompt_tokens": 21, "response_tokens_est": 45, "n_tiles": 6, "output_text": "The image depicts a cluttered bedroom setting with a brown dog sitting on a bed. The bed is covered with various items, including clothes, pillows, and bedding, creating a messy and disorganized appearance.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25595.0, "ram_available_mb": 100177.2, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 25595.2, "ram_available_mb": 100176.9, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.89, "peak": 15.22, "min": 14.32}, "VDD_GPU": {"avg": 30.75, "peak": 38.19, "min": 24.42}, "VIN": {"avg": 75.5, "peak": 109.41, "min": 63.4}, "VDD_CPU_SOC_MSS": {"avg": 15.49, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 30.75, "energy_joules_est": 56.26, "sample_count": 14, "duration_seconds": 1.83}, "timestamp": "2026-01-16T15:53:25.003594"}
{"image_index": 211, "image_name": "000000022192.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022192.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2229.792, "latencies_ms": [2229.792], "images_per_second": 0.448, "prompt_tokens": 19, "response_tokens_est": 60, "n_tiles": 6, "output_text": "The image shows a brown dog with a white patch on its chest, sitting on a bed with various items scattered around. The lighting is soft and natural, suggesting it might be daytime. The bed has a white sheet and a brown pillow, and there are clothes and other items on the floor.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.2, "ram_available_mb": 100176.9, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 25595.2, "ram_available_mb": 100176.9, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.96, "peak": 15.32, "min": 14.32}, "VDD_GPU": {"avg": 29.75, "peak": 38.21, "min": 24.03}, "VIN": {"avg": 73.46, "peak": 113.22, "min": 61.42}, "VDD_CPU_SOC_MSS": {"avg": 15.58, "peak": 16.14, "min": 14.96}}, "power_watts_avg": 29.75, "energy_joules_est": 66.35, "sample_count": 17, "duration_seconds": 2.23}, "timestamp": "2026-01-16T15:53:27.239732"}
{"image_index": 212, "image_name": "000000022371.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022371.jpg", "image_width": 425, "image_height": 282, "image_resolution": "425x282", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1549.22, "latencies_ms": [1549.22], "images_per_second": 0.645, "prompt_tokens": 9, "response_tokens_est": 36, "n_tiles": 6, "output_text": "The image depicts a man in a white shirt and blue tie, seated at a desk with a laptop and a pen, appearing to be deep in thought or contemplation.", "error": null, "sys_before": {"cpu_percent": 47.1, "ram_used_mb": 25595.0, "ram_available_mb": 100177.2, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 25595.2, "ram_available_mb": 100176.9, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.58, "peak": 15.11, "min": 13.92}, "VDD_GPU": {"avg": 30.82, "peak": 37.03, "min": 24.82}, "VIN": {"avg": 77.04, "peak": 114.1, "min": 64.69}, "VDD_CPU_SOC_MSS": {"avg": 15.32, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 30.82, "energy_joules_est": 47.76, "sample_count": 12, "duration_seconds": 1.55}, "timestamp": "2026-01-16T15:53:28.840200"}
{"image_index": 212, "image_name": "000000022371.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022371.jpg", "image_width": 425, "image_height": 282, "image_resolution": "425x282", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1910.98, "latencies_ms": [1910.98], "images_per_second": 0.523, "prompt_tokens": 23, "response_tokens_est": 48, "n_tiles": 6, "output_text": "object: 1 laptop\nobject: 1 pen\nobject: 1 notebook\nobject: 1 notebook\nobject: 1 pen\nobject: 1 pen\nobject: 1 notebook\nobject: 1 notebook", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.2, "ram_available_mb": 100176.9, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 25595.0, "ram_available_mb": 100177.2, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.87, "peak": 15.22, "min": 14.22}, "VDD_GPU": {"avg": 30.49, "peak": 38.21, "min": 24.42}, "VIN": {"avg": 69.87, "peak": 120.02, "min": 55.7}, "VDD_CPU_SOC_MSS": {"avg": 15.49, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 30.49, "energy_joules_est": 58.28, "sample_count": 15, "duration_seconds": 1.911}, "timestamp": "2026-01-16T15:53:30.757518"}
{"image_index": 212, "image_name": "000000022371.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022371.jpg", "image_width": 425, "image_height": 282, "image_resolution": "425x282", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2717.593, "latencies_ms": [2717.593], "images_per_second": 0.368, "prompt_tokens": 27, "response_tokens_est": 78, "n_tiles": 6, "output_text": "The main objects in the image are a laptop, a pen, and a person. The laptop is positioned on the left side of the image, while the person is sitting in front of it. The pen is placed on the right side of the person, near their hand. The background is blurred, indicating that the focus is on the person and the objects in front of them.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.0, "ram_available_mb": 100177.2, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 25595.0, "ram_available_mb": 100177.2, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.0, "peak": 15.32, "min": 14.32}, "VDD_GPU": {"avg": 28.58, "peak": 38.19, "min": 23.64}, "VIN": {"avg": 67.36, "peak": 101.17, "min": 57.82}, "VDD_CPU_SOC_MSS": {"avg": 15.71, "peak": 16.14, "min": 14.96}}, "power_watts_avg": 28.58, "energy_joules_est": 77.68, "sample_count": 21, "duration_seconds": 2.718}, "timestamp": "2026-01-16T15:53:33.481434"}
{"image_index": 212, "image_name": "000000022371.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022371.jpg", "image_width": 425, "image_height": 282, "image_resolution": "425x282", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2492.744, "latencies_ms": [2492.744], "images_per_second": 0.401, "prompt_tokens": 21, "response_tokens_est": 70, "n_tiles": 6, "output_text": "The image depicts a man sitting at a desk, deeply engrossed in writing on a piece of paper. He is wearing a white shirt and a blue tie, and is holding a pen in his right hand. The setting appears to be an office or a study room, with a blurred background that suggests a modern, clean environment.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.0, "ram_available_mb": 100177.2, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 25595.0, "ram_available_mb": 100177.2, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.97, "peak": 15.32, "min": 14.32}, "VDD_GPU": {"avg": 28.96, "peak": 37.82, "min": 24.03}, "VIN": {"avg": 70.96, "peak": 125.2, "min": 60.16}, "VDD_CPU_SOC_MSS": {"avg": 15.6, "peak": 16.14, "min": 14.96}}, "power_watts_avg": 28.96, "energy_joules_est": 72.2, "sample_count": 19, "duration_seconds": 2.493}, "timestamp": "2026-01-16T15:53:35.980038"}
{"image_index": 212, "image_name": "000000022371.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022371.jpg", "image_width": 425, "image_height": 282, "image_resolution": "425x282", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1670.527, "latencies_ms": [1670.527], "images_per_second": 0.599, "prompt_tokens": 19, "response_tokens_est": 39, "n_tiles": 6, "output_text": "The man in the image is wearing a white shirt with a blue tie. The background is a light blue wall, and the lighting is bright, creating a clear and well-lit scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.0, "ram_available_mb": 100177.2, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 25595.0, "ram_available_mb": 100177.2, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.85, "peak": 15.11, "min": 14.32}, "VDD_GPU": {"avg": 31.18, "peak": 38.21, "min": 24.82}, "VIN": {"avg": 71.27, "peak": 99.04, "min": 58.83}, "VDD_CPU_SOC_MSS": {"avg": 15.54, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 31.18, "energy_joules_est": 52.1, "sample_count": 13, "duration_seconds": 1.671}, "timestamp": "2026-01-16T15:53:37.656980"}
{"image_index": 213, "image_name": "000000022396.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022396.jpg", "image_width": 640, "image_height": 495, "image_resolution": "640x495", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2381.768, "latencies_ms": [2381.768], "images_per_second": 0.42, "prompt_tokens": 9, "response_tokens_est": 29, "n_tiles": 12, "output_text": "The image depicts a large, bright moon in the sky, with a clear blue backdrop, and a single airplane flying in the distance.", "error": null, "sys_before": {"cpu_percent": 39.4, "ram_used_mb": 25595.0, "ram_available_mb": 100177.2, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 25595.0, "ram_available_mb": 100177.2, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.2, "peak": 16.15, "min": 14.2}, "VDD_GPU": {"avg": 35.37, "peak": 41.36, "min": 27.97}, "VIN": {"avg": 77.74, "peak": 119.28, "min": 58.53}, "VDD_CPU_SOC_MSS": {"avg": 16.01, "peak": 17.32, "min": 15.35}}, "power_watts_avg": 35.37, "energy_joules_est": 84.26, "sample_count": 18, "duration_seconds": 2.382}, "timestamp": "2026-01-16T15:53:40.113319"}
{"image_index": 213, "image_name": "000000022396.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022396.jpg", "image_width": 640, "image_height": 495, "image_resolution": "640x495", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2711.122, "latencies_ms": [2711.122], "images_per_second": 0.369, "prompt_tokens": 23, "response_tokens_est": 35, "n_tiles": 12, "output_text": "1. Moon\n2. Airplane\n3. Sky\n4. Earth\n5. Sun\n6. Stars\n7. Clouds\n8. Atmosphere", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.0, "ram_available_mb": 100177.2, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 25595.0, "ram_available_mb": 100177.2, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.06, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 35.45, "peak": 43.33, "min": 27.17}, "VIN": {"avg": 79.27, "peak": 134.78, "min": 59.31}, "VDD_CPU_SOC_MSS": {"avg": 15.69, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 35.45, "energy_joules_est": 96.13, "sample_count": 21, "duration_seconds": 2.712}, "timestamp": "2026-01-16T15:53:42.830903"}
{"image_index": 213, "image_name": "000000022396.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022396.jpg", "image_width": 640, "image_height": 495, "image_resolution": "640x495", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3048.527, "latencies_ms": [3048.527], "images_per_second": 0.328, "prompt_tokens": 27, "response_tokens_est": 45, "n_tiles": 12, "output_text": "The main objects in the image are the moon and the airplane. The moon is positioned in the background, while the airplane is in the foreground. The airplane is closer to the viewer, while the moon is further away.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.0, "ram_available_mb": 100177.2, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 25595.2, "ram_available_mb": 100177.0, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.09, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 34.66, "peak": 42.54, "min": 26.79}, "VIN": {"avg": 77.89, "peak": 130.41, "min": 59.2}, "VDD_CPU_SOC_MSS": {"avg": 15.69, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 34.66, "energy_joules_est": 105.67, "sample_count": 23, "duration_seconds": 3.049}, "timestamp": "2026-01-16T15:53:45.885512"}
{"image_index": 213, "image_name": "000000022396.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022396.jpg", "image_width": 640, "image_height": 495, "image_resolution": "640x495", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3446.604, "latencies_ms": [3446.604], "images_per_second": 0.29, "prompt_tokens": 21, "response_tokens_est": 57, "n_tiles": 12, "output_text": "The image captures a clear blue sky with a large, bright moon visible in the background. The moon is positioned slightly to the left of the center of the image, and it appears to be fully illuminated, suggesting it is either a full moon or a very close moon phase.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.2, "ram_available_mb": 100177.0, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25594.9, "ram_available_mb": 100177.2, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.1, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 33.38, "peak": 42.94, "min": 26.39}, "VIN": {"avg": 71.46, "peak": 113.64, "min": 56.54}, "VDD_CPU_SOC_MSS": {"avg": 15.75, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 33.38, "energy_joules_est": 115.06, "sample_count": 27, "duration_seconds": 3.447}, "timestamp": "2026-01-16T15:53:49.341658"}
{"image_index": 213, "image_name": "000000022396.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022396.jpg", "image_width": 640, "image_height": 495, "image_resolution": "640x495", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3381.863, "latencies_ms": [3381.863], "images_per_second": 0.296, "prompt_tokens": 19, "response_tokens_est": 55, "n_tiles": 12, "output_text": "The image features a large, partially illuminated moon with a reddish hue, set against a clear blue sky. The lighting is soft and diffused, with no harsh shadows, indicating either a cloudy day or a time when the sun is not directly overhead.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25594.9, "ram_available_mb": 100177.2, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 25594.9, "ram_available_mb": 100177.2, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.08, "peak": 15.65, "min": 14.43}, "VDD_GPU": {"avg": 33.59, "peak": 42.15, "min": 26.39}, "VIN": {"avg": 76.74, "peak": 121.43, "min": 58.47}, "VDD_CPU_SOC_MSS": {"avg": 15.65, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 33.59, "energy_joules_est": 113.61, "sample_count": 26, "duration_seconds": 3.382}, "timestamp": "2026-01-16T15:53:52.729933"}
{"image_index": 214, "image_name": "000000022479.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022479.jpg", "image_width": 640, "image_height": 605, "image_resolution": "640x605", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 624.056, "latencies_ms": [624.056], "images_per_second": 1.602, "prompt_tokens": 9, "response_tokens_est": 25, "n_tiles": 1, "output_text": "A skateboarder is performing a trick in a skate park, with a colorful board and a focus on the action.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 25594.9, "ram_available_mb": 100177.2, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25594.9, "ram_available_mb": 100177.2, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 4845.7, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.94, "peak": 15.22, "min": 14.61}, "VDD_GPU": {"avg": 25.01, "peak": 27.57, "min": 22.85}, "VIN": {"avg": 63.45, "peak": 66.98, "min": 60.64}, "VDD_CPU_SOC_MSS": {"avg": 15.55, "peak": 16.14, "min": 14.96}}, "power_watts_avg": 25.01, "energy_joules_est": 15.62, "sample_count": 4, "duration_seconds": 0.625}, "timestamp": "2026-01-16T15:53:53.379988"}
{"image_index": 214, "image_name": "000000022479.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022479.jpg", "image_width": 640, "image_height": 605, "image_resolution": "640x605", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1912.248, "latencies_ms": [1912.248], "images_per_second": 0.523, "prompt_tokens": 23, "response_tokens_est": 86, "n_tiles": 1, "output_text": "skateboarder: 1\nskateboard: 1\nskateboarder's clothes: 1\nskateboarder's hair: 1\nskateboarder's earphones: 1\nskateboarder's shirt: 1\nskateboarder's pants: 1\nskateboarder's legs: 1\nskateboarder's arms: 1", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25594.9, "ram_available_mb": 100177.2, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25595.2, "ram_available_mb": 100177.0, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 4856.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.61, "peak": 15.82, "min": 15.01}, "VDD_GPU": {"avg": 21.61, "peak": 26.0, "min": 20.09}, "VIN": {"avg": 63.0, "peak": 70.03, "min": 52.32}, "VDD_CPU_SOC_MSS": {"avg": 16.42, "peak": 16.54, "min": 15.75}}, "power_watts_avg": 21.61, "energy_joules_est": 41.34, "sample_count": 14, "duration_seconds": 1.913}, "timestamp": "2026-01-16T15:53:55.298613"}
{"image_index": 214, "image_name": "000000022479.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022479.jpg", "image_width": 640, "image_height": 605, "image_resolution": "640x605", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1631.358, "latencies_ms": [1631.358], "images_per_second": 0.613, "prompt_tokens": 27, "response_tokens_est": 73, "n_tiles": 1, "output_text": "The main object in the foreground is a skateboarder performing a trick on a concrete ramp. The skateboarder is positioned near the center of the image, with the skateboard tilted at an angle, indicating a mid-air maneuver. In the background, there are palm trees and a building, suggesting an outdoor setting, possibly a park or skate park.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.2, "ram_available_mb": 100177.0, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25594.9, "ram_available_mb": 100177.2, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 4859.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.66, "peak": 15.82, "min": 15.32}, "VDD_GPU": {"avg": 21.3, "peak": 24.42, "min": 20.09}, "VIN": {"avg": 64.04, "peak": 69.48, "min": 60.39}, "VDD_CPU_SOC_MSS": {"avg": 16.54, "peak": 16.93, "min": 16.14}}, "power_watts_avg": 21.3, "energy_joules_est": 34.76, "sample_count": 12, "duration_seconds": 1.632}, "timestamp": "2026-01-16T15:53:56.935856"}
{"image_index": 214, "image_name": "000000022479.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022479.jpg", "image_width": 640, "image_height": 605, "image_resolution": "640x605", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1657.467, "latencies_ms": [1657.467], "images_per_second": 0.603, "prompt_tokens": 21, "response_tokens_est": 74, "n_tiles": 1, "output_text": "The image captures a dynamic moment at a skate park, where a skateboarder is performing a trick. The skateboarder, dressed in a colorful t-shirt and black pants, is airborne, showcasing impressive skill and control. The background features a park setting with palm trees, a concrete ramp, and other recreational structures, creating a vibrant and energetic atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25594.9, "ram_available_mb": 100177.2, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25594.9, "ram_available_mb": 100177.2, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 4854.7, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.63, "peak": 15.72, "min": 15.32}, "VDD_GPU": {"avg": 21.27, "peak": 24.42, "min": 20.09}, "VIN": {"avg": 65.07, "peak": 69.16, "min": 61.05}, "VDD_CPU_SOC_MSS": {"avg": 16.47, "peak": 16.54, "min": 16.14}}, "power_watts_avg": 21.27, "energy_joules_est": 35.26, "sample_count": 12, "duration_seconds": 1.658}, "timestamp": "2026-01-16T15:53:58.599582"}
{"image_index": 214, "image_name": "000000022479.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022479.jpg", "image_width": 640, "image_height": 605, "image_resolution": "640x605", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 901.685, "latencies_ms": [901.685], "images_per_second": 1.109, "prompt_tokens": 19, "response_tokens_est": 39, "n_tiles": 1, "output_text": "The skateboarder is wearing a colorful tie-dye shirt and black pants. The scene is brightly lit by natural sunlight, casting shadows on the skateboarder and the surrounding environment.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25594.9, "ram_available_mb": 100177.2, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25595.2, "ram_available_mb": 100177.0, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 4853.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.42, "peak": 15.62, "min": 15.11}, "VDD_GPU": {"avg": 22.33, "peak": 24.42, "min": 20.89}, "VIN": {"avg": 63.09, "peak": 71.13, "min": 58.42}, "VDD_CPU_SOC_MSS": {"avg": 16.41, "peak": 16.54, "min": 16.14}}, "power_watts_avg": 22.33, "energy_joules_est": 20.14, "sample_count": 6, "duration_seconds": 0.902}, "timestamp": "2026-01-16T15:53:59.507627"}
{"image_index": 215, "image_name": "000000022589.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022589.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2448.393, "latencies_ms": [2448.393], "images_per_second": 0.408, "prompt_tokens": 9, "response_tokens_est": 31, "n_tiles": 12, "output_text": "The image shows a group of sheep in a pastoral setting, with a wire fence enclosing them and a lush, green forest in the background.", "error": null, "sys_before": {"cpu_percent": 45.1, "ram_used_mb": 25594.9, "ram_available_mb": 100177.2, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 25595.2, "ram_available_mb": 100177.0, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.0, "peak": 15.85, "min": 14.51}, "VDD_GPU": {"avg": 34.73, "peak": 40.18, "min": 27.19}, "VIN": {"avg": 70.38, "peak": 99.03, "min": 55.01}, "VDD_CPU_SOC_MSS": {"avg": 15.92, "peak": 16.92, "min": 14.96}}, "power_watts_avg": 34.73, "energy_joules_est": 85.04, "sample_count": 18, "duration_seconds": 2.449}, "timestamp": "2026-01-16T15:54:02.026677"}
{"image_index": 215, "image_name": "000000022589.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022589.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2885.127, "latencies_ms": [2885.127], "images_per_second": 0.347, "prompt_tokens": 23, "response_tokens_est": 40, "n_tiles": 12, "output_text": "1. Sheep\n2. Sheep\n3. Sheep\n4. Sheep\n5. Sheep\n6. Sheep\n7. Sheep\n8. Sheep", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.2, "ram_available_mb": 100177.0, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 25595.2, "ram_available_mb": 100177.0, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.75, "peak": 15.24, "min": 14.22}, "VDD_GPU": {"avg": 34.92, "peak": 42.53, "min": 26.39}, "VIN": {"avg": 74.3, "peak": 120.59, "min": 58.67}, "VDD_CPU_SOC_MSS": {"avg": 15.31, "peak": 16.14, "min": 14.56}}, "power_watts_avg": 34.92, "energy_joules_est": 100.77, "sample_count": 21, "duration_seconds": 2.886}, "timestamp": "2026-01-16T15:54:04.919673"}
{"image_index": 215, "image_name": "000000022589.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022589.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3298.892, "latencies_ms": [3298.892], "images_per_second": 0.303, "prompt_tokens": 27, "response_tokens_est": 52, "n_tiles": 12, "output_text": "The main objects in the image are sheep, which are located in the foreground. The sheep are positioned near a wire fence, which is in the background. The wire fence is near the sheep, creating a clear spatial relationship between the foreground and background elements.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.2, "ram_available_mb": 100177.0, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 25594.9, "ram_available_mb": 100177.2, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.76, "peak": 15.34, "min": 14.12}, "VDD_GPU": {"avg": 33.73, "peak": 42.94, "min": 26.0}, "VIN": {"avg": 72.86, "peak": 119.13, "min": 57.77}, "VDD_CPU_SOC_MSS": {"avg": 15.29, "peak": 16.14, "min": 14.56}}, "power_watts_avg": 33.73, "energy_joules_est": 111.29, "sample_count": 25, "duration_seconds": 3.299}, "timestamp": "2026-01-16T15:54:08.226234"}
{"image_index": 215, "image_name": "000000022589.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022589.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3917.64, "latencies_ms": [3917.64], "images_per_second": 0.255, "prompt_tokens": 21, "response_tokens_est": 70, "n_tiles": 12, "output_text": "The image depicts a pastoral scene with a flock of sheep in a lush, green forested area. The sheep are gathered around a hay bale, with their woolly coats and dense, fluffy wool visible. The setting suggests a rural, agricultural environment, likely a farm or a pasture, where sheep are being tended to.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25594.9, "ram_available_mb": 100177.2, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25594.7, "ram_available_mb": 100177.5, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.76, "peak": 15.34, "min": 14.22}, "VDD_GPU": {"avg": 32.35, "peak": 42.15, "min": 25.6}, "VIN": {"avg": 71.96, "peak": 121.58, "min": 53.13}, "VDD_CPU_SOC_MSS": {"avg": 15.22, "peak": 16.14, "min": 14.56}}, "power_watts_avg": 32.35, "energy_joules_est": 126.76, "sample_count": 29, "duration_seconds": 3.918}, "timestamp": "2026-01-16T15:54:12.151895"}
{"image_index": 215, "image_name": "000000022589.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022589.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3398.27, "latencies_ms": [3398.27], "images_per_second": 0.294, "prompt_tokens": 19, "response_tokens_est": 55, "n_tiles": 12, "output_text": "The image features a flock of sheep with fluffy, light brown wool. The sheep are situated in a lush, green forested area, with a clear sky overhead. The lighting is natural, suggesting it is daytime, and the overall atmosphere is serene and peaceful.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25594.7, "ram_available_mb": 100177.5, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25594.7, "ram_available_mb": 100177.5, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.75, "peak": 15.34, "min": 14.12}, "VDD_GPU": {"avg": 33.51, "peak": 42.53, "min": 26.0}, "VIN": {"avg": 72.47, "peak": 107.37, "min": 58.76}, "VDD_CPU_SOC_MSS": {"avg": 15.28, "peak": 16.14, "min": 14.56}}, "power_watts_avg": 33.51, "energy_joules_est": 113.89, "sample_count": 26, "duration_seconds": 3.399}, "timestamp": "2026-01-16T15:54:15.556750"}
{"image_index": 216, "image_name": "000000022623.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022623.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2416.858, "latencies_ms": [2416.858], "images_per_second": 0.414, "prompt_tokens": 9, "response_tokens_est": 30, "n_tiles": 12, "output_text": "The image shows a close-up of a vintage-style, metallic, circular button on a computer keyboard, with a red and white symbol on it.", "error": null, "sys_before": {"cpu_percent": 40.5, "ram_used_mb": 25594.7, "ram_available_mb": 100177.5, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 25594.7, "ram_available_mb": 100177.5, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.12, "peak": 16.15, "min": 13.72}, "VDD_GPU": {"avg": 35.3, "peak": 41.76, "min": 27.57}, "VIN": {"avg": 76.93, "peak": 131.04, "min": 58.59}, "VDD_CPU_SOC_MSS": {"avg": 15.85, "peak": 17.32, "min": 14.57}}, "power_watts_avg": 35.3, "energy_joules_est": 85.34, "sample_count": 18, "duration_seconds": 2.418}, "timestamp": "2026-01-16T15:54:18.056107"}
{"image_index": 216, "image_name": "000000022623.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022623.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2979.639, "latencies_ms": [2979.639], "images_per_second": 0.336, "prompt_tokens": 23, "response_tokens_est": 43, "n_tiles": 12, "output_text": "1. Keyboard\n2. Mouse\n3. Computer mouse\n4. Computer mouse pad\n5. Computer mouse pad\n6. Computer mouse pad\n7. Computer mouse pad\n8. Computer mouse pad", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25594.7, "ram_available_mb": 100177.5, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 25594.4, "ram_available_mb": 100177.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.09, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 34.63, "peak": 42.94, "min": 26.79}, "VIN": {"avg": 79.08, "peak": 115.36, "min": 58.85}, "VDD_CPU_SOC_MSS": {"avg": 15.73, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 34.63, "energy_joules_est": 103.2, "sample_count": 23, "duration_seconds": 2.98}, "timestamp": "2026-01-16T15:54:21.041989"}
{"image_index": 216, "image_name": "000000022623.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022623.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3752.221, "latencies_ms": [3752.221], "images_per_second": 0.267, "prompt_tokens": 27, "response_tokens_est": 66, "n_tiles": 12, "output_text": "The main object in the foreground is a silver-colored electronic device with a round button on its surface. The button has a red symbol on it. The background is slightly blurred, but it appears to be a dark surface. The device is positioned near the center of the image, with the button being the most prominent feature.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25594.4, "ram_available_mb": 100177.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25594.7, "ram_available_mb": 100177.5, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.13, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 32.86, "peak": 42.13, "min": 26.39}, "VIN": {"avg": 73.77, "peak": 110.97, "min": 58.6}, "VDD_CPU_SOC_MSS": {"avg": 15.8, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 32.86, "energy_joules_est": 123.32, "sample_count": 28, "duration_seconds": 3.753}, "timestamp": "2026-01-16T15:54:24.806281"}
{"image_index": 216, "image_name": "000000022623.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022623.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3516.718, "latencies_ms": [3516.718], "images_per_second": 0.284, "prompt_tokens": 21, "response_tokens_est": 59, "n_tiles": 12, "output_text": "The image shows a close-up view of a computer keyboard with a focus on the keys. The keys are illuminated by a light source, creating a warm and inviting atmosphere. The keyboard appears to be placed on a desk or table, and the overall setting suggests a workspace or study area.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25594.7, "ram_available_mb": 100177.5, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 25594.4, "ram_available_mb": 100177.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.08, "peak": 15.65, "min": 14.43}, "VDD_GPU": {"avg": 33.26, "peak": 42.94, "min": 26.39}, "VIN": {"avg": 75.07, "peak": 114.79, "min": 58.37}, "VDD_CPU_SOC_MSS": {"avg": 15.7, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 33.26, "energy_joules_est": 116.99, "sample_count": 27, "duration_seconds": 3.517}, "timestamp": "2026-01-16T15:54:28.329516"}
{"image_index": 216, "image_name": "000000022623.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022623.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3482.936, "latencies_ms": [3482.936], "images_per_second": 0.287, "prompt_tokens": 19, "response_tokens_est": 58, "n_tiles": 12, "output_text": "The image features a close-up of a silver-colored electronic device with a glossy finish. The lighting is soft and warm, casting a gentle glow on the device's surface. The device appears to be made of a metallic material, possibly aluminum, and has a sleek, modern design.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25594.4, "ram_available_mb": 100177.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25594.6, "ram_available_mb": 100177.5, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.09, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 33.62, "peak": 42.94, "min": 26.39}, "VIN": {"avg": 75.9, "peak": 119.08, "min": 58.58}, "VDD_CPU_SOC_MSS": {"avg": 15.69, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 33.62, "energy_joules_est": 117.11, "sample_count": 26, "duration_seconds": 3.483}, "timestamp": "2026-01-16T15:54:31.819002"}
{"image_index": 217, "image_name": "000000022705.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022705.jpg", "image_width": 482, "image_height": 640, "image_resolution": "482x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2479.025, "latencies_ms": [2479.025], "images_per_second": 0.403, "prompt_tokens": 9, "response_tokens_est": 32, "n_tiles": 12, "output_text": "A woman is standing in a kitchen, holding a glass of orange juice, wearing a black dress with a starry pattern, and smiling at the camera.", "error": null, "sys_before": {"cpu_percent": 41.7, "ram_used_mb": 25594.6, "ram_available_mb": 100177.5, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 25594.4, "ram_available_mb": 100177.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.16, "peak": 16.15, "min": 14.0}, "VDD_GPU": {"avg": 35.06, "peak": 41.76, "min": 27.57}, "VIN": {"avg": 74.95, "peak": 120.46, "min": 58.57}, "VDD_CPU_SOC_MSS": {"avg": 16.04, "peak": 17.32, "min": 14.96}}, "power_watts_avg": 35.06, "energy_joules_est": 86.93, "sample_count": 19, "duration_seconds": 2.479}, "timestamp": "2026-01-16T15:54:34.380150"}
{"image_index": 217, "image_name": "000000022705.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022705.jpg", "image_width": 482, "image_height": 640, "image_resolution": "482x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 5847.072, "latencies_ms": [5847.072], "images_per_second": 0.171, "prompt_tokens": 23, "response_tokens_est": 129, "n_tiles": 12, "output_text": "- woman: 1\n- dress: 1\n- glass: 1\n- necklace: 1\n- purse: 0\n- handbag: 0\n- shoes: 1\n- table: 0\n- cabinet: 1\n- refrigerator: 1\n- wall: 0\n- door: 0\n- countertop: 1\n- sink: 0\n- cabinet door: 0\n- drawer: 0\n- light switch: 0\n- wall outlet: 0\n- appliance: 1\n- floor: 1\n- tile:", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25594.4, "ram_available_mb": 100177.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 25594.6, "ram_available_mb": 100177.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.13, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 30.55, "peak": 42.54, "min": 26.0}, "VIN": {"avg": 72.71, "peak": 113.92, "min": 58.2}, "VDD_CPU_SOC_MSS": {"avg": 15.77, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 30.55, "energy_joules_est": 178.64, "sample_count": 46, "duration_seconds": 5.848}, "timestamp": "2026-01-16T15:54:40.234340"}
{"image_index": 217, "image_name": "000000022705.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022705.jpg", "image_width": 482, "image_height": 640, "image_resolution": "482x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3184.238, "latencies_ms": [3184.238], "images_per_second": 0.314, "prompt_tokens": 27, "response_tokens_est": 49, "n_tiles": 12, "output_text": "The main object in the foreground is a woman standing next to a stainless steel refrigerator. She is holding a glass of orange juice and wearing a black dress with a starry pattern. The background features wooden cabinets and a tiled backsplash.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25594.6, "ram_available_mb": 100177.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 25594.4, "ram_available_mb": 100177.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.1, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 34.1, "peak": 42.15, "min": 26.39}, "VIN": {"avg": 76.15, "peak": 114.51, "min": 58.25}, "VDD_CPU_SOC_MSS": {"avg": 15.73, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 34.1, "energy_joules_est": 108.6, "sample_count": 24, "duration_seconds": 3.185}, "timestamp": "2026-01-16T15:54:43.425400"}
{"image_index": 217, "image_name": "000000022705.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022705.jpg", "image_width": 482, "image_height": 640, "image_resolution": "482x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3653.946, "latencies_ms": [3653.946], "images_per_second": 0.274, "prompt_tokens": 21, "response_tokens_est": 63, "n_tiles": 12, "output_text": "The image depicts a woman standing in a kitchen, holding a glass of orange juice. She is wearing a black dress and black shoes, and the kitchen has wooden cabinets and a stainless steel refrigerator. The woman appears to be in a relaxed and happy mood, possibly enjoying a casual drink in her home kitchen.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25594.4, "ram_available_mb": 100177.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25594.4, "ram_available_mb": 100177.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.09, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 33.13, "peak": 42.94, "min": 26.39}, "VIN": {"avg": 75.99, "peak": 127.19, "min": 56.34}, "VDD_CPU_SOC_MSS": {"avg": 15.77, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 33.13, "energy_joules_est": 121.06, "sample_count": 28, "duration_seconds": 3.654}, "timestamp": "2026-01-16T15:54:47.089903"}
{"image_index": 217, "image_name": "000000022705.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022705.jpg", "image_width": 482, "image_height": 640, "image_resolution": "482x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2911.775, "latencies_ms": [2911.775], "images_per_second": 0.343, "prompt_tokens": 19, "response_tokens_est": 41, "n_tiles": 12, "output_text": "The image features a woman standing in a kitchen with a metallic refrigerator and wooden cabinets. The kitchen has a warm, cozy atmosphere with beige and brown tones, and the lighting is soft and ambient.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25594.4, "ram_available_mb": 100177.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 25594.4, "ram_available_mb": 100177.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.08, "peak": 15.65, "min": 14.43}, "VDD_GPU": {"avg": 34.74, "peak": 42.54, "min": 26.79}, "VIN": {"avg": 77.78, "peak": 130.37, "min": 58.58}, "VDD_CPU_SOC_MSS": {"avg": 15.71, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 34.74, "energy_joules_est": 101.17, "sample_count": 22, "duration_seconds": 2.912}, "timestamp": "2026-01-16T15:54:50.008381"}
{"image_index": 218, "image_name": "000000022755.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022755.jpg", "image_width": 640, "image_height": 479, "image_resolution": "640x479", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2513.169, "latencies_ms": [2513.169], "images_per_second": 0.398, "prompt_tokens": 9, "response_tokens_est": 33, "n_tiles": 12, "output_text": "The image shows a close-up view of a bus mirror reflecting a yellow school bus on a road, with other vehicles and a cloudy sky in the background.", "error": null, "sys_before": {"cpu_percent": 41.7, "ram_used_mb": 25594.4, "ram_available_mb": 100177.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 25594.4, "ram_available_mb": 100177.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.14, "peak": 16.15, "min": 13.9}, "VDD_GPU": {"avg": 34.97, "peak": 41.36, "min": 27.18}, "VIN": {"avg": 75.04, "peak": 111.09, "min": 58.41}, "VDD_CPU_SOC_MSS": {"avg": 16.02, "peak": 17.32, "min": 14.96}}, "power_watts_avg": 34.97, "energy_joules_est": 87.9, "sample_count": 19, "duration_seconds": 2.514}, "timestamp": "2026-01-16T15:54:52.606248"}
{"image_index": 218, "image_name": "000000022755.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022755.jpg", "image_width": 640, "image_height": 479, "image_resolution": "640x479", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2611.183, "latencies_ms": [2611.183], "images_per_second": 0.383, "prompt_tokens": 23, "response_tokens_est": 32, "n_tiles": 12, "output_text": "1. Bus\n2. Car\n3. Car\n4. Car\n5. Car\n6. Car\n7. Car\n8. Car", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25594.4, "ram_available_mb": 100177.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 25594.4, "ram_available_mb": 100177.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.08, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 35.92, "peak": 42.92, "min": 27.57}, "VIN": {"avg": 77.96, "peak": 137.76, "min": 49.97}, "VDD_CPU_SOC_MSS": {"avg": 15.73, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 35.92, "energy_joules_est": 93.81, "sample_count": 20, "duration_seconds": 2.612}, "timestamp": "2026-01-16T15:54:55.223648"}
{"image_index": 218, "image_name": "000000022755.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022755.jpg", "image_width": 640, "image_height": 479, "image_resolution": "640x479", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 4226.74, "latencies_ms": [4226.74], "images_per_second": 0.237, "prompt_tokens": 27, "response_tokens_est": 80, "n_tiles": 12, "output_text": "The main object in the foreground is a bicycle handlebar, which is positioned near the center of the image. The bicycle handlebar is in sharp focus, while the background is slightly blurred, indicating that the focus is on the handlebar itself. The background features a bus and a road, with the bus being slightly out of focus, suggesting that the bus is farther away from the camera.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25594.4, "ram_available_mb": 100177.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 25594.4, "ram_available_mb": 100177.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.11, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 32.26, "peak": 42.92, "min": 26.39}, "VIN": {"avg": 72.06, "peak": 118.28, "min": 54.92}, "VDD_CPU_SOC_MSS": {"avg": 15.76, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 32.26, "energy_joules_est": 136.37, "sample_count": 32, "duration_seconds": 4.227}, "timestamp": "2026-01-16T15:54:59.456919"}
{"image_index": 218, "image_name": "000000022755.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022755.jpg", "image_width": 640, "image_height": 479, "image_resolution": "640x479", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3673.805, "latencies_ms": [3673.805], "images_per_second": 0.272, "prompt_tokens": 21, "response_tokens_est": 63, "n_tiles": 12, "output_text": "The image depicts a scene from a bus stop, with a focus on a bus mirror reflecting the surroundings. The bus stop is situated outdoors, and the mirror captures a glimpse of a busy street with cars and a yellow bus. The setting appears to be during the daytime, with a cloudy sky overhead.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25594.4, "ram_available_mb": 100177.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25594.4, "ram_available_mb": 100177.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.81, "peak": 15.34, "min": 14.22}, "VDD_GPU": {"avg": 32.83, "peak": 42.94, "min": 26.0}, "VIN": {"avg": 72.55, "peak": 119.24, "min": 58.32}, "VDD_CPU_SOC_MSS": {"avg": 15.37, "peak": 16.14, "min": 14.56}}, "power_watts_avg": 32.83, "energy_joules_est": 120.62, "sample_count": 29, "duration_seconds": 3.674}, "timestamp": "2026-01-16T15:55:03.137365"}
{"image_index": 218, "image_name": "000000022755.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022755.jpg", "image_width": 640, "image_height": 479, "image_resolution": "640x479", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2745.395, "latencies_ms": [2745.395], "images_per_second": 0.364, "prompt_tokens": 19, "response_tokens_est": 36, "n_tiles": 12, "output_text": "The image features a yellow school bus with a reflective surface, reflecting the surrounding environment. The bus is parked on a street during the day, with a cloudy sky overhead.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25594.4, "ram_available_mb": 100177.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 25594.4, "ram_available_mb": 100177.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.77, "peak": 15.34, "min": 14.12}, "VDD_GPU": {"avg": 35.09, "peak": 42.54, "min": 26.79}, "VIN": {"avg": 73.33, "peak": 121.78, "min": 58.21}, "VDD_CPU_SOC_MSS": {"avg": 15.35, "peak": 16.14, "min": 14.56}}, "power_watts_avg": 35.09, "energy_joules_est": 96.35, "sample_count": 21, "duration_seconds": 2.746}, "timestamp": "2026-01-16T15:55:05.889374"}
{"image_index": 219, "image_name": "000000022892.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022892.jpg", "image_width": 500, "image_height": 334, "image_resolution": "500x334", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1256.057, "latencies_ms": [1256.057], "images_per_second": 0.796, "prompt_tokens": 9, "response_tokens_est": 25, "n_tiles": 6, "output_text": "A gray cat is sitting on a wooden table, observing a small plant with green leaves and a small pot of soil.", "error": null, "sys_before": {"cpu_percent": 29.5, "ram_used_mb": 25594.4, "ram_available_mb": 100177.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25594.4, "ram_available_mb": 100177.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.28, "peak": 14.81, "min": 13.82}, "VDD_GPU": {"avg": 33.13, "peak": 37.82, "min": 27.18}, "VIN": {"avg": 69.56, "peak": 79.65, "min": 63.69}, "VDD_CPU_SOC_MSS": {"avg": 15.0, "peak": 15.36, "min": 14.57}}, "power_watts_avg": 33.13, "energy_joules_est": 41.63, "sample_count": 9, "duration_seconds": 1.257}, "timestamp": "2026-01-16T15:55:07.201346"}
{"image_index": 219, "image_name": "000000022892.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022892.jpg", "image_width": 500, "image_height": 334, "image_resolution": "500x334", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1517.503, "latencies_ms": [1517.503], "images_per_second": 0.659, "prompt_tokens": 23, "response_tokens_est": 32, "n_tiles": 6, "output_text": "cat: 1\ndog: 1\nplant: 2\npot: 2\ntray: 1\ncans: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25594.4, "ram_available_mb": 100177.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25594.4, "ram_available_mb": 100177.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.55, "peak": 14.81, "min": 14.02}, "VDD_GPU": {"avg": 32.84, "peak": 38.98, "min": 25.6}, "VIN": {"avg": 73.47, "peak": 106.71, "min": 60.29}, "VDD_CPU_SOC_MSS": {"avg": 15.21, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 32.84, "energy_joules_est": 49.85, "sample_count": 11, "duration_seconds": 1.518}, "timestamp": "2026-01-16T15:55:08.725538"}
{"image_index": 219, "image_name": "000000022892.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022892.jpg", "image_width": 500, "image_height": 334, "image_resolution": "500x334", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2051.216, "latencies_ms": [2051.216], "images_per_second": 0.488, "prompt_tokens": 27, "response_tokens_est": 51, "n_tiles": 6, "output_text": "The main objects in the image are a cat and a dog. The cat is positioned in the foreground, while the dog is slightly blurred in the background. The cat is sitting on a wooden table, and the dog is standing near the table.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25594.4, "ram_available_mb": 100177.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 25594.6, "ram_available_mb": 100177.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.67, "peak": 15.01, "min": 14.02}, "VDD_GPU": {"avg": 30.17, "peak": 38.59, "min": 23.64}, "VIN": {"avg": 68.62, "peak": 109.28, "min": 56.47}, "VDD_CPU_SOC_MSS": {"avg": 15.22, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 30.17, "energy_joules_est": 61.89, "sample_count": 15, "duration_seconds": 2.051}, "timestamp": "2026-01-16T15:55:10.786983"}
{"image_index": 219, "image_name": "000000022892.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022892.jpg", "image_width": 500, "image_height": 334, "image_resolution": "500x334", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1935.85, "latencies_ms": [1935.85], "images_per_second": 0.517, "prompt_tokens": 21, "response_tokens_est": 47, "n_tiles": 6, "output_text": "The image depicts a cozy indoor scene with a cat and a dog sitting on a wooden table. The cat is looking at the dog, and both are surrounded by various plants and gardening supplies, suggesting a relaxed and domestic atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25594.6, "ram_available_mb": 100177.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 25594.6, "ram_available_mb": 100177.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.68, "peak": 15.01, "min": 14.02}, "VDD_GPU": {"avg": 29.96, "peak": 38.21, "min": 23.64}, "VIN": {"avg": 71.54, "peak": 112.68, "min": 58.37}, "VDD_CPU_SOC_MSS": {"avg": 15.35, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 29.96, "energy_joules_est": 58.02, "sample_count": 15, "duration_seconds": 1.937}, "timestamp": "2026-01-16T15:55:12.730077"}
{"image_index": 219, "image_name": "000000022892.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022892.jpg", "image_width": 500, "image_height": 334, "image_resolution": "500x334", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1486.812, "latencies_ms": [1486.812], "images_per_second": 0.673, "prompt_tokens": 19, "response_tokens_est": 31, "n_tiles": 6, "output_text": "The cat in the image has a gray coat and is sitting on a wooden table. The lighting is bright, and the weather appears to be clear.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25594.6, "ram_available_mb": 100177.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 25594.3, "ram_available_mb": 100177.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.65, "peak": 14.91, "min": 14.12}, "VDD_GPU": {"avg": 32.01, "peak": 38.6, "min": 25.21}, "VIN": {"avg": 75.12, "peak": 117.56, "min": 62.93}, "VDD_CPU_SOC_MSS": {"avg": 15.21, "peak": 15.36, "min": 14.57}}, "power_watts_avg": 32.01, "energy_joules_est": 47.6, "sample_count": 11, "duration_seconds": 1.487}, "timestamp": "2026-01-16T15:55:14.223991"}
{"image_index": 220, "image_name": "000000022935.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022935.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2401.985, "latencies_ms": [2401.985], "images_per_second": 0.416, "prompt_tokens": 9, "response_tokens_est": 30, "n_tiles": 12, "output_text": "The image shows a female soccer player in a blue jersey with a logo on the chest, holding a soccer ball, and wearing a headband.", "error": null, "sys_before": {"cpu_percent": 45.9, "ram_used_mb": 25594.3, "ram_available_mb": 100177.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 25594.6, "ram_available_mb": 100177.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.88, "peak": 15.85, "min": 13.79}, "VDD_GPU": {"avg": 35.43, "peak": 41.76, "min": 27.57}, "VIN": {"avg": 83.54, "peak": 125.78, "min": 59.22}, "VDD_CPU_SOC_MSS": {"avg": 15.75, "peak": 16.92, "min": 14.96}}, "power_watts_avg": 35.43, "energy_joules_est": 85.12, "sample_count": 18, "duration_seconds": 2.402}, "timestamp": "2026-01-16T15:55:16.709859"}
{"image_index": 220, "image_name": "000000022935.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022935.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2587.581, "latencies_ms": [2587.581], "images_per_second": 0.386, "prompt_tokens": 23, "response_tokens_est": 32, "n_tiles": 12, "output_text": "ball: 1\nwoman: 2\nshorts: 1\nshirt: 1\nheadband: 1\nhair: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25594.6, "ram_available_mb": 100177.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 25594.2, "ram_available_mb": 100177.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.82, "peak": 15.34, "min": 14.22}, "VDD_GPU": {"avg": 36.19, "peak": 42.54, "min": 27.57}, "VIN": {"avg": 77.28, "peak": 121.02, "min": 59.0}, "VDD_CPU_SOC_MSS": {"avg": 15.46, "peak": 16.14, "min": 14.96}}, "power_watts_avg": 36.19, "energy_joules_est": 93.66, "sample_count": 19, "duration_seconds": 2.588}, "timestamp": "2026-01-16T15:55:19.303978"}
{"image_index": 220, "image_name": "000000022935.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022935.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 4223.173, "latencies_ms": [4223.173], "images_per_second": 0.237, "prompt_tokens": 27, "response_tokens_est": 79, "n_tiles": 12, "output_text": "The main object in the foreground is a young woman wearing a blue sports jersey with a logo on the left side of her chest. She is holding a volleyball in her right hand, which is near her body. The background features another person wearing a yellow shirt, slightly out of focus. The volleyball and the woman are in the foreground, while the other person is in the background.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25594.0, "ram_available_mb": 100178.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25593.7, "ram_available_mb": 100178.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.78, "peak": 15.34, "min": 14.22}, "VDD_GPU": {"avg": 32.03, "peak": 42.53, "min": 25.6}, "VIN": {"avg": 70.59, "peak": 121.31, "min": 56.63}, "VDD_CPU_SOC_MSS": {"avg": 15.3, "peak": 16.14, "min": 14.56}}, "power_watts_avg": 32.03, "energy_joules_est": 135.28, "sample_count": 32, "duration_seconds": 4.223}, "timestamp": "2026-01-16T15:55:23.534085"}
{"image_index": 220, "image_name": "000000022935.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022935.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3813.166, "latencies_ms": [3813.166], "images_per_second": 0.262, "prompt_tokens": 21, "response_tokens_est": 67, "n_tiles": 12, "output_text": "The image captures a moment during a women's soccer match. The players are actively engaged in the game, with one player in a blue jersey and the other in a yellow jersey. The scene takes place on a grassy field, and both players are focused on the game, with the player in blue preparing to make a play.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25593.7, "ram_available_mb": 100178.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25593.7, "ram_available_mb": 100178.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.77, "peak": 15.34, "min": 14.22}, "VDD_GPU": {"avg": 32.42, "peak": 42.15, "min": 25.6}, "VIN": {"avg": 70.82, "peak": 119.13, "min": 55.1}, "VDD_CPU_SOC_MSS": {"avg": 15.26, "peak": 16.14, "min": 14.56}}, "power_watts_avg": 32.42, "energy_joules_est": 123.63, "sample_count": 29, "duration_seconds": 3.813}, "timestamp": "2026-01-16T15:55:27.353816"}
{"image_index": 220, "image_name": "000000022935.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022935.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3711.107, "latencies_ms": [3711.107], "images_per_second": 0.269, "prompt_tokens": 19, "response_tokens_est": 64, "n_tiles": 12, "output_text": "The image features a young woman in a blue sports jersey with white and red accents, standing on a grassy field. The lighting is bright, casting a clear view of her attire and the surrounding environment. The weather appears to be clear and sunny, enhancing the vivid colors of her jersey and the green grass.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25593.7, "ram_available_mb": 100178.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25593.7, "ram_available_mb": 100178.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.73, "peak": 15.34, "min": 14.12}, "VDD_GPU": {"avg": 32.75, "peak": 42.54, "min": 26.0}, "VIN": {"avg": 72.34, "peak": 122.0, "min": 58.5}, "VDD_CPU_SOC_MSS": {"avg": 15.2, "peak": 16.14, "min": 14.56}}, "power_watts_avg": 32.75, "energy_joules_est": 121.56, "sample_count": 29, "duration_seconds": 3.712}, "timestamp": "2026-01-16T15:55:31.073693"}
{"image_index": 221, "image_name": "000000022969.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022969.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1951.604, "latencies_ms": [1951.604], "images_per_second": 0.512, "prompt_tokens": 9, "response_tokens_est": 16, "n_tiles": 12, "output_text": "A giraffe is standing in a pen, eating from a wooden fence.", "error": null, "sys_before": {"cpu_percent": 45.6, "ram_used_mb": 25593.7, "ram_available_mb": 100178.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 2.3, "ram_used_mb": 25593.7, "ram_available_mb": 100178.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.08, "peak": 16.15, "min": 13.59}, "VDD_GPU": {"avg": 36.92, "peak": 41.76, "min": 30.33}, "VIN": {"avg": 81.15, "peak": 112.37, "min": 60.39}, "VDD_CPU_SOC_MSS": {"avg": 16.01, "peak": 17.32, "min": 14.57}}, "power_watts_avg": 36.92, "energy_joules_est": 72.06, "sample_count": 15, "duration_seconds": 1.952}, "timestamp": "2026-01-16T15:55:33.120896"}
{"image_index": 221, "image_name": "000000022969.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022969.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2176.804, "latencies_ms": [2176.804], "images_per_second": 0.459, "prompt_tokens": 23, "response_tokens_est": 19, "n_tiles": 12, "output_text": "giraffe: 1\nfence: 1\ntrees: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25593.7, "ram_available_mb": 100178.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 2.5, "ram_used_mb": 25593.7, "ram_available_mb": 100178.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.11, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 38.18, "peak": 43.73, "min": 30.32}, "VIN": {"avg": 83.6, "peak": 118.92, "min": 58.19}, "VDD_CPU_SOC_MSS": {"avg": 15.84, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 38.18, "energy_joules_est": 83.12, "sample_count": 16, "duration_seconds": 2.177}, "timestamp": "2026-01-16T15:55:35.307902"}
{"image_index": 221, "image_name": "000000022969.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022969.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3354.309, "latencies_ms": [3354.309], "images_per_second": 0.298, "prompt_tokens": 27, "response_tokens_est": 54, "n_tiles": 12, "output_text": "The giraffe is positioned in the foreground, standing near the wooden fence. The background features a dense forest with tall trees, creating a natural and serene setting. The fence is located to the left of the giraffe, separating it from the grassy area.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25593.7, "ram_available_mb": 100178.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 25593.7, "ram_available_mb": 100178.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.09, "peak": 15.65, "min": 14.43}, "VDD_GPU": {"avg": 34.05, "peak": 43.71, "min": 26.39}, "VIN": {"avg": 76.28, "peak": 116.91, "min": 58.56}, "VDD_CPU_SOC_MSS": {"avg": 15.68, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 34.05, "energy_joules_est": 114.23, "sample_count": 26, "duration_seconds": 3.355}, "timestamp": "2026-01-16T15:55:38.668601"}
{"image_index": 221, "image_name": "000000022969.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022969.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3034.163, "latencies_ms": [3034.163], "images_per_second": 0.33, "prompt_tokens": 21, "response_tokens_est": 45, "n_tiles": 12, "output_text": "The image depicts a giraffe standing in a grassy area with a wooden fence in the background. The giraffe appears to be eating from a patch of dirt, surrounded by lush green trees and a clear sky.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25593.7, "ram_available_mb": 100178.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 25593.7, "ram_available_mb": 100178.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.11, "peak": 15.65, "min": 14.63}, "VDD_GPU": {"avg": 34.32, "peak": 42.94, "min": 26.79}, "VIN": {"avg": 72.34, "peak": 108.91, "min": 58.65}, "VDD_CPU_SOC_MSS": {"avg": 15.75, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 34.32, "energy_joules_est": 104.15, "sample_count": 24, "duration_seconds": 3.035}, "timestamp": "2026-01-16T15:55:41.708891"}
{"image_index": 221, "image_name": "000000022969.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000022969.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3817.447, "latencies_ms": [3817.447], "images_per_second": 0.262, "prompt_tokens": 19, "response_tokens_est": 68, "n_tiles": 12, "output_text": "The giraffes in the image have a distinctive pattern of brown and white spots on their bodies, which stands out against the greenery of the surrounding trees. The lighting is bright and natural, indicating that the photo was taken during the day. The weather appears to be clear and sunny, with no visible signs of rain or clouds.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25593.7, "ram_available_mb": 100178.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25593.7, "ram_available_mb": 100178.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.11, "peak": 15.65, "min": 14.63}, "VDD_GPU": {"avg": 32.69, "peak": 42.92, "min": 26.39}, "VIN": {"avg": 74.07, "peak": 116.21, "min": 58.74}, "VDD_CPU_SOC_MSS": {"avg": 15.72, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 32.69, "energy_joules_est": 124.8, "sample_count": 30, "duration_seconds": 3.818}, "timestamp": "2026-01-16T15:55:45.533517"}
{"image_index": 222, "image_name": "000000023023.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023023.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1460.0, "latencies_ms": [1460.0], "images_per_second": 0.685, "prompt_tokens": 9, "response_tokens_est": 64, "n_tiles": 1, "output_text": "The image shows a black and white photograph of three items: a large, dark-colored suitcase with a handle and a zipper compartment, a smaller, black trash bag with a zipper, and a black, rectangular bag with a zipper, all placed on a carpeted floor against a backdrop of curtains.", "error": null, "sys_before": {"cpu_percent": 17.2, "ram_used_mb": 25593.7, "ram_available_mb": 100178.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25593.9, "ram_available_mb": 100178.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 4845.7, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.39, "peak": 15.72, "min": 14.51}, "VDD_GPU": {"avg": 22.42, "peak": 27.59, "min": 20.1}, "VIN": {"avg": 62.34, "peak": 65.61, "min": 56.77}, "VDD_CPU_SOC_MSS": {"avg": 16.11, "peak": 16.54, "min": 14.96}}, "power_watts_avg": 22.42, "energy_joules_est": 32.74, "sample_count": 11, "duration_seconds": 1.46}, "timestamp": "2026-01-16T15:55:47.019096"}
{"image_index": 222, "image_name": "000000023023.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023023.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1284.036, "latencies_ms": [1284.036], "images_per_second": 0.779, "prompt_tokens": 23, "response_tokens_est": 57, "n_tiles": 1, "output_text": "1. Luggage: 2\n2. Bag: 2\n3. Bag: 1\n4. Bag: 1\n5. Bag: 1\n6. Bag: 1\n7. Bag: 1\n8. Bag: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25593.9, "ram_available_mb": 100178.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25593.9, "ram_available_mb": 100178.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 4856.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.59, "peak": 15.72, "min": 15.32}, "VDD_GPU": {"avg": 21.76, "peak": 24.42, "min": 20.48}, "VIN": {"avg": 62.8, "peak": 65.67, "min": 57.22}, "VDD_CPU_SOC_MSS": {"avg": 16.45, "peak": 16.54, "min": 16.14}}, "power_watts_avg": 21.76, "energy_joules_est": 27.95, "sample_count": 9, "duration_seconds": 1.285}, "timestamp": "2026-01-16T15:55:48.310156"}
{"image_index": 222, "image_name": "000000023023.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023023.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1720.943, "latencies_ms": [1720.943], "images_per_second": 0.581, "prompt_tokens": 27, "response_tokens_est": 77, "n_tiles": 1, "output_text": "The main objects in the image are a large gray suitcase and a black bag. The suitcase is positioned in the foreground, slightly to the right, while the black bag is located to the right of the suitcase, closer to the background. The suitcase is closer to the viewer, while the black bag is further away, creating a sense of depth in the image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25593.9, "ram_available_mb": 100178.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25593.4, "ram_available_mb": 100178.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 4859.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.63, "peak": 15.72, "min": 15.32}, "VDD_GPU": {"avg": 21.21, "peak": 24.41, "min": 20.09}, "VIN": {"avg": 63.17, "peak": 71.63, "min": 53.58}, "VDD_CPU_SOC_MSS": {"avg": 16.48, "peak": 16.54, "min": 16.14}}, "power_watts_avg": 21.21, "energy_joules_est": 36.51, "sample_count": 13, "duration_seconds": 1.721}, "timestamp": "2026-01-16T15:55:50.036829"}
{"image_index": 222, "image_name": "000000023023.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023023.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1096.525, "latencies_ms": [1096.525], "images_per_second": 0.912, "prompt_tokens": 21, "response_tokens_est": 48, "n_tiles": 1, "output_text": "The image depicts a black and white scene featuring a suitcase, a trash bag, and a small pile of books on a carpeted floor. The setting appears to be indoors, possibly in a room with curtains in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25593.4, "ram_available_mb": 100178.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25593.4, "ram_available_mb": 100178.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 4854.7, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.6, "peak": 15.72, "min": 15.32}, "VDD_GPU": {"avg": 21.86, "peak": 24.42, "min": 20.48}, "VIN": {"avg": 62.52, "peak": 63.81, "min": 61.92}, "VDD_CPU_SOC_MSS": {"avg": 16.44, "peak": 16.54, "min": 16.14}}, "power_watts_avg": 21.86, "energy_joules_est": 23.98, "sample_count": 8, "duration_seconds": 1.097}, "timestamp": "2026-01-16T15:55:51.138826"}
{"image_index": 222, "image_name": "000000023023.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023023.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1524.351, "latencies_ms": [1524.351], "images_per_second": 0.656, "prompt_tokens": 19, "response_tokens_est": 68, "n_tiles": 1, "output_text": "The image features a black and white photograph of a suitcase, a trash bag, and a black bag on a carpeted floor. The suitcase is gray with a zipper and a handle, while the trash bag is black with a reflective surface. The lighting is soft and even, and the background consists of draped curtains.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25593.4, "ram_available_mb": 100178.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25593.4, "ram_available_mb": 100178.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 4853.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.63, "peak": 15.82, "min": 15.22}, "VDD_GPU": {"avg": 21.38, "peak": 24.43, "min": 20.09}, "VIN": {"avg": 64.28, "peak": 72.18, "min": 60.18}, "VDD_CPU_SOC_MSS": {"avg": 16.47, "peak": 16.54, "min": 16.14}}, "power_watts_avg": 21.38, "energy_joules_est": 32.6, "sample_count": 11, "duration_seconds": 1.525}, "timestamp": "2026-01-16T15:55:52.669308"}
{"image_index": 223, "image_name": "000000023034.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023034.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1228.112, "latencies_ms": [1228.112], "images_per_second": 0.814, "prompt_tokens": 9, "response_tokens_est": 24, "n_tiles": 6, "output_text": "A man in a blue shirt and a red bandana is riding a horse through a rocky, forested area.", "error": null, "sys_before": {"cpu_percent": 42.3, "ram_used_mb": 25593.4, "ram_available_mb": 100178.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25593.4, "ram_available_mb": 100178.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.61, "peak": 14.91, "min": 14.12}, "VDD_GPU": {"avg": 31.64, "peak": 36.24, "min": 26.79}, "VIN": {"avg": 67.7, "peak": 123.05, "min": 50.27}, "VDD_CPU_SOC_MSS": {"avg": 15.44, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 31.64, "energy_joules_est": 38.87, "sample_count": 9, "duration_seconds": 1.229}, "timestamp": "2026-01-16T15:55:53.938781"}
{"image_index": 223, "image_name": "000000023034.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023034.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1484.998, "latencies_ms": [1484.998], "images_per_second": 0.673, "prompt_tokens": 23, "response_tokens_est": 32, "n_tiles": 6, "output_text": "1. Man\n2. Horse\n3. Horse\n4. Man\n5. Horse\n6. Man\n7. Horse\n8. Man", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25593.4, "ram_available_mb": 100178.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 25593.4, "ram_available_mb": 100178.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.74, "peak": 15.11, "min": 14.22}, "VDD_GPU": {"avg": 33.05, "peak": 39.0, "min": 26.0}, "VIN": {"avg": 82.55, "peak": 116.67, "min": 63.76}, "VDD_CPU_SOC_MSS": {"avg": 15.32, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 33.05, "energy_joules_est": 49.09, "sample_count": 11, "duration_seconds": 1.485}, "timestamp": "2026-01-16T15:55:55.429885"}
{"image_index": 223, "image_name": "000000023034.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023034.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2229.198, "latencies_ms": [2229.198], "images_per_second": 0.449, "prompt_tokens": 27, "response_tokens_est": 60, "n_tiles": 6, "output_text": "The main objects in the image are a man and two horses. The man is riding a horse in the foreground, while the other horse is in the background. The man is wearing a blue shirt and has a backpack on his back. The horses are walking on a rocky path surrounded by trees.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25593.4, "ram_available_mb": 100178.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 25593.4, "ram_available_mb": 100178.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.94, "peak": 15.32, "min": 14.32}, "VDD_GPU": {"avg": 29.84, "peak": 38.59, "min": 24.03}, "VIN": {"avg": 74.85, "peak": 110.05, "min": 51.19}, "VDD_CPU_SOC_MSS": {"avg": 15.52, "peak": 16.14, "min": 14.96}}, "power_watts_avg": 29.84, "energy_joules_est": 66.53, "sample_count": 17, "duration_seconds": 2.23}, "timestamp": "2026-01-16T15:55:57.665079"}
{"image_index": 223, "image_name": "000000023034.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023034.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2124.742, "latencies_ms": [2124.742], "images_per_second": 0.471, "prompt_tokens": 21, "response_tokens_est": 56, "n_tiles": 6, "output_text": "The image depicts a man in a blue shirt and a red bandana riding a horse through a forested area. The man is smiling and appears to be enjoying the ride. The setting is a natural environment with trees and rocky terrain, suggesting a rural or wilderness area.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25593.4, "ram_available_mb": 100178.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 25593.7, "ram_available_mb": 100178.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.91, "peak": 15.22, "min": 14.32}, "VDD_GPU": {"avg": 29.84, "peak": 37.82, "min": 24.03}, "VIN": {"avg": 64.64, "peak": 73.68, "min": 55.61}, "VDD_CPU_SOC_MSS": {"avg": 15.5, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 29.84, "energy_joules_est": 63.41, "sample_count": 16, "duration_seconds": 2.125}, "timestamp": "2026-01-16T15:55:59.795761"}
{"image_index": 223, "image_name": "000000023034.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023034.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1762.015, "latencies_ms": [1762.015], "images_per_second": 0.568, "prompt_tokens": 19, "response_tokens_est": 41, "n_tiles": 6, "output_text": "The image features a man in a blue shirt and an orange bandana, riding a horse through a forested area. The sunlight filters through the trees, casting dappled shadows on the ground.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25593.7, "ram_available_mb": 100178.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 25593.4, "ram_available_mb": 100178.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.69, "peak": 15.01, "min": 14.12}, "VDD_GPU": {"avg": 31.18, "peak": 38.6, "min": 24.42}, "VIN": {"avg": 75.09, "peak": 116.42, "min": 62.24}, "VDD_CPU_SOC_MSS": {"avg": 15.2, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 31.18, "energy_joules_est": 54.95, "sample_count": 13, "duration_seconds": 1.762}, "timestamp": "2026-01-16T15:56:01.567524"}
{"image_index": 224, "image_name": "000000023126.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023126.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1017.867, "latencies_ms": [1017.867], "images_per_second": 0.982, "prompt_tokens": 9, "response_tokens_est": 16, "n_tiles": 6, "output_text": "A man is riding a horse in a blurry black and white image.", "error": null, "sys_before": {"cpu_percent": 43.1, "ram_used_mb": 25593.4, "ram_available_mb": 100178.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 25593.6, "ram_available_mb": 100178.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.2, "peak": 14.4, "min": 13.82}, "VDD_GPU": {"avg": 33.99, "peak": 37.03, "min": 30.33}, "VIN": {"avg": 87.36, "peak": 126.69, "min": 55.86}, "VDD_CPU_SOC_MSS": {"avg": 14.9, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 33.99, "energy_joules_est": 34.62, "sample_count": 7, "duration_seconds": 1.019}, "timestamp": "2026-01-16T15:56:02.641187"}
{"image_index": 224, "image_name": "000000023126.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023126.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1860.352, "latencies_ms": [1860.352], "images_per_second": 0.538, "prompt_tokens": 23, "response_tokens_est": 46, "n_tiles": 6, "output_text": "1. Horse\n2. Rider\n3. Horse's mane\n4. Horse's tail\n5. Horse's hooves\n6. Horse's bridle\n7. Horse's saddle\n8. Horse's legs", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25593.6, "ram_available_mb": 100178.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 25593.4, "ram_available_mb": 100178.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.73, "peak": 15.22, "min": 14.12}, "VDD_GPU": {"avg": 31.54, "peak": 39.39, "min": 24.42}, "VIN": {"avg": 72.08, "peak": 113.67, "min": 59.45}, "VDD_CPU_SOC_MSS": {"avg": 15.38, "peak": 15.75, "min": 14.57}}, "power_watts_avg": 31.54, "energy_joules_est": 58.68, "sample_count": 14, "duration_seconds": 1.861}, "timestamp": "2026-01-16T15:56:04.507266"}
{"image_index": 224, "image_name": "000000023126.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023126.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2340.311, "latencies_ms": [2340.311], "images_per_second": 0.427, "prompt_tokens": 27, "response_tokens_est": 64, "n_tiles": 6, "output_text": "The main object in the foreground is a horse, which is being ridden by a man. The horse is positioned near the center of the image, slightly to the right. The background features a blurred horse, indicating motion. The man is standing on the horse's back, near the center of the image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25593.4, "ram_available_mb": 100178.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 25593.4, "ram_available_mb": 100178.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.97, "peak": 15.32, "min": 14.32}, "VDD_GPU": {"avg": 29.28, "peak": 38.19, "min": 24.03}, "VIN": {"avg": 75.86, "peak": 114.97, "min": 62.59}, "VDD_CPU_SOC_MSS": {"avg": 15.55, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 29.28, "energy_joules_est": 68.53, "sample_count": 18, "duration_seconds": 2.341}, "timestamp": "2026-01-16T15:56:06.853754"}
{"image_index": 224, "image_name": "000000023126.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023126.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2633.064, "latencies_ms": [2633.064], "images_per_second": 0.38, "prompt_tokens": 21, "response_tokens_est": 75, "n_tiles": 6, "output_text": "The image captures a dynamic scene of a man riding a horse in what appears to be a race or competition. The man is dressed in a racing outfit, and the horse is in motion, creating a sense of speed and energy. The setting is an outdoor arena, likely a racetrack, with a blurred background indicating the fast-paced nature of the event.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25593.4, "ram_available_mb": 100178.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 25593.4, "ram_available_mb": 100178.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.04, "peak": 15.32, "min": 14.32}, "VDD_GPU": {"avg": 28.5, "peak": 38.19, "min": 23.64}, "VIN": {"avg": 74.4, "peak": 116.39, "min": 61.69}, "VDD_CPU_SOC_MSS": {"avg": 15.63, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 28.5, "energy_joules_est": 75.05, "sample_count": 20, "duration_seconds": 2.634}, "timestamp": "2026-01-16T15:56:09.493671"}
{"image_index": 224, "image_name": "000000023126.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023126.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2199.45, "latencies_ms": [2199.45], "images_per_second": 0.455, "prompt_tokens": 19, "response_tokens_est": 59, "n_tiles": 6, "output_text": "The image is in black and white, which creates a stark contrast between the horse and rider, emphasizing the motion and speed of the scene. The lighting is diffused, likely due to an overcast sky, which softens the shadows and highlights the details of the horse and rider.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25593.4, "ram_available_mb": 100178.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 25593.4, "ram_available_mb": 100178.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.96, "peak": 15.32, "min": 14.32}, "VDD_GPU": {"avg": 29.61, "peak": 38.21, "min": 24.04}, "VIN": {"avg": 68.25, "peak": 85.72, "min": 59.82}, "VDD_CPU_SOC_MSS": {"avg": 15.56, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 29.61, "energy_joules_est": 65.14, "sample_count": 17, "duration_seconds": 2.2}, "timestamp": "2026-01-16T15:56:11.699294"}
{"image_index": 225, "image_name": "000000023230.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023230.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2420.588, "latencies_ms": [2420.588], "images_per_second": 0.413, "prompt_tokens": 9, "response_tokens_est": 30, "n_tiles": 12, "output_text": "The image depicts a serene scene of a group of geese swimming in a calm body of water surrounded by tall grass and shrubs.", "error": null, "sys_before": {"cpu_percent": 28.8, "ram_used_mb": 25593.4, "ram_available_mb": 100178.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 25593.4, "ram_available_mb": 100178.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.25, "peak": 16.15, "min": 14.3}, "VDD_GPU": {"avg": 35.12, "peak": 41.36, "min": 27.57}, "VIN": {"avg": 76.37, "peak": 111.87, "min": 58.18}, "VDD_CPU_SOC_MSS": {"avg": 16.05, "peak": 17.32, "min": 15.35}}, "power_watts_avg": 35.12, "energy_joules_est": 85.02, "sample_count": 18, "duration_seconds": 2.421}, "timestamp": "2026-01-16T15:56:14.176638"}
{"image_index": 225, "image_name": "000000023230.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023230.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2074.395, "latencies_ms": [2074.395], "images_per_second": 0.482, "prompt_tokens": 23, "response_tokens_est": 16, "n_tiles": 12, "output_text": "birds: 2\nwater: 1\ngrass: 1", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25593.4, "ram_available_mb": 100178.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 2.2, "ram_used_mb": 25593.4, "ram_available_mb": 100178.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.1, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 37.74, "peak": 43.33, "min": 30.33}, "VIN": {"avg": 77.24, "peak": 116.06, "min": 58.67}, "VDD_CPU_SOC_MSS": {"avg": 15.82, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 37.74, "energy_joules_est": 78.31, "sample_count": 16, "duration_seconds": 2.075}, "timestamp": "2026-01-16T15:56:16.257654"}
{"image_index": 225, "image_name": "000000023230.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023230.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3315.702, "latencies_ms": [3315.702], "images_per_second": 0.302, "prompt_tokens": 27, "response_tokens_est": 53, "n_tiles": 12, "output_text": "The main objects in the image are geese swimming in a body of water. The geese are positioned in the foreground, with their heads and necks visible. The background consists of a dense, green bushy area, which is slightly out of focus.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25593.4, "ram_available_mb": 100178.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 25593.4, "ram_available_mb": 100178.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.08, "peak": 15.65, "min": 14.43}, "VDD_GPU": {"avg": 34.3, "peak": 43.73, "min": 26.39}, "VIN": {"avg": 74.04, "peak": 102.49, "min": 54.29}, "VDD_CPU_SOC_MSS": {"avg": 15.7, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 34.3, "energy_joules_est": 113.74, "sample_count": 25, "duration_seconds": 3.316}, "timestamp": "2026-01-16T15:56:19.579576"}
{"image_index": 225, "image_name": "000000023230.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023230.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3011.209, "latencies_ms": [3011.209], "images_per_second": 0.332, "prompt_tokens": 21, "response_tokens_est": 44, "n_tiles": 12, "output_text": "The image depicts a serene natural scene featuring a calm body of water with a group of geese swimming. The geese are surrounded by tall grass and shrubs, creating a peaceful and tranquil environment.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25593.4, "ram_available_mb": 100178.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 25593.4, "ram_available_mb": 100178.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.1, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 34.53, "peak": 42.54, "min": 26.79}, "VIN": {"avg": 78.72, "peak": 121.73, "min": 58.86}, "VDD_CPU_SOC_MSS": {"avg": 15.71, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 34.53, "energy_joules_est": 103.99, "sample_count": 23, "duration_seconds": 3.012}, "timestamp": "2026-01-16T15:56:22.598406"}
{"image_index": 225, "image_name": "000000023230.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023230.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2775.922, "latencies_ms": [2775.922], "images_per_second": 0.36, "prompt_tokens": 19, "response_tokens_est": 37, "n_tiles": 12, "output_text": "The image depicts a serene scene of a calm body of water with a lush green shoreline. The lighting is bright and natural, suggesting it is daytime with clear skies.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25593.4, "ram_available_mb": 100178.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 25593.4, "ram_available_mb": 100178.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.09, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 35.22, "peak": 42.53, "min": 27.18}, "VIN": {"avg": 77.57, "peak": 128.6, "min": 58.61}, "VDD_CPU_SOC_MSS": {"avg": 15.67, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 35.22, "energy_joules_est": 97.78, "sample_count": 21, "duration_seconds": 2.776}, "timestamp": "2026-01-16T15:56:25.380616"}
{"image_index": 226, "image_name": "000000023272.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023272.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2423.412, "latencies_ms": [2423.412], "images_per_second": 0.413, "prompt_tokens": 9, "response_tokens_est": 30, "n_tiles": 12, "output_text": "A black car with a shiny chrome grille and a Mercedes emblem is parked in front of a building with a green fence and a window.", "error": null, "sys_before": {"cpu_percent": 15.4, "ram_used_mb": 25593.4, "ram_available_mb": 100178.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 25593.4, "ram_available_mb": 100178.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.19, "peak": 16.15, "min": 14.12}, "VDD_GPU": {"avg": 35.28, "peak": 42.54, "min": 27.57}, "VIN": {"avg": 75.19, "peak": 120.46, "min": 51.22}, "VDD_CPU_SOC_MSS": {"avg": 15.85, "peak": 17.32, "min": 14.96}}, "power_watts_avg": 35.28, "energy_joules_est": 85.52, "sample_count": 19, "duration_seconds": 2.424}, "timestamp": "2026-01-16T15:56:27.860324"}
{"image_index": 226, "image_name": "000000023272.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023272.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2646.146, "latencies_ms": [2646.146], "images_per_second": 0.378, "prompt_tokens": 23, "response_tokens_est": 33, "n_tiles": 12, "output_text": "car: 1\nmirror: 1\nbuilding: 1\nwindow: 1\ntrees: 1\nfence: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25593.4, "ram_available_mb": 100178.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 25593.4, "ram_available_mb": 100178.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.1, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 36.02, "peak": 42.94, "min": 27.57}, "VIN": {"avg": 83.4, "peak": 118.4, "min": 58.09}, "VDD_CPU_SOC_MSS": {"avg": 15.75, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 36.02, "energy_joules_est": 95.33, "sample_count": 20, "duration_seconds": 2.646}, "timestamp": "2026-01-16T15:56:30.514913"}
{"image_index": 226, "image_name": "000000023272.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023272.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3757.651, "latencies_ms": [3757.651], "images_per_second": 0.266, "prompt_tokens": 27, "response_tokens_est": 66, "n_tiles": 12, "output_text": "The main objects in the image are a car and a cat. The car is positioned in the foreground, with its front facing the camera. The cat is perched on the car's roof, near the windshield. The background features a building and a green fence, which are further away from the car and the cat.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25593.4, "ram_available_mb": 100178.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25593.6, "ram_available_mb": 100178.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.11, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 32.78, "peak": 42.53, "min": 26.39}, "VIN": {"avg": 73.54, "peak": 119.72, "min": 58.93}, "VDD_CPU_SOC_MSS": {"avg": 15.65, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 32.78, "energy_joules_est": 123.19, "sample_count": 29, "duration_seconds": 3.758}, "timestamp": "2026-01-16T15:56:34.279105"}
{"image_index": 226, "image_name": "000000023272.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023272.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3106.168, "latencies_ms": [3106.168], "images_per_second": 0.322, "prompt_tokens": 21, "response_tokens_est": 47, "n_tiles": 12, "output_text": "The image depicts a scene of a black car parked in front of a building with a green fence and a tree in the background. A ginger and white cat is perched on the car's roof, seemingly enjoying the view.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25593.6, "ram_available_mb": 100178.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 25593.3, "ram_available_mb": 100178.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.12, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 34.49, "peak": 42.94, "min": 26.79}, "VIN": {"avg": 75.18, "peak": 104.76, "min": 56.5}, "VDD_CPU_SOC_MSS": {"avg": 15.66, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 34.49, "energy_joules_est": 107.16, "sample_count": 23, "duration_seconds": 3.107}, "timestamp": "2026-01-16T15:56:37.392261"}
{"image_index": 226, "image_name": "000000023272.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023272.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2374.21, "latencies_ms": [2374.21], "images_per_second": 0.421, "prompt_tokens": 19, "response_tokens_est": 25, "n_tiles": 12, "output_text": "The car in the image is black with a shiny, reflective surface. The lighting is bright, indicating it is daytime.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25593.3, "ram_available_mb": 100178.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 25593.3, "ram_available_mb": 100178.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.11, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 36.57, "peak": 42.15, "min": 28.36}, "VIN": {"avg": 78.66, "peak": 110.87, "min": 58.75}, "VDD_CPU_SOC_MSS": {"avg": 15.75, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 36.57, "energy_joules_est": 86.84, "sample_count": 18, "duration_seconds": 2.375}, "timestamp": "2026-01-16T15:56:39.772712"}
{"image_index": 227, "image_name": "000000023359.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023359.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1311.693, "latencies_ms": [1311.693], "images_per_second": 0.762, "prompt_tokens": 9, "response_tokens_est": 27, "n_tiles": 6, "output_text": "A snowboarder is captured mid-air, performing a trick against a clear blue sky, with snowflakes scattered around him.", "error": null, "sys_before": {"cpu_percent": 26.5, "ram_used_mb": 25593.3, "ram_available_mb": 100178.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 25593.3, "ram_available_mb": 100178.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.43, "peak": 14.81, "min": 13.92}, "VDD_GPU": {"avg": 33.66, "peak": 38.21, "min": 27.57}, "VIN": {"avg": 67.48, "peak": 108.99, "min": 51.91}, "VDD_CPU_SOC_MSS": {"avg": 15.05, "peak": 15.36, "min": 14.57}}, "power_watts_avg": 33.66, "energy_joules_est": 44.17, "sample_count": 9, "duration_seconds": 1.312}, "timestamp": "2026-01-16T15:56:41.120912"}
{"image_index": 227, "image_name": "000000023359.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023359.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2178.879, "latencies_ms": [2178.879], "images_per_second": 0.459, "prompt_tokens": 23, "response_tokens_est": 56, "n_tiles": 6, "output_text": "1. Snowboarder\n2. Snow\n3. Snowboard\n4. Snowboarder's clothing\n5. Snowboarder's hat\n6. Snowboarder's gloves\n7. Snowboarder's goggles\n8. Snowboarder's helmet", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25593.3, "ram_available_mb": 100178.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 25593.6, "ram_available_mb": 100178.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.74, "peak": 15.01, "min": 14.12}, "VDD_GPU": {"avg": 30.01, "peak": 38.98, "min": 23.64}, "VIN": {"avg": 69.81, "peak": 115.87, "min": 55.2}, "VDD_CPU_SOC_MSS": {"avg": 15.23, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 30.01, "energy_joules_est": 65.4, "sample_count": 16, "duration_seconds": 2.179}, "timestamp": "2026-01-16T15:56:43.305825"}
{"image_index": 227, "image_name": "000000023359.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023359.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2160.978, "latencies_ms": [2160.978], "images_per_second": 0.463, "prompt_tokens": 27, "response_tokens_est": 55, "n_tiles": 6, "output_text": "The main object in the image is the snowboarder, who is in the foreground. The snowboarder is performing a jump, with snow and debris flying around him. The background features a clear blue sky, providing a contrasting backdrop to the snowy scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25593.6, "ram_available_mb": 100178.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 25593.3, "ram_available_mb": 100178.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.72, "peak": 15.01, "min": 14.02}, "VDD_GPU": {"avg": 29.64, "peak": 38.6, "min": 23.64}, "VIN": {"avg": 69.55, "peak": 117.38, "min": 56.56}, "VDD_CPU_SOC_MSS": {"avg": 15.23, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 29.64, "energy_joules_est": 64.06, "sample_count": 16, "duration_seconds": 2.161}, "timestamp": "2026-01-16T15:56:45.476636"}
{"image_index": 227, "image_name": "000000023359.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023359.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2834.797, "latencies_ms": [2834.797], "images_per_second": 0.353, "prompt_tokens": 21, "response_tokens_est": 79, "n_tiles": 6, "output_text": "The image captures a snowboarder mid-air, performing a trick against a clear blue sky. The snowboarder is dressed in a brown jacket and bright yellow pants, and is surrounded by snowflakes in the air. The scene is set on a snowy slope, with the snowboarder's motion and the snowflakes adding a dynamic element to the tranquil winter landscape.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25593.3, "ram_available_mb": 100178.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 25593.6, "ram_available_mb": 100178.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.79, "peak": 15.11, "min": 14.02}, "VDD_GPU": {"avg": 27.89, "peak": 38.21, "min": 23.24}, "VIN": {"avg": 69.77, "peak": 106.42, "min": 56.98}, "VDD_CPU_SOC_MSS": {"avg": 15.24, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 27.89, "energy_joules_est": 79.07, "sample_count": 21, "duration_seconds": 2.835}, "timestamp": "2026-01-16T15:56:48.317623"}
{"image_index": 227, "image_name": "000000023359.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023359.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2156.568, "latencies_ms": [2156.568], "images_per_second": 0.464, "prompt_tokens": 19, "response_tokens_est": 55, "n_tiles": 6, "output_text": "The snowboarder is dressed in a brown jacket and bright yellow pants, contrasting sharply against the clear blue sky. The snowboard is white, and the snowboarder is captured mid-air, with snowflakes scattered around, indicating a bright and sunny day.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25593.6, "ram_available_mb": 100178.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 25593.6, "ram_available_mb": 100178.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.74, "peak": 15.01, "min": 14.12}, "VDD_GPU": {"avg": 29.39, "peak": 37.82, "min": 23.64}, "VIN": {"avg": 67.17, "peak": 112.82, "min": 55.53}, "VDD_CPU_SOC_MSS": {"avg": 15.23, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 29.39, "energy_joules_est": 63.39, "sample_count": 16, "duration_seconds": 2.157}, "timestamp": "2026-01-16T15:56:50.484178"}
{"image_index": 228, "image_name": "000000023666.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023666.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2286.399, "latencies_ms": [2286.399], "images_per_second": 0.437, "prompt_tokens": 9, "response_tokens_est": 26, "n_tiles": 12, "output_text": "The image shows a small bathroom with a white bathtub and a wooden toilet, both placed on a tiled floor.", "error": null, "sys_before": {"cpu_percent": 28.4, "ram_used_mb": 25593.6, "ram_available_mb": 100178.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 25593.3, "ram_available_mb": 100178.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.19, "peak": 16.15, "min": 14.1}, "VDD_GPU": {"avg": 35.21, "peak": 41.36, "min": 27.97}, "VIN": {"avg": 74.87, "peak": 113.95, "min": 58.26}, "VDD_CPU_SOC_MSS": {"avg": 15.85, "peak": 16.92, "min": 14.96}}, "power_watts_avg": 35.21, "energy_joules_est": 80.52, "sample_count": 18, "duration_seconds": 2.287}, "timestamp": "2026-01-16T15:56:52.826523"}
{"image_index": 228, "image_name": "000000023666.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023666.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2647.312, "latencies_ms": [2647.312], "images_per_second": 0.378, "prompt_tokens": 23, "response_tokens_est": 33, "n_tiles": 12, "output_text": "toilet: 1\nsink: 1\ntub: 1\npipe: 1\nwall: 1\ndoor: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25593.3, "ram_available_mb": 100178.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 25593.3, "ram_available_mb": 100178.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.08, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 35.52, "peak": 42.92, "min": 27.18}, "VIN": {"avg": 74.77, "peak": 111.96, "min": 58.35}, "VDD_CPU_SOC_MSS": {"avg": 15.71, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 35.52, "energy_joules_est": 94.06, "sample_count": 21, "duration_seconds": 2.648}, "timestamp": "2026-01-16T15:56:55.480564"}
{"image_index": 228, "image_name": "000000023666.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023666.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3623.361, "latencies_ms": [3623.361], "images_per_second": 0.276, "prompt_tokens": 27, "response_tokens_est": 62, "n_tiles": 12, "output_text": "The main objects in the image are a toilet and a bathtub. The toilet is located in the foreground, while the bathtub is positioned in the background. The toilet is situated near the bathtub, with the bathtub's faucet visible on the right side of the image.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25593.3, "ram_available_mb": 100178.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25593.3, "ram_available_mb": 100178.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.08, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 33.21, "peak": 42.94, "min": 26.39}, "VIN": {"avg": 71.89, "peak": 115.34, "min": 55.74}, "VDD_CPU_SOC_MSS": {"avg": 15.65, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 33.21, "energy_joules_est": 120.35, "sample_count": 28, "duration_seconds": 3.624}, "timestamp": "2026-01-16T15:56:59.111066"}
{"image_index": 228, "image_name": "000000023666.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023666.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3269.377, "latencies_ms": [3269.377], "images_per_second": 0.306, "prompt_tokens": 21, "response_tokens_est": 51, "n_tiles": 12, "output_text": "The image depicts a small, unlit bathroom with a wooden toilet and a white bathtub. The setting appears to be indoors, possibly in a residential or small commercial space, with a dark wooden door and white walls visible in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25593.3, "ram_available_mb": 100178.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 25593.3, "ram_available_mb": 100178.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.85, "peak": 15.44, "min": 14.22}, "VDD_GPU": {"avg": 33.81, "peak": 42.94, "min": 26.0}, "VIN": {"avg": 72.41, "peak": 113.27, "min": 53.53}, "VDD_CPU_SOC_MSS": {"avg": 15.43, "peak": 16.14, "min": 14.56}}, "power_watts_avg": 33.81, "energy_joules_est": 110.56, "sample_count": 25, "duration_seconds": 3.27}, "timestamp": "2026-01-16T15:57:02.391506"}
{"image_index": 228, "image_name": "000000023666.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023666.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3094.738, "latencies_ms": [3094.738], "images_per_second": 0.323, "prompt_tokens": 19, "response_tokens_est": 46, "n_tiles": 12, "output_text": "The image shows a small, rustic bathroom with a wooden toilet seat and basin. The walls and floor are tiled in a reddish-brown color, and the lighting is dim, creating a cozy and intimate atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25593.3, "ram_available_mb": 100178.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 25593.3, "ram_available_mb": 100178.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.73, "peak": 15.34, "min": 14.12}, "VDD_GPU": {"avg": 34.14, "peak": 42.94, "min": 26.0}, "VIN": {"avg": 76.94, "peak": 122.11, "min": 58.6}, "VDD_CPU_SOC_MSS": {"avg": 15.27, "peak": 16.14, "min": 14.56}}, "power_watts_avg": 34.14, "energy_joules_est": 105.67, "sample_count": 24, "duration_seconds": 3.095}, "timestamp": "2026-01-16T15:57:05.493118"}
{"image_index": 229, "image_name": "000000023751.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023751.jpg", "image_width": 430, "image_height": 640, "image_resolution": "430x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1308.776, "latencies_ms": [1308.776], "images_per_second": 0.764, "prompt_tokens": 9, "response_tokens_est": 27, "n_tiles": 6, "output_text": "The image shows a statue of a child holding a kite, set against a backdrop of a building with a light-colored facade.", "error": null, "sys_before": {"cpu_percent": 38.4, "ram_used_mb": 25593.3, "ram_available_mb": 100178.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 25593.3, "ram_available_mb": 100178.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.33, "peak": 14.91, "min": 13.82}, "VDD_GPU": {"avg": 32.57, "peak": 37.82, "min": 26.39}, "VIN": {"avg": 70.98, "peak": 120.13, "min": 61.97}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 32.57, "energy_joules_est": 42.65, "sample_count": 10, "duration_seconds": 1.31}, "timestamp": "2026-01-16T15:57:06.858499"}
{"image_index": 229, "image_name": "000000023751.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023751.jpg", "image_width": 430, "image_height": 640, "image_resolution": "430x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1696.959, "latencies_ms": [1696.959], "images_per_second": 0.589, "prompt_tokens": 23, "response_tokens_est": 40, "n_tiles": 6, "output_text": "1. Statue\n2. Statue\n3. Statue\n4. Statue\n5. Statue\n6. Statue\n7. Statue\n8. Statue", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25593.3, "ram_available_mb": 100178.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 25593.3, "ram_available_mb": 100178.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.8, "peak": 15.22, "min": 14.22}, "VDD_GPU": {"avg": 31.84, "peak": 39.39, "min": 24.82}, "VIN": {"avg": 68.73, "peak": 85.95, "min": 61.3}, "VDD_CPU_SOC_MSS": {"avg": 15.41, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 31.84, "energy_joules_est": 54.05, "sample_count": 13, "duration_seconds": 1.698}, "timestamp": "2026-01-16T15:57:08.561633"}
{"image_index": 229, "image_name": "000000023751.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023751.jpg", "image_width": 430, "image_height": 640, "image_resolution": "430x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1777.84, "latencies_ms": [1777.84], "images_per_second": 0.562, "prompt_tokens": 27, "response_tokens_est": 43, "n_tiles": 6, "output_text": "The main objects in the image are a statue and a kite. The statue is located in the foreground, standing on a raised platform. The kite is in the background, flying high above the statue.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25593.3, "ram_available_mb": 100178.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 25593.3, "ram_available_mb": 100178.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.84, "peak": 15.22, "min": 14.32}, "VDD_GPU": {"avg": 31.45, "peak": 38.21, "min": 24.82}, "VIN": {"avg": 71.29, "peak": 123.05, "min": 57.81}, "VDD_CPU_SOC_MSS": {"avg": 15.56, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 31.45, "energy_joules_est": 55.93, "sample_count": 13, "duration_seconds": 1.778}, "timestamp": "2026-01-16T15:57:10.346191"}
{"image_index": 229, "image_name": "000000023751.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023751.jpg", "image_width": 430, "image_height": 640, "image_resolution": "430x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2060.963, "latencies_ms": [2060.963], "images_per_second": 0.485, "prompt_tokens": 21, "response_tokens_est": 54, "n_tiles": 6, "output_text": "The image depicts a statue of a child standing on a pedestal, with a colorful kite flying in the background. The setting appears to be an urban environment, possibly a park or public square, with a building and a clear sky visible in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25593.3, "ram_available_mb": 100178.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 25593.3, "ram_available_mb": 100178.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.93, "peak": 15.32, "min": 14.32}, "VDD_GPU": {"avg": 30.01, "peak": 38.21, "min": 24.03}, "VIN": {"avg": 69.18, "peak": 115.39, "min": 56.9}, "VDD_CPU_SOC_MSS": {"avg": 15.5, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 30.01, "energy_joules_est": 61.87, "sample_count": 16, "duration_seconds": 2.062}, "timestamp": "2026-01-16T15:57:12.413348"}
{"image_index": 229, "image_name": "000000023751.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023751.jpg", "image_width": 430, "image_height": 640, "image_resolution": "430x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2258.488, "latencies_ms": [2258.488], "images_per_second": 0.443, "prompt_tokens": 19, "response_tokens_est": 61, "n_tiles": 6, "output_text": "The notable visual attributes of the image include a statue of a child holding a kite, which is made of metal. The statue is set against a backdrop of a building with a light-colored facade and a clear sky. The lighting is natural, suggesting daytime, and the weather appears to be clear.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25593.3, "ram_available_mb": 100178.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 25593.3, "ram_available_mb": 100178.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.97, "peak": 15.32, "min": 14.32}, "VDD_GPU": {"avg": 29.52, "peak": 38.21, "min": 24.03}, "VIN": {"avg": 71.2, "peak": 109.38, "min": 57.46}, "VDD_CPU_SOC_MSS": {"avg": 15.56, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 29.52, "energy_joules_est": 66.68, "sample_count": 17, "duration_seconds": 2.259}, "timestamp": "2026-01-16T15:57:14.677853"}
{"image_index": 230, "image_name": "000000023781.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023781.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1677.8, "latencies_ms": [1677.8], "images_per_second": 0.596, "prompt_tokens": 9, "response_tokens_est": 41, "n_tiles": 6, "output_text": "The image showcases a variety of fresh vegetables and fruits arranged neatly on a table, including green beans, carrots, beetroots, and strawberries, all presented in baskets and containers.", "error": null, "sys_before": {"cpu_percent": 38.6, "ram_used_mb": 25593.3, "ram_available_mb": 100178.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 25593.3, "ram_available_mb": 100178.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.64, "peak": 15.22, "min": 13.92}, "VDD_GPU": {"avg": 30.57, "peak": 37.42, "min": 24.82}, "VIN": {"avg": 70.75, "peak": 118.28, "min": 51.96}, "VDD_CPU_SOC_MSS": {"avg": 15.35, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 30.57, "energy_joules_est": 51.31, "sample_count": 13, "duration_seconds": 1.678}, "timestamp": "2026-01-16T15:57:16.400908"}
{"image_index": 230, "image_name": "000000023781.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023781.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1806.529, "latencies_ms": [1806.529], "images_per_second": 0.554, "prompt_tokens": 23, "response_tokens_est": 44, "n_tiles": 6, "output_text": "strawberries: 10\nbroccoli: 1\ncucumber: 1\nradishes: 10\npeas: 1\ncarrots: 8\npotatoes: 8", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25593.3, "ram_available_mb": 100178.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 25593.3, "ram_available_mb": 100178.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.84, "peak": 15.22, "min": 14.32}, "VDD_GPU": {"avg": 31.21, "peak": 38.21, "min": 24.82}, "VIN": {"avg": 69.17, "peak": 106.61, "min": 54.41}, "VDD_CPU_SOC_MSS": {"avg": 15.41, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 31.21, "energy_joules_est": 56.4, "sample_count": 13, "duration_seconds": 1.807}, "timestamp": "2026-01-16T15:57:18.213531"}
{"image_index": 230, "image_name": "000000023781.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023781.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 4213.229, "latencies_ms": [4213.229], "images_per_second": 0.237, "prompt_tokens": 27, "response_tokens_est": 129, "n_tiles": 6, "output_text": "The main objects in the image are a variety of fresh vegetables and fruits arranged on a table. The vegetables and fruits are placed in a way that showcases their spatial relationships. In the foreground, there is a white plastic bag filled with green peas, carrots, and potatoes. To the left of the bag, there is a wooden crate filled with red strawberries. Behind the strawberries, there is a large green cucumber. In the background, there are more vegetables and fruits, including a bunch of green onions, a bunch of asparagus, and a bunch of radishes. The vegetables and fruits are arranged in a way that", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25593.3, "ram_available_mb": 100178.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 25593.3, "ram_available_mb": 100178.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.89, "peak": 15.11, "min": 14.12}, "VDD_GPU": {"avg": 26.43, "peak": 38.59, "min": 23.24}, "VIN": {"avg": 66.67, "peak": 75.44, "min": 60.84}, "VDD_CPU_SOC_MSS": {"avg": 15.32, "peak": 15.74, "min": 14.56}}, "power_watts_avg": 26.43, "energy_joules_est": 111.36, "sample_count": 33, "duration_seconds": 4.214}, "timestamp": "2026-01-16T15:57:22.432505"}
{"image_index": 230, "image_name": "000000023781.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023781.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2828.317, "latencies_ms": [2828.317], "images_per_second": 0.354, "prompt_tokens": 21, "response_tokens_est": 79, "n_tiles": 6, "output_text": "The image depicts a vibrant and colorful assortment of fresh produce arranged on a table. The scene is set in a market or grocery store, showcasing a variety of vegetables and fruits, including green beans, carrots, beets, strawberries, and radishes. The produce is neatly displayed in baskets and containers, highlighting the freshness and variety of the items available for purchase.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25593.3, "ram_available_mb": 100178.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 25593.5, "ram_available_mb": 100178.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.8, "peak": 15.01, "min": 14.02}, "VDD_GPU": {"avg": 27.77, "peak": 38.21, "min": 23.24}, "VIN": {"avg": 71.77, "peak": 117.33, "min": 59.18}, "VDD_CPU_SOC_MSS": {"avg": 15.28, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 27.77, "energy_joules_est": 78.55, "sample_count": 22, "duration_seconds": 2.829}, "timestamp": "2026-01-16T15:57:25.267428"}
{"image_index": 230, "image_name": "000000023781.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023781.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2833.098, "latencies_ms": [2833.098], "images_per_second": 0.353, "prompt_tokens": 19, "response_tokens_est": 79, "n_tiles": 6, "output_text": "The image showcases a vibrant and colorful assortment of fresh produce, including bright red strawberries, green broccoli, red radishes, green peas, carrots, and potatoes. The lighting is natural, with a soft, diffused glow that highlights the freshness and vibrancy of the colors. The overall atmosphere is warm and inviting, suggesting a sunny day at a local farmer's market.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25593.5, "ram_available_mb": 100178.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 25593.8, "ram_available_mb": 100178.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.79, "peak": 15.01, "min": 14.12}, "VDD_GPU": {"avg": 27.93, "peak": 38.21, "min": 22.86}, "VIN": {"avg": 70.58, "peak": 120.89, "min": 61.09}, "VDD_CPU_SOC_MSS": {"avg": 15.41, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 27.93, "energy_joules_est": 79.14, "sample_count": 21, "duration_seconds": 2.833}, "timestamp": "2026-01-16T15:57:28.106513"}
{"image_index": 231, "image_name": "000000023899.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023899.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1277.721, "latencies_ms": [1277.721], "images_per_second": 0.783, "prompt_tokens": 9, "response_tokens_est": 26, "n_tiles": 6, "output_text": "Two men are sitting on a couch, laughing and holding controllers, while a projector is on a table in the background.", "error": null, "sys_before": {"cpu_percent": 40.3, "ram_used_mb": 25593.5, "ram_available_mb": 100178.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25593.5, "ram_available_mb": 100178.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.32, "peak": 14.81, "min": 13.82}, "VDD_GPU": {"avg": 32.21, "peak": 36.63, "min": 27.18}, "VIN": {"avg": 73.74, "peak": 107.97, "min": 49.25}, "VDD_CPU_SOC_MSS": {"avg": 15.0, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 32.21, "energy_joules_est": 41.17, "sample_count": 9, "duration_seconds": 1.278}, "timestamp": "2026-01-16T15:57:29.445591"}
{"image_index": 231, "image_name": "000000023899.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023899.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2122.543, "latencies_ms": [2122.543], "images_per_second": 0.471, "prompt_tokens": 23, "response_tokens_est": 56, "n_tiles": 6, "output_text": "1. Person: 2\n2. Person: 2\n3. Person: 2\n4. Person: 2\n5. Person: 2\n6. Person: 2\n7. Person: 2\n8. Person: 2", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25593.5, "ram_available_mb": 100178.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 25593.8, "ram_available_mb": 100178.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.91, "peak": 15.32, "min": 14.22}, "VDD_GPU": {"avg": 30.16, "peak": 38.6, "min": 24.03}, "VIN": {"avg": 69.41, "peak": 101.32, "min": 54.75}, "VDD_CPU_SOC_MSS": {"avg": 15.45, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 30.16, "energy_joules_est": 64.02, "sample_count": 16, "duration_seconds": 2.123}, "timestamp": "2026-01-16T15:57:31.573985"}
{"image_index": 231, "image_name": "000000023899.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023899.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2695.168, "latencies_ms": [2695.168], "images_per_second": 0.371, "prompt_tokens": 27, "response_tokens_est": 77, "n_tiles": 6, "output_text": "The main objects in the image are three people sitting on a couch. The person on the left is wearing glasses and appears to be looking towards the right. The person in the center is laughing and holding a remote control. The person on the right is also holding a remote control and appears to be laughing. The couch is in the background, and the people are in the foreground.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25593.8, "ram_available_mb": 100178.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 25593.7, "ram_available_mb": 100178.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.0, "peak": 15.32, "min": 14.32}, "VDD_GPU": {"avg": 28.62, "peak": 37.8, "min": 23.64}, "VIN": {"avg": 66.98, "peak": 80.98, "min": 58.16}, "VDD_CPU_SOC_MSS": {"avg": 15.57, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 28.62, "energy_joules_est": 77.15, "sample_count": 20, "duration_seconds": 2.696}, "timestamp": "2026-01-16T15:57:34.279161"}
{"image_index": 231, "image_name": "000000023899.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023899.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2394.556, "latencies_ms": [2394.556], "images_per_second": 0.418, "prompt_tokens": 21, "response_tokens_est": 66, "n_tiles": 6, "output_text": "The scene depicts a cozy, dimly lit room where three individuals are engaged in a shared activity. They are seated on a couch, each holding a white remote control, and appear to be playing a video game. The atmosphere suggests a relaxed and intimate setting, with the focus on their shared experience and the casual interaction.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25593.7, "ram_available_mb": 100178.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 25593.5, "ram_available_mb": 100178.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.97, "peak": 15.32, "min": 14.32}, "VDD_GPU": {"avg": 29.04, "peak": 37.82, "min": 23.64}, "VIN": {"avg": 68.59, "peak": 114.84, "min": 55.22}, "VDD_CPU_SOC_MSS": {"avg": 15.53, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 29.04, "energy_joules_est": 69.56, "sample_count": 18, "duration_seconds": 2.395}, "timestamp": "2026-01-16T15:57:36.680140"}
{"image_index": 231, "image_name": "000000023899.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023899.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1804.689, "latencies_ms": [1804.689], "images_per_second": 0.554, "prompt_tokens": 19, "response_tokens_est": 44, "n_tiles": 6, "output_text": "The image features a dimly lit room with a warm, soft glow from a projector on a table. The individuals are seated on colorful, patterned cushions, and the overall atmosphere is cozy and intimate.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25593.5, "ram_available_mb": 100178.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 25593.5, "ram_available_mb": 100178.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.88, "peak": 15.22, "min": 14.32}, "VDD_GPU": {"avg": 30.99, "peak": 38.19, "min": 24.82}, "VIN": {"avg": 76.66, "peak": 113.78, "min": 59.41}, "VDD_CPU_SOC_MSS": {"avg": 15.47, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 30.99, "energy_joules_est": 55.94, "sample_count": 13, "duration_seconds": 1.805}, "timestamp": "2026-01-16T15:57:38.490683"}
{"image_index": 232, "image_name": "000000023937.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023937.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1470.24, "latencies_ms": [1470.24], "images_per_second": 0.68, "prompt_tokens": 9, "response_tokens_est": 33, "n_tiles": 6, "output_text": "The image depicts a serene pastoral scene with a group of cows lying on a lush green field, surrounded by a tree trunk and some greenery.", "error": null, "sys_before": {"cpu_percent": 41.3, "ram_used_mb": 25593.5, "ram_available_mb": 100178.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 25593.5, "ram_available_mb": 100178.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.53, "peak": 15.01, "min": 13.92}, "VDD_GPU": {"avg": 31.4, "peak": 37.41, "min": 25.6}, "VIN": {"avg": 69.74, "peak": 116.88, "min": 51.71}, "VDD_CPU_SOC_MSS": {"avg": 15.25, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 31.4, "energy_joules_est": 46.18, "sample_count": 11, "duration_seconds": 1.471}, "timestamp": "2026-01-16T15:57:40.009783"}
{"image_index": 232, "image_name": "000000023937.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023937.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1544.703, "latencies_ms": [1544.703], "images_per_second": 0.647, "prompt_tokens": 23, "response_tokens_est": 33, "n_tiles": 6, "output_text": "1. Cows\n2. Grass\n3. Trees\n4. Trees\n5. Trees\n6. Trees\n7. Trees\n8. Trees", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25593.5, "ram_available_mb": 100178.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 25593.5, "ram_available_mb": 100178.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.61, "peak": 14.91, "min": 14.12}, "VDD_GPU": {"avg": 32.08, "peak": 38.6, "min": 25.21}, "VIN": {"avg": 75.3, "peak": 117.09, "min": 56.8}, "VDD_CPU_SOC_MSS": {"avg": 15.17, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 32.08, "energy_joules_est": 49.56, "sample_count": 11, "duration_seconds": 1.545}, "timestamp": "2026-01-16T15:57:41.561345"}
{"image_index": 232, "image_name": "000000023937.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023937.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2361.045, "latencies_ms": [2361.045], "images_per_second": 0.424, "prompt_tokens": 27, "response_tokens_est": 62, "n_tiles": 6, "output_text": "The main objects in the image are a group of cows lying on a lush green field. The cows are positioned in the foreground, with the foreground being the grassy area where they are resting. The cows are near the center of the image, with the background showing more of the field and some trees.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25593.5, "ram_available_mb": 100178.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 25593.5, "ram_available_mb": 100178.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.73, "peak": 15.11, "min": 14.02}, "VDD_GPU": {"avg": 28.95, "peak": 38.59, "min": 23.24}, "VIN": {"avg": 65.61, "peak": 97.88, "min": 55.91}, "VDD_CPU_SOC_MSS": {"avg": 15.31, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 28.95, "energy_joules_est": 68.36, "sample_count": 18, "duration_seconds": 2.361}, "timestamp": "2026-01-16T15:57:43.928734"}
{"image_index": 232, "image_name": "000000023937.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023937.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2635.149, "latencies_ms": [2635.149], "images_per_second": 0.379, "prompt_tokens": 21, "response_tokens_est": 72, "n_tiles": 6, "output_text": "The image depicts a serene pastoral scene with a lush green field in the foreground, where a group of cows is peacefully resting. The cows are scattered across the grass, with some lying down and others standing. The setting appears to be a rural area, possibly a farm, with a tree trunk visible in the right side of the image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25593.5, "ram_available_mb": 100178.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 25593.5, "ram_available_mb": 100178.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.78, "peak": 15.01, "min": 14.02}, "VDD_GPU": {"avg": 28.24, "peak": 38.19, "min": 23.24}, "VIN": {"avg": 70.18, "peak": 118.74, "min": 48.59}, "VDD_CPU_SOC_MSS": {"avg": 15.35, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 28.24, "energy_joules_est": 74.42, "sample_count": 20, "duration_seconds": 2.635}, "timestamp": "2026-01-16T15:57:46.570253"}
{"image_index": 232, "image_name": "000000023937.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000023937.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1599.773, "latencies_ms": [1599.773], "images_per_second": 0.625, "prompt_tokens": 19, "response_tokens_est": 35, "n_tiles": 6, "output_text": "The image depicts a lush, green field with a variety of grasses and small plants. The lighting is soft and natural, suggesting a sunny day with minimal shadows.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25593.5, "ram_available_mb": 100178.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 25593.5, "ram_available_mb": 100178.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.65, "peak": 14.91, "min": 14.12}, "VDD_GPU": {"avg": 31.35, "peak": 38.21, "min": 24.83}, "VIN": {"avg": 77.04, "peak": 121.59, "min": 60.21}, "VDD_CPU_SOC_MSS": {"avg": 15.19, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 31.35, "energy_joules_est": 50.16, "sample_count": 12, "duration_seconds": 1.6}, "timestamp": "2026-01-16T15:57:48.175808"}
{"image_index": 233, "image_name": "000000024021.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000024021.jpg", "image_width": 640, "image_height": 390, "image_resolution": "640x390", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1638.667, "latencies_ms": [1638.667], "images_per_second": 0.61, "prompt_tokens": 9, "response_tokens_est": 38, "n_tiles": 6, "output_text": "The image is a black and white photograph of a large group of children and a few adults posing for a school photo in Goodmayes Boys' School, dated April 1929.", "error": null, "sys_before": {"cpu_percent": 32.7, "ram_used_mb": 25593.5, "ram_available_mb": 100178.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25593.5, "ram_available_mb": 100178.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.46, "peak": 14.91, "min": 13.82}, "VDD_GPU": {"avg": 30.86, "peak": 37.42, "min": 24.42}, "VIN": {"avg": 82.97, "peak": 125.41, "min": 63.6}, "VDD_CPU_SOC_MSS": {"avg": 15.02, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 30.86, "energy_joules_est": 50.58, "sample_count": 12, "duration_seconds": 1.639}, "timestamp": "2026-01-16T15:57:49.859048"}
{"image_index": 233, "image_name": "000000024021.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000024021.jpg", "image_width": 640, "image_height": 390, "image_resolution": "640x390", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 4219.133, "latencies_ms": [4219.133], "images_per_second": 0.237, "prompt_tokens": 23, "response_tokens_est": 128, "n_tiles": 6, "output_text": "object: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject:", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25593.5, "ram_available_mb": 100178.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 25593.5, "ram_available_mb": 100178.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.86, "peak": 15.01, "min": 14.02}, "VDD_GPU": {"avg": 26.38, "peak": 38.21, "min": 22.85}, "VIN": {"avg": 65.59, "peak": 77.93, "min": 54.85}, "VDD_CPU_SOC_MSS": {"avg": 15.32, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 26.38, "energy_joules_est": 111.31, "sample_count": 32, "duration_seconds": 4.219}, "timestamp": "2026-01-16T15:57:54.084080"}
{"image_index": 233, "image_name": "000000024021.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000024021.jpg", "image_width": 640, "image_height": 390, "image_resolution": "640x390", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2356.396, "latencies_ms": [2356.396], "images_per_second": 0.424, "prompt_tokens": 27, "response_tokens_est": 62, "n_tiles": 6, "output_text": "The main objects in the image are the students of Goodmayes Boys' School, arranged in a large group. The students are positioned in the foreground, with the school building serving as the background. The students are arranged in a semi-circular formation, with some sitting on the ground and others standing.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25593.5, "ram_available_mb": 100178.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 25593.2, "ram_available_mb": 100179.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.75, "peak": 15.01, "min": 14.02}, "VDD_GPU": {"avg": 28.75, "peak": 37.8, "min": 23.24}, "VIN": {"avg": 69.78, "peak": 115.18, "min": 54.21}, "VDD_CPU_SOC_MSS": {"avg": 15.29, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 28.75, "energy_joules_est": 67.75, "sample_count": 18, "duration_seconds": 2.357}, "timestamp": "2026-01-16T15:57:56.447299"}
{"image_index": 233, "image_name": "000000024021.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000024021.jpg", "image_width": 640, "image_height": 390, "image_resolution": "640x390", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2020.146, "latencies_ms": [2020.146], "images_per_second": 0.495, "prompt_tokens": 21, "response_tokens_est": 50, "n_tiles": 6, "output_text": "The image depicts a large group of children and a few adults posing for a school photograph in front of a brick building. The setting is likely a school, as indicated by the attire of the individuals and the formal nature of the photograph.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25593.2, "ram_available_mb": 100179.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 25593.2, "ram_available_mb": 100179.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.68, "peak": 15.01, "min": 14.02}, "VDD_GPU": {"avg": 29.83, "peak": 38.21, "min": 23.64}, "VIN": {"avg": 67.21, "peak": 109.13, "min": 55.25}, "VDD_CPU_SOC_MSS": {"avg": 15.2, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 29.83, "energy_joules_est": 60.27, "sample_count": 15, "duration_seconds": 2.02}, "timestamp": "2026-01-16T15:57:58.473413"}
{"image_index": 233, "image_name": "000000024021.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000024021.jpg", "image_width": 640, "image_height": 390, "image_resolution": "640x390", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2046.199, "latencies_ms": [2046.199], "images_per_second": 0.489, "prompt_tokens": 19, "response_tokens_est": 51, "n_tiles": 6, "output_text": "The black and white photograph features a large group of children and a few adults posing for a school photo. The lighting is natural, likely from daylight, and the image has a vintage feel, suggesting it was taken in the early 20th century.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25593.2, "ram_available_mb": 100179.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 25593.5, "ram_available_mb": 100178.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.69, "peak": 15.01, "min": 14.02}, "VDD_GPU": {"avg": 29.91, "peak": 38.21, "min": 23.64}, "VIN": {"avg": 66.01, "peak": 87.23, "min": 55.42}, "VDD_CPU_SOC_MSS": {"avg": 15.22, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 29.91, "energy_joules_est": 61.21, "sample_count": 15, "duration_seconds": 2.046}, "timestamp": "2026-01-16T15:58:00.526130"}
{"image_index": 234, "image_name": "000000024027.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000024027.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2774.467, "latencies_ms": [2774.467], "images_per_second": 0.36, "prompt_tokens": 9, "response_tokens_est": 41, "n_tiles": 12, "output_text": "The image depicts a vibrant kite flying high in the sky, with a clear blue sky and fluffy white clouds in the background, surrounded by a park with green grass and a few trees.", "error": null, "sys_before": {"cpu_percent": 44.9, "ram_used_mb": 25593.2, "ram_available_mb": 100179.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 25593.5, "ram_available_mb": 100178.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.15, "peak": 16.15, "min": 13.9}, "VDD_GPU": {"avg": 33.99, "peak": 40.97, "min": 26.79}, "VIN": {"avg": 76.28, "peak": 121.75, "min": 58.22}, "VDD_CPU_SOC_MSS": {"avg": 15.8, "peak": 16.92, "min": 14.96}}, "power_watts_avg": 33.99, "energy_joules_est": 94.31, "sample_count": 21, "duration_seconds": 2.775}, "timestamp": "2026-01-16T15:58:03.387284"}
{"image_index": 234, "image_name": "000000024027.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000024027.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2879.009, "latencies_ms": [2879.009], "images_per_second": 0.347, "prompt_tokens": 23, "response_tokens_est": 40, "n_tiles": 12, "output_text": "object: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25593.5, "ram_available_mb": 100178.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 25593.2, "ram_available_mb": 100179.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.06, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 34.92, "peak": 42.94, "min": 26.79}, "VIN": {"avg": 77.31, "peak": 126.91, "min": 58.44}, "VDD_CPU_SOC_MSS": {"avg": 15.65, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 34.92, "energy_joules_est": 100.56, "sample_count": 22, "duration_seconds": 2.88}, "timestamp": "2026-01-16T15:58:06.274603"}
{"image_index": 234, "image_name": "000000024027.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000024027.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3680.6, "latencies_ms": [3680.6], "images_per_second": 0.272, "prompt_tokens": 27, "response_tokens_est": 64, "n_tiles": 12, "output_text": "The main objects in the image are a kite and a person. The kite is in the foreground, flying high in the sky, while the person is near the kite, standing on the grass. The background features a park with trees and buildings, providing a serene setting for the kite flying.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25593.2, "ram_available_mb": 100179.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 25593.2, "ram_available_mb": 100179.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.11, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 33.08, "peak": 42.54, "min": 26.39}, "VIN": {"avg": 75.59, "peak": 123.74, "min": 57.75}, "VDD_CPU_SOC_MSS": {"avg": 15.69, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 33.08, "energy_joules_est": 121.76, "sample_count": 28, "duration_seconds": 3.681}, "timestamp": "2026-01-16T15:58:09.961381"}
{"image_index": 234, "image_name": "000000024027.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000024027.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3636.903, "latencies_ms": [3636.903], "images_per_second": 0.275, "prompt_tokens": 21, "response_tokens_est": 63, "n_tiles": 12, "output_text": "The image depicts a serene outdoor scene with a clear blue sky adorned with fluffy white clouds. In the foreground, there is a grassy field with a few trees and a few buildings in the background. A person is seen walking on the grass, and there are a few cars parked nearby.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25593.2, "ram_available_mb": 100179.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25593.2, "ram_available_mb": 100179.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.13, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 32.97, "peak": 41.76, "min": 26.39}, "VIN": {"avg": 72.32, "peak": 114.16, "min": 55.11}, "VDD_CPU_SOC_MSS": {"avg": 15.68, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 32.97, "energy_joules_est": 119.92, "sample_count": 28, "duration_seconds": 3.637}, "timestamp": "2026-01-16T15:58:13.604677"}
{"image_index": 234, "image_name": "000000024027.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000024027.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3610.129, "latencies_ms": [3610.129], "images_per_second": 0.277, "prompt_tokens": 19, "response_tokens_est": 62, "n_tiles": 12, "output_text": "The image features a vibrant kite with a gradient of colors, predominantly in shades of purple, blue, and orange, soaring against a partly cloudy sky. The lighting is bright and natural, suggesting it is daytime, and the weather appears to be pleasant with a mix of clouds and clear patches.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25593.2, "ram_available_mb": 100179.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 25593.2, "ram_available_mb": 100179.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.1, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 33.07, "peak": 42.92, "min": 26.39}, "VIN": {"avg": 74.84, "peak": 115.78, "min": 57.96}, "VDD_CPU_SOC_MSS": {"avg": 15.66, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 33.07, "energy_joules_est": 119.41, "sample_count": 28, "duration_seconds": 3.611}, "timestamp": "2026-01-16T15:58:17.221186"}
{"image_index": 235, "image_name": "000000024144.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000024144.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2224.518, "latencies_ms": [2224.518], "images_per_second": 0.45, "prompt_tokens": 9, "response_tokens_est": 24, "n_tiles": 12, "output_text": "The image shows a slice of pizza with melted cheese and pepperoni toppings, resting on a cardboard pizza box.", "error": null, "sys_before": {"cpu_percent": 20.5, "ram_used_mb": 25593.2, "ram_available_mb": 100179.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 2.6, "ram_used_mb": 25593.2, "ram_available_mb": 100179.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.19, "peak": 16.15, "min": 14.12}, "VDD_GPU": {"avg": 35.82, "peak": 41.76, "min": 28.36}, "VIN": {"avg": 73.87, "peak": 127.72, "min": 57.91}, "VDD_CPU_SOC_MSS": {"avg": 16.0, "peak": 17.32, "min": 14.96}}, "power_watts_avg": 35.82, "energy_joules_est": 79.71, "sample_count": 17, "duration_seconds": 2.225}, "timestamp": "2026-01-16T15:58:19.502795"}
{"image_index": 235, "image_name": "000000024144.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000024144.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2781.661, "latencies_ms": [2781.661], "images_per_second": 0.359, "prompt_tokens": 23, "response_tokens_est": 37, "n_tiles": 12, "output_text": "1. Pizza\n2. Pizza box\n3. Pita bread\n4. Cheese\n5. Tomato\n6. Pepperoni\n7. Sauce\n8. Meat", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25593.2, "ram_available_mb": 100179.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 25593.2, "ram_available_mb": 100179.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.07, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 35.45, "peak": 43.33, "min": 27.18}, "VIN": {"avg": 75.82, "peak": 107.04, "min": 44.45}, "VDD_CPU_SOC_MSS": {"avg": 15.69, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 35.45, "energy_joules_est": 98.63, "sample_count": 21, "duration_seconds": 2.782}, "timestamp": "2026-01-16T15:58:22.293429"}
{"image_index": 235, "image_name": "000000024144.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000024144.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3256.232, "latencies_ms": [3256.232], "images_per_second": 0.307, "prompt_tokens": 27, "response_tokens_est": 51, "n_tiles": 12, "output_text": "The main object in the image is a pizza placed on a cardboard box. The pizza is positioned in the foreground, with its crust and toppings clearly visible. The cardboard box is situated in the background, providing a neutral backdrop that highlights the pizza.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25593.2, "ram_available_mb": 100179.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 25593.2, "ram_available_mb": 100179.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.08, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 33.95, "peak": 42.92, "min": 26.39}, "VIN": {"avg": 73.59, "peak": 103.57, "min": 58.76}, "VDD_CPU_SOC_MSS": {"avg": 15.68, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 33.95, "energy_joules_est": 110.56, "sample_count": 25, "duration_seconds": 3.257}, "timestamp": "2026-01-16T15:58:25.556092"}
{"image_index": 235, "image_name": "000000024144.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000024144.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3418.696, "latencies_ms": [3418.696], "images_per_second": 0.293, "prompt_tokens": 21, "response_tokens_est": 56, "n_tiles": 12, "output_text": "The image shows a slice of pizza placed on a cardboard pizza box. The pizza appears to be freshly baked, with melted cheese and a crispy crust. The setting suggests that the pizza is ready to be eaten, and the box is likely being used for transport or storage.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25593.2, "ram_available_mb": 100179.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25593.2, "ram_available_mb": 100179.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.09, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 33.48, "peak": 42.94, "min": 26.39}, "VIN": {"avg": 73.26, "peak": 108.16, "min": 58.75}, "VDD_CPU_SOC_MSS": {"avg": 15.67, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 33.48, "energy_joules_est": 114.48, "sample_count": 26, "duration_seconds": 3.419}, "timestamp": "2026-01-16T15:58:28.981826"}
{"image_index": 235, "image_name": "000000024144.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000024144.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3384.412, "latencies_ms": [3384.412], "images_per_second": 0.295, "prompt_tokens": 19, "response_tokens_est": 55, "n_tiles": 12, "output_text": "The image shows a freshly baked pizza with a golden-brown crust, melted cheese, and a red tomato topping. The lighting is bright, highlighting the colors and textures of the pizza. The pizza is placed on a cardboard pizza box, which has a slightly worn appearance.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25593.2, "ram_available_mb": 100179.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 25593.2, "ram_available_mb": 100179.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.08, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 33.53, "peak": 42.94, "min": 26.39}, "VIN": {"avg": 74.73, "peak": 111.26, "min": 55.48}, "VDD_CPU_SOC_MSS": {"avg": 15.66, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 33.53, "energy_joules_est": 113.49, "sample_count": 26, "duration_seconds": 3.385}, "timestamp": "2026-01-16T15:58:32.373627"}
{"image_index": 236, "image_name": "000000024243.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000024243.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2256.567, "latencies_ms": [2256.567], "images_per_second": 0.443, "prompt_tokens": 9, "response_tokens_est": 25, "n_tiles": 12, "output_text": "A woman is sitting on a bench outside a refrigerator, holding a beer in her hand, and smiling at the camera.", "error": null, "sys_before": {"cpu_percent": 42.6, "ram_used_mb": 25593.2, "ram_available_mb": 100179.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 25593.4, "ram_available_mb": 100178.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.11, "peak": 16.15, "min": 13.79}, "VDD_GPU": {"avg": 35.73, "peak": 41.76, "min": 28.36}, "VIN": {"avg": 76.04, "peak": 115.74, "min": 62.83}, "VDD_CPU_SOC_MSS": {"avg": 15.84, "peak": 17.32, "min": 14.57}}, "power_watts_avg": 35.73, "energy_joules_est": 80.64, "sample_count": 17, "duration_seconds": 2.257}, "timestamp": "2026-01-16T15:58:34.715733"}
{"image_index": 236, "image_name": "000000024243.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000024243.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2711.932, "latencies_ms": [2711.932], "images_per_second": 0.369, "prompt_tokens": 23, "response_tokens_est": 35, "n_tiles": 12, "output_text": "1. Woman\n2. Beer\n3. Refrigerator\n4. Window\n5. Street\n6. Person\n7. Person\n8. Person", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25593.4, "ram_available_mb": 100178.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 25593.2, "ram_available_mb": 100179.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.07, "peak": 15.54, "min": 14.53}, "VDD_GPU": {"avg": 35.56, "peak": 43.31, "min": 27.18}, "VIN": {"avg": 72.64, "peak": 111.67, "min": 58.48}, "VDD_CPU_SOC_MSS": {"avg": 15.71, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 35.56, "energy_joules_est": 96.45, "sample_count": 21, "duration_seconds": 2.712}, "timestamp": "2026-01-16T15:58:37.434252"}
{"image_index": 236, "image_name": "000000024243.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000024243.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3588.76, "latencies_ms": [3588.76], "images_per_second": 0.279, "prompt_tokens": 27, "response_tokens_est": 61, "n_tiles": 12, "output_text": "The main objects in the image are a woman sitting on a bench and a refrigerator. The woman is positioned in the foreground, sitting on the bench, while the refrigerator is located in the background, slightly to the right. The bench is situated near the refrigerator, and the woman is leaning against it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25593.2, "ram_available_mb": 100179.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25593.2, "ram_available_mb": 100179.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.1, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 33.25, "peak": 42.94, "min": 26.39}, "VIN": {"avg": 72.48, "peak": 109.6, "min": 57.18}, "VDD_CPU_SOC_MSS": {"avg": 15.66, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 33.25, "energy_joules_est": 119.35, "sample_count": 28, "duration_seconds": 3.589}, "timestamp": "2026-01-16T15:58:41.029519"}
{"image_index": 236, "image_name": "000000024243.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000024243.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3751.478, "latencies_ms": [3751.478], "images_per_second": 0.267, "prompt_tokens": 21, "response_tokens_est": 66, "n_tiles": 12, "output_text": "The image depicts a scene on a city street with a woman sitting on a bench next to a white refrigerator. She is wearing a brown jacket and jeans, and is holding a beer in her hand. The setting appears to be a typical urban environment with a sidewalk, a few people walking, and a few parked cars.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25593.2, "ram_available_mb": 100179.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25593.4, "ram_available_mb": 100178.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.11, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 32.72, "peak": 42.13, "min": 26.39}, "VIN": {"avg": 74.41, "peak": 145.5, "min": 56.94}, "VDD_CPU_SOC_MSS": {"avg": 15.69, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 32.72, "energy_joules_est": 122.76, "sample_count": 29, "duration_seconds": 3.752}, "timestamp": "2026-01-16T15:58:44.787288"}
{"image_index": 236, "image_name": "000000024243.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000024243.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3351.787, "latencies_ms": [3351.787], "images_per_second": 0.298, "prompt_tokens": 19, "response_tokens_est": 54, "n_tiles": 12, "output_text": "The image depicts a scene with a woman sitting on a bench next to a white refrigerator. The woman is wearing a brown jacket and jeans, and she is holding a beer in her hand. The lighting is dim, and the weather appears to be overcast.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25593.4, "ram_available_mb": 100178.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 25593.4, "ram_available_mb": 100178.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.1, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 33.63, "peak": 42.53, "min": 26.39}, "VIN": {"avg": 76.32, "peak": 129.61, "min": 58.62}, "VDD_CPU_SOC_MSS": {"avg": 15.7, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 33.63, "energy_joules_est": 112.74, "sample_count": 26, "duration_seconds": 3.352}, "timestamp": "2026-01-16T15:58:48.146168"}
{"image_index": 237, "image_name": "000000024567.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000024567.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2461.318, "latencies_ms": [2461.318], "images_per_second": 0.406, "prompt_tokens": 9, "response_tokens_est": 31, "n_tiles": 12, "output_text": "The image shows a person holding a tray filled with hot dogs, each hot dog being wrapped in a bun and topped with a red ketchup.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 25593.4, "ram_available_mb": 100178.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 25593.4, "ram_available_mb": 100178.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.18, "peak": 16.15, "min": 14.12}, "VDD_GPU": {"avg": 35.06, "peak": 41.76, "min": 27.18}, "VIN": {"avg": 73.44, "peak": 106.57, "min": 58.22}, "VDD_CPU_SOC_MSS": {"avg": 15.99, "peak": 17.32, "min": 14.96}}, "power_watts_avg": 35.06, "energy_joules_est": 86.31, "sample_count": 19, "duration_seconds": 2.462}, "timestamp": "2026-01-16T15:58:50.665486"}
{"image_index": 237, "image_name": "000000024567.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000024567.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 5852.82, "latencies_ms": [5852.82], "images_per_second": 0.171, "prompt_tokens": 23, "response_tokens_est": 129, "n_tiles": 12, "output_text": "hot dog: 8\nbuns: 8\nfork: 1\nfork: 1\nfork: 1\nfork: 1\nfork: 1\nfork: 1\nfork: 1\nfork: 1\nfork: 1\nfork: 1\nfork: 1\nfork: 1\nfork: 1\nfork: 1\nfork: 1\nfork: 1\nfork: 1\nfork: 1\nfork: 1\nfork: 1\nfork: 1\nfork: 1\nfork: 1\nfork", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25593.4, "ram_available_mb": 100178.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 25593.6, "ram_available_mb": 100178.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.13, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 30.58, "peak": 42.94, "min": 26.0}, "VIN": {"avg": 72.46, "peak": 119.4, "min": 58.73}, "VDD_CPU_SOC_MSS": {"avg": 15.74, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 30.58, "energy_joules_est": 178.99, "sample_count": 46, "duration_seconds": 5.853}, "timestamp": "2026-01-16T15:58:56.525121"}
{"image_index": 237, "image_name": "000000024567.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000024567.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3902.44, "latencies_ms": [3902.44], "images_per_second": 0.256, "prompt_tokens": 27, "response_tokens_est": 70, "n_tiles": 12, "output_text": "The main objects in the image are a tray of hot dogs, which are placed on a blue-lined aluminum foil tray. The hot dogs are arranged in a row, with the closest ones to the foreground and the far ones in the background. The tray is situated on a wooden surface, and there is a white chair visible in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25593.6, "ram_available_mb": 100178.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25593.9, "ram_available_mb": 100178.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.09, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 32.64, "peak": 42.54, "min": 26.0}, "VIN": {"avg": 76.12, "peak": 119.93, "min": 55.22}, "VDD_CPU_SOC_MSS": {"avg": 15.67, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 32.64, "energy_joules_est": 127.39, "sample_count": 30, "duration_seconds": 3.903}, "timestamp": "2026-01-16T15:59:00.434022"}
{"image_index": 237, "image_name": "000000024567.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000024567.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3847.938, "latencies_ms": [3847.938], "images_per_second": 0.26, "prompt_tokens": 21, "response_tokens_est": 68, "n_tiles": 12, "output_text": "The image depicts a man holding a tray of hot dogs, which are placed on a piece of aluminum foil. The hot dogs are arranged in a neat row, with each bun holding a different colored hot dog. The man is wearing a green shirt and a hat, and the background shows a grassy area with a wooden fence.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25593.9, "ram_available_mb": 100178.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25593.9, "ram_available_mb": 100178.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.76, "peak": 15.34, "min": 14.12}, "VDD_GPU": {"avg": 32.52, "peak": 42.53, "min": 26.0}, "VIN": {"avg": 75.24, "peak": 145.92, "min": 58.86}, "VDD_CPU_SOC_MSS": {"avg": 15.23, "peak": 16.14, "min": 14.56}}, "power_watts_avg": 32.52, "energy_joules_est": 125.15, "sample_count": 30, "duration_seconds": 3.848}, "timestamp": "2026-01-16T15:59:04.290471"}
{"image_index": 237, "image_name": "000000024567.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000024567.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3470.321, "latencies_ms": [3470.321], "images_per_second": 0.288, "prompt_tokens": 19, "response_tokens_est": 57, "n_tiles": 12, "output_text": "The image features a tray of hot dogs, each coated in a glossy, red sauce. The hot dogs are placed on a light-colored, foil-lined tray, and the tray is placed on a wooden surface. The lighting is bright and natural, suggesting a sunny day.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25593.9, "ram_available_mb": 100178.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25593.6, "ram_available_mb": 100178.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.73, "peak": 15.34, "min": 14.12}, "VDD_GPU": {"avg": 33.27, "peak": 42.53, "min": 26.0}, "VIN": {"avg": 72.88, "peak": 112.06, "min": 42.75}, "VDD_CPU_SOC_MSS": {"avg": 15.21, "peak": 16.14, "min": 14.56}}, "power_watts_avg": 33.27, "energy_joules_est": 115.47, "sample_count": 27, "duration_seconds": 3.471}, "timestamp": "2026-01-16T15:59:07.767170"}
{"image_index": 238, "image_name": "000000024610.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000024610.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2661.344, "latencies_ms": [2661.344], "images_per_second": 0.376, "prompt_tokens": 9, "response_tokens_est": 37, "n_tiles": 12, "output_text": "The image shows a cluttered living room with a bookshelf filled with various books, a computer on a table, a couch with a backpack, and a television on the floor.", "error": null, "sys_before": {"cpu_percent": 25.6, "ram_used_mb": 25593.6, "ram_available_mb": 100178.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 25593.9, "ram_available_mb": 100178.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.11, "peak": 16.15, "min": 13.82}, "VDD_GPU": {"avg": 34.64, "peak": 42.15, "min": 27.19}, "VIN": {"avg": 77.7, "peak": 106.33, "min": 58.37}, "VDD_CPU_SOC_MSS": {"avg": 15.94, "peak": 17.32, "min": 14.57}}, "power_watts_avg": 34.64, "energy_joules_est": 92.21, "sample_count": 20, "duration_seconds": 2.662}, "timestamp": "2026-01-16T15:59:10.488409"}
{"image_index": 238, "image_name": "000000024610.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000024610.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2710.796, "latencies_ms": [2710.796], "images_per_second": 0.369, "prompt_tokens": 23, "response_tokens_est": 35, "n_tiles": 12, "output_text": "1. Laptop\n2. Chair\n3. Bookshelf\n4. Book\n5. Backpack\n6. TV stand\n7. TV\n8. Rug", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25593.9, "ram_available_mb": 100178.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 25593.6, "ram_available_mb": 100178.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.09, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 35.34, "peak": 42.94, "min": 27.18}, "VIN": {"avg": 77.06, "peak": 120.89, "min": 58.61}, "VDD_CPU_SOC_MSS": {"avg": 15.78, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 35.34, "energy_joules_est": 95.81, "sample_count": 21, "duration_seconds": 2.711}, "timestamp": "2026-01-16T15:59:13.205310"}
{"image_index": 238, "image_name": "000000024610.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000024610.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3590.257, "latencies_ms": [3590.257], "images_per_second": 0.279, "prompt_tokens": 27, "response_tokens_est": 61, "n_tiles": 12, "output_text": "The main objects in the image are a bookshelf filled with books, a computer monitor, a chair, and a couch. The bookshelf is located in the background, while the computer monitor is in the foreground. The chair is positioned near the bookshelf, and the couch is in the foreground.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25593.6, "ram_available_mb": 100178.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25593.6, "ram_available_mb": 100178.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.11, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 33.27, "peak": 43.31, "min": 26.39}, "VIN": {"avg": 77.84, "peak": 130.05, "min": 58.51}, "VDD_CPU_SOC_MSS": {"avg": 15.77, "peak": 16.53, "min": 15.35}}, "power_watts_avg": 33.27, "energy_joules_est": 119.46, "sample_count": 28, "duration_seconds": 3.591}, "timestamp": "2026-01-16T15:59:16.802740"}
{"image_index": 238, "image_name": "000000024610.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000024610.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3717.83, "latencies_ms": [3717.83], "images_per_second": 0.269, "prompt_tokens": 21, "response_tokens_est": 65, "n_tiles": 12, "output_text": "The image depicts a cozy living room with a cluttered desk and chair, a bookshelf filled with books, and a couch with various items on it. The room appears to be in a state of transition, possibly between a living space and a bedroom, as indicated by the presence of a bed and a backpack.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25593.6, "ram_available_mb": 100178.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 25593.6, "ram_available_mb": 100178.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.1, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 32.83, "peak": 42.54, "min": 26.39}, "VIN": {"avg": 75.35, "peak": 119.45, "min": 58.56}, "VDD_CPU_SOC_MSS": {"avg": 15.79, "peak": 16.53, "min": 15.35}}, "power_watts_avg": 32.83, "energy_joules_est": 122.08, "sample_count": 29, "duration_seconds": 3.718}, "timestamp": "2026-01-16T15:59:20.527784"}
{"image_index": 238, "image_name": "000000024610.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000024610.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3647.899, "latencies_ms": [3647.899], "images_per_second": 0.274, "prompt_tokens": 19, "response_tokens_est": 63, "n_tiles": 12, "output_text": "The image depicts a cozy, dimly lit room with a beige wall and a brown carpet. The lighting is soft and warm, creating a comfortable atmosphere. The room features a wooden desk with a laptop, a bookshelf filled with books, a chair, and a couch with various items on it.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25593.6, "ram_available_mb": 100178.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25593.6, "ram_available_mb": 100178.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.1, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 32.97, "peak": 42.54, "min": 26.39}, "VIN": {"avg": 71.79, "peak": 105.95, "min": 58.5}, "VDD_CPU_SOC_MSS": {"avg": 15.82, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 32.97, "energy_joules_est": 120.28, "sample_count": 28, "duration_seconds": 3.648}, "timestamp": "2026-01-16T15:59:24.181989"}
{"image_index": 239, "image_name": "000000024919.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000024919.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1101.576, "latencies_ms": [1101.576], "images_per_second": 0.908, "prompt_tokens": 9, "response_tokens_est": 19, "n_tiles": 6, "output_text": "Two elephants are walking through a grassy savannah, surrounded by dense vegetation and trees.", "error": null, "sys_before": {"cpu_percent": 20.8, "ram_used_mb": 25593.6, "ram_available_mb": 100178.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 25593.6, "ram_available_mb": 100178.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.37, "peak": 14.71, "min": 13.92}, "VDD_GPU": {"avg": 34.22, "peak": 37.82, "min": 29.15}, "VIN": {"avg": 75.87, "peak": 97.47, "min": 63.41}, "VDD_CPU_SOC_MSS": {"avg": 15.1, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 34.22, "energy_joules_est": 37.72, "sample_count": 8, "duration_seconds": 1.102}, "timestamp": "2026-01-16T15:59:25.322320"}
{"image_index": 239, "image_name": "000000024919.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000024919.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1137.972, "latencies_ms": [1137.972], "images_per_second": 0.879, "prompt_tokens": 23, "response_tokens_est": 19, "n_tiles": 6, "output_text": "elephant: 2\nbushes: 10\ntrees: 5", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25593.6, "ram_available_mb": 100178.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 25593.6, "ram_available_mb": 100178.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.57, "peak": 14.83, "min": 14.22}, "VDD_GPU": {"avg": 35.5, "peak": 39.0, "min": 29.54}, "VIN": {"avg": 66.48, "peak": 79.04, "min": 55.9}, "VDD_CPU_SOC_MSS": {"avg": 15.15, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 35.5, "energy_joules_est": 40.42, "sample_count": 8, "duration_seconds": 1.139}, "timestamp": "2026-01-16T15:59:26.466585"}
{"image_index": 239, "image_name": "000000024919.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000024919.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2414.912, "latencies_ms": [2414.912], "images_per_second": 0.414, "prompt_tokens": 27, "response_tokens_est": 67, "n_tiles": 6, "output_text": "The main objects in the image are two elephants, one in the foreground and the other slightly behind it. The foreground elephant is closer to the camera, while the background elephant is further away. The elephants are situated in a grassy savanna, with the foreground elephant standing in the middle ground and the background elephant slightly behind it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25593.6, "ram_available_mb": 100178.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 25593.8, "ram_available_mb": 100178.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.94, "peak": 15.32, "min": 14.22}, "VDD_GPU": {"avg": 29.81, "peak": 39.39, "min": 24.04}, "VIN": {"avg": 72.25, "peak": 111.58, "min": 62.67}, "VDD_CPU_SOC_MSS": {"avg": 15.55, "peak": 16.14, "min": 14.96}}, "power_watts_avg": 29.81, "energy_joules_est": 72.01, "sample_count": 18, "duration_seconds": 2.416}, "timestamp": "2026-01-16T15:59:28.888944"}
{"image_index": 239, "image_name": "000000024919.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000024919.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1962.788, "latencies_ms": [1962.788], "images_per_second": 0.509, "prompt_tokens": 21, "response_tokens_est": 50, "n_tiles": 6, "output_text": "The image depicts a serene African savanna landscape with two elephants walking through a dense thicket of greenery. The elephants appear to be in a natural, undisturbed environment, surrounded by tall grasses and various shrubs.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25593.8, "ram_available_mb": 100178.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 25593.6, "ram_available_mb": 100178.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.9, "peak": 15.22, "min": 14.32}, "VDD_GPU": {"avg": 30.2, "peak": 37.8, "min": 24.42}, "VIN": {"avg": 67.24, "peak": 88.72, "min": 57.86}, "VDD_CPU_SOC_MSS": {"avg": 15.56, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 30.2, "energy_joules_est": 59.29, "sample_count": 15, "duration_seconds": 1.963}, "timestamp": "2026-01-16T15:59:30.857893"}
{"image_index": 239, "image_name": "000000024919.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000024919.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2527.865, "latencies_ms": [2527.865], "images_per_second": 0.396, "prompt_tokens": 19, "response_tokens_est": 71, "n_tiles": 6, "output_text": "The image depicts two elephants in a natural, grassy environment with a misty, overcast sky. The elephants are brown, with the larger one having prominent tusks and the smaller one having a smaller head. The lighting is soft and diffused, with a muted color palette, contributing to the serene and peaceful atmosphere of the scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25593.6, "ram_available_mb": 100178.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 25593.6, "ram_available_mb": 100178.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.98, "peak": 15.32, "min": 14.32}, "VDD_GPU": {"avg": 28.82, "peak": 38.19, "min": 23.64}, "VIN": {"avg": 71.66, "peak": 112.74, "min": 55.26}, "VDD_CPU_SOC_MSS": {"avg": 15.6, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 28.82, "energy_joules_est": 72.87, "sample_count": 19, "duration_seconds": 2.528}, "timestamp": "2026-01-16T15:59:33.391791"}
{"image_index": 240, "image_name": "000000025057.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025057.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1495.678, "latencies_ms": [1495.678], "images_per_second": 0.669, "prompt_tokens": 9, "response_tokens_est": 34, "n_tiles": 6, "output_text": "A shirtless man in sunglasses and a baseball cap is holding a frisbee in his right hand while standing on a grassy field with trees in the background.", "error": null, "sys_before": {"cpu_percent": 25.6, "ram_used_mb": 25593.6, "ram_available_mb": 100178.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25593.6, "ram_available_mb": 100178.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.61, "peak": 15.01, "min": 14.02}, "VDD_GPU": {"avg": 31.37, "peak": 37.42, "min": 25.6}, "VIN": {"avg": 67.68, "peak": 107.37, "min": 50.96}, "VDD_CPU_SOC_MSS": {"avg": 15.28, "peak": 15.75, "min": 14.57}}, "power_watts_avg": 31.37, "energy_joules_est": 46.94, "sample_count": 11, "duration_seconds": 1.496}, "timestamp": "2026-01-16T15:59:34.922524"}
{"image_index": 240, "image_name": "000000025057.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025057.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1590.422, "latencies_ms": [1590.422], "images_per_second": 0.629, "prompt_tokens": 23, "response_tokens_est": 36, "n_tiles": 6, "output_text": "1. Frisbee\n2. Man\n3. Sunglasses\n4. Cap\n5. Shirt\n6. Shorts\n7. Belt\n8. Shoes", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25593.6, "ram_available_mb": 100178.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25593.6, "ram_available_mb": 100178.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.78, "peak": 15.11, "min": 14.22}, "VDD_GPU": {"avg": 32.0, "peak": 38.59, "min": 25.21}, "VIN": {"avg": 66.63, "peak": 92.9, "min": 55.78}, "VDD_CPU_SOC_MSS": {"avg": 15.39, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 32.0, "energy_joules_est": 50.92, "sample_count": 12, "duration_seconds": 1.591}, "timestamp": "2026-01-16T15:59:36.519214"}
{"image_index": 240, "image_name": "000000025057.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025057.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2392.45, "latencies_ms": [2392.45], "images_per_second": 0.418, "prompt_tokens": 27, "response_tokens_est": 66, "n_tiles": 6, "output_text": "The main object in the foreground is a man standing on a grassy field. He is holding a white frisbee in his right hand and appears to be preparing to throw it. In the background, there is another person walking away from the camera. The trees and clear blue sky provide a natural backdrop to the scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25593.6, "ram_available_mb": 100178.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 25594.1, "ram_available_mb": 100178.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.97, "peak": 15.32, "min": 14.32}, "VDD_GPU": {"avg": 29.52, "peak": 38.6, "min": 24.03}, "VIN": {"avg": 73.46, "peak": 118.07, "min": 62.81}, "VDD_CPU_SOC_MSS": {"avg": 15.49, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 29.52, "energy_joules_est": 70.64, "sample_count": 18, "duration_seconds": 2.393}, "timestamp": "2026-01-16T15:59:38.918106"}
{"image_index": 240, "image_name": "000000025057.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025057.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2656.24, "latencies_ms": [2656.24], "images_per_second": 0.376, "prompt_tokens": 21, "response_tokens_est": 76, "n_tiles": 6, "output_text": "The image depicts a man standing on a grassy field, holding a white frisbee in his right hand. He is wearing a white baseball cap, sunglasses, and a necklace. In the background, there is a dense line of trees, and another person is visible walking away from the camera. The setting appears to be a park or recreational area during the daytime.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25594.1, "ram_available_mb": 100178.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 25593.8, "ram_available_mb": 100178.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.02, "peak": 15.32, "min": 14.32}, "VDD_GPU": {"avg": 28.47, "peak": 37.8, "min": 23.64}, "VIN": {"avg": 72.32, "peak": 116.08, "min": 61.81}, "VDD_CPU_SOC_MSS": {"avg": 15.56, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 28.47, "energy_joules_est": 75.64, "sample_count": 21, "duration_seconds": 2.657}, "timestamp": "2026-01-16T15:59:41.580240"}
{"image_index": 240, "image_name": "000000025057.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025057.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2173.035, "latencies_ms": [2173.035], "images_per_second": 0.46, "prompt_tokens": 19, "response_tokens_est": 58, "n_tiles": 6, "output_text": "The image depicts a man in a green shirt and khaki shorts, barefoot, holding a green bottle in his right hand, and wearing a white baseball cap with a logo. The setting is a lush green field under a clear blue sky, suggesting a warm, sunny day.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25593.8, "ram_available_mb": 100178.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 25594.1, "ram_available_mb": 100178.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.95, "peak": 15.32, "min": 14.32}, "VDD_GPU": {"avg": 29.43, "peak": 37.82, "min": 24.04}, "VIN": {"avg": 67.76, "peak": 91.85, "min": 56.65}, "VDD_CPU_SOC_MSS": {"avg": 15.49, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 29.43, "energy_joules_est": 63.97, "sample_count": 17, "duration_seconds": 2.173}, "timestamp": "2026-01-16T15:59:43.759342"}
{"image_index": 241, "image_name": "000000025096.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025096.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2254.009, "latencies_ms": [2254.009], "images_per_second": 0.444, "prompt_tokens": 9, "response_tokens_est": 25, "n_tiles": 12, "output_text": "A young boy is sitting on a bed, looking at a cake decorated with a toy airplane and a toy airplane figure.", "error": null, "sys_before": {"cpu_percent": 24.1, "ram_used_mb": 25593.8, "ram_available_mb": 100178.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 2.6, "ram_used_mb": 25593.8, "ram_available_mb": 100178.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.25, "peak": 16.15, "min": 14.4}, "VDD_GPU": {"avg": 35.75, "peak": 41.36, "min": 28.36}, "VIN": {"avg": 75.65, "peak": 108.62, "min": 55.87}, "VDD_CPU_SOC_MSS": {"avg": 15.95, "peak": 17.32, "min": 15.35}}, "power_watts_avg": 35.75, "energy_joules_est": 80.6, "sample_count": 17, "duration_seconds": 2.254}, "timestamp": "2026-01-16T15:59:46.058941"}
{"image_index": 241, "image_name": "000000025096.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025096.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 5856.198, "latencies_ms": [5856.198], "images_per_second": 0.171, "prompt_tokens": 23, "response_tokens_est": 128, "n_tiles": 12, "output_text": "object: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject:", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25593.8, "ram_available_mb": 100178.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 25593.8, "ram_available_mb": 100178.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.11, "peak": 15.65, "min": 14.43}, "VDD_GPU": {"avg": 30.54, "peak": 42.94, "min": 25.99}, "VIN": {"avg": 71.43, "peak": 113.72, "min": 57.72}, "VDD_CPU_SOC_MSS": {"avg": 15.75, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 30.54, "energy_joules_est": 178.87, "sample_count": 46, "duration_seconds": 5.857}, "timestamp": "2026-01-16T15:59:51.922226"}
{"image_index": 241, "image_name": "000000025096.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025096.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3855.916, "latencies_ms": [3855.916], "images_per_second": 0.259, "prompt_tokens": 27, "response_tokens_est": 69, "n_tiles": 12, "output_text": "The main object in the foreground is a cake with a chocolate icing design, which is placed on a table covered with a colorful tablecloth. In the background, there is a boy wearing a blue jersey, and a white plate is visible on the table. The boy is leaning over the table, possibly preparing to cut the cake.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25593.8, "ram_available_mb": 100178.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25594.0, "ram_available_mb": 100178.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.1, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 32.71, "peak": 42.54, "min": 26.39}, "VIN": {"avg": 78.31, "peak": 126.79, "min": 58.67}, "VDD_CPU_SOC_MSS": {"avg": 15.72, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 32.71, "energy_joules_est": 126.14, "sample_count": 30, "duration_seconds": 3.856}, "timestamp": "2026-01-16T15:59:55.784287"}
{"image_index": 241, "image_name": "000000025096.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025096.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3684.551, "latencies_ms": [3684.551], "images_per_second": 0.271, "prompt_tokens": 21, "response_tokens_est": 64, "n_tiles": 12, "output_text": "The image depicts a young boy sitting at a table, focused on a cake. The cake is decorated with various elements, including a toy airplane, a toy dinosaur, and chocolate pieces. The boy is wearing a blue shirt and appears to be enjoying the cake, with a white plate in front of him.", "error": null, "sys_before": {"cpu_percent": 40.0, "ram_used_mb": 25594.0, "ram_available_mb": 100178.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25593.8, "ram_available_mb": 100178.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.09, "peak": 15.75, "min": 14.53}, "VDD_GPU": {"avg": 32.8, "peak": 42.53, "min": 26.39}, "VIN": {"avg": 73.27, "peak": 97.82, "min": 58.76}, "VDD_CPU_SOC_MSS": {"avg": 15.69, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 32.8, "energy_joules_est": 120.87, "sample_count": 29, "duration_seconds": 3.685}, "timestamp": "2026-01-16T15:59:59.475187"}
{"image_index": 241, "image_name": "000000025096.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025096.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2712.728, "latencies_ms": [2712.728], "images_per_second": 0.369, "prompt_tokens": 19, "response_tokens_est": 35, "n_tiles": 12, "output_text": "The cake is richly decorated with chocolate and red icing, and it is placed on a colorful tablecloth. The lighting is warm, creating a cozy atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25593.8, "ram_available_mb": 100178.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 25593.8, "ram_available_mb": 100178.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.83, "peak": 15.34, "min": 14.12}, "VDD_GPU": {"avg": 35.32, "peak": 42.54, "min": 26.79}, "VIN": {"avg": 74.31, "peak": 114.39, "min": 58.65}, "VDD_CPU_SOC_MSS": {"avg": 15.41, "peak": 16.14, "min": 14.56}}, "power_watts_avg": 35.32, "energy_joules_est": 95.83, "sample_count": 21, "duration_seconds": 2.713}, "timestamp": "2026-01-16T16:00:02.194306"}
{"image_index": 242, "image_name": "000000025139.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025139.jpg", "image_width": 500, "image_height": 334, "image_resolution": "500x334", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1234.418, "latencies_ms": [1234.418], "images_per_second": 0.81, "prompt_tokens": 9, "response_tokens_est": 24, "n_tiles": 6, "output_text": "The image shows a close-up of a zebra's face, with its distinctive black and white stripes clearly visible.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25593.8, "ram_available_mb": 100178.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25593.8, "ram_available_mb": 100178.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.33, "peak": 14.81, "min": 13.82}, "VDD_GPU": {"avg": 33.44, "peak": 38.21, "min": 27.18}, "VIN": {"avg": 77.31, "peak": 114.58, "min": 50.44}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 33.44, "energy_joules_est": 41.31, "sample_count": 9, "duration_seconds": 1.235}, "timestamp": "2026-01-16T16:00:03.468334"}
{"image_index": 242, "image_name": "000000025139.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025139.jpg", "image_width": 500, "image_height": 334, "image_resolution": "500x334", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1086.847, "latencies_ms": [1086.847], "images_per_second": 0.92, "prompt_tokens": 23, "response_tokens_est": 17, "n_tiles": 6, "output_text": "zebra: 1\nfence: 1\nground: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25594.0, "ram_available_mb": 100178.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 25594.0, "ram_available_mb": 100178.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.63, "peak": 14.83, "min": 14.22}, "VDD_GPU": {"avg": 35.25, "peak": 38.98, "min": 29.54}, "VIN": {"avg": 81.65, "peak": 116.73, "min": 59.84}, "VDD_CPU_SOC_MSS": {"avg": 15.3, "peak": 15.74, "min": 14.96}}, "power_watts_avg": 35.25, "energy_joules_est": 38.32, "sample_count": 8, "duration_seconds": 1.087}, "timestamp": "2026-01-16T16:00:04.560991"}
{"image_index": 242, "image_name": "000000025139.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025139.jpg", "image_width": 500, "image_height": 334, "image_resolution": "500x334", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2857.465, "latencies_ms": [2857.465], "images_per_second": 0.35, "prompt_tokens": 27, "response_tokens_est": 83, "n_tiles": 6, "output_text": "The main object in the foreground is a zebra, which is positioned near the right side of the image. The zebra's stripes are clearly visible, and it appears to be standing close to a metal fence. In the background, there is another zebra, partially visible, and it seems to be further away from the foreground zebra. The zebra in the background is also near a metal fence.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25594.0, "ram_available_mb": 100178.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 25594.0, "ram_available_mb": 100178.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.97, "peak": 15.32, "min": 14.22}, "VDD_GPU": {"avg": 28.9, "peak": 39.38, "min": 23.64}, "VIN": {"avg": 70.71, "peak": 112.19, "min": 61.2}, "VDD_CPU_SOC_MSS": {"avg": 15.57, "peak": 16.14, "min": 14.96}}, "power_watts_avg": 28.9, "energy_joules_est": 82.59, "sample_count": 22, "duration_seconds": 2.858}, "timestamp": "2026-01-16T16:00:07.424479"}
{"image_index": 242, "image_name": "000000025139.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025139.jpg", "image_width": 500, "image_height": 334, "image_resolution": "500x334", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2098.426, "latencies_ms": [2098.426], "images_per_second": 0.477, "prompt_tokens": 21, "response_tokens_est": 55, "n_tiles": 6, "output_text": "The image depicts a zebra standing in a pen, with its head turned slightly to the side. The zebra appears to be in a zoo or a similar controlled environment, as indicated by the presence of a metal fence and the ground covered with dirt and rocks.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25594.0, "ram_available_mb": 100178.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 25594.3, "ram_available_mb": 100177.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.94, "peak": 15.22, "min": 14.32}, "VDD_GPU": {"avg": 30.12, "peak": 38.21, "min": 24.03}, "VIN": {"avg": 72.21, "peak": 112.45, "min": 56.73}, "VDD_CPU_SOC_MSS": {"avg": 15.54, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 30.12, "energy_joules_est": 63.22, "sample_count": 15, "duration_seconds": 2.099}, "timestamp": "2026-01-16T16:00:09.529422"}
{"image_index": 242, "image_name": "000000025139.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025139.jpg", "image_width": 500, "image_height": 334, "image_resolution": "500x334", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1941.909, "latencies_ms": [1941.909], "images_per_second": 0.515, "prompt_tokens": 19, "response_tokens_est": 49, "n_tiles": 6, "output_text": "The zebra in the image has a striking black and white striped pattern, which is highly visible against the natural backdrop. The lighting is bright and natural, casting shadows on the ground, indicating that the photo was taken during the day.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25594.3, "ram_available_mb": 100177.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 25594.0, "ram_available_mb": 100178.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.88, "peak": 15.22, "min": 14.32}, "VDD_GPU": {"avg": 30.58, "peak": 38.21, "min": 24.42}, "VIN": {"avg": 67.53, "peak": 114.09, "min": 56.78}, "VDD_CPU_SOC_MSS": {"avg": 15.55, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 30.58, "energy_joules_est": 59.4, "sample_count": 14, "duration_seconds": 1.942}, "timestamp": "2026-01-16T16:00:11.481384"}
{"image_index": 243, "image_name": "000000025181.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025181.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1582.142, "latencies_ms": [1582.142], "images_per_second": 0.632, "prompt_tokens": 9, "response_tokens_est": 37, "n_tiles": 6, "output_text": "The image depicts a black and white photograph of a train station platform with a train approaching, and a sign indicating the station's name, \"La Spezia Centrale.\"", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 25594.0, "ram_available_mb": 100178.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 25594.3, "ram_available_mb": 100177.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.58, "peak": 15.01, "min": 13.92}, "VDD_GPU": {"avg": 31.3, "peak": 37.42, "min": 25.21}, "VIN": {"avg": 71.66, "peak": 106.54, "min": 50.03}, "VDD_CPU_SOC_MSS": {"avg": 15.28, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 31.3, "energy_joules_est": 49.54, "sample_count": 11, "duration_seconds": 1.583}, "timestamp": "2026-01-16T16:00:13.103726"}
{"image_index": 243, "image_name": "000000025181.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025181.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1735.072, "latencies_ms": [1735.072], "images_per_second": 0.576, "prompt_tokens": 23, "response_tokens_est": 40, "n_tiles": 6, "output_text": "object: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 25594.3, "ram_available_mb": 100177.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 25594.5, "ram_available_mb": 100177.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.67, "peak": 15.01, "min": 14.12}, "VDD_GPU": {"avg": 31.06, "peak": 38.59, "min": 24.03}, "VIN": {"avg": 67.45, "peak": 105.76, "min": 55.28}, "VDD_CPU_SOC_MSS": {"avg": 15.2, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 31.06, "energy_joules_est": 53.9, "sample_count": 13, "duration_seconds": 1.735}, "timestamp": "2026-01-16T16:00:14.846770"}
{"image_index": 243, "image_name": "000000025181.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025181.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2275.95, "latencies_ms": [2275.95], "images_per_second": 0.439, "prompt_tokens": 27, "response_tokens_est": 59, "n_tiles": 6, "output_text": "The main objects in the image are the train and the platform. The train is positioned on the right side of the image, while the platform is on the left side. The train is closer to the foreground, and the platform is further back, creating a clear spatial relationship between the two.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25594.5, "ram_available_mb": 100177.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 25594.5, "ram_available_mb": 100177.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.71, "peak": 15.01, "min": 14.02}, "VDD_GPU": {"avg": 29.24, "peak": 38.19, "min": 23.25}, "VIN": {"avg": 74.22, "peak": 113.47, "min": 57.33}, "VDD_CPU_SOC_MSS": {"avg": 15.24, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 29.24, "energy_joules_est": 66.56, "sample_count": 17, "duration_seconds": 2.276}, "timestamp": "2026-01-16T16:00:17.128928"}
{"image_index": 243, "image_name": "000000025181.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025181.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1822.9, "latencies_ms": [1822.9], "images_per_second": 0.549, "prompt_tokens": 21, "response_tokens_est": 43, "n_tiles": 6, "output_text": "The image depicts a railway station with a train stationed at the platform. The station is in a rural or semi-rural setting, with a clear sky overhead and a few people visible on the platform.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25594.5, "ram_available_mb": 100177.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 25594.5, "ram_available_mb": 100177.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.68, "peak": 15.01, "min": 14.12}, "VDD_GPU": {"avg": 30.66, "peak": 38.19, "min": 24.03}, "VIN": {"avg": 68.28, "peak": 114.31, "min": 55.39}, "VDD_CPU_SOC_MSS": {"avg": 15.23, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 30.66, "energy_joules_est": 55.9, "sample_count": 13, "duration_seconds": 1.823}, "timestamp": "2026-01-16T16:00:18.961944"}
{"image_index": 243, "image_name": "000000025181.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025181.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1799.767, "latencies_ms": [1799.767], "images_per_second": 0.556, "prompt_tokens": 19, "response_tokens_est": 42, "n_tiles": 6, "output_text": "The image is a black and white photograph of a train station platform. The platform is made of brick, and there is a concrete pillar in the center. The station has overhead lighting and a covered area.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25594.5, "ram_available_mb": 100177.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25594.5, "ram_available_mb": 100177.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.61, "peak": 14.91, "min": 14.02}, "VDD_GPU": {"avg": 30.76, "peak": 38.21, "min": 24.03}, "VIN": {"avg": 70.49, "peak": 110.3, "min": 58.27}, "VDD_CPU_SOC_MSS": {"avg": 15.2, "peak": 15.36, "min": 14.57}}, "power_watts_avg": 30.76, "energy_joules_est": 55.38, "sample_count": 13, "duration_seconds": 1.801}, "timestamp": "2026-01-16T16:00:20.772384"}
{"image_index": 244, "image_name": "000000025228.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025228.jpg", "image_width": 640, "image_height": 436, "image_resolution": "640x436", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1155.701, "latencies_ms": [1155.701], "images_per_second": 0.865, "prompt_tokens": 9, "response_tokens_est": 21, "n_tiles": 6, "output_text": "A person is sitting on a surfboard in the water, with the sun setting in the background.", "error": null, "sys_before": {"cpu_percent": 18.6, "ram_used_mb": 25594.5, "ram_available_mb": 100177.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25594.7, "ram_available_mb": 100177.4, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.37, "peak": 14.71, "min": 13.92}, "VDD_GPU": {"avg": 33.38, "peak": 37.03, "min": 28.36}, "VIN": {"avg": 71.16, "peak": 118.14, "min": 50.08}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 33.38, "energy_joules_est": 38.59, "sample_count": 8, "duration_seconds": 1.156}, "timestamp": "2026-01-16T16:00:21.968927"}
{"image_index": 244, "image_name": "000000025228.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025228.jpg", "image_width": 640, "image_height": 436, "image_resolution": "640x436", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1380.467, "latencies_ms": [1380.467], "images_per_second": 0.724, "prompt_tokens": 23, "response_tokens_est": 28, "n_tiles": 6, "output_text": "surfboard: 1\nwoman: 1\nwater: 1\nocean: 1\nsun: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25594.7, "ram_available_mb": 100177.4, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 25594.5, "ram_available_mb": 100177.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.66, "peak": 15.01, "min": 14.22}, "VDD_GPU": {"avg": 33.64, "peak": 39.0, "min": 26.79}, "VIN": {"avg": 72.94, "peak": 109.26, "min": 59.03}, "VDD_CPU_SOC_MSS": {"avg": 15.31, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 33.64, "energy_joules_est": 46.46, "sample_count": 10, "duration_seconds": 1.381}, "timestamp": "2026-01-16T16:00:23.355640"}
{"image_index": 244, "image_name": "000000025228.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025228.jpg", "image_width": 640, "image_height": 436, "image_resolution": "640x436", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2908.323, "latencies_ms": [2908.323], "images_per_second": 0.344, "prompt_tokens": 27, "response_tokens_est": 85, "n_tiles": 6, "output_text": "The main object in the foreground is a person sitting on a red surfboard. The person is positioned near the center of the image, with their back to the camera. The background features a vast body of water, likely the ocean, with a cloudy sky above. The surfboard is positioned in the foreground, closer to the camera, while the person is further away, creating a sense of depth in the image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25594.5, "ram_available_mb": 100177.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 25594.7, "ram_available_mb": 100177.5, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.0, "peak": 15.32, "min": 14.22}, "VDD_GPU": {"avg": 28.52, "peak": 39.0, "min": 23.64}, "VIN": {"avg": 73.38, "peak": 118.28, "min": 62.54}, "VDD_CPU_SOC_MSS": {"avg": 15.55, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 28.52, "energy_joules_est": 82.95, "sample_count": 22, "duration_seconds": 2.909}, "timestamp": "2026-01-16T16:00:26.269965"}
{"image_index": 244, "image_name": "000000025228.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025228.jpg", "image_width": 640, "image_height": 436, "image_resolution": "640x436", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2819.701, "latencies_ms": [2819.701], "images_per_second": 0.355, "prompt_tokens": 21, "response_tokens_est": 82, "n_tiles": 6, "output_text": "The image captures a serene scene of a person sitting on a surfboard, likely preparing for or enjoying a ride on the water. The setting is at dusk, with the sky displaying a gradient of colors from orange to purple, indicating the time of day. The person is dressed in a black wetsuit, and the surfboard is prominently red with a visible logo or design on it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25594.7, "ram_available_mb": 100177.5, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 25594.7, "ram_available_mb": 100177.5, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.02, "peak": 15.32, "min": 14.32}, "VDD_GPU": {"avg": 28.18, "peak": 37.8, "min": 23.64}, "VIN": {"avg": 71.48, "peak": 125.19, "min": 58.28}, "VDD_CPU_SOC_MSS": {"avg": 15.57, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 28.18, "energy_joules_est": 79.47, "sample_count": 22, "duration_seconds": 2.82}, "timestamp": "2026-01-16T16:00:29.095608"}
{"image_index": 244, "image_name": "000000025228.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025228.jpg", "image_width": 640, "image_height": 436, "image_resolution": "640x436", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1780.063, "latencies_ms": [1780.063], "images_per_second": 0.562, "prompt_tokens": 19, "response_tokens_est": 43, "n_tiles": 6, "output_text": "The image captures a woman in a black wetsuit sitting on a red surfboard, with the sun setting in the background. The sky is filled with dramatic clouds, casting a warm glow over the scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25594.7, "ram_available_mb": 100177.5, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25594.7, "ram_available_mb": 100177.5, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.82, "peak": 15.11, "min": 14.32}, "VDD_GPU": {"avg": 31.02, "peak": 37.8, "min": 24.82}, "VIN": {"avg": 67.73, "peak": 107.19, "min": 55.36}, "VDD_CPU_SOC_MSS": {"avg": 15.44, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 31.02, "energy_joules_est": 55.23, "sample_count": 13, "duration_seconds": 1.781}, "timestamp": "2026-01-16T16:00:30.881444"}
{"image_index": 245, "image_name": "000000025386.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025386.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1391.846, "latencies_ms": [1391.846], "images_per_second": 0.718, "prompt_tokens": 9, "response_tokens_est": 30, "n_tiles": 6, "output_text": "A man and a woman are seated in a train carriage, with the man holding chopsticks and the woman holding a small bag of snacks.", "error": null, "sys_before": {"cpu_percent": 28.3, "ram_used_mb": 25594.7, "ram_available_mb": 100177.5, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 25594.5, "ram_available_mb": 100177.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.5, "peak": 14.91, "min": 13.92}, "VDD_GPU": {"avg": 32.34, "peak": 37.42, "min": 26.4}, "VIN": {"avg": 72.74, "peak": 118.01, "min": 61.53}, "VDD_CPU_SOC_MSS": {"avg": 15.16, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 32.34, "energy_joules_est": 45.03, "sample_count": 10, "duration_seconds": 1.393}, "timestamp": "2026-01-16T16:00:32.309268"}
{"image_index": 245, "image_name": "000000025386.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025386.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2286.142, "latencies_ms": [2286.142], "images_per_second": 0.437, "prompt_tokens": 23, "response_tokens_est": 62, "n_tiles": 6, "output_text": "1. Man: 1\n2. Woman: 1\n3. Table: 1\n4. Plate: 1\n5. Chopsticks: 2\n6. Bag: 1\n7. Bag of food: 1\n8. Tray: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25594.5, "ram_available_mb": 100177.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 25594.5, "ram_available_mb": 100177.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.91, "peak": 15.32, "min": 14.32}, "VDD_GPU": {"avg": 29.8, "peak": 38.6, "min": 24.03}, "VIN": {"avg": 65.68, "peak": 75.72, "min": 55.66}, "VDD_CPU_SOC_MSS": {"avg": 15.45, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 29.8, "energy_joules_est": 68.14, "sample_count": 17, "duration_seconds": 2.286}, "timestamp": "2026-01-16T16:00:34.601427"}
{"image_index": 245, "image_name": "000000025386.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025386.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3169.561, "latencies_ms": [3169.561], "images_per_second": 0.316, "prompt_tokens": 27, "response_tokens_est": 95, "n_tiles": 6, "output_text": "The main objects in the image are a man and a woman seated next to each other. The man is on the left side of the image, while the woman is on the right side. The man is holding chopsticks and appears to be eating something from a small plate. The woman is holding a plastic bag and seems to be eating or preparing to eat something. The background features a window with curtains, a yellow sign, and a glimpse of a train interior.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25594.5, "ram_available_mb": 100177.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 25594.5, "ram_available_mb": 100177.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.04, "peak": 15.32, "min": 14.32}, "VDD_GPU": {"avg": 27.8, "peak": 37.82, "min": 23.64}, "VIN": {"avg": 65.94, "peak": 76.75, "min": 55.62}, "VDD_CPU_SOC_MSS": {"avg": 15.57, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 27.8, "energy_joules_est": 88.13, "sample_count": 24, "duration_seconds": 3.17}, "timestamp": "2026-01-16T16:00:37.777097"}
{"image_index": 245, "image_name": "000000025386.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025386.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2580.711, "latencies_ms": [2580.711], "images_per_second": 0.387, "prompt_tokens": 21, "response_tokens_est": 73, "n_tiles": 6, "output_text": "The image depicts a man and a woman seated in a train carriage, sharing a meal. The man is holding chopsticks and appears to be eating a sushi roll, while the woman is holding a small bag and seems to be enjoying her meal as well. The setting is inside a train, with a window and a seat visible in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25594.5, "ram_available_mb": 100177.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 25594.5, "ram_available_mb": 100177.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.99, "peak": 15.32, "min": 14.22}, "VDD_GPU": {"avg": 28.78, "peak": 37.82, "min": 23.64}, "VIN": {"avg": 72.42, "peak": 125.31, "min": 55.5}, "VDD_CPU_SOC_MSS": {"avg": 15.46, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 28.78, "energy_joules_est": 74.29, "sample_count": 19, "duration_seconds": 2.581}, "timestamp": "2026-01-16T16:00:40.364127"}
{"image_index": 245, "image_name": "000000025386.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025386.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2230.013, "latencies_ms": [2230.013], "images_per_second": 0.448, "prompt_tokens": 19, "response_tokens_est": 60, "n_tiles": 6, "output_text": "The image depicts a man and a woman seated in a train carriage, with the man wearing a plaid shirt and the woman in a red and white checkered top. The lighting is bright, and the carriage appears to be well-lit, with a yellow sign visible in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25594.5, "ram_available_mb": 100177.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 25594.5, "ram_available_mb": 100177.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.94, "peak": 15.32, "min": 14.32}, "VDD_GPU": {"avg": 29.47, "peak": 38.19, "min": 24.03}, "VIN": {"avg": 71.2, "peak": 120.5, "min": 57.58}, "VDD_CPU_SOC_MSS": {"avg": 15.47, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 29.47, "energy_joules_est": 65.73, "sample_count": 17, "duration_seconds": 2.23}, "timestamp": "2026-01-16T16:00:42.599958"}
{"image_index": 246, "image_name": "000000025393.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025393.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1953.35, "latencies_ms": [1953.35], "images_per_second": 0.512, "prompt_tokens": 9, "response_tokens_est": 16, "n_tiles": 12, "output_text": "A man in a suit and tie is walking down a street at night.", "error": null, "sys_before": {"cpu_percent": 24.2, "ram_used_mb": 25594.5, "ram_available_mb": 100177.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 2.3, "ram_used_mb": 25594.7, "ram_available_mb": 100177.5, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.3, "peak": 16.15, "min": 14.4}, "VDD_GPU": {"avg": 36.8, "peak": 41.36, "min": 29.54}, "VIN": {"avg": 76.67, "peak": 104.88, "min": 58.09}, "VDD_CPU_SOC_MSS": {"avg": 16.05, "peak": 17.32, "min": 15.35}}, "power_watts_avg": 36.8, "energy_joules_est": 71.91, "sample_count": 14, "duration_seconds": 1.954}, "timestamp": "2026-01-16T16:00:44.606013"}
{"image_index": 246, "image_name": "000000025393.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025393.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2671.242, "latencies_ms": [2671.242], "images_per_second": 0.374, "prompt_tokens": 23, "response_tokens_est": 34, "n_tiles": 12, "output_text": "1. Man\n2. Shirt\n3. Tie\n4. Suits\n5. Shoes\n6. Street light\n7. Building\n8. Car", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25594.7, "ram_available_mb": 100177.5, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 25594.4, "ram_available_mb": 100177.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.08, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 36.14, "peak": 42.94, "min": 27.57}, "VIN": {"avg": 75.34, "peak": 116.38, "min": 58.66}, "VDD_CPU_SOC_MSS": {"avg": 15.69, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 36.14, "energy_joules_est": 96.56, "sample_count": 20, "duration_seconds": 2.672}, "timestamp": "2026-01-16T16:00:47.283809"}
{"image_index": 246, "image_name": "000000025393.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025393.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 4390.524, "latencies_ms": [4390.524], "images_per_second": 0.228, "prompt_tokens": 27, "response_tokens_est": 85, "n_tiles": 12, "output_text": "The main object in the foreground is a man dressed in a suit and tie, walking on a sidewalk. The man is positioned near a street lamp and a building with a sign that reads \"Hierro Albero.\" The background features a building with a balcony and a sign that reads \"Hierro Albero.\" The man is walking towards the camera, and the street lamp is positioned to his left.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25594.4, "ram_available_mb": 100177.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25594.4, "ram_available_mb": 100177.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.12, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 31.95, "peak": 42.54, "min": 26.39}, "VIN": {"avg": 70.74, "peak": 102.3, "min": 56.23}, "VDD_CPU_SOC_MSS": {"avg": 15.61, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 31.95, "energy_joules_est": 140.29, "sample_count": 34, "duration_seconds": 4.391}, "timestamp": "2026-01-16T16:00:51.680728"}
{"image_index": 246, "image_name": "000000025393.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025393.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 5034.929, "latencies_ms": [5034.929], "images_per_second": 0.199, "prompt_tokens": 21, "response_tokens_est": 104, "n_tiles": 12, "output_text": "The image depicts a nighttime scene on a city street, featuring two men dressed in formal attire. One man is wearing a white shirt and black tie, while the other is dressed in a dark suit and pink shirt with a striped tie. Both men are walking, with the man in the suit appearing to be in motion, possibly walking briskly. The background shows a building with a sign reading \"Hierro Albero,\" and there are streetlights and a lamppost visible.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25594.4, "ram_available_mb": 100177.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25594.4, "ram_available_mb": 100177.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.11, "peak": 15.65, "min": 14.63}, "VDD_GPU": {"avg": 30.92, "peak": 42.54, "min": 26.0}, "VIN": {"avg": 71.1, "peak": 113.03, "min": 54.57}, "VDD_CPU_SOC_MSS": {"avg": 15.74, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 30.92, "energy_joules_est": 155.69, "sample_count": 40, "duration_seconds": 5.035}, "timestamp": "2026-01-16T16:00:56.723649"}
{"image_index": 246, "image_name": "000000025393.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025393.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3663.571, "latencies_ms": [3663.571], "images_per_second": 0.273, "prompt_tokens": 19, "response_tokens_est": 63, "n_tiles": 12, "output_text": "The image depicts a man dressed in a formal black suit and tie walking on a city street at night. The lighting is dim, with streetlights illuminating the scene, casting a warm glow on the man's face. The background features buildings with lit signs, a crosswalk, and a metal pole.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25594.4, "ram_available_mb": 100177.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25594.7, "ram_available_mb": 100177.5, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.07, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 32.84, "peak": 42.53, "min": 25.99}, "VIN": {"avg": 72.43, "peak": 108.49, "min": 58.44}, "VDD_CPU_SOC_MSS": {"avg": 15.66, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 32.84, "energy_joules_est": 120.32, "sample_count": 28, "duration_seconds": 3.664}, "timestamp": "2026-01-16T16:01:00.393307"}
{"image_index": 247, "image_name": "000000025394.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025394.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2049.195, "latencies_ms": [2049.195], "images_per_second": 0.488, "prompt_tokens": 9, "response_tokens_est": 19, "n_tiles": 12, "output_text": "A man is holding a bottle of wine and a glass of wine in a bar setting.", "error": null, "sys_before": {"cpu_percent": 45.2, "ram_used_mb": 25594.7, "ram_available_mb": 100177.5, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 2.2, "ram_used_mb": 25594.7, "ram_available_mb": 100177.5, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.12, "peak": 16.15, "min": 13.69}, "VDD_GPU": {"avg": 36.38, "peak": 41.76, "min": 29.15}, "VIN": {"avg": 84.23, "peak": 118.8, "min": 54.85}, "VDD_CPU_SOC_MSS": {"avg": 15.92, "peak": 16.92, "min": 14.96}}, "power_watts_avg": 36.38, "energy_joules_est": 74.57, "sample_count": 16, "duration_seconds": 2.05}, "timestamp": "2026-01-16T16:01:02.534674"}
{"image_index": 247, "image_name": "000000025394.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025394.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 5846.152, "latencies_ms": [5846.152], "images_per_second": 0.171, "prompt_tokens": 23, "response_tokens_est": 129, "n_tiles": 12, "output_text": "- Glass: 2\n- Wine glass: 1\n- Bottle: 1\n- Wine bottle: 1\n- Candle holder: 1\n- Candle: 1\n- Candle holder: 1\n- Candle: 1\n- Candle holder: 1\n- Candle: 1\n- Candle holder: 1\n- Candle: 1\n- Candle holder: 1\n- Candle: 1\n- Candle holder: 1\n- Candle: 1\n- Candle holder: 1\n- Candle:", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25594.7, "ram_available_mb": 100177.5, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 25594.4, "ram_available_mb": 100177.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.14, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 30.64, "peak": 43.73, "min": 26.0}, "VIN": {"avg": 72.1, "peak": 115.75, "min": 45.26}, "VDD_CPU_SOC_MSS": {"avg": 15.75, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 30.64, "energy_joules_est": 179.15, "sample_count": 45, "duration_seconds": 5.847}, "timestamp": "2026-01-16T16:01:08.387615"}
{"image_index": 247, "image_name": "000000025394.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025394.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3651.756, "latencies_ms": [3651.756], "images_per_second": 0.274, "prompt_tokens": 27, "response_tokens_est": 63, "n_tiles": 12, "output_text": "The main object in the foreground is a glass of red wine, held by a person's hand. The glass is placed on a wooden table. In the background, there is a man wearing glasses and a gray sweater, standing near a wooden cabinet filled with wine bottles. The cabinet is situated behind the man.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25594.4, "ram_available_mb": 100177.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25594.4, "ram_available_mb": 100177.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.11, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 33.09, "peak": 42.54, "min": 26.39}, "VIN": {"avg": 74.14, "peak": 115.17, "min": 58.61}, "VDD_CPU_SOC_MSS": {"avg": 15.72, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 33.09, "energy_joules_est": 120.85, "sample_count": 27, "duration_seconds": 3.652}, "timestamp": "2026-01-16T16:01:12.047958"}
{"image_index": 247, "image_name": "000000025394.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025394.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3781.008, "latencies_ms": [3781.008], "images_per_second": 0.264, "prompt_tokens": 21, "response_tokens_est": 67, "n_tiles": 12, "output_text": "The image depicts a man in a cozy, dimly lit bar setting, holding a bottle of wine and a glass. He appears to be engaged in a conversation or perhaps tasting the wine. The background features a wooden bar counter with various bottles and glasses, a wine rack, and a small table with a menu or card.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25594.4, "ram_available_mb": 100177.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25594.4, "ram_available_mb": 100177.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.11, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 32.71, "peak": 42.15, "min": 26.39}, "VIN": {"avg": 74.03, "peak": 129.26, "min": 58.44}, "VDD_CPU_SOC_MSS": {"avg": 15.7, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 32.71, "energy_joules_est": 123.69, "sample_count": 29, "duration_seconds": 3.781}, "timestamp": "2026-01-16T16:01:15.835261"}
{"image_index": 247, "image_name": "000000025394.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025394.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3101.579, "latencies_ms": [3101.579], "images_per_second": 0.322, "prompt_tokens": 19, "response_tokens_est": 47, "n_tiles": 12, "output_text": "The image depicts a man in a cozy, dimly lit bar setting. He is holding a wine glass and a bottle, with a bottle opener nearby. The lighting is warm and subdued, creating a relaxed atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25594.4, "ram_available_mb": 100177.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 25594.4, "ram_available_mb": 100177.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.1, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 34.11, "peak": 42.15, "min": 26.4}, "VIN": {"avg": 73.96, "peak": 116.05, "min": 56.28}, "VDD_CPU_SOC_MSS": {"avg": 15.68, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 34.11, "energy_joules_est": 105.81, "sample_count": 24, "duration_seconds": 3.102}, "timestamp": "2026-01-16T16:01:18.943383"}
{"image_index": 248, "image_name": "000000025424.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025424.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1416.245, "latencies_ms": [1416.245], "images_per_second": 0.706, "prompt_tokens": 9, "response_tokens_est": 31, "n_tiles": 6, "output_text": "A tennis player is captured mid-action, with his racket in motion, preparing to hit a tennis ball on a well-maintained grass court.", "error": null, "sys_before": {"cpu_percent": 42.5, "ram_used_mb": 25594.4, "ram_available_mb": 100177.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 25593.3, "ram_available_mb": 100178.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.44, "peak": 15.01, "min": 13.92}, "VDD_GPU": {"avg": 32.46, "peak": 37.82, "min": 26.39}, "VIN": {"avg": 82.21, "peak": 126.41, "min": 61.83}, "VDD_CPU_SOC_MSS": {"avg": 15.27, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 32.46, "energy_joules_est": 45.98, "sample_count": 10, "duration_seconds": 1.417}, "timestamp": "2026-01-16T16:01:20.421000"}
{"image_index": 248, "image_name": "000000025424.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025424.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1801.974, "latencies_ms": [1801.974], "images_per_second": 0.555, "prompt_tokens": 23, "response_tokens_est": 44, "n_tiles": 6, "output_text": "1. Tennis player\n2. Tennis racket\n3. Tennis ball\n4. White shirt\n5. White shorts\n6. White wristband\n7. White wristband\n8. White wristband", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25593.3, "ram_available_mb": 100178.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 25593.3, "ram_available_mb": 100178.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.82, "peak": 15.11, "min": 14.22}, "VDD_GPU": {"avg": 31.57, "peak": 38.6, "min": 24.82}, "VIN": {"avg": 77.69, "peak": 129.46, "min": 60.41}, "VDD_CPU_SOC_MSS": {"avg": 15.47, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 31.57, "energy_joules_est": 56.9, "sample_count": 13, "duration_seconds": 1.802}, "timestamp": "2026-01-16T16:01:22.233196"}
{"image_index": 248, "image_name": "000000025424.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025424.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3312.75, "latencies_ms": [3312.75], "images_per_second": 0.302, "prompt_tokens": 27, "response_tokens_est": 96, "n_tiles": 6, "output_text": "The main object in the foreground is a tennis player, who is captured mid-action, with their right arm extended and the racket in motion. The player is wearing a white shirt and white shorts, and their torso is slightly tilted towards the left side of the image. The background features a well-maintained grassy tennis court, with white boundary lines visible. The tennis ball is in the upper right corner of the image, slightly blurred, indicating motion.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25593.3, "ram_available_mb": 100178.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 25593.6, "ram_available_mb": 100178.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.84, "peak": 15.01, "min": 14.12}, "VDD_GPU": {"avg": 27.38, "peak": 38.59, "min": 23.24}, "VIN": {"avg": 67.68, "peak": 107.8, "min": 56.48}, "VDD_CPU_SOC_MSS": {"avg": 15.39, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 27.38, "energy_joules_est": 90.71, "sample_count": 25, "duration_seconds": 3.313}, "timestamp": "2026-01-16T16:01:25.555270"}
{"image_index": 248, "image_name": "000000025424.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025424.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2888.036, "latencies_ms": [2888.036], "images_per_second": 0.346, "prompt_tokens": 21, "response_tokens_est": 81, "n_tiles": 6, "output_text": "The image captures a moment during a tennis match on a well-maintained grass court. A player is in the midst of a powerful serve, with their racket swung high and their body leaning forward, indicating the force and intensity of the shot. The player is dressed in white attire, and the background shows the lush green grass of the court, emphasizing the sport's natural setting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25593.6, "ram_available_mb": 100178.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 25593.3, "ram_available_mb": 100178.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.79, "peak": 15.11, "min": 14.02}, "VDD_GPU": {"avg": 27.82, "peak": 37.82, "min": 23.24}, "VIN": {"avg": 67.5, "peak": 98.8, "min": 55.75}, "VDD_CPU_SOC_MSS": {"avg": 15.35, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 27.82, "energy_joules_est": 80.35, "sample_count": 22, "duration_seconds": 2.888}, "timestamp": "2026-01-16T16:01:28.451499"}
{"image_index": 248, "image_name": "000000025424.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025424.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2629.352, "latencies_ms": [2629.352], "images_per_second": 0.38, "prompt_tokens": 19, "response_tokens_est": 72, "n_tiles": 6, "output_text": "The image captures a tennis player in mid-action, wearing a white shirt and white shorts. The player is holding a tennis racket, and the background shows a well-maintained grass court with white boundary lines. The lighting is bright, indicating it is daytime, and the colors are vibrant, with the green grass and white lines providing a stark contrast.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25593.3, "ram_available_mb": 100178.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 25593.3, "ram_available_mb": 100178.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.78, "peak": 15.01, "min": 14.12}, "VDD_GPU": {"avg": 28.22, "peak": 38.19, "min": 23.24}, "VIN": {"avg": 68.38, "peak": 117.44, "min": 57.49}, "VDD_CPU_SOC_MSS": {"avg": 15.26, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 28.22, "energy_joules_est": 74.21, "sample_count": 20, "duration_seconds": 2.63}, "timestamp": "2026-01-16T16:01:31.087049"}
{"image_index": 249, "image_name": "000000025560.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025560.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2214.449, "latencies_ms": [2214.449], "images_per_second": 0.452, "prompt_tokens": 9, "response_tokens_est": 24, "n_tiles": 12, "output_text": "A ginger and white cat is perched on top of a wooden shelf, seemingly watching something on the television screen.", "error": null, "sys_before": {"cpu_percent": 46.7, "ram_used_mb": 25593.3, "ram_available_mb": 100178.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 2.6, "ram_used_mb": 25593.6, "ram_available_mb": 100178.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.14, "peak": 16.15, "min": 13.9}, "VDD_GPU": {"avg": 35.47, "peak": 40.97, "min": 28.36}, "VIN": {"avg": 75.65, "peak": 102.8, "min": 58.56}, "VDD_CPU_SOC_MSS": {"avg": 16.07, "peak": 17.32, "min": 14.96}}, "power_watts_avg": 35.47, "energy_joules_est": 78.55, "sample_count": 17, "duration_seconds": 2.215}, "timestamp": "2026-01-16T16:01:33.384325"}
{"image_index": 249, "image_name": "000000025560.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025560.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2807.713, "latencies_ms": [2807.713], "images_per_second": 0.356, "prompt_tokens": 23, "response_tokens_est": 38, "n_tiles": 12, "output_text": "1. Cat\n2. TV\n3. Shelf\n4. Coffee cup\n5. TV stand\n6. Remote control\n7. DVD player\n8. Bookshelf", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25593.6, "ram_available_mb": 100178.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 25593.3, "ram_available_mb": 100178.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.08, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 35.45, "peak": 43.31, "min": 27.18}, "VIN": {"avg": 79.75, "peak": 121.19, "min": 59.26}, "VDD_CPU_SOC_MSS": {"avg": 15.67, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 35.45, "energy_joules_est": 99.55, "sample_count": 21, "duration_seconds": 2.808}, "timestamp": "2026-01-16T16:01:36.200450"}
{"image_index": 249, "image_name": "000000025560.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025560.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3926.52, "latencies_ms": [3926.52], "images_per_second": 0.255, "prompt_tokens": 27, "response_tokens_est": 71, "n_tiles": 12, "output_text": "The main object in the foreground is a cat, which is perched on a wooden shelf. The cat is positioned near the bottom right corner of the image. In the background, there is a television set on a stand, slightly to the left of the cat. The television is positioned behind the cat, making it the focal point of the image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25593.3, "ram_available_mb": 100178.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25593.3, "ram_available_mb": 100178.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.1, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 32.57, "peak": 42.92, "min": 26.39}, "VIN": {"avg": 73.1, "peak": 117.22, "min": 58.55}, "VDD_CPU_SOC_MSS": {"avg": 15.69, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 32.57, "energy_joules_est": 127.91, "sample_count": 31, "duration_seconds": 3.927}, "timestamp": "2026-01-16T16:01:40.134520"}
{"image_index": 249, "image_name": "000000025560.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025560.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3857.063, "latencies_ms": [3857.063], "images_per_second": 0.259, "prompt_tokens": 21, "response_tokens_est": 69, "n_tiles": 12, "output_text": "The image depicts a cozy indoor setting with a television screen displaying a man in a suit. A ginger and white cat is perched on a wooden shelf next to the television, seemingly curious about the screen. The room appears to be a living room or a study, with a bookshelf and a cup of coffee visible in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25593.3, "ram_available_mb": 100178.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25593.3, "ram_available_mb": 100178.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.1, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 32.63, "peak": 42.15, "min": 26.39}, "VIN": {"avg": 74.35, "peak": 114.66, "min": 57.81}, "VDD_CPU_SOC_MSS": {"avg": 15.63, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 32.63, "energy_joules_est": 125.87, "sample_count": 30, "duration_seconds": 3.857}, "timestamp": "2026-01-16T16:01:43.997802"}
{"image_index": 249, "image_name": "000000025560.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025560.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 4113.208, "latencies_ms": [4113.208], "images_per_second": 0.243, "prompt_tokens": 19, "response_tokens_est": 77, "n_tiles": 12, "output_text": "The image features a cat with a white and orange coat, perched on a wooden shelf. The cat's fur appears soft and fluffy, and its tail is slightly curled. The background includes a television set with a dark screen, a white mug on a table, and a wooden bookshelf. The lighting in the room is dim, creating a cozy and intimate atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25593.3, "ram_available_mb": 100178.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25593.5, "ram_available_mb": 100178.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.11, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 32.24, "peak": 42.15, "min": 26.39}, "VIN": {"avg": 72.01, "peak": 114.48, "min": 52.84}, "VDD_CPU_SOC_MSS": {"avg": 15.67, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 32.24, "energy_joules_est": 132.62, "sample_count": 32, "duration_seconds": 4.114}, "timestamp": "2026-01-16T16:01:48.117983"}
{"image_index": 250, "image_name": "000000025593.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025593.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2275.171, "latencies_ms": [2275.171], "images_per_second": 0.44, "prompt_tokens": 9, "response_tokens_est": 26, "n_tiles": 12, "output_text": "The image shows a blue circular sign with white silhouettes of a person and a bicycle, indicating a pedestrian crossing area.", "error": null, "sys_before": {"cpu_percent": 46.0, "ram_used_mb": 25593.5, "ram_available_mb": 100178.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 25593.5, "ram_available_mb": 100178.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.15, "peak": 16.05, "min": 13.9}, "VDD_GPU": {"avg": 35.85, "peak": 41.36, "min": 28.76}, "VIN": {"avg": 76.75, "peak": 111.63, "min": 58.09}, "VDD_CPU_SOC_MSS": {"avg": 15.86, "peak": 16.92, "min": 14.96}}, "power_watts_avg": 35.85, "energy_joules_est": 81.58, "sample_count": 17, "duration_seconds": 2.276}, "timestamp": "2026-01-16T16:01:50.483045"}
{"image_index": 250, "image_name": "000000025593.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025593.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2268.863, "latencies_ms": [2268.863], "images_per_second": 0.441, "prompt_tokens": 23, "response_tokens_est": 22, "n_tiles": 12, "output_text": "- signpost\n- bicycle\n- person\n- tree\n- sky\n- pole\n- sign", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25593.5, "ram_available_mb": 100178.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 2.4, "ram_used_mb": 25593.5, "ram_available_mb": 100178.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.13, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 37.4, "peak": 42.94, "min": 29.15}, "VIN": {"avg": 78.74, "peak": 116.6, "min": 54.33}, "VDD_CPU_SOC_MSS": {"avg": 15.86, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 37.4, "energy_joules_est": 84.87, "sample_count": 17, "duration_seconds": 2.269}, "timestamp": "2026-01-16T16:01:52.757974"}
{"image_index": 250, "image_name": "000000025593.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025593.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3211.682, "latencies_ms": [3211.682], "images_per_second": 0.311, "prompt_tokens": 27, "response_tokens_est": 50, "n_tiles": 12, "output_text": "The main objects in the image are a blue street sign and a tree. The street sign is positioned in the foreground, with the tree partially obscuring it. The tree is located near the sign, providing a natural backdrop to the urban setting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25593.5, "ram_available_mb": 100178.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 25593.5, "ram_available_mb": 100178.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.09, "peak": 15.65, "min": 14.43}, "VDD_GPU": {"avg": 34.05, "peak": 43.33, "min": 26.39}, "VIN": {"avg": 72.25, "peak": 112.64, "min": 56.83}, "VDD_CPU_SOC_MSS": {"avg": 15.7, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 34.05, "energy_joules_est": 109.38, "sample_count": 25, "duration_seconds": 3.212}, "timestamp": "2026-01-16T16:01:55.976453"}
{"image_index": 250, "image_name": "000000025593.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025593.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3518.868, "latencies_ms": [3518.868], "images_per_second": 0.284, "prompt_tokens": 21, "response_tokens_est": 59, "n_tiles": 12, "output_text": "The image depicts a street sign with a blue background and white silhouettes of a person and a bicycle. The sign is mounted on a pole and is surrounded by green foliage. The setting appears to be outdoors during the daytime, with a clear blue sky in the background.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25593.5, "ram_available_mb": 100178.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25593.5, "ram_available_mb": 100178.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.09, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 33.39, "peak": 42.94, "min": 26.39}, "VIN": {"avg": 73.31, "peak": 112.19, "min": 58.4}, "VDD_CPU_SOC_MSS": {"avg": 15.73, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 33.39, "energy_joules_est": 117.51, "sample_count": 27, "duration_seconds": 3.519}, "timestamp": "2026-01-16T16:01:59.501570"}
{"image_index": 250, "image_name": "000000025593.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025593.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2955.269, "latencies_ms": [2955.269], "images_per_second": 0.338, "prompt_tokens": 19, "response_tokens_est": 42, "n_tiles": 12, "output_text": "The image features a blue circular sign with white silhouettes of a person and a bicycle. The sign is mounted on a pole and is surrounded by lush green foliage under a clear blue sky.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25593.5, "ram_available_mb": 100178.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 25593.5, "ram_available_mb": 100178.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.82, "peak": 15.34, "min": 14.12}, "VDD_GPU": {"avg": 34.49, "peak": 42.94, "min": 26.39}, "VIN": {"avg": 75.69, "peak": 128.08, "min": 45.09}, "VDD_CPU_SOC_MSS": {"avg": 15.37, "peak": 16.14, "min": 14.56}}, "power_watts_avg": 34.49, "energy_joules_est": 101.94, "sample_count": 23, "duration_seconds": 2.956}, "timestamp": "2026-01-16T16:02:02.465425"}
{"image_index": 251, "image_name": "000000025603.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025603.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2456.463, "latencies_ms": [2456.463], "images_per_second": 0.407, "prompt_tokens": 9, "response_tokens_est": 31, "n_tiles": 12, "output_text": "A young girl with dark hair is sitting at a table in a restaurant, eating a slice of pizza with a glass of water in front of her.", "error": null, "sys_before": {"cpu_percent": 46.1, "ram_used_mb": 25593.5, "ram_available_mb": 100178.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 25593.5, "ram_available_mb": 100178.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.04, "peak": 16.05, "min": 13.49}, "VDD_GPU": {"avg": 35.17, "peak": 41.36, "min": 27.97}, "VIN": {"avg": 75.38, "peak": 115.07, "min": 58.52}, "VDD_CPU_SOC_MSS": {"avg": 15.85, "peak": 16.92, "min": 14.57}}, "power_watts_avg": 35.17, "energy_joules_est": 86.4, "sample_count": 18, "duration_seconds": 2.457}, "timestamp": "2026-01-16T16:02:05.023885"}
{"image_index": 251, "image_name": "000000025603.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025603.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2848.424, "latencies_ms": [2848.424], "images_per_second": 0.351, "prompt_tokens": 23, "response_tokens_est": 39, "n_tiles": 12, "output_text": "1. Pizza\n2. Glass of water\n3. Napkin\n4. Glass\n5. Pieces of pizza\n6. Book\n7. Water bottle\n8. Glass", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25593.5, "ram_available_mb": 100178.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 25593.5, "ram_available_mb": 100178.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.08, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 35.09, "peak": 43.31, "min": 26.79}, "VIN": {"avg": 77.23, "peak": 116.19, "min": 58.98}, "VDD_CPU_SOC_MSS": {"avg": 15.8, "peak": 16.53, "min": 15.35}}, "power_watts_avg": 35.09, "energy_joules_est": 99.97, "sample_count": 22, "duration_seconds": 2.849}, "timestamp": "2026-01-16T16:02:07.879724"}
{"image_index": 251, "image_name": "000000025603.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025603.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 4768.576, "latencies_ms": [4768.576], "images_per_second": 0.21, "prompt_tokens": 27, "response_tokens_est": 96, "n_tiles": 12, "output_text": "The main objects in the image are a glass of water, a glass of soda, a pizza, a napkin, and a book. The pizza is on the table in the foreground, while the glass of soda and the glass of water are placed near the pizza. The napkin is placed in front of the pizza. The book is on the table near the pizza. The pizza is the most prominent object in the foreground, while the other items are arranged around it.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 25593.5, "ram_available_mb": 100178.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 25593.5, "ram_available_mb": 100178.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.11, "peak": 15.75, "min": 14.53}, "VDD_GPU": {"avg": 31.51, "peak": 43.33, "min": 26.0}, "VIN": {"avg": 74.24, "peak": 114.59, "min": 59.16}, "VDD_CPU_SOC_MSS": {"avg": 15.78, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 31.51, "energy_joules_est": 150.27, "sample_count": 38, "duration_seconds": 4.769}, "timestamp": "2026-01-16T16:02:12.654735"}
{"image_index": 251, "image_name": "000000025603.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025603.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3852.293, "latencies_ms": [3852.293], "images_per_second": 0.26, "prompt_tokens": 21, "response_tokens_est": 69, "n_tiles": 12, "output_text": "The image depicts a young girl sitting at a table in a dimly lit restaurant, possibly a pizzeria, with a pizza on the table. She is holding a glass of water and appears to be enjoying her meal. The setting is cozy and intimate, with other patrons visible in the background, creating a casual dining atmosphere.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25593.5, "ram_available_mb": 100178.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25593.5, "ram_available_mb": 100178.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.1, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 32.75, "peak": 42.54, "min": 26.39}, "VIN": {"avg": 73.08, "peak": 110.32, "min": 58.99}, "VDD_CPU_SOC_MSS": {"avg": 15.81, "peak": 16.53, "min": 15.35}}, "power_watts_avg": 32.75, "energy_joules_est": 126.19, "sample_count": 30, "duration_seconds": 3.853}, "timestamp": "2026-01-16T16:02:16.517918"}
{"image_index": 251, "image_name": "000000025603.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025603.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3348.567, "latencies_ms": [3348.567], "images_per_second": 0.299, "prompt_tokens": 19, "response_tokens_est": 54, "n_tiles": 12, "output_text": "The image depicts a young girl with dark hair and blue eyes, wearing a white sleeveless top and a gold bracelet. She is seated at a wooden table in a dimly lit restaurant, with a glass of water and a partially eaten pizza on the table.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25593.5, "ram_available_mb": 100178.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25593.5, "ram_available_mb": 100178.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.1, "peak": 15.65, "min": 14.43}, "VDD_GPU": {"avg": 33.64, "peak": 42.53, "min": 26.39}, "VIN": {"avg": 78.29, "peak": 115.93, "min": 58.47}, "VDD_CPU_SOC_MSS": {"avg": 15.75, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 33.64, "energy_joules_est": 112.66, "sample_count": 26, "duration_seconds": 3.349}, "timestamp": "2026-01-16T16:02:19.876919"}
{"image_index": 252, "image_name": "000000025986.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025986.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2794.545, "latencies_ms": [2794.545], "images_per_second": 0.358, "prompt_tokens": 9, "response_tokens_est": 41, "n_tiles": 12, "output_text": "The image shows a kitchen countertop with various food items, including a bowl of broccoli, a plate of rice and vegetables, a glass of water, and a plate with a partially eaten dish.", "error": null, "sys_before": {"cpu_percent": 25.3, "ram_used_mb": 25593.5, "ram_available_mb": 100178.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 25593.5, "ram_available_mb": 100178.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.17, "peak": 16.15, "min": 13.9}, "VDD_GPU": {"avg": 34.04, "peak": 42.15, "min": 26.79}, "VIN": {"avg": 77.97, "peak": 119.81, "min": 58.67}, "VDD_CPU_SOC_MSS": {"avg": 15.94, "peak": 17.32, "min": 14.57}}, "power_watts_avg": 34.04, "energy_joules_est": 95.16, "sample_count": 22, "duration_seconds": 2.795}, "timestamp": "2026-01-16T16:02:22.732653"}
{"image_index": 252, "image_name": "000000025986.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025986.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 5854.228, "latencies_ms": [5854.228], "images_per_second": 0.171, "prompt_tokens": 23, "response_tokens_est": 129, "n_tiles": 12, "output_text": "- plate: 1\n- bowl: 2\n- spoon: 1\n- cup: 1\n- plate: 1\n- bowl: 1\n- plate: 1\n- bowl: 1\n- plate: 1\n- bowl: 1\n- plate: 1\n- bowl: 1\n- plate: 1\n- bowl: 1\n- plate: 1\n- bowl: 1\n- plate: 1\n- bowl: 1\n- plate: 1\n- bowl: 1\n- plate: 1\n- bowl", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25593.5, "ram_available_mb": 100178.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 25593.5, "ram_available_mb": 100178.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.13, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 30.46, "peak": 42.92, "min": 26.0}, "VIN": {"avg": 71.69, "peak": 104.48, "min": 47.23}, "VDD_CPU_SOC_MSS": {"avg": 15.78, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 30.46, "energy_joules_est": 178.33, "sample_count": 46, "duration_seconds": 5.855}, "timestamp": "2026-01-16T16:02:28.593086"}
{"image_index": 252, "image_name": "000000025986.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025986.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3591.176, "latencies_ms": [3591.176], "images_per_second": 0.278, "prompt_tokens": 27, "response_tokens_est": 61, "n_tiles": 12, "output_text": "The main objects in the image are a bowl of broccoli and a plate of food. The broccoli is in the bowl, while the plate of food is in the foreground. The plate of food is positioned near the edge of the table, with a spoon resting on the bowl of broccoli.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25593.5, "ram_available_mb": 100178.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25593.5, "ram_available_mb": 100178.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.09, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 33.06, "peak": 42.53, "min": 26.39}, "VIN": {"avg": 75.15, "peak": 117.92, "min": 56.9}, "VDD_CPU_SOC_MSS": {"avg": 15.7, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 33.06, "energy_joules_est": 118.73, "sample_count": 28, "duration_seconds": 3.591}, "timestamp": "2026-01-16T16:02:32.190604"}
{"image_index": 252, "image_name": "000000025986.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025986.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3916.978, "latencies_ms": [3916.978], "images_per_second": 0.255, "prompt_tokens": 21, "response_tokens_est": 71, "n_tiles": 12, "output_text": "The image depicts a kitchen scene with a focus on a plate of food. The plate contains a mix of cooked vegetables, possibly including broccoli, and a portion of what appears to be a meat-based dish. The setting is a kitchen with a stainless steel countertop, and there are other kitchen utensils and ingredients visible in the background.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25593.5, "ram_available_mb": 100178.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25593.7, "ram_available_mb": 100178.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.1, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 32.53, "peak": 42.94, "min": 26.39}, "VIN": {"avg": 74.75, "peak": 114.8, "min": 58.89}, "VDD_CPU_SOC_MSS": {"avg": 15.7, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 32.53, "energy_joules_est": 127.44, "sample_count": 31, "duration_seconds": 3.917}, "timestamp": "2026-01-16T16:02:36.114047"}
{"image_index": 252, "image_name": "000000025986.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000025986.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3411.998, "latencies_ms": [3411.998], "images_per_second": 0.293, "prompt_tokens": 19, "response_tokens_est": 56, "n_tiles": 12, "output_text": "The image shows a kitchen counter with various food items. The counter is made of metal, and there is a clear glass of water on it. The lighting is bright, and the colors are vibrant, with the food items being a mix of green, orange, and white.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25593.7, "ram_available_mb": 100178.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 25593.5, "ram_available_mb": 100178.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.1, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 33.59, "peak": 42.94, "min": 26.39}, "VIN": {"avg": 73.44, "peak": 114.26, "min": 54.28}, "VDD_CPU_SOC_MSS": {"avg": 15.71, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 33.59, "energy_joules_est": 114.63, "sample_count": 26, "duration_seconds": 3.413}, "timestamp": "2026-01-16T16:02:39.532555"}
{"image_index": 253, "image_name": "000000026204.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000026204.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1261.413, "latencies_ms": [1261.413], "images_per_second": 0.793, "prompt_tokens": 9, "response_tokens_est": 25, "n_tiles": 6, "output_text": "A bustling city street scene with a variety of vehicles, including a bus and cars, is captured in the image.", "error": null, "sys_before": {"cpu_percent": 18.4, "ram_used_mb": 25593.5, "ram_available_mb": 100178.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25593.5, "ram_available_mb": 100178.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.41, "peak": 14.81, "min": 13.92}, "VDD_GPU": {"avg": 33.31, "peak": 37.42, "min": 27.57}, "VIN": {"avg": 78.56, "peak": 124.46, "min": 61.0}, "VDD_CPU_SOC_MSS": {"avg": 15.0, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 33.31, "energy_joules_est": 42.04, "sample_count": 9, "duration_seconds": 1.262}, "timestamp": "2026-01-16T16:02:40.833441"}
{"image_index": 253, "image_name": "000000026204.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000026204.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1488.115, "latencies_ms": [1488.115], "images_per_second": 0.672, "prompt_tokens": 23, "response_tokens_est": 32, "n_tiles": 6, "output_text": "1. Bus\n2. Car\n3. Car\n4. Car\n5. Car\n6. Car\n7. Car\n8. Car", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25593.5, "ram_available_mb": 100178.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 25593.5, "ram_available_mb": 100178.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.73, "peak": 15.11, "min": 14.22}, "VDD_GPU": {"avg": 32.76, "peak": 38.98, "min": 26.0}, "VIN": {"avg": 76.35, "peak": 114.32, "min": 56.85}, "VDD_CPU_SOC_MSS": {"avg": 15.35, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 32.76, "energy_joules_est": 48.77, "sample_count": 11, "duration_seconds": 1.489}, "timestamp": "2026-01-16T16:02:42.327471"}
{"image_index": 253, "image_name": "000000026204.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000026204.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2825.484, "latencies_ms": [2825.484], "images_per_second": 0.354, "prompt_tokens": 27, "response_tokens_est": 82, "n_tiles": 6, "output_text": "The main objects in the image are a bus, a car, and a building. The bus is in the foreground, slightly to the right, and is moving towards the left side of the frame. The car is in the foreground, closer to the viewer, and is positioned in the center of the image. The building is in the background, to the right, and is taller than the other objects.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25593.5, "ram_available_mb": 100178.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 25593.5, "ram_available_mb": 100178.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.01, "peak": 15.32, "min": 14.32}, "VDD_GPU": {"avg": 28.5, "peak": 38.59, "min": 23.64}, "VIN": {"avg": 71.59, "peak": 99.27, "min": 62.94}, "VDD_CPU_SOC_MSS": {"avg": 15.55, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 28.5, "energy_joules_est": 80.53, "sample_count": 22, "duration_seconds": 2.826}, "timestamp": "2026-01-16T16:02:45.159223"}
{"image_index": 253, "image_name": "000000026204.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000026204.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3066.159, "latencies_ms": [3066.159], "images_per_second": 0.326, "prompt_tokens": 21, "response_tokens_est": 91, "n_tiles": 6, "output_text": "The image depicts a bustling urban street scene, likely in a city with a mix of commercial and residential buildings. The street is busy with various vehicles, including a white bus with the number 1, a silver sedan, and several cars parked along the curb. Pedestrians can be seen walking on the sidewalks, and there are street signs indicating directions and traffic rules. The overall atmosphere is typical of a busy city street during daytime.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25593.5, "ram_available_mb": 100178.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 25593.5, "ram_available_mb": 100178.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.04, "peak": 15.32, "min": 14.32}, "VDD_GPU": {"avg": 27.99, "peak": 38.21, "min": 23.64}, "VIN": {"avg": 68.86, "peak": 106.47, "min": 56.06}, "VDD_CPU_SOC_MSS": {"avg": 15.63, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 27.99, "energy_joules_est": 85.83, "sample_count": 23, "duration_seconds": 3.067}, "timestamp": "2026-01-16T16:02:48.231393"}
{"image_index": 253, "image_name": "000000026204.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000026204.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2740.991, "latencies_ms": [2740.991], "images_per_second": 0.365, "prompt_tokens": 19, "response_tokens_est": 79, "n_tiles": 6, "output_text": "The image depicts a bustling urban street scene with a mix of vehicles, pedestrians, and buildings. Notable visual attributes include the vibrant red car in the foreground, the white bus with the number \"1\" on it, and the clear, bright lighting that illuminates the scene. The weather appears to be clear and sunny, contributing to the overall lively atmosphere of the city street.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25593.5, "ram_available_mb": 100178.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 25593.5, "ram_available_mb": 100178.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.99, "peak": 15.32, "min": 14.32}, "VDD_GPU": {"avg": 28.43, "peak": 38.19, "min": 23.64}, "VIN": {"avg": 68.82, "peak": 98.64, "min": 56.21}, "VDD_CPU_SOC_MSS": {"avg": 15.54, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 28.43, "energy_joules_est": 77.94, "sample_count": 21, "duration_seconds": 2.741}, "timestamp": "2026-01-16T16:02:50.978634"}
{"image_index": 254, "image_name": "000000026465.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000026465.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2554.624, "latencies_ms": [2554.624], "images_per_second": 0.391, "prompt_tokens": 9, "response_tokens_est": 34, "n_tiles": 12, "output_text": "The image shows a laptop with a black keyboard and a screen displaying a desktop interface, alongside a smartphone and a small electronic device, all resting on a white surface.", "error": null, "sys_before": {"cpu_percent": 24.1, "ram_used_mb": 25593.5, "ram_available_mb": 100178.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 25593.4, "ram_available_mb": 100178.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.23, "peak": 16.15, "min": 14.4}, "VDD_GPU": {"avg": 34.72, "peak": 41.76, "min": 27.57}, "VIN": {"avg": 75.72, "peak": 109.08, "min": 58.63}, "VDD_CPU_SOC_MSS": {"avg": 16.1, "peak": 17.32, "min": 15.35}}, "power_watts_avg": 34.72, "energy_joules_est": 88.72, "sample_count": 19, "duration_seconds": 2.555}, "timestamp": "2026-01-16T16:02:53.579810"}
{"image_index": 254, "image_name": "000000026465.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000026465.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2346.933, "latencies_ms": [2346.933], "images_per_second": 0.426, "prompt_tokens": 23, "response_tokens_est": 24, "n_tiles": 12, "output_text": "- laptop: 1\n- smartphone: 1\n- calculator: 1\n- notebook: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25593.4, "ram_available_mb": 100178.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 2.5, "ram_used_mb": 25593.4, "ram_available_mb": 100178.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.08, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 36.63, "peak": 42.94, "min": 28.36}, "VIN": {"avg": 76.0, "peak": 118.26, "min": 56.51}, "VDD_CPU_SOC_MSS": {"avg": 15.79, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 36.63, "energy_joules_est": 85.99, "sample_count": 18, "duration_seconds": 2.347}, "timestamp": "2026-01-16T16:02:55.933134"}
{"image_index": 254, "image_name": "000000026465.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000026465.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3793.843, "latencies_ms": [3793.843], "images_per_second": 0.264, "prompt_tokens": 27, "response_tokens_est": 67, "n_tiles": 12, "output_text": "The main objects in the image are a laptop, a smartphone, and a small electronic device. The laptop is positioned in the foreground, with the smartphone and the small electronic device placed near it. The smartphone is slightly to the right of the laptop, while the small electronic device is closer to the bottom left corner of the image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25593.4, "ram_available_mb": 100178.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25592.7, "ram_available_mb": 100179.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.07, "peak": 15.65, "min": 14.43}, "VDD_GPU": {"avg": 33.11, "peak": 43.31, "min": 26.39}, "VIN": {"avg": 73.1, "peak": 114.5, "min": 58.26}, "VDD_CPU_SOC_MSS": {"avg": 15.77, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 33.11, "energy_joules_est": 125.63, "sample_count": 29, "duration_seconds": 3.794}, "timestamp": "2026-01-16T16:02:59.733402"}
{"image_index": 254, "image_name": "000000026465.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000026465.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 4137.291, "latencies_ms": [4137.291], "images_per_second": 0.242, "prompt_tokens": 21, "response_tokens_est": 77, "n_tiles": 12, "output_text": "The image shows a cluttered desk with various items scattered around. The desk appears to be in a room with a white wall and a window with a partially visible view outside. The items on the desk include a laptop, a smartphone, a small black object that could be a remote or a phone, and a black object that might be a remote control or a small electronic device.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25592.7, "ram_available_mb": 100179.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 25592.7, "ram_available_mb": 100179.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.83, "peak": 15.34, "min": 14.22}, "VDD_GPU": {"avg": 31.91, "peak": 41.74, "min": 26.0}, "VIN": {"avg": 72.63, "peak": 118.83, "min": 60.36}, "VDD_CPU_SOC_MSS": {"avg": 15.46, "peak": 16.14, "min": 14.56}}, "power_watts_avg": 31.91, "energy_joules_est": 132.04, "sample_count": 32, "duration_seconds": 4.138}, "timestamp": "2026-01-16T16:03:03.877331"}
{"image_index": 254, "image_name": "000000026465.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000026465.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2916.73, "latencies_ms": [2916.73], "images_per_second": 0.343, "prompt_tokens": 19, "response_tokens_est": 41, "n_tiles": 12, "output_text": "The image shows a laptop with a black keyboard and a silver or grey screen. The laptop is placed on a white surface, and there is a reflection of a person's hand visible on the screen.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25592.7, "ram_available_mb": 100179.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 25592.9, "ram_available_mb": 100179.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.78, "peak": 15.34, "min": 14.12}, "VDD_GPU": {"avg": 34.75, "peak": 42.53, "min": 26.39}, "VIN": {"avg": 78.13, "peak": 118.24, "min": 58.42}, "VDD_CPU_SOC_MSS": {"avg": 15.35, "peak": 16.14, "min": 14.56}}, "power_watts_avg": 34.75, "energy_joules_est": 101.37, "sample_count": 22, "duration_seconds": 2.917}, "timestamp": "2026-01-16T16:03:06.800491"}
{"image_index": 255, "image_name": "000000026564.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000026564.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1794.981, "latencies_ms": [1794.981], "images_per_second": 0.557, "prompt_tokens": 9, "response_tokens_est": 45, "n_tiles": 6, "output_text": "The image shows a well-organized workspace with a desktop computer, a keyboard, a mouse, and a laptop, all placed on a desk with various items such as books, a water bottle, and a pen.", "error": null, "sys_before": {"cpu_percent": 18.7, "ram_used_mb": 25592.9, "ram_available_mb": 100179.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 25593.2, "ram_available_mb": 100179.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.56, "peak": 15.11, "min": 13.82}, "VDD_GPU": {"avg": 30.85, "peak": 37.82, "min": 24.42}, "VIN": {"avg": 65.96, "peak": 101.17, "min": 54.98}, "VDD_CPU_SOC_MSS": {"avg": 15.2, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 30.85, "energy_joules_est": 55.38, "sample_count": 13, "duration_seconds": 1.795}, "timestamp": "2026-01-16T16:03:08.636044"}
{"image_index": 255, "image_name": "000000026564.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000026564.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1807.24, "latencies_ms": [1807.24], "images_per_second": 0.553, "prompt_tokens": 23, "response_tokens_est": 44, "n_tiles": 6, "output_text": "- computer monitor: 1\n- keyboard: 1\n- mouse: 1\n- laptop: 1\n- book: 5\n- pen: 1\n- water bottle: 1", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25593.2, "ram_available_mb": 100179.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 25593.2, "ram_available_mb": 100179.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.84, "peak": 15.22, "min": 14.32}, "VDD_GPU": {"avg": 31.27, "peak": 38.21, "min": 24.82}, "VIN": {"avg": 65.68, "peak": 87.84, "min": 53.69}, "VDD_CPU_SOC_MSS": {"avg": 15.38, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 31.27, "energy_joules_est": 56.53, "sample_count": 13, "duration_seconds": 1.808}, "timestamp": "2026-01-16T16:03:10.449442"}
{"image_index": 255, "image_name": "000000026564.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000026564.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2718.548, "latencies_ms": [2718.548], "images_per_second": 0.368, "prompt_tokens": 27, "response_tokens_est": 78, "n_tiles": 6, "output_text": "The main objects in the image are a desk with various items on it, including a laptop, a keyboard, a mouse, and a water bottle. The laptop is positioned in the background, slightly to the right, while the keyboard and mouse are in the foreground, closer to the viewer. The water bottle is placed near the keyboard, indicating that it is easily accessible for the user.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25593.2, "ram_available_mb": 100179.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 25592.9, "ram_available_mb": 100179.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.02, "peak": 15.32, "min": 14.32}, "VDD_GPU": {"avg": 28.59, "peak": 38.19, "min": 23.64}, "VIN": {"avg": 67.7, "peak": 100.0, "min": 55.53}, "VDD_CPU_SOC_MSS": {"avg": 15.62, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 28.59, "energy_joules_est": 77.73, "sample_count": 21, "duration_seconds": 2.719}, "timestamp": "2026-01-16T16:03:13.173870"}
{"image_index": 255, "image_name": "000000026564.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000026564.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3093.444, "latencies_ms": [3093.444], "images_per_second": 0.323, "prompt_tokens": 21, "response_tokens_est": 92, "n_tiles": 6, "output_text": "The image depicts a well-organized workspace with a desk set up for a computer. The desk is equipped with a laptop, a keyboard, a mouse, and a cup of coffee. The workspace is brightly lit by natural light coming through a window, and there are various items scattered around, including books, a pen, and a small stuffed animal. The overall setting suggests a comfortable and functional workspace, likely in a home or office environment.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25592.9, "ram_available_mb": 100179.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 25592.9, "ram_available_mb": 100179.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.05, "peak": 15.32, "min": 14.32}, "VDD_GPU": {"avg": 27.87, "peak": 37.82, "min": 23.64}, "VIN": {"avg": 66.56, "peak": 94.93, "min": 57.03}, "VDD_CPU_SOC_MSS": {"avg": 15.57, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 27.87, "energy_joules_est": 86.23, "sample_count": 24, "duration_seconds": 3.094}, "timestamp": "2026-01-16T16:03:16.273244"}
{"image_index": 255, "image_name": "000000026564.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000026564.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2261.221, "latencies_ms": [2261.221], "images_per_second": 0.442, "prompt_tokens": 19, "response_tokens_est": 61, "n_tiles": 6, "output_text": "The image depicts a well-organized workspace with a light-colored desk. The desk is equipped with a laptop, a keyboard, a mouse, and a water bottle. The lighting is bright, likely from natural light coming through a window, and the overall atmosphere appears to be calm and tidy.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25592.9, "ram_available_mb": 100179.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 25592.9, "ram_available_mb": 100179.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.93, "peak": 15.32, "min": 14.32}, "VDD_GPU": {"avg": 29.38, "peak": 37.8, "min": 24.03}, "VIN": {"avg": 67.84, "peak": 111.57, "min": 57.85}, "VDD_CPU_SOC_MSS": {"avg": 15.47, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 29.38, "energy_joules_est": 66.45, "sample_count": 17, "duration_seconds": 2.262}, "timestamp": "2026-01-16T16:03:18.540926"}
{"image_index": 256, "image_name": "000000026690.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000026690.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1177.035, "latencies_ms": [1177.035], "images_per_second": 0.85, "prompt_tokens": 9, "response_tokens_est": 22, "n_tiles": 6, "output_text": "A skateboarder is performing a trick in an indoor arena, with spectators watching from the stands.", "error": null, "sys_before": {"cpu_percent": 38.6, "ram_used_mb": 25592.9, "ram_available_mb": 100179.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 25592.9, "ram_available_mb": 100179.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.44, "peak": 14.81, "min": 13.92}, "VDD_GPU": {"avg": 32.61, "peak": 37.03, "min": 27.18}, "VIN": {"avg": 61.64, "peak": 75.27, "min": 50.61}, "VDD_CPU_SOC_MSS": {"avg": 15.18, "peak": 15.36, "min": 14.57}}, "power_watts_avg": 32.61, "energy_joules_est": 38.4, "sample_count": 9, "duration_seconds": 1.177}, "timestamp": "2026-01-16T16:03:19.765005"}
{"image_index": 256, "image_name": "000000026690.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000026690.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2366.255, "latencies_ms": [2366.255], "images_per_second": 0.423, "prompt_tokens": 23, "response_tokens_est": 65, "n_tiles": 6, "output_text": "1. Skateboarder\n2. Skateboard\n3. Skateboarder helmet\n4. Skateboarder gloves\n5. Skateboarder knee pads\n6. Skateboarder wrist guards\n7. Skateboarder wrist guards\n8. Skateboarder wrist guards", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25592.9, "ram_available_mb": 100179.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 25592.9, "ram_available_mb": 100179.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.89, "peak": 15.22, "min": 14.22}, "VDD_GPU": {"avg": 29.69, "peak": 38.98, "min": 24.03}, "VIN": {"avg": 72.09, "peak": 110.16, "min": 61.64}, "VDD_CPU_SOC_MSS": {"avg": 15.53, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 29.69, "energy_joules_est": 70.27, "sample_count": 18, "duration_seconds": 2.367}, "timestamp": "2026-01-16T16:03:22.137541"}
{"image_index": 256, "image_name": "000000026690.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000026690.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3281.263, "latencies_ms": [3281.263], "images_per_second": 0.305, "prompt_tokens": 27, "response_tokens_est": 99, "n_tiles": 6, "output_text": "The main object in the foreground is a skateboarder performing a trick in mid-air. The skateboarder is wearing a black shirt and gray shorts, and is holding onto the skateboard with both hands. The skateboarder is positioned near the center of the image, with the audience in the background. The audience is seated on a tiered seating area, with some individuals standing and others sitting. The skateboarder is near the edge of the image, closer to the foreground.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25592.9, "ram_available_mb": 100179.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 25592.9, "ram_available_mb": 100179.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.07, "peak": 15.32, "min": 14.32}, "VDD_GPU": {"avg": 27.59, "peak": 38.21, "min": 23.64}, "VIN": {"avg": 69.33, "peak": 102.34, "min": 62.42}, "VDD_CPU_SOC_MSS": {"avg": 15.61, "peak": 16.14, "min": 14.96}}, "power_watts_avg": 27.59, "energy_joules_est": 90.54, "sample_count": 26, "duration_seconds": 3.282}, "timestamp": "2026-01-16T16:03:25.424704"}
{"image_index": 256, "image_name": "000000026690.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000026690.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2599.863, "latencies_ms": [2599.863], "images_per_second": 0.385, "prompt_tokens": 21, "response_tokens_est": 74, "n_tiles": 6, "output_text": "The image captures a dynamic moment at an indoor skateboarding event, likely a competition or exhibition. A skateboarder is in mid-air, performing a trick, while a crowd of spectators watches from the bleachers. The setting is an indoor arena with a high ceiling, and the atmosphere is energetic, with the skateboarder's performance being the focal point.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25592.9, "ram_available_mb": 100179.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 25592.9, "ram_available_mb": 100179.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.01, "peak": 15.32, "min": 14.32}, "VDD_GPU": {"avg": 28.64, "peak": 37.8, "min": 23.64}, "VIN": {"avg": 69.6, "peak": 120.84, "min": 60.25}, "VDD_CPU_SOC_MSS": {"avg": 15.61, "peak": 16.14, "min": 14.96}}, "power_watts_avg": 28.64, "energy_joules_est": 74.47, "sample_count": 20, "duration_seconds": 2.6}, "timestamp": "2026-01-16T16:03:28.030610"}
{"image_index": 256, "image_name": "000000026690.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000026690.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2015.758, "latencies_ms": [2015.758], "images_per_second": 0.496, "prompt_tokens": 19, "response_tokens_est": 52, "n_tiles": 6, "output_text": "The image depicts a vibrant and dynamic scene in an indoor skate park. The lighting is bright, highlighting the action and the participants. The skateboarder is wearing a black helmet and a black shirt, while the audience is dressed in casual attire.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25592.9, "ram_available_mb": 100179.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 25592.9, "ram_available_mb": 100179.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.91, "peak": 15.22, "min": 14.32}, "VDD_GPU": {"avg": 30.17, "peak": 38.21, "min": 24.03}, "VIN": {"avg": 72.09, "peak": 111.54, "min": 61.63}, "VDD_CPU_SOC_MSS": {"avg": 15.56, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 30.17, "energy_joules_est": 60.84, "sample_count": 15, "duration_seconds": 2.017}, "timestamp": "2026-01-16T16:03:30.052779"}
{"image_index": 257, "image_name": "000000026926.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000026926.jpg", "image_width": 423, "image_height": 640, "image_resolution": "423x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1278.855, "latencies_ms": [1278.855], "images_per_second": 0.782, "prompt_tokens": 9, "response_tokens_est": 26, "n_tiles": 6, "output_text": "The image shows a red fire hydrant with a black face painted on it, standing on a sidewalk next to a street.", "error": null, "sys_before": {"cpu_percent": 47.1, "ram_used_mb": 25592.9, "ram_available_mb": 100179.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 25593.2, "ram_available_mb": 100179.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.45, "peak": 14.81, "min": 13.92}, "VDD_GPU": {"avg": 32.56, "peak": 37.01, "min": 27.18}, "VIN": {"avg": 75.2, "peak": 120.83, "min": 53.4}, "VDD_CPU_SOC_MSS": {"avg": 15.22, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 32.56, "energy_joules_est": 41.66, "sample_count": 9, "duration_seconds": 1.28}, "timestamp": "2026-01-16T16:03:31.385646"}
{"image_index": 257, "image_name": "000000026926.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000026926.jpg", "image_width": 423, "image_height": 640, "image_resolution": "423x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1704.472, "latencies_ms": [1704.472], "images_per_second": 0.587, "prompt_tokens": 23, "response_tokens_est": 40, "n_tiles": 6, "output_text": "object: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25593.2, "ram_available_mb": 100179.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 25592.9, "ram_available_mb": 100179.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.78, "peak": 15.11, "min": 14.22}, "VDD_GPU": {"avg": 31.33, "peak": 38.6, "min": 24.82}, "VIN": {"avg": 68.3, "peak": 114.39, "min": 55.66}, "VDD_CPU_SOC_MSS": {"avg": 15.41, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 31.33, "energy_joules_est": 53.41, "sample_count": 13, "duration_seconds": 1.705}, "timestamp": "2026-01-16T16:03:33.096304"}
{"image_index": 257, "image_name": "000000026926.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000026926.jpg", "image_width": 423, "image_height": 640, "image_resolution": "423x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2440.195, "latencies_ms": [2440.195], "images_per_second": 0.41, "prompt_tokens": 27, "response_tokens_est": 65, "n_tiles": 6, "output_text": "The main object in the foreground is a red fire hydrant with a black handle and a black cap. The fire hydrant is positioned on the right side of the image, near the sidewalk. In the background, there are trees and a few buildings, indicating that the fire hydrant is situated in an urban environment.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25592.9, "ram_available_mb": 100179.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 25592.9, "ram_available_mb": 100179.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.78, "peak": 15.01, "min": 14.12}, "VDD_GPU": {"avg": 28.8, "peak": 38.59, "min": 23.24}, "VIN": {"avg": 68.59, "peak": 108.48, "min": 57.12}, "VDD_CPU_SOC_MSS": {"avg": 15.39, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 28.8, "energy_joules_est": 70.29, "sample_count": 19, "duration_seconds": 2.44}, "timestamp": "2026-01-16T16:03:35.542408"}
{"image_index": 257, "image_name": "000000026926.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000026926.jpg", "image_width": 423, "image_height": 640, "image_resolution": "423x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2353.802, "latencies_ms": [2353.802], "images_per_second": 0.425, "prompt_tokens": 21, "response_tokens_est": 62, "n_tiles": 6, "output_text": "The image depicts a vibrant red fire hydrant situated on a city street. The hydrant is painted with a black smiley face, adding a playful and cheerful element to the urban scene. The background shows a street lined with buildings, trees, and parked cars, indicating a typical city environment.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25592.9, "ram_available_mb": 100179.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 25592.9, "ram_available_mb": 100179.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.73, "peak": 15.01, "min": 14.02}, "VDD_GPU": {"avg": 28.89, "peak": 38.19, "min": 23.24}, "VIN": {"avg": 69.8, "peak": 116.37, "min": 59.68}, "VDD_CPU_SOC_MSS": {"avg": 15.33, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 28.89, "energy_joules_est": 68.02, "sample_count": 18, "duration_seconds": 2.354}, "timestamp": "2026-01-16T16:03:37.902702"}
{"image_index": 257, "image_name": "000000026926.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000026926.jpg", "image_width": 423, "image_height": 640, "image_resolution": "423x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2629.839, "latencies_ms": [2629.839], "images_per_second": 0.38, "prompt_tokens": 19, "response_tokens_est": 72, "n_tiles": 6, "output_text": "The notable visual attributes of the image include a vivid red fire hydrant with a slightly weathered and rusty appearance, featuring a black cap and a black handle. The lighting is natural, with sunlight casting shadows on the hydrant, indicating a sunny day. The materials used are metal for the hydrant and a concrete or asphalt surface for the sidewalk.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25592.9, "ram_available_mb": 100179.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 25592.9, "ram_available_mb": 100179.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.79, "peak": 15.11, "min": 14.12}, "VDD_GPU": {"avg": 28.28, "peak": 38.19, "min": 23.24}, "VIN": {"avg": 72.17, "peak": 111.59, "min": 57.14}, "VDD_CPU_SOC_MSS": {"avg": 15.33, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 28.28, "energy_joules_est": 74.38, "sample_count": 20, "duration_seconds": 2.63}, "timestamp": "2026-01-16T16:03:40.542798"}
{"image_index": 258, "image_name": "000000026941.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000026941.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1468.345, "latencies_ms": [1468.345], "images_per_second": 0.681, "prompt_tokens": 9, "response_tokens_est": 33, "n_tiles": 6, "output_text": "The image shows a collection of old, worn suitcases stacked on top of each other, with one suitcase in the foreground being the largest and most prominent.", "error": null, "sys_before": {"cpu_percent": 40.3, "ram_used_mb": 25592.9, "ram_available_mb": 100179.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 25593.1, "ram_available_mb": 100179.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.43, "peak": 15.01, "min": 13.82}, "VDD_GPU": {"avg": 31.08, "peak": 37.03, "min": 25.6}, "VIN": {"avg": 70.9, "peak": 108.87, "min": 52.11}, "VDD_CPU_SOC_MSS": {"avg": 15.18, "peak": 15.75, "min": 14.57}}, "power_watts_avg": 31.08, "energy_joules_est": 45.65, "sample_count": 11, "duration_seconds": 1.469}, "timestamp": "2026-01-16T16:03:42.071856"}
{"image_index": 258, "image_name": "000000026941.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000026941.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2326.919, "latencies_ms": [2326.919], "images_per_second": 0.43, "prompt_tokens": 23, "response_tokens_est": 61, "n_tiles": 6, "output_text": "1. Green cart\n2. 2 green suitcases\n3. 2 brown suitcases\n4. 2 blue suitcases\n5. 2 green suitcases\n6. 2 brown suitcases\n7. 2 blue suitcases\n8. 2 green suitcases", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25593.1, "ram_available_mb": 100179.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 25593.1, "ram_available_mb": 100179.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.73, "peak": 15.01, "min": 14.12}, "VDD_GPU": {"avg": 29.27, "peak": 39.38, "min": 23.24}, "VIN": {"avg": 71.51, "peak": 115.62, "min": 61.9}, "VDD_CPU_SOC_MSS": {"avg": 15.28, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 29.27, "energy_joules_est": 68.12, "sample_count": 17, "duration_seconds": 2.327}, "timestamp": "2026-01-16T16:03:44.404818"}
{"image_index": 258, "image_name": "000000026941.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000026941.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3025.204, "latencies_ms": [3025.204], "images_per_second": 0.331, "prompt_tokens": 27, "response_tokens_est": 86, "n_tiles": 6, "output_text": "The main objects in the image are a collection of old suitcases stacked on top of each other. The suitcases are positioned in the foreground, with the largest one at the bottom and the smallest one at the top. The green cart is situated to the left of the suitcases, and the green door is to the right of the suitcases. The background features a green door and a poster with a child on it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25593.1, "ram_available_mb": 100179.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 25593.1, "ram_available_mb": 100179.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.81, "peak": 15.11, "min": 14.12}, "VDD_GPU": {"avg": 27.63, "peak": 38.21, "min": 23.24}, "VIN": {"avg": 69.44, "peak": 118.05, "min": 61.24}, "VDD_CPU_SOC_MSS": {"avg": 15.37, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 27.63, "energy_joules_est": 83.6, "sample_count": 23, "duration_seconds": 3.026}, "timestamp": "2026-01-16T16:03:47.435904"}
{"image_index": 258, "image_name": "000000026941.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000026941.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2688.865, "latencies_ms": [2688.865], "images_per_second": 0.372, "prompt_tokens": 21, "response_tokens_est": 74, "n_tiles": 6, "output_text": "The image depicts a collection of old, weathered suitcases stacked on top of each other on a green cart. The suitcases appear to be from different eras, with varying colors and designs, suggesting they have been used for travel. The setting appears to be outdoors, possibly in a storage area or a place where old luggage is being stored or displayed.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25593.1, "ram_available_mb": 100179.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 25593.1, "ram_available_mb": 100179.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.8, "peak": 15.01, "min": 14.12}, "VDD_GPU": {"avg": 28.02, "peak": 38.19, "min": 23.24}, "VIN": {"avg": 68.87, "peak": 114.88, "min": 56.2}, "VDD_CPU_SOC_MSS": {"avg": 15.37, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 28.02, "energy_joules_est": 75.35, "sample_count": 21, "duration_seconds": 2.689}, "timestamp": "2026-01-16T16:03:50.131034"}
{"image_index": 258, "image_name": "000000026941.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000026941.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2441.129, "latencies_ms": [2441.129], "images_per_second": 0.41, "prompt_tokens": 19, "response_tokens_est": 65, "n_tiles": 6, "output_text": "The image features a collection of vintage suitcases stacked on top of each other, with a green cart in the foreground. The suitcases are made of various materials, including metal and fabric, and exhibit signs of wear and age. The lighting is natural, suggesting daytime, and the weather appears to be clear and sunny.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25593.1, "ram_available_mb": 100179.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 25593.1, "ram_available_mb": 100179.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.76, "peak": 15.11, "min": 14.02}, "VDD_GPU": {"avg": 28.61, "peak": 38.21, "min": 23.24}, "VIN": {"avg": 69.41, "peak": 91.5, "min": 62.1}, "VDD_CPU_SOC_MSS": {"avg": 15.29, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 28.61, "energy_joules_est": 69.85, "sample_count": 19, "duration_seconds": 2.442}, "timestamp": "2026-01-16T16:03:52.578681"}
{"image_index": 259, "image_name": "000000027186.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000027186.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1279.245, "latencies_ms": [1279.245], "images_per_second": 0.782, "prompt_tokens": 9, "response_tokens_est": 26, "n_tiles": 6, "output_text": "A young girl is sitting on a couch, wearing a pink dress with a floral pattern, and holding a white remote control.", "error": null, "sys_before": {"cpu_percent": 42.3, "ram_used_mb": 25593.1, "ram_available_mb": 100179.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 25593.1, "ram_available_mb": 100179.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.34, "peak": 14.81, "min": 13.92}, "VDD_GPU": {"avg": 32.34, "peak": 37.42, "min": 26.79}, "VIN": {"avg": 68.99, "peak": 97.61, "min": 53.81}, "VDD_CPU_SOC_MSS": {"avg": 15.05, "peak": 15.36, "min": 14.57}}, "power_watts_avg": 32.34, "energy_joules_est": 41.39, "sample_count": 9, "duration_seconds": 1.28}, "timestamp": "2026-01-16T16:03:53.917687"}
{"image_index": 259, "image_name": "000000027186.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000027186.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1702.39, "latencies_ms": [1702.39], "images_per_second": 0.587, "prompt_tokens": 23, "response_tokens_est": 40, "n_tiles": 6, "output_text": "object: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25593.1, "ram_available_mb": 100179.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 25593.1, "ram_available_mb": 100179.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.77, "peak": 15.11, "min": 14.22}, "VDD_GPU": {"avg": 31.66, "peak": 38.6, "min": 24.82}, "VIN": {"avg": 70.68, "peak": 91.83, "min": 57.53}, "VDD_CPU_SOC_MSS": {"avg": 15.35, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 31.66, "energy_joules_est": 53.91, "sample_count": 13, "duration_seconds": 1.703}, "timestamp": "2026-01-16T16:03:55.626472"}
{"image_index": 259, "image_name": "000000027186.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000027186.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2531.717, "latencies_ms": [2531.717], "images_per_second": 0.395, "prompt_tokens": 27, "response_tokens_est": 71, "n_tiles": 6, "output_text": "The main object in the foreground is a young girl wearing a pink dress with a floral pattern. She is holding a white remote control in her right hand. The background features a window with white blinds, partially drawn, allowing natural light to enter the room. The couch is positioned to the right of the girl, with a brown pillow on it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25593.1, "ram_available_mb": 100179.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 25593.4, "ram_available_mb": 100178.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.96, "peak": 15.22, "min": 14.32}, "VDD_GPU": {"avg": 28.97, "peak": 38.21, "min": 23.64}, "VIN": {"avg": 65.33, "peak": 89.08, "min": 56.24}, "VDD_CPU_SOC_MSS": {"avg": 15.52, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 28.97, "energy_joules_est": 73.36, "sample_count": 19, "duration_seconds": 2.532}, "timestamp": "2026-01-16T16:03:58.164208"}
{"image_index": 259, "image_name": "000000027186.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000027186.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2450.806, "latencies_ms": [2450.806], "images_per_second": 0.408, "prompt_tokens": 21, "response_tokens_est": 67, "n_tiles": 6, "output_text": "The image depicts a young girl sitting on a couch in a cozy living room. She is holding a white remote control and appears to be watching something on the screen, possibly a video game. The room has a warm ambiance with blinds partially drawn, allowing natural light to filter in, creating a comfortable and inviting atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25593.4, "ram_available_mb": 100178.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 25593.1, "ram_available_mb": 100179.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.94, "peak": 15.22, "min": 14.32}, "VDD_GPU": {"avg": 28.68, "peak": 37.82, "min": 23.25}, "VIN": {"avg": 70.6, "peak": 127.32, "min": 58.55}, "VDD_CPU_SOC_MSS": {"avg": 15.56, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 28.68, "energy_joules_est": 70.3, "sample_count": 19, "duration_seconds": 2.451}, "timestamp": "2026-01-16T16:04:00.621429"}
{"image_index": 259, "image_name": "000000027186.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000027186.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2017.961, "latencies_ms": [2017.961], "images_per_second": 0.496, "prompt_tokens": 19, "response_tokens_est": 50, "n_tiles": 6, "output_text": "The image features a young girl in a pink dress with a floral pattern, sitting on a beige couch. The room has a warm, cozy ambiance with natural light coming through partially closed white blinds, creating a soft and inviting atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25593.1, "ram_available_mb": 100179.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 25593.1, "ram_available_mb": 100179.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.72, "peak": 15.01, "min": 14.12}, "VDD_GPU": {"avg": 29.83, "peak": 38.6, "min": 23.64}, "VIN": {"avg": 74.53, "peak": 104.85, "min": 61.66}, "VDD_CPU_SOC_MSS": {"avg": 15.25, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 29.83, "energy_joules_est": 60.21, "sample_count": 15, "duration_seconds": 2.018}, "timestamp": "2026-01-16T16:04:02.645623"}
{"image_index": 260, "image_name": "000000027620.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000027620.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2719.741, "latencies_ms": [2719.741], "images_per_second": 0.368, "prompt_tokens": 9, "response_tokens_est": 39, "n_tiles": 12, "output_text": "The image shows a cluttered office desk with a computer, a keyboard, a mouse, headphones, a trash can, and a chair, all arranged in a somewhat disorganized manner.", "error": null, "sys_before": {"cpu_percent": 44.0, "ram_used_mb": 25593.1, "ram_available_mb": 100179.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 25593.6, "ram_available_mb": 100178.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.14, "peak": 16.15, "min": 14.0}, "VDD_GPU": {"avg": 34.04, "peak": 40.97, "min": 26.79}, "VIN": {"avg": 76.67, "peak": 108.21, "min": 58.21}, "VDD_CPU_SOC_MSS": {"avg": 15.97, "peak": 17.32, "min": 14.96}}, "power_watts_avg": 34.04, "energy_joules_est": 92.59, "sample_count": 21, "duration_seconds": 2.72}, "timestamp": "2026-01-16T16:04:05.450982"}
{"image_index": 260, "image_name": "000000027620.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000027620.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2743.787, "latencies_ms": [2743.787], "images_per_second": 0.364, "prompt_tokens": 23, "response_tokens_est": 36, "n_tiles": 12, "output_text": "1. Desk\n2. Computer monitor\n3. Laptop\n4. Headphones\n5. Cable\n6. Keyboard\n7. Mouse\n8. Trash can", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25593.6, "ram_available_mb": 100178.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 25593.3, "ram_available_mb": 100178.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.07, "peak": 15.65, "min": 14.43}, "VDD_GPU": {"avg": 35.26, "peak": 42.15, "min": 27.18}, "VIN": {"avg": 75.13, "peak": 111.38, "min": 58.72}, "VDD_CPU_SOC_MSS": {"avg": 15.73, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 35.26, "energy_joules_est": 96.76, "sample_count": 21, "duration_seconds": 2.744}, "timestamp": "2026-01-16T16:04:08.202225"}
{"image_index": 260, "image_name": "000000027620.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000027620.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 4943.14, "latencies_ms": [4943.14], "images_per_second": 0.202, "prompt_tokens": 27, "response_tokens_est": 101, "n_tiles": 12, "output_text": "The main objects in the image are a desk, a computer, a keyboard, a mouse, headphones, a trash can, and a chair. The desk is positioned in the foreground, with the computer and keyboard placed on top of it. The keyboard is near the mouse, and the mouse is on the desk. The headphones are placed on the desk, and the trash can is positioned to the left of the chair. The chair is in the foreground, with the computer and keyboard on the desk.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25593.3, "ram_available_mb": 100178.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 25593.3, "ram_available_mb": 100178.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.1, "peak": 15.65, "min": 14.43}, "VDD_GPU": {"avg": 31.18, "peak": 42.15, "min": 26.0}, "VIN": {"avg": 70.24, "peak": 106.96, "min": 58.6}, "VDD_CPU_SOC_MSS": {"avg": 15.72, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 31.18, "energy_joules_est": 154.14, "sample_count": 39, "duration_seconds": 4.943}, "timestamp": "2026-01-16T16:04:13.152666"}
{"image_index": 260, "image_name": "000000027620.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000027620.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3856.441, "latencies_ms": [3856.441], "images_per_second": 0.259, "prompt_tokens": 21, "response_tokens_est": 69, "n_tiles": 12, "output_text": "The image depicts a cluttered office desk with various items scattered around. The desk is cluttered with a computer monitor, a keyboard, a mouse, a pair of headphones, a trash can, and other miscellaneous objects. The setting appears to be an office or workspace, with a window in the background and a radiator visible.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25593.3, "ram_available_mb": 100178.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 25593.3, "ram_available_mb": 100178.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.1, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 32.68, "peak": 42.94, "min": 26.0}, "VIN": {"avg": 79.43, "peak": 122.21, "min": 58.32}, "VDD_CPU_SOC_MSS": {"avg": 15.79, "peak": 16.53, "min": 15.35}}, "power_watts_avg": 32.68, "energy_joules_est": 126.04, "sample_count": 30, "duration_seconds": 3.857}, "timestamp": "2026-01-16T16:04:17.015577"}
{"image_index": 260, "image_name": "000000027620.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000027620.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3585.753, "latencies_ms": [3585.753], "images_per_second": 0.279, "prompt_tokens": 19, "response_tokens_est": 61, "n_tiles": 12, "output_text": "The image shows a cluttered office desk with a cluttered computer setup. The desk is cluttered with various items such as a computer monitor, keyboard, mouse, headphones, and a trash can. The lighting in the room is dim, and the overall atmosphere appears to be somewhat disorganized.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25593.3, "ram_available_mb": 100178.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 25593.5, "ram_available_mb": 100178.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.08, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 33.23, "peak": 42.94, "min": 26.39}, "VIN": {"avg": 76.43, "peak": 115.92, "min": 58.51}, "VDD_CPU_SOC_MSS": {"avg": 15.8, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 33.23, "energy_joules_est": 119.17, "sample_count": 27, "duration_seconds": 3.586}, "timestamp": "2026-01-16T16:04:20.607661"}
{"image_index": 261, "image_name": "000000027696.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000027696.jpg", "image_width": 640, "image_height": 410, "image_resolution": "640x410", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1788.823, "latencies_ms": [1788.823], "images_per_second": 0.559, "prompt_tokens": 9, "response_tokens_est": 45, "n_tiles": 6, "output_text": "The image shows a freshly baked pizza with a variety of toppings, including melted cheese, sliced green peppers, and chunks of mushrooms, all resting on a white plate with a red and white checkered tablecloth.", "error": null, "sys_before": {"cpu_percent": 35.3, "ram_used_mb": 25593.5, "ram_available_mb": 100178.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 25593.5, "ram_available_mb": 100178.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.59, "peak": 15.22, "min": 13.92}, "VDD_GPU": {"avg": 30.76, "peak": 38.21, "min": 24.42}, "VIN": {"avg": 76.4, "peak": 128.66, "min": 58.41}, "VDD_CPU_SOC_MSS": {"avg": 15.35, "peak": 15.75, "min": 14.57}}, "power_watts_avg": 30.76, "energy_joules_est": 55.03, "sample_count": 13, "duration_seconds": 1.789}, "timestamp": "2026-01-16T16:04:22.450868"}
{"image_index": 261, "image_name": "000000027696.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000027696.jpg", "image_width": 640, "image_height": 410, "image_resolution": "640x410", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1594.04, "latencies_ms": [1594.04], "images_per_second": 0.627, "prompt_tokens": 23, "response_tokens_est": 36, "n_tiles": 6, "output_text": "1. Pizza\n2. Pork\n3. Mushrooms\n4. Pepperoni\n5. Tomato\n6. Cheese\n7. Sauce\n8. Plate", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25593.5, "ram_available_mb": 100178.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 25593.3, "ram_available_mb": 100178.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.81, "peak": 15.11, "min": 14.32}, "VDD_GPU": {"avg": 31.84, "peak": 38.21, "min": 25.21}, "VIN": {"avg": 76.64, "peak": 116.11, "min": 58.02}, "VDD_CPU_SOC_MSS": {"avg": 15.39, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 31.84, "energy_joules_est": 50.77, "sample_count": 12, "duration_seconds": 1.595}, "timestamp": "2026-01-16T16:04:24.051057"}
{"image_index": 261, "image_name": "000000027696.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000027696.jpg", "image_width": 640, "image_height": 410, "image_resolution": "640x410", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2603.448, "latencies_ms": [2603.448], "images_per_second": 0.384, "prompt_tokens": 27, "response_tokens_est": 71, "n_tiles": 6, "output_text": "The main object in the foreground is a freshly baked pizza with toppings such as pepperoni, mushrooms, and red bell peppers. The pizza is placed on a white plate, which is placed on a red and white checkered tablecloth. In the background, there is a fork partially visible, suggesting that someone is about to eat the pizza.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25593.3, "ram_available_mb": 100178.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 25593.3, "ram_available_mb": 100178.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.79, "peak": 15.01, "min": 14.12}, "VDD_GPU": {"avg": 28.52, "peak": 38.98, "min": 23.24}, "VIN": {"avg": 66.09, "peak": 83.6, "min": 55.68}, "VDD_CPU_SOC_MSS": {"avg": 15.43, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 28.52, "energy_joules_est": 74.26, "sample_count": 20, "duration_seconds": 2.604}, "timestamp": "2026-01-16T16:04:26.662695"}
{"image_index": 261, "image_name": "000000027696.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000027696.jpg", "image_width": 640, "image_height": 410, "image_resolution": "640x410", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2376.982, "latencies_ms": [2376.982], "images_per_second": 0.421, "prompt_tokens": 21, "response_tokens_est": 63, "n_tiles": 6, "output_text": "The image shows a freshly baked pizza with toppings of green peppers, red tomatoes, and chunks of cheese on a white plate. The pizza is placed on a red and white checkered tablecloth, and a person's hand is visible holding a fork, indicating that the pizza is ready to be eaten.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25593.3, "ram_available_mb": 100178.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 25593.7, "ram_available_mb": 100178.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.78, "peak": 15.11, "min": 14.12}, "VDD_GPU": {"avg": 28.67, "peak": 38.19, "min": 23.24}, "VIN": {"avg": 71.07, "peak": 108.62, "min": 58.11}, "VDD_CPU_SOC_MSS": {"avg": 15.29, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 28.67, "energy_joules_est": 68.16, "sample_count": 18, "duration_seconds": 2.377}, "timestamp": "2026-01-16T16:04:29.045798"}
{"image_index": 261, "image_name": "000000027696.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000027696.jpg", "image_width": 640, "image_height": 410, "image_resolution": "640x410", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2153.149, "latencies_ms": [2153.149], "images_per_second": 0.464, "prompt_tokens": 19, "response_tokens_est": 55, "n_tiles": 6, "output_text": "The image features a freshly baked pizza with a golden-brown crust, topped with melted cheese, red tomato sauce, green bell peppers, and chunks of mushrooms. The lighting is bright, highlighting the vibrant colors of the pizza and the checkered tablecloth beneath it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25593.7, "ram_available_mb": 100178.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 25593.5, "ram_available_mb": 100178.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.7, "peak": 15.01, "min": 14.12}, "VDD_GPU": {"avg": 29.57, "peak": 38.21, "min": 23.64}, "VIN": {"avg": 65.52, "peak": 107.89, "min": 55.43}, "VDD_CPU_SOC_MSS": {"avg": 15.23, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 29.57, "energy_joules_est": 63.68, "sample_count": 16, "duration_seconds": 2.154}, "timestamp": "2026-01-16T16:04:31.204974"}
{"image_index": 262, "image_name": "000000027768.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000027768.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1789.137, "latencies_ms": [1789.137], "images_per_second": 0.559, "prompt_tokens": 9, "response_tokens_est": 79, "n_tiles": 1, "output_text": "The image depicts a modern city bus, specifically the Metropolitan Transit System, with the route number 5, as indicated on the side of the bus. The bus is white with red accents and features large windows, allowing passengers to view the surroundings. The bus is parked on a street with a clear sky above, and there are trees in the background, suggesting a suburban or urban setting.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 25593.5, "ram_available_mb": 100178.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 25593.7, "ram_available_mb": 100178.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 4845.7, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.46, "peak": 15.82, "min": 14.61}, "VDD_GPU": {"avg": 21.42, "peak": 25.6, "min": 20.09}, "VIN": {"avg": 63.25, "peak": 70.83, "min": 55.96}, "VDD_CPU_SOC_MSS": {"avg": 16.42, "peak": 16.93, "min": 14.96}}, "power_watts_avg": 21.42, "energy_joules_est": 38.34, "sample_count": 14, "duration_seconds": 1.79}, "timestamp": "2026-01-16T16:04:33.017277"}
{"image_index": 262, "image_name": "000000027768.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000027768.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1038.841, "latencies_ms": [1038.841], "images_per_second": 0.963, "prompt_tokens": 23, "response_tokens_est": 45, "n_tiles": 1, "output_text": "object: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25593.7, "ram_available_mb": 100178.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 25593.7, "ram_available_mb": 100178.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 4856.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.57, "peak": 15.72, "min": 15.32}, "VDD_GPU": {"avg": 21.87, "peak": 24.43, "min": 20.48}, "VIN": {"avg": 62.87, "peak": 64.87, "min": 60.66}, "VDD_CPU_SOC_MSS": {"avg": 16.49, "peak": 16.93, "min": 16.14}}, "power_watts_avg": 21.87, "energy_joules_est": 22.73, "sample_count": 8, "duration_seconds": 1.039}, "timestamp": "2026-01-16T16:04:34.063831"}
{"image_index": 262, "image_name": "000000027768.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000027768.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2458.754, "latencies_ms": [2458.754], "images_per_second": 0.407, "prompt_tokens": 27, "response_tokens_est": 110, "n_tiles": 1, "output_text": "The main object in the image is a bus, which is positioned in the foreground. The bus is white with red accents and has a digital display showing the route number \"D 1\" and the destination \"DONATEM\". The bus is parked on the street, with a few other vehicles visible in the background. The bus has a large front windshield and a side door, and there is a driver visible inside the bus. The bus is surrounded by trees and buildings in the background, indicating that it is in an urban setting.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25593.7, "ram_available_mb": 100178.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25593.7, "ram_available_mb": 100178.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 4859.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.65, "peak": 15.93, "min": 15.11}, "VDD_GPU": {"avg": 20.84, "peak": 24.42, "min": 20.09}, "VIN": {"avg": 63.47, "peak": 67.39, "min": 56.3}, "VDD_CPU_SOC_MSS": {"avg": 16.72, "peak": 16.94, "min": 16.14}}, "power_watts_avg": 20.84, "energy_joules_est": 51.25, "sample_count": 19, "duration_seconds": 2.459}, "timestamp": "2026-01-16T16:04:36.528215"}
{"image_index": 262, "image_name": "000000027768.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000027768.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2324.061, "latencies_ms": [2324.061], "images_per_second": 0.43, "prompt_tokens": 21, "response_tokens_est": 104, "n_tiles": 1, "output_text": "The image depicts a city bus in motion on a city street, with a clear blue sky overhead. The bus is part of the Metropolitan Transit System, as indicated by the text on its side. The bus is white with red accents and has a digital display showing the route number \"D\" and destination \"DONATE.\" The bus is parked on the side of the street, and a person is visible inside the bus, possibly a driver. The surrounding area includes buildings and trees, suggesting an urban setting.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25593.7, "ram_available_mb": 100178.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 25593.9, "ram_available_mb": 100178.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 4854.7, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.72, "peak": 15.93, "min": 15.32}, "VDD_GPU": {"avg": 20.88, "peak": 24.42, "min": 20.09}, "VIN": {"avg": 63.6, "peak": 70.99, "min": 56.61}, "VDD_CPU_SOC_MSS": {"avg": 16.51, "peak": 16.93, "min": 16.14}}, "power_watts_avg": 20.88, "energy_joules_est": 48.53, "sample_count": 18, "duration_seconds": 2.324}, "timestamp": "2026-01-16T16:04:38.858155"}
{"image_index": 262, "image_name": "000000027768.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000027768.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1278.481, "latencies_ms": [1278.481], "images_per_second": 0.782, "prompt_tokens": 19, "response_tokens_est": 56, "n_tiles": 1, "output_text": "The bus is predominantly white with red accents, featuring a modern design with a large front windshield and a digital display. The windows are tinted, and the bus has a sleek, streamlined appearance. The scene is bright and sunny, with shadows indicating a clear day.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25593.9, "ram_available_mb": 100178.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 25593.9, "ram_available_mb": 100178.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 4853.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.68, "peak": 15.82, "min": 15.42}, "VDD_GPU": {"avg": 21.39, "peak": 24.03, "min": 20.09}, "VIN": {"avg": 63.02, "peak": 65.54, "min": 60.63}, "VDD_CPU_SOC_MSS": {"avg": 16.46, "peak": 16.93, "min": 16.14}}, "power_watts_avg": 21.39, "energy_joules_est": 27.36, "sample_count": 10, "duration_seconds": 1.279}, "timestamp": "2026-01-16T16:04:40.142747"}
{"image_index": 263, "image_name": "000000027932.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000027932.jpg", "image_width": 288, "image_height": 307, "image_resolution": "288x307", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 804.729, "latencies_ms": [804.729], "images_per_second": 1.243, "prompt_tokens": 9, "response_tokens_est": 34, "n_tiles": 1, "output_text": "The image shows a baseball glove and a baseball cap resting on a ground with a small amount of gravel, with the cap's logo prominently displayed on the front.", "error": null, "sys_before": {"cpu_percent": 40.0, "ram_used_mb": 25593.9, "ram_available_mb": 100178.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 25593.9, "ram_available_mb": 100178.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 4845.7, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.52, "peak": 15.72, "min": 15.32}, "VDD_GPU": {"avg": 22.14, "peak": 23.64, "min": 20.88}, "VIN": {"avg": 64.12, "peak": 64.72, "min": 63.18}, "VDD_CPU_SOC_MSS": {"avg": 16.38, "peak": 16.54, "min": 16.14}}, "power_watts_avg": 22.14, "energy_joules_est": 17.83, "sample_count": 5, "duration_seconds": 0.805}, "timestamp": "2026-01-16T16:04:40.961905"}
{"image_index": 263, "image_name": "000000027932.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000027932.jpg", "image_width": 288, "image_height": 307, "image_resolution": "288x307", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 372.029, "latencies_ms": [372.029], "images_per_second": 2.688, "prompt_tokens": 23, "response_tokens_est": 14, "n_tiles": 1, "output_text": "baseball cap: 1\nbaseball glove: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25593.9, "ram_available_mb": 100178.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 25593.9, "ram_available_mb": 100178.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 4856.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.42, "peak": 15.52, "min": 15.32}, "VDD_GPU": {"avg": 24.03, "peak": 24.83, "min": 23.24}, "VIN": {"avg": 63.56, "peak": 65.23, "min": 61.89}, "VDD_CPU_SOC_MSS": {"avg": 16.34, "peak": 16.54, "min": 16.14}}, "power_watts_avg": 24.03, "energy_joules_est": 8.96, "sample_count": 2, "duration_seconds": 0.373}, "timestamp": "2026-01-16T16:04:41.340351"}
{"image_index": 263, "image_name": "000000027932.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000027932.jpg", "image_width": 288, "image_height": 307, "image_resolution": "288x307", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1577.187, "latencies_ms": [1577.187], "images_per_second": 0.634, "prompt_tokens": 27, "response_tokens_est": 70, "n_tiles": 1, "output_text": "The main objects in the image are a baseball glove and a baseball cap. The baseball glove is positioned on the left side of the image, while the baseball cap is placed on top of the glove. The baseball cap is in the foreground, and the glove is slightly behind it. The background is out of focus, emphasizing the foreground objects.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25593.9, "ram_available_mb": 100178.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 25593.9, "ram_available_mb": 100178.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 4859.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.57, "peak": 15.72, "min": 15.11}, "VDD_GPU": {"avg": 21.77, "peak": 25.99, "min": 20.1}, "VIN": {"avg": 64.67, "peak": 72.5, "min": 58.97}, "VDD_CPU_SOC_MSS": {"avg": 16.47, "peak": 16.54, "min": 16.14}}, "power_watts_avg": 21.77, "energy_joules_est": 34.34, "sample_count": 12, "duration_seconds": 1.578}, "timestamp": "2026-01-16T16:04:42.925602"}
{"image_index": 263, "image_name": "000000027932.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000027932.jpg", "image_width": 288, "image_height": 307, "image_resolution": "288x307", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1055.999, "latencies_ms": [1055.999], "images_per_second": 0.947, "prompt_tokens": 21, "response_tokens_est": 46, "n_tiles": 1, "output_text": "The image depicts a baseball glove and a baseball cap placed on a gravel surface, likely a baseball field. The glove is open, revealing a baseball inside, and the cap is worn with a white logo on the front.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25593.9, "ram_available_mb": 100178.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 25593.9, "ram_available_mb": 100178.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 4854.7, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.61, "peak": 15.72, "min": 15.42}, "VDD_GPU": {"avg": 21.67, "peak": 23.64, "min": 20.49}, "VIN": {"avg": 63.61, "peak": 66.14, "min": 60.82}, "VDD_CPU_SOC_MSS": {"avg": 16.48, "peak": 16.54, "min": 16.14}}, "power_watts_avg": 21.67, "energy_joules_est": 22.89, "sample_count": 7, "duration_seconds": 1.056}, "timestamp": "2026-01-16T16:04:43.987448"}
{"image_index": 263, "image_name": "000000027932.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000027932.jpg", "image_width": 288, "image_height": 307, "image_resolution": "288x307", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1122.647, "latencies_ms": [1122.647], "images_per_second": 0.891, "prompt_tokens": 19, "response_tokens_est": 49, "n_tiles": 1, "output_text": "The image features a dark navy baseball cap with a white \"U\" logo, resting on a baseball glove. The glove is made of leather with metal accents. The cap and glove are illuminated by natural light, suggesting a sunny day.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25593.9, "ram_available_mb": 100178.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 25593.9, "ram_available_mb": 100178.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 4853.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.62, "peak": 15.82, "min": 15.32}, "VDD_GPU": {"avg": 21.72, "peak": 24.03, "min": 20.48}, "VIN": {"avg": 63.43, "peak": 69.19, "min": 56.95}, "VDD_CPU_SOC_MSS": {"avg": 16.49, "peak": 16.93, "min": 16.14}}, "power_watts_avg": 21.72, "energy_joules_est": 24.39, "sample_count": 8, "duration_seconds": 1.123}, "timestamp": "2026-01-16T16:04:45.115768"}
{"image_index": 264, "image_name": "000000027972.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000027972.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1116.151, "latencies_ms": [1116.151], "images_per_second": 0.896, "prompt_tokens": 9, "response_tokens_est": 20, "n_tiles": 6, "output_text": "A surfer is riding a wave in the ocean, wearing a red shirt and black shorts.", "error": null, "sys_before": {"cpu_percent": 42.3, "ram_used_mb": 25593.9, "ram_available_mb": 100178.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 25593.9, "ram_available_mb": 100178.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.6, "peak": 14.91, "min": 14.22}, "VDD_GPU": {"avg": 32.6, "peak": 37.42, "min": 27.57}, "VIN": {"avg": 72.6, "peak": 93.69, "min": 51.82}, "VDD_CPU_SOC_MSS": {"avg": 15.45, "peak": 16.14, "min": 14.96}}, "power_watts_avg": 32.6, "energy_joules_est": 36.41, "sample_count": 8, "duration_seconds": 1.117}, "timestamp": "2026-01-16T16:04:46.272906"}
{"image_index": 264, "image_name": "000000027972.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000027972.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1273.404, "latencies_ms": [1273.404], "images_per_second": 0.785, "prompt_tokens": 23, "response_tokens_est": 24, "n_tiles": 6, "output_text": "surfboard: 1\nsurfer: 1\nwater: 1\nocean: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25593.9, "ram_available_mb": 100178.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 25594.2, "ram_available_mb": 100178.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.65, "peak": 14.91, "min": 14.22}, "VDD_GPU": {"avg": 34.31, "peak": 39.38, "min": 27.57}, "VIN": {"avg": 73.67, "peak": 107.73, "min": 59.53}, "VDD_CPU_SOC_MSS": {"avg": 15.26, "peak": 15.36, "min": 14.96}}, "power_watts_avg": 34.31, "energy_joules_est": 43.71, "sample_count": 9, "duration_seconds": 1.274}, "timestamp": "2026-01-16T16:04:47.552297"}
{"image_index": 264, "image_name": "000000027972.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000027972.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2131.469, "latencies_ms": [2131.469], "images_per_second": 0.469, "prompt_tokens": 27, "response_tokens_est": 56, "n_tiles": 6, "output_text": "The main object in the foreground is a surfer riding a wave. The surfer is positioned on a surfboard, which is partially submerged in the water. The background features a large, frothy wave, indicating the surfer's position in relation to the wave.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25594.2, "ram_available_mb": 100178.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 25594.2, "ram_available_mb": 100178.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.85, "peak": 15.22, "min": 14.22}, "VDD_GPU": {"avg": 30.25, "peak": 38.59, "min": 24.03}, "VIN": {"avg": 67.74, "peak": 92.51, "min": 62.25}, "VDD_CPU_SOC_MSS": {"avg": 15.43, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 30.25, "energy_joules_est": 64.49, "sample_count": 16, "duration_seconds": 2.132}, "timestamp": "2026-01-16T16:04:49.689919"}
{"image_index": 264, "image_name": "000000027972.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000027972.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2586.29, "latencies_ms": [2586.29], "images_per_second": 0.387, "prompt_tokens": 21, "response_tokens_est": 73, "n_tiles": 6, "output_text": "The image captures a surfer riding a wave in a vibrant blue ocean. The surfer, dressed in a red shirt and black wetsuit, is skillfully maneuvering a white surfboard with yellow and green accents. The scene is set in a dynamic and energetic environment, showcasing the thrill and excitement of surfing in a natural, aquatic setting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25594.2, "ram_available_mb": 100178.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 25594.1, "ram_available_mb": 100178.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.94, "peak": 15.22, "min": 14.32}, "VDD_GPU": {"avg": 28.65, "peak": 37.8, "min": 23.64}, "VIN": {"avg": 68.77, "peak": 115.36, "min": 59.16}, "VDD_CPU_SOC_MSS": {"avg": 15.56, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 28.65, "energy_joules_est": 74.1, "sample_count": 19, "duration_seconds": 2.587}, "timestamp": "2026-01-16T16:04:52.283056"}
{"image_index": 264, "image_name": "000000027972.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000027972.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1956.55, "latencies_ms": [1956.55], "images_per_second": 0.511, "prompt_tokens": 19, "response_tokens_est": 50, "n_tiles": 6, "output_text": "The image features a surfer riding a wave in a vibrant turquoise ocean. The surfer is wearing a red shirt and black shorts, and the wave is breaking with white foam. The lighting is bright and natural, suggesting a sunny day.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25594.1, "ram_available_mb": 100178.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 25594.6, "ram_available_mb": 100177.5, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.89, "peak": 15.22, "min": 14.22}, "VDD_GPU": {"avg": 30.5, "peak": 38.21, "min": 24.42}, "VIN": {"avg": 75.14, "peak": 116.54, "min": 58.23}, "VDD_CPU_SOC_MSS": {"avg": 15.58, "peak": 16.14, "min": 14.96}}, "power_watts_avg": 30.5, "energy_joules_est": 59.68, "sample_count": 14, "duration_seconds": 1.957}, "timestamp": "2026-01-16T16:04:54.246678"}
{"image_index": 265, "image_name": "000000027982.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000027982.jpg", "image_width": 500, "image_height": 334, "image_resolution": "500x334", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1624.295, "latencies_ms": [1624.295], "images_per_second": 0.616, "prompt_tokens": 9, "response_tokens_est": 39, "n_tiles": 6, "output_text": "The image shows a black and white photograph of a bathroom, featuring a toilet with its lid open, a toilet brush, a toilet paper holder, and a toilet paper roll on the floor.", "error": null, "sys_before": {"cpu_percent": 37.9, "ram_used_mb": 25594.1, "ram_available_mb": 100178.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 25594.1, "ram_available_mb": 100178.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.61, "peak": 15.11, "min": 13.92}, "VDD_GPU": {"avg": 30.89, "peak": 37.42, "min": 24.82}, "VIN": {"avg": 69.68, "peak": 123.87, "min": 59.24}, "VDD_CPU_SOC_MSS": {"avg": 15.32, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 30.89, "energy_joules_est": 50.18, "sample_count": 12, "duration_seconds": 1.625}, "timestamp": "2026-01-16T16:04:55.918875"}
{"image_index": 265, "image_name": "000000027982.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000027982.jpg", "image_width": 500, "image_height": 334, "image_resolution": "500x334", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1660.961, "latencies_ms": [1660.961], "images_per_second": 0.602, "prompt_tokens": 23, "response_tokens_est": 37, "n_tiles": 6, "output_text": "toilet paper: 8\ntoilet brush: 1\ntoilet paper roll: 1\ntoilet paper bag: 1\ntoilet paper: 8", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25594.1, "ram_available_mb": 100178.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25594.4, "ram_available_mb": 100177.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.69, "peak": 14.91, "min": 14.22}, "VDD_GPU": {"avg": 31.77, "peak": 38.21, "min": 24.82}, "VIN": {"avg": 77.31, "peak": 115.52, "min": 62.6}, "VDD_CPU_SOC_MSS": {"avg": 15.19, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 31.77, "energy_joules_est": 52.78, "sample_count": 12, "duration_seconds": 1.661}, "timestamp": "2026-01-16T16:04:57.585824"}
{"image_index": 265, "image_name": "000000027982.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000027982.jpg", "image_width": 500, "image_height": 334, "image_resolution": "500x334", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2137.549, "latencies_ms": [2137.549], "images_per_second": 0.468, "prompt_tokens": 27, "response_tokens_est": 54, "n_tiles": 6, "output_text": "The toilet is positioned in the foreground, with its lid open and seat down. To the right of the toilet, there is a stack of toilet paper rolls. The background features a wall with a patterned tile design and a small, dark object on the floor.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25594.4, "ram_available_mb": 100177.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 25594.1, "ram_available_mb": 100178.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.7, "peak": 15.01, "min": 14.12}, "VDD_GPU": {"avg": 29.44, "peak": 38.59, "min": 23.64}, "VIN": {"avg": 65.66, "peak": 83.52, "min": 54.84}, "VDD_CPU_SOC_MSS": {"avg": 15.23, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 29.44, "energy_joules_est": 62.94, "sample_count": 16, "duration_seconds": 2.138}, "timestamp": "2026-01-16T16:04:59.729407"}
{"image_index": 265, "image_name": "000000027982.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000027982.jpg", "image_width": 500, "image_height": 334, "image_resolution": "500x334", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2447.139, "latencies_ms": [2447.139], "images_per_second": 0.409, "prompt_tokens": 21, "response_tokens_est": 65, "n_tiles": 6, "output_text": "The image depicts a bathroom setting with a white toilet positioned against a tiled wall. The toilet is accompanied by a toilet brush and a roll of toilet paper, indicating that the scene is likely in a bathroom. The overall setting appears to be a typical bathroom environment with a focus on the toilet and its associated items.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25594.1, "ram_available_mb": 100178.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 25594.4, "ram_available_mb": 100177.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.71, "peak": 15.01, "min": 14.02}, "VDD_GPU": {"avg": 28.55, "peak": 37.82, "min": 23.24}, "VIN": {"avg": 68.71, "peak": 107.55, "min": 56.08}, "VDD_CPU_SOC_MSS": {"avg": 15.31, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 28.55, "energy_joules_est": 69.87, "sample_count": 19, "duration_seconds": 2.447}, "timestamp": "2026-01-16T16:05:02.182646"}
{"image_index": 265, "image_name": "000000027982.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000027982.jpg", "image_width": 500, "image_height": 334, "image_resolution": "500x334", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2197.464, "latencies_ms": [2197.464], "images_per_second": 0.455, "prompt_tokens": 19, "response_tokens_est": 56, "n_tiles": 6, "output_text": "The image is a black and white photograph of a bathroom setting. The walls are tiled with large, rectangular, light-colored tiles, and the floor is made of dark tiles. The lighting is soft and diffused, casting gentle shadows and highlighting the textures of the materials.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25594.4, "ram_available_mb": 100177.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 25593.3, "ram_available_mb": 100178.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.67, "peak": 15.01, "min": 14.02}, "VDD_GPU": {"avg": 28.96, "peak": 38.19, "min": 23.24}, "VIN": {"avg": 68.83, "peak": 106.95, "min": 60.29}, "VDD_CPU_SOC_MSS": {"avg": 15.19, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 28.96, "energy_joules_est": 63.65, "sample_count": 17, "duration_seconds": 2.198}, "timestamp": "2026-01-16T16:05:04.385903"}
{"image_index": 266, "image_name": "000000028285.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000028285.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1582.986, "latencies_ms": [1582.986], "images_per_second": 0.632, "prompt_tokens": 9, "response_tokens_est": 37, "n_tiles": 6, "output_text": "The image features a white clock tower with a blue dome, set against a backdrop of a clear blue sky and lush green trees, with a tiled roof visible in the foreground.", "error": null, "sys_before": {"cpu_percent": 47.1, "ram_used_mb": 25593.1, "ram_available_mb": 100179.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25593.3, "ram_available_mb": 100178.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.47, "peak": 15.01, "min": 13.82}, "VDD_GPU": {"avg": 30.56, "peak": 36.62, "min": 24.82}, "VIN": {"avg": 67.78, "peak": 78.28, "min": 60.68}, "VDD_CPU_SOC_MSS": {"avg": 15.16, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 30.56, "energy_joules_est": 48.4, "sample_count": 12, "duration_seconds": 1.584}, "timestamp": "2026-01-16T16:05:06.020767"}
{"image_index": 266, "image_name": "000000028285.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000028285.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1742.256, "latencies_ms": [1742.256], "images_per_second": 0.574, "prompt_tokens": 23, "response_tokens_est": 40, "n_tiles": 6, "output_text": "object: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25593.3, "ram_available_mb": 100178.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 25593.6, "ram_available_mb": 100178.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.62, "peak": 14.91, "min": 14.02}, "VDD_GPU": {"avg": 31.03, "peak": 38.6, "min": 24.03}, "VIN": {"avg": 71.62, "peak": 115.31, "min": 49.71}, "VDD_CPU_SOC_MSS": {"avg": 15.17, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 31.03, "energy_joules_est": 54.08, "sample_count": 13, "duration_seconds": 1.743}, "timestamp": "2026-01-16T16:05:07.769256"}
{"image_index": 266, "image_name": "000000028285.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000028285.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2224.582, "latencies_ms": [2224.582], "images_per_second": 0.45, "prompt_tokens": 27, "response_tokens_est": 57, "n_tiles": 6, "output_text": "The main objects in the image are a white clock tower and a tiled roof. The clock tower is positioned in the background, while the tiled roof is in the foreground. The clock tower is near the tiled roof, creating a clear spatial relationship between the two objects.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25593.6, "ram_available_mb": 100178.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 25593.8, "ram_available_mb": 100178.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.7, "peak": 15.01, "min": 14.02}, "VDD_GPU": {"avg": 29.08, "peak": 38.21, "min": 23.24}, "VIN": {"avg": 72.24, "peak": 116.07, "min": 62.29}, "VDD_CPU_SOC_MSS": {"avg": 15.21, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 29.08, "energy_joules_est": 64.7, "sample_count": 17, "duration_seconds": 2.225}, "timestamp": "2026-01-16T16:05:09.999887"}
{"image_index": 266, "image_name": "000000028285.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000028285.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2282.046, "latencies_ms": [2282.046], "images_per_second": 0.438, "prompt_tokens": 21, "response_tokens_est": 59, "n_tiles": 6, "output_text": "The image depicts a quaint, historic building with a white tower and a clock, set against a backdrop of a clear blue sky and lush greenery. The scene is likely in a tropical or subtropical region, given the presence of the greenery and the style of the architecture.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25593.8, "ram_available_mb": 100178.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 25593.6, "ram_available_mb": 100178.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.68, "peak": 15.01, "min": 14.02}, "VDD_GPU": {"avg": 28.92, "peak": 37.82, "min": 23.25}, "VIN": {"avg": 68.19, "peak": 115.52, "min": 55.36}, "VDD_CPU_SOC_MSS": {"avg": 15.33, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 28.92, "energy_joules_est": 66.01, "sample_count": 17, "duration_seconds": 2.283}, "timestamp": "2026-01-16T16:05:12.288340"}
{"image_index": 266, "image_name": "000000028285.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000028285.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2046.335, "latencies_ms": [2046.335], "images_per_second": 0.489, "prompt_tokens": 19, "response_tokens_est": 51, "n_tiles": 6, "output_text": "The image features a white clock tower with a blue dome, set against a clear blue sky. The tower is adorned with intricate metalwork and has a tiled roof. The lighting is bright and natural, suggesting it is daytime with clear weather.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25593.6, "ram_available_mb": 100178.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 25593.6, "ram_available_mb": 100178.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.65, "peak": 15.01, "min": 14.02}, "VDD_GPU": {"avg": 29.62, "peak": 38.6, "min": 23.64}, "VIN": {"avg": 66.82, "peak": 117.22, "min": 56.29}, "VDD_CPU_SOC_MSS": {"avg": 15.23, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 29.62, "energy_joules_est": 60.62, "sample_count": 16, "duration_seconds": 2.047}, "timestamp": "2026-01-16T16:05:14.340914"}
{"image_index": 267, "image_name": "000000028449.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000028449.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1414.248, "latencies_ms": [1414.248], "images_per_second": 0.707, "prompt_tokens": 9, "response_tokens_est": 31, "n_tiles": 6, "output_text": "The image depicts a group of elephants in a natural, outdoor setting, with one elephant in the foreground prominently displaying its trunk and tusks.", "error": null, "sys_before": {"cpu_percent": 41.3, "ram_used_mb": 25593.6, "ram_available_mb": 100178.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 25593.8, "ram_available_mb": 100178.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.41, "peak": 15.01, "min": 13.82}, "VDD_GPU": {"avg": 31.75, "peak": 37.42, "min": 26.0}, "VIN": {"avg": 73.72, "peak": 117.45, "min": 56.56}, "VDD_CPU_SOC_MSS": {"avg": 15.15, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 31.75, "energy_joules_est": 44.91, "sample_count": 10, "duration_seconds": 1.414}, "timestamp": "2026-01-16T16:05:15.813694"}
{"image_index": 267, "image_name": "000000028449.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000028449.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1112.736, "latencies_ms": [1112.736], "images_per_second": 0.899, "prompt_tokens": 23, "response_tokens_est": 18, "n_tiles": 6, "output_text": "elephant: 3\nbushes: 1\ntree: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25593.8, "ram_available_mb": 100178.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 25587.5, "ram_available_mb": 100184.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.64, "peak": 14.83, "min": 14.22}, "VDD_GPU": {"avg": 34.86, "peak": 38.6, "min": 29.15}, "VIN": {"avg": 72.23, "peak": 96.51, "min": 59.14}, "VDD_CPU_SOC_MSS": {"avg": 15.21, "peak": 15.36, "min": 14.96}}, "power_watts_avg": 34.86, "energy_joules_est": 38.8, "sample_count": 8, "duration_seconds": 1.113}, "timestamp": "2026-01-16T16:05:16.932507"}
{"image_index": 267, "image_name": "000000028449.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000028449.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2159.262, "latencies_ms": [2159.262], "images_per_second": 0.463, "prompt_tokens": 27, "response_tokens_est": 55, "n_tiles": 6, "output_text": "The main objects in the image are elephants. The foreground features a large elephant with its trunk raised, while the background shows several other elephants, some of which are slightly out of focus. The elephants are positioned in a natural, outdoor setting with greenery and dirt ground.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25587.5, "ram_available_mb": 100184.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 25587.6, "ram_available_mb": 100184.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.7, "peak": 15.01, "min": 14.02}, "VDD_GPU": {"avg": 29.94, "peak": 39.39, "min": 23.64}, "VIN": {"avg": 67.07, "peak": 89.93, "min": 56.39}, "VDD_CPU_SOC_MSS": {"avg": 15.26, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 29.94, "energy_joules_est": 64.66, "sample_count": 17, "duration_seconds": 2.16}, "timestamp": "2026-01-16T16:05:19.098470"}
{"image_index": 267, "image_name": "000000028449.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000028449.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2322.123, "latencies_ms": [2322.123], "images_per_second": 0.431, "prompt_tokens": 21, "response_tokens_est": 61, "n_tiles": 6, "output_text": "The image depicts a group of elephants in a natural, outdoor setting. The elephants are gathered together, with some of them standing close to each other while others are further away. The scene appears to be in a savannah or grassland environment, with trees and bushes visible in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25587.6, "ram_available_mb": 100184.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 25587.6, "ram_available_mb": 100184.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.73, "peak": 15.01, "min": 14.02}, "VDD_GPU": {"avg": 28.73, "peak": 38.21, "min": 23.24}, "VIN": {"avg": 71.96, "peak": 99.99, "min": 53.69}, "VDD_CPU_SOC_MSS": {"avg": 15.4, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 28.73, "energy_joules_est": 66.73, "sample_count": 18, "duration_seconds": 2.323}, "timestamp": "2026-01-16T16:05:21.426973"}
{"image_index": 267, "image_name": "000000028449.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000028449.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2130.202, "latencies_ms": [2130.202], "images_per_second": 0.469, "prompt_tokens": 19, "response_tokens_est": 54, "n_tiles": 6, "output_text": "The image features a group of elephants in a natural, outdoor setting. The elephants are depicted with their skin appearing dark and textured, likely due to the natural environment. The lighting is soft and natural, suggesting it is daytime with ample sunlight illuminating the scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25587.6, "ram_available_mb": 100184.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 25587.4, "ram_available_mb": 100184.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.7, "peak": 15.01, "min": 14.02}, "VDD_GPU": {"avg": 29.52, "peak": 38.21, "min": 23.64}, "VIN": {"avg": 70.95, "peak": 114.58, "min": 60.83}, "VDD_CPU_SOC_MSS": {"avg": 15.3, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 29.52, "energy_joules_est": 62.89, "sample_count": 16, "duration_seconds": 2.13}, "timestamp": "2026-01-16T16:05:23.563056"}
{"image_index": 268, "image_name": "000000028452.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000028452.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2518.37, "latencies_ms": [2518.37], "images_per_second": 0.397, "prompt_tokens": 9, "response_tokens_est": 33, "n_tiles": 12, "output_text": "The image shows a refrigerator with a partially open door, revealing a small section of the interior, and a toilet with its lid open, revealing a toilet bowl.", "error": null, "sys_before": {"cpu_percent": 43.9, "ram_used_mb": 25587.4, "ram_available_mb": 100184.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 25588.8, "ram_available_mb": 100183.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.09, "peak": 16.15, "min": 13.79}, "VDD_GPU": {"avg": 34.48, "peak": 40.95, "min": 27.19}, "VIN": {"avg": 81.69, "peak": 131.76, "min": 58.37}, "VDD_CPU_SOC_MSS": {"avg": 15.91, "peak": 17.32, "min": 14.96}}, "power_watts_avg": 34.48, "energy_joules_est": 86.84, "sample_count": 19, "duration_seconds": 2.519}, "timestamp": "2026-01-16T16:05:26.175407"}
{"image_index": 268, "image_name": "000000028452.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000028452.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 3417.656, "latencies_ms": [3417.656], "images_per_second": 0.293, "prompt_tokens": 23, "response_tokens_est": 56, "n_tiles": 12, "output_text": "- Refrigerator: 1\n- Water bottle: 1\n- Bowl: 1\n- Bowl holder: 1\n- Dish rack: 1\n- Dishware: 1\n- Dish: 1\n- Dish rack: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25588.8, "ram_available_mb": 100183.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 25588.8, "ram_available_mb": 100183.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.08, "peak": 15.65, "min": 14.43}, "VDD_GPU": {"avg": 33.6, "peak": 42.54, "min": 26.39}, "VIN": {"avg": 75.77, "peak": 116.46, "min": 55.34}, "VDD_CPU_SOC_MSS": {"avg": 15.61, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 33.6, "energy_joules_est": 114.85, "sample_count": 26, "duration_seconds": 3.418}, "timestamp": "2026-01-16T16:05:29.604247"}
{"image_index": 268, "image_name": "000000028452.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000028452.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3385.649, "latencies_ms": [3385.649], "images_per_second": 0.295, "prompt_tokens": 27, "response_tokens_est": 55, "n_tiles": 12, "output_text": "The refrigerator is located in the foreground, with its door open, revealing its contents. The sink is situated to the right of the refrigerator, with a bottle of liquid placed on the countertop. The toilet is in the background, slightly to the right of the sink.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25588.8, "ram_available_mb": 100183.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 25588.8, "ram_available_mb": 100183.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.07, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 33.6, "peak": 42.53, "min": 26.39}, "VIN": {"avg": 77.12, "peak": 117.84, "min": 58.62}, "VDD_CPU_SOC_MSS": {"avg": 15.59, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 33.6, "energy_joules_est": 113.78, "sample_count": 26, "duration_seconds": 3.386}, "timestamp": "2026-01-16T16:05:32.996321"}
{"image_index": 268, "image_name": "000000028452.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000028452.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3887.056, "latencies_ms": [3887.056], "images_per_second": 0.257, "prompt_tokens": 21, "response_tokens_est": 70, "n_tiles": 12, "output_text": "The image shows a small, well-lit refrigerator with a closed door, situated in a kitchen or dining area. The refrigerator is empty, and there is a small sink or dish drainer on the countertop. The setting appears to be a domestic environment, possibly a kitchen or dining area, with a focus on the refrigerator and its contents.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25588.8, "ram_available_mb": 100183.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25589.1, "ram_available_mb": 100183.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.09, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 32.64, "peak": 42.94, "min": 26.39}, "VIN": {"avg": 73.42, "peak": 113.2, "min": 58.3}, "VDD_CPU_SOC_MSS": {"avg": 15.63, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 32.64, "energy_joules_est": 126.89, "sample_count": 30, "duration_seconds": 3.888}, "timestamp": "2026-01-16T16:05:36.889959"}
{"image_index": 268, "image_name": "000000028452.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000028452.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2875.483, "latencies_ms": [2875.483], "images_per_second": 0.348, "prompt_tokens": 19, "response_tokens_est": 40, "n_tiles": 12, "output_text": "The image shows a white refrigerator with a glass door, illuminated by a warm, soft light. The refrigerator is placed on a tiled floor, and the overall atmosphere is cozy and inviting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25589.1, "ram_available_mb": 100183.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 25589.3, "ram_available_mb": 100182.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.09, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 34.57, "peak": 41.76, "min": 26.79}, "VIN": {"avg": 76.35, "peak": 116.29, "min": 58.79}, "VDD_CPU_SOC_MSS": {"avg": 15.65, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 34.57, "energy_joules_est": 99.42, "sample_count": 22, "duration_seconds": 2.876}, "timestamp": "2026-01-16T16:05:39.772157"}
{"image_index": 269, "image_name": "000000028809.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000028809.jpg", "image_width": 498, "image_height": 500, "image_resolution": "498x500", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 866.469, "latencies_ms": [866.469], "images_per_second": 1.154, "prompt_tokens": 9, "response_tokens_est": 36, "n_tiles": 1, "output_text": "The image shows a close-up view of a bunch of ripe, yellow bananas arranged on a surface, with a blurred background that suggests a bright, possibly indoor environment.", "error": null, "sys_before": {"cpu_percent": 22.2, "ram_used_mb": 25589.3, "ram_available_mb": 100182.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 25589.3, "ram_available_mb": 100182.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 4845.7, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.06, "peak": 15.42, "min": 14.51}, "VDD_GPU": {"avg": 23.97, "peak": 27.57, "min": 21.67}, "VIN": {"avg": 61.59, "peak": 66.62, "min": 50.63}, "VDD_CPU_SOC_MSS": {"avg": 15.75, "peak": 16.14, "min": 14.96}}, "power_watts_avg": 23.97, "energy_joules_est": 20.79, "sample_count": 6, "duration_seconds": 0.867}, "timestamp": "2026-01-16T16:05:40.662870"}
{"image_index": 269, "image_name": "000000028809.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000028809.jpg", "image_width": 498, "image_height": 500, "image_resolution": "498x500", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 222.098, "latencies_ms": [222.098], "images_per_second": 4.503, "prompt_tokens": 23, "response_tokens_est": 7, "n_tiles": 1, "output_text": "- Bananas: 8", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25589.3, "ram_available_mb": 100182.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 25589.3, "ram_available_mb": 100182.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 4856.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.22, "peak": 15.22, "min": 15.22}, "VDD_GPU": {"avg": 24.82, "peak": 24.82, "min": 24.82}, "VIN": {"avg": 61.62, "peak": 61.62, "min": 61.62}, "VDD_CPU_SOC_MSS": {"avg": 15.75, "peak": 15.75, "min": 15.75}}, "power_watts_avg": 24.82, "energy_joules_est": 5.52, "sample_count": 1, "duration_seconds": 0.223}, "timestamp": "2026-01-16T16:05:40.890785"}
{"image_index": 269, "image_name": "000000028809.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000028809.jpg", "image_width": 498, "image_height": 500, "image_resolution": "498x500", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1298.926, "latencies_ms": [1298.926], "images_per_second": 0.77, "prompt_tokens": 27, "response_tokens_est": 57, "n_tiles": 1, "output_text": "The main objects in the image are bananas, which are located in the foreground. The bananas are arranged in a row, with some partially obscured by the other bananas. The background is blurred, indicating that the focus is on the bananas in the foreground.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25589.3, "ram_available_mb": 100182.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25589.3, "ram_available_mb": 100182.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 4859.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.43, "peak": 15.72, "min": 14.91}, "VDD_GPU": {"avg": 22.61, "peak": 27.18, "min": 20.48}, "VIN": {"avg": 63.73, "peak": 67.6, "min": 59.41}, "VDD_CPU_SOC_MSS": {"avg": 16.3, "peak": 16.54, "min": 15.74}}, "power_watts_avg": 22.61, "energy_joules_est": 29.37, "sample_count": 10, "duration_seconds": 1.299}, "timestamp": "2026-01-16T16:05:42.195230"}
{"image_index": 269, "image_name": "000000028809.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000028809.jpg", "image_width": 498, "image_height": 500, "image_resolution": "498x500", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1395.23, "latencies_ms": [1395.23], "images_per_second": 0.717, "prompt_tokens": 21, "response_tokens_est": 60, "n_tiles": 1, "output_text": "The image shows a close-up view of a bunch of ripe bananas on a shelf. The bananas are yellow and appear fresh, with some having visible labels attached to them. The setting is likely a grocery store or a similar retail environment, where the bananas are being displayed for sale.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25589.3, "ram_available_mb": 100182.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25589.0, "ram_available_mb": 100183.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 4854.7, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.49, "peak": 15.62, "min": 15.22}, "VDD_GPU": {"avg": 21.28, "peak": 24.42, "min": 20.1}, "VIN": {"avg": 62.84, "peak": 68.73, "min": 56.87}, "VDD_CPU_SOC_MSS": {"avg": 16.42, "peak": 16.54, "min": 16.14}}, "power_watts_avg": 21.28, "energy_joules_est": 29.7, "sample_count": 10, "duration_seconds": 1.396}, "timestamp": "2026-01-16T16:05:43.596242"}
{"image_index": 269, "image_name": "000000028809.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000028809.jpg", "image_width": 498, "image_height": 500, "image_resolution": "498x500", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1274.381, "latencies_ms": [1274.381], "images_per_second": 0.785, "prompt_tokens": 19, "response_tokens_est": 55, "n_tiles": 1, "output_text": "The image features a bunch of ripe, yellow bananas with a slightly greenish hue, indicating ripeness. The bananas are placed on a dark surface, possibly a shelf, and are illuminated by soft, diffused lighting, creating a warm and inviting atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25589.0, "ram_available_mb": 100183.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25589.3, "ram_available_mb": 100182.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 4853.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.52, "peak": 15.82, "min": 15.01}, "VDD_GPU": {"avg": 21.37, "peak": 24.04, "min": 20.09}, "VIN": {"avg": 63.93, "peak": 71.64, "min": 59.1}, "VDD_CPU_SOC_MSS": {"avg": 16.44, "peak": 16.54, "min": 16.14}}, "power_watts_avg": 21.37, "energy_joules_est": 27.24, "sample_count": 9, "duration_seconds": 1.275}, "timestamp": "2026-01-16T16:05:44.876471"}
{"image_index": 270, "image_name": "000000028993.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000028993.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1006.642, "latencies_ms": [1006.642], "images_per_second": 0.993, "prompt_tokens": 9, "response_tokens_est": 43, "n_tiles": 1, "output_text": "The image depicts a row of rusty, cylindrical objects, possibly fire hydrants, situated on a paved surface in an urban setting, with a backdrop of modern buildings and a clear sky.", "error": null, "sys_before": {"cpu_percent": 10.0, "ram_used_mb": 25589.3, "ram_available_mb": 100182.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25589.0, "ram_available_mb": 100183.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 4845.7, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.48, "peak": 15.72, "min": 15.22}, "VDD_GPU": {"avg": 21.61, "peak": 23.64, "min": 20.48}, "VIN": {"avg": 63.03, "peak": 64.33, "min": 61.2}, "VDD_CPU_SOC_MSS": {"avg": 16.42, "peak": 16.54, "min": 16.14}}, "power_watts_avg": 21.61, "energy_joules_est": 21.76, "sample_count": 7, "duration_seconds": 1.007}, "timestamp": "2026-01-16T16:05:45.905208"}
{"image_index": 270, "image_name": "000000028993.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000028993.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1040.448, "latencies_ms": [1040.448], "images_per_second": 0.961, "prompt_tokens": 23, "response_tokens_est": 45, "n_tiles": 1, "output_text": "object: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25589.0, "ram_available_mb": 100183.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25589.0, "ram_available_mb": 100183.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 4856.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.59, "peak": 15.72, "min": 15.32}, "VDD_GPU": {"avg": 22.01, "peak": 24.42, "min": 20.49}, "VIN": {"avg": 64.12, "peak": 65.84, "min": 63.13}, "VDD_CPU_SOC_MSS": {"avg": 16.42, "peak": 16.54, "min": 16.14}}, "power_watts_avg": 22.01, "energy_joules_est": 22.91, "sample_count": 7, "duration_seconds": 1.041}, "timestamp": "2026-01-16T16:05:46.951840"}
{"image_index": 270, "image_name": "000000028993.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000028993.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1307.984, "latencies_ms": [1307.984], "images_per_second": 0.765, "prompt_tokens": 27, "response_tokens_est": 57, "n_tiles": 1, "output_text": "The main objects in the image are three rusty, cylindrical objects placed in the foreground. They are positioned near a large, ornate building in the background. The building is situated on a paved area with a few trees and other urban structures visible in the distance.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25589.0, "ram_available_mb": 100183.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25589.0, "ram_available_mb": 100183.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 4859.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.62, "peak": 15.82, "min": 15.32}, "VDD_GPU": {"avg": 21.51, "peak": 24.43, "min": 20.09}, "VIN": {"avg": 63.01, "peak": 66.89, "min": 56.29}, "VDD_CPU_SOC_MSS": {"avg": 16.42, "peak": 16.54, "min": 16.14}}, "power_watts_avg": 21.51, "energy_joules_est": 28.14, "sample_count": 10, "duration_seconds": 1.308}, "timestamp": "2026-01-16T16:05:48.265373"}
{"image_index": 270, "image_name": "000000028993.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000028993.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1696.726, "latencies_ms": [1696.726], "images_per_second": 0.589, "prompt_tokens": 21, "response_tokens_est": 75, "n_tiles": 1, "output_text": "The image depicts a cityscape with a row of rusty, cylindrical fire hydrants standing on a paved area. In the background, there are tall buildings and a few trees, suggesting an urban environment. The scene appears to be a public space, possibly a plaza or a city square, with the fire hydrants serving as a focal point.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25589.0, "ram_available_mb": 100183.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25589.0, "ram_available_mb": 100183.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 4854.7, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.63, "peak": 15.72, "min": 15.32}, "VDD_GPU": {"avg": 21.05, "peak": 24.03, "min": 20.1}, "VIN": {"avg": 63.61, "peak": 66.07, "min": 61.91}, "VDD_CPU_SOC_MSS": {"avg": 16.47, "peak": 16.54, "min": 16.14}}, "power_watts_avg": 21.05, "energy_joules_est": 35.73, "sample_count": 12, "duration_seconds": 1.697}, "timestamp": "2026-01-16T16:05:49.967961"}
{"image_index": 270, "image_name": "000000028993.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000028993.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1282.125, "latencies_ms": [1282.125], "images_per_second": 0.78, "prompt_tokens": 19, "response_tokens_est": 56, "n_tiles": 1, "output_text": "The image features a row of rust-colored, cylindrical objects, likely fire hydrants, set against a backdrop of a cityscape with modern high-rise buildings. The scene is illuminated by natural daylight, casting soft shadows and highlighting the textures of the metal surfaces.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25589.0, "ram_available_mb": 100183.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25588.8, "ram_available_mb": 100183.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 4853.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.61, "peak": 15.82, "min": 15.22}, "VDD_GPU": {"avg": 21.36, "peak": 24.03, "min": 20.09}, "VIN": {"avg": 64.8, "peak": 69.82, "min": 62.34}, "VDD_CPU_SOC_MSS": {"avg": 16.41, "peak": 16.54, "min": 16.14}}, "power_watts_avg": 21.36, "energy_joules_est": 27.39, "sample_count": 9, "duration_seconds": 1.282}, "timestamp": "2026-01-16T16:05:51.256118"}
{"image_index": 271, "image_name": "000000029187.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000029187.jpg", "image_width": 640, "image_height": 491, "image_resolution": "640x491", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2113.512, "latencies_ms": [2113.512], "images_per_second": 0.473, "prompt_tokens": 9, "response_tokens_est": 21, "n_tiles": 12, "output_text": "A jockey is riding a horse in a race, with the number 8 on the saddle.", "error": null, "sys_before": {"cpu_percent": 47.1, "ram_used_mb": 25588.8, "ram_available_mb": 100183.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 2.5, "ram_used_mb": 25588.8, "ram_available_mb": 100183.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.26, "peak": 16.15, "min": 14.4}, "VDD_GPU": {"avg": 35.4, "peak": 40.18, "min": 27.97}, "VIN": {"avg": 80.84, "peak": 118.37, "min": 58.68}, "VDD_CPU_SOC_MSS": {"avg": 16.06, "peak": 17.32, "min": 15.35}}, "power_watts_avg": 35.4, "energy_joules_est": 74.83, "sample_count": 16, "duration_seconds": 2.114}, "timestamp": "2026-01-16T16:05:53.448381"}
{"image_index": 271, "image_name": "000000029187.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000029187.jpg", "image_width": 640, "image_height": 491, "image_resolution": "640x491", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 5856.243, "latencies_ms": [5856.243], "images_per_second": 0.171, "prompt_tokens": 23, "response_tokens_est": 128, "n_tiles": 12, "output_text": "object: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject:", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25588.8, "ram_available_mb": 100183.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 25588.8, "ram_available_mb": 100183.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.1, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 30.42, "peak": 43.31, "min": 26.0}, "VIN": {"avg": 70.66, "peak": 116.41, "min": 55.5}, "VDD_CPU_SOC_MSS": {"avg": 15.48, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 30.42, "energy_joules_est": 178.16, "sample_count": 46, "duration_seconds": 5.857}, "timestamp": "2026-01-16T16:05:59.311042"}
{"image_index": 271, "image_name": "000000029187.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000029187.jpg", "image_width": 640, "image_height": 491, "image_resolution": "640x491", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 4788.284, "latencies_ms": [4788.284], "images_per_second": 0.209, "prompt_tokens": 27, "response_tokens_est": 95, "n_tiles": 12, "output_text": "The main object in the foreground is a brown horse with a white blaze on its face, wearing a bridle and a saddle. The number \"8\" is attached to the saddle. The background features a dirt track with a yellow and blue advertisement board, a person in a yellow jacket and helmet, and a white fence. The horse is positioned near the center of the image, slightly to the left, while the person and the advertisement board are in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25588.8, "ram_available_mb": 100183.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25588.8, "ram_available_mb": 100183.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.78, "peak": 15.34, "min": 14.12}, "VDD_GPU": {"avg": 31.15, "peak": 42.94, "min": 25.6}, "VIN": {"avg": 70.63, "peak": 118.95, "min": 55.05}, "VDD_CPU_SOC_MSS": {"avg": 15.23, "peak": 16.14, "min": 14.56}}, "power_watts_avg": 31.15, "energy_joules_est": 149.17, "sample_count": 37, "duration_seconds": 4.789}, "timestamp": "2026-01-16T16:06:04.105987"}
{"image_index": 271, "image_name": "000000029187.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000029187.jpg", "image_width": 640, "image_height": 491, "image_resolution": "640x491", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 4986.118, "latencies_ms": [4986.118], "images_per_second": 0.201, "prompt_tokens": 21, "response_tokens_est": 100, "n_tiles": 12, "output_text": "The image depicts a horse race track with a brown horse wearing a bridle and a number 8 on its saddle. The horse is trotting on the track, and a person in a yellow jacket and helmet is riding the horse. The background features a yellow and black sign with the word \"Magnum\" and a blue and white sign with the word \"VINSTONE\" visible. The setting appears to be a sunny day, and the track is surrounded by grassy areas.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25588.8, "ram_available_mb": 100183.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25588.5, "ram_available_mb": 100183.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.72, "peak": 15.34, "min": 14.12}, "VDD_GPU": {"avg": 30.81, "peak": 42.53, "min": 25.6}, "VIN": {"avg": 71.03, "peak": 112.95, "min": 57.06}, "VDD_CPU_SOC_MSS": {"avg": 15.18, "peak": 16.14, "min": 14.56}}, "power_watts_avg": 30.81, "energy_joules_est": 153.63, "sample_count": 39, "duration_seconds": 4.986}, "timestamp": "2026-01-16T16:06:09.098364"}
{"image_index": 271, "image_name": "000000029187.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000029187.jpg", "image_width": 640, "image_height": 491, "image_resolution": "640x491", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3329.312, "latencies_ms": [3329.312], "images_per_second": 0.3, "prompt_tokens": 19, "response_tokens_est": 53, "n_tiles": 12, "output_text": "The horse in the image is brown with a white blaze on its face and a black bridle. The horse is wearing a red saddle and a white number tag. The lighting is bright, indicating it is daytime, and the weather appears to be clear.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25588.5, "ram_available_mb": 100183.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 25588.5, "ram_available_mb": 100183.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.74, "peak": 15.34, "min": 14.12}, "VDD_GPU": {"avg": 33.49, "peak": 42.53, "min": 26.0}, "VIN": {"avg": 73.9, "peak": 122.29, "min": 58.39}, "VDD_CPU_SOC_MSS": {"avg": 15.28, "peak": 16.14, "min": 14.56}}, "power_watts_avg": 33.49, "energy_joules_est": 111.53, "sample_count": 26, "duration_seconds": 3.33}, "timestamp": "2026-01-16T16:06:12.434501"}
{"image_index": 272, "image_name": "000000029393.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000029393.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2020.115, "latencies_ms": [2020.115], "images_per_second": 0.495, "prompt_tokens": 9, "response_tokens_est": 18, "n_tiles": 12, "output_text": "A brown dog is standing on a blue ledge, looking directly at the camera.", "error": null, "sys_before": {"cpu_percent": 47.1, "ram_used_mb": 25588.5, "ram_available_mb": 100183.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 2.5, "ram_used_mb": 25588.5, "ram_available_mb": 100183.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.04, "peak": 16.15, "min": 13.49}, "VDD_GPU": {"avg": 36.34, "peak": 40.97, "min": 29.94}, "VIN": {"avg": 76.29, "peak": 116.83, "min": 58.56}, "VDD_CPU_SOC_MSS": {"avg": 15.93, "peak": 17.32, "min": 14.57}}, "power_watts_avg": 36.34, "energy_joules_est": 73.43, "sample_count": 15, "duration_seconds": 2.021}, "timestamp": "2026-01-16T16:06:14.553552"}
{"image_index": 272, "image_name": "000000029393.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000029393.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2432.981, "latencies_ms": [2432.981], "images_per_second": 0.411, "prompt_tokens": 23, "response_tokens_est": 27, "n_tiles": 12, "output_text": "dog: 1\nlemon: 1\ntree: 1\nfence: 1\nbench: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25588.5, "ram_available_mb": 100183.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 2.6, "ram_used_mb": 25588.5, "ram_available_mb": 100183.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.1, "peak": 15.65, "min": 14.63}, "VDD_GPU": {"avg": 36.63, "peak": 43.73, "min": 27.97}, "VIN": {"avg": 73.88, "peak": 113.55, "min": 58.29}, "VDD_CPU_SOC_MSS": {"avg": 15.74, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 36.63, "energy_joules_est": 89.14, "sample_count": 19, "duration_seconds": 2.434}, "timestamp": "2026-01-16T16:06:16.993349"}
{"image_index": 272, "image_name": "000000029393.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000029393.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3747.122, "latencies_ms": [3747.122], "images_per_second": 0.267, "prompt_tokens": 27, "response_tokens_est": 66, "n_tiles": 12, "output_text": "The main object in the foreground is a brown dog standing on a blue ledge. The dog is positioned near the left side of the image. In the background, there is a wooden fence and a tree with green leaves. The fence is located behind the tree, and the tree is situated to the right of the fence.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25588.5, "ram_available_mb": 100183.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25588.5, "ram_available_mb": 100183.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.09, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 32.9, "peak": 42.54, "min": 26.39}, "VIN": {"avg": 70.01, "peak": 93.79, "min": 57.0}, "VDD_CPU_SOC_MSS": {"avg": 15.72, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 32.9, "energy_joules_est": 123.29, "sample_count": 29, "duration_seconds": 3.747}, "timestamp": "2026-01-16T16:06:20.746867"}
{"image_index": 272, "image_name": "000000029393.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000029393.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3474.614, "latencies_ms": [3474.614], "images_per_second": 0.288, "prompt_tokens": 21, "response_tokens_est": 58, "n_tiles": 12, "output_text": "The image depicts a serene outdoor scene with a brown dog standing on a stone ledge, surrounded by lush greenery and a wooden fence. The dog appears to be looking directly at the camera, while the sunlight casts shadows on the ground, creating a peaceful and natural atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25588.5, "ram_available_mb": 100183.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 25588.7, "ram_available_mb": 100183.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.12, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 33.47, "peak": 42.54, "min": 26.39}, "VIN": {"avg": 76.01, "peak": 112.69, "min": 61.64}, "VDD_CPU_SOC_MSS": {"avg": 15.62, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 33.47, "energy_joules_est": 116.31, "sample_count": 26, "duration_seconds": 3.475}, "timestamp": "2026-01-16T16:06:24.227902"}
{"image_index": 272, "image_name": "000000029393.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000029393.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3176.875, "latencies_ms": [3176.875], "images_per_second": 0.315, "prompt_tokens": 19, "response_tokens_est": 49, "n_tiles": 12, "output_text": "The image features a dog standing on a blue ledge, with its fur appearing light brown and white. The dog is surrounded by lush green foliage, and the scene is bathed in bright sunlight, casting shadows on the ground.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25588.7, "ram_available_mb": 100183.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 25588.5, "ram_available_mb": 100183.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.09, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 34.19, "peak": 42.94, "min": 26.39}, "VIN": {"avg": 73.35, "peak": 118.97, "min": 54.45}, "VDD_CPU_SOC_MSS": {"avg": 15.66, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 34.19, "energy_joules_est": 108.63, "sample_count": 24, "duration_seconds": 3.177}, "timestamp": "2026-01-16T16:06:27.411821"}
{"image_index": 273, "image_name": "000000029397.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000029397.jpg", "image_width": 640, "image_height": 449, "image_resolution": "640x449", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1306.491, "latencies_ms": [1306.491], "images_per_second": 0.765, "prompt_tokens": 9, "response_tokens_est": 27, "n_tiles": 6, "output_text": "The image shows a wooden bench with a blue sign attached to its side, placed on a red brick surface against a concrete wall.", "error": null, "sys_before": {"cpu_percent": 36.8, "ram_used_mb": 25588.5, "ram_available_mb": 100183.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 25588.5, "ram_available_mb": 100183.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.38, "peak": 14.91, "min": 13.82}, "VDD_GPU": {"avg": 32.38, "peak": 37.8, "min": 26.39}, "VIN": {"avg": 75.67, "peak": 121.25, "min": 54.29}, "VDD_CPU_SOC_MSS": {"avg": 15.0, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 32.38, "energy_joules_est": 42.31, "sample_count": 10, "duration_seconds": 1.307}, "timestamp": "2026-01-16T16:06:28.788650"}
{"image_index": 273, "image_name": "000000029397.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000029397.jpg", "image_width": 640, "image_height": 449, "image_resolution": "640x449", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1693.519, "latencies_ms": [1693.519], "images_per_second": 0.59, "prompt_tokens": 23, "response_tokens_est": 40, "n_tiles": 6, "output_text": "object: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25588.5, "ram_available_mb": 100183.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 25588.7, "ram_available_mb": 100183.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.76, "peak": 15.11, "min": 14.22}, "VDD_GPU": {"avg": 32.13, "peak": 38.59, "min": 25.21}, "VIN": {"avg": 75.99, "peak": 116.09, "min": 59.96}, "VDD_CPU_SOC_MSS": {"avg": 15.39, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 32.13, "energy_joules_est": 54.43, "sample_count": 12, "duration_seconds": 1.694}, "timestamp": "2026-01-16T16:06:30.488270"}
{"image_index": 273, "image_name": "000000029397.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000029397.jpg", "image_width": 640, "image_height": 449, "image_resolution": "640x449", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2094.58, "latencies_ms": [2094.58], "images_per_second": 0.477, "prompt_tokens": 27, "response_tokens_est": 55, "n_tiles": 6, "output_text": "The main object in the foreground is a wooden bench with a blue sign attached to its side. The bench is positioned on a red brick floor, and the sign is placed near the bench's base. The background features a concrete wall, which is slightly out of focus.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25588.7, "ram_available_mb": 100183.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 25588.5, "ram_available_mb": 100183.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.9, "peak": 15.22, "min": 14.22}, "VDD_GPU": {"avg": 30.08, "peak": 38.21, "min": 24.03}, "VIN": {"avg": 69.68, "peak": 117.17, "min": 60.89}, "VDD_CPU_SOC_MSS": {"avg": 15.48, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 30.08, "energy_joules_est": 63.01, "sample_count": 16, "duration_seconds": 2.095}, "timestamp": "2026-01-16T16:06:32.588769"}
{"image_index": 273, "image_name": "000000029397.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000029397.jpg", "image_width": 640, "image_height": 449, "image_resolution": "640x449", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2504.485, "latencies_ms": [2504.485], "images_per_second": 0.399, "prompt_tokens": 21, "response_tokens_est": 70, "n_tiles": 6, "output_text": "The image depicts a wooden bench placed on a brick-paved surface, likely outdoors. The bench has a rustic appearance with visible wood grain and metal pegs for support. A blue sign with handwritten text is attached to the bench, although the content of the text is not entirely clear. The setting suggests a casual, possibly urban environment.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25588.5, "ram_available_mb": 100183.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 25589.0, "ram_available_mb": 100183.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.97, "peak": 15.32, "min": 14.32}, "VDD_GPU": {"avg": 28.84, "peak": 37.82, "min": 23.64}, "VIN": {"avg": 71.15, "peak": 110.29, "min": 55.99}, "VDD_CPU_SOC_MSS": {"avg": 15.5, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 28.84, "energy_joules_est": 72.24, "sample_count": 19, "duration_seconds": 2.505}, "timestamp": "2026-01-16T16:06:35.099241"}
{"image_index": 273, "image_name": "000000029397.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000029397.jpg", "image_width": 640, "image_height": 449, "image_resolution": "640x449", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1975.345, "latencies_ms": [1975.345], "images_per_second": 0.506, "prompt_tokens": 19, "response_tokens_est": 51, "n_tiles": 6, "output_text": "The image features a wooden bench with a rustic appearance, painted in a warm, natural brown color. The bench is placed on a red brick surface, and the lighting is soft and diffused, casting gentle shadows and highlighting the texture of the wood.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25589.0, "ram_available_mb": 100183.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 25589.0, "ram_available_mb": 100183.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.95, "peak": 15.32, "min": 14.32}, "VDD_GPU": {"avg": 30.58, "peak": 38.19, "min": 24.42}, "VIN": {"avg": 77.29, "peak": 110.79, "min": 61.28}, "VDD_CPU_SOC_MSS": {"avg": 15.52, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 30.58, "energy_joules_est": 60.42, "sample_count": 14, "duration_seconds": 1.976}, "timestamp": "2026-01-16T16:06:37.080736"}
{"image_index": 274, "image_name": "000000029596.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000029596.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1837.026, "latencies_ms": [1837.026], "images_per_second": 0.544, "prompt_tokens": 9, "response_tokens_est": 47, "n_tiles": 6, "output_text": "The image depicts a well-appointed hotel room with a cozy, inviting atmosphere, featuring a red sofa, a wooden dining table with a white tablecloth, a wooden cabinet, and a television mounted on the wall.", "error": null, "sys_before": {"cpu_percent": 44.3, "ram_used_mb": 25589.0, "ram_available_mb": 100183.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 25588.7, "ram_available_mb": 100183.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.65, "peak": 15.11, "min": 13.92}, "VDD_GPU": {"avg": 29.91, "peak": 37.03, "min": 24.42}, "VIN": {"avg": 70.78, "peak": 124.1, "min": 51.91}, "VDD_CPU_SOC_MSS": {"avg": 15.38, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 29.91, "energy_joules_est": 54.97, "sample_count": 14, "duration_seconds": 1.838}, "timestamp": "2026-01-16T16:06:38.977656"}
{"image_index": 274, "image_name": "000000029596.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000029596.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1703.179, "latencies_ms": [1703.179], "images_per_second": 0.587, "prompt_tokens": 23, "response_tokens_est": 40, "n_tiles": 6, "output_text": "object: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25588.7, "ram_available_mb": 100183.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 25588.7, "ram_available_mb": 100183.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.81, "peak": 15.11, "min": 14.32}, "VDD_GPU": {"avg": 31.41, "peak": 38.59, "min": 24.82}, "VIN": {"avg": 75.9, "peak": 104.83, "min": 57.8}, "VDD_CPU_SOC_MSS": {"avg": 15.35, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 31.41, "energy_joules_est": 53.51, "sample_count": 12, "duration_seconds": 1.704}, "timestamp": "2026-01-16T16:06:40.690957"}
{"image_index": 274, "image_name": "000000029596.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000029596.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3395.896, "latencies_ms": [3395.896], "images_per_second": 0.294, "prompt_tokens": 27, "response_tokens_est": 103, "n_tiles": 6, "output_text": "The main objects in the image are arranged in a way that showcases a cozy and well-organized living space. The foreground features a red sofa with a beige pillow, positioned near a wooden side table with a lamp. In the background, there is a large window with curtains, allowing natural light to illuminate the room. To the right, a wooden cabinet with a television on top is visible. The overall arrangement creates a sense of comfort and functionality, making the space inviting and easy to navigate.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25588.7, "ram_available_mb": 100183.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 25588.9, "ram_available_mb": 100183.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.02, "peak": 15.22, "min": 14.22}, "VDD_GPU": {"avg": 27.43, "peak": 38.6, "min": 23.64}, "VIN": {"avg": 66.37, "peak": 101.47, "min": 55.28}, "VDD_CPU_SOC_MSS": {"avg": 15.54, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 27.43, "energy_joules_est": 93.16, "sample_count": 27, "duration_seconds": 3.396}, "timestamp": "2026-01-16T16:06:44.092946"}
{"image_index": 274, "image_name": "000000029596.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000029596.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3175.767, "latencies_ms": [3175.767], "images_per_second": 0.315, "prompt_tokens": 21, "response_tokens_est": 95, "n_tiles": 6, "output_text": "The image depicts a well-appointed hotel room with a cozy and inviting atmosphere. The room features a red sofa, a wooden dining table with a white tablecloth, a wooden cabinet, and a television on the wall. The room is well-lit with natural light streaming in through a window dressed with curtains, and there are decorative items such as a vase and a lamp on the table. The overall setting suggests a comfortable and relaxing environment for guests.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25588.9, "ram_available_mb": 100183.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 25588.7, "ram_available_mb": 100183.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.02, "peak": 15.22, "min": 14.32}, "VDD_GPU": {"avg": 27.66, "peak": 38.21, "min": 23.64}, "VIN": {"avg": 69.54, "peak": 108.01, "min": 59.6}, "VDD_CPU_SOC_MSS": {"avg": 15.6, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 27.66, "energy_joules_est": 87.85, "sample_count": 24, "duration_seconds": 3.176}, "timestamp": "2026-01-16T16:06:47.274498"}
{"image_index": 274, "image_name": "000000029596.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000029596.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1672.943, "latencies_ms": [1672.943], "images_per_second": 0.598, "prompt_tokens": 19, "response_tokens_est": 39, "n_tiles": 6, "output_text": "The room is warmly lit with natural light streaming through large windows covered with sheer curtains. The walls are painted in a soft yellow hue, complemented by a wooden dining table and chairs.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25588.7, "ram_available_mb": 100183.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25588.9, "ram_available_mb": 100183.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.79, "peak": 15.11, "min": 14.22}, "VDD_GPU": {"avg": 31.54, "peak": 37.82, "min": 25.21}, "VIN": {"avg": 68.05, "peak": 110.63, "min": 55.41}, "VDD_CPU_SOC_MSS": {"avg": 15.32, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 31.54, "energy_joules_est": 52.78, "sample_count": 12, "duration_seconds": 1.674}, "timestamp": "2026-01-16T16:06:48.953718"}
{"image_index": 275, "image_name": "000000029640.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000029640.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2121.55, "latencies_ms": [2121.55], "images_per_second": 0.471, "prompt_tokens": 9, "response_tokens_est": 55, "n_tiles": 6, "output_text": "The image shows a well-prepared and colorful vegetable and meat stir-fry in a black wok, with a variety of vegetables including broccoli, carrots, and zucchini, and pieces of ham or bacon, all cooked together in a savory sauce.", "error": null, "sys_before": {"cpu_percent": 43.2, "ram_used_mb": 25588.7, "ram_available_mb": 100183.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 25588.7, "ram_available_mb": 100183.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.56, "peak": 15.01, "min": 13.82}, "VDD_GPU": {"avg": 28.9, "peak": 37.42, "min": 23.24}, "VIN": {"avg": 69.64, "peak": 112.9, "min": 54.69}, "VDD_CPU_SOC_MSS": {"avg": 15.23, "peak": 15.75, "min": 14.57}}, "power_watts_avg": 28.9, "energy_joules_est": 61.33, "sample_count": 16, "duration_seconds": 2.122}, "timestamp": "2026-01-16T16:06:51.134102"}
{"image_index": 275, "image_name": "000000029640.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000029640.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1491.465, "latencies_ms": [1491.465], "images_per_second": 0.67, "prompt_tokens": 23, "response_tokens_est": 31, "n_tiles": 6, "output_text": "broccoli: 10\ncarrots: 3\nonions: 1\nsausage: 1\npotatoes: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25588.7, "ram_available_mb": 100183.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 25588.7, "ram_available_mb": 100183.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.57, "peak": 14.81, "min": 14.02}, "VDD_GPU": {"avg": 32.23, "peak": 38.59, "min": 25.6}, "VIN": {"avg": 72.67, "peak": 98.58, "min": 62.91}, "VDD_CPU_SOC_MSS": {"avg": 15.14, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 32.23, "energy_joules_est": 48.08, "sample_count": 11, "duration_seconds": 1.492}, "timestamp": "2026-01-16T16:06:52.632038"}
{"image_index": 275, "image_name": "000000029640.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000029640.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2861.921, "latencies_ms": [2861.921], "images_per_second": 0.349, "prompt_tokens": 27, "response_tokens_est": 80, "n_tiles": 6, "output_text": "The main object in the foreground is a black frying pan containing a colorful and well-arranged salad. The salad includes green broccoli, red bell peppers, orange carrots, and pieces of ham. The pan is placed on a dark surface, likely a countertop, and there is a metal utensil partially visible in the background, suggesting that the salad is being prepared or served.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25588.7, "ram_available_mb": 100183.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 25588.7, "ram_available_mb": 100183.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.78, "peak": 15.11, "min": 14.02}, "VDD_GPU": {"avg": 28.11, "peak": 38.59, "min": 23.24}, "VIN": {"avg": 70.25, "peak": 115.79, "min": 57.18}, "VDD_CPU_SOC_MSS": {"avg": 15.28, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 28.11, "energy_joules_est": 80.46, "sample_count": 22, "duration_seconds": 2.862}, "timestamp": "2026-01-16T16:06:55.500080"}
{"image_index": 275, "image_name": "000000029640.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000029640.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2096.933, "latencies_ms": [2096.933], "images_per_second": 0.477, "prompt_tokens": 21, "response_tokens_est": 53, "n_tiles": 6, "output_text": "The image shows a close-up view of a black frying pan containing a colorful and nutritious vegetable and meat mixture. The pan is on a stove, and a metal spatula is partially visible, suggesting that the dish is being prepared or cooked.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25588.7, "ram_available_mb": 100183.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 25588.7, "ram_available_mb": 100183.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.73, "peak": 15.01, "min": 14.12}, "VDD_GPU": {"avg": 29.57, "peak": 38.19, "min": 23.64}, "VIN": {"avg": 70.94, "peak": 99.37, "min": 63.2}, "VDD_CPU_SOC_MSS": {"avg": 15.28, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 29.57, "energy_joules_est": 62.02, "sample_count": 16, "duration_seconds": 2.097}, "timestamp": "2026-01-16T16:06:57.603725"}
{"image_index": 275, "image_name": "000000029640.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000029640.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1707.861, "latencies_ms": [1707.861], "images_per_second": 0.586, "prompt_tokens": 19, "response_tokens_est": 39, "n_tiles": 6, "output_text": "The image showcases a vibrant and colorful dish of broccoli, ham, and other vegetables in a black frying pan. The lighting is bright, highlighting the fresh colors of the ingredients.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25588.7, "ram_available_mb": 100183.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25588.7, "ram_available_mb": 100183.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.62, "peak": 14.91, "min": 14.02}, "VDD_GPU": {"avg": 30.94, "peak": 38.21, "min": 24.43}, "VIN": {"avg": 66.42, "peak": 83.34, "min": 55.62}, "VDD_CPU_SOC_MSS": {"avg": 15.17, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 30.94, "energy_joules_est": 52.86, "sample_count": 13, "duration_seconds": 1.708}, "timestamp": "2026-01-16T16:06:59.317714"}
{"image_index": 276, "image_name": "000000029675.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000029675.jpg", "image_width": 478, "image_height": 640, "image_resolution": "478x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2184.359, "latencies_ms": [2184.359], "images_per_second": 0.458, "prompt_tokens": 9, "response_tokens_est": 23, "n_tiles": 12, "output_text": "The image shows a close-up of two hot dogs with mustard on them, placed on a dark plate.", "error": null, "sys_before": {"cpu_percent": 44.4, "ram_used_mb": 25588.7, "ram_available_mb": 100183.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 2.6, "ram_used_mb": 25588.7, "ram_available_mb": 100183.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.9, "peak": 15.85, "min": 13.79}, "VDD_GPU": {"avg": 35.94, "peak": 40.97, "min": 28.76}, "VIN": {"avg": 76.35, "peak": 123.32, "min": 58.07}, "VDD_CPU_SOC_MSS": {"avg": 15.77, "peak": 16.92, "min": 14.96}}, "power_watts_avg": 35.94, "energy_joules_est": 78.52, "sample_count": 16, "duration_seconds": 2.185}, "timestamp": "2026-01-16T16:07:01.590841"}
{"image_index": 276, "image_name": "000000029675.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000029675.jpg", "image_width": 478, "image_height": 640, "image_resolution": "478x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2375.846, "latencies_ms": [2375.846], "images_per_second": 0.421, "prompt_tokens": 23, "response_tokens_est": 25, "n_tiles": 12, "output_text": "hot dog: 2\nbun: 2\nketchup: 2\nmustard: 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25588.7, "ram_available_mb": 100183.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 2.6, "ram_used_mb": 25588.7, "ram_available_mb": 100183.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.74, "peak": 15.34, "min": 14.12}, "VDD_GPU": {"avg": 36.9, "peak": 43.33, "min": 28.36}, "VIN": {"avg": 77.91, "peak": 119.34, "min": 58.3}, "VDD_CPU_SOC_MSS": {"avg": 15.35, "peak": 16.14, "min": 14.56}}, "power_watts_avg": 36.9, "energy_joules_est": 87.69, "sample_count": 18, "duration_seconds": 2.376}, "timestamp": "2026-01-16T16:07:03.973078"}
{"image_index": 276, "image_name": "000000029675.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000029675.jpg", "image_width": 478, "image_height": 640, "image_resolution": "478x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3684.567, "latencies_ms": [3684.567], "images_per_second": 0.271, "prompt_tokens": 27, "response_tokens_est": 63, "n_tiles": 12, "output_text": "The main objects in the image are two hot dogs placed on a dark plate. The hot dogs are positioned in the foreground, with one hot dog slightly overlapping the other. The background features a dark surface, possibly a table or countertop, which provides a contrasting backdrop to the brightly colored hot dogs.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25588.7, "ram_available_mb": 100183.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25588.7, "ram_available_mb": 100183.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.73, "peak": 15.34, "min": 14.12}, "VDD_GPU": {"avg": 32.87, "peak": 42.92, "min": 26.0}, "VIN": {"avg": 72.55, "peak": 116.8, "min": 56.68}, "VDD_CPU_SOC_MSS": {"avg": 15.21, "peak": 16.14, "min": 14.56}}, "power_watts_avg": 32.87, "energy_joules_est": 121.12, "sample_count": 28, "duration_seconds": 3.685}, "timestamp": "2026-01-16T16:07:07.669656"}
{"image_index": 276, "image_name": "000000029675.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000029675.jpg", "image_width": 478, "image_height": 640, "image_resolution": "478x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2993.802, "latencies_ms": [2993.802], "images_per_second": 0.334, "prompt_tokens": 21, "response_tokens_est": 43, "n_tiles": 12, "output_text": "The image shows a close-up of two hot dogs placed on a dark surface, possibly a table. The hot dogs are topped with yellow mustard and appear to be freshly cooked, with a crispy bun.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25588.7, "ram_available_mb": 100183.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 25588.7, "ram_available_mb": 100183.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.72, "peak": 15.34, "min": 14.12}, "VDD_GPU": {"avg": 34.34, "peak": 42.54, "min": 26.39}, "VIN": {"avg": 73.03, "peak": 117.82, "min": 56.78}, "VDD_CPU_SOC_MSS": {"avg": 15.23, "peak": 16.14, "min": 14.56}}, "power_watts_avg": 34.34, "energy_joules_est": 102.82, "sample_count": 23, "duration_seconds": 2.994}, "timestamp": "2026-01-16T16:07:10.670089"}
{"image_index": 276, "image_name": "000000029675.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000029675.jpg", "image_width": 478, "image_height": 640, "image_resolution": "478x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3402.07, "latencies_ms": [3402.07], "images_per_second": 0.294, "prompt_tokens": 19, "response_tokens_est": 55, "n_tiles": 12, "output_text": "The image features a hot dog with a yellow mustard and ketchup topping, placed on a dark plate. The lighting is soft and natural, casting a warm glow on the food. The background is blurred, emphasizing the hot dog as the focal point.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25588.7, "ram_available_mb": 100183.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 25588.4, "ram_available_mb": 100183.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.73, "peak": 15.34, "min": 14.12}, "VDD_GPU": {"avg": 33.4, "peak": 42.92, "min": 26.0}, "VIN": {"avg": 75.47, "peak": 120.72, "min": 59.08}, "VDD_CPU_SOC_MSS": {"avg": 15.23, "peak": 16.14, "min": 14.56}}, "power_watts_avg": 33.4, "energy_joules_est": 113.65, "sample_count": 26, "duration_seconds": 3.403}, "timestamp": "2026-01-16T16:07:14.083021"}
{"image_index": 277, "image_name": "000000029984.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000029984.jpg", "image_width": 640, "image_height": 492, "image_resolution": "640x492", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2185.409, "latencies_ms": [2185.409], "images_per_second": 0.458, "prompt_tokens": 9, "response_tokens_est": 23, "n_tiles": 12, "output_text": "A group of people are swimming in the ocean, with a green umbrella providing shade on the sandy beach.", "error": null, "sys_before": {"cpu_percent": 46.0, "ram_used_mb": 25588.4, "ram_available_mb": 100183.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 2.6, "ram_used_mb": 25588.4, "ram_available_mb": 100183.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.04, "peak": 16.05, "min": 13.49}, "VDD_GPU": {"avg": 35.66, "peak": 41.36, "min": 28.37}, "VIN": {"avg": 76.9, "peak": 109.01, "min": 59.01}, "VDD_CPU_SOC_MSS": {"avg": 15.77, "peak": 16.92, "min": 14.57}}, "power_watts_avg": 35.66, "energy_joules_est": 77.94, "sample_count": 17, "duration_seconds": 2.186}, "timestamp": "2026-01-16T16:07:16.362450"}
{"image_index": 277, "image_name": "000000029984.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000029984.jpg", "image_width": 640, "image_height": 492, "image_resolution": "640x492", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2877.777, "latencies_ms": [2877.777], "images_per_second": 0.347, "prompt_tokens": 23, "response_tokens_est": 40, "n_tiles": 12, "output_text": "object: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25588.4, "ram_available_mb": 100183.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 25588.4, "ram_available_mb": 100183.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.07, "peak": 15.65, "min": 14.43}, "VDD_GPU": {"avg": 34.92, "peak": 43.33, "min": 26.79}, "VIN": {"avg": 77.06, "peak": 124.7, "min": 58.75}, "VDD_CPU_SOC_MSS": {"avg": 15.64, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 34.92, "energy_joules_est": 100.52, "sample_count": 23, "duration_seconds": 2.879}, "timestamp": "2026-01-16T16:07:19.247035"}
{"image_index": 277, "image_name": "000000029984.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000029984.jpg", "image_width": 640, "image_height": 492, "image_resolution": "640x492", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3625.832, "latencies_ms": [3625.832], "images_per_second": 0.276, "prompt_tokens": 27, "response_tokens_est": 62, "n_tiles": 12, "output_text": "The main objects in the image are a beach umbrella, a beach chair, and the ocean. The beach umbrella is located in the foreground, near the sandy shore. The beach chair is positioned in the middle ground, near the ocean. The ocean is in the background, stretching out to the horizon.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25588.4, "ram_available_mb": 100183.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25588.7, "ram_available_mb": 100183.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.07, "peak": 15.65, "min": 14.43}, "VDD_GPU": {"avg": 33.17, "peak": 42.54, "min": 26.39}, "VIN": {"avg": 74.17, "peak": 109.59, "min": 58.58}, "VDD_CPU_SOC_MSS": {"avg": 15.63, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 33.17, "energy_joules_est": 120.29, "sample_count": 28, "duration_seconds": 3.627}, "timestamp": "2026-01-16T16:07:22.879952"}
{"image_index": 277, "image_name": "000000029984.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000029984.jpg", "image_width": 640, "image_height": 492, "image_resolution": "640x492", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3518.323, "latencies_ms": [3518.323], "images_per_second": 0.284, "prompt_tokens": 21, "response_tokens_est": 59, "n_tiles": 12, "output_text": "The image depicts a serene beach scene with a calm ocean and a sandy shore. There are several people in the water, enjoying the sun and the sea breeze. In the foreground, there are colorful beach chairs and umbrellas, suggesting a leisurely day at the beach.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25588.7, "ram_available_mb": 100183.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 25588.4, "ram_available_mb": 100183.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.09, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 33.31, "peak": 42.94, "min": 26.39}, "VIN": {"avg": 74.71, "peak": 113.43, "min": 58.82}, "VDD_CPU_SOC_MSS": {"avg": 15.66, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 33.31, "energy_joules_est": 117.2, "sample_count": 27, "duration_seconds": 3.519}, "timestamp": "2026-01-16T16:07:26.404550"}
{"image_index": 277, "image_name": "000000029984.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000029984.jpg", "image_width": 640, "image_height": 492, "image_resolution": "640x492", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3347.381, "latencies_ms": [3347.381], "images_per_second": 0.299, "prompt_tokens": 19, "response_tokens_est": 54, "n_tiles": 12, "output_text": "The image depicts a beach scene with a clear sky and calm ocean waters. The sandy shore is visible, and there are several beach chairs with colorful covers, including pink, blue, and green. The lighting is bright and natural, suggesting it is daytime.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25588.4, "ram_available_mb": 100183.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 25588.4, "ram_available_mb": 100183.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.07, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 33.51, "peak": 42.54, "min": 26.39}, "VIN": {"avg": 77.57, "peak": 129.39, "min": 59.33}, "VDD_CPU_SOC_MSS": {"avg": 15.64, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 33.51, "energy_joules_est": 112.19, "sample_count": 26, "duration_seconds": 3.348}, "timestamp": "2026-01-16T16:07:29.758314"}
{"image_index": 278, "image_name": "000000030213.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000030213.jpg", "image_width": 640, "image_height": 449, "image_resolution": "640x449", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1897.282, "latencies_ms": [1897.282], "images_per_second": 0.527, "prompt_tokens": 9, "response_tokens_est": 49, "n_tiles": 6, "output_text": "The image depicts a vintage kitchen with a retro design, featuring a green-painted wooden cabinet, a white sink with a silver faucet, a wooden table with various items, and a basket of fruit on top of a white cabinet.", "error": null, "sys_before": {"cpu_percent": 25.4, "ram_used_mb": 25588.4, "ram_available_mb": 100183.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 25588.4, "ram_available_mb": 100183.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.63, "peak": 15.22, "min": 13.92}, "VDD_GPU": {"avg": 30.33, "peak": 37.82, "min": 24.42}, "VIN": {"avg": 72.87, "peak": 117.83, "min": 57.85}, "VDD_CPU_SOC_MSS": {"avg": 15.21, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 30.33, "energy_joules_est": 57.57, "sample_count": 14, "duration_seconds": 1.898}, "timestamp": "2026-01-16T16:07:31.703469"}
{"image_index": 278, "image_name": "000000030213.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000030213.jpg", "image_width": 640, "image_height": 449, "image_resolution": "640x449", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 4057.607, "latencies_ms": [4057.607], "images_per_second": 0.246, "prompt_tokens": 23, "response_tokens_est": 129, "n_tiles": 6, "output_text": "- Kitchen\n  - cabinets: 2\n  - drawers: 4\n  - sink: 1\n  - table: 1\n  - chairs: 2\n- Kitchen utensils\n  - kettle: 1\n  - mixer: 1\n  - bowl: 1\n  - cup: 1\n  - spoon: 1\n  - juicer: 1\n  - juicer-lid: 1\n  - juicer-base: 1\n  - juicer: 1\n  - juicer-lid: 1\n  - ju", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25588.4, "ram_available_mb": 100183.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 25588.4, "ram_available_mb": 100183.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.1, "peak": 15.32, "min": 14.32}, "VDD_GPU": {"avg": 26.8, "peak": 37.8, "min": 23.64}, "VIN": {"avg": 67.45, "peak": 85.75, "min": 60.01}, "VDD_CPU_SOC_MSS": {"avg": 15.62, "peak": 16.14, "min": 14.96}}, "power_watts_avg": 26.8, "energy_joules_est": 108.76, "sample_count": 32, "duration_seconds": 4.058}, "timestamp": "2026-01-16T16:07:35.767226"}
{"image_index": 278, "image_name": "000000030213.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000030213.jpg", "image_width": 640, "image_height": 449, "image_resolution": "640x449", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3489.482, "latencies_ms": [3489.482], "images_per_second": 0.287, "prompt_tokens": 27, "response_tokens_est": 107, "n_tiles": 6, "output_text": "The main objects in the image are arranged in a way that showcases the spatial relationships between them. The left side of the image features a green dresser with drawers, a blue kettle, and a white microwave. In the foreground, there is a white table with a black top, a wooden chair, and a basket. The right side of the image includes a white cabinet with a mirror above it, a white sink, and a basket of fruit. The background is filled with a patterned wallpaper and a wooden cabinet.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25588.4, "ram_available_mb": 100183.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 25588.4, "ram_available_mb": 100183.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.07, "peak": 15.32, "min": 14.32}, "VDD_GPU": {"avg": 27.31, "peak": 38.21, "min": 23.64}, "VIN": {"avg": 70.84, "peak": 111.23, "min": 62.11}, "VDD_CPU_SOC_MSS": {"avg": 15.62, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 27.31, "energy_joules_est": 95.31, "sample_count": 27, "duration_seconds": 3.49}, "timestamp": "2026-01-16T16:07:39.263289"}
{"image_index": 278, "image_name": "000000030213.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000030213.jpg", "image_width": 640, "image_height": 449, "image_resolution": "640x449", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2388.211, "latencies_ms": [2388.211], "images_per_second": 0.419, "prompt_tokens": 21, "response_tokens_est": 66, "n_tiles": 6, "output_text": "The image depicts a vintage kitchen with a retro aesthetic, featuring a green-painted wooden cabinet, a white sink with a silver faucet, and a wooden table with various items on it. The room has a patterned wallpaper with palm trees, and there are several wooden chairs and a basket of fruit on the counter.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25588.4, "ram_available_mb": 100183.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 25588.4, "ram_available_mb": 100183.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.97, "peak": 15.32, "min": 14.32}, "VDD_GPU": {"avg": 29.21, "peak": 38.19, "min": 24.03}, "VIN": {"avg": 72.72, "peak": 117.58, "min": 63.66}, "VDD_CPU_SOC_MSS": {"avg": 15.57, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 29.21, "energy_joules_est": 69.77, "sample_count": 18, "duration_seconds": 2.389}, "timestamp": "2026-01-16T16:07:41.657681"}
{"image_index": 278, "image_name": "000000030213.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000030213.jpg", "image_width": 640, "image_height": 449, "image_resolution": "640x449", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1512.922, "latencies_ms": [1512.922], "images_per_second": 0.661, "prompt_tokens": 19, "response_tokens_est": 33, "n_tiles": 6, "output_text": "The room features a vintage aesthetic with a green-painted wall adorned with palm leaf wallpaper. The lighting is warm and soft, casting a cozy ambiance.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25588.4, "ram_available_mb": 100183.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 25588.6, "ram_available_mb": 100183.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.79, "peak": 15.01, "min": 14.32}, "VDD_GPU": {"avg": 32.09, "peak": 38.21, "min": 25.6}, "VIN": {"avg": 69.07, "peak": 90.29, "min": 60.39}, "VDD_CPU_SOC_MSS": {"avg": 15.39, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 32.09, "energy_joules_est": 48.56, "sample_count": 11, "duration_seconds": 1.513}, "timestamp": "2026-01-16T16:07:43.176546"}
{"image_index": 279, "image_name": "000000030494.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000030494.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2112.094, "latencies_ms": [2112.094], "images_per_second": 0.473, "prompt_tokens": 9, "response_tokens_est": 21, "n_tiles": 12, "output_text": "A black and white dog is standing on a dirt path surrounded by fallen leaves and a tree trunk.", "error": null, "sys_before": {"cpu_percent": 50.4, "ram_used_mb": 25588.6, "ram_available_mb": 100183.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 2.4, "ram_used_mb": 25588.6, "ram_available_mb": 100183.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.87, "peak": 15.75, "min": 13.79}, "VDD_GPU": {"avg": 36.14, "peak": 41.36, "min": 28.76}, "VIN": {"avg": 78.28, "peak": 121.96, "min": 54.36}, "VDD_CPU_SOC_MSS": {"avg": 15.7, "peak": 16.92, "min": 14.96}}, "power_watts_avg": 36.14, "energy_joules_est": 76.34, "sample_count": 16, "duration_seconds": 2.112}, "timestamp": "2026-01-16T16:07:45.379432"}
{"image_index": 279, "image_name": "000000030494.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000030494.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2238.804, "latencies_ms": [2238.804], "images_per_second": 0.447, "prompt_tokens": 23, "response_tokens_est": 21, "n_tiles": 12, "output_text": "dog: 1\nleaves: 1\ntree: 1\nground: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25588.6, "ram_available_mb": 100183.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 2.5, "ram_used_mb": 25588.6, "ram_available_mb": 100183.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.75, "peak": 15.34, "min": 14.12}, "VDD_GPU": {"avg": 37.57, "peak": 43.31, "min": 29.15}, "VIN": {"avg": 79.07, "peak": 117.53, "min": 57.13}, "VDD_CPU_SOC_MSS": {"avg": 15.37, "peak": 16.14, "min": 14.56}}, "power_watts_avg": 37.57, "energy_joules_est": 84.12, "sample_count": 17, "duration_seconds": 2.239}, "timestamp": "2026-01-16T16:07:47.624582"}
{"image_index": 279, "image_name": "000000030494.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000030494.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 4160.143, "latencies_ms": [4160.143], "images_per_second": 0.24, "prompt_tokens": 27, "response_tokens_est": 77, "n_tiles": 12, "output_text": "The main object in the foreground is a dog, which is positioned near the center of the image. The dog is facing to the right, with its head turned slightly towards the camera. The background consists of a tree trunk and a leafy area, which are slightly out of focus. The dog is not directly near the tree trunk, but it is in close proximity to it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25588.6, "ram_available_mb": 100183.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25588.6, "ram_available_mb": 100183.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.73, "peak": 15.34, "min": 14.12}, "VDD_GPU": {"avg": 32.15, "peak": 43.31, "min": 25.6}, "VIN": {"avg": 71.49, "peak": 123.88, "min": 47.97}, "VDD_CPU_SOC_MSS": {"avg": 15.19, "peak": 16.14, "min": 14.56}}, "power_watts_avg": 32.15, "energy_joules_est": 133.76, "sample_count": 33, "duration_seconds": 4.161}, "timestamp": "2026-01-16T16:07:51.791264"}
{"image_index": 279, "image_name": "000000030494.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000030494.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2955.057, "latencies_ms": [2955.057], "images_per_second": 0.338, "prompt_tokens": 21, "response_tokens_est": 42, "n_tiles": 12, "output_text": "The image depicts a small dog in a lush, green forest setting. The dog is standing on a dirt path surrounded by fallen leaves and greenery, with its tail raised and ears perked up.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25588.6, "ram_available_mb": 100183.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 25588.6, "ram_available_mb": 100183.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.75, "peak": 15.24, "min": 14.12}, "VDD_GPU": {"avg": 34.55, "peak": 42.54, "min": 26.39}, "VIN": {"avg": 74.8, "peak": 130.44, "min": 58.33}, "VDD_CPU_SOC_MSS": {"avg": 15.35, "peak": 16.14, "min": 14.56}}, "power_watts_avg": 34.55, "energy_joules_est": 102.12, "sample_count": 22, "duration_seconds": 2.956}, "timestamp": "2026-01-16T16:07:54.753778"}
{"image_index": 279, "image_name": "000000030494.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000030494.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3540.475, "latencies_ms": [3540.475], "images_per_second": 0.282, "prompt_tokens": 19, "response_tokens_est": 59, "n_tiles": 12, "output_text": "The image depicts a dog standing on a forest floor covered with brown leaves and dirt. The dog has a black and white coat, and its fur appears to be slightly wet, suggesting recent rain. The lighting is natural, with sunlight filtering through the trees, casting shadows on the ground.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25588.6, "ram_available_mb": 100183.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25588.6, "ram_available_mb": 100183.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.73, "peak": 15.24, "min": 14.12}, "VDD_GPU": {"avg": 32.99, "peak": 42.54, "min": 26.0}, "VIN": {"avg": 69.69, "peak": 110.48, "min": 54.0}, "VDD_CPU_SOC_MSS": {"avg": 15.25, "peak": 16.14, "min": 14.56}}, "power_watts_avg": 32.99, "energy_joules_est": 116.82, "sample_count": 27, "duration_seconds": 3.541}, "timestamp": "2026-01-16T16:07:58.300689"}
{"image_index": 280, "image_name": "000000030504.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000030504.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2273.858, "latencies_ms": [2273.858], "images_per_second": 0.44, "prompt_tokens": 9, "response_tokens_est": 25, "n_tiles": 12, "output_text": "A skier is skiing down a snowy slope, wearing a helmet and goggles, and carrying a backpack.", "error": null, "sys_before": {"cpu_percent": 41.1, "ram_used_mb": 25588.6, "ram_available_mb": 100183.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 2.6, "ram_used_mb": 25588.6, "ram_available_mb": 100183.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.01, "peak": 16.05, "min": 13.49}, "VDD_GPU": {"avg": 35.59, "peak": 41.76, "min": 27.97}, "VIN": {"avg": 77.16, "peak": 116.93, "min": 60.83}, "VDD_CPU_SOC_MSS": {"avg": 15.75, "peak": 16.92, "min": 14.18}}, "power_watts_avg": 35.59, "energy_joules_est": 80.94, "sample_count": 17, "duration_seconds": 2.274}, "timestamp": "2026-01-16T16:08:00.667997"}
{"image_index": 280, "image_name": "000000030504.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000030504.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2887.662, "latencies_ms": [2887.662], "images_per_second": 0.346, "prompt_tokens": 23, "response_tokens_est": 40, "n_tiles": 12, "output_text": "1. Skis\n2. Skier\n3. Snow\n4. Snowboard\n5. Snowboarder\n6. Snow\n7. Snowboard\n8. Snowboarder", "error": null, "sys_before": {"cpu_percent": 16.7, "ram_used_mb": 25588.6, "ram_available_mb": 100183.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 25588.6, "ram_available_mb": 100183.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.76, "peak": 15.34, "min": 14.12}, "VDD_GPU": {"avg": 35.0, "peak": 42.92, "min": 26.39}, "VIN": {"avg": 75.7, "peak": 121.96, "min": 58.39}, "VDD_CPU_SOC_MSS": {"avg": 15.35, "peak": 16.14, "min": 14.56}}, "power_watts_avg": 35.0, "energy_joules_est": 101.08, "sample_count": 22, "duration_seconds": 2.888}, "timestamp": "2026-01-16T16:08:03.562055"}
{"image_index": 280, "image_name": "000000030504.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000030504.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 4266.904, "latencies_ms": [4266.904], "images_per_second": 0.234, "prompt_tokens": 27, "response_tokens_est": 80, "n_tiles": 12, "output_text": "The main object in the foreground is a person skiing, wearing green ski boots and holding ski poles. The person is positioned near the center of the image, slightly to the left. In the background, there is a snowy landscape with trees and a clear blue sky. The person's shadow is cast on the snow, indicating that the sun is shining from the right side of the image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25588.6, "ram_available_mb": 100183.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 25588.6, "ram_available_mb": 100183.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.74, "peak": 15.24, "min": 14.22}, "VDD_GPU": {"avg": 31.76, "peak": 42.53, "min": 25.6}, "VIN": {"avg": 72.03, "peak": 122.35, "min": 55.6}, "VDD_CPU_SOC_MSS": {"avg": 15.31, "peak": 16.14, "min": 14.56}}, "power_watts_avg": 31.76, "energy_joules_est": 135.53, "sample_count": 34, "duration_seconds": 4.267}, "timestamp": "2026-01-16T16:08:07.835348"}
{"image_index": 280, "image_name": "000000030504.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000030504.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3638.105, "latencies_ms": [3638.105], "images_per_second": 0.275, "prompt_tokens": 21, "response_tokens_est": 62, "n_tiles": 12, "output_text": "The image depicts a person skiing down a snowy slope. The individual is wearing a helmet, goggles, and ski gear, including a jacket and pants, and is holding ski poles. The snowy landscape is covered with tracks from previous skiers, and the sky is clear and blue.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25588.6, "ram_available_mb": 100183.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25588.6, "ram_available_mb": 100183.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.76, "peak": 15.34, "min": 14.12}, "VDD_GPU": {"avg": 32.76, "peak": 42.54, "min": 26.0}, "VIN": {"avg": 75.48, "peak": 133.14, "min": 50.62}, "VDD_CPU_SOC_MSS": {"avg": 15.24, "peak": 16.14, "min": 14.56}}, "power_watts_avg": 32.76, "energy_joules_est": 119.21, "sample_count": 28, "duration_seconds": 3.639}, "timestamp": "2026-01-16T16:08:11.480559"}
{"image_index": 280, "image_name": "000000030504.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000030504.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3332.193, "latencies_ms": [3332.193], "images_per_second": 0.3, "prompt_tokens": 19, "response_tokens_est": 53, "n_tiles": 12, "output_text": "The image depicts a person skiing in a snowy landscape. The individual is wearing a white helmet, a black jacket, and green ski boots. The snowy terrain is illuminated by bright sunlight, casting shadows and highlighting the texture of the snow.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25588.6, "ram_available_mb": 100183.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 25588.6, "ram_available_mb": 100183.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.74, "peak": 15.34, "min": 14.12}, "VDD_GPU": {"avg": 33.43, "peak": 42.53, "min": 26.0}, "VIN": {"avg": 70.34, "peak": 120.1, "min": 54.94}, "VDD_CPU_SOC_MSS": {"avg": 15.29, "peak": 16.14, "min": 14.56}}, "power_watts_avg": 33.43, "energy_joules_est": 111.41, "sample_count": 26, "duration_seconds": 3.333}, "timestamp": "2026-01-16T16:08:14.819169"}
{"image_index": 281, "image_name": "000000030675.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000030675.jpg", "image_width": 640, "image_height": 360, "image_resolution": "640x360", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1100.969, "latencies_ms": [1100.969], "images_per_second": 0.908, "prompt_tokens": 9, "response_tokens_est": 41, "n_tiles": 2, "output_text": "The image depicts a large, orange locomotive with the number 6309, labeled \"BNSF,\" traveling on a railway track surrounded by a barren landscape with leafless trees.", "error": null, "sys_before": {"cpu_percent": 28.9, "ram_used_mb": 25588.6, "ram_available_mb": 100183.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25588.8, "ram_available_mb": 100183.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5055.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.64, "peak": 15.52, "min": 13.79}, "VDD_GPU": {"avg": 26.59, "peak": 34.27, "min": 22.45}, "VIN": {"avg": 65.41, "peak": 76.0, "min": 62.86}, "VDD_CPU_SOC_MSS": {"avg": 15.35, "peak": 16.14, "min": 14.17}}, "power_watts_avg": 26.59, "energy_joules_est": 29.29, "sample_count": 8, "duration_seconds": 1.101}, "timestamp": "2026-01-16T16:08:15.952803"}
{"image_index": 281, "image_name": "000000030675.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000030675.jpg", "image_width": 640, "image_height": 360, "image_resolution": "640x360", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1089.142, "latencies_ms": [1089.142], "images_per_second": 0.918, "prompt_tokens": 23, "response_tokens_est": 40, "n_tiles": 2, "output_text": "object: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25588.8, "ram_available_mb": 100183.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25588.6, "ram_available_mb": 100183.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5064.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.24, "peak": 15.62, "min": 14.71}, "VDD_GPU": {"avg": 25.8, "peak": 31.91, "min": 22.06}, "VIN": {"avg": 67.1, "peak": 99.48, "min": 59.31}, "VDD_CPU_SOC_MSS": {"avg": 15.65, "peak": 16.14, "min": 14.96}}, "power_watts_avg": 25.8, "energy_joules_est": 28.11, "sample_count": 8, "duration_seconds": 1.089}, "timestamp": "2026-01-16T16:08:17.047543"}
{"image_index": 281, "image_name": "000000030675.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000030675.jpg", "image_width": 640, "image_height": 360, "image_resolution": "640x360", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2541.122, "latencies_ms": [2541.122], "images_per_second": 0.394, "prompt_tokens": 27, "response_tokens_est": 104, "n_tiles": 2, "output_text": "The main object in the image is a train, which is positioned in the foreground on the right side of the frame. The train is painted in orange and yellow, with the number \"6309\" prominently displayed on its side. The train is traveling on a set of railroad tracks that run parallel to the ground. In the background, there is a line of leafless trees, indicating that the scene takes place during a season when trees are not in full foliage, such as late fall or winter.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25588.6, "ram_available_mb": 100183.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25588.6, "ram_available_mb": 100183.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5067.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.43, "peak": 15.82, "min": 14.81}, "VDD_GPU": {"avg": 22.94, "peak": 31.11, "min": 20.87}, "VIN": {"avg": 66.37, "peak": 106.58, "min": 59.6}, "VDD_CPU_SOC_MSS": {"avg": 16.16, "peak": 16.54, "min": 15.35}}, "power_watts_avg": 22.94, "energy_joules_est": 58.3, "sample_count": 20, "duration_seconds": 2.542}, "timestamp": "2026-01-16T16:08:19.594700"}
{"image_index": 281, "image_name": "000000030675.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000030675.jpg", "image_width": 640, "image_height": 360, "image_resolution": "640x360", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1334.715, "latencies_ms": [1334.715], "images_per_second": 0.749, "prompt_tokens": 21, "response_tokens_est": 51, "n_tiles": 2, "output_text": "The image depicts a train traveling on a railway track, with a clear blue sky overhead and leafless trees in the background. The train is painted in bright orange and black colors, and the number \"6309\" is visible on its side.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25588.6, "ram_available_mb": 100183.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 25588.6, "ram_available_mb": 100183.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5063.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.2, "peak": 15.62, "min": 14.71}, "VDD_GPU": {"avg": 24.42, "peak": 29.93, "min": 21.28}, "VIN": {"avg": 66.89, "peak": 99.2, "min": 58.76}, "VDD_CPU_SOC_MSS": {"avg": 15.83, "peak": 16.54, "min": 15.36}}, "power_watts_avg": 24.42, "energy_joules_est": 32.61, "sample_count": 10, "duration_seconds": 1.335}, "timestamp": "2026-01-16T16:08:20.935628"}
{"image_index": 281, "image_name": "000000030675.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000030675.jpg", "image_width": 640, "image_height": 360, "image_resolution": "640x360", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1856.32, "latencies_ms": [1856.32], "images_per_second": 0.539, "prompt_tokens": 19, "response_tokens_est": 74, "n_tiles": 2, "output_text": "The image features a bright orange locomotive with black and yellow accents, prominently displaying the letters \"BNSF\" on its side. The locomotive is set against a clear blue sky, with bare trees in the background, indicating a winter or early spring season. The lighting is natural, suggesting daytime, and the overall scene is bright and clear.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25588.6, "ram_available_mb": 100183.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25588.6, "ram_available_mb": 100183.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5062.6, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.21, "peak": 15.52, "min": 14.61}, "VDD_GPU": {"avg": 23.72, "peak": 31.91, "min": 20.88}, "VIN": {"avg": 64.94, "peak": 95.39, "min": 56.52}, "VDD_CPU_SOC_MSS": {"avg": 16.09, "peak": 16.55, "min": 15.36}}, "power_watts_avg": 23.72, "energy_joules_est": 44.04, "sample_count": 14, "duration_seconds": 1.857}, "timestamp": "2026-01-16T16:08:22.797741"}
{"image_index": 282, "image_name": "000000030785.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000030785.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2780.034, "latencies_ms": [2780.034], "images_per_second": 0.36, "prompt_tokens": 9, "response_tokens_est": 41, "n_tiles": 12, "output_text": "The image shows a plate of food, including a piece of bread topped with guacamole, a bowl of broccoli, and a small bowl of what appears to be a sauce or dressing.", "error": null, "sys_before": {"cpu_percent": 46.7, "ram_used_mb": 25588.6, "ram_available_mb": 100183.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 25588.8, "ram_available_mb": 100183.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.2, "peak": 16.05, "min": 14.2}, "VDD_GPU": {"avg": 33.73, "peak": 40.57, "min": 26.79}, "VIN": {"avg": 75.08, "peak": 122.15, "min": 58.42}, "VDD_CPU_SOC_MSS": {"avg": 16.05, "peak": 17.32, "min": 15.35}}, "power_watts_avg": 33.73, "energy_joules_est": 93.8, "sample_count": 21, "duration_seconds": 2.781}, "timestamp": "2026-01-16T16:08:25.646578"}
{"image_index": 282, "image_name": "000000030785.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000030785.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2849.602, "latencies_ms": [2849.602], "images_per_second": 0.351, "prompt_tokens": 23, "response_tokens_est": 39, "n_tiles": 12, "output_text": "- plate: 1\n- bowl: 2\n- broccoli: 2\n- avocado: 1\n- bread: 1\n- lettuce: 0", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25588.8, "ram_available_mb": 100183.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 25588.6, "ram_available_mb": 100183.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.06, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 34.97, "peak": 42.94, "min": 26.79}, "VIN": {"avg": 79.06, "peak": 113.38, "min": 58.31}, "VDD_CPU_SOC_MSS": {"avg": 15.69, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 34.97, "energy_joules_est": 99.67, "sample_count": 22, "duration_seconds": 2.85}, "timestamp": "2026-01-16T16:08:28.502593"}
{"image_index": 282, "image_name": "000000030785.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000030785.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3757.166, "latencies_ms": [3757.166], "images_per_second": 0.266, "prompt_tokens": 27, "response_tokens_est": 66, "n_tiles": 12, "output_text": "The main objects in the image are a plate of food and a bowl of broccoli. The plate of food is positioned in the foreground, with the bowl of broccoli placed to the right of it. The plate of food is placed on a wooden surface, while the bowl of broccoli is also on the same surface.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25588.6, "ram_available_mb": 100183.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25588.8, "ram_available_mb": 100183.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.09, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 32.81, "peak": 42.94, "min": 26.0}, "VIN": {"avg": 76.72, "peak": 117.52, "min": 58.57}, "VDD_CPU_SOC_MSS": {"avg": 15.73, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 32.81, "energy_joules_est": 123.3, "sample_count": 30, "duration_seconds": 3.758}, "timestamp": "2026-01-16T16:08:32.266612"}
{"image_index": 282, "image_name": "000000030785.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000030785.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3552.917, "latencies_ms": [3552.917], "images_per_second": 0.281, "prompt_tokens": 21, "response_tokens_est": 60, "n_tiles": 12, "output_text": "The image depicts a plate of food arranged on a wooden table. The plate contains a portion of avocado, a serving of broccoli, and a small bowl of what appears to be a sauce or dressing. The setting suggests a casual dining environment, possibly a home or a casual restaurant.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25588.8, "ram_available_mb": 100183.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25588.8, "ram_available_mb": 100183.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.08, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 33.06, "peak": 42.54, "min": 26.39}, "VIN": {"avg": 71.47, "peak": 100.2, "min": 58.55}, "VDD_CPU_SOC_MSS": {"avg": 15.7, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 33.06, "energy_joules_est": 117.48, "sample_count": 27, "duration_seconds": 3.554}, "timestamp": "2026-01-16T16:08:35.827004"}
{"image_index": 282, "image_name": "000000030785.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000030785.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 4190.369, "latencies_ms": [4190.369], "images_per_second": 0.239, "prompt_tokens": 19, "response_tokens_est": 79, "n_tiles": 12, "output_text": "The image features a plate of food with a light blue rim and a white interior. The food includes a piece of bread topped with a green spread, broccoli, and a small bowl of what appears to be a sauce or dressing. The lighting is warm and soft, casting a gentle glow on the food, and the wooden surface beneath the plate adds a rustic touch to the overall presentation.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25588.8, "ram_available_mb": 100183.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25588.8, "ram_available_mb": 100183.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.09, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 32.04, "peak": 42.53, "min": 26.0}, "VIN": {"avg": 73.34, "peak": 117.2, "min": 58.47}, "VDD_CPU_SOC_MSS": {"avg": 15.66, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 32.04, "energy_joules_est": 134.29, "sample_count": 33, "duration_seconds": 4.191}, "timestamp": "2026-01-16T16:08:40.024447"}
{"image_index": 283, "image_name": "000000030828.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000030828.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1123.994, "latencies_ms": [1123.994], "images_per_second": 0.89, "prompt_tokens": 9, "response_tokens_est": 20, "n_tiles": 6, "output_text": "A person is lying on a bench in a park, covered with a quilted orange blanket.", "error": null, "sys_before": {"cpu_percent": 42.7, "ram_used_mb": 25588.8, "ram_available_mb": 100183.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25588.8, "ram_available_mb": 100183.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.24, "peak": 14.61, "min": 13.82}, "VDD_GPU": {"avg": 33.53, "peak": 37.42, "min": 28.36}, "VIN": {"avg": 70.82, "peak": 82.22, "min": 61.88}, "VDD_CPU_SOC_MSS": {"avg": 15.06, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 33.53, "energy_joules_est": 37.7, "sample_count": 8, "duration_seconds": 1.124}, "timestamp": "2026-01-16T16:08:41.211396"}
{"image_index": 283, "image_name": "000000030828.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000030828.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1738.91, "latencies_ms": [1738.91], "images_per_second": 0.575, "prompt_tokens": 23, "response_tokens_est": 40, "n_tiles": 6, "output_text": "object: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25588.8, "ram_available_mb": 100183.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 25588.8, "ram_available_mb": 100183.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.6, "peak": 14.91, "min": 14.02}, "VDD_GPU": {"avg": 31.7, "peak": 39.0, "min": 24.42}, "VIN": {"avg": 68.86, "peak": 107.04, "min": 55.13}, "VDD_CPU_SOC_MSS": {"avg": 15.17, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 31.7, "energy_joules_est": 55.14, "sample_count": 13, "duration_seconds": 1.739}, "timestamp": "2026-01-16T16:08:42.956499"}
{"image_index": 283, "image_name": "000000030828.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000030828.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2664.627, "latencies_ms": [2664.627], "images_per_second": 0.375, "prompt_tokens": 27, "response_tokens_est": 73, "n_tiles": 6, "output_text": "The main objects in the image are a bench and a person lying on it. The bench is located in the foreground, while the person is lying on it. The person is near the bench, and the bench is near the person. The background features a fence and some greenery, indicating that the scene is set in a park or a similar outdoor area.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25588.8, "ram_available_mb": 100183.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 25588.8, "ram_available_mb": 100183.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.77, "peak": 15.11, "min": 14.02}, "VDD_GPU": {"avg": 28.23, "peak": 38.6, "min": 23.24}, "VIN": {"avg": 68.54, "peak": 114.8, "min": 59.44}, "VDD_CPU_SOC_MSS": {"avg": 15.43, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 28.23, "energy_joules_est": 75.24, "sample_count": 21, "duration_seconds": 2.665}, "timestamp": "2026-01-16T16:08:45.627464"}
{"image_index": 283, "image_name": "000000030828.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000030828.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1905.088, "latencies_ms": [1905.088], "images_per_second": 0.525, "prompt_tokens": 21, "response_tokens_est": 46, "n_tiles": 6, "output_text": "The image depicts a park bench with a person lying down on it, covered with a quilted orange blanket. The bench is situated on a grassy area with a metal fence and a parking meter visible in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25588.8, "ram_available_mb": 100183.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 25588.8, "ram_available_mb": 100183.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.69, "peak": 15.01, "min": 14.12}, "VDD_GPU": {"avg": 30.33, "peak": 38.21, "min": 24.03}, "VIN": {"avg": 68.54, "peak": 103.73, "min": 55.34}, "VDD_CPU_SOC_MSS": {"avg": 15.32, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 30.33, "energy_joules_est": 57.8, "sample_count": 14, "duration_seconds": 1.906}, "timestamp": "2026-01-16T16:08:47.539301"}
{"image_index": 283, "image_name": "000000030828.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000030828.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2522.645, "latencies_ms": [2522.645], "images_per_second": 0.396, "prompt_tokens": 19, "response_tokens_est": 68, "n_tiles": 6, "output_text": "The image depicts a bench in a park with a vivid orange quilted cover, contrasting sharply against the green grass and the blue sky. The bench is made of metal, and there are two parking meters visible in the background. The lighting is natural, suggesting it is daytime, and the overall atmosphere is serene and peaceful.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25588.8, "ram_available_mb": 100183.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 25587.0, "ram_available_mb": 100185.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.79, "peak": 15.11, "min": 14.12}, "VDD_GPU": {"avg": 28.55, "peak": 38.6, "min": 23.24}, "VIN": {"avg": 69.68, "peak": 107.53, "min": 60.8}, "VDD_CPU_SOC_MSS": {"avg": 15.42, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 28.55, "energy_joules_est": 72.04, "sample_count": 19, "duration_seconds": 2.523}, "timestamp": "2026-01-16T16:08:50.068885"}
{"image_index": 284, "image_name": "000000031050.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031050.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1871.549, "latencies_ms": [1871.549], "images_per_second": 0.534, "prompt_tokens": 9, "response_tokens_est": 48, "n_tiles": 6, "output_text": "The image shows a display of various decorative items, including a large, ornate vase with a blue and yellow pattern, a small, decorative plant, and a metallic sculpture, all arranged on a white shelf against a brown wall.", "error": null, "sys_before": {"cpu_percent": 42.1, "ram_used_mb": 25587.0, "ram_available_mb": 100185.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 25586.9, "ram_available_mb": 100185.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.58, "peak": 15.22, "min": 13.82}, "VDD_GPU": {"avg": 29.8, "peak": 37.03, "min": 24.42}, "VIN": {"avg": 71.56, "peak": 121.16, "min": 54.87}, "VDD_CPU_SOC_MSS": {"avg": 15.33, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 29.8, "energy_joules_est": 55.78, "sample_count": 14, "duration_seconds": 1.872}, "timestamp": "2026-01-16T16:08:52.002720"}
{"image_index": 284, "image_name": "000000031050.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031050.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1700.074, "latencies_ms": [1700.074], "images_per_second": 0.588, "prompt_tokens": 23, "response_tokens_est": 40, "n_tiles": 6, "output_text": "object: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25586.9, "ram_available_mb": 100185.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 25586.9, "ram_available_mb": 100185.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.83, "peak": 15.22, "min": 14.22}, "VDD_GPU": {"avg": 31.15, "peak": 37.82, "min": 24.82}, "VIN": {"avg": 67.41, "peak": 74.39, "min": 55.45}, "VDD_CPU_SOC_MSS": {"avg": 15.35, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 31.15, "energy_joules_est": 52.97, "sample_count": 13, "duration_seconds": 1.701}, "timestamp": "2026-01-16T16:08:53.709016"}
{"image_index": 284, "image_name": "000000031050.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031050.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2505.558, "latencies_ms": [2505.558], "images_per_second": 0.399, "prompt_tokens": 27, "response_tokens_est": 70, "n_tiles": 6, "output_text": "The main object in the foreground is a large, colorful vase with a blue and yellow pattern. It is placed on a white pedestal. To the left of the vase, there is a smaller, dark-colored vase. In the background, there is a brown wall with various decorative items, including a small sculpture and a plant.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25586.9, "ram_available_mb": 100185.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 25587.2, "ram_available_mb": 100185.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.97, "peak": 15.32, "min": 14.22}, "VDD_GPU": {"avg": 28.97, "peak": 38.21, "min": 23.64}, "VIN": {"avg": 69.76, "peak": 91.84, "min": 61.74}, "VDD_CPU_SOC_MSS": {"avg": 15.52, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 28.97, "energy_joules_est": 72.59, "sample_count": 19, "duration_seconds": 2.506}, "timestamp": "2026-01-16T16:08:56.220802"}
{"image_index": 284, "image_name": "000000031050.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031050.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3385.757, "latencies_ms": [3385.757], "images_per_second": 0.295, "prompt_tokens": 21, "response_tokens_est": 103, "n_tiles": 6, "output_text": "The image depicts an indoor setting, likely a museum or gallery, showcasing various decorative items. The focal point is a large, ornate vase with a blue and yellow pattern, placed on a white pedestal. To the left, there is a smaller, dark-colored vase with a similar design. The background features a wall with a brownish hue and a few small, white informational plaques. The overall atmosphere suggests a display of art and artifacts, possibly related to cultural or historical themes.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25587.2, "ram_available_mb": 100185.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 25587.2, "ram_available_mb": 100185.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.04, "peak": 15.32, "min": 14.32}, "VDD_GPU": {"avg": 27.48, "peak": 38.21, "min": 23.64}, "VIN": {"avg": 68.51, "peak": 107.45, "min": 57.17}, "VDD_CPU_SOC_MSS": {"avg": 15.57, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 27.48, "energy_joules_est": 93.05, "sample_count": 26, "duration_seconds": 3.386}, "timestamp": "2026-01-16T16:08:59.616761"}
{"image_index": 284, "image_name": "000000031050.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031050.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2214.306, "latencies_ms": [2214.306], "images_per_second": 0.452, "prompt_tokens": 19, "response_tokens_est": 57, "n_tiles": 6, "output_text": "The image showcases a collection of art pieces, including a strikingly colorful vase with blue and yellow hues, a rustic wooden sculpture, and a dark, metallic sculpture. The lighting is soft and warm, casting gentle shadows that enhance the textures and colors of the objects.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25587.2, "ram_available_mb": 100185.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 25586.9, "ram_available_mb": 100185.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.77, "peak": 15.01, "min": 14.12}, "VDD_GPU": {"avg": 29.08, "peak": 37.82, "min": 23.25}, "VIN": {"avg": 69.83, "peak": 99.97, "min": 53.8}, "VDD_CPU_SOC_MSS": {"avg": 15.21, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 29.08, "energy_joules_est": 64.41, "sample_count": 17, "duration_seconds": 2.215}, "timestamp": "2026-01-16T16:09:01.837499"}
{"image_index": 285, "image_name": "000000031093.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031093.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1496.246, "latencies_ms": [1496.246], "images_per_second": 0.668, "prompt_tokens": 9, "response_tokens_est": 34, "n_tiles": 6, "output_text": "A skateboarder is performing a trick on a curved ramp at a skate park, wearing a white t-shirt, blue jeans, and a helmet with protective gear.", "error": null, "sys_before": {"cpu_percent": 44.4, "ram_used_mb": 25586.9, "ram_available_mb": 100185.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25587.2, "ram_available_mb": 100185.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.5, "peak": 15.01, "min": 13.92}, "VDD_GPU": {"avg": 31.12, "peak": 37.42, "min": 25.21}, "VIN": {"avg": 75.87, "peak": 101.49, "min": 63.63}, "VDD_CPU_SOC_MSS": {"avg": 15.25, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 31.12, "energy_joules_est": 46.58, "sample_count": 11, "duration_seconds": 1.497}, "timestamp": "2026-01-16T16:09:03.381953"}
{"image_index": 285, "image_name": "000000031093.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031093.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1726.023, "latencies_ms": [1726.023], "images_per_second": 0.579, "prompt_tokens": 23, "response_tokens_est": 41, "n_tiles": 6, "output_text": "1. Skateboarder\n2. Helmet\n3. T-shirt\n4. Gloves\n5. Pads\n6. Leggings\n7. Shoes\n8. Ramp", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25587.2, "ram_available_mb": 100185.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 25586.9, "ram_available_mb": 100185.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.79, "peak": 15.11, "min": 14.22}, "VDD_GPU": {"avg": 31.39, "peak": 38.21, "min": 24.82}, "VIN": {"avg": 71.71, "peak": 117.29, "min": 63.32}, "VDD_CPU_SOC_MSS": {"avg": 15.38, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 31.39, "energy_joules_est": 54.19, "sample_count": 13, "duration_seconds": 1.726}, "timestamp": "2026-01-16T16:09:05.114156"}
{"image_index": 285, "image_name": "000000031093.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031093.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3528.118, "latencies_ms": [3528.118], "images_per_second": 0.283, "prompt_tokens": 27, "response_tokens_est": 104, "n_tiles": 6, "output_text": "The main object in the foreground is a skateboarder performing a trick on a concrete ramp. The skateboarder is wearing a white t-shirt, blue jeans, and a helmet. The skateboard is in motion, with the skateboarder's feet on the board. In the background, there is another person standing on the ramp, and a person is working on a machine near the ramp. The skateboarder is positioned near the center of the image, while the background elements are slightly out of focus.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25586.9, "ram_available_mb": 100185.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 25586.9, "ram_available_mb": 100185.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.86, "peak": 15.11, "min": 14.02}, "VDD_GPU": {"avg": 27.06, "peak": 38.98, "min": 23.25}, "VIN": {"avg": 65.87, "peak": 88.72, "min": 56.4}, "VDD_CPU_SOC_MSS": {"avg": 15.51, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 27.06, "energy_joules_est": 95.49, "sample_count": 28, "duration_seconds": 3.529}, "timestamp": "2026-01-16T16:09:08.649550"}
{"image_index": 285, "image_name": "000000031093.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031093.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2765.152, "latencies_ms": [2765.152], "images_per_second": 0.362, "prompt_tokens": 21, "response_tokens_est": 77, "n_tiles": 6, "output_text": "The image captures a skateboarder performing a trick in a concrete bowl at a skate park during twilight. The skateboarder is dressed in a white t-shirt, blue jeans, and a helmet, with protective gear on their knees. The setting is a well-maintained skate park with a curved concrete bowl, and other skateboarders can be seen in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25586.9, "ram_available_mb": 100185.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 25586.9, "ram_available_mb": 100185.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.83, "peak": 15.11, "min": 14.02}, "VDD_GPU": {"avg": 27.99, "peak": 38.21, "min": 23.24}, "VIN": {"avg": 67.4, "peak": 93.08, "min": 62.09}, "VDD_CPU_SOC_MSS": {"avg": 15.47, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 27.99, "energy_joules_est": 77.41, "sample_count": 21, "duration_seconds": 2.766}, "timestamp": "2026-01-16T16:09:11.421123"}
{"image_index": 285, "image_name": "000000031093.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031093.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2041.968, "latencies_ms": [2041.968], "images_per_second": 0.49, "prompt_tokens": 19, "response_tokens_est": 51, "n_tiles": 6, "output_text": "The skateboarder is wearing a white t-shirt, blue jeans, and white sneakers. The skateboard is black with white wheels. The setting is outdoors during the evening, with the lighting casting long shadows and creating a cool, dim atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25586.9, "ram_available_mb": 100185.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 25586.9, "ram_available_mb": 100185.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.73, "peak": 15.01, "min": 14.12}, "VDD_GPU": {"avg": 29.63, "peak": 38.19, "min": 23.64}, "VIN": {"avg": 74.16, "peak": 118.07, "min": 63.48}, "VDD_CPU_SOC_MSS": {"avg": 15.25, "peak": 15.36, "min": 14.57}}, "power_watts_avg": 29.63, "energy_joules_est": 60.51, "sample_count": 15, "duration_seconds": 2.042}, "timestamp": "2026-01-16T16:09:13.469405"}
{"image_index": 286, "image_name": "000000031118.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031118.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1786.961, "latencies_ms": [1786.961], "images_per_second": 0.56, "prompt_tokens": 9, "response_tokens_est": 45, "n_tiles": 6, "output_text": "The image depicts a bustling city street at night, illuminated by a variety of colorful lights, including a large Christmas tree adorned with white and blue lights, and a historic clock tower with a clock face.", "error": null, "sys_before": {"cpu_percent": 43.3, "ram_used_mb": 25586.9, "ram_available_mb": 100185.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 25587.2, "ram_available_mb": 100185.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.58, "peak": 15.22, "min": 13.82}, "VDD_GPU": {"avg": 30.18, "peak": 37.42, "min": 24.42}, "VIN": {"avg": 68.7, "peak": 90.23, "min": 50.02}, "VDD_CPU_SOC_MSS": {"avg": 15.29, "peak": 15.75, "min": 14.57}}, "power_watts_avg": 30.18, "energy_joules_est": 53.94, "sample_count": 13, "duration_seconds": 1.787}, "timestamp": "2026-01-16T16:09:15.308506"}
{"image_index": 286, "image_name": "000000031118.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031118.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1700.72, "latencies_ms": [1700.72], "images_per_second": 0.588, "prompt_tokens": 23, "response_tokens_est": 40, "n_tiles": 6, "output_text": "object: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25587.2, "ram_available_mb": 100185.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 25586.9, "ram_available_mb": 100185.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.82, "peak": 15.11, "min": 14.32}, "VDD_GPU": {"avg": 31.15, "peak": 37.8, "min": 24.82}, "VIN": {"avg": 66.29, "peak": 94.61, "min": 56.79}, "VDD_CPU_SOC_MSS": {"avg": 15.45, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 31.15, "energy_joules_est": 52.99, "sample_count": 13, "duration_seconds": 1.701}, "timestamp": "2026-01-16T16:09:17.015510"}
{"image_index": 286, "image_name": "000000031118.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031118.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2778.154, "latencies_ms": [2778.154], "images_per_second": 0.36, "prompt_tokens": 27, "response_tokens_est": 77, "n_tiles": 6, "output_text": "The main objects in the image are a large Christmas tree and a clock tower. The Christmas tree is situated in the foreground, while the clock tower is in the background. The tree is decorated with white lights and blue lights, and it is positioned near the clock tower. The clock tower is illuminated with white lights, and it is situated on the left side of the image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25586.9, "ram_available_mb": 100185.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 25586.9, "ram_available_mb": 100185.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.8, "peak": 15.01, "min": 14.12}, "VDD_GPU": {"avg": 28.12, "peak": 38.59, "min": 23.24}, "VIN": {"avg": 70.64, "peak": 118.21, "min": 60.7}, "VDD_CPU_SOC_MSS": {"avg": 15.32, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 28.12, "energy_joules_est": 78.13, "sample_count": 21, "duration_seconds": 2.778}, "timestamp": "2026-01-16T16:09:19.800724"}
{"image_index": 286, "image_name": "000000031118.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031118.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2385.598, "latencies_ms": [2385.598], "images_per_second": 0.419, "prompt_tokens": 21, "response_tokens_est": 63, "n_tiles": 6, "output_text": "The image depicts a bustling city street at night, illuminated by a variety of lights, including string lights and street lamps. A large Christmas tree adorned with white lights stands prominently in the center, surrounded by people and shops. The scene suggests a festive atmosphere, likely during the holiday season.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25586.9, "ram_available_mb": 100185.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 25586.9, "ram_available_mb": 100185.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.74, "peak": 15.01, "min": 14.02}, "VDD_GPU": {"avg": 28.69, "peak": 37.82, "min": 23.24}, "VIN": {"avg": 68.48, "peak": 94.49, "min": 60.9}, "VDD_CPU_SOC_MSS": {"avg": 15.27, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 28.69, "energy_joules_est": 68.45, "sample_count": 18, "duration_seconds": 2.386}, "timestamp": "2026-01-16T16:09:22.192756"}
{"image_index": 286, "image_name": "000000031118.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031118.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2745.204, "latencies_ms": [2745.204], "images_per_second": 0.364, "prompt_tokens": 19, "response_tokens_est": 76, "n_tiles": 6, "output_text": "The image depicts a festive scene at night, featuring a large Christmas tree adorned with white lights and blue lights. The tree is situated in front of a historic building with a clock tower, illuminated by warm, yellow lights. The ground appears wet, suggesting recent rain, and the overall atmosphere is bright and cheerful, with a mix of traditional and modern elements.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25586.9, "ram_available_mb": 100185.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 25586.9, "ram_available_mb": 100185.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.79, "peak": 15.11, "min": 14.02}, "VDD_GPU": {"avg": 28.01, "peak": 37.82, "min": 23.25}, "VIN": {"avg": 68.52, "peak": 113.68, "min": 52.92}, "VDD_CPU_SOC_MSS": {"avg": 15.35, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 28.01, "energy_joules_est": 76.9, "sample_count": 21, "duration_seconds": 2.745}, "timestamp": "2026-01-16T16:09:24.944535"}
{"image_index": 287, "image_name": "000000031217.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031217.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1285.2, "latencies_ms": [1285.2], "images_per_second": 0.778, "prompt_tokens": 9, "response_tokens_est": 26, "n_tiles": 6, "output_text": "A young man is playing tennis on a court, with a net separating him from the background of trees and foliage.", "error": null, "sys_before": {"cpu_percent": 42.3, "ram_used_mb": 25586.9, "ram_available_mb": 100185.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25586.9, "ram_available_mb": 100185.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.35, "peak": 14.91, "min": 13.92}, "VDD_GPU": {"avg": 32.17, "peak": 37.42, "min": 26.79}, "VIN": {"avg": 68.6, "peak": 84.99, "min": 51.24}, "VDD_CPU_SOC_MSS": {"avg": 15.0, "peak": 15.36, "min": 14.57}}, "power_watts_avg": 32.17, "energy_joules_est": 41.36, "sample_count": 9, "duration_seconds": 1.286}, "timestamp": "2026-01-16T16:09:26.290373"}
{"image_index": 287, "image_name": "000000031217.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031217.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1646.408, "latencies_ms": [1646.408], "images_per_second": 0.607, "prompt_tokens": 23, "response_tokens_est": 38, "n_tiles": 6, "output_text": "1. Tennis player\n2. Tennis racket\n3. Tennis ball\n4. Tennis court\n5. Fence\n6. Trees\n7. Net\n8. Person", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25586.9, "ram_available_mb": 100185.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 25586.9, "ram_available_mb": 100185.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.77, "peak": 15.11, "min": 14.22}, "VDD_GPU": {"avg": 31.97, "peak": 39.0, "min": 25.21}, "VIN": {"avg": 75.52, "peak": 112.97, "min": 56.92}, "VDD_CPU_SOC_MSS": {"avg": 15.39, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 31.97, "energy_joules_est": 52.65, "sample_count": 12, "duration_seconds": 1.647}, "timestamp": "2026-01-16T16:09:27.942791"}
{"image_index": 287, "image_name": "000000031217.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031217.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2692.596, "latencies_ms": [2692.596], "images_per_second": 0.371, "prompt_tokens": 27, "response_tokens_est": 77, "n_tiles": 6, "output_text": "The main object in the foreground is a young male tennis player, dressed in a blue shirt and black shorts, holding a tennis racket. He is positioned on a tennis court, with a chain-link fence and trees in the background. The tennis ball is in the air, slightly to the right of the player, indicating that he is in the middle of a serve or return.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25586.9, "ram_available_mb": 100185.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 25586.9, "ram_available_mb": 100185.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.96, "peak": 15.32, "min": 14.22}, "VDD_GPU": {"avg": 28.53, "peak": 38.21, "min": 23.64}, "VIN": {"avg": 72.16, "peak": 107.99, "min": 62.06}, "VDD_CPU_SOC_MSS": {"avg": 15.54, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 28.53, "energy_joules_est": 76.83, "sample_count": 21, "duration_seconds": 2.693}, "timestamp": "2026-01-16T16:09:30.641607"}
{"image_index": 287, "image_name": "000000031217.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031217.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2716.563, "latencies_ms": [2716.563], "images_per_second": 0.368, "prompt_tokens": 21, "response_tokens_est": 78, "n_tiles": 6, "output_text": "The image depicts a young male tennis player in a blue shirt and black shorts, actively engaged in a tennis match on a court. The court is surrounded by a chain-link fence and a dense forest backdrop, indicating an outdoor setting. The player is holding a tennis racket and appears to be preparing to hit a tennis ball, showcasing a moment of intense concentration and athleticism.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25586.9, "ram_available_mb": 100185.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 25587.1, "ram_available_mb": 100185.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.98, "peak": 15.32, "min": 14.32}, "VDD_GPU": {"avg": 28.31, "peak": 37.82, "min": 23.64}, "VIN": {"avg": 66.99, "peak": 83.5, "min": 48.16}, "VDD_CPU_SOC_MSS": {"avg": 15.54, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 28.31, "energy_joules_est": 76.92, "sample_count": 21, "duration_seconds": 2.717}, "timestamp": "2026-01-16T16:09:33.364192"}
{"image_index": 287, "image_name": "000000031217.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031217.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1933.346, "latencies_ms": [1933.346], "images_per_second": 0.517, "prompt_tokens": 19, "response_tokens_est": 49, "n_tiles": 6, "output_text": "The tennis player is wearing a blue shirt and black shorts, with blue socks and blue shoes. The court is made of gray concrete, and the background features a chain-link fence and trees. The lighting is natural, suggesting it is daytime.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25587.1, "ram_available_mb": 100185.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 25587.1, "ram_available_mb": 100185.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.89, "peak": 15.22, "min": 14.32}, "VDD_GPU": {"avg": 30.31, "peak": 37.82, "min": 24.03}, "VIN": {"avg": 67.53, "peak": 100.0, "min": 56.7}, "VDD_CPU_SOC_MSS": {"avg": 15.43, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 30.31, "energy_joules_est": 58.61, "sample_count": 15, "duration_seconds": 1.934}, "timestamp": "2026-01-16T16:09:35.303886"}
{"image_index": 288, "image_name": "000000031248.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031248.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1787.01, "latencies_ms": [1787.01], "images_per_second": 0.56, "prompt_tokens": 9, "response_tokens_est": 45, "n_tiles": 6, "output_text": "The image depicts a well-appointed living room with a classic fireplace, a plush armchair, and a small side table with a lamp, all set against a backdrop of bookshelves filled with books.", "error": null, "sys_before": {"cpu_percent": 40.6, "ram_used_mb": 25587.1, "ram_available_mb": 100185.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 25587.4, "ram_available_mb": 100184.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.61, "peak": 15.22, "min": 13.92}, "VDD_GPU": {"avg": 30.18, "peak": 37.42, "min": 24.42}, "VIN": {"avg": 69.62, "peak": 94.45, "min": 52.49}, "VDD_CPU_SOC_MSS": {"avg": 15.23, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 30.18, "energy_joules_est": 53.94, "sample_count": 13, "duration_seconds": 1.787}, "timestamp": "2026-01-16T16:09:37.149201"}
{"image_index": 288, "image_name": "000000031248.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031248.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1734.158, "latencies_ms": [1734.158], "images_per_second": 0.577, "prompt_tokens": 23, "response_tokens_est": 40, "n_tiles": 6, "output_text": "object: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25587.4, "ram_available_mb": 100184.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 25587.1, "ram_available_mb": 100185.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.69, "peak": 15.01, "min": 14.12}, "VDD_GPU": {"avg": 31.06, "peak": 38.19, "min": 24.42}, "VIN": {"avg": 69.91, "peak": 115.35, "min": 56.04}, "VDD_CPU_SOC_MSS": {"avg": 15.17, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 31.06, "energy_joules_est": 53.88, "sample_count": 13, "duration_seconds": 1.735}, "timestamp": "2026-01-16T16:09:38.890775"}
{"image_index": 288, "image_name": "000000031248.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031248.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2357.506, "latencies_ms": [2357.506], "images_per_second": 0.424, "prompt_tokens": 27, "response_tokens_est": 62, "n_tiles": 6, "output_text": "The main objects in the image are a bookshelf filled with books, a fireplace, a chair, and a lamp. The bookshelf is located in the background, while the chair and lamp are in the foreground. The bookshelf is near the fireplace, and the chair is positioned close to the lamp.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25587.1, "ram_available_mb": 100185.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 25587.1, "ram_available_mb": 100185.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.72, "peak": 15.01, "min": 14.02}, "VDD_GPU": {"avg": 29.02, "peak": 38.6, "min": 23.24}, "VIN": {"avg": 72.85, "peak": 116.93, "min": 62.43}, "VDD_CPU_SOC_MSS": {"avg": 15.25, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 29.02, "energy_joules_est": 68.43, "sample_count": 18, "duration_seconds": 2.358}, "timestamp": "2026-01-16T16:09:41.255269"}
{"image_index": 288, "image_name": "000000031248.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031248.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2464.377, "latencies_ms": [2464.377], "images_per_second": 0.406, "prompt_tokens": 21, "response_tokens_est": 66, "n_tiles": 6, "output_text": "The image depicts a cozy, well-appointed living room with a classic and elegant decor. The room features a white fireplace with a green marble surround, a plush armchair, a small side table with a lamp, and a potted plant. The setting suggests a comfortable and inviting space for relaxation and conversation.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25587.1, "ram_available_mb": 100185.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 25587.1, "ram_available_mb": 100185.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.73, "peak": 15.11, "min": 14.02}, "VDD_GPU": {"avg": 28.42, "peak": 37.8, "min": 23.24}, "VIN": {"avg": 70.25, "peak": 109.49, "min": 60.43}, "VDD_CPU_SOC_MSS": {"avg": 15.29, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 28.42, "energy_joules_est": 70.05, "sample_count": 19, "duration_seconds": 2.465}, "timestamp": "2026-01-16T16:09:43.726141"}
{"image_index": 288, "image_name": "000000031248.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031248.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2069.172, "latencies_ms": [2069.172], "images_per_second": 0.483, "prompt_tokens": 19, "response_tokens_est": 52, "n_tiles": 6, "output_text": "The room features a classic and elegant design with a white fireplace, marble mantel, and a white mantelpiece. The room is well-lit with natural light, and the furniture is made of wood and upholstered in cream-colored fabric.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25587.1, "ram_available_mb": 100185.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 25587.1, "ram_available_mb": 100185.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.7, "peak": 15.01, "min": 14.02}, "VDD_GPU": {"avg": 29.64, "peak": 38.19, "min": 23.64}, "VIN": {"avg": 64.59, "peak": 78.74, "min": 57.61}, "VDD_CPU_SOC_MSS": {"avg": 15.25, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 29.64, "energy_joules_est": 61.35, "sample_count": 16, "duration_seconds": 2.07}, "timestamp": "2026-01-16T16:09:45.801975"}
{"image_index": 289, "image_name": "000000031269.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031269.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2515.021, "latencies_ms": [2515.021], "images_per_second": 0.398, "prompt_tokens": 9, "response_tokens_est": 33, "n_tiles": 12, "output_text": "The image depicts a group of zebras grazing in a grassy field, with their distinctive black and white striped patterns visible on their bodies.", "error": null, "sys_before": {"cpu_percent": 42.3, "ram_used_mb": 25587.1, "ram_available_mb": 100185.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 25587.1, "ram_available_mb": 100185.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.11, "peak": 16.15, "min": 13.79}, "VDD_GPU": {"avg": 34.62, "peak": 40.97, "min": 27.18}, "VIN": {"avg": 79.87, "peak": 130.04, "min": 58.41}, "VDD_CPU_SOC_MSS": {"avg": 16.04, "peak": 17.32, "min": 14.96}}, "power_watts_avg": 34.62, "energy_joules_est": 87.08, "sample_count": 19, "duration_seconds": 2.515}, "timestamp": "2026-01-16T16:09:48.411553"}
{"image_index": 289, "image_name": "000000031269.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031269.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2110.303, "latencies_ms": [2110.303], "images_per_second": 0.474, "prompt_tokens": 23, "response_tokens_est": 17, "n_tiles": 12, "output_text": "zebra: 2\ngrass: 1\ntrees: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25587.1, "ram_available_mb": 100185.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 2.3, "ram_used_mb": 25587.1, "ram_available_mb": 100185.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.08, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 37.64, "peak": 42.94, "min": 30.33}, "VIN": {"avg": 83.3, "peak": 114.23, "min": 58.3}, "VDD_CPU_SOC_MSS": {"avg": 15.87, "peak": 16.53, "min": 15.35}}, "power_watts_avg": 37.64, "energy_joules_est": 79.45, "sample_count": 16, "duration_seconds": 2.111}, "timestamp": "2026-01-16T16:09:50.529238"}
{"image_index": 289, "image_name": "000000031269.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031269.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 4428.597, "latencies_ms": [4428.597], "images_per_second": 0.226, "prompt_tokens": 27, "response_tokens_est": 86, "n_tiles": 12, "output_text": "The main objects in the image are two zebras. The foreground features a zebra with a prominent black and white striped pattern, standing in a muddy patch. The background shows another zebra, slightly out of focus, grazing in the same area. The zebra in the foreground is closer to the viewer, while the one in the background is further away, creating a sense of depth in the image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25587.1, "ram_available_mb": 100185.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 25587.8, "ram_available_mb": 100184.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.07, "peak": 15.54, "min": 14.43}, "VDD_GPU": {"avg": 32.04, "peak": 43.73, "min": 26.0}, "VIN": {"avg": 71.52, "peak": 113.8, "min": 58.66}, "VDD_CPU_SOC_MSS": {"avg": 15.69, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 32.04, "energy_joules_est": 141.9, "sample_count": 35, "duration_seconds": 4.429}, "timestamp": "2026-01-16T16:09:54.964196"}
{"image_index": 289, "image_name": "000000031269.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031269.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2980.5, "latencies_ms": [2980.5], "images_per_second": 0.336, "prompt_tokens": 21, "response_tokens_est": 43, "n_tiles": 12, "output_text": "The image depicts a serene savanna landscape with two zebras grazing in a field. The zebras are surrounded by tall grasses and scattered trees, creating a natural and peaceful environment.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25587.8, "ram_available_mb": 100184.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 25587.8, "ram_available_mb": 100184.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.08, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 34.48, "peak": 42.94, "min": 26.79}, "VIN": {"avg": 75.76, "peak": 118.03, "min": 59.02}, "VDD_CPU_SOC_MSS": {"avg": 15.69, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 34.48, "energy_joules_est": 102.78, "sample_count": 23, "duration_seconds": 2.981}, "timestamp": "2026-01-16T16:09:57.951691"}
{"image_index": 289, "image_name": "000000031269.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031269.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3757.797, "latencies_ms": [3757.797], "images_per_second": 0.266, "prompt_tokens": 19, "response_tokens_est": 65, "n_tiles": 12, "output_text": "The image depicts a scene of two zebras grazing in a grassy field. The zebras have distinctive black and white stripes on their bodies, and the lighting is bright and natural, suggesting it is daytime. The grass is tall and dry, indicating a dry season or a region with less rainfall.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25587.8, "ram_available_mb": 100184.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25588.1, "ram_available_mb": 100184.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.92, "peak": 15.54, "min": 14.53}, "VDD_GPU": {"avg": 32.75, "peak": 42.94, "min": 26.0}, "VIN": {"avg": 71.68, "peak": 110.9, "min": 58.86}, "VDD_CPU_SOC_MSS": {"avg": 15.45, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 32.75, "energy_joules_est": 123.08, "sample_count": 29, "duration_seconds": 3.758}, "timestamp": "2026-01-16T16:10:01.716183"}
{"image_index": 290, "image_name": "000000031296.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031296.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1468.894, "latencies_ms": [1468.894], "images_per_second": 0.681, "prompt_tokens": 9, "response_tokens_est": 33, "n_tiles": 6, "output_text": "The image depicts a group of people seated around a long table in a restaurant, with various items on the table and a few people standing in the background.", "error": null, "sys_before": {"cpu_percent": 38.6, "ram_used_mb": 25588.1, "ram_available_mb": 100184.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 25588.1, "ram_available_mb": 100184.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.37, "peak": 15.01, "min": 13.82}, "VDD_GPU": {"avg": 31.58, "peak": 37.03, "min": 25.6}, "VIN": {"avg": 76.31, "peak": 127.3, "min": 62.78}, "VDD_CPU_SOC_MSS": {"avg": 15.0, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 31.58, "energy_joules_est": 46.4, "sample_count": 11, "duration_seconds": 1.469}, "timestamp": "2026-01-16T16:10:03.251762"}
{"image_index": 290, "image_name": "000000031296.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031296.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 4055.411, "latencies_ms": [4055.411], "images_per_second": 0.247, "prompt_tokens": 23, "response_tokens_est": 129, "n_tiles": 6, "output_text": "- table: 8\n- chairs: 8\n- people: 10\n- tablecloth: 1\n- napkins: 2\n- cups: 2\n- plates: 2\n- condiments: 2\n- cups of coffee: 2\n- cups of tea: 2\n- bottles: 2\n- glasses: 2\n- napkins: 2\n- plates: 2\n- cups: 2\n- condiments: 2\n- napkins: 2\n- cups: 2\n- condiments: 2\n- nap", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25588.1, "ram_available_mb": 100184.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 25588.1, "ram_available_mb": 100184.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.09, "peak": 15.32, "min": 14.22}, "VDD_GPU": {"avg": 26.85, "peak": 38.21, "min": 23.64}, "VIN": {"avg": 67.71, "peak": 106.45, "min": 57.42}, "VDD_CPU_SOC_MSS": {"avg": 15.71, "peak": 16.14, "min": 14.96}}, "power_watts_avg": 26.85, "energy_joules_est": 108.9, "sample_count": 31, "duration_seconds": 4.056}, "timestamp": "2026-01-16T16:10:07.313197"}
{"image_index": 290, "image_name": "000000031296.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031296.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2873.262, "latencies_ms": [2873.262], "images_per_second": 0.348, "prompt_tokens": 27, "response_tokens_est": 84, "n_tiles": 6, "output_text": "In the image, the main objects are a group of people seated around a long wooden table in a restaurant. The table is positioned in the foreground, with the people seated around it. The background shows additional tables and chairs, indicating a larger dining area. The people are positioned near the center of the table, with some facing towards the camera and others turned towards each other, creating a sense of interaction and conversation.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25588.1, "ram_available_mb": 100184.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 25588.0, "ram_available_mb": 100184.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.02, "peak": 15.32, "min": 14.22}, "VDD_GPU": {"avg": 28.08, "peak": 37.82, "min": 23.64}, "VIN": {"avg": 66.34, "peak": 85.05, "min": 57.04}, "VDD_CPU_SOC_MSS": {"avg": 15.55, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 28.08, "energy_joules_est": 80.69, "sample_count": 22, "duration_seconds": 2.874}, "timestamp": "2026-01-16T16:10:10.192722"}
{"image_index": 290, "image_name": "000000031296.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031296.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1987.31, "latencies_ms": [1987.31], "images_per_second": 0.503, "prompt_tokens": 21, "response_tokens_est": 51, "n_tiles": 6, "output_text": "The image depicts a cozy, dimly lit restaurant with a group of people gathered around a long wooden table. The setting appears to be a casual dining area, possibly a diner or a small restaurant, with patrons seated and engaged in conversation.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25588.0, "ram_available_mb": 100184.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 25588.3, "ram_available_mb": 100183.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.92, "peak": 15.32, "min": 14.32}, "VDD_GPU": {"avg": 30.12, "peak": 37.8, "min": 24.42}, "VIN": {"avg": 66.08, "peak": 85.67, "min": 55.53}, "VDD_CPU_SOC_MSS": {"avg": 15.54, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 30.12, "energy_joules_est": 59.87, "sample_count": 15, "duration_seconds": 1.988}, "timestamp": "2026-01-16T16:10:12.186052"}
{"image_index": 290, "image_name": "000000031296.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031296.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1456.396, "latencies_ms": [1456.396], "images_per_second": 0.687, "prompt_tokens": 19, "response_tokens_est": 31, "n_tiles": 6, "output_text": "The image depicts a warmly lit, rustic restaurant with wooden floors and brick walls. The lighting is soft and warm, creating a cozy atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25588.3, "ram_available_mb": 100183.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 25588.5, "ram_available_mb": 100183.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.8, "peak": 15.11, "min": 14.32}, "VDD_GPU": {"avg": 32.16, "peak": 37.82, "min": 25.6}, "VIN": {"avg": 73.92, "peak": 109.46, "min": 54.68}, "VDD_CPU_SOC_MSS": {"avg": 15.35, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 32.16, "energy_joules_est": 46.85, "sample_count": 11, "duration_seconds": 1.457}, "timestamp": "2026-01-16T16:10:13.648809"}
{"image_index": 291, "image_name": "000000031322.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031322.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1339.113, "latencies_ms": [1339.113], "images_per_second": 0.747, "prompt_tokens": 9, "response_tokens_est": 28, "n_tiles": 6, "output_text": "A group of white swans is swimming in a calm body of water, with a few of them standing on the water's surface.", "error": null, "sys_before": {"cpu_percent": 40.7, "ram_used_mb": 25588.5, "ram_available_mb": 100183.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 25588.3, "ram_available_mb": 100183.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.38, "peak": 14.91, "min": 13.82}, "VDD_GPU": {"avg": 32.14, "peak": 37.82, "min": 26.0}, "VIN": {"avg": 75.41, "peak": 100.0, "min": 58.98}, "VDD_CPU_SOC_MSS": {"avg": 15.08, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 32.14, "energy_joules_est": 43.05, "sample_count": 10, "duration_seconds": 1.339}, "timestamp": "2026-01-16T16:10:15.049412"}
{"image_index": 291, "image_name": "000000031322.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031322.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 788.421, "latencies_ms": [788.421], "images_per_second": 1.268, "prompt_tokens": 23, "response_tokens_est": 6, "n_tiles": 6, "output_text": "swan: 8", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25588.3, "ram_available_mb": 100183.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 2.3, "ram_used_mb": 25588.3, "ram_available_mb": 100183.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.53, "peak": 14.73, "min": 14.02}, "VDD_GPU": {"avg": 36.87, "peak": 39.0, "min": 32.7}, "VIN": {"avg": 88.72, "peak": 116.88, "min": 60.85}, "VDD_CPU_SOC_MSS": {"avg": 15.19, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 36.87, "energy_joules_est": 29.09, "sample_count": 5, "duration_seconds": 0.789}, "timestamp": "2026-01-16T16:10:15.848392"}
{"image_index": 291, "image_name": "000000031322.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031322.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2583.2, "latencies_ms": [2583.2], "images_per_second": 0.387, "prompt_tokens": 27, "response_tokens_est": 70, "n_tiles": 6, "output_text": "The main objects in the image are a group of white swans swimming in a body of water. The foreground features a single white swan, while the background shows a row of boats moored along the shore. The water is calm, and the scene is peaceful, with the swans and boats creating a serene and harmonious composition.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25588.3, "ram_available_mb": 100183.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 25588.3, "ram_available_mb": 100183.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.64, "peak": 15.11, "min": 13.82}, "VDD_GPU": {"avg": 29.68, "peak": 40.57, "min": 23.25}, "VIN": {"avg": 67.46, "peak": 98.27, "min": 55.04}, "VDD_CPU_SOC_MSS": {"avg": 15.12, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 29.68, "energy_joules_est": 76.69, "sample_count": 20, "duration_seconds": 2.584}, "timestamp": "2026-01-16T16:10:18.438440"}
{"image_index": 291, "image_name": "000000031322.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031322.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1824.126, "latencies_ms": [1824.126], "images_per_second": 0.548, "prompt_tokens": 21, "response_tokens_est": 43, "n_tiles": 6, "output_text": "The image depicts a serene marina scene with several white swans swimming in a calm body of water. The setting is peaceful, with the swans peacefully floating and enjoying the tranquil environment.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25588.3, "ram_available_mb": 100183.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 25588.5, "ram_available_mb": 100183.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.67, "peak": 14.91, "min": 14.12}, "VDD_GPU": {"avg": 30.3, "peak": 37.8, "min": 24.03}, "VIN": {"avg": 72.06, "peak": 115.1, "min": 61.19}, "VDD_CPU_SOC_MSS": {"avg": 15.18, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 30.3, "energy_joules_est": 55.28, "sample_count": 14, "duration_seconds": 1.825}, "timestamp": "2026-01-16T16:10:20.269102"}
{"image_index": 291, "image_name": "000000031322.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031322.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2496.824, "latencies_ms": [2496.824], "images_per_second": 0.401, "prompt_tokens": 19, "response_tokens_est": 67, "n_tiles": 6, "output_text": "The image depicts a serene scene of white swans swimming in a calm body of water, likely a lake or a calm sea, with a clear sky overhead. The lighting is soft and warm, suggesting it is either early morning or late afternoon, with the sun casting a gentle glow on the water and the swans.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25588.5, "ram_available_mb": 100183.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 25588.3, "ram_available_mb": 100183.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.73, "peak": 15.01, "min": 14.02}, "VDD_GPU": {"avg": 28.53, "peak": 38.19, "min": 23.25}, "VIN": {"avg": 69.71, "peak": 112.44, "min": 56.75}, "VDD_CPU_SOC_MSS": {"avg": 15.33, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 28.53, "energy_joules_est": 71.25, "sample_count": 19, "duration_seconds": 2.497}, "timestamp": "2026-01-16T16:10:22.772615"}
{"image_index": 292, "image_name": "000000031620.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031620.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2115.385, "latencies_ms": [2115.385], "images_per_second": 0.473, "prompt_tokens": 9, "response_tokens_est": 21, "n_tiles": 12, "output_text": "A bride and groom are cutting a cake together in a decorated room with a white canopy overhead.", "error": null, "sys_before": {"cpu_percent": 42.1, "ram_used_mb": 25588.3, "ram_available_mb": 100183.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 2.5, "ram_used_mb": 25588.3, "ram_available_mb": 100183.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.13, "peak": 16.15, "min": 13.79}, "VDD_GPU": {"avg": 35.75, "peak": 40.97, "min": 28.76}, "VIN": {"avg": 77.83, "peak": 113.91, "min": 58.77}, "VDD_CPU_SOC_MSS": {"avg": 16.09, "peak": 17.32, "min": 14.96}}, "power_watts_avg": 35.75, "energy_joules_est": 75.64, "sample_count": 16, "duration_seconds": 2.116}, "timestamp": "2026-01-16T16:10:24.985241"}
{"image_index": 292, "image_name": "000000031620.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031620.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 5856.912, "latencies_ms": [5856.912], "images_per_second": 0.171, "prompt_tokens": 23, "response_tokens_est": 128, "n_tiles": 12, "output_text": "- tablecloth: 1\n- table: 1\n- chairs: 1\n- microphone: 1\n- speaker: 1\n- cake: 1\n- cake stand: 1\n- tablecloth: 1\n- table: 1\n- chairs: 1\n- microphone: 1\n- speaker: 1\n- cake: 1\n- cake stand: 1\n- tablecloth: 1\n- table: 1\n- chairs: 1\n- microphone: 1\n- speaker: 1\n- cake: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25588.3, "ram_available_mb": 100183.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25588.5, "ram_available_mb": 100183.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.09, "peak": 15.54, "min": 14.32}, "VDD_GPU": {"avg": 30.49, "peak": 43.73, "min": 26.0}, "VIN": {"avg": 74.1, "peak": 117.07, "min": 58.48}, "VDD_CPU_SOC_MSS": {"avg": 15.73, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 30.49, "energy_joules_est": 178.59, "sample_count": 46, "duration_seconds": 5.857}, "timestamp": "2026-01-16T16:10:30.848981"}
{"image_index": 292, "image_name": "000000031620.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031620.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3627.811, "latencies_ms": [3627.811], "images_per_second": 0.276, "prompt_tokens": 27, "response_tokens_est": 62, "n_tiles": 12, "output_text": "The main objects in the image are the bride and groom, who are standing near a table. The table is positioned in the foreground, with the bride and groom positioned near it. The background features a stage with a microphone stand and a guitar, indicating that the event is likely a wedding reception or ceremony.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25588.5, "ram_available_mb": 100183.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25588.3, "ram_available_mb": 100183.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.07, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 32.95, "peak": 42.54, "min": 26.39}, "VIN": {"avg": 75.26, "peak": 120.93, "min": 58.26}, "VDD_CPU_SOC_MSS": {"avg": 15.61, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 32.95, "energy_joules_est": 119.55, "sample_count": 28, "duration_seconds": 3.628}, "timestamp": "2026-01-16T16:10:34.483127"}
{"image_index": 292, "image_name": "000000031620.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031620.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3520.417, "latencies_ms": [3520.417], "images_per_second": 0.284, "prompt_tokens": 21, "response_tokens_est": 59, "n_tiles": 12, "output_text": "The image depicts a wedding reception taking place in a tented venue. Guests are seated around tables covered with white tablecloths, while a couple stands at the center, cutting a cake together. The setting is intimate and festive, with string lights and decorative elements enhancing the ambiance.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25588.3, "ram_available_mb": 100183.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25588.5, "ram_available_mb": 100183.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.07, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 33.2, "peak": 42.94, "min": 26.39}, "VIN": {"avg": 79.04, "peak": 133.85, "min": 59.05}, "VDD_CPU_SOC_MSS": {"avg": 15.64, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 33.2, "energy_joules_est": 116.9, "sample_count": 27, "duration_seconds": 3.521}, "timestamp": "2026-01-16T16:10:38.011080"}
{"image_index": 292, "image_name": "000000031620.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031620.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3712.95, "latencies_ms": [3712.95], "images_per_second": 0.269, "prompt_tokens": 19, "response_tokens_est": 65, "n_tiles": 12, "output_text": "The image depicts a wedding reception with a warm and inviting atmosphere. The venue features a ceiling adorned with white drapes and a string of colorful triangular flags, creating a festive ambiance. The lighting is soft and warm, with a combination of natural and artificial sources, enhancing the overall mood of the event.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25588.5, "ram_available_mb": 100183.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25588.5, "ram_available_mb": 100183.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.09, "peak": 15.65, "min": 14.43}, "VDD_GPU": {"avg": 32.74, "peak": 42.54, "min": 26.4}, "VIN": {"avg": 76.62, "peak": 142.93, "min": 58.53}, "VDD_CPU_SOC_MSS": {"avg": 15.68, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 32.74, "energy_joules_est": 121.57, "sample_count": 29, "duration_seconds": 3.713}, "timestamp": "2026-01-16T16:10:41.731094"}
{"image_index": 293, "image_name": "000000031735.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031735.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2781.56, "latencies_ms": [2781.56], "images_per_second": 0.36, "prompt_tokens": 9, "response_tokens_est": 41, "n_tiles": 12, "output_text": "The image depicts a cozy living room with a red wall, a dark blue sofa, a small round wooden table with a lamp, a potted plant, and a framed picture on the wall.", "error": null, "sys_before": {"cpu_percent": 38.3, "ram_used_mb": 25588.5, "ram_available_mb": 100183.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 25588.5, "ram_available_mb": 100183.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.1, "peak": 16.15, "min": 13.69}, "VDD_GPU": {"avg": 34.12, "peak": 41.76, "min": 26.79}, "VIN": {"avg": 73.8, "peak": 109.98, "min": 58.8}, "VDD_CPU_SOC_MSS": {"avg": 15.84, "peak": 17.32, "min": 14.96}}, "power_watts_avg": 34.12, "energy_joules_est": 94.91, "sample_count": 21, "duration_seconds": 2.782}, "timestamp": "2026-01-16T16:10:44.618517"}
{"image_index": 293, "image_name": "000000031735.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031735.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 3184.59, "latencies_ms": [3184.59], "images_per_second": 0.314, "prompt_tokens": 23, "response_tokens_est": 49, "n_tiles": 12, "output_text": "- table: 2\n- sofa: 1\n- lamp: 1\n- plant: 1\n- vase: 1\n- window: 1\n- curtain: 1\n- rug: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25588.5, "ram_available_mb": 100183.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 25588.5, "ram_available_mb": 100183.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.08, "peak": 15.65, "min": 14.43}, "VDD_GPU": {"avg": 33.97, "peak": 42.54, "min": 26.39}, "VIN": {"avg": 73.24, "peak": 121.29, "min": 58.29}, "VDD_CPU_SOC_MSS": {"avg": 15.64, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 33.97, "energy_joules_est": 108.21, "sample_count": 25, "duration_seconds": 3.185}, "timestamp": "2026-01-16T16:10:47.810687"}
{"image_index": 293, "image_name": "000000031735.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031735.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 4497.799, "latencies_ms": [4497.799], "images_per_second": 0.222, "prompt_tokens": 27, "response_tokens_est": 88, "n_tiles": 12, "output_text": "The main objects in the image are a sofa, a coffee table, and a small round table. The sofa is positioned in the foreground, with a blue throw blanket draped over it. The coffee table is located to the left of the sofa, and the small round table is positioned to the right of the sofa. The sofa is near the center of the image, while the coffee table and small round table are positioned further back.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25588.5, "ram_available_mb": 100183.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 25588.5, "ram_available_mb": 100183.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.1, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 31.7, "peak": 42.15, "min": 26.0}, "VIN": {"avg": 75.67, "peak": 112.39, "min": 58.23}, "VDD_CPU_SOC_MSS": {"avg": 15.68, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 31.7, "energy_joules_est": 142.59, "sample_count": 35, "duration_seconds": 4.498}, "timestamp": "2026-01-16T16:10:52.314876"}
{"image_index": 293, "image_name": "000000031735.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031735.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 4257.343, "latencies_ms": [4257.343], "images_per_second": 0.235, "prompt_tokens": 21, "response_tokens_est": 81, "n_tiles": 12, "output_text": "The image depicts a cozy living room with a rich red wall and a large window allowing natural light to filter in. The room features a dark blue sofa adorned with cushions, a small round wooden table with a lamp, a glass coffee table, and a potted plant. The overall atmosphere is warm and inviting, with a mix of traditional and modern elements creating a comfortable and stylish space.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25588.5, "ram_available_mb": 100183.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 25588.5, "ram_available_mb": 100183.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.09, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 31.96, "peak": 42.54, "min": 26.0}, "VIN": {"avg": 72.21, "peak": 114.9, "min": 58.37}, "VDD_CPU_SOC_MSS": {"avg": 15.71, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 31.96, "energy_joules_est": 136.08, "sample_count": 33, "duration_seconds": 4.258}, "timestamp": "2026-01-16T16:10:56.579438"}
{"image_index": 293, "image_name": "000000031735.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031735.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3011.276, "latencies_ms": [3011.276], "images_per_second": 0.332, "prompt_tokens": 19, "response_tokens_est": 44, "n_tiles": 12, "output_text": "The room features a rich red wall, a large window with white lace curtains, and a wooden floor. The lighting is warm and natural, with sunlight streaming in through the window, creating a cozy and inviting atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25588.5, "ram_available_mb": 100183.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 25588.7, "ram_available_mb": 100183.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.07, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 34.35, "peak": 42.54, "min": 26.79}, "VIN": {"avg": 72.07, "peak": 92.7, "min": 58.54}, "VDD_CPU_SOC_MSS": {"avg": 15.73, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 34.35, "energy_joules_est": 103.45, "sample_count": 23, "duration_seconds": 3.012}, "timestamp": "2026-01-16T16:10:59.597432"}
{"image_index": 294, "image_name": "000000031749.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031749.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1465.107, "latencies_ms": [1465.107], "images_per_second": 0.683, "prompt_tokens": 9, "response_tokens_est": 42, "n_tiles": 4, "output_text": "The image depicts a surreal and whimsical scene featuring a clock with a human face, a woman with red hair, and a wooden table, all set against a textured, vintage background.", "error": null, "sys_before": {"cpu_percent": 29.3, "ram_used_mb": 25588.7, "ram_available_mb": 100183.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 25588.5, "ram_available_mb": 100183.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4606.7, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5444.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.45, "peak": 15.01, "min": 13.72}, "VDD_GPU": {"avg": 28.04, "peak": 34.66, "min": 22.85}, "VIN": {"avg": 66.38, "peak": 86.0, "min": 55.35}, "VDD_CPU_SOC_MSS": {"avg": 15.21, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 28.04, "energy_joules_est": 41.09, "sample_count": 11, "duration_seconds": 1.465}, "timestamp": "2026-01-16T16:11:01.109492"}
{"image_index": 294, "image_name": "000000031749.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031749.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1417.022, "latencies_ms": [1417.022], "images_per_second": 0.706, "prompt_tokens": 23, "response_tokens_est": 40, "n_tiles": 4, "output_text": "object: 1\nobject: 2\nobject: 3\nobject: 4\nobject: 5\nobject: 6\nobject: 7\nobject: 8", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25588.5, "ram_available_mb": 100183.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 25588.5, "ram_available_mb": 100183.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4606.7, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5455.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.64, "peak": 15.11, "min": 14.02}, "VDD_GPU": {"avg": 28.0, "peak": 35.06, "min": 22.85}, "VIN": {"avg": 70.96, "peak": 107.78, "min": 60.79}, "VDD_CPU_SOC_MSS": {"avg": 15.28, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 28.0, "energy_joules_est": 39.69, "sample_count": 11, "duration_seconds": 1.417}, "timestamp": "2026-01-16T16:11:02.532852"}
{"image_index": 294, "image_name": "000000031749.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031749.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3347.285, "latencies_ms": [3347.285], "images_per_second": 0.299, "prompt_tokens": 27, "response_tokens_est": 114, "n_tiles": 4, "output_text": "The main object in the image is a clock with a face that is partially obscured by a humanoid figure. The clock is positioned on the left side of the image, with its face facing towards the right. The figure, which appears to be a doll or a mannequin, is placed in the foreground, slightly to the right of the clock. The background is a plain, light-colored wall, which helps to highlight the clock and the figure. The overall composition places the clock and the figure in the foreground, with the background being a neutral space.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25588.5, "ram_available_mb": 100183.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 25587.1, "ram_available_mb": 100185.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4606.7, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5458.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.93, "peak": 15.22, "min": 14.0}, "VDD_GPU": {"avg": 24.59, "peak": 35.06, "min": 21.67}, "VIN": {"avg": 66.2, "peak": 100.51, "min": 58.41}, "VDD_CPU_SOC_MSS": {"avg": 15.58, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 24.59, "energy_joules_est": 82.32, "sample_count": 26, "duration_seconds": 3.348}, "timestamp": "2026-01-16T16:11:05.886133"}
{"image_index": 294, "image_name": "000000031749.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031749.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2506.015, "latencies_ms": [2506.015], "images_per_second": 0.399, "prompt_tokens": 21, "response_tokens_est": 82, "n_tiles": 4, "output_text": "The image depicts a surreal and whimsical scene featuring a clock with a human face. The clock is placed on a wooden surface, and the face of the clock has a human-like appearance with a red and white striped pattern. The clock's hands are pointing to approximately 10:10. The overall setting appears to be indoors, possibly in a room with a warm, ambient light.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25587.1, "ram_available_mb": 100185.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 25587.0, "ram_available_mb": 100185.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4606.7, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5453.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.88, "peak": 15.22, "min": 14.02}, "VDD_GPU": {"avg": 25.57, "peak": 35.06, "min": 22.06}, "VIN": {"avg": 67.18, "peak": 103.02, "min": 50.67}, "VDD_CPU_SOC_MSS": {"avg": 15.52, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 25.57, "energy_joules_est": 64.09, "sample_count": 19, "duration_seconds": 2.506}, "timestamp": "2026-01-16T16:11:08.398274"}
{"image_index": 294, "image_name": "000000031749.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031749.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1831.336, "latencies_ms": [1831.336], "images_per_second": 0.546, "prompt_tokens": 19, "response_tokens_est": 56, "n_tiles": 4, "output_text": "The image features a clock with a vintage design, predominantly in a warm yellow hue. The clock's hands are black, and it has a white face with black numerals. The clock is placed on a wooden surface, and the background has a textured, aged appearance.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25587.0, "ram_available_mb": 100185.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 25587.3, "ram_available_mb": 100184.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4606.7, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5451.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.73, "peak": 15.11, "min": 14.02}, "VDD_GPU": {"avg": 26.88, "peak": 34.27, "min": 22.45}, "VIN": {"avg": 69.83, "peak": 107.05, "min": 58.12}, "VDD_CPU_SOC_MSS": {"avg": 15.41, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 26.88, "energy_joules_est": 49.23, "sample_count": 13, "duration_seconds": 1.832}, "timestamp": "2026-01-16T16:11:10.237537"}
{"image_index": 295, "image_name": "000000031817.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031817.jpg", "image_width": 334, "image_height": 640, "image_resolution": "334x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1057.049, "latencies_ms": [1057.049], "images_per_second": 0.946, "prompt_tokens": 9, "response_tokens_est": 38, "n_tiles": 2, "output_text": "A person is sitting on a motorcycle, wearing a beige jacket, beige pants, and black shoes, with a helmet on their head, and appears to be looking downwards.", "error": null, "sys_before": {"cpu_percent": 34.5, "ram_used_mb": 25587.3, "ram_available_mb": 100184.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25587.0, "ram_available_mb": 100185.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5055.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.75, "peak": 15.22, "min": 14.2}, "VDD_GPU": {"avg": 25.41, "peak": 31.12, "min": 21.66}, "VIN": {"avg": 69.58, "peak": 111.68, "min": 62.02}, "VDD_CPU_SOC_MSS": {"avg": 15.5, "peak": 16.14, "min": 14.96}}, "power_watts_avg": 25.41, "energy_joules_est": 26.87, "sample_count": 8, "duration_seconds": 1.058}, "timestamp": "2026-01-16T16:11:11.319262"}
{"image_index": 295, "image_name": "000000031817.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031817.jpg", "image_width": 334, "image_height": 640, "image_resolution": "334x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1277.033, "latencies_ms": [1277.033], "images_per_second": 0.783, "prompt_tokens": 23, "response_tokens_est": 47, "n_tiles": 2, "output_text": "helmet: 1\nshirt: 1\npants: 1\nsocks: 1\nfootwear: 1\nmotorcycle: 1\nseat: 1\nseatbelt: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25587.0, "ram_available_mb": 100185.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25587.0, "ram_available_mb": 100185.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5064.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.05, "peak": 15.52, "min": 14.51}, "VDD_GPU": {"avg": 25.12, "peak": 31.51, "min": 21.27}, "VIN": {"avg": 67.15, "peak": 102.18, "min": 57.66}, "VDD_CPU_SOC_MSS": {"avg": 15.83, "peak": 16.14, "min": 15.35}}, "power_watts_avg": 25.12, "energy_joules_est": 32.09, "sample_count": 9, "duration_seconds": 1.277}, "timestamp": "2026-01-16T16:11:12.602352"}
{"image_index": 295, "image_name": "000000031817.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031817.jpg", "image_width": 334, "image_height": 640, "image_resolution": "334x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1548.239, "latencies_ms": [1548.239], "images_per_second": 0.646, "prompt_tokens": 27, "response_tokens_est": 58, "n_tiles": 2, "output_text": "The main object in the foreground is a person wearing a beige jacket and beige pants, seated on a motorcycle. The motorcycle is positioned to the right of the person. In the background, there are other people and a red vehicle, which is further away from the main subject.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25587.0, "ram_available_mb": 100185.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25587.3, "ram_available_mb": 100184.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5067.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.06, "peak": 15.42, "min": 14.51}, "VDD_GPU": {"avg": 24.07, "peak": 31.12, "min": 20.88}, "VIN": {"avg": 66.15, "peak": 103.05, "min": 59.6}, "VDD_CPU_SOC_MSS": {"avg": 15.88, "peak": 16.14, "min": 15.35}}, "power_watts_avg": 24.07, "energy_joules_est": 37.27, "sample_count": 12, "duration_seconds": 1.549}, "timestamp": "2026-01-16T16:11:14.156538"}
{"image_index": 295, "image_name": "000000031817.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031817.jpg", "image_width": 334, "image_height": 640, "image_resolution": "334x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1659.13, "latencies_ms": [1659.13], "images_per_second": 0.603, "prompt_tokens": 21, "response_tokens_est": 63, "n_tiles": 2, "output_text": "The image depicts a person riding a motorcycle, wearing a beige jacket and beige pants, with a helmet on their head. The individual is seated on the motorcycle, with their hands resting on the handlebars. In the background, there are other people and a red vehicle, suggesting an outdoor setting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25587.3, "ram_available_mb": 100184.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25587.3, "ram_available_mb": 100184.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5063.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.17, "peak": 15.52, "min": 14.61}, "VDD_GPU": {"avg": 23.77, "peak": 30.73, "min": 20.88}, "VIN": {"avg": 67.98, "peak": 94.88, "min": 61.52}, "VDD_CPU_SOC_MSS": {"avg": 15.91, "peak": 16.14, "min": 15.35}}, "power_watts_avg": 23.77, "energy_joules_est": 39.45, "sample_count": 12, "duration_seconds": 1.66}, "timestamp": "2026-01-16T16:11:15.821812"}
{"image_index": 295, "image_name": "000000031817.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000031817.jpg", "image_width": 334, "image_height": 640, "image_resolution": "334x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1295.272, "latencies_ms": [1295.272], "images_per_second": 0.772, "prompt_tokens": 19, "response_tokens_est": 48, "n_tiles": 2, "output_text": "The rider is wearing a beige jacket and khaki pants, with a black helmet and black shoes. The helmet has a clear visor, and the jacket appears to be made of a light, possibly cotton or cotton blend material.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25587.3, "ram_available_mb": 100184.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 25587.3, "ram_available_mb": 100184.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5062.6, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.17, "peak": 15.42, "min": 14.71}, "VDD_GPU": {"avg": 24.66, "peak": 30.73, "min": 21.27}, "VIN": {"avg": 65.63, "peak": 96.06, "min": 53.73}, "VDD_CPU_SOC_MSS": {"avg": 15.82, "peak": 16.14, "min": 15.35}}, "power_watts_avg": 24.66, "energy_joules_est": 31.96, "sample_count": 10, "duration_seconds": 1.296}, "timestamp": "2026-01-16T16:11:17.123593"}
{"image_index": 296, "image_name": "000000032038.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032038.jpg", "image_width": 333, "image_height": 500, "image_resolution": "333x500", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1414.675, "latencies_ms": [1414.675], "images_per_second": 0.707, "prompt_tokens": 9, "response_tokens_est": 31, "n_tiles": 6, "output_text": "A large, freshly baked pizza with melted cheese and green herbs is placed on a wooden table, with a person's hand partially visible in the background.", "error": null, "sys_before": {"cpu_percent": 41.7, "ram_used_mb": 25587.3, "ram_available_mb": 100184.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 25587.2, "ram_available_mb": 100184.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.61, "peak": 15.01, "min": 14.02}, "VDD_GPU": {"avg": 30.98, "peak": 36.63, "min": 25.6}, "VIN": {"avg": 71.26, "peak": 127.87, "min": 49.91}, "VDD_CPU_SOC_MSS": {"avg": 15.32, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 30.98, "energy_joules_est": 43.84, "sample_count": 11, "duration_seconds": 1.415}, "timestamp": "2026-01-16T16:11:18.579032"}
{"image_index": 296, "image_name": "000000032038.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032038.jpg", "image_width": 333, "image_height": 500, "image_resolution": "333x500", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 4063.088, "latencies_ms": [4063.088], "images_per_second": 0.246, "prompt_tokens": 23, "response_tokens_est": 129, "n_tiles": 6, "output_text": "pizza: 1\nbutter: 1\nbutter knife: 1\nbutter paper: 1\nbutter tray: 1\nbutter pan: 1\nbutter container: 1\nbutter bowl: 1\nbutter tray: 1\nbutter tray: 1\nbutter tray: 1\nbutter tray: 1\nbutter tray: 1\nbutter tray: 1\nbutter tray: 1\nbutter tray: 1\nbutter tray: 1\nbutter tray: 1\nbutter tray:", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25587.2, "ram_available_mb": 100184.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 25587.2, "ram_available_mb": 100184.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.05, "peak": 15.32, "min": 14.22}, "VDD_GPU": {"avg": 26.81, "peak": 38.21, "min": 23.64}, "VIN": {"avg": 69.22, "peak": 110.51, "min": 61.38}, "VDD_CPU_SOC_MSS": {"avg": 15.61, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 26.81, "energy_joules_est": 108.94, "sample_count": 31, "duration_seconds": 4.063}, "timestamp": "2026-01-16T16:11:22.648737"}
{"image_index": 296, "image_name": "000000032038.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032038.jpg", "image_width": 333, "image_height": 500, "image_resolution": "333x500", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2211.183, "latencies_ms": [2211.183], "images_per_second": 0.452, "prompt_tokens": 27, "response_tokens_est": 59, "n_tiles": 6, "output_text": "The main object in the foreground is a freshly baked pizza with melted cheese and green herbs on top. The pizza is placed on a wooden surface, likely a table or countertop. In the background, there are other kitchen utensils and containers, suggesting that this is a kitchen setting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25587.2, "ram_available_mb": 100184.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 25587.2, "ram_available_mb": 100184.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.91, "peak": 15.22, "min": 14.22}, "VDD_GPU": {"avg": 29.69, "peak": 38.19, "min": 24.03}, "VIN": {"avg": 76.15, "peak": 113.87, "min": 62.01}, "VDD_CPU_SOC_MSS": {"avg": 15.57, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 29.69, "energy_joules_est": 65.66, "sample_count": 16, "duration_seconds": 2.211}, "timestamp": "2026-01-16T16:11:24.865975"}
{"image_index": 296, "image_name": "000000032038.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032038.jpg", "image_width": 333, "image_height": 500, "image_resolution": "333x500", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2634.264, "latencies_ms": [2634.264], "images_per_second": 0.38, "prompt_tokens": 21, "response_tokens_est": 75, "n_tiles": 6, "output_text": "The image depicts a pizza on a wooden table, with a person's hand partially visible on the left side, suggesting they are either serving or preparing the pizza. The pizza appears to be freshly baked, with melted cheese and green herbs sprinkled on top. The setting is likely a kitchen or dining area, with other kitchen utensils and containers in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25587.2, "ram_available_mb": 100184.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 25587.5, "ram_available_mb": 100184.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.95, "peak": 15.32, "min": 14.22}, "VDD_GPU": {"avg": 28.68, "peak": 38.59, "min": 23.64}, "VIN": {"avg": 71.47, "peak": 102.64, "min": 60.24}, "VDD_CPU_SOC_MSS": {"avg": 15.61, "peak": 16.14, "min": 14.96}}, "power_watts_avg": 28.68, "energy_joules_est": 75.57, "sample_count": 20, "duration_seconds": 2.635}, "timestamp": "2026-01-16T16:11:27.507001"}
{"image_index": 296, "image_name": "000000032038.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032038.jpg", "image_width": 333, "image_height": 500, "image_resolution": "333x500", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1722.604, "latencies_ms": [1722.604], "images_per_second": 0.581, "prompt_tokens": 19, "response_tokens_est": 41, "n_tiles": 6, "output_text": "The image features a freshly baked pizza with a golden-brown crust and a melted, gooey cheese topping. The lighting is dim, casting shadows and highlighting the texture of the crust and cheese.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25587.5, "ram_available_mb": 100184.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 25587.5, "ram_available_mb": 100184.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.8, "peak": 15.22, "min": 14.22}, "VDD_GPU": {"avg": 31.45, "peak": 37.82, "min": 25.21}, "VIN": {"avg": 77.46, "peak": 120.94, "min": 60.02}, "VDD_CPU_SOC_MSS": {"avg": 15.52, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 31.45, "energy_joules_est": 54.19, "sample_count": 12, "duration_seconds": 1.723}, "timestamp": "2026-01-16T16:11:29.235635"}
{"image_index": 297, "image_name": "000000032081.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032081.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1927.738, "latencies_ms": [1927.738], "images_per_second": 0.519, "prompt_tokens": 9, "response_tokens_est": 16, "n_tiles": 12, "output_text": "A tennis player is preparing to hit a tennis ball on a grass court.", "error": null, "sys_before": {"cpu_percent": 45.4, "ram_used_mb": 25587.2, "ram_available_mb": 100184.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 2.3, "ram_used_mb": 25587.2, "ram_available_mb": 100184.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.96, "peak": 15.85, "min": 14.1}, "VDD_GPU": {"avg": 36.74, "peak": 40.97, "min": 29.94}, "VIN": {"avg": 77.41, "peak": 120.45, "min": 58.41}, "VDD_CPU_SOC_MSS": {"avg": 15.9, "peak": 16.92, "min": 15.35}}, "power_watts_avg": 36.74, "energy_joules_est": 70.84, "sample_count": 15, "duration_seconds": 1.928}, "timestamp": "2026-01-16T16:11:31.236518"}
{"image_index": 297, "image_name": "000000032081.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032081.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2688.6, "latencies_ms": [2688.6], "images_per_second": 0.372, "prompt_tokens": 23, "response_tokens_est": 34, "n_tiles": 12, "output_text": "woman: 1\ntennis racket: 1\ntennis ball: 1\ntennis court: 1\ntennis net: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25587.2, "ram_available_mb": 100184.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 25587.5, "ram_available_mb": 100184.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.72, "peak": 15.34, "min": 14.12}, "VDD_GPU": {"avg": 35.64, "peak": 43.73, "min": 26.79}, "VIN": {"avg": 78.57, "peak": 121.24, "min": 61.44}, "VDD_CPU_SOC_MSS": {"avg": 15.35, "peak": 16.14, "min": 14.56}}, "power_watts_avg": 35.64, "energy_joules_est": 95.84, "sample_count": 21, "duration_seconds": 2.689}, "timestamp": "2026-01-16T16:11:33.932017"}
{"image_index": 297, "image_name": "000000032081.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032081.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3477.409, "latencies_ms": [3477.409], "images_per_second": 0.288, "prompt_tokens": 27, "response_tokens_est": 57, "n_tiles": 12, "output_text": "The main object in the foreground is a tennis player, who is positioned near the net. The player is wearing a white outfit and white shoes, and is holding a tennis racket. The background features a tennis court with a net, and the grass is well-maintained.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25587.5, "ram_available_mb": 100184.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25587.2, "ram_available_mb": 100185.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.74, "peak": 15.34, "min": 14.12}, "VDD_GPU": {"avg": 33.06, "peak": 42.54, "min": 25.6}, "VIN": {"avg": 73.48, "peak": 118.23, "min": 58.72}, "VDD_CPU_SOC_MSS": {"avg": 15.24, "peak": 16.14, "min": 14.56}}, "power_watts_avg": 33.06, "energy_joules_est": 114.98, "sample_count": 27, "duration_seconds": 3.478}, "timestamp": "2026-01-16T16:11:37.417548"}
{"image_index": 297, "image_name": "000000032081.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032081.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3750.997, "latencies_ms": [3750.997], "images_per_second": 0.267, "prompt_tokens": 21, "response_tokens_est": 65, "n_tiles": 12, "output_text": "The image depicts a tennis player in a white outfit, including a skirt and shoes, preparing to hit a tennis ball on a well-maintained grass court. The player is captured in a dynamic pose, with one arm raised and the other holding the racket, indicating the moment of action during a tennis match.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25587.2, "ram_available_mb": 100185.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25587.2, "ram_available_mb": 100185.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.72, "peak": 15.34, "min": 14.12}, "VDD_GPU": {"avg": 32.43, "peak": 42.53, "min": 25.6}, "VIN": {"avg": 75.07, "peak": 123.03, "min": 58.51}, "VDD_CPU_SOC_MSS": {"avg": 15.29, "peak": 16.14, "min": 14.56}}, "power_watts_avg": 32.43, "energy_joules_est": 121.66, "sample_count": 29, "duration_seconds": 3.751}, "timestamp": "2026-01-16T16:11:41.175968"}
{"image_index": 297, "image_name": "000000032081.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032081.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2959.561, "latencies_ms": [2959.561], "images_per_second": 0.338, "prompt_tokens": 19, "response_tokens_est": 42, "n_tiles": 12, "output_text": "The tennis player is wearing a white outfit, including a skirt and a sleeveless top, paired with white shoes. The lighting is bright, indicating it is daytime, and the weather appears to be clear.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25587.2, "ram_available_mb": 100185.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 25587.2, "ram_available_mb": 100185.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.71, "peak": 15.34, "min": 14.12}, "VDD_GPU": {"avg": 34.27, "peak": 42.53, "min": 26.0}, "VIN": {"avg": 75.38, "peak": 117.75, "min": 54.64}, "VDD_CPU_SOC_MSS": {"avg": 15.28, "peak": 16.14, "min": 14.56}}, "power_watts_avg": 34.27, "energy_joules_est": 101.44, "sample_count": 23, "duration_seconds": 2.96}, "timestamp": "2026-01-16T16:11:44.142217"}
{"image_index": 298, "image_name": "000000032285.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032285.jpg", "image_width": 640, "image_height": 423, "image_resolution": "640x423", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1337.578, "latencies_ms": [1337.578], "images_per_second": 0.748, "prompt_tokens": 9, "response_tokens_est": 28, "n_tiles": 6, "output_text": "The image shows a small bathroom with a white toilet, a towel hanging on the wall, a shower curtain, and a towel rack.", "error": null, "sys_before": {"cpu_percent": 41.1, "ram_used_mb": 25587.2, "ram_available_mb": 100185.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 25587.2, "ram_available_mb": 100185.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.3, "peak": 14.91, "min": 13.82}, "VDD_GPU": {"avg": 32.18, "peak": 37.42, "min": 26.0}, "VIN": {"avg": 71.74, "peak": 123.28, "min": 54.76}, "VDD_CPU_SOC_MSS": {"avg": 15.04, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 32.18, "energy_joules_est": 43.05, "sample_count": 10, "duration_seconds": 1.338}, "timestamp": "2026-01-16T16:11:45.537334"}
{"image_index": 298, "image_name": "000000032285.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032285.jpg", "image_width": 640, "image_height": 423, "image_resolution": "640x423", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1756.837, "latencies_ms": [1756.837], "images_per_second": 0.569, "prompt_tokens": 23, "response_tokens_est": 42, "n_tiles": 6, "output_text": "1. Toilet\n2. Shower curtain\n3. Towel\n4. Shelf\n5. Floor tiles\n6. Wall tiles\n7. Wall outlet\n8. Recessed light", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25587.2, "ram_available_mb": 100185.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25587.2, "ram_available_mb": 100185.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.77, "peak": 15.11, "min": 14.22}, "VDD_GPU": {"avg": 31.39, "peak": 38.59, "min": 24.82}, "VIN": {"avg": 74.45, "peak": 110.87, "min": 59.29}, "VDD_CPU_SOC_MSS": {"avg": 15.47, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 31.39, "energy_joules_est": 55.16, "sample_count": 13, "duration_seconds": 1.757}, "timestamp": "2026-01-16T16:11:47.301334"}
{"image_index": 298, "image_name": "000000032285.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032285.jpg", "image_width": 640, "image_height": 423, "image_resolution": "640x423", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3065.07, "latencies_ms": [3065.07], "images_per_second": 0.326, "prompt_tokens": 27, "response_tokens_est": 91, "n_tiles": 6, "output_text": "The toilet is positioned on the left side of the image, with its lid closed. The toilet paper roll is placed near the toilet. The shower curtain is hanging on the right side of the toilet, partially covering the bathtub. The bathtub is located in the background, and the shower curtain is near the bathtub. The shelves are located to the right of the bathtub, and the towels are placed on the shelves.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 25587.2, "ram_available_mb": 100185.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 25587.2, "ram_available_mb": 100185.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.03, "peak": 15.32, "min": 14.22}, "VDD_GPU": {"avg": 27.87, "peak": 38.21, "min": 23.64}, "VIN": {"avg": 70.16, "peak": 113.8, "min": 59.07}, "VDD_CPU_SOC_MSS": {"avg": 15.63, "peak": 16.14, "min": 14.96}}, "power_watts_avg": 27.87, "energy_joules_est": 85.44, "sample_count": 24, "duration_seconds": 3.066}, "timestamp": "2026-01-16T16:11:50.372778"}
{"image_index": 298, "image_name": "000000032285.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032285.jpg", "image_width": 640, "image_height": 423, "image_resolution": "640x423", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2287.062, "latencies_ms": [2287.062], "images_per_second": 0.437, "prompt_tokens": 21, "response_tokens_est": 62, "n_tiles": 6, "output_text": "The image depicts a small bathroom with a white toilet and a bathtub. The walls are painted in a light yellow color, and there is a towel rack above the toilet. The floor is tiled with brown tiles. The shower curtain is open, revealing a glimpse of the bathtub.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25587.2, "ram_available_mb": 100185.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 25587.2, "ram_available_mb": 100185.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.93, "peak": 15.22, "min": 14.32}, "VDD_GPU": {"avg": 29.4, "peak": 37.8, "min": 24.03}, "VIN": {"avg": 70.47, "peak": 119.4, "min": 56.42}, "VDD_CPU_SOC_MSS": {"avg": 15.56, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 29.4, "energy_joules_est": 67.25, "sample_count": 17, "duration_seconds": 2.288}, "timestamp": "2026-01-16T16:11:52.666353"}
{"image_index": 298, "image_name": "000000032285.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032285.jpg", "image_width": 640, "image_height": 423, "image_resolution": "640x423", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1858.164, "latencies_ms": [1858.164], "images_per_second": 0.538, "prompt_tokens": 19, "response_tokens_est": 46, "n_tiles": 6, "output_text": "The bathroom features a light yellow wall, a white toilet, and a white bathtub. The lighting is soft and natural, likely from a nearby window, and the materials include ceramic tiles for the floor and white fixtures.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25587.2, "ram_available_mb": 100185.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 25587.7, "ram_available_mb": 100184.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.88, "peak": 15.22, "min": 14.32}, "VDD_GPU": {"avg": 30.78, "peak": 38.21, "min": 24.42}, "VIN": {"avg": 69.42, "peak": 99.9, "min": 54.75}, "VDD_CPU_SOC_MSS": {"avg": 15.44, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 30.78, "energy_joules_est": 57.22, "sample_count": 14, "duration_seconds": 1.859}, "timestamp": "2026-01-16T16:11:54.531286"}
{"image_index": 299, "image_name": "000000032334.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032334.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2394.658, "latencies_ms": [2394.658], "images_per_second": 0.418, "prompt_tokens": 9, "response_tokens_est": 29, "n_tiles": 12, "output_text": "The image shows a group of people sitting at a table in a restaurant, with two of them holding wine glasses and smiling at the camera.", "error": null, "sys_before": {"cpu_percent": 48.5, "ram_used_mb": 25587.7, "ram_available_mb": 100184.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 25587.4, "ram_available_mb": 100184.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.1, "peak": 16.05, "min": 13.79}, "VDD_GPU": {"avg": 35.1, "peak": 40.97, "min": 27.57}, "VIN": {"avg": 74.88, "peak": 120.46, "min": 58.71}, "VDD_CPU_SOC_MSS": {"avg": 15.81, "peak": 16.92, "min": 14.96}}, "power_watts_avg": 35.1, "energy_joules_est": 84.07, "sample_count": 18, "duration_seconds": 2.395}, "timestamp": "2026-01-16T16:11:57.021383"}
{"image_index": 299, "image_name": "000000032334.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032334.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2648.089, "latencies_ms": [2648.089], "images_per_second": 0.378, "prompt_tokens": 23, "response_tokens_est": 33, "n_tiles": 12, "output_text": "1. Glasses\n2. Wine\n3. Woman\n4. Man\n5. Table\n6. Menu\n7. Person\n8. Room", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25587.4, "ram_available_mb": 100184.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 25587.4, "ram_available_mb": 100184.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.06, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 35.83, "peak": 42.94, "min": 27.57}, "VIN": {"avg": 73.33, "peak": 110.46, "min": 58.6}, "VDD_CPU_SOC_MSS": {"avg": 15.71, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 35.83, "energy_joules_est": 94.89, "sample_count": 20, "duration_seconds": 2.648}, "timestamp": "2026-01-16T16:11:59.676332"}
{"image_index": 299, "image_name": "000000032334.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032334.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3926.166, "latencies_ms": [3926.166], "images_per_second": 0.255, "prompt_tokens": 27, "response_tokens_est": 70, "n_tiles": 12, "output_text": "The main objects in the image are two people, one in the foreground and one in the background. The person in the foreground is holding a wine glass, while the person in the background is partially visible and appears to be standing. The table in the foreground has a menu and a glass, while the background features a window and a person standing.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25587.4, "ram_available_mb": 100184.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25587.4, "ram_available_mb": 100184.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.77, "peak": 15.34, "min": 14.12}, "VDD_GPU": {"avg": 32.54, "peak": 42.94, "min": 25.6}, "VIN": {"avg": 70.34, "peak": 119.86, "min": 55.43}, "VDD_CPU_SOC_MSS": {"avg": 15.23, "peak": 16.14, "min": 14.56}}, "power_watts_avg": 32.54, "energy_joules_est": 127.77, "sample_count": 30, "duration_seconds": 3.926}, "timestamp": "2026-01-16T16:12:03.609058"}
{"image_index": 299, "image_name": "000000032334.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032334.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3683.346, "latencies_ms": [3683.346], "images_per_second": 0.271, "prompt_tokens": 21, "response_tokens_est": 63, "n_tiles": 12, "output_text": "The image depicts a casual indoor setting, likely a restaurant or bar, where a group of people is gathered. The individuals are engaged in conversation, with some holding wine glasses, suggesting a social or celebratory occasion. The atmosphere appears relaxed and convivial, with the focus on the interaction between the people.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25587.4, "ram_available_mb": 100184.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25587.7, "ram_available_mb": 100184.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.73, "peak": 15.34, "min": 14.12}, "VDD_GPU": {"avg": 32.69, "peak": 41.74, "min": 25.6}, "VIN": {"avg": 73.87, "peak": 115.2, "min": 58.24}, "VDD_CPU_SOC_MSS": {"avg": 15.23, "peak": 16.14, "min": 14.56}}, "power_watts_avg": 32.69, "energy_joules_est": 120.42, "sample_count": 28, "duration_seconds": 3.684}, "timestamp": "2026-01-16T16:12:07.299151"}
{"image_index": 299, "image_name": "000000032334.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032334.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2714.006, "latencies_ms": [2714.006], "images_per_second": 0.368, "prompt_tokens": 19, "response_tokens_est": 35, "n_tiles": 12, "output_text": "The image features a brightly lit indoor setting with natural light streaming in through a window. The individuals are wearing casual clothing, and the overall atmosphere is warm and inviting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25587.4, "ram_available_mb": 100184.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 25587.4, "ram_available_mb": 100184.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.73, "peak": 15.34, "min": 14.12}, "VDD_GPU": {"avg": 35.32, "peak": 42.53, "min": 26.79}, "VIN": {"avg": 75.08, "peak": 123.02, "min": 58.78}, "VDD_CPU_SOC_MSS": {"avg": 15.35, "peak": 16.14, "min": 14.56}}, "power_watts_avg": 35.32, "energy_joules_est": 95.88, "sample_count": 21, "duration_seconds": 2.715}, "timestamp": "2026-01-16T16:12:10.020709"}
{"image_index": 300, "image_name": "000000032570.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032570.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1366.621, "latencies_ms": [1366.621], "images_per_second": 0.732, "prompt_tokens": 9, "response_tokens_est": 29, "n_tiles": 6, "output_text": "A surfer is riding a wave in the ocean, skillfully maneuvering the surfboard amidst the frothy, blue-green waters.", "error": null, "sys_before": {"cpu_percent": 43.5, "ram_used_mb": 25587.4, "ram_available_mb": 100184.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 25587.4, "ram_available_mb": 100184.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.31, "peak": 14.91, "min": 13.82}, "VDD_GPU": {"avg": 32.38, "peak": 37.8, "min": 26.39}, "VIN": {"avg": 79.12, "peak": 118.51, "min": 63.49}, "VDD_CPU_SOC_MSS": {"avg": 15.04, "peak": 15.74, "min": 14.56}}, "power_watts_avg": 32.38, "energy_joules_est": 44.27, "sample_count": 10, "duration_seconds": 1.367}, "timestamp": "2026-01-16T16:12:11.449272"}
{"image_index": 300, "image_name": "000000032570.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032570.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1275.45, "latencies_ms": [1275.45], "images_per_second": 0.784, "prompt_tokens": 23, "response_tokens_est": 24, "n_tiles": 6, "output_text": "surfboard: 1\nsurfer: 1\nocean: 1\nwave: 1", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25587.4, "ram_available_mb": 100184.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 25587.4, "ram_available_mb": 100184.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.64, "peak": 14.91, "min": 14.22}, "VDD_GPU": {"avg": 33.92, "peak": 38.21, "min": 27.57}, "VIN": {"avg": 64.52, "peak": 77.38, "min": 56.69}, "VDD_CPU_SOC_MSS": {"avg": 15.22, "peak": 15.35, "min": 14.96}}, "power_watts_avg": 33.92, "energy_joules_est": 43.27, "sample_count": 9, "duration_seconds": 1.276}, "timestamp": "2026-01-16T16:12:12.731239"}
{"image_index": 300, "image_name": "000000032570.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032570.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1518.195, "latencies_ms": [1518.195], "images_per_second": 0.659, "prompt_tokens": 27, "response_tokens_est": 33, "n_tiles": 6, "output_text": "The main object in the foreground is a surfer riding a wave. The wave is in the background, and the surfer is near the water's surface.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25587.4, "ram_available_mb": 100184.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25587.7, "ram_available_mb": 100184.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.71, "peak": 15.01, "min": 14.22}, "VDD_GPU": {"avg": 32.66, "peak": 38.59, "min": 25.6}, "VIN": {"avg": 72.47, "peak": 121.35, "min": 60.04}, "VDD_CPU_SOC_MSS": {"avg": 15.24, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 32.66, "energy_joules_est": 49.59, "sample_count": 11, "duration_seconds": 1.518}, "timestamp": "2026-01-16T16:12:14.256012"}
{"image_index": 300, "image_name": "000000032570.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032570.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2592.805, "latencies_ms": [2592.805], "images_per_second": 0.386, "prompt_tokens": 21, "response_tokens_est": 73, "n_tiles": 6, "output_text": "The image captures a dynamic scene of a surfer riding a wave in the ocean. The surfer, dressed in a black wetsuit, is skillfully maneuvering a surfboard amidst the frothy white water. The setting is a sunny beach with clear blue skies, and the water appears to be a light turquoise color, indicating shallow waters.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25587.7, "ram_available_mb": 100184.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 25587.7, "ram_available_mb": 100184.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.91, "peak": 15.22, "min": 14.22}, "VDD_GPU": {"avg": 28.96, "peak": 38.59, "min": 23.64}, "VIN": {"avg": 69.53, "peak": 91.82, "min": 58.71}, "VDD_CPU_SOC_MSS": {"avg": 15.5, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 28.96, "energy_joules_est": 75.1, "sample_count": 19, "duration_seconds": 2.593}, "timestamp": "2026-01-16T16:12:16.855228"}
{"image_index": 300, "image_name": "000000032570.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032570.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2080.955, "latencies_ms": [2080.955], "images_per_second": 0.481, "prompt_tokens": 19, "response_tokens_est": 54, "n_tiles": 6, "output_text": "The image captures a surfer riding a wave in clear, blue-green water, with the sunlight creating a bright and dynamic atmosphere. The surfer is wearing a black wetsuit, and the wave is breaking with white foam, indicating a strong and powerful current.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25587.7, "ram_available_mb": 100184.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 25587.4, "ram_available_mb": 100184.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.9, "peak": 15.22, "min": 14.32}, "VDD_GPU": {"avg": 29.71, "peak": 38.19, "min": 24.03}, "VIN": {"avg": 71.67, "peak": 107.0, "min": 61.69}, "VDD_CPU_SOC_MSS": {"avg": 15.48, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 29.71, "energy_joules_est": 61.83, "sample_count": 16, "duration_seconds": 2.081}, "timestamp": "2026-01-16T16:12:18.942896"}
{"image_index": 301, "image_name": "000000032610.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032610.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1689.541, "latencies_ms": [1689.541], "images_per_second": 0.592, "prompt_tokens": 9, "response_tokens_est": 41, "n_tiles": 6, "output_text": "The image shows a cluttered desk with various electronic devices, including laptops, a tablet, and a smartphone, all connected to a network, and a black bag with a red interior is also present.", "error": null, "sys_before": {"cpu_percent": 41.9, "ram_used_mb": 25587.4, "ram_available_mb": 100184.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 25587.4, "ram_available_mb": 100184.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.54, "peak": 15.11, "min": 13.82}, "VDD_GPU": {"avg": 30.33, "peak": 37.03, "min": 24.43}, "VIN": {"avg": 76.36, "peak": 123.22, "min": 51.08}, "VDD_CPU_SOC_MSS": {"avg": 15.29, "peak": 15.75, "min": 14.57}}, "power_watts_avg": 30.33, "energy_joules_est": 51.26, "sample_count": 13, "duration_seconds": 1.69}, "timestamp": "2026-01-16T16:12:20.693438"}
{"image_index": 301, "image_name": "000000032610.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032610.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1679.69, "latencies_ms": [1679.69], "images_per_second": 0.595, "prompt_tokens": 23, "response_tokens_est": 39, "n_tiles": 6, "output_text": "1. Laptop bag\n2. Laptop\n3. Laptop bag\n4. Laptop bag\n5. Laptop bag\n6. Laptop bag\n7. Laptop bag\n8. Laptop bag", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 25587.4, "ram_available_mb": 100184.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 25587.6, "ram_available_mb": 100184.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.75, "peak": 15.11, "min": 14.22}, "VDD_GPU": {"avg": 31.71, "peak": 38.19, "min": 25.21}, "VIN": {"avg": 75.17, "peak": 113.68, "min": 56.19}, "VDD_CPU_SOC_MSS": {"avg": 15.32, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 31.71, "energy_joules_est": 53.27, "sample_count": 12, "duration_seconds": 1.68}, "timestamp": "2026-01-16T16:12:22.379418"}
{"image_index": 301, "image_name": "000000032610.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032610.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3353.711, "latencies_ms": [3353.711], "images_per_second": 0.298, "prompt_tokens": 27, "response_tokens_est": 101, "n_tiles": 6, "output_text": "The main objects in the image are a laptop, a black bag, and a black computer tower. The laptop is positioned in the foreground on the right side of the image, while the black bag is on the left side, closer to the foreground. The black computer tower is in the background, slightly to the right. The laptop screen is visible, displaying a webpage with various icons and text. The laptop and the computer tower are connected by a black cable, which is also visible on the table.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25587.6, "ram_available_mb": 100184.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 25587.6, "ram_available_mb": 100184.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.01, "peak": 15.22, "min": 14.22}, "VDD_GPU": {"avg": 27.56, "peak": 38.21, "min": 23.64}, "VIN": {"avg": 70.24, "peak": 108.65, "min": 58.27}, "VDD_CPU_SOC_MSS": {"avg": 15.6, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 27.56, "energy_joules_est": 92.44, "sample_count": 26, "duration_seconds": 3.354}, "timestamp": "2026-01-16T16:12:25.739399"}
{"image_index": 301, "image_name": "000000032610.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032610.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1889.708, "latencies_ms": [1889.708], "images_per_second": 0.529, "prompt_tokens": 21, "response_tokens_est": 47, "n_tiles": 6, "output_text": "The image depicts a cluttered workspace with various electronic devices and cables spread across a wooden surface. The setting appears to be a home or office environment, possibly a study or a workspace, with a focus on technology and electronics.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25587.6, "ram_available_mb": 100184.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 25587.6, "ram_available_mb": 100184.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.83, "peak": 15.11, "min": 14.32}, "VDD_GPU": {"avg": 30.7, "peak": 37.82, "min": 24.42}, "VIN": {"avg": 67.24, "peak": 85.81, "min": 56.93}, "VDD_CPU_SOC_MSS": {"avg": 15.38, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 30.7, "energy_joules_est": 58.02, "sample_count": 14, "duration_seconds": 1.89}, "timestamp": "2026-01-16T16:12:27.639269"}
{"image_index": 301, "image_name": "000000032610.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032610.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1889.21, "latencies_ms": [1889.21], "images_per_second": 0.529, "prompt_tokens": 19, "response_tokens_est": 47, "n_tiles": 6, "output_text": "The image shows a cluttered desk with various electronic devices, including laptops, a tablet, and a smartphone. The desk is cluttered with cables and devices, and the lighting is dim, suggesting an indoor setting with artificial light.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25587.6, "ram_available_mb": 100184.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 25587.6, "ram_available_mb": 100184.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.82, "peak": 15.11, "min": 14.22}, "VDD_GPU": {"avg": 30.64, "peak": 38.21, "min": 24.42}, "VIN": {"avg": 78.86, "peak": 112.66, "min": 65.16}, "VDD_CPU_SOC_MSS": {"avg": 15.38, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 30.64, "energy_joules_est": 57.89, "sample_count": 14, "duration_seconds": 1.889}, "timestamp": "2026-01-16T16:12:29.534629"}
{"image_index": 302, "image_name": "000000032735.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032735.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1549.613, "latencies_ms": [1549.613], "images_per_second": 0.645, "prompt_tokens": 9, "response_tokens_est": 36, "n_tiles": 6, "output_text": "A skier is captured mid-air, performing a jump over a snowy slope, with their skis angled sharply upwards and their body stretched out in a dynamic pose.", "error": null, "sys_before": {"cpu_percent": 44.9, "ram_used_mb": 25587.6, "ram_available_mb": 100184.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 25587.9, "ram_available_mb": 100184.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.48, "peak": 15.01, "min": 13.92}, "VDD_GPU": {"avg": 31.23, "peak": 37.03, "min": 25.6}, "VIN": {"avg": 69.37, "peak": 105.95, "min": 58.01}, "VDD_CPU_SOC_MSS": {"avg": 15.24, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 31.23, "energy_joules_est": 48.41, "sample_count": 11, "duration_seconds": 1.55}, "timestamp": "2026-01-16T16:12:31.144209"}
{"image_index": 302, "image_name": "000000032735.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032735.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1710.804, "latencies_ms": [1710.804], "images_per_second": 0.585, "prompt_tokens": 23, "response_tokens_est": 39, "n_tiles": 6, "output_text": "1. Skier\n2. Ski poles\n3. Ski\n4. Ski poles\n5. Ski jacket\n6. Ski pants\n7. Ski boots\n8. Ski helmet", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25587.9, "ram_available_mb": 100184.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 25587.9, "ram_available_mb": 100184.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.68, "peak": 15.01, "min": 14.12}, "VDD_GPU": {"avg": 31.02, "peak": 38.59, "min": 24.42}, "VIN": {"avg": 68.0, "peak": 98.52, "min": 55.95}, "VDD_CPU_SOC_MSS": {"avg": 15.2, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 31.02, "energy_joules_est": 53.08, "sample_count": 13, "duration_seconds": 1.711}, "timestamp": "2026-01-16T16:12:32.861366"}
{"image_index": 302, "image_name": "000000032735.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032735.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2692.378, "latencies_ms": [2692.378], "images_per_second": 0.371, "prompt_tokens": 27, "response_tokens_est": 75, "n_tiles": 6, "output_text": "The main object in the foreground is a skier in mid-air, performing a jump. The skier is wearing a bright orange and green suit, and is holding ski poles. The background features a snowy mountain slope, with another skier visible further away. The skier in the foreground is closer to the camera, while the other skier is further away.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25587.9, "ram_available_mb": 100184.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 25587.6, "ram_available_mb": 100184.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.84, "peak": 15.11, "min": 14.02}, "VDD_GPU": {"avg": 28.23, "peak": 38.6, "min": 23.24}, "VIN": {"avg": 71.43, "peak": 114.81, "min": 61.97}, "VDD_CPU_SOC_MSS": {"avg": 15.43, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 28.23, "energy_joules_est": 76.01, "sample_count": 21, "duration_seconds": 2.693}, "timestamp": "2026-01-16T16:12:35.559973"}
{"image_index": 302, "image_name": "000000032735.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032735.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3015.835, "latencies_ms": [3015.835], "images_per_second": 0.332, "prompt_tokens": 21, "response_tokens_est": 87, "n_tiles": 6, "output_text": "The image captures a dynamic scene of a skier in mid-air, performing a jump over a snowy slope. The skier is dressed in a vibrant red and green suit, with a green helmet and goggles, and is holding ski poles. In the background, another skier is visible, also in motion, descending the slope. The sky is clear, and the overall setting suggests a sunny day at a ski resort.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25587.6, "ram_available_mb": 100184.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 25587.6, "ram_available_mb": 100184.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.91, "peak": 15.22, "min": 14.12}, "VDD_GPU": {"avg": 27.68, "peak": 37.8, "min": 23.24}, "VIN": {"avg": 68.21, "peak": 105.16, "min": 56.02}, "VDD_CPU_SOC_MSS": {"avg": 15.49, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 27.68, "energy_joules_est": 83.49, "sample_count": 23, "duration_seconds": 3.016}, "timestamp": "2026-01-16T16:12:38.582145"}
{"image_index": 302, "image_name": "000000032735.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032735.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2965.043, "latencies_ms": [2965.043], "images_per_second": 0.337, "prompt_tokens": 19, "response_tokens_est": 85, "n_tiles": 6, "output_text": "The image captures a skier in mid-air, dressed in a vibrant red and green suit, with a green helmet and goggles. The skier is performing a jump over a snowy slope, with a clear blue sky and a few clouds in the background. The lighting is bright and natural, suggesting it is daytime, and the snow appears to be freshly groomed, indicating a well-maintained ski resort.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25587.6, "ram_available_mb": 100184.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 25587.6, "ram_available_mb": 100184.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.88, "peak": 15.11, "min": 14.12}, "VDD_GPU": {"avg": 27.76, "peak": 37.82, "min": 23.24}, "VIN": {"avg": 68.19, "peak": 111.33, "min": 56.21}, "VDD_CPU_SOC_MSS": {"avg": 15.46, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 27.76, "energy_joules_est": 82.32, "sample_count": 23, "duration_seconds": 2.965}, "timestamp": "2026-01-16T16:12:41.553502"}
{"image_index": 303, "image_name": "000000032811.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032811.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2253.201, "latencies_ms": [2253.201], "images_per_second": 0.444, "prompt_tokens": 9, "response_tokens_est": 25, "n_tiles": 12, "output_text": "A small bird is perched on the rusty metal railing of a boat, looking out at the calm water.", "error": null, "sys_before": {"cpu_percent": 51.4, "ram_used_mb": 25587.6, "ram_available_mb": 100184.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 25587.6, "ram_available_mb": 100184.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.14, "peak": 16.05, "min": 13.9}, "VDD_GPU": {"avg": 35.24, "peak": 40.57, "min": 28.36}, "VIN": {"avg": 71.38, "peak": 107.93, "min": 58.49}, "VDD_CPU_SOC_MSS": {"avg": 15.98, "peak": 17.32, "min": 14.96}}, "power_watts_avg": 35.24, "energy_joules_est": 79.41, "sample_count": 17, "duration_seconds": 2.254}, "timestamp": "2026-01-16T16:12:43.884420"}
{"image_index": 303, "image_name": "000000032811.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032811.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2414.017, "latencies_ms": [2414.017], "images_per_second": 0.414, "prompt_tokens": 23, "response_tokens_est": 26, "n_tiles": 12, "output_text": "bird: 1\nwindow: 1\ndoor: 1\nframe: 1\ndoor handle: 1", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25587.6, "ram_available_mb": 100184.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 2.6, "ram_used_mb": 25587.6, "ram_available_mb": 100184.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.08, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 36.72, "peak": 42.94, "min": 28.36}, "VIN": {"avg": 75.97, "peak": 105.07, "min": 58.81}, "VDD_CPU_SOC_MSS": {"avg": 15.7, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 36.72, "energy_joules_est": 88.66, "sample_count": 18, "duration_seconds": 2.414}, "timestamp": "2026-01-16T16:12:46.305161"}
{"image_index": 303, "image_name": "000000032811.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032811.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3460.851, "latencies_ms": [3460.851], "images_per_second": 0.289, "prompt_tokens": 27, "response_tokens_est": 57, "n_tiles": 12, "output_text": "The main object in the foreground is a bird standing on the rusty metal railing of a boat. The bird is positioned near the railing, which is located near the water. The background features a dark, dimly lit room with a window that overlooks the water.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25587.6, "ram_available_mb": 100184.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 25587.6, "ram_available_mb": 100184.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.06, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 33.38, "peak": 42.54, "min": 26.39}, "VIN": {"avg": 74.74, "peak": 116.74, "min": 58.81}, "VDD_CPU_SOC_MSS": {"avg": 15.61, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 33.38, "energy_joules_est": 115.53, "sample_count": 27, "duration_seconds": 3.461}, "timestamp": "2026-01-16T16:12:49.772748"}
{"image_index": 303, "image_name": "000000032811.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032811.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2954.746, "latencies_ms": [2954.746], "images_per_second": 0.338, "prompt_tokens": 21, "response_tokens_est": 42, "n_tiles": 12, "output_text": "The image depicts a dimly lit room with a large, old-fashioned television set on a stand. A bird is perched on the edge of the television screen, seemingly observing the scene outside.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25587.6, "ram_available_mb": 100184.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 25587.8, "ram_available_mb": 100184.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.06, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 34.37, "peak": 42.54, "min": 26.79}, "VIN": {"avg": 74.48, "peak": 113.43, "min": 58.16}, "VDD_CPU_SOC_MSS": {"avg": 15.66, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 34.37, "energy_joules_est": 101.57, "sample_count": 23, "duration_seconds": 2.955}, "timestamp": "2026-01-16T16:12:52.734358"}
{"image_index": 303, "image_name": "000000032811.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032811.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3014.168, "latencies_ms": [3014.168], "images_per_second": 0.332, "prompt_tokens": 19, "response_tokens_est": 44, "n_tiles": 12, "output_text": "The image shows a dark, dimly lit room with a rusted window frame and a small, dark bird perched on the ledge. The lighting is low, casting shadows and creating a moody atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25587.8, "ram_available_mb": 100184.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25587.6, "ram_available_mb": 100184.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.07, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 34.39, "peak": 42.15, "min": 26.79}, "VIN": {"avg": 73.65, "peak": 118.12, "min": 58.72}, "VDD_CPU_SOC_MSS": {"avg": 15.75, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 34.39, "energy_joules_est": 103.67, "sample_count": 23, "duration_seconds": 3.014}, "timestamp": "2026-01-16T16:12:55.758122"}
{"image_index": 304, "image_name": "000000032817.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032817.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2023.241, "latencies_ms": [2023.241], "images_per_second": 0.494, "prompt_tokens": 9, "response_tokens_est": 18, "n_tiles": 12, "output_text": "A man is bending over a toilet in a bathroom, picking up a white towel.", "error": null, "sys_before": {"cpu_percent": 43.9, "ram_used_mb": 25587.6, "ram_available_mb": 100184.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 25587.6, "ram_available_mb": 100184.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.1, "peak": 16.15, "min": 13.69}, "VDD_GPU": {"avg": 36.63, "peak": 41.36, "min": 29.94}, "VIN": {"avg": 81.2, "peak": 130.89, "min": 57.22}, "VDD_CPU_SOC_MSS": {"avg": 15.98, "peak": 17.32, "min": 14.57}}, "power_watts_avg": 36.63, "energy_joules_est": 74.13, "sample_count": 15, "duration_seconds": 2.024}, "timestamp": "2026-01-16T16:12:57.881630"}
{"image_index": 304, "image_name": "000000032817.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032817.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 3037.291, "latencies_ms": [3037.291], "images_per_second": 0.329, "prompt_tokens": 23, "response_tokens_est": 44, "n_tiles": 12, "output_text": "1. Toilet\n2. Trash can\n3. Shelf\n4. Trash bag\n5. Shelf\n6. Trash bag\n7. Shelf\n8. Trash bag", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25587.6, "ram_available_mb": 100184.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25587.6, "ram_available_mb": 100184.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.0, "peak": 15.65, "min": 14.43}, "VDD_GPU": {"avg": 34.78, "peak": 43.73, "min": 26.39}, "VIN": {"avg": 74.63, "peak": 119.33, "min": 58.94}, "VDD_CPU_SOC_MSS": {"avg": 15.68, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 34.78, "energy_joules_est": 105.65, "sample_count": 23, "duration_seconds": 3.038}, "timestamp": "2026-01-16T16:13:00.925769"}
{"image_index": 304, "image_name": "000000032817.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032817.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3303.578, "latencies_ms": [3303.578], "images_per_second": 0.303, "prompt_tokens": 27, "response_tokens_est": 52, "n_tiles": 12, "output_text": "The main objects in the image are a toilet, a trash can, and a shelf. The toilet is located in the foreground, while the trash can is positioned near the toilet. The shelf is situated in the background, providing storage space for various items.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25587.6, "ram_available_mb": 100184.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 25587.6, "ram_available_mb": 100184.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.75, "peak": 15.34, "min": 14.12}, "VDD_GPU": {"avg": 33.45, "peak": 42.53, "min": 26.0}, "VIN": {"avg": 72.83, "peak": 116.41, "min": 59.17}, "VDD_CPU_SOC_MSS": {"avg": 15.31, "peak": 16.14, "min": 14.56}}, "power_watts_avg": 33.45, "energy_joules_est": 110.52, "sample_count": 26, "duration_seconds": 3.304}, "timestamp": "2026-01-16T16:13:04.236251"}
{"image_index": 304, "image_name": "000000032817.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032817.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2642.341, "latencies_ms": [2642.341], "images_per_second": 0.378, "prompt_tokens": 21, "response_tokens_est": 33, "n_tiles": 12, "output_text": "The image depicts a cluttered bathroom with a toilet and a trash can. A person is seen bending over the toilet, possibly cleaning or organizing the area.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25587.6, "ram_available_mb": 100184.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 25587.8, "ram_available_mb": 100184.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.76, "peak": 15.34, "min": 14.12}, "VDD_GPU": {"avg": 35.57, "peak": 42.54, "min": 27.18}, "VIN": {"avg": 74.28, "peak": 120.87, "min": 57.4}, "VDD_CPU_SOC_MSS": {"avg": 15.37, "peak": 16.14, "min": 14.56}}, "power_watts_avg": 35.57, "energy_joules_est": 94.01, "sample_count": 20, "duration_seconds": 2.643}, "timestamp": "2026-01-16T16:13:06.890493"}
{"image_index": 304, "image_name": "000000032817.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032817.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2470.904, "latencies_ms": [2470.904], "images_per_second": 0.405, "prompt_tokens": 19, "response_tokens_est": 28, "n_tiles": 12, "output_text": "The image shows a bathroom with a white toilet and a black trash can. The lighting is dim, and the walls are light-colored.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25587.8, "ram_available_mb": 100184.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 25587.6, "ram_available_mb": 100184.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.75, "peak": 15.34, "min": 14.12}, "VDD_GPU": {"avg": 36.24, "peak": 42.94, "min": 27.57}, "VIN": {"avg": 82.91, "peak": 133.65, "min": 58.68}, "VDD_CPU_SOC_MSS": {"avg": 15.33, "peak": 16.14, "min": 14.56}}, "power_watts_avg": 36.24, "energy_joules_est": 89.57, "sample_count": 19, "duration_seconds": 2.472}, "timestamp": "2026-01-16T16:13:09.371256"}
{"image_index": 305, "image_name": "000000032861.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032861.jpg", "image_width": 426, "image_height": 640, "image_resolution": "426x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1258.875, "latencies_ms": [1258.875], "images_per_second": 0.794, "prompt_tokens": 9, "response_tokens_est": 25, "n_tiles": 6, "output_text": "A man is standing in a rainy room, holding an umbrella, and looking at his reflection in the doorway.", "error": null, "sys_before": {"cpu_percent": 36.9, "ram_used_mb": 25587.6, "ram_available_mb": 100184.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25587.6, "ram_available_mb": 100184.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.19, "peak": 14.81, "min": 13.72}, "VDD_GPU": {"avg": 33.26, "peak": 37.8, "min": 27.18}, "VIN": {"avg": 78.83, "peak": 126.79, "min": 61.27}, "VDD_CPU_SOC_MSS": {"avg": 14.87, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 33.26, "energy_joules_est": 41.89, "sample_count": 9, "duration_seconds": 1.259}, "timestamp": "2026-01-16T16:13:10.693490"}
{"image_index": 305, "image_name": "000000032861.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032861.jpg", "image_width": 426, "image_height": 640, "image_resolution": "426x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1537.832, "latencies_ms": [1537.832], "images_per_second": 0.65, "prompt_tokens": 23, "response_tokens_est": 34, "n_tiles": 6, "output_text": "1. Person\n2. Rain\n3. Door\n4. Wall\n5. Mirror\n6. Umbrella\n7. Window\n8. Light", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25587.6, "ram_available_mb": 100184.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25587.6, "ram_available_mb": 100184.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.75, "peak": 15.11, "min": 14.22}, "VDD_GPU": {"avg": 32.37, "peak": 38.6, "min": 25.6}, "VIN": {"avg": 74.86, "peak": 106.47, "min": 62.79}, "VDD_CPU_SOC_MSS": {"avg": 15.35, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 32.37, "energy_joules_est": 49.79, "sample_count": 12, "duration_seconds": 1.538}, "timestamp": "2026-01-16T16:13:12.237507"}
{"image_index": 305, "image_name": "000000032861.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032861.jpg", "image_width": 426, "image_height": 640, "image_resolution": "426x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2919.307, "latencies_ms": [2919.307], "images_per_second": 0.343, "prompt_tokens": 27, "response_tokens_est": 82, "n_tiles": 6, "output_text": "The main object in the foreground is a person holding an umbrella, standing in a corridor with red walls. The person is positioned near the center of the image, slightly to the left. The background features a doorway with a red door and a white frame, and a framed picture hanging on the wall. The person's reflection is visible in the doorway, creating a sense of depth in the scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25587.6, "ram_available_mb": 100184.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 25587.6, "ram_available_mb": 100184.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.84, "peak": 15.11, "min": 14.12}, "VDD_GPU": {"avg": 27.83, "peak": 38.59, "min": 23.24}, "VIN": {"avg": 66.56, "peak": 104.02, "min": 57.82}, "VDD_CPU_SOC_MSS": {"avg": 15.44, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 27.83, "energy_joules_est": 81.26, "sample_count": 23, "duration_seconds": 2.92}, "timestamp": "2026-01-16T16:13:15.163130"}
{"image_index": 305, "image_name": "000000032861.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032861.jpg", "image_width": 426, "image_height": 640, "image_resolution": "426x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2467.109, "latencies_ms": [2467.109], "images_per_second": 0.405, "prompt_tokens": 21, "response_tokens_est": 66, "n_tiles": 6, "output_text": "The scene depicts a man standing in a corridor, holding an umbrella to shield himself from the rain. The rain is visibly falling, creating a dynamic and somewhat dramatic atmosphere. The man is dressed in a blue shirt and dark pants, and the corridor is painted in a striking red color, adding to the visual contrast.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25587.6, "ram_available_mb": 100184.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 25587.5, "ram_available_mb": 100184.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.78, "peak": 15.01, "min": 14.12}, "VDD_GPU": {"avg": 28.44, "peak": 37.8, "min": 23.24}, "VIN": {"avg": 67.89, "peak": 103.38, "min": 62.47}, "VDD_CPU_SOC_MSS": {"avg": 15.35, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 28.44, "energy_joules_est": 70.18, "sample_count": 19, "duration_seconds": 2.468}, "timestamp": "2026-01-16T16:13:17.636611"}
{"image_index": 305, "image_name": "000000032861.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032861.jpg", "image_width": 426, "image_height": 640, "image_resolution": "426x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2518.343, "latencies_ms": [2518.343], "images_per_second": 0.397, "prompt_tokens": 19, "response_tokens_est": 68, "n_tiles": 6, "output_text": "The image depicts a person standing in a corridor under a dark blue umbrella, which is being used to shield from the rain. The rain is falling heavily, creating a dynamic and somewhat dramatic visual effect. The lighting is dim, with the person's reflection visible in the mirror, adding a layer of depth and complexity to the scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25587.5, "ram_available_mb": 100184.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 25587.8, "ram_available_mb": 100184.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.76, "peak": 15.01, "min": 14.02}, "VDD_GPU": {"avg": 28.36, "peak": 38.21, "min": 23.24}, "VIN": {"avg": 71.78, "peak": 114.71, "min": 60.28}, "VDD_CPU_SOC_MSS": {"avg": 15.37, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 28.36, "energy_joules_est": 71.43, "sample_count": 20, "duration_seconds": 2.519}, "timestamp": "2026-01-16T16:13:20.161304"}
{"image_index": 306, "image_name": "000000032887.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032887.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2314.648, "latencies_ms": [2314.648], "images_per_second": 0.432, "prompt_tokens": 9, "response_tokens_est": 27, "n_tiles": 12, "output_text": "A man in hiking gear stands on a rocky path surrounded by lush greenery, with a signpost in the background indicating directions.", "error": null, "sys_before": {"cpu_percent": 44.9, "ram_used_mb": 25587.5, "ram_available_mb": 100184.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 2.6, "ram_used_mb": 25587.5, "ram_available_mb": 100184.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.1, "peak": 16.15, "min": 13.79}, "VDD_GPU": {"avg": 35.19, "peak": 40.97, "min": 27.97}, "VIN": {"avg": 77.44, "peak": 116.19, "min": 58.59}, "VDD_CPU_SOC_MSS": {"avg": 16.05, "peak": 17.32, "min": 14.96}}, "power_watts_avg": 35.19, "energy_joules_est": 81.46, "sample_count": 18, "duration_seconds": 2.315}, "timestamp": "2026-01-16T16:13:22.568480"}
{"image_index": 306, "image_name": "000000032887.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032887.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 3585.547, "latencies_ms": [3585.547], "images_per_second": 0.279, "prompt_tokens": 23, "response_tokens_est": 61, "n_tiles": 12, "output_text": "1. Man: 1\n2. Backpack: 1\n3. Hiking stick: 1\n4. Rock: 1\n5. Path: 1\n6. Waterfall: 1\n7. Tree: 1\n8. Stone steps: 1", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 25587.5, "ram_available_mb": 100184.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25587.8, "ram_available_mb": 100184.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.09, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 33.22, "peak": 42.94, "min": 26.39}, "VIN": {"avg": 74.02, "peak": 134.28, "min": 56.09}, "VDD_CPU_SOC_MSS": {"avg": 15.77, "peak": 16.53, "min": 15.35}}, "power_watts_avg": 33.22, "energy_joules_est": 119.13, "sample_count": 28, "duration_seconds": 3.586}, "timestamp": "2026-01-16T16:13:26.160613"}
{"image_index": 306, "image_name": "000000032887.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032887.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3679.638, "latencies_ms": [3679.638], "images_per_second": 0.272, "prompt_tokens": 27, "response_tokens_est": 64, "n_tiles": 12, "output_text": "The main object in the foreground is a man wearing a red long-sleeve shirt and blue jeans, equipped with a backpack and walking stick. He is standing on a rocky path surrounded by greenery and rocks. In the background, there is a signpost with directional arrows, indicating the path's direction.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25587.8, "ram_available_mb": 100184.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25588.0, "ram_available_mb": 100184.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.11, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 33.04, "peak": 42.94, "min": 26.39}, "VIN": {"avg": 73.48, "peak": 110.15, "min": 58.85}, "VDD_CPU_SOC_MSS": {"avg": 15.75, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 33.04, "energy_joules_est": 121.59, "sample_count": 28, "duration_seconds": 3.68}, "timestamp": "2026-01-16T16:13:29.850094"}
{"image_index": 306, "image_name": "000000032887.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032887.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3348.514, "latencies_ms": [3348.514], "images_per_second": 0.299, "prompt_tokens": 21, "response_tokens_est": 54, "n_tiles": 12, "output_text": "The image depicts a hiker in a forested area, equipped with a backpack and trekking poles, navigating a rocky path. The hiker is surrounded by lush greenery and appears to be in a natural setting, possibly a park or a hiking trail.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25588.0, "ram_available_mb": 100184.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25587.8, "ram_available_mb": 100184.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.09, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 33.57, "peak": 42.94, "min": 26.39}, "VIN": {"avg": 74.28, "peak": 115.75, "min": 58.4}, "VDD_CPU_SOC_MSS": {"avg": 15.72, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 33.57, "energy_joules_est": 112.43, "sample_count": 26, "duration_seconds": 3.349}, "timestamp": "2026-01-16T16:13:33.209266"}
{"image_index": 306, "image_name": "000000032887.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032887.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3715.2, "latencies_ms": [3715.2], "images_per_second": 0.269, "prompt_tokens": 19, "response_tokens_est": 65, "n_tiles": 12, "output_text": "The image features a person in a red long-sleeve shirt and blue jeans, equipped with a backpack and walking stick, standing on a rocky path surrounded by lush greenery. The lighting is natural, suggesting it is daytime, and the weather appears to be mild, with no visible signs of rain or extreme heat.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25587.8, "ram_available_mb": 100184.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25587.8, "ram_available_mb": 100184.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.09, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 32.8, "peak": 42.54, "min": 26.39}, "VIN": {"avg": 71.1, "peak": 106.87, "min": 58.53}, "VDD_CPU_SOC_MSS": {"avg": 15.61, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 32.8, "energy_joules_est": 121.87, "sample_count": 29, "duration_seconds": 3.716}, "timestamp": "2026-01-16T16:13:36.931611"}
{"image_index": 307, "image_name": "000000032901.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032901.jpg", "image_width": 640, "image_height": 546, "image_resolution": "640x546", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2245.871, "latencies_ms": [2245.871], "images_per_second": 0.445, "prompt_tokens": 9, "response_tokens_est": 25, "n_tiles": 12, "output_text": "The image shows three men standing together in a room, with two of them smiling and one man leaning on a chair.", "error": null, "sys_before": {"cpu_percent": 42.9, "ram_used_mb": 25587.8, "ram_available_mb": 100184.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 2.6, "ram_used_mb": 25587.8, "ram_available_mb": 100184.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.14, "peak": 16.15, "min": 13.9}, "VDD_GPU": {"avg": 35.92, "peak": 41.36, "min": 28.37}, "VIN": {"avg": 74.85, "peak": 106.54, "min": 58.76}, "VDD_CPU_SOC_MSS": {"avg": 15.91, "peak": 17.32, "min": 14.96}}, "power_watts_avg": 35.92, "energy_joules_est": 80.68, "sample_count": 17, "duration_seconds": 2.246}, "timestamp": "2026-01-16T16:13:39.264786"}
{"image_index": 307, "image_name": "000000032901.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032901.jpg", "image_width": 640, "image_height": 546, "image_resolution": "640x546", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2474.442, "latencies_ms": [2474.442], "images_per_second": 0.404, "prompt_tokens": 23, "response_tokens_est": 28, "n_tiles": 12, "output_text": "- Two men\n- Two men\n- Two men\n- Two men\n- Two men\n- Two men\n- Two men", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25587.8, "ram_available_mb": 100184.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 2.6, "ram_used_mb": 25588.0, "ram_available_mb": 100184.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.07, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 36.4, "peak": 43.33, "min": 27.97}, "VIN": {"avg": 77.71, "peak": 116.24, "min": 58.55}, "VDD_CPU_SOC_MSS": {"avg": 15.72, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 36.4, "energy_joules_est": 90.09, "sample_count": 19, "duration_seconds": 2.475}, "timestamp": "2026-01-16T16:13:41.749965"}
{"image_index": 307, "image_name": "000000032901.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032901.jpg", "image_width": 640, "image_height": 546, "image_resolution": "640x546", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 5097.694, "latencies_ms": [5097.694], "images_per_second": 0.196, "prompt_tokens": 27, "response_tokens_est": 106, "n_tiles": 12, "output_text": "The main objects in the image are three men standing in a room. The man on the left is wearing a checkered shirt and has his left hand on the shoulder of the man next to him. The man in the middle is wearing a striped shirt and has his right hand on the shoulder of the man on the right. The man on the right is wearing a light blue shirt and has his left hand on the back of the man next to him. The background includes a red chair and a shelf with various bottles.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25588.0, "ram_available_mb": 100184.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25587.8, "ram_available_mb": 100184.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.12, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 31.28, "peak": 43.33, "min": 26.39}, "VIN": {"avg": 71.18, "peak": 105.86, "min": 58.73}, "VDD_CPU_SOC_MSS": {"avg": 15.69, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 31.28, "energy_joules_est": 159.47, "sample_count": 40, "duration_seconds": 5.098}, "timestamp": "2026-01-16T16:13:46.854451"}
{"image_index": 307, "image_name": "000000032901.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032901.jpg", "image_width": 640, "image_height": 546, "image_resolution": "640x546", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 4358.782, "latencies_ms": [4358.782], "images_per_second": 0.229, "prompt_tokens": 21, "response_tokens_est": 84, "n_tiles": 12, "output_text": "The image depicts a group of three men standing in an indoor setting, likely a room or hall. They are smiling and appear to be posing for a photo. The men are dressed in casual to semi-formal attire, with one wearing a checkered shirt and the other two in button-up shirts. The background shows a red wall and some chairs, suggesting the setting could be a social gathering or event.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25587.8, "ram_available_mb": 100184.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 25588.0, "ram_available_mb": 100184.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.1, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 31.86, "peak": 42.53, "min": 26.0}, "VIN": {"avg": 72.16, "peak": 116.21, "min": 58.37}, "VDD_CPU_SOC_MSS": {"avg": 15.76, "peak": 16.53, "min": 15.35}}, "power_watts_avg": 31.86, "energy_joules_est": 138.89, "sample_count": 34, "duration_seconds": 4.359}, "timestamp": "2026-01-16T16:13:51.219912"}
{"image_index": 307, "image_name": "000000032901.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032901.jpg", "image_width": 640, "image_height": 546, "image_resolution": "640x546", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 4726.591, "latencies_ms": [4726.591], "images_per_second": 0.212, "prompt_tokens": 19, "response_tokens_est": 95, "n_tiles": 12, "output_text": "The image features three men standing in an indoor setting. The man on the left is wearing a light blue checkered shirt and dark pants, the man in the middle is dressed in a white shirt and light-colored pants, and the man on the right is wearing a light blue shirt and dark pants. The lighting is warm and ambient, with a soft glow illuminating the scene. The colors are muted, with the men's clothing and the background being primarily neutral tones.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25588.0, "ram_available_mb": 100184.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 25587.7, "ram_available_mb": 100184.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.1, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 31.42, "peak": 42.54, "min": 26.0}, "VIN": {"avg": 74.87, "peak": 118.17, "min": 58.61}, "VDD_CPU_SOC_MSS": {"avg": 15.66, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 31.42, "energy_joules_est": 148.53, "sample_count": 37, "duration_seconds": 4.727}, "timestamp": "2026-01-16T16:13:55.957584"}
{"image_index": 308, "image_name": "000000032941.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032941.jpg", "image_width": 458, "image_height": 640, "image_resolution": "458x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2184.982, "latencies_ms": [2184.982], "images_per_second": 0.458, "prompt_tokens": 9, "response_tokens_est": 23, "n_tiles": 12, "output_text": "A yellow traffic sign with a black border is mounted on a black pole, indicating a one-way traffic direction.", "error": null, "sys_before": {"cpu_percent": 43.9, "ram_used_mb": 25587.7, "ram_available_mb": 100184.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 2.5, "ram_used_mb": 25587.7, "ram_available_mb": 100184.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.12, "peak": 16.15, "min": 13.79}, "VDD_GPU": {"avg": 35.66, "peak": 41.36, "min": 28.36}, "VIN": {"avg": 77.28, "peak": 120.37, "min": 58.39}, "VDD_CPU_SOC_MSS": {"avg": 15.91, "peak": 17.32, "min": 14.96}}, "power_watts_avg": 35.66, "energy_joules_est": 77.95, "sample_count": 17, "duration_seconds": 2.186}, "timestamp": "2026-01-16T16:13:58.232017"}
{"image_index": 308, "image_name": "000000032941.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032941.jpg", "image_width": 458, "image_height": 640, "image_resolution": "458x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2906.798, "latencies_ms": [2906.798], "images_per_second": 0.344, "prompt_tokens": 23, "response_tokens_est": 40, "n_tiles": 12, "output_text": "object: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25587.7, "ram_available_mb": 100184.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 25588.0, "ram_available_mb": 100184.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.97, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 34.8, "peak": 43.33, "min": 26.4}, "VIN": {"avg": 75.44, "peak": 111.05, "min": 58.5}, "VDD_CPU_SOC_MSS": {"avg": 15.51, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 34.8, "energy_joules_est": 101.17, "sample_count": 23, "duration_seconds": 2.907}, "timestamp": "2026-01-16T16:14:01.149563"}
{"image_index": 308, "image_name": "000000032941.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032941.jpg", "image_width": 458, "image_height": 640, "image_resolution": "458x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3886.577, "latencies_ms": [3886.577], "images_per_second": 0.257, "prompt_tokens": 27, "response_tokens_est": 69, "n_tiles": 12, "output_text": "The main objects in the image are a traffic light, a yellow sign, and a street corner. The traffic light is positioned in the foreground, near the sidewalk, while the yellow sign is placed on the sidewalk near the street corner. The street corner is situated in the background, providing a clear spatial relationship between the foreground and background objects.", "error": null, "sys_before": {"cpu_percent": 16.7, "ram_used_mb": 25588.0, "ram_available_mb": 100184.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25588.0, "ram_available_mb": 100184.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.75, "peak": 15.34, "min": 14.12}, "VDD_GPU": {"avg": 32.34, "peak": 42.53, "min": 25.6}, "VIN": {"avg": 72.76, "peak": 107.12, "min": 58.58}, "VDD_CPU_SOC_MSS": {"avg": 15.33, "peak": 16.14, "min": 14.56}}, "power_watts_avg": 32.34, "energy_joules_est": 125.71, "sample_count": 30, "duration_seconds": 3.887}, "timestamp": "2026-01-16T16:14:05.044661"}
{"image_index": 308, "image_name": "000000032941.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032941.jpg", "image_width": 458, "image_height": 640, "image_resolution": "458x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 4846.669, "latencies_ms": [4846.669], "images_per_second": 0.206, "prompt_tokens": 21, "response_tokens_est": 97, "n_tiles": 12, "output_text": "The image depicts a street scene in an urban area during the daytime. The street is lined with buildings, and there is a traffic light visible in the foreground. A yellow sign with a black border is placed on the sidewalk, and a black metal structure is situated on the sidewalk. A few cars are visible on the road, and a red double-decker bus is parked on the side. The sky is overcast, and there are a few pedestrians walking on the sidewalk.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 25588.0, "ram_available_mb": 100184.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25588.2, "ram_available_mb": 100184.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.74, "peak": 15.34, "min": 14.12}, "VDD_GPU": {"avg": 31.1, "peak": 42.54, "min": 25.6}, "VIN": {"avg": 71.13, "peak": 106.28, "min": 56.94}, "VDD_CPU_SOC_MSS": {"avg": 15.28, "peak": 16.14, "min": 14.96}}, "power_watts_avg": 31.1, "energy_joules_est": 150.75, "sample_count": 38, "duration_seconds": 4.847}, "timestamp": "2026-01-16T16:14:09.898140"}
{"image_index": 308, "image_name": "000000032941.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000032941.jpg", "image_width": 458, "image_height": 640, "image_resolution": "458x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3742.537, "latencies_ms": [3742.537], "images_per_second": 0.267, "prompt_tokens": 19, "response_tokens_est": 65, "n_tiles": 12, "output_text": "The image depicts a street scene with a yellow traffic sign mounted on a black pole. The sign is illuminated by a bright light, indicating it is likely daytime. The surrounding area is wet, suggesting recent rain, and the buildings are made of brick and stone, with some windows showing reflections of the sky.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25588.2, "ram_available_mb": 100184.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25588.2, "ram_available_mb": 100184.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.73, "peak": 15.34, "min": 14.12}, "VDD_GPU": {"avg": 32.52, "peak": 42.53, "min": 25.6}, "VIN": {"avg": 71.85, "peak": 121.76, "min": 58.21}, "VDD_CPU_SOC_MSS": {"avg": 15.21, "peak": 16.14, "min": 14.56}}, "power_watts_avg": 32.52, "energy_joules_est": 121.73, "sample_count": 30, "duration_seconds": 3.743}, "timestamp": "2026-01-16T16:14:13.651692"}
{"image_index": 309, "image_name": "000000033005.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033005.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1178.34, "latencies_ms": [1178.34], "images_per_second": 0.849, "prompt_tokens": 9, "response_tokens_est": 22, "n_tiles": 6, "output_text": "A young man is standing on a tennis court, preparing to hit a tennis ball with his racket.", "error": null, "sys_before": {"cpu_percent": 42.0, "ram_used_mb": 25588.2, "ram_available_mb": 100184.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25588.5, "ram_available_mb": 100183.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.13, "peak": 14.61, "min": 13.72}, "VDD_GPU": {"avg": 33.53, "peak": 37.41, "min": 28.36}, "VIN": {"avg": 74.93, "peak": 123.07, "min": 56.94}, "VDD_CPU_SOC_MSS": {"avg": 14.81, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 33.53, "energy_joules_est": 39.52, "sample_count": 8, "duration_seconds": 1.179}, "timestamp": "2026-01-16T16:14:14.899836"}
{"image_index": 309, "image_name": "000000033005.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033005.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1725.021, "latencies_ms": [1725.021], "images_per_second": 0.58, "prompt_tokens": 23, "response_tokens_est": 41, "n_tiles": 6, "output_text": "1. Tennis court\n2. Tennis player\n3. Tennis racket\n4. Tennis ball\n5. Fence\n6. Green net\n7. Green fence\n8. Red sign", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25588.5, "ram_available_mb": 100183.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25588.2, "ram_available_mb": 100184.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.77, "peak": 15.22, "min": 14.22}, "VDD_GPU": {"avg": 31.87, "peak": 38.98, "min": 24.82}, "VIN": {"avg": 72.42, "peak": 104.38, "min": 61.69}, "VDD_CPU_SOC_MSS": {"avg": 15.38, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 31.87, "energy_joules_est": 54.99, "sample_count": 13, "duration_seconds": 1.725}, "timestamp": "2026-01-16T16:14:16.631096"}
{"image_index": 309, "image_name": "000000033005.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033005.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2582.136, "latencies_ms": [2582.136], "images_per_second": 0.387, "prompt_tokens": 27, "response_tokens_est": 73, "n_tiles": 6, "output_text": "The main object in the foreground is a young man standing on a tennis court, holding a tennis racket. The background features a green fence and a sign with the text \"VOX SPORTS\" and \"prince rule the court.\" The tennis court is surrounded by a fence, and the sign is positioned near the top right corner of the image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25588.2, "ram_available_mb": 100184.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 25588.2, "ram_available_mb": 100184.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.01, "peak": 15.32, "min": 14.32}, "VDD_GPU": {"avg": 28.85, "peak": 38.59, "min": 23.64}, "VIN": {"avg": 72.95, "peak": 110.51, "min": 63.67}, "VDD_CPU_SOC_MSS": {"avg": 15.65, "peak": 16.14, "min": 14.96}}, "power_watts_avg": 28.85, "energy_joules_est": 74.51, "sample_count": 20, "duration_seconds": 2.583}, "timestamp": "2026-01-16T16:14:19.219738"}
{"image_index": 309, "image_name": "000000033005.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033005.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2636.034, "latencies_ms": [2636.034], "images_per_second": 0.379, "prompt_tokens": 21, "response_tokens_est": 75, "n_tiles": 6, "output_text": "The image depicts a young male tennis player on a tennis court during nighttime. He is dressed in a white shirt and black shorts, holding a tennis racket, and appears to be preparing to serve or return a shot. The court is enclosed by a green fence, and there is a sign with the text \"VOX SPORTS\" in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25588.2, "ram_available_mb": 100184.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 25588.2, "ram_available_mb": 100184.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.98, "peak": 15.32, "min": 14.32}, "VDD_GPU": {"avg": 28.58, "peak": 38.21, "min": 23.64}, "VIN": {"avg": 72.52, "peak": 117.84, "min": 60.24}, "VDD_CPU_SOC_MSS": {"avg": 15.63, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 28.58, "energy_joules_est": 75.35, "sample_count": 20, "duration_seconds": 2.636}, "timestamp": "2026-01-16T16:14:21.862778"}
{"image_index": 309, "image_name": "000000033005.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033005.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1456.591, "latencies_ms": [1456.591], "images_per_second": 0.687, "prompt_tokens": 19, "response_tokens_est": 31, "n_tiles": 6, "output_text": "The image depicts a tennis court at night, illuminated by artificial lighting. The court surface is green, and the net is made of metal.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25588.2, "ram_available_mb": 100184.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 25588.5, "ram_available_mb": 100183.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.78, "peak": 15.01, "min": 14.32}, "VDD_GPU": {"avg": 32.65, "peak": 37.8, "min": 26.39}, "VIN": {"avg": 73.05, "peak": 88.25, "min": 60.71}, "VDD_CPU_SOC_MSS": {"avg": 15.39, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 32.65, "energy_joules_est": 47.57, "sample_count": 10, "duration_seconds": 1.457}, "timestamp": "2026-01-16T16:14:23.325606"}
{"image_index": 310, "image_name": "000000033104.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033104.jpg", "image_width": 428, "image_height": 500, "image_resolution": "428x500", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2581.492, "latencies_ms": [2581.492], "images_per_second": 0.387, "prompt_tokens": 9, "response_tokens_est": 35, "n_tiles": 12, "output_text": "A group of skiers is gathered around a net, with one skier in a white helmet and another in a black helmet, both wearing protective gear and goggles.", "error": null, "sys_before": {"cpu_percent": 49.5, "ram_used_mb": 25588.5, "ram_available_mb": 100183.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 25588.4, "ram_available_mb": 100183.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.17, "peak": 16.15, "min": 14.1}, "VDD_GPU": {"avg": 34.47, "peak": 41.36, "min": 26.79}, "VIN": {"avg": 76.43, "peak": 108.13, "min": 58.48}, "VDD_CPU_SOC_MSS": {"avg": 16.04, "peak": 17.32, "min": 15.35}}, "power_watts_avg": 34.47, "energy_joules_est": 89.0, "sample_count": 20, "duration_seconds": 2.582}, "timestamp": "2026-01-16T16:14:25.980757"}
{"image_index": 310, "image_name": "000000033104.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033104.jpg", "image_width": 428, "image_height": 500, "image_resolution": "428x500", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 5845.486, "latencies_ms": [5845.486], "images_per_second": 0.171, "prompt_tokens": 23, "response_tokens_est": 129, "n_tiles": 12, "output_text": "object: helmet\n1\nobject: helmet\n1\nobject: helmet\n1\nobject: helmet\n1\nobject: helmet\n1\nobject: helmet\n1\nobject: helmet\n1\nobject: helmet\n1\nobject: helmet\n1\nobject: helmet\n1\nobject: helmet\n1\nobject: helmet\n1\nobject: helmet\n1\nobject: helmet\n1\nobject: helmet\n1\nobject: helmet\n1\nobject: helmet\n1\nobject: helmet\n1\nobject: helmet\n1\nobject: helmet\n1\nobject: helmet\n1\nobject:", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25588.4, "ram_available_mb": 100183.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25588.4, "ram_available_mb": 100183.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.13, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 30.55, "peak": 43.33, "min": 26.0}, "VIN": {"avg": 74.26, "peak": 141.36, "min": 56.13}, "VDD_CPU_SOC_MSS": {"avg": 15.72, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 30.55, "energy_joules_est": 178.6, "sample_count": 46, "duration_seconds": 5.846}, "timestamp": "2026-01-16T16:14:31.834323"}
{"image_index": 310, "image_name": "000000033104.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033104.jpg", "image_width": 428, "image_height": 500, "image_resolution": "428x500", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 4051.964, "latencies_ms": [4051.964], "images_per_second": 0.247, "prompt_tokens": 27, "response_tokens_est": 75, "n_tiles": 12, "output_text": "The main objects in the image are skiers and a net. The skiers are positioned in the foreground, with one skier wearing a helmet and goggles, and the other skier wearing a helmet and goggles. The net is situated between the skiers, with the skiers standing near it. The background features other skiers and a snowy environment.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25588.4, "ram_available_mb": 100183.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25588.7, "ram_available_mb": 100183.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.12, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 32.39, "peak": 42.92, "min": 26.0}, "VIN": {"avg": 76.25, "peak": 123.27, "min": 55.69}, "VDD_CPU_SOC_MSS": {"avg": 15.75, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 32.39, "energy_joules_est": 131.26, "sample_count": 31, "duration_seconds": 4.053}, "timestamp": "2026-01-16T16:14:35.893195"}
{"image_index": 310, "image_name": "000000033104.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033104.jpg", "image_width": 428, "image_height": 500, "image_resolution": "428x500", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 4019.416, "latencies_ms": [4019.416], "images_per_second": 0.249, "prompt_tokens": 21, "response_tokens_est": 74, "n_tiles": 12, "output_text": "The image depicts a group of skiers in a snowy environment, likely at a ski resort. They are gathered around a blue netting, possibly for safety or to mark a specific area. The skiers are dressed in winter gear, including helmets, goggles, and ski jackets, and are engaged in conversation or preparing for their skiing activities.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25588.7, "ram_available_mb": 100183.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25588.4, "ram_available_mb": 100183.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.1, "peak": 15.65, "min": 14.63}, "VDD_GPU": {"avg": 32.41, "peak": 42.94, "min": 26.0}, "VIN": {"avg": 73.92, "peak": 109.95, "min": 58.39}, "VDD_CPU_SOC_MSS": {"avg": 15.77, "peak": 16.53, "min": 15.35}}, "power_watts_avg": 32.41, "energy_joules_est": 130.28, "sample_count": 31, "duration_seconds": 4.02}, "timestamp": "2026-01-16T16:14:39.919441"}
{"image_index": 310, "image_name": "000000033104.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033104.jpg", "image_width": 428, "image_height": 500, "image_resolution": "428x500", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3314.401, "latencies_ms": [3314.401], "images_per_second": 0.302, "prompt_tokens": 19, "response_tokens_est": 53, "n_tiles": 12, "output_text": "The image depicts a group of skiers in winter gear, including helmets and goggles, standing on a snowy slope. The skiers are wearing blue and white jackets, and the scene is illuminated by natural light, suggesting it is daytime.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25588.4, "ram_available_mb": 100183.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 25588.4, "ram_available_mb": 100183.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.08, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 33.69, "peak": 42.94, "min": 26.39}, "VIN": {"avg": 72.57, "peak": 111.68, "min": 51.16}, "VDD_CPU_SOC_MSS": {"avg": 15.72, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 33.69, "energy_joules_est": 111.68, "sample_count": 26, "duration_seconds": 3.315}, "timestamp": "2026-01-16T16:14:43.240393"}
{"image_index": 311, "image_name": "000000033109.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033109.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2155.154, "latencies_ms": [2155.154], "images_per_second": 0.464, "prompt_tokens": 9, "response_tokens_est": 22, "n_tiles": 12, "output_text": "A large blue truck is driving down a wet street with other vehicles and a residential area in the background.", "error": null, "sys_before": {"cpu_percent": 42.4, "ram_used_mb": 25588.4, "ram_available_mb": 100183.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 2.5, "ram_used_mb": 25588.4, "ram_available_mb": 100183.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.13, "peak": 16.15, "min": 13.82}, "VDD_GPU": {"avg": 36.21, "peak": 41.36, "min": 29.14}, "VIN": {"avg": 82.75, "peak": 130.17, "min": 60.93}, "VDD_CPU_SOC_MSS": {"avg": 16.04, "peak": 17.32, "min": 14.96}}, "power_watts_avg": 36.21, "energy_joules_est": 78.05, "sample_count": 16, "duration_seconds": 2.155}, "timestamp": "2026-01-16T16:14:45.481348"}
{"image_index": 311, "image_name": "000000033109.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033109.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2811.391, "latencies_ms": [2811.391], "images_per_second": 0.356, "prompt_tokens": 23, "response_tokens_est": 38, "n_tiles": 12, "output_text": "1. Blue truck\n2. Blue car\n3. Bus\n4. House\n5. Street light\n6. Tree\n7. Sidewalk\n8. Green grass", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25588.4, "ram_available_mb": 100183.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 25588.4, "ram_available_mb": 100183.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.09, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 35.27, "peak": 43.31, "min": 26.79}, "VIN": {"avg": 77.5, "peak": 127.21, "min": 58.68}, "VDD_CPU_SOC_MSS": {"avg": 15.66, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 35.27, "energy_joules_est": 99.17, "sample_count": 22, "duration_seconds": 2.812}, "timestamp": "2026-01-16T16:14:48.299845"}
{"image_index": 311, "image_name": "000000033109.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033109.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 4159.038, "latencies_ms": [4159.038], "images_per_second": 0.24, "prompt_tokens": 27, "response_tokens_est": 78, "n_tiles": 12, "output_text": "The blue truck is positioned in the foreground, slightly to the right, and is the main focus of the image. The road is wet, indicating recent rain, and the truck is traveling on it. The background features residential buildings, a street lamp, and a tree, providing context to the scene. The truck is near the sidewalk, which is on the right side of the image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25588.4, "ram_available_mb": 100183.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25588.4, "ram_available_mb": 100183.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.11, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 32.33, "peak": 42.94, "min": 26.0}, "VIN": {"avg": 74.75, "peak": 120.34, "min": 58.17}, "VDD_CPU_SOC_MSS": {"avg": 15.68, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 32.33, "energy_joules_est": 134.48, "sample_count": 32, "duration_seconds": 4.16}, "timestamp": "2026-01-16T16:14:52.465470"}
{"image_index": 311, "image_name": "000000033109.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033109.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3549.767, "latencies_ms": [3549.767], "images_per_second": 0.282, "prompt_tokens": 21, "response_tokens_est": 60, "n_tiles": 12, "output_text": "The image depicts a wet street scene with a blue truck driving on the road. The truck is followed by a bus and several cars, all navigating the wet road. The setting appears to be a residential area with houses and trees in the background, and the weather seems to be overcast.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25588.4, "ram_available_mb": 100183.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25588.4, "ram_available_mb": 100183.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.09, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 33.19, "peak": 42.94, "min": 26.4}, "VIN": {"avg": 73.07, "peak": 113.13, "min": 58.84}, "VDD_CPU_SOC_MSS": {"avg": 15.74, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 33.19, "energy_joules_est": 117.83, "sample_count": 28, "duration_seconds": 3.55}, "timestamp": "2026-01-16T16:14:56.021834"}
{"image_index": 311, "image_name": "000000033109.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033109.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3683.707, "latencies_ms": [3683.707], "images_per_second": 0.271, "prompt_tokens": 19, "response_tokens_est": 64, "n_tiles": 12, "output_text": "The image features a blue truck with the word \"HUISMANGROUP.COM\" on its side, driving on a wet road. The truck is surrounded by a residential area with houses, trees, and a street lamp. The lighting is soft and diffused, suggesting it might be early morning or late afternoon.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25588.4, "ram_available_mb": 100183.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25588.4, "ram_available_mb": 100183.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.08, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 32.79, "peak": 42.54, "min": 26.39}, "VIN": {"avg": 72.81, "peak": 113.82, "min": 58.61}, "VDD_CPU_SOC_MSS": {"avg": 15.77, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 32.79, "energy_joules_est": 120.8, "sample_count": 29, "duration_seconds": 3.684}, "timestamp": "2026-01-16T16:14:59.712871"}
{"image_index": 312, "image_name": "000000033114.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033114.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2420.379, "latencies_ms": [2420.379], "images_per_second": 0.413, "prompt_tokens": 9, "response_tokens_est": 30, "n_tiles": 12, "output_text": "The image shows a large airplane on the tarmac, with its landing gear extended, and several red and white traffic lights in the foreground.", "error": null, "sys_before": {"cpu_percent": 46.0, "ram_used_mb": 25588.4, "ram_available_mb": 100183.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 25588.6, "ram_available_mb": 100183.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.84, "peak": 15.75, "min": 13.79}, "VDD_GPU": {"avg": 35.06, "peak": 41.76, "min": 27.18}, "VIN": {"avg": 74.46, "peak": 106.5, "min": 59.43}, "VDD_CPU_SOC_MSS": {"avg": 15.5, "peak": 16.92, "min": 14.57}}, "power_watts_avg": 35.06, "energy_joules_est": 84.87, "sample_count": 19, "duration_seconds": 2.421}, "timestamp": "2026-01-16T16:15:02.224020"}
{"image_index": 312, "image_name": "000000033114.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033114.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2884.32, "latencies_ms": [2884.32], "images_per_second": 0.347, "prompt_tokens": 23, "response_tokens_est": 40, "n_tiles": 12, "output_text": "object: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25588.6, "ram_available_mb": 100183.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 25588.4, "ram_available_mb": 100183.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.75, "peak": 15.34, "min": 14.12}, "VDD_GPU": {"avg": 35.07, "peak": 42.92, "min": 26.39}, "VIN": {"avg": 74.0, "peak": 105.9, "min": 58.57}, "VDD_CPU_SOC_MSS": {"avg": 15.46, "peak": 16.14, "min": 14.56}}, "power_watts_avg": 35.07, "energy_joules_est": 101.17, "sample_count": 22, "duration_seconds": 2.885}, "timestamp": "2026-01-16T16:15:05.114862"}
{"image_index": 312, "image_name": "000000033114.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033114.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 4131.366, "latencies_ms": [4131.366], "images_per_second": 0.242, "prompt_tokens": 27, "response_tokens_est": 76, "n_tiles": 12, "output_text": "The main objects in the image are a row of traffic lights positioned in the foreground, with a few more in the background. The traffic lights are situated near the edge of a road, with a metal fence running parallel to the road. The background features a hazy landscape with mountains and a body of water, while the foreground shows a mix of grass and a concrete surface.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25588.4, "ram_available_mb": 100183.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25596.5, "ram_available_mb": 100175.7, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.74, "peak": 15.34, "min": 14.12}, "VDD_GPU": {"avg": 31.96, "peak": 42.53, "min": 26.0}, "VIN": {"avg": 70.54, "peak": 111.84, "min": 55.88}, "VDD_CPU_SOC_MSS": {"avg": 15.22, "peak": 16.14, "min": 14.96}}, "power_watts_avg": 31.96, "energy_joules_est": 132.06, "sample_count": 33, "duration_seconds": 4.132}, "timestamp": "2026-01-16T16:15:09.253379"}
{"image_index": 312, "image_name": "000000033114.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033114.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 4194.378, "latencies_ms": [4194.378], "images_per_second": 0.238, "prompt_tokens": 21, "response_tokens_est": 78, "n_tiles": 12, "output_text": "The image depicts a scene at an airport with a large airplane on the tarmac, preparing to take off or having just landed. The background shows a hazy, overcast sky and a distant mountain range, suggesting a location that is likely in a region with a cooler climate. The foreground features several red and white traffic lights, indicating a controlled area for vehicles and pedestrians.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25596.5, "ram_available_mb": 100175.7, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25596.3, "ram_available_mb": 100175.9, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.75, "peak": 15.34, "min": 14.12}, "VDD_GPU": {"avg": 31.74, "peak": 42.53, "min": 25.6}, "VIN": {"avg": 72.78, "peak": 125.75, "min": 54.71}, "VDD_CPU_SOC_MSS": {"avg": 15.32, "peak": 16.14, "min": 14.56}}, "power_watts_avg": 31.74, "energy_joules_est": 133.14, "sample_count": 33, "duration_seconds": 4.195}, "timestamp": "2026-01-16T16:15:13.454668"}
{"image_index": 312, "image_name": "000000033114.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033114.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3883.188, "latencies_ms": [3883.188], "images_per_second": 0.258, "prompt_tokens": 19, "response_tokens_est": 69, "n_tiles": 12, "output_text": "The image features a scene with a wet runway, where a large airplane is taxiing. The runway is surrounded by a fence with red and white lights, and there are several red traffic lights in the foreground. The lighting is soft and diffused, likely due to the overcast sky, and the overall atmosphere is calm and serene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25596.3, "ram_available_mb": 100175.9, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 25596.5, "ram_available_mb": 100175.7, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.74, "peak": 15.34, "min": 14.12}, "VDD_GPU": {"avg": 32.52, "peak": 42.54, "min": 26.0}, "VIN": {"avg": 72.52, "peak": 113.88, "min": 45.15}, "VDD_CPU_SOC_MSS": {"avg": 15.34, "peak": 16.14, "min": 14.56}}, "power_watts_avg": 32.52, "energy_joules_est": 126.3, "sample_count": 30, "duration_seconds": 3.884}, "timestamp": "2026-01-16T16:15:17.344607"}
{"image_index": 313, "image_name": "000000033221.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033221.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1208.493, "latencies_ms": [1208.493], "images_per_second": 0.827, "prompt_tokens": 9, "response_tokens_est": 23, "n_tiles": 6, "output_text": "A group of people is walking on a sandy beach, with one person carrying a baby in a carrier.", "error": null, "sys_before": {"cpu_percent": 42.9, "ram_used_mb": 25596.2, "ram_available_mb": 100175.9, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25596.2, "ram_available_mb": 100175.9, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.26, "peak": 14.81, "min": 13.82}, "VDD_GPU": {"avg": 32.91, "peak": 37.42, "min": 27.18}, "VIN": {"avg": 77.25, "peak": 115.11, "min": 60.41}, "VDD_CPU_SOC_MSS": {"avg": 14.92, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 32.91, "energy_joules_est": 39.78, "sample_count": 9, "duration_seconds": 1.209}, "timestamp": "2026-01-16T16:15:18.608981"}
{"image_index": 313, "image_name": "000000033221.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033221.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2130.183, "latencies_ms": [2130.183], "images_per_second": 0.469, "prompt_tokens": 23, "response_tokens_est": 56, "n_tiles": 6, "output_text": "1. Woman: 2\n2. Woman: 2\n3. Woman: 2\n4. Woman: 2\n5. Woman: 2\n6. Woman: 2\n7. Woman: 2\n8. Woman: 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25596.2, "ram_available_mb": 100175.9, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 25596.2, "ram_available_mb": 100175.9, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.86, "peak": 15.22, "min": 14.22}, "VDD_GPU": {"avg": 30.33, "peak": 38.98, "min": 24.03}, "VIN": {"avg": 66.7, "peak": 79.94, "min": 57.76}, "VDD_CPU_SOC_MSS": {"avg": 15.45, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 30.33, "energy_joules_est": 64.62, "sample_count": 16, "duration_seconds": 2.131}, "timestamp": "2026-01-16T16:15:20.745031"}
{"image_index": 313, "image_name": "000000033221.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033221.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2556.322, "latencies_ms": [2556.322], "images_per_second": 0.391, "prompt_tokens": 27, "response_tokens_est": 72, "n_tiles": 6, "output_text": "The main objects in the image are a group of people walking on a sandy beach. The person in the foreground is holding a blue towel and wearing an orange shirt. The person in the background is holding a baseball bat and wearing a black jacket. The group is walking towards the right side of the image, with the beach and mountains in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25596.2, "ram_available_mb": 100175.9, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 25596.7, "ram_available_mb": 100175.4, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.98, "peak": 15.32, "min": 14.32}, "VDD_GPU": {"avg": 28.64, "peak": 38.19, "min": 23.64}, "VIN": {"avg": 68.82, "peak": 109.31, "min": 57.58}, "VDD_CPU_SOC_MSS": {"avg": 15.59, "peak": 16.14, "min": 14.96}}, "power_watts_avg": 28.64, "energy_joules_est": 73.22, "sample_count": 20, "duration_seconds": 2.557}, "timestamp": "2026-01-16T16:15:23.307367"}
{"image_index": 313, "image_name": "000000033221.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033221.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2203.66, "latencies_ms": [2203.66], "images_per_second": 0.454, "prompt_tokens": 21, "response_tokens_est": 59, "n_tiles": 6, "output_text": "The scene is set on a sandy beach with a group of people walking along the shore. The individuals are dressed in casual beach attire, and some are carrying bags. The setting appears to be a sunny day with clear skies, and there are some trees and mountains in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25596.7, "ram_available_mb": 100175.4, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 25596.5, "ram_available_mb": 100175.7, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.94, "peak": 15.32, "min": 14.32}, "VDD_GPU": {"avg": 29.61, "peak": 38.21, "min": 24.03}, "VIN": {"avg": 67.87, "peak": 104.37, "min": 56.32}, "VDD_CPU_SOC_MSS": {"avg": 15.49, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 29.61, "energy_joules_est": 65.26, "sample_count": 17, "duration_seconds": 2.204}, "timestamp": "2026-01-16T16:15:25.517427"}
{"image_index": 313, "image_name": "000000033221.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033221.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1752.391, "latencies_ms": [1752.391], "images_per_second": 0.571, "prompt_tokens": 19, "response_tokens_est": 42, "n_tiles": 6, "output_text": "The image depicts a sunny beach scene with clear blue skies and a few scattered clouds. The sandy beach is dotted with orange traffic cones, and a few people are walking along the shoreline.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25596.5, "ram_available_mb": 100175.7, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25596.5, "ram_available_mb": 100175.7, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.86, "peak": 15.22, "min": 14.32}, "VDD_GPU": {"avg": 31.21, "peak": 37.82, "min": 24.82}, "VIN": {"avg": 69.63, "peak": 91.27, "min": 58.43}, "VDD_CPU_SOC_MSS": {"avg": 15.5, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 31.21, "energy_joules_est": 54.71, "sample_count": 13, "duration_seconds": 1.753}, "timestamp": "2026-01-16T16:15:27.276433"}
{"image_index": 314, "image_name": "000000033368.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033368.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1314.886, "latencies_ms": [1314.886], "images_per_second": 0.761, "prompt_tokens": 9, "response_tokens_est": 27, "n_tiles": 6, "output_text": "A tennis player is standing on a blue court, holding a tennis racket, and appears to be contemplating his next move.", "error": null, "sys_before": {"cpu_percent": 45.2, "ram_used_mb": 25596.5, "ram_available_mb": 100175.7, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 25596.5, "ram_available_mb": 100175.7, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.46, "peak": 14.91, "min": 13.92}, "VDD_GPU": {"avg": 32.06, "peak": 37.41, "min": 26.0}, "VIN": {"avg": 77.03, "peak": 127.1, "min": 53.45}, "VDD_CPU_SOC_MSS": {"avg": 15.2, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 32.06, "energy_joules_est": 42.19, "sample_count": 10, "duration_seconds": 1.316}, "timestamp": "2026-01-16T16:15:28.640082"}
{"image_index": 314, "image_name": "000000033368.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033368.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1725.266, "latencies_ms": [1725.266], "images_per_second": 0.58, "prompt_tokens": 23, "response_tokens_est": 41, "n_tiles": 6, "output_text": "1. Tennis player\n2. Tennis racket\n3. Tennis ball\n4. Tennis court\n5. Blue surface\n6. White shorts\n7. White socks\n8. White shoes", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25596.5, "ram_available_mb": 100175.7, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25596.7, "ram_available_mb": 100175.5, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.8, "peak": 15.11, "min": 14.22}, "VDD_GPU": {"avg": 31.6, "peak": 38.6, "min": 24.82}, "VIN": {"avg": 77.09, "peak": 114.26, "min": 64.29}, "VDD_CPU_SOC_MSS": {"avg": 15.35, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 31.6, "energy_joules_est": 54.53, "sample_count": 13, "duration_seconds": 1.726}, "timestamp": "2026-01-16T16:15:30.371874"}
{"image_index": 314, "image_name": "000000033368.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033368.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2609.281, "latencies_ms": [2609.281], "images_per_second": 0.383, "prompt_tokens": 27, "response_tokens_est": 74, "n_tiles": 6, "output_text": "The main object in the image is a tennis player standing on a blue tennis court. The player is positioned in the foreground, slightly to the left, and is holding a tennis racket in his right hand. The court is marked with white lines and features a logo near the bottom right corner. The background is mostly dark, emphasizing the player and the court.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25596.7, "ram_available_mb": 100175.5, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 25596.5, "ram_available_mb": 100175.7, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.0, "peak": 15.32, "min": 14.32}, "VDD_GPU": {"avg": 28.79, "peak": 38.19, "min": 23.64}, "VIN": {"avg": 74.71, "peak": 123.85, "min": 61.72}, "VDD_CPU_SOC_MSS": {"avg": 15.55, "peak": 16.14, "min": 14.96}}, "power_watts_avg": 28.79, "energy_joules_est": 75.13, "sample_count": 20, "duration_seconds": 2.61}, "timestamp": "2026-01-16T16:15:32.987291"}
{"image_index": 314, "image_name": "000000033368.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033368.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3083.861, "latencies_ms": [3083.861], "images_per_second": 0.324, "prompt_tokens": 21, "response_tokens_est": 92, "n_tiles": 6, "output_text": "The image depicts a tennis player on a blue court, likely during a match or practice session. The player is dressed in a light blue shirt, white shorts, and white socks, holding a tennis racket in his right hand. The court is marked with the word \"TENNIS\" in white letters, and the player appears to be in a moment of rest or contemplation, possibly after a point or during a break in the game.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25596.5, "ram_available_mb": 100175.7, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 25596.5, "ram_available_mb": 100175.7, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.06, "peak": 15.32, "min": 14.32}, "VDD_GPU": {"avg": 27.84, "peak": 38.19, "min": 23.64}, "VIN": {"avg": 68.93, "peak": 106.09, "min": 57.94}, "VDD_CPU_SOC_MSS": {"avg": 15.57, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 27.84, "energy_joules_est": 85.87, "sample_count": 24, "duration_seconds": 3.084}, "timestamp": "2026-01-16T16:15:36.081797"}
{"image_index": 314, "image_name": "000000033368.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033368.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2305.827, "latencies_ms": [2305.827], "images_per_second": 0.434, "prompt_tokens": 19, "response_tokens_est": 63, "n_tiles": 6, "output_text": "The image features a tennis player on a blue court, illuminated by bright sunlight. The player is wearing a light blue shirt, white shorts, and white socks, with a tennis racket in hand. The bright lighting casts a shadow of the player on the court, highlighting the texture of the blue surface.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25596.5, "ram_available_mb": 100175.7, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 25596.2, "ram_available_mb": 100176.0, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.94, "peak": 15.32, "min": 14.32}, "VDD_GPU": {"avg": 29.45, "peak": 38.21, "min": 24.03}, "VIN": {"avg": 76.56, "peak": 127.79, "min": 56.99}, "VDD_CPU_SOC_MSS": {"avg": 15.47, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 29.45, "energy_joules_est": 67.92, "sample_count": 17, "duration_seconds": 2.306}, "timestamp": "2026-01-16T16:15:38.398193"}
{"image_index": 315, "image_name": "000000033638.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033638.jpg", "image_width": 425, "image_height": 640, "image_resolution": "425x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1067.435, "latencies_ms": [1067.435], "images_per_second": 0.937, "prompt_tokens": 9, "response_tokens_est": 18, "n_tiles": 6, "output_text": "A woman is standing by a black stove, looking at a bowl on the counter.", "error": null, "sys_before": {"cpu_percent": 36.4, "ram_used_mb": 25596.2, "ram_available_mb": 100176.0, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 25596.2, "ram_available_mb": 100176.0, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.39, "peak": 14.71, "min": 13.92}, "VDD_GPU": {"avg": 33.14, "peak": 37.03, "min": 28.36}, "VIN": {"avg": 69.91, "peak": 115.49, "min": 56.39}, "VDD_CPU_SOC_MSS": {"avg": 15.2, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 33.14, "energy_joules_est": 35.39, "sample_count": 8, "duration_seconds": 1.068}, "timestamp": "2026-01-16T16:15:39.510660"}
{"image_index": 315, "image_name": "000000033638.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033638.jpg", "image_width": 425, "image_height": 640, "image_resolution": "425x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1969.402, "latencies_ms": [1969.402], "images_per_second": 0.508, "prompt_tokens": 23, "response_tokens_est": 50, "n_tiles": 6, "output_text": "1. Kitchen counter\n2. Kitchen sink\n3. Kitchen cabinet\n4. Kitchen utensils\n5. Kitchen utensils\n6. Kitchen utensils\n7. Kitchen utensils\n8. Kitchen utensils", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25596.2, "ram_available_mb": 100176.0, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 25596.2, "ram_available_mb": 100176.0, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.81, "peak": 15.22, "min": 14.22}, "VDD_GPU": {"avg": 31.4, "peak": 38.98, "min": 24.42}, "VIN": {"avg": 65.47, "peak": 75.89, "min": 55.9}, "VDD_CPU_SOC_MSS": {"avg": 15.33, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 31.4, "energy_joules_est": 61.86, "sample_count": 14, "duration_seconds": 1.97}, "timestamp": "2026-01-16T16:15:41.487024"}
{"image_index": 315, "image_name": "000000033638.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033638.jpg", "image_width": 425, "image_height": 640, "image_resolution": "425x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3313.477, "latencies_ms": [3313.477], "images_per_second": 0.302, "prompt_tokens": 27, "response_tokens_est": 100, "n_tiles": 6, "output_text": "The main object in the foreground is a black, vintage-style stove with a metal base and a brick surround. To the left of the stove, there is a woman wearing a floral top and khaki shorts, standing barefoot. In the background, there is a white cabinet, a window with a white curtain, and various kitchen items such as plates, a blue can, and a metal pot. The stove is positioned near the woman, and the cabinet is to the left of the window.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25596.2, "ram_available_mb": 100176.0, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 25596.0, "ram_available_mb": 100176.2, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.05, "peak": 15.32, "min": 14.22}, "VDD_GPU": {"avg": 27.64, "peak": 38.21, "min": 23.64}, "VIN": {"avg": 70.12, "peak": 110.93, "min": 59.63}, "VDD_CPU_SOC_MSS": {"avg": 15.59, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 27.64, "energy_joules_est": 91.6, "sample_count": 25, "duration_seconds": 3.314}, "timestamp": "2026-01-16T16:15:44.807737"}
{"image_index": 315, "image_name": "000000033638.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033638.jpg", "image_width": 425, "image_height": 640, "image_resolution": "425x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2040.82, "latencies_ms": [2040.82], "images_per_second": 0.49, "prompt_tokens": 21, "response_tokens_est": 53, "n_tiles": 6, "output_text": "The image depicts a kitchen scene with a woman standing by a black stove, seemingly engaged in cooking or preparing food. The kitchen is cluttered with various kitchen items, including plates, bowls, and utensils, indicating a busy and lived-in space.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 25596.0, "ram_available_mb": 100176.2, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 25595.9, "ram_available_mb": 100176.2, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.91, "peak": 15.22, "min": 14.32}, "VDD_GPU": {"avg": 30.23, "peak": 37.82, "min": 24.03}, "VIN": {"avg": 66.46, "peak": 94.02, "min": 56.82}, "VDD_CPU_SOC_MSS": {"avg": 15.46, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 30.23, "energy_joules_est": 61.7, "sample_count": 15, "duration_seconds": 2.041}, "timestamp": "2026-01-16T16:15:46.855298"}
{"image_index": 315, "image_name": "000000033638.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033638.jpg", "image_width": 425, "image_height": 640, "image_resolution": "425x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2364.786, "latencies_ms": [2364.786], "images_per_second": 0.423, "prompt_tokens": 19, "response_tokens_est": 65, "n_tiles": 6, "output_text": "The image depicts a kitchen scene with a woman standing by a black stove, wearing a floral top and khaki shorts. The lighting is dim, with a single hanging light fixture providing illumination. The kitchen has a rustic, vintage feel, with wooden cabinets, a brick stove, and various kitchen items scattered around.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25595.9, "ram_available_mb": 100176.2, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 25595.7, "ram_available_mb": 100176.5, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.95, "peak": 15.22, "min": 14.32}, "VDD_GPU": {"avg": 29.32, "peak": 38.21, "min": 23.64}, "VIN": {"avg": 75.66, "peak": 118.92, "min": 62.48}, "VDD_CPU_SOC_MSS": {"avg": 15.53, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 29.32, "energy_joules_est": 69.35, "sample_count": 18, "duration_seconds": 2.365}, "timestamp": "2026-01-16T16:15:49.226560"}
{"image_index": 316, "image_name": "000000033707.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033707.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1675.329, "latencies_ms": [1675.329], "images_per_second": 0.597, "prompt_tokens": 9, "response_tokens_est": 74, "n_tiles": 1, "output_text": "The image depicts a serene scene of two giraffes standing close to each other, with one giraffe slightly taller than the other, both displaying their characteristic long necks and spotted coats. They are positioned in front of a beige building with a wooden door, surrounded by a rocky landscape and lush greenery, creating a tranquil and natural environment.", "error": null, "sys_before": {"cpu_percent": 9.5, "ram_used_mb": 25595.7, "ram_available_mb": 100176.5, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25595.7, "ram_available_mb": 100176.5, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 4845.7, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.46, "peak": 15.72, "min": 14.81}, "VDD_GPU": {"avg": 21.64, "peak": 26.01, "min": 20.1}, "VIN": {"avg": 63.37, "peak": 72.08, "min": 55.71}, "VDD_CPU_SOC_MSS": {"avg": 16.26, "peak": 16.54, "min": 15.36}}, "power_watts_avg": 21.64, "energy_joules_est": 36.26, "sample_count": 13, "duration_seconds": 1.676}, "timestamp": "2026-01-16T16:15:50.923496"}
{"image_index": 316, "image_name": "000000033707.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033707.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1013.712, "latencies_ms": [1013.712], "images_per_second": 0.986, "prompt_tokens": 23, "response_tokens_est": 44, "n_tiles": 1, "output_text": "- giraffe: 2\n- building: 1\n- tree: 1\n- fence: 1\n- rocks: 1\n- dirt path: 1\n- grass: 1", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25595.7, "ram_available_mb": 100176.5, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25595.9, "ram_available_mb": 100176.2, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 4856.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.53, "peak": 15.72, "min": 15.11}, "VDD_GPU": {"avg": 21.95, "peak": 24.04, "min": 20.48}, "VIN": {"avg": 64.0, "peak": 67.15, "min": 60.52}, "VDD_CPU_SOC_MSS": {"avg": 16.37, "peak": 16.54, "min": 16.14}}, "power_watts_avg": 21.95, "energy_joules_est": 22.26, "sample_count": 7, "duration_seconds": 1.014}, "timestamp": "2026-01-16T16:15:51.943412"}
{"image_index": 316, "image_name": "000000033707.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033707.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1315.4, "latencies_ms": [1315.4], "images_per_second": 0.76, "prompt_tokens": 27, "response_tokens_est": 58, "n_tiles": 1, "output_text": "The main objects in the image are two giraffes standing in front of a building. The giraffe on the left is closer to the camera, while the one on the right is slightly behind it. The building is in the background, and the rocky ground is in the foreground.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.9, "ram_available_mb": 100176.2, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25595.9, "ram_available_mb": 100176.2, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 4859.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.62, "peak": 15.72, "min": 15.32}, "VDD_GPU": {"avg": 21.6, "peak": 24.43, "min": 20.1}, "VIN": {"avg": 63.03, "peak": 67.47, "min": 60.54}, "VDD_CPU_SOC_MSS": {"avg": 16.42, "peak": 16.54, "min": 16.14}}, "power_watts_avg": 21.6, "energy_joules_est": 28.42, "sample_count": 10, "duration_seconds": 1.316}, "timestamp": "2026-01-16T16:15:53.264824"}
{"image_index": 316, "image_name": "000000033707.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033707.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1420.96, "latencies_ms": [1420.96], "images_per_second": 0.704, "prompt_tokens": 21, "response_tokens_est": 63, "n_tiles": 1, "output_text": "The image depicts a serene scene at a zoo, featuring two giraffes standing close to each other in front of a beige building. The giraffes are surrounded by a rocky terrain, and the background includes lush green trees and a cloudy sky, creating a tranquil and natural environment.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.9, "ram_available_mb": 100176.2, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25595.7, "ram_available_mb": 100176.5, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 4854.7, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.58, "peak": 15.72, "min": 15.22}, "VDD_GPU": {"avg": 21.42, "peak": 24.43, "min": 20.1}, "VIN": {"avg": 62.96, "peak": 67.05, "min": 60.23}, "VDD_CPU_SOC_MSS": {"avg": 16.43, "peak": 16.54, "min": 16.14}}, "power_watts_avg": 21.42, "energy_joules_est": 30.45, "sample_count": 11, "duration_seconds": 1.421}, "timestamp": "2026-01-16T16:15:54.691822"}
{"image_index": 316, "image_name": "000000033707.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033707.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1352.853, "latencies_ms": [1352.853], "images_per_second": 0.739, "prompt_tokens": 19, "response_tokens_est": 60, "n_tiles": 1, "output_text": "The image depicts a giraffe standing next to a building with a beige facade and a stone pathway. The giraffe is characterized by its long neck, brown and tan coat, and the building is made of concrete. The lighting is soft and diffused, suggesting an overcast day.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25595.7, "ram_available_mb": 100176.5, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25595.9, "ram_available_mb": 100176.2, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 4853.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.61, "peak": 15.72, "min": 15.32}, "VDD_GPU": {"avg": 21.43, "peak": 24.03, "min": 20.09}, "VIN": {"avg": 62.34, "peak": 64.58, "min": 59.91}, "VDD_CPU_SOC_MSS": {"avg": 16.46, "peak": 16.54, "min": 16.14}}, "power_watts_avg": 21.43, "energy_joules_est": 29.0, "sample_count": 10, "duration_seconds": 1.353}, "timestamp": "2026-01-16T16:15:56.050768"}
{"image_index": 317, "image_name": "000000033759.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033759.jpg", "image_width": 640, "image_height": 457, "image_resolution": "640x457", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2580.877, "latencies_ms": [2580.877], "images_per_second": 0.387, "prompt_tokens": 9, "response_tokens_est": 35, "n_tiles": 12, "output_text": "A young baseball player is in the middle of a game, swinging a bat at a flying ball, while wearing a green jersey and white pants, and a black helmet.", "error": null, "sys_before": {"cpu_percent": 45.1, "ram_used_mb": 25595.9, "ram_available_mb": 100176.2, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 25595.9, "ram_available_mb": 100176.2, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.21, "peak": 16.15, "min": 14.3}, "VDD_GPU": {"avg": 33.98, "peak": 40.18, "min": 27.18}, "VIN": {"avg": 78.22, "peak": 121.09, "min": 58.84}, "VDD_CPU_SOC_MSS": {"avg": 16.08, "peak": 17.32, "min": 15.35}}, "power_watts_avg": 33.98, "energy_joules_est": 87.71, "sample_count": 20, "duration_seconds": 2.581}, "timestamp": "2026-01-16T16:15:58.711056"}
{"image_index": 317, "image_name": "000000033759.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033759.jpg", "image_width": 640, "image_height": 457, "image_resolution": "640x457", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2973.71, "latencies_ms": [2973.71], "images_per_second": 0.336, "prompt_tokens": 23, "response_tokens_est": 42, "n_tiles": 12, "output_text": "baseball bat: 1\nbaseball: 1\nhelmet: 1\nuniform: 1\nsocks: 2\nglove: 1\nfootwear: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.9, "ram_available_mb": 100176.2, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 25595.9, "ram_available_mb": 100176.3, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.89, "peak": 15.54, "min": 14.32}, "VDD_GPU": {"avg": 34.41, "peak": 42.54, "min": 26.39}, "VIN": {"avg": 75.93, "peak": 119.67, "min": 56.81}, "VDD_CPU_SOC_MSS": {"avg": 15.47, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 34.41, "energy_joules_est": 102.35, "sample_count": 23, "duration_seconds": 2.975}, "timestamp": "2026-01-16T16:16:01.693018"}
{"image_index": 317, "image_name": "000000033759.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033759.jpg", "image_width": 640, "image_height": 457, "image_resolution": "640x457", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 4365.181, "latencies_ms": [4365.181], "images_per_second": 0.229, "prompt_tokens": 27, "response_tokens_est": 83, "n_tiles": 12, "output_text": "The main object in the foreground is a baseball player wearing a green jersey and white pants, holding a blue bat. The player is positioned near the center of the image, slightly to the left. In the background, there is a chain-link fence, which is positioned behind the player. The fence is tall and extends across the entire width of the image, creating a clear boundary between the foreground and the background.", "error": null, "sys_before": {"cpu_percent": 16.7, "ram_used_mb": 25595.9, "ram_available_mb": 100176.3, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 25595.9, "ram_available_mb": 100176.3, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.75, "peak": 15.34, "min": 14.12}, "VDD_GPU": {"avg": 31.71, "peak": 42.94, "min": 25.6}, "VIN": {"avg": 69.56, "peak": 116.43, "min": 56.76}, "VDD_CPU_SOC_MSS": {"avg": 15.18, "peak": 16.14, "min": 14.56}}, "power_watts_avg": 31.71, "energy_joules_est": 138.43, "sample_count": 34, "duration_seconds": 4.366}, "timestamp": "2026-01-16T16:16:06.064950"}
{"image_index": 317, "image_name": "000000033759.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033759.jpg", "image_width": 640, "image_height": 457, "image_resolution": "640x457", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3606.585, "latencies_ms": [3606.585], "images_per_second": 0.277, "prompt_tokens": 21, "response_tokens_est": 61, "n_tiles": 12, "output_text": "The image depicts a young baseball player in a green jersey and white pants, equipped with a blue helmet and a baseball bat, positioned on a dirt field with a chain-link fence in the background. The player appears to be in the middle of a batting stance, ready to hit the incoming baseball.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.9, "ram_available_mb": 100176.3, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25595.9, "ram_available_mb": 100176.3, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.75, "peak": 15.34, "min": 14.12}, "VDD_GPU": {"avg": 32.89, "peak": 42.54, "min": 26.0}, "VIN": {"avg": 72.49, "peak": 103.2, "min": 55.21}, "VDD_CPU_SOC_MSS": {"avg": 15.24, "peak": 16.14, "min": 14.56}}, "power_watts_avg": 32.89, "energy_joules_est": 118.63, "sample_count": 28, "duration_seconds": 3.607}, "timestamp": "2026-01-16T16:16:09.678189"}
{"image_index": 317, "image_name": "000000033759.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033759.jpg", "image_width": 640, "image_height": 457, "image_resolution": "640x457", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2746.848, "latencies_ms": [2746.848], "images_per_second": 0.364, "prompt_tokens": 19, "response_tokens_est": 36, "n_tiles": 12, "output_text": "The baseball player is wearing a green jersey with yellow accents, white pants, and green socks. The scene is brightly lit by natural sunlight, casting shadows on the ground.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.9, "ram_available_mb": 100176.3, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 25595.9, "ram_available_mb": 100176.3, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.73, "peak": 15.34, "min": 14.12}, "VDD_GPU": {"avg": 35.0, "peak": 42.54, "min": 26.79}, "VIN": {"avg": 76.07, "peak": 120.02, "min": 58.53}, "VDD_CPU_SOC_MSS": {"avg": 15.32, "peak": 16.14, "min": 14.56}}, "power_watts_avg": 35.0, "energy_joules_est": 96.17, "sample_count": 22, "duration_seconds": 2.748}, "timestamp": "2026-01-16T16:16:12.434375"}
{"image_index": 318, "image_name": "000000033854.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033854.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1448.444, "latencies_ms": [1448.444], "images_per_second": 0.69, "prompt_tokens": 9, "response_tokens_est": 32, "n_tiles": 6, "output_text": "A vintage car and motorcycle are parked on a cobblestone street, with a crowd of people gathered around them, and a red bus in the background.", "error": null, "sys_before": {"cpu_percent": 36.5, "ram_used_mb": 25595.9, "ram_available_mb": 100176.3, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25595.9, "ram_available_mb": 100176.3, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.3, "peak": 15.01, "min": 13.72}, "VDD_GPU": {"avg": 31.76, "peak": 37.42, "min": 25.6}, "VIN": {"avg": 71.18, "peak": 124.42, "min": 58.46}, "VDD_CPU_SOC_MSS": {"avg": 15.03, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 31.76, "energy_joules_est": 46.02, "sample_count": 11, "duration_seconds": 1.449}, "timestamp": "2026-01-16T16:16:13.949935"}
{"image_index": 318, "image_name": "000000033854.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033854.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1485.866, "latencies_ms": [1485.866], "images_per_second": 0.673, "prompt_tokens": 23, "response_tokens_est": 32, "n_tiles": 6, "output_text": "car: 2\nmotorcycle: 3\nbus: 1\nstreet light: 1\nbuilding: 1\nflag: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.9, "ram_available_mb": 100176.3, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 25595.9, "ram_available_mb": 100176.3, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.72, "peak": 15.11, "min": 14.22}, "VDD_GPU": {"avg": 32.69, "peak": 38.21, "min": 26.0}, "VIN": {"avg": 75.85, "peak": 104.98, "min": 62.15}, "VDD_CPU_SOC_MSS": {"avg": 15.39, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 32.69, "energy_joules_est": 48.59, "sample_count": 11, "duration_seconds": 1.486}, "timestamp": "2026-01-16T16:16:15.444730"}
{"image_index": 318, "image_name": "000000033854.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033854.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2661.785, "latencies_ms": [2661.785], "images_per_second": 0.376, "prompt_tokens": 27, "response_tokens_est": 76, "n_tiles": 6, "output_text": "The main objects in the image are parked cars and motorcycles. The closest car is parked on the left side, while the motorcycle is parked on the right side. The background features a crowd of people, a red bus, and a building under construction. The scene is set on a cobblestone street, and the overall atmosphere appears to be a public event or gathering.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.9, "ram_available_mb": 100176.3, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 25595.9, "ram_available_mb": 100176.3, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.0, "peak": 15.32, "min": 14.22}, "VDD_GPU": {"avg": 28.62, "peak": 38.59, "min": 23.64}, "VIN": {"avg": 68.08, "peak": 112.67, "min": 53.61}, "VDD_CPU_SOC_MSS": {"avg": 15.54, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 28.62, "energy_joules_est": 76.19, "sample_count": 21, "duration_seconds": 2.662}, "timestamp": "2026-01-16T16:16:18.112645"}
{"image_index": 318, "image_name": "000000033854.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033854.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1987.932, "latencies_ms": [1987.932], "images_per_second": 0.503, "prompt_tokens": 21, "response_tokens_est": 51, "n_tiles": 6, "output_text": "The image depicts a bustling outdoor scene with vintage cars parked on a cobblestone street. The setting appears to be a public area, possibly a fair or a historical exhibition, with various people milling about and a red bus in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.9, "ram_available_mb": 100176.3, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 25595.6, "ram_available_mb": 100176.5, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.9, "peak": 15.22, "min": 14.32}, "VDD_GPU": {"avg": 30.25, "peak": 37.82, "min": 24.42}, "VIN": {"avg": 65.28, "peak": 82.05, "min": 57.21}, "VDD_CPU_SOC_MSS": {"avg": 15.49, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 30.25, "energy_joules_est": 60.15, "sample_count": 15, "duration_seconds": 1.988}, "timestamp": "2026-01-16T16:16:20.107019"}
{"image_index": 318, "image_name": "000000033854.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000033854.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2041.563, "latencies_ms": [2041.563], "images_per_second": 0.49, "prompt_tokens": 19, "response_tokens_est": 53, "n_tiles": 6, "output_text": "The image features a cobblestone street lined with vintage cars, including a black car and a blue car, parked along the side. The cobblestones are arranged in a pattern, and the street is illuminated by natural light, suggesting it is daytime.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.6, "ram_available_mb": 100176.5, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 25595.6, "ram_available_mb": 100176.5, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.92, "peak": 15.32, "min": 14.32}, "VDD_GPU": {"avg": 30.2, "peak": 38.21, "min": 24.03}, "VIN": {"avg": 75.5, "peak": 122.22, "min": 56.86}, "VDD_CPU_SOC_MSS": {"avg": 15.54, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 30.2, "energy_joules_est": 61.66, "sample_count": 15, "duration_seconds": 2.042}, "timestamp": "2026-01-16T16:16:22.154582"}
{"image_index": 319, "image_name": "000000034071.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034071.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2034.801, "latencies_ms": [2034.801], "images_per_second": 0.491, "prompt_tokens": 9, "response_tokens_est": 54, "n_tiles": 6, "output_text": "The image depicts a close-up view of two parking meters, both with their respective signs and numbers visible, set against a blurred background of a cityscape during what appears to be either sunrise or sunset, with warm, golden light illuminating the scene.", "error": null, "sys_before": {"cpu_percent": 42.7, "ram_used_mb": 25595.6, "ram_available_mb": 100176.5, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 25595.9, "ram_available_mb": 100176.3, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.68, "peak": 15.22, "min": 13.92}, "VDD_GPU": {"avg": 29.46, "peak": 37.01, "min": 24.03}, "VIN": {"avg": 72.97, "peak": 119.56, "min": 52.13}, "VDD_CPU_SOC_MSS": {"avg": 15.41, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 29.46, "energy_joules_est": 59.96, "sample_count": 15, "duration_seconds": 2.035}, "timestamp": "2026-01-16T16:16:24.252453"}
{"image_index": 319, "image_name": "000000034071.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034071.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1005.522, "latencies_ms": [1005.522], "images_per_second": 0.995, "prompt_tokens": 23, "response_tokens_est": 14, "n_tiles": 6, "output_text": "parking meter: 2\nparking meter: 1", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25595.9, "ram_available_mb": 100176.3, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 25595.6, "ram_available_mb": 100176.5, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.64, "peak": 14.83, "min": 14.22}, "VDD_GPU": {"avg": 35.06, "peak": 37.82, "min": 31.12}, "VIN": {"avg": 79.31, "peak": 117.43, "min": 59.87}, "VDD_CPU_SOC_MSS": {"avg": 15.24, "peak": 15.36, "min": 14.96}}, "power_watts_avg": 35.06, "energy_joules_est": 35.26, "sample_count": 7, "duration_seconds": 1.006}, "timestamp": "2026-01-16T16:16:25.264981"}
{"image_index": 319, "image_name": "000000034071.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034071.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2505.369, "latencies_ms": [2505.369], "images_per_second": 0.399, "prompt_tokens": 27, "response_tokens_est": 70, "n_tiles": 6, "output_text": "The main objects in the image are two parking meters positioned in the foreground. The foreground is the closest and most detailed part of the image, while the background is slightly blurred, indicating a shallow depth of field. The parking meters are positioned near each other, with the closest one slightly to the left and the one further back to the right.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25595.6, "ram_available_mb": 100176.5, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 25595.6, "ram_available_mb": 100176.5, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.91, "peak": 15.22, "min": 14.22}, "VDD_GPU": {"avg": 29.69, "peak": 39.38, "min": 23.64}, "VIN": {"avg": 74.08, "peak": 116.82, "min": 56.15}, "VDD_CPU_SOC_MSS": {"avg": 15.46, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 29.69, "energy_joules_est": 74.4, "sample_count": 19, "duration_seconds": 2.506}, "timestamp": "2026-01-16T16:16:27.776726"}
{"image_index": 319, "image_name": "000000034071.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034071.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3010.507, "latencies_ms": [3010.507], "images_per_second": 0.332, "prompt_tokens": 21, "response_tokens_est": 89, "n_tiles": 6, "output_text": "The image depicts a dimly lit scene with two parking meters in focus, situated in an urban environment. The setting appears to be during twilight or early evening, as the sky is dim with a warm, golden hue, and the lights from nearby buildings are visible in the background. The focus on the parking meters suggests a moment of quiet, possibly indicating that the area is not heavily trafficked at the time the photo was taken.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.6, "ram_available_mb": 100176.5, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 25595.9, "ram_available_mb": 100176.3, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.02, "peak": 15.32, "min": 14.22}, "VDD_GPU": {"avg": 27.9, "peak": 38.19, "min": 23.64}, "VIN": {"avg": 68.23, "peak": 105.33, "min": 55.87}, "VDD_CPU_SOC_MSS": {"avg": 15.58, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 27.9, "energy_joules_est": 84.0, "sample_count": 23, "duration_seconds": 3.011}, "timestamp": "2026-01-16T16:16:30.793006"}
{"image_index": 319, "image_name": "000000034071.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034071.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2443.563, "latencies_ms": [2443.563], "images_per_second": 0.409, "prompt_tokens": 19, "response_tokens_est": 68, "n_tiles": 6, "output_text": "The image features two parking meters with a warm, golden hue, likely due to the lighting conditions. The lighting creates a soft, blurred effect, with bokeh lights visible in the background, suggesting a cityscape at sunset. The colors are dominated by warm tones, with the metallic surfaces of the meters reflecting the ambient light.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.9, "ram_available_mb": 100176.3, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 25596.1, "ram_available_mb": 100176.1, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.97, "peak": 15.32, "min": 14.32}, "VDD_GPU": {"avg": 28.98, "peak": 38.59, "min": 23.64}, "VIN": {"avg": 68.72, "peak": 96.03, "min": 57.68}, "VDD_CPU_SOC_MSS": {"avg": 15.48, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 28.98, "energy_joules_est": 70.82, "sample_count": 19, "duration_seconds": 2.444}, "timestamp": "2026-01-16T16:16:33.242592"}
{"image_index": 320, "image_name": "000000034139.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034139.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2386.027, "latencies_ms": [2386.027], "images_per_second": 0.419, "prompt_tokens": 9, "response_tokens_est": 29, "n_tiles": 12, "output_text": "The image shows a brown suitcase with various stickers and a plaque on it, placed on a platform, with three people standing behind it.", "error": null, "sys_before": {"cpu_percent": 40.7, "ram_used_mb": 25595.9, "ram_available_mb": 100176.3, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 25595.9, "ram_available_mb": 100176.3, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.18, "peak": 16.15, "min": 14.2}, "VDD_GPU": {"avg": 35.1, "peak": 40.97, "min": 27.57}, "VIN": {"avg": 78.76, "peak": 112.81, "min": 59.12}, "VDD_CPU_SOC_MSS": {"avg": 16.03, "peak": 17.32, "min": 15.35}}, "power_watts_avg": 35.1, "energy_joules_est": 83.77, "sample_count": 18, "duration_seconds": 2.386}, "timestamp": "2026-01-16T16:16:35.698997"}
{"image_index": 320, "image_name": "000000034139.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034139.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2649.748, "latencies_ms": [2649.748], "images_per_second": 0.377, "prompt_tokens": 23, "response_tokens_est": 33, "n_tiles": 12, "output_text": "1. Suitcase\n2. Man\n3. Woman\n4. Man\n5. Man\n6. Man\n7. Man\n8. Man", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.9, "ram_available_mb": 100176.3, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 25595.9, "ram_available_mb": 100176.3, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.07, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 35.7, "peak": 42.92, "min": 27.57}, "VIN": {"avg": 73.35, "peak": 113.65, "min": 53.28}, "VDD_CPU_SOC_MSS": {"avg": 15.75, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 35.7, "energy_joules_est": 94.61, "sample_count": 20, "duration_seconds": 2.65}, "timestamp": "2026-01-16T16:16:38.355083"}
{"image_index": 320, "image_name": "000000034139.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034139.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 4097.266, "latencies_ms": [4097.266], "images_per_second": 0.244, "prompt_tokens": 27, "response_tokens_est": 76, "n_tiles": 12, "output_text": "The main object in the foreground is a brown suitcase with various stickers and a sign on it. The suitcase is placed on a platform. In the background, there is a man and a woman standing next to each other. The man is wearing a black jacket and the woman is wearing a blue jacket. The sign on the suitcase is near the man and woman.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.9, "ram_available_mb": 100176.3, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25595.6, "ram_available_mb": 100176.6, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.08, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 32.26, "peak": 42.94, "min": 26.01}, "VIN": {"avg": 76.92, "peak": 130.4, "min": 56.01}, "VDD_CPU_SOC_MSS": {"avg": 15.6, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 32.26, "energy_joules_est": 132.19, "sample_count": 32, "duration_seconds": 4.098}, "timestamp": "2026-01-16T16:16:42.458889"}
{"image_index": 320, "image_name": "000000034139.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034139.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 4045.943, "latencies_ms": [4045.943], "images_per_second": 0.247, "prompt_tokens": 21, "response_tokens_est": 75, "n_tiles": 12, "output_text": "The image depicts a group of people standing in front of a building with a sign that reads \"Fidelity Investments.\" The setting appears to be an outdoor area, possibly a plaza or a public square, with a large, brown suitcase prominently displayed on a platform. The individuals are dressed in casual attire, and the atmosphere seems relaxed and informal.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.6, "ram_available_mb": 100176.6, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25596.1, "ram_available_mb": 100176.1, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.11, "peak": 15.65, "min": 14.43}, "VDD_GPU": {"avg": 32.38, "peak": 42.54, "min": 26.01}, "VIN": {"avg": 74.29, "peak": 133.89, "min": 58.73}, "VDD_CPU_SOC_MSS": {"avg": 15.75, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 32.38, "energy_joules_est": 131.02, "sample_count": 31, "duration_seconds": 4.046}, "timestamp": "2026-01-16T16:16:46.511137"}
{"image_index": 320, "image_name": "000000034139.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034139.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 4480.218, "latencies_ms": [4480.218], "images_per_second": 0.223, "prompt_tokens": 19, "response_tokens_est": 88, "n_tiles": 12, "output_text": "The image features a brown leather suitcase with various stickers and a sign on it. The suitcase is placed on a platform with a sign that reads \"GOD BLESS AMERICA\" by J. Seward Johnson. The background includes a building with a sign for \"Fidelity Investments\" and a couple of people standing nearby. The lighting is natural, suggesting it is daytime, and the weather appears to be clear.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25596.1, "ram_available_mb": 100176.1, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25596.1, "ram_available_mb": 100176.1, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.12, "peak": 15.75, "min": 14.53}, "VDD_GPU": {"avg": 31.66, "peak": 42.54, "min": 26.39}, "VIN": {"avg": 73.39, "peak": 113.65, "min": 50.94}, "VDD_CPU_SOC_MSS": {"avg": 15.76, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 31.66, "energy_joules_est": 141.86, "sample_count": 35, "duration_seconds": 4.481}, "timestamp": "2026-01-16T16:16:50.997769"}
{"image_index": 321, "image_name": "000000034205.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034205.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1682.998, "latencies_ms": [1682.998], "images_per_second": 0.594, "prompt_tokens": 9, "response_tokens_est": 41, "n_tiles": 6, "output_text": "The image showcases a plate of seared scallops, garnished with fresh green parsley, and accompanied by a side of broccoli, all presented in a visually appealing and appetizing manner.", "error": null, "sys_before": {"cpu_percent": 42.0, "ram_used_mb": 25596.1, "ram_available_mb": 100176.1, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 25596.1, "ram_available_mb": 100176.1, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.56, "peak": 15.11, "min": 13.82}, "VDD_GPU": {"avg": 30.69, "peak": 37.42, "min": 24.42}, "VIN": {"avg": 66.34, "peak": 109.5, "min": 55.11}, "VDD_CPU_SOC_MSS": {"avg": 15.29, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 30.69, "energy_joules_est": 51.66, "sample_count": 13, "duration_seconds": 1.683}, "timestamp": "2026-01-16T16:16:52.743178"}
{"image_index": 321, "image_name": "000000034205.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034205.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 4077.212, "latencies_ms": [4077.212], "images_per_second": 0.245, "prompt_tokens": 23, "response_tokens_est": 128, "n_tiles": 6, "output_text": "object: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject:", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25596.1, "ram_available_mb": 100176.1, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 25595.8, "ram_available_mb": 100176.3, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.06, "peak": 15.32, "min": 14.22}, "VDD_GPU": {"avg": 26.75, "peak": 38.19, "min": 23.64}, "VIN": {"avg": 67.62, "peak": 99.95, "min": 56.53}, "VDD_CPU_SOC_MSS": {"avg": 15.61, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 26.75, "energy_joules_est": 109.08, "sample_count": 32, "duration_seconds": 4.078}, "timestamp": "2026-01-16T16:16:56.826326"}
{"image_index": 321, "image_name": "000000034205.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034205.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2131.017, "latencies_ms": [2131.017], "images_per_second": 0.469, "prompt_tokens": 27, "response_tokens_est": 56, "n_tiles": 6, "output_text": "The main objects in the image are a plate of food, which includes a piece of fish, mushrooms, and broccoli. The fish is positioned in the foreground, with the mushrooms and broccoli surrounding it. The broccoli is in the background, slightly out of focus.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.8, "ram_available_mb": 100176.3, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 25595.8, "ram_available_mb": 100176.4, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.89, "peak": 15.22, "min": 14.22}, "VDD_GPU": {"avg": 29.79, "peak": 38.21, "min": 24.03}, "VIN": {"avg": 73.08, "peak": 107.1, "min": 60.69}, "VDD_CPU_SOC_MSS": {"avg": 15.48, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 29.79, "energy_joules_est": 63.5, "sample_count": 16, "duration_seconds": 2.132}, "timestamp": "2026-01-16T16:16:58.963419"}
{"image_index": 321, "image_name": "000000034205.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034205.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3020.243, "latencies_ms": [3020.243], "images_per_second": 0.331, "prompt_tokens": 21, "response_tokens_est": 86, "n_tiles": 6, "output_text": "The image depicts a close-up view of a dish featuring a piece of grilled fish, likely a salmon or similar type, garnished with fresh green herbs. The fish is placed on a bed of saut\u00e9ed mushrooms, which appear to be cooked to a tender consistency. The setting suggests a culinary presentation, possibly in a restaurant or a home kitchen, where the fish and mushrooms are being served as a main course.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.8, "ram_available_mb": 100176.4, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 25596.1, "ram_available_mb": 100176.1, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.89, "peak": 15.01, "min": 14.32}, "VDD_GPU": {"avg": 27.54, "peak": 38.19, "min": 22.85}, "VIN": {"avg": 69.13, "peak": 117.49, "min": 57.3}, "VDD_CPU_SOC_MSS": {"avg": 15.47, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 27.54, "energy_joules_est": 83.19, "sample_count": 24, "duration_seconds": 3.021}, "timestamp": "2026-01-16T16:17:01.989692"}
{"image_index": 321, "image_name": "000000034205.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034205.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2247.569, "latencies_ms": [2247.569], "images_per_second": 0.445, "prompt_tokens": 19, "response_tokens_est": 58, "n_tiles": 6, "output_text": "The image showcases a dish featuring a grilled or seared piece of fish, garnished with fresh green herbs and a sprinkle of black pepper. The lighting highlights the golden-brown crust of the fish, while the vibrant green of the herbs and broccoli adds a fresh contrast.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25596.1, "ram_available_mb": 100176.1, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 25595.8, "ram_available_mb": 100176.4, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.71, "peak": 15.01, "min": 14.02}, "VDD_GPU": {"avg": 28.99, "peak": 37.82, "min": 23.24}, "VIN": {"avg": 72.23, "peak": 116.88, "min": 61.06}, "VDD_CPU_SOC_MSS": {"avg": 15.35, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 28.99, "energy_joules_est": 65.17, "sample_count": 17, "duration_seconds": 2.248}, "timestamp": "2026-01-16T16:17:04.243231"}
{"image_index": 322, "image_name": "000000034257.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034257.jpg", "image_width": 500, "image_height": 309, "image_resolution": "500x309", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1604.454, "latencies_ms": [1604.454], "images_per_second": 0.623, "prompt_tokens": 9, "response_tokens_est": 38, "n_tiles": 6, "output_text": "The image shows a variety of fresh vegetables, including carrots, cabbage, and leafy greens, arranged in baskets on a wooden surface, likely at a market or grocery store.", "error": null, "sys_before": {"cpu_percent": 44.0, "ram_used_mb": 25595.8, "ram_available_mb": 100176.4, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 25595.8, "ram_available_mb": 100176.4, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.5, "peak": 15.11, "min": 13.82}, "VDD_GPU": {"avg": 30.69, "peak": 37.8, "min": 24.82}, "VIN": {"avg": 70.64, "peak": 119.76, "min": 57.51}, "VDD_CPU_SOC_MSS": {"avg": 15.22, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 30.69, "energy_joules_est": 49.25, "sample_count": 12, "duration_seconds": 1.605}, "timestamp": "2026-01-16T16:17:05.905275"}
{"image_index": 322, "image_name": "000000034257.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034257.jpg", "image_width": 500, "image_height": 309, "image_resolution": "500x309", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1302.547, "latencies_ms": [1302.547], "images_per_second": 0.768, "prompt_tokens": 23, "response_tokens_est": 25, "n_tiles": 6, "output_text": "carrots: 20\nonions: 10\nlettuce: 10\ncabbage: 10", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.8, "ram_available_mb": 100176.4, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 25596.1, "ram_available_mb": 100176.1, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.67, "peak": 14.91, "min": 14.22}, "VDD_GPU": {"avg": 33.75, "peak": 38.21, "min": 27.57}, "VIN": {"avg": 73.87, "peak": 110.94, "min": 56.07}, "VDD_CPU_SOC_MSS": {"avg": 15.27, "peak": 15.36, "min": 14.96}}, "power_watts_avg": 33.75, "energy_joules_est": 43.98, "sample_count": 9, "duration_seconds": 1.303}, "timestamp": "2026-01-16T16:17:07.213881"}
{"image_index": 322, "image_name": "000000034257.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034257.jpg", "image_width": 500, "image_height": 309, "image_resolution": "500x309", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2240.605, "latencies_ms": [2240.605], "images_per_second": 0.446, "prompt_tokens": 27, "response_tokens_est": 58, "n_tiles": 6, "output_text": "The main objects in the image are a basket filled with carrots and a bunch of green vegetables. The carrots are in the foreground, while the green vegetables are in the background. The basket is placed near the center of the image, and the vegetables are stacked on top of each other.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25596.1, "ram_available_mb": 100176.1, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 25595.8, "ram_available_mb": 100176.4, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.73, "peak": 15.01, "min": 14.02}, "VDD_GPU": {"avg": 29.57, "peak": 38.98, "min": 23.25}, "VIN": {"avg": 67.09, "peak": 102.1, "min": 55.74}, "VDD_CPU_SOC_MSS": {"avg": 15.24, "peak": 15.36, "min": 14.57}}, "power_watts_avg": 29.57, "energy_joules_est": 66.26, "sample_count": 17, "duration_seconds": 2.241}, "timestamp": "2026-01-16T16:17:09.460412"}
{"image_index": 322, "image_name": "000000034257.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034257.jpg", "image_width": 500, "image_height": 309, "image_resolution": "500x309", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2628.944, "latencies_ms": [2628.944], "images_per_second": 0.38, "prompt_tokens": 21, "response_tokens_est": 72, "n_tiles": 6, "output_text": "The image depicts a vibrant and colorful display of fresh vegetables at a market stall. The stall is filled with a variety of leafy greens, including lettuce, cabbage, and kale, alongside a basket of carrots and a bunch of onions. The scene is bright and inviting, showcasing the freshness and variety of produce available for purchase.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.8, "ram_available_mb": 100176.4, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 25595.8, "ram_available_mb": 100176.4, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.81, "peak": 15.11, "min": 14.12}, "VDD_GPU": {"avg": 28.28, "peak": 37.8, "min": 23.24}, "VIN": {"avg": 69.79, "peak": 112.42, "min": 56.8}, "VDD_CPU_SOC_MSS": {"avg": 15.28, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 28.28, "energy_joules_est": 74.36, "sample_count": 20, "duration_seconds": 2.629}, "timestamp": "2026-01-16T16:17:12.095300"}
{"image_index": 322, "image_name": "000000034257.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034257.jpg", "image_width": 500, "image_height": 309, "image_resolution": "500x309", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2772.19, "latencies_ms": [2772.19], "images_per_second": 0.361, "prompt_tokens": 19, "response_tokens_est": 77, "n_tiles": 6, "output_text": "The image showcases a vibrant and colorful display of fresh vegetables, including leafy greens, carrots, and onions, all arranged in a basket. The lighting is bright and natural, highlighting the freshness and vibrancy of the produce. The colors range from the deep green of the leafy greens to the bright orange of the carrots, creating a visually appealing and appetizing scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.8, "ram_available_mb": 100176.4, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 25595.8, "ram_available_mb": 100176.4, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.8, "peak": 15.01, "min": 14.12}, "VDD_GPU": {"avg": 28.06, "peak": 38.19, "min": 23.24}, "VIN": {"avg": 68.73, "peak": 111.43, "min": 56.41}, "VDD_CPU_SOC_MSS": {"avg": 15.24, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 28.06, "energy_joules_est": 77.8, "sample_count": 21, "duration_seconds": 2.773}, "timestamp": "2026-01-16T16:17:14.873746"}
{"image_index": 323, "image_name": "000000034417.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034417.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2378.084, "latencies_ms": [2378.084], "images_per_second": 0.421, "prompt_tokens": 9, "response_tokens_est": 29, "n_tiles": 12, "output_text": "The image depicts a bustling bakery with multiple doughnut machines in operation, producing a large number of freshly baked doughnuts.", "error": null, "sys_before": {"cpu_percent": 51.3, "ram_used_mb": 25595.8, "ram_available_mb": 100176.4, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 25596.0, "ram_available_mb": 100176.1, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.16, "peak": 16.15, "min": 14.0}, "VDD_GPU": {"avg": 34.88, "peak": 40.97, "min": 27.57}, "VIN": {"avg": 76.63, "peak": 108.92, "min": 59.46}, "VDD_CPU_SOC_MSS": {"avg": 16.03, "peak": 17.32, "min": 15.35}}, "power_watts_avg": 34.88, "energy_joules_est": 82.96, "sample_count": 18, "duration_seconds": 2.379}, "timestamp": "2026-01-16T16:17:17.335557"}
{"image_index": 323, "image_name": "000000034417.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034417.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 5857.026, "latencies_ms": [5857.026], "images_per_second": 0.171, "prompt_tokens": 23, "response_tokens_est": 128, "n_tiles": 12, "output_text": "object: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject:", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25596.0, "ram_available_mb": 100176.1, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 25595.8, "ram_available_mb": 100176.4, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.09, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 30.57, "peak": 42.94, "min": 26.0}, "VIN": {"avg": 70.8, "peak": 105.35, "min": 58.42}, "VDD_CPU_SOC_MSS": {"avg": 15.66, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 30.57, "energy_joules_est": 179.06, "sample_count": 46, "duration_seconds": 5.857}, "timestamp": "2026-01-16T16:17:23.198864"}
{"image_index": 323, "image_name": "000000034417.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034417.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3506.425, "latencies_ms": [3506.425], "images_per_second": 0.285, "prompt_tokens": 27, "response_tokens_est": 59, "n_tiles": 12, "output_text": "The main objects in the image are a doughnut machine and a doughnut stand. The doughnut machine is located in the foreground, with doughnuts being processed on the conveyor belt. The doughnut stand is situated in the background, with doughnuts being displayed on the shelves.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.8, "ram_available_mb": 100176.4, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 25595.8, "ram_available_mb": 100176.4, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.11, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 33.4, "peak": 42.53, "min": 26.39}, "VIN": {"avg": 76.66, "peak": 125.64, "min": 33.49}, "VDD_CPU_SOC_MSS": {"avg": 15.75, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 33.4, "energy_joules_est": 117.13, "sample_count": 27, "duration_seconds": 3.507}, "timestamp": "2026-01-16T16:17:26.711865"}
{"image_index": 323, "image_name": "000000034417.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034417.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 4673.285, "latencies_ms": [4673.285], "images_per_second": 0.214, "prompt_tokens": 21, "response_tokens_est": 94, "n_tiles": 12, "output_text": "The image depicts a bustling bakery or doughnut shop with a focus on the production line for donuts. The setting is indoors, likely in a commercial kitchen or bakery, with various donuts being prepared and packaged. The workers are actively engaged in their tasks, with one person in a white hat handling dough and another in a red shirt preparing dough. The overall atmosphere is busy and industrious, with the donuts being the central activity.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.8, "ram_available_mb": 100176.4, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 25595.8, "ram_available_mb": 100176.4, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.14, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 31.6, "peak": 42.94, "min": 26.39}, "VIN": {"avg": 70.95, "peak": 107.32, "min": 58.19}, "VDD_CPU_SOC_MSS": {"avg": 15.74, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 31.6, "energy_joules_est": 147.69, "sample_count": 36, "duration_seconds": 4.674}, "timestamp": "2026-01-16T16:17:31.391979"}
{"image_index": 323, "image_name": "000000034417.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034417.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3011.535, "latencies_ms": [3011.535], "images_per_second": 0.332, "prompt_tokens": 19, "response_tokens_est": 44, "n_tiles": 12, "output_text": "The image depicts a bakery with a bright and clean interior, featuring a mix of stainless steel and glass materials. The lighting is artificial, with a mix of overhead and ambient light sources illuminating the space.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.8, "ram_available_mb": 100176.4, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 25595.8, "ram_available_mb": 100176.4, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.1, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 34.39, "peak": 42.94, "min": 26.79}, "VIN": {"avg": 75.73, "peak": 120.24, "min": 42.87}, "VDD_CPU_SOC_MSS": {"avg": 15.68, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 34.39, "energy_joules_est": 103.58, "sample_count": 24, "duration_seconds": 3.012}, "timestamp": "2026-01-16T16:17:34.410283"}
{"image_index": 324, "image_name": "000000034452.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034452.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2392.831, "latencies_ms": [2392.831], "images_per_second": 0.418, "prompt_tokens": 9, "response_tokens_est": 29, "n_tiles": 12, "output_text": "A man is playing frisbee in a forest, wearing a green jacket and khaki shorts, with a white cap on his head.", "error": null, "sys_before": {"cpu_percent": 42.5, "ram_used_mb": 25595.8, "ram_available_mb": 100176.4, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 25595.8, "ram_available_mb": 100176.4, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.08, "peak": 16.15, "min": 13.59}, "VDD_GPU": {"avg": 35.34, "peak": 41.76, "min": 27.97}, "VIN": {"avg": 75.17, "peak": 110.03, "min": 58.04}, "VDD_CPU_SOC_MSS": {"avg": 15.79, "peak": 17.32, "min": 14.57}}, "power_watts_avg": 35.34, "energy_joules_est": 84.57, "sample_count": 18, "duration_seconds": 2.393}, "timestamp": "2026-01-16T16:17:36.902007"}
{"image_index": 324, "image_name": "000000034452.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034452.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2881.236, "latencies_ms": [2881.236], "images_per_second": 0.347, "prompt_tokens": 23, "response_tokens_est": 40, "n_tiles": 12, "output_text": "frisbee: 1\nman: 1\nhat: 1\nshorts: 1\nsocks: 1\nfootwear: 1\ntree: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.8, "ram_available_mb": 100176.4, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 25595.8, "ram_available_mb": 100176.4, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.08, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 35.2, "peak": 43.33, "min": 26.79}, "VIN": {"avg": 72.41, "peak": 110.59, "min": 51.73}, "VDD_CPU_SOC_MSS": {"avg": 15.67, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 35.2, "energy_joules_est": 101.44, "sample_count": 22, "duration_seconds": 2.882}, "timestamp": "2026-01-16T16:17:39.789918"}
{"image_index": 324, "image_name": "000000034452.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034452.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3984.325, "latencies_ms": [3984.325], "images_per_second": 0.251, "prompt_tokens": 27, "response_tokens_est": 73, "n_tiles": 12, "output_text": "The main object in the foreground is a man wearing a green jacket and khaki shorts, who is bending over to pick up an orange frisbee. The background consists of a dense forest with tall trees and a mix of green and brown leaves on the ground. The man is positioned near the center of the image, with the trees forming a natural backdrop.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.8, "ram_available_mb": 100176.4, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25595.8, "ram_available_mb": 100176.4, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.1, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 32.45, "peak": 42.54, "min": 26.39}, "VIN": {"avg": 71.46, "peak": 114.67, "min": 58.06}, "VDD_CPU_SOC_MSS": {"avg": 15.66, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 32.45, "energy_joules_est": 129.31, "sample_count": 32, "duration_seconds": 3.985}, "timestamp": "2026-01-16T16:17:43.782109"}
{"image_index": 324, "image_name": "000000034452.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034452.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3453.816, "latencies_ms": [3453.816], "images_per_second": 0.29, "prompt_tokens": 21, "response_tokens_est": 57, "n_tiles": 12, "output_text": "The image depicts a serene forest scene with a man in a green jacket and cap playing with an orange frisbee. The man is bending over, focusing on the frisbee, while surrounded by tall trees with bare branches, indicating it might be autumn or winter.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.8, "ram_available_mb": 100176.4, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 25595.8, "ram_available_mb": 100176.4, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.08, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 33.36, "peak": 42.94, "min": 26.39}, "VIN": {"avg": 76.1, "peak": 143.88, "min": 54.77}, "VDD_CPU_SOC_MSS": {"avg": 15.7, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 33.36, "energy_joules_est": 115.24, "sample_count": 27, "duration_seconds": 3.454}, "timestamp": "2026-01-16T16:17:47.242347"}
{"image_index": 324, "image_name": "000000034452.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034452.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3547.771, "latencies_ms": [3547.771], "images_per_second": 0.282, "prompt_tokens": 19, "response_tokens_est": 60, "n_tiles": 12, "output_text": "The image depicts a forest scene with a man wearing a green jacket and khaki shorts. The ground is covered with fallen leaves and pine needles, and the trees have thin, bare branches. The lighting is natural, suggesting it is daytime, and the overall atmosphere is serene and peaceful.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.8, "ram_available_mb": 100176.4, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 25595.5, "ram_available_mb": 100176.7, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.09, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 33.14, "peak": 42.54, "min": 26.39}, "VIN": {"avg": 75.41, "peak": 114.88, "min": 58.15}, "VDD_CPU_SOC_MSS": {"avg": 15.62, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 33.14, "energy_joules_est": 117.59, "sample_count": 28, "duration_seconds": 3.548}, "timestamp": "2026-01-16T16:17:50.796741"}
{"image_index": 325, "image_name": "000000034760.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034760.jpg", "image_width": 426, "image_height": 640, "image_resolution": "426x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1414.591, "latencies_ms": [1414.591], "images_per_second": 0.707, "prompt_tokens": 9, "response_tokens_est": 31, "n_tiles": 6, "output_text": "The image shows a bathroom with a white sink, a toilet, and a shower curtain, all set against a beige tiled floor and walls.", "error": null, "sys_before": {"cpu_percent": 38.9, "ram_used_mb": 25595.5, "ram_available_mb": 100176.7, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 25595.5, "ram_available_mb": 100176.7, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.49, "peak": 15.01, "min": 13.92}, "VDD_GPU": {"avg": 31.8, "peak": 37.42, "min": 25.6}, "VIN": {"avg": 70.26, "peak": 111.39, "min": 59.2}, "VDD_CPU_SOC_MSS": {"avg": 15.21, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 31.8, "energy_joules_est": 44.99, "sample_count": 11, "duration_seconds": 1.415}, "timestamp": "2026-01-16T16:17:52.268510"}
{"image_index": 325, "image_name": "000000034760.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034760.jpg", "image_width": 426, "image_height": 640, "image_resolution": "426x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1702.648, "latencies_ms": [1702.648], "images_per_second": 0.587, "prompt_tokens": 23, "response_tokens_est": 40, "n_tiles": 6, "output_text": "object: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.5, "ram_available_mb": 100176.7, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25595.5, "ram_available_mb": 100176.7, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.8, "peak": 15.22, "min": 14.22}, "VDD_GPU": {"avg": 31.42, "peak": 38.19, "min": 24.82}, "VIN": {"avg": 70.11, "peak": 117.81, "min": 57.31}, "VDD_CPU_SOC_MSS": {"avg": 15.35, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 31.42, "energy_joules_est": 53.51, "sample_count": 13, "duration_seconds": 1.703}, "timestamp": "2026-01-16T16:17:53.976987"}
{"image_index": 325, "image_name": "000000034760.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034760.jpg", "image_width": 426, "image_height": 640, "image_resolution": "426x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2820.499, "latencies_ms": [2820.499], "images_per_second": 0.355, "prompt_tokens": 27, "response_tokens_est": 82, "n_tiles": 6, "output_text": "The bathroom sink is located on the left side of the image, with a white porcelain toilet and a white towel rack to its right. The shower curtain is partially visible on the right side, with a towel hanging on the rack. The floor is tiled, and there is a small white towel on the sink. The overall layout is straightforward, with the sink and toilet positioned near the shower area.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.5, "ram_available_mb": 100176.7, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 25595.5, "ram_available_mb": 100176.7, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.03, "peak": 15.32, "min": 14.32}, "VDD_GPU": {"avg": 28.42, "peak": 38.21, "min": 23.64}, "VIN": {"avg": 72.3, "peak": 116.04, "min": 55.24}, "VDD_CPU_SOC_MSS": {"avg": 15.78, "peak": 16.14, "min": 14.96}}, "power_watts_avg": 28.42, "energy_joules_est": 80.17, "sample_count": 22, "duration_seconds": 2.821}, "timestamp": "2026-01-16T16:17:56.803626"}
{"image_index": 325, "image_name": "000000034760.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034760.jpg", "image_width": 426, "image_height": 640, "image_resolution": "426x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2580.345, "latencies_ms": [2580.345], "images_per_second": 0.388, "prompt_tokens": 21, "response_tokens_est": 73, "n_tiles": 6, "output_text": "The image depicts a bathroom with a beige tiled floor and walls. The room features a white sink with a gold faucet, a toilet, and a shower curtain with blue and white stripes. The bathroom appears to be well-maintained and clean, with a few items like a blue towel and a can of AAI on the countertop.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25595.5, "ram_available_mb": 100176.7, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 25595.7, "ram_available_mb": 100176.4, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.0, "peak": 15.32, "min": 14.32}, "VDD_GPU": {"avg": 28.6, "peak": 37.82, "min": 23.64}, "VIN": {"avg": 66.79, "peak": 117.79, "min": 56.77}, "VDD_CPU_SOC_MSS": {"avg": 15.65, "peak": 16.14, "min": 14.96}}, "power_watts_avg": 28.6, "energy_joules_est": 73.81, "sample_count": 20, "duration_seconds": 2.581}, "timestamp": "2026-01-16T16:17:59.389757"}
{"image_index": 325, "image_name": "000000034760.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034760.jpg", "image_width": 426, "image_height": 640, "image_resolution": "426x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1850.943, "latencies_ms": [1850.943], "images_per_second": 0.54, "prompt_tokens": 19, "response_tokens_est": 44, "n_tiles": 6, "output_text": "The bathroom features a beige tiled floor and walls, with a white sink and faucet. The shower curtain is striped in blue and white. The lighting is soft and warm, creating a cozy atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.7, "ram_available_mb": 100176.4, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 25595.7, "ram_available_mb": 100176.4, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.79, "peak": 15.01, "min": 14.32}, "VDD_GPU": {"avg": 30.47, "peak": 38.21, "min": 24.03}, "VIN": {"avg": 67.98, "peak": 113.1, "min": 56.67}, "VDD_CPU_SOC_MSS": {"avg": 15.44, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 30.47, "energy_joules_est": 56.41, "sample_count": 14, "duration_seconds": 1.851}, "timestamp": "2026-01-16T16:18:01.246501"}
{"image_index": 326, "image_name": "000000034873.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034873.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2447.507, "latencies_ms": [2447.507], "images_per_second": 0.409, "prompt_tokens": 9, "response_tokens_est": 31, "n_tiles": 12, "output_text": "The image shows a kitchen with a wooden dining table and chairs, a black countertop, a white sink, and a window with greenery outside.", "error": null, "sys_before": {"cpu_percent": 44.8, "ram_used_mb": 25595.7, "ram_available_mb": 100176.4, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 25595.7, "ram_available_mb": 100176.4, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.15, "peak": 16.05, "min": 14.0}, "VDD_GPU": {"avg": 34.85, "peak": 40.97, "min": 27.57}, "VIN": {"avg": 77.36, "peak": 121.74, "min": 58.85}, "VDD_CPU_SOC_MSS": {"avg": 15.85, "peak": 16.92, "min": 15.35}}, "power_watts_avg": 34.85, "energy_joules_est": 85.31, "sample_count": 19, "duration_seconds": 2.448}, "timestamp": "2026-01-16T16:18:03.769325"}
{"image_index": 326, "image_name": "000000034873.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034873.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2410.722, "latencies_ms": [2410.722], "images_per_second": 0.415, "prompt_tokens": 23, "response_tokens_est": 26, "n_tiles": 12, "output_text": "- sink\n- faucet\n- countertop\n- cabinet\n- table\n- chairs\n- window\n- door", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.7, "ram_available_mb": 100176.4, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 2.5, "ram_used_mb": 25596.0, "ram_available_mb": 100176.2, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.07, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 36.42, "peak": 43.33, "min": 27.97}, "VIN": {"avg": 78.92, "peak": 115.35, "min": 58.68}, "VDD_CPU_SOC_MSS": {"avg": 15.81, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 36.42, "energy_joules_est": 87.82, "sample_count": 19, "duration_seconds": 2.411}, "timestamp": "2026-01-16T16:18:06.186383"}
{"image_index": 326, "image_name": "000000034873.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034873.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3824.766, "latencies_ms": [3824.766], "images_per_second": 0.261, "prompt_tokens": 27, "response_tokens_est": 68, "n_tiles": 12, "output_text": "The main objects in the image are a kitchen sink, a wooden dining table, and a wooden cabinet. The sink is located in the foreground, near the cabinet, while the dining table is in the background, slightly to the right. The cabinet is positioned to the left of the sink, and the table is situated behind the cabinet.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25596.0, "ram_available_mb": 100176.2, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25595.7, "ram_available_mb": 100176.5, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.09, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 32.88, "peak": 42.94, "min": 26.39}, "VIN": {"avg": 74.38, "peak": 106.6, "min": 58.76}, "VDD_CPU_SOC_MSS": {"avg": 15.73, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 32.88, "energy_joules_est": 125.77, "sample_count": 30, "duration_seconds": 3.825}, "timestamp": "2026-01-16T16:18:10.017526"}
{"image_index": 326, "image_name": "000000034873.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034873.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3554.436, "latencies_ms": [3554.436], "images_per_second": 0.281, "prompt_tokens": 21, "response_tokens_est": 60, "n_tiles": 12, "output_text": "The image depicts a well-organized kitchen with a modern design. The kitchen features a sleek black countertop, a white sink, and a wooden dining table surrounded by six chairs. The room is brightly lit with natural light coming through a window, creating a clean and inviting atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.7, "ram_available_mb": 100176.5, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25596.0, "ram_available_mb": 100176.2, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.09, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 33.03, "peak": 42.54, "min": 26.39}, "VIN": {"avg": 73.9, "peak": 117.07, "min": 57.68}, "VDD_CPU_SOC_MSS": {"avg": 15.75, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 33.03, "energy_joules_est": 117.42, "sample_count": 28, "duration_seconds": 3.555}, "timestamp": "2026-01-16T16:18:13.578246"}
{"image_index": 326, "image_name": "000000034873.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000034873.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3047.462, "latencies_ms": [3047.462], "images_per_second": 0.328, "prompt_tokens": 19, "response_tokens_est": 45, "n_tiles": 12, "output_text": "The kitchen features a sleek, modern design with a dark granite countertop and a light wooden cabinet. The room is well-lit with natural light streaming in through a large window, creating a bright and airy atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25596.0, "ram_available_mb": 100176.2, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 25595.7, "ram_available_mb": 100176.5, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.07, "peak": 15.65, "min": 14.43}, "VDD_GPU": {"avg": 34.24, "peak": 42.54, "min": 26.8}, "VIN": {"avg": 75.64, "peak": 115.38, "min": 58.85}, "VDD_CPU_SOC_MSS": {"avg": 15.75, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 34.24, "energy_joules_est": 104.37, "sample_count": 24, "duration_seconds": 3.048}, "timestamp": "2026-01-16T16:18:16.632361"}
{"image_index": 327, "image_name": "000000035062.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000035062.jpg", "image_width": 425, "image_height": 640, "image_resolution": "425x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1177.969, "latencies_ms": [1177.969], "images_per_second": 0.849, "prompt_tokens": 9, "response_tokens_est": 22, "n_tiles": 6, "output_text": "A baby is sleeping peacefully in a bed covered with a blanket adorned with white daisy flowers.", "error": null, "sys_before": {"cpu_percent": 44.6, "ram_used_mb": 25595.7, "ram_available_mb": 100176.5, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25595.7, "ram_available_mb": 100176.5, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.31, "peak": 14.81, "min": 13.82}, "VDD_GPU": {"avg": 33.35, "peak": 37.82, "min": 27.57}, "VIN": {"avg": 82.01, "peak": 116.68, "min": 60.52}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 33.35, "energy_joules_est": 39.3, "sample_count": 9, "duration_seconds": 1.178}, "timestamp": "2026-01-16T16:18:17.875035"}
{"image_index": 327, "image_name": "000000035062.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000035062.jpg", "image_width": 425, "image_height": 640, "image_resolution": "425x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1246.328, "latencies_ms": [1246.328], "images_per_second": 0.802, "prompt_tokens": 23, "response_tokens_est": 23, "n_tiles": 6, "output_text": "bed: 1\npillow: 1\nblanket: 1\ndaisy: 8", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25595.7, "ram_available_mb": 100176.5, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 25595.7, "ram_available_mb": 100176.5, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.63, "peak": 14.91, "min": 14.22}, "VDD_GPU": {"avg": 34.53, "peak": 39.0, "min": 27.97}, "VIN": {"avg": 72.39, "peak": 111.42, "min": 57.93}, "VDD_CPU_SOC_MSS": {"avg": 15.26, "peak": 15.36, "min": 14.96}}, "power_watts_avg": 34.53, "energy_joules_est": 43.05, "sample_count": 9, "duration_seconds": 1.247}, "timestamp": "2026-01-16T16:18:19.127931"}
{"image_index": 327, "image_name": "000000035062.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000035062.jpg", "image_width": 425, "image_height": 640, "image_resolution": "425x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3236.121, "latencies_ms": [3236.121], "images_per_second": 0.309, "prompt_tokens": 27, "response_tokens_est": 93, "n_tiles": 6, "output_text": "The main object in the image is a bed with a patterned blanket. The blanket is spread out on the bed, covering the entire surface. The pattern on the blanket consists of white daisies with orange centers. The bed is positioned against a wall, and there is a small piece of furniture, possibly a nightstand, to the right of the bed. The overall scene is dimly lit, with the primary focus on the bed and its blanket.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.7, "ram_available_mb": 100176.5, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 25596.0, "ram_available_mb": 100176.2, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.81, "peak": 15.11, "min": 14.02}, "VDD_GPU": {"avg": 27.78, "peak": 39.39, "min": 23.24}, "VIN": {"avg": 67.41, "peak": 113.17, "min": 52.89}, "VDD_CPU_SOC_MSS": {"avg": 15.39, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 27.78, "energy_joules_est": 89.91, "sample_count": 25, "duration_seconds": 3.236}, "timestamp": "2026-01-16T16:18:22.370741"}
{"image_index": 327, "image_name": "000000035062.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000035062.jpg", "image_width": 425, "image_height": 640, "image_resolution": "425x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2331.872, "latencies_ms": [2331.872], "images_per_second": 0.429, "prompt_tokens": 21, "response_tokens_est": 61, "n_tiles": 6, "output_text": "The image depicts a cozy, dimly lit bedroom scene where a baby is lying on a bed covered with a dark blue blanket adorned with white daisy patterns. The baby appears to be sleeping peacefully, surrounded by the soft glow of a lamp, creating a serene and intimate atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25596.0, "ram_available_mb": 100176.2, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 25595.9, "ram_available_mb": 100176.2, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.75, "peak": 15.01, "min": 14.12}, "VDD_GPU": {"avg": 28.85, "peak": 38.19, "min": 23.24}, "VIN": {"avg": 67.79, "peak": 102.04, "min": 59.5}, "VDD_CPU_SOC_MSS": {"avg": 15.31, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 28.85, "energy_joules_est": 67.28, "sample_count": 17, "duration_seconds": 2.332}, "timestamp": "2026-01-16T16:18:24.709077"}
{"image_index": 327, "image_name": "000000035062.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000035062.jpg", "image_width": 425, "image_height": 640, "image_resolution": "425x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1494.677, "latencies_ms": [1494.677], "images_per_second": 0.669, "prompt_tokens": 19, "response_tokens_est": 31, "n_tiles": 6, "output_text": "The image features a bed with a dark blue blanket adorned with white daisy patterns. The lighting is dim, creating a cozy and intimate atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.9, "ram_available_mb": 100176.2, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25595.9, "ram_available_mb": 100176.2, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.56, "peak": 14.81, "min": 14.02}, "VDD_GPU": {"avg": 31.83, "peak": 38.6, "min": 25.21}, "VIN": {"avg": 70.03, "peak": 107.85, "min": 57.13}, "VDD_CPU_SOC_MSS": {"avg": 15.21, "peak": 15.36, "min": 14.57}}, "power_watts_avg": 31.83, "energy_joules_est": 47.58, "sample_count": 11, "duration_seconds": 1.495}, "timestamp": "2026-01-16T16:18:26.209808"}
{"image_index": 328, "image_name": "000000035197.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000035197.jpg", "image_width": 429, "image_height": 640, "image_resolution": "429x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1504.265, "latencies_ms": [1504.265], "images_per_second": 0.665, "prompt_tokens": 9, "response_tokens_est": 33, "n_tiles": 6, "output_text": "A skateboarder is captured mid-action on a skateboard, with a black and white photograph emphasizing the contrast between the skateboard and the urban environment.", "error": null, "sys_before": {"cpu_percent": 43.6, "ram_used_mb": 25595.9, "ram_available_mb": 100176.2, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 25595.9, "ram_available_mb": 100176.2, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.39, "peak": 14.81, "min": 13.82}, "VDD_GPU": {"avg": 31.37, "peak": 37.82, "min": 25.22}, "VIN": {"avg": 79.37, "peak": 121.26, "min": 60.5}, "VDD_CPU_SOC_MSS": {"avg": 15.1, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 31.37, "energy_joules_est": 47.2, "sample_count": 11, "duration_seconds": 1.505}, "timestamp": "2026-01-16T16:18:27.756866"}
{"image_index": 328, "image_name": "000000035197.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000035197.jpg", "image_width": 429, "image_height": 640, "image_resolution": "429x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2246.245, "latencies_ms": [2246.245], "images_per_second": 0.445, "prompt_tokens": 23, "response_tokens_est": 58, "n_tiles": 6, "output_text": "skateboard: 1\nskateboarder: 1\nskateboard: 1\nskateboarder: 1\nskateboard: 1\nskateboard: 1\nskateboard: 1\nskateboard: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.9, "ram_available_mb": 100176.2, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 25596.2, "ram_available_mb": 100176.0, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.71, "peak": 15.01, "min": 14.12}, "VDD_GPU": {"avg": 29.49, "peak": 38.6, "min": 23.24}, "VIN": {"avg": 68.69, "peak": 95.37, "min": 55.22}, "VDD_CPU_SOC_MSS": {"avg": 15.28, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 29.49, "energy_joules_est": 66.25, "sample_count": 16, "duration_seconds": 2.247}, "timestamp": "2026-01-16T16:18:30.013854"}
{"image_index": 328, "image_name": "000000035197.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000035197.jpg", "image_width": 429, "image_height": 640, "image_resolution": "429x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3251.833, "latencies_ms": [3251.833], "images_per_second": 0.308, "prompt_tokens": 27, "response_tokens_est": 94, "n_tiles": 6, "output_text": "The main object in the foreground is a skateboard, which is positioned near the bottom left corner of the image. The skateboard has a visible wheel and a deck. In the background, there is a person standing on the sidewalk, slightly to the right of the skateboard. The person is wearing shorts and sneakers, and their shadow is cast on the ground. The building in the background is tall and has a flat roof, with a few windows visible.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25596.2, "ram_available_mb": 100176.0, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 25596.2, "ram_available_mb": 100176.0, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.83, "peak": 15.11, "min": 14.02}, "VDD_GPU": {"avg": 27.33, "peak": 38.19, "min": 23.24}, "VIN": {"avg": 71.47, "peak": 119.11, "min": 61.63}, "VDD_CPU_SOC_MSS": {"avg": 15.32, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 27.33, "energy_joules_est": 88.88, "sample_count": 24, "duration_seconds": 3.252}, "timestamp": "2026-01-16T16:18:33.271826"}
{"image_index": 328, "image_name": "000000035197.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000035197.jpg", "image_width": 429, "image_height": 640, "image_resolution": "429x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2270.676, "latencies_ms": [2270.676], "images_per_second": 0.44, "prompt_tokens": 21, "response_tokens_est": 59, "n_tiles": 6, "output_text": "The image depicts a skateboarder in motion, captured in black and white, on a concrete surface with a large, angular concrete structure in the background. The skateboarder is wearing casual attire, including shorts and sneakers, and is performing a trick on the skateboard.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.9, "ram_available_mb": 100176.2, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 25595.9, "ram_available_mb": 100176.2, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.74, "peak": 15.01, "min": 14.02}, "VDD_GPU": {"avg": 29.06, "peak": 37.8, "min": 23.24}, "VIN": {"avg": 69.32, "peak": 115.96, "min": 57.4}, "VDD_CPU_SOC_MSS": {"avg": 15.19, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 29.06, "energy_joules_est": 66.0, "sample_count": 17, "duration_seconds": 2.271}, "timestamp": "2026-01-16T16:18:35.548678"}
{"image_index": 328, "image_name": "000000035197.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000035197.jpg", "image_width": 429, "image_height": 640, "image_resolution": "429x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2881.301, "latencies_ms": [2881.301], "images_per_second": 0.347, "prompt_tokens": 19, "response_tokens_est": 81, "n_tiles": 6, "output_text": "The image is a black and white photograph featuring a skateboarder in motion, captured with a high-contrast, sharp focus. The skateboarder is wearing a white t-shirt, plaid shorts, and sneakers, with a shadow cast on the ground. The lighting is bright, likely from a clear sky, and the skateboarder's shadow is distinctly visible on the ground.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.9, "ram_available_mb": 100176.2, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 25595.9, "ram_available_mb": 100176.2, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.81, "peak": 15.01, "min": 14.12}, "VDD_GPU": {"avg": 27.75, "peak": 37.82, "min": 23.24}, "VIN": {"avg": 67.95, "peak": 116.28, "min": 54.46}, "VDD_CPU_SOC_MSS": {"avg": 15.46, "peak": 15.75, "min": 14.57}}, "power_watts_avg": 27.75, "energy_joules_est": 79.96, "sample_count": 22, "duration_seconds": 2.882}, "timestamp": "2026-01-16T16:18:38.436045"}
{"image_index": 329, "image_name": "000000035279.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000035279.jpg", "image_width": 640, "image_height": 463, "image_resolution": "640x463", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2582.09, "latencies_ms": [2582.09], "images_per_second": 0.387, "prompt_tokens": 9, "response_tokens_est": 35, "n_tiles": 12, "output_text": "The image shows a person lying on the floor with various items scattered around, including a laptop, a camera, a smartphone, a book, and a tennis racket.", "error": null, "sys_before": {"cpu_percent": 46.5, "ram_used_mb": 25595.9, "ram_available_mb": 100176.2, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 25595.9, "ram_available_mb": 100176.3, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.13, "peak": 16.15, "min": 13.9}, "VDD_GPU": {"avg": 34.37, "peak": 40.97, "min": 27.18}, "VIN": {"avg": 74.27, "peak": 107.66, "min": 58.63}, "VDD_CPU_SOC_MSS": {"avg": 15.84, "peak": 17.32, "min": 14.96}}, "power_watts_avg": 34.37, "energy_joules_est": 88.76, "sample_count": 20, "duration_seconds": 2.582}, "timestamp": "2026-01-16T16:18:41.103001"}
{"image_index": 329, "image_name": "000000035279.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000035279.jpg", "image_width": 640, "image_height": 463, "image_resolution": "640x463", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2745.567, "latencies_ms": [2745.567], "images_per_second": 0.364, "prompt_tokens": 23, "response_tokens_est": 36, "n_tiles": 12, "output_text": "1. Laptop\n2. Phone\n3. Camera\n4. Book\n5. Tennis racket\n6. Tennis ball\n7. Water bottle\n8. Key", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.9, "ram_available_mb": 100176.3, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 25595.7, "ram_available_mb": 100176.5, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.06, "peak": 15.54, "min": 14.43}, "VDD_GPU": {"avg": 35.15, "peak": 42.94, "min": 26.79}, "VIN": {"avg": 75.23, "peak": 116.14, "min": 58.56}, "VDD_CPU_SOC_MSS": {"avg": 15.6, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 35.15, "energy_joules_est": 96.53, "sample_count": 22, "duration_seconds": 2.746}, "timestamp": "2026-01-16T16:18:43.855416"}
{"image_index": 329, "image_name": "000000035279.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000035279.jpg", "image_width": 640, "image_height": 463, "image_resolution": "640x463", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 4130.435, "latencies_ms": [4130.435], "images_per_second": 0.242, "prompt_tokens": 27, "response_tokens_est": 77, "n_tiles": 12, "output_text": "The main objects in the image are scattered across the carpeted floor. The laptop is positioned in the background, slightly to the left, while the smartphone is in the foreground, closer to the camera. The book titled \"The Men Who Ruled India\" is placed near the bottom right corner of the image, with the author's name, Philip Mason, visible on the cover.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.7, "ram_available_mb": 100176.5, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25595.4, "ram_available_mb": 100176.8, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.09, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 32.2, "peak": 42.54, "min": 26.0}, "VIN": {"avg": 72.9, "peak": 114.47, "min": 57.6}, "VDD_CPU_SOC_MSS": {"avg": 15.52, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 32.2, "energy_joules_est": 133.01, "sample_count": 33, "duration_seconds": 4.131}, "timestamp": "2026-01-16T16:18:47.992705"}
{"image_index": 329, "image_name": "000000035279.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000035279.jpg", "image_width": 640, "image_height": 463, "image_resolution": "640x463", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 4126.255, "latencies_ms": [4126.255], "images_per_second": 0.242, "prompt_tokens": 21, "response_tokens_est": 77, "n_tiles": 12, "output_text": "The image depicts a cluttered indoor setting with a person lying on the floor, surrounded by various items. The person appears to be in a relaxed or possibly sleepy state, with their head resting on their hand. The surrounding items include a laptop, a smartphone, a camera, a book, and other miscellaneous objects, suggesting a casual and somewhat disorganized environment.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.4, "ram_available_mb": 100176.8, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 25595.4, "ram_available_mb": 100176.8, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.09, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 32.26, "peak": 42.94, "min": 26.0}, "VIN": {"avg": 70.26, "peak": 103.6, "min": 44.1}, "VDD_CPU_SOC_MSS": {"avg": 15.72, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 32.26, "energy_joules_est": 133.13, "sample_count": 32, "duration_seconds": 4.127}, "timestamp": "2026-01-16T16:18:52.129372"}
{"image_index": 329, "image_name": "000000035279.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000035279.jpg", "image_width": 640, "image_height": 463, "image_resolution": "640x463", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3756.354, "latencies_ms": [3756.354], "images_per_second": 0.266, "prompt_tokens": 19, "response_tokens_est": 66, "n_tiles": 12, "output_text": "The image features a laptop with a green snake graphic on its screen, a black camera, a black smartphone, a red and black tennis racket, and a book titled \"The Men Who Ruled India\" by Philip Mason. The lighting is bright, and the overall setting appears to be indoors with a carpeted floor.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.4, "ram_available_mb": 100176.8, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25595.4, "ram_available_mb": 100176.8, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.08, "peak": 15.65, "min": 14.43}, "VDD_GPU": {"avg": 32.79, "peak": 42.54, "min": 26.0}, "VIN": {"avg": 77.55, "peak": 144.53, "min": 58.58}, "VDD_CPU_SOC_MSS": {"avg": 15.67, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 32.79, "energy_joules_est": 123.18, "sample_count": 29, "duration_seconds": 3.757}, "timestamp": "2026-01-16T16:18:55.892158"}
{"image_index": 330, "image_name": "000000035326.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000035326.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2483.668, "latencies_ms": [2483.668], "images_per_second": 0.403, "prompt_tokens": 9, "response_tokens_est": 32, "n_tiles": 12, "output_text": "The image shows a kitchen countertop with a black stove top, a black oven door, and various kitchen utensils and containers on the countertop.", "error": null, "sys_before": {"cpu_percent": 43.7, "ram_used_mb": 25595.4, "ram_available_mb": 100176.8, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 25595.6, "ram_available_mb": 100176.5, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.16, "peak": 16.15, "min": 13.9}, "VDD_GPU": {"avg": 35.19, "peak": 41.36, "min": 27.57}, "VIN": {"avg": 79.0, "peak": 122.21, "min": 58.7}, "VDD_CPU_SOC_MSS": {"avg": 16.07, "peak": 17.32, "min": 14.96}}, "power_watts_avg": 35.19, "energy_joules_est": 87.41, "sample_count": 18, "duration_seconds": 2.484}, "timestamp": "2026-01-16T16:18:58.463296"}
{"image_index": 330, "image_name": "000000035326.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000035326.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 3877.486, "latencies_ms": [3877.486], "images_per_second": 0.258, "prompt_tokens": 23, "response_tokens_est": 68, "n_tiles": 12, "output_text": "- stove: 1\n- oven: 1\n- oven mitt: 1\n- oven rack: 1\n- oven door: 1\n- oven handle: 1\n- oven knob: 1\n- oven pan: 1\n- oven tray: 1\n- oven drawer: 1", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25595.6, "ram_available_mb": 100176.5, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25595.6, "ram_available_mb": 100176.5, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.89, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 32.57, "peak": 42.54, "min": 25.6}, "VIN": {"avg": 73.99, "peak": 121.53, "min": 59.05}, "VDD_CPU_SOC_MSS": {"avg": 15.5, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 32.57, "energy_joules_est": 126.3, "sample_count": 29, "duration_seconds": 3.878}, "timestamp": "2026-01-16T16:19:02.347179"}
{"image_index": 330, "image_name": "000000035326.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000035326.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 4678.253, "latencies_ms": [4678.253], "images_per_second": 0.214, "prompt_tokens": 27, "response_tokens_est": 92, "n_tiles": 12, "output_text": "The main objects in the image are a kitchen countertop, a stove, and various kitchen utensils. The countertop is located in the foreground, while the stove is situated in the middle ground. The kitchen utensils are placed near the stove, with one knife and a pair of tongs on the countertop and a spoon on the stove. The background features a backsplash with marble tiles, and a black oven door is partially visible.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 25595.6, "ram_available_mb": 100176.5, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 25595.4, "ram_available_mb": 100176.8, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.74, "peak": 15.34, "min": 14.22}, "VDD_GPU": {"avg": 31.31, "peak": 42.54, "min": 25.6}, "VIN": {"avg": 72.32, "peak": 108.86, "min": 58.64}, "VDD_CPU_SOC_MSS": {"avg": 15.17, "peak": 16.14, "min": 14.56}}, "power_watts_avg": 31.31, "energy_joules_est": 146.5, "sample_count": 37, "duration_seconds": 4.679}, "timestamp": "2026-01-16T16:19:07.032508"}
{"image_index": 330, "image_name": "000000035326.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000035326.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3782.888, "latencies_ms": [3782.888], "images_per_second": 0.264, "prompt_tokens": 21, "response_tokens_est": 66, "n_tiles": 12, "output_text": "The image depicts a kitchen countertop with a black stove and a black oven. The countertop is adorned with various kitchen items, including a black trash can, a black oven mitt, and a black oven rack. The setting appears to be a well-organized kitchen with a focus on functionality and cleanliness.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25595.4, "ram_available_mb": 100176.8, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25595.4, "ram_available_mb": 100176.8, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.75, "peak": 15.34, "min": 14.12}, "VDD_GPU": {"avg": 32.5, "peak": 42.15, "min": 26.0}, "VIN": {"avg": 74.48, "peak": 126.77, "min": 58.73}, "VDD_CPU_SOC_MSS": {"avg": 15.31, "peak": 16.14, "min": 14.56}}, "power_watts_avg": 32.5, "energy_joules_est": 122.96, "sample_count": 29, "duration_seconds": 3.783}, "timestamp": "2026-01-16T16:19:10.822159"}
{"image_index": 330, "image_name": "000000035326.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000035326.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3782.083, "latencies_ms": [3782.083], "images_per_second": 0.264, "prompt_tokens": 19, "response_tokens_est": 66, "n_tiles": 12, "output_text": "The image shows a kitchen counter with a dark countertop and a black stove. The counter is adorned with various kitchen items, including a black trash can, a black oven, and a black stove with white cooking rings. The lighting in the kitchen is dim, and the overall atmosphere appears to be cozy and lived-in.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.4, "ram_available_mb": 100176.8, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25595.1, "ram_available_mb": 100177.1, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.72, "peak": 15.34, "min": 14.12}, "VDD_GPU": {"avg": 32.46, "peak": 42.54, "min": 25.6}, "VIN": {"avg": 70.57, "peak": 121.0, "min": 54.66}, "VDD_CPU_SOC_MSS": {"avg": 15.27, "peak": 16.14, "min": 14.56}}, "power_watts_avg": 32.46, "energy_joules_est": 122.78, "sample_count": 30, "duration_seconds": 3.783}, "timestamp": "2026-01-16T16:19:14.611174"}
{"image_index": 331, "image_name": "000000035682.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000035682.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2850.965, "latencies_ms": [2850.965], "images_per_second": 0.351, "prompt_tokens": 9, "response_tokens_est": 43, "n_tiles": 12, "output_text": "A young man is sitting at a table in a restaurant, holding a donut with pink icing and colorful sprinkles, while a tray of donuts and a cup of coffee are visible in the background.", "error": null, "sys_before": {"cpu_percent": 47.2, "ram_used_mb": 25595.1, "ram_available_mb": 100177.1, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25594.9, "ram_available_mb": 100177.3, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.09, "peak": 16.05, "min": 13.59}, "VDD_GPU": {"avg": 33.89, "peak": 41.36, "min": 26.79}, "VIN": {"avg": 73.49, "peak": 109.59, "min": 58.08}, "VDD_CPU_SOC_MSS": {"avg": 15.89, "peak": 17.32, "min": 14.57}}, "power_watts_avg": 33.89, "energy_joules_est": 96.63, "sample_count": 22, "duration_seconds": 2.851}, "timestamp": "2026-01-16T16:19:17.552532"}
{"image_index": 331, "image_name": "000000035682.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000035682.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 4218.139, "latencies_ms": [4218.139], "images_per_second": 0.237, "prompt_tokens": 23, "response_tokens_est": 80, "n_tiles": 12, "output_text": "- donut: 3\n- cup: 1\n- plate: 1\n- tray: 1\n- napkin: 1\n- donut with frosting: 1\n- donut with sprinkles: 1\n- donut with chocolate: 1\n- donut with lemon: 1\n- donut with strawberry: 1", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25594.9, "ram_available_mb": 100177.3, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 25594.9, "ram_available_mb": 100177.3, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.12, "peak": 15.65, "min": 14.63}, "VDD_GPU": {"avg": 32.15, "peak": 42.94, "min": 26.0}, "VIN": {"avg": 71.87, "peak": 116.4, "min": 58.69}, "VDD_CPU_SOC_MSS": {"avg": 15.72, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 32.15, "energy_joules_est": 135.63, "sample_count": 32, "duration_seconds": 4.219}, "timestamp": "2026-01-16T16:19:21.777436"}
{"image_index": 331, "image_name": "000000035682.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000035682.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 4801.541, "latencies_ms": [4801.541], "images_per_second": 0.208, "prompt_tokens": 27, "response_tokens_est": 97, "n_tiles": 12, "output_text": "The main object in the foreground is a plate of donuts, which are placed on a tray. The plate is positioned near the center of the image, with the donuts arranged in a visually appealing manner. In the background, there is a person seated at a counter, and a table with various items, including a cup of coffee and a tablet. The person is slightly out of focus, indicating that the main focus is on the donuts and the tray they are on.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25594.9, "ram_available_mb": 100177.3, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25594.9, "ram_available_mb": 100177.3, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.12, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 31.41, "peak": 42.54, "min": 26.39}, "VIN": {"avg": 72.29, "peak": 111.6, "min": 49.91}, "VDD_CPU_SOC_MSS": {"avg": 15.67, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 31.41, "energy_joules_est": 150.83, "sample_count": 38, "duration_seconds": 4.802}, "timestamp": "2026-01-16T16:19:26.585905"}
{"image_index": 331, "image_name": "000000035682.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000035682.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3846.436, "latencies_ms": [3846.436], "images_per_second": 0.26, "prompt_tokens": 21, "response_tokens_est": 69, "n_tiles": 12, "output_text": "The image depicts a cozy and inviting setting, likely inside a caf\u00e9 or restaurant. The scene is filled with various items on a table, including a plate of donuts, a cup of coffee, and a tray with a tray of donuts. The background shows a person sitting at a table, possibly enjoying their meal or coffee.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25594.9, "ram_available_mb": 100177.3, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25594.9, "ram_available_mb": 100177.3, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.1, "peak": 15.65, "min": 14.63}, "VDD_GPU": {"avg": 32.64, "peak": 42.94, "min": 26.39}, "VIN": {"avg": 74.73, "peak": 113.99, "min": 58.81}, "VDD_CPU_SOC_MSS": {"avg": 15.71, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 32.64, "energy_joules_est": 125.56, "sample_count": 30, "duration_seconds": 3.847}, "timestamp": "2026-01-16T16:19:30.439709"}
{"image_index": 331, "image_name": "000000035682.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000035682.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 4019.311, "latencies_ms": [4019.311], "images_per_second": 0.249, "prompt_tokens": 19, "response_tokens_est": 74, "n_tiles": 12, "output_text": "The image features a warmly lit indoor setting with a focus on a tray of donuts. The donuts are adorned with various colorful sprinkles and glazed in shades of pink and brown. The tray is placed on a table, and the background includes a blurred figure of a person and some indistinct items, creating a cozy and inviting atmosphere.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25594.9, "ram_available_mb": 100177.3, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25594.9, "ram_available_mb": 100177.3, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.1, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 32.43, "peak": 42.53, "min": 26.39}, "VIN": {"avg": 74.37, "peak": 118.81, "min": 55.63}, "VDD_CPU_SOC_MSS": {"avg": 15.73, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 32.43, "energy_joules_est": 130.36, "sample_count": 31, "duration_seconds": 4.02}, "timestamp": "2026-01-16T16:19:34.469648"}
{"image_index": 332, "image_name": "000000035770.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000035770.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2140.337, "latencies_ms": [2140.337], "images_per_second": 0.467, "prompt_tokens": 9, "response_tokens_est": 22, "n_tiles": 12, "output_text": "The image shows a bathroom sink with a decorative plate on the side, featuring a cat and a turtle.", "error": null, "sys_before": {"cpu_percent": 44.4, "ram_used_mb": 25594.9, "ram_available_mb": 100177.3, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 2.4, "ram_used_mb": 25594.9, "ram_available_mb": 100177.3, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.12, "peak": 16.15, "min": 13.69}, "VDD_GPU": {"avg": 36.16, "peak": 41.36, "min": 29.15}, "VIN": {"avg": 77.11, "peak": 113.84, "min": 58.05}, "VDD_CPU_SOC_MSS": {"avg": 15.97, "peak": 16.92, "min": 14.96}}, "power_watts_avg": 36.16, "energy_joules_est": 77.41, "sample_count": 16, "duration_seconds": 2.141}, "timestamp": "2026-01-16T16:19:36.706490"}
{"image_index": 332, "image_name": "000000035770.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000035770.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 3256.312, "latencies_ms": [3256.312], "images_per_second": 0.307, "prompt_tokens": 23, "response_tokens_est": 52, "n_tiles": 12, "output_text": "1. Sink\n2. Faucet\n3. Faucet handle\n4. Faucet base\n5. Faucet base\n6. Faucet base\n7. Faucet base\n8. Faucet base", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25594.9, "ram_available_mb": 100177.3, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25594.9, "ram_available_mb": 100177.3, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.16, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 34.03, "peak": 42.94, "min": 26.79}, "VIN": {"avg": 79.33, "peak": 147.23, "min": 58.91}, "VDD_CPU_SOC_MSS": {"avg": 15.79, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 34.03, "energy_joules_est": 110.83, "sample_count": 25, "duration_seconds": 3.257}, "timestamp": "2026-01-16T16:19:39.969332"}
{"image_index": 332, "image_name": "000000035770.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000035770.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 4166.961, "latencies_ms": [4166.961], "images_per_second": 0.24, "prompt_tokens": 27, "response_tokens_est": 78, "n_tiles": 12, "output_text": "The main objects in the image are a white bathtub and a decorative plate. The bathtub is positioned in the foreground, with a clear glass panel on its side. The decorative plate is placed near the bathtub, with its design featuring black and white shapes. The background includes a toilet and a tiled wall, while the floor is not visible in the image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25594.9, "ram_available_mb": 100177.3, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25594.8, "ram_available_mb": 100177.3, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.1, "peak": 15.75, "min": 14.53}, "VDD_GPU": {"avg": 32.11, "peak": 42.54, "min": 26.0}, "VIN": {"avg": 71.95, "peak": 116.19, "min": 57.66}, "VDD_CPU_SOC_MSS": {"avg": 15.58, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 32.11, "energy_joules_est": 133.81, "sample_count": 33, "duration_seconds": 4.167}, "timestamp": "2026-01-16T16:19:44.142565"}
{"image_index": 332, "image_name": "000000035770.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000035770.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 4025.057, "latencies_ms": [4025.057], "images_per_second": 0.248, "prompt_tokens": 21, "response_tokens_est": 74, "n_tiles": 12, "output_text": "The image depicts a bathroom setting with a white bathtub and a glass shower enclosure. The bathtub is positioned against a wall with a tiled backsplash, and there is a small decorative plate with a turtle design on the left side of the bathtub. The overall scene appears to be a modern bathroom with a clean and minimalist design.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25594.8, "ram_available_mb": 100177.3, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25594.8, "ram_available_mb": 100177.3, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.08, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 32.31, "peak": 42.94, "min": 26.0}, "VIN": {"avg": 73.47, "peak": 121.7, "min": 58.59}, "VDD_CPU_SOC_MSS": {"avg": 15.61, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 32.31, "energy_joules_est": 130.07, "sample_count": 31, "duration_seconds": 4.026}, "timestamp": "2026-01-16T16:19:48.175051"}
{"image_index": 332, "image_name": "000000035770.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000035770.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3381.862, "latencies_ms": [3381.862], "images_per_second": 0.296, "prompt_tokens": 19, "response_tokens_est": 55, "n_tiles": 12, "output_text": "The image features a bathroom with a white bathtub and a white sink. The bathtub has a blue tiled surround, and the sink has a clear glass top. The lighting is bright, and the overall color scheme is neutral with white and blue tones.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25594.8, "ram_available_mb": 100177.3, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25594.6, "ram_available_mb": 100177.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.09, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 33.57, "peak": 42.94, "min": 26.39}, "VIN": {"avg": 78.07, "peak": 128.08, "min": 49.38}, "VDD_CPU_SOC_MSS": {"avg": 15.64, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 33.57, "energy_joules_est": 113.55, "sample_count": 26, "duration_seconds": 3.382}, "timestamp": "2026-01-16T16:19:51.567902"}
{"image_index": 333, "image_name": "000000035963.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000035963.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 3318.527, "latencies_ms": [3318.527], "images_per_second": 0.301, "prompt_tokens": 9, "response_tokens_est": 57, "n_tiles": 12, "output_text": "A small, white teddy bear is sitting on a wooden sign that reads \"Joseph Panis\" and \"Marilyn,\" with a sign that says \"Died 1-1-53\" and \"Died 4-11-98\" in the background.", "error": null, "sys_before": {"cpu_percent": 41.8, "ram_used_mb": 25594.6, "ram_available_mb": 100177.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25594.6, "ram_available_mb": 100177.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.14, "peak": 16.05, "min": 13.79}, "VDD_GPU": {"avg": 32.71, "peak": 41.36, "min": 26.39}, "VIN": {"avg": 75.72, "peak": 114.58, "min": 59.78}, "VDD_CPU_SOC_MSS": {"avg": 15.97, "peak": 17.32, "min": 14.96}}, "power_watts_avg": 32.71, "energy_joules_est": 108.56, "sample_count": 25, "duration_seconds": 3.319}, "timestamp": "2026-01-16T16:19:54.979128"}
{"image_index": 333, "image_name": "000000035963.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000035963.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2878.313, "latencies_ms": [2878.313], "images_per_second": 0.347, "prompt_tokens": 23, "response_tokens_est": 40, "n_tiles": 12, "output_text": "object: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25594.6, "ram_available_mb": 100177.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 25594.6, "ram_available_mb": 100177.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.09, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 34.97, "peak": 42.54, "min": 26.79}, "VIN": {"avg": 73.77, "peak": 114.75, "min": 58.52}, "VDD_CPU_SOC_MSS": {"avg": 15.71, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 34.97, "energy_joules_est": 100.67, "sample_count": 22, "duration_seconds": 2.879}, "timestamp": "2026-01-16T16:19:57.863948"}
{"image_index": 333, "image_name": "000000035963.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000035963.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 5946.287, "latencies_ms": [5946.287], "images_per_second": 0.168, "prompt_tokens": 27, "response_tokens_est": 129, "n_tiles": 12, "output_text": "The main object in the foreground is a large, white teddy bear with a wooden sign that reads \"Joseph Panis\" and \"Mystie Malulani.\" The bear is positioned on the left side of the image, leaning against the sign. In the background, there is a wooden sign with the text \"DIED 1-1-53\" and \"4-11-98,\" indicating a memorial or memorial site. The sign is placed on the right side of the image, near the bear. The bear and the sign are both situated on a grassy area with some red bricks and dirt, creating a contrast between", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25594.6, "ram_available_mb": 100177.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 25594.6, "ram_available_mb": 100177.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.87, "peak": 15.65, "min": 14.43}, "VDD_GPU": {"avg": 30.1, "peak": 42.53, "min": 25.6}, "VIN": {"avg": 70.76, "peak": 122.47, "min": 58.14}, "VDD_CPU_SOC_MSS": {"avg": 15.5, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 30.1, "energy_joules_est": 178.99, "sample_count": 47, "duration_seconds": 5.947}, "timestamp": "2026-01-16T16:20:03.816449"}
{"image_index": 333, "image_name": "000000035963.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000035963.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 4471.064, "latencies_ms": [4471.064], "images_per_second": 0.224, "prompt_tokens": 21, "response_tokens_est": 86, "n_tiles": 12, "output_text": "The image depicts a serene outdoor scene with a small, weathered wooden sign reading \"Joseph Panis\" and a white teddy bear sitting on top of it. The teddy bear is positioned on a patch of grass, with a few small red bricks and a few green plants in the background. The setting appears to be a garden or park, with a paved path and a building visible in the distance.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25594.6, "ram_available_mb": 100177.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 25595.1, "ram_available_mb": 100177.1, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.74, "peak": 15.34, "min": 14.12}, "VDD_GPU": {"avg": 31.39, "peak": 42.53, "min": 25.6}, "VIN": {"avg": 70.42, "peak": 119.8, "min": 55.69}, "VDD_CPU_SOC_MSS": {"avg": 15.39, "peak": 16.14, "min": 14.96}}, "power_watts_avg": 31.39, "energy_joules_est": 140.36, "sample_count": 35, "duration_seconds": 4.471}, "timestamp": "2026-01-16T16:20:08.294023"}
{"image_index": 333, "image_name": "000000035963.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000035963.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3782.284, "latencies_ms": [3782.284], "images_per_second": 0.264, "prompt_tokens": 19, "response_tokens_est": 66, "n_tiles": 12, "output_text": "The notable visual attributes of the image include a white teddy bear with a distressed appearance, placed on a weathered wooden sign that reads \"Joseph Panis\" and \"Marilyn.\" The setting is outdoors with a mix of greenery and a dirt path, and the lighting is natural, suggesting it is daytime.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.1, "ram_available_mb": 100177.1, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25594.8, "ram_available_mb": 100177.4, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.73, "peak": 15.34, "min": 14.12}, "VDD_GPU": {"avg": 32.63, "peak": 42.54, "min": 25.61}, "VIN": {"avg": 72.57, "peak": 122.42, "min": 58.1}, "VDD_CPU_SOC_MSS": {"avg": 15.31, "peak": 16.14, "min": 14.96}}, "power_watts_avg": 32.63, "energy_joules_est": 123.44, "sample_count": 29, "duration_seconds": 3.783}, "timestamp": "2026-01-16T16:20:12.087212"}
{"image_index": 334, "image_name": "000000036494.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000036494.jpg", "image_width": 640, "image_height": 434, "image_resolution": "640x434", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1765.198, "latencies_ms": [1765.198], "images_per_second": 0.567, "prompt_tokens": 9, "response_tokens_est": 44, "n_tiles": 6, "output_text": "The image depicts a bustling restaurant with patrons seated at tables, engaged in conversation and enjoying their meals, while a large clock hangs prominently in the background, adding a touch of elegance to the space.", "error": null, "sys_before": {"cpu_percent": 39.2, "ram_used_mb": 25594.8, "ram_available_mb": 100177.4, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 25594.8, "ram_available_mb": 100177.4, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.48, "peak": 15.11, "min": 13.82}, "VDD_GPU": {"avg": 30.66, "peak": 37.82, "min": 24.82}, "VIN": {"avg": 73.7, "peak": 124.62, "min": 54.65}, "VDD_CPU_SOC_MSS": {"avg": 15.17, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 30.66, "energy_joules_est": 54.13, "sample_count": 13, "duration_seconds": 1.766}, "timestamp": "2026-01-16T16:20:13.911022"}
{"image_index": 334, "image_name": "000000036494.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000036494.jpg", "image_width": 640, "image_height": 434, "image_resolution": "640x434", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 4056.268, "latencies_ms": [4056.268], "images_per_second": 0.247, "prompt_tokens": 23, "response_tokens_est": 128, "n_tiles": 6, "output_text": "object: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject:", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25594.8, "ram_available_mb": 100177.4, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 25594.8, "ram_available_mb": 100177.4, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.11, "peak": 15.32, "min": 14.32}, "VDD_GPU": {"avg": 26.86, "peak": 38.21, "min": 23.64}, "VIN": {"avg": 67.5, "peak": 106.2, "min": 56.11}, "VDD_CPU_SOC_MSS": {"avg": 15.61, "peak": 16.14, "min": 14.96}}, "power_watts_avg": 26.86, "energy_joules_est": 108.96, "sample_count": 31, "duration_seconds": 4.057}, "timestamp": "2026-01-16T16:20:17.973595"}
{"image_index": 334, "image_name": "000000036494.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000036494.jpg", "image_width": 640, "image_height": 434, "image_resolution": "640x434", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2713.299, "latencies_ms": [2713.299], "images_per_second": 0.369, "prompt_tokens": 27, "response_tokens_est": 78, "n_tiles": 6, "output_text": "The main objects in the image are the dining tables and chairs, which are arranged in a semi-circle around the central area. The tables are positioned in the foreground, with people seated at them. The background features the large clock, which is mounted on the wall and serves as a focal point. The clock is positioned near the ceiling, adding to the overall architectural design of the space.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25594.8, "ram_available_mb": 100177.4, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 25595.1, "ram_available_mb": 100177.1, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.04, "peak": 15.32, "min": 14.32}, "VDD_GPU": {"avg": 28.27, "peak": 38.21, "min": 23.64}, "VIN": {"avg": 68.85, "peak": 111.51, "min": 56.0}, "VDD_CPU_SOC_MSS": {"avg": 15.58, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 28.27, "energy_joules_est": 76.71, "sample_count": 21, "duration_seconds": 2.714}, "timestamp": "2026-01-16T16:20:20.693092"}
{"image_index": 334, "image_name": "000000036494.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000036494.jpg", "image_width": 640, "image_height": 434, "image_resolution": "640x434", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2258.456, "latencies_ms": [2258.456], "images_per_second": 0.443, "prompt_tokens": 21, "response_tokens_est": 61, "n_tiles": 6, "output_text": "The image depicts a bustling restaurant interior with a large clock prominently displayed on the wall. The scene is filled with patrons seated at tables, engaged in conversations and dining. The warm lighting and rustic decor create a cozy atmosphere, while the clock adds a touch of elegance to the setting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.1, "ram_available_mb": 100177.1, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 25595.3, "ram_available_mb": 100176.9, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.95, "peak": 15.32, "min": 14.32}, "VDD_GPU": {"avg": 29.49, "peak": 38.19, "min": 24.03}, "VIN": {"avg": 70.77, "peak": 111.28, "min": 58.58}, "VDD_CPU_SOC_MSS": {"avg": 15.52, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 29.49, "energy_joules_est": 66.62, "sample_count": 17, "duration_seconds": 2.259}, "timestamp": "2026-01-16T16:20:22.957544"}
{"image_index": 334, "image_name": "000000036494.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000036494.jpg", "image_width": 640, "image_height": 434, "image_resolution": "640x434", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1991.071, "latencies_ms": [1991.071], "images_per_second": 0.502, "prompt_tokens": 19, "response_tokens_est": 51, "n_tiles": 6, "output_text": "The image depicts a bustling restaurant with a warm, inviting ambiance. The lighting is soft and warm, creating a cozy atmosphere. The color palette includes earthy tones, with wooden furniture and a large clock adding a touch of vintage charm.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.3, "ram_available_mb": 100176.9, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 25595.3, "ram_available_mb": 100176.9, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.9, "peak": 15.22, "min": 14.32}, "VDD_GPU": {"avg": 30.31, "peak": 38.21, "min": 24.42}, "VIN": {"avg": 72.16, "peak": 115.87, "min": 56.94}, "VDD_CPU_SOC_MSS": {"avg": 15.48, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 30.31, "energy_joules_est": 60.36, "sample_count": 15, "duration_seconds": 1.992}, "timestamp": "2026-01-16T16:20:24.954571"}
{"image_index": 335, "image_name": "000000036539.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000036539.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1046.058, "latencies_ms": [1046.058], "images_per_second": 0.956, "prompt_tokens": 9, "response_tokens_est": 17, "n_tiles": 6, "output_text": "A person is skiing down a snowy slope with a child following behind.", "error": null, "sys_before": {"cpu_percent": 47.1, "ram_used_mb": 25595.3, "ram_available_mb": 100176.9, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 25595.5, "ram_available_mb": 100176.6, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.32, "peak": 14.51, "min": 13.92}, "VDD_GPU": {"avg": 33.82, "peak": 37.03, "min": 29.94}, "VIN": {"avg": 62.1, "peak": 75.82, "min": 55.04}, "VDD_CPU_SOC_MSS": {"avg": 15.07, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 33.82, "energy_joules_est": 35.39, "sample_count": 7, "duration_seconds": 1.047}, "timestamp": "2026-01-16T16:20:26.056702"}
{"image_index": 335, "image_name": "000000036539.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000036539.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1677.625, "latencies_ms": [1677.625], "images_per_second": 0.596, "prompt_tokens": 23, "response_tokens_est": 38, "n_tiles": 6, "output_text": "1. Person\n2. Ski poles\n3. Ski\n4. Snowboard\n5. Snow\n6. Snowboarder\n7. Ski jacket\n8. Ski pants", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.5, "ram_available_mb": 100176.6, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 25595.3, "ram_available_mb": 100176.9, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.58, "peak": 15.01, "min": 13.92}, "VDD_GPU": {"avg": 32.08, "peak": 39.38, "min": 24.42}, "VIN": {"avg": 73.28, "peak": 104.47, "min": 62.01}, "VDD_CPU_SOC_MSS": {"avg": 15.11, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 32.08, "energy_joules_est": 53.83, "sample_count": 13, "duration_seconds": 1.678}, "timestamp": "2026-01-16T16:20:27.740514"}
{"image_index": 335, "image_name": "000000036539.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000036539.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3608.787, "latencies_ms": [3608.787], "images_per_second": 0.277, "prompt_tokens": 27, "response_tokens_est": 107, "n_tiles": 6, "output_text": "In the image, the main objects are the two individuals and the snowy landscape. The person on the left is standing near the snow, wearing a black jacket and black pants, and is holding a pair of skis. The person on the right is also standing near the snow, wearing a black jacket and black pants, and is holding a pair of skis. The background features a snowy slope with trees and a clear sky. The foreground is dominated by the snowy landscape, while the background is slightly blurred.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25595.3, "ram_available_mb": 100176.9, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 25595.3, "ram_available_mb": 100176.9, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.87, "peak": 15.11, "min": 14.02}, "VDD_GPU": {"avg": 27.01, "peak": 38.59, "min": 23.24}, "VIN": {"avg": 67.9, "peak": 120.06, "min": 58.42}, "VDD_CPU_SOC_MSS": {"avg": 15.5, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 27.01, "energy_joules_est": 97.49, "sample_count": 28, "duration_seconds": 3.609}, "timestamp": "2026-01-16T16:20:31.355682"}
{"image_index": 335, "image_name": "000000036539.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000036539.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2551.782, "latencies_ms": [2551.782], "images_per_second": 0.392, "prompt_tokens": 21, "response_tokens_est": 69, "n_tiles": 6, "output_text": "The image depicts a snowy mountain slope with a family of three enjoying a winter day. The family is dressed in winter gear, including jackets, pants, and snow boots, and are engaged in skiing activities. The setting is a snowy mountain, and the family appears to be having a fun and active day on the slopes.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.3, "ram_available_mb": 100176.9, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 25595.3, "ram_available_mb": 100176.9, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.8, "peak": 15.01, "min": 14.12}, "VDD_GPU": {"avg": 28.26, "peak": 37.8, "min": 23.24}, "VIN": {"avg": 68.11, "peak": 104.16, "min": 58.62}, "VDD_CPU_SOC_MSS": {"avg": 15.39, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 28.26, "energy_joules_est": 72.13, "sample_count": 20, "duration_seconds": 2.552}, "timestamp": "2026-01-16T16:20:33.913533"}
{"image_index": 335, "image_name": "000000036539.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000036539.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2238.083, "latencies_ms": [2238.083], "images_per_second": 0.447, "prompt_tokens": 19, "response_tokens_est": 58, "n_tiles": 6, "output_text": "The image depicts a snowy landscape with a bright, sunny day. The snow is pristine and untouched, with a few scattered rocks and trees in the background. The sunlight casts sharp shadows, highlighting the texture of the snow and the details of the skiers' gear.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.3, "ram_available_mb": 100176.9, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 25595.5, "ram_available_mb": 100176.7, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.73, "peak": 15.01, "min": 14.02}, "VDD_GPU": {"avg": 29.29, "peak": 38.21, "min": 23.24}, "VIN": {"avg": 71.06, "peak": 116.81, "min": 55.55}, "VDD_CPU_SOC_MSS": {"avg": 15.4, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 29.29, "energy_joules_est": 65.57, "sample_count": 17, "duration_seconds": 2.239}, "timestamp": "2026-01-16T16:20:36.157581"}
{"image_index": 336, "image_name": "000000036660.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000036660.jpg", "image_width": 500, "image_height": 323, "image_resolution": "500x323", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1389.512, "latencies_ms": [1389.512], "images_per_second": 0.72, "prompt_tokens": 9, "response_tokens_est": 30, "n_tiles": 6, "output_text": "The image shows a pair of feet wearing black flip-flops, with a small electronic device and a black car partially visible on a wooden surface.", "error": null, "sys_before": {"cpu_percent": 37.3, "ram_used_mb": 25595.5, "ram_available_mb": 100176.7, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25595.5, "ram_available_mb": 100176.7, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.36, "peak": 14.91, "min": 13.82}, "VDD_GPU": {"avg": 31.86, "peak": 36.62, "min": 26.39}, "VIN": {"avg": 73.96, "peak": 96.75, "min": 62.07}, "VDD_CPU_SOC_MSS": {"avg": 15.15, "peak": 15.74, "min": 14.56}}, "power_watts_avg": 31.86, "energy_joules_est": 44.29, "sample_count": 10, "duration_seconds": 1.39}, "timestamp": "2026-01-16T16:20:37.607171"}
{"image_index": 336, "image_name": "000000036660.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000036660.jpg", "image_width": 500, "image_height": 323, "image_resolution": "500x323", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1505.151, "latencies_ms": [1505.151], "images_per_second": 0.664, "prompt_tokens": 23, "response_tokens_est": 33, "n_tiles": 6, "output_text": "1. Flip phone\n2. Battery\n3. Phone\n4. Battery\n5. Phone\n6. Battery\n7. Phone\n8. Battery", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.5, "ram_available_mb": 100176.7, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 25595.5, "ram_available_mb": 100176.7, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.79, "peak": 15.11, "min": 14.22}, "VDD_GPU": {"avg": 32.69, "peak": 38.6, "min": 26.0}, "VIN": {"avg": 72.27, "peak": 116.81, "min": 58.63}, "VDD_CPU_SOC_MSS": {"avg": 15.35, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 32.69, "energy_joules_est": 49.21, "sample_count": 11, "duration_seconds": 1.505}, "timestamp": "2026-01-16T16:20:39.119235"}
{"image_index": 336, "image_name": "000000036660.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000036660.jpg", "image_width": 500, "image_height": 323, "image_resolution": "500x323", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2436.672, "latencies_ms": [2436.672], "images_per_second": 0.41, "prompt_tokens": 27, "response_tokens_est": 65, "n_tiles": 6, "output_text": "The main objects in the image are a black flip phone, a black flip phone, and a black flip phone. The flip phones are positioned in the foreground, with the one on the left slightly closer to the viewer. The flip phones are placed on a wooden surface, with the one on the right being further away.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25595.5, "ram_available_mb": 100176.7, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 25595.8, "ram_available_mb": 100176.4, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.8, "peak": 15.01, "min": 14.12}, "VDD_GPU": {"avg": 28.94, "peak": 38.59, "min": 23.24}, "VIN": {"avg": 68.1, "peak": 107.59, "min": 56.37}, "VDD_CPU_SOC_MSS": {"avg": 15.39, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 28.94, "energy_joules_est": 70.52, "sample_count": 19, "duration_seconds": 2.437}, "timestamp": "2026-01-16T16:20:41.561595"}
{"image_index": 336, "image_name": "000000036660.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000036660.jpg", "image_width": 500, "image_height": 323, "image_resolution": "500x323", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1544.064, "latencies_ms": [1544.064], "images_per_second": 0.648, "prompt_tokens": 21, "response_tokens_est": 33, "n_tiles": 6, "output_text": "The image shows a person's feet resting on a wooden surface, possibly a table or floor. The person is wearing black flip-flops with white soles.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.8, "ram_available_mb": 100176.4, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25595.5, "ram_available_mb": 100176.7, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.61, "peak": 14.91, "min": 14.02}, "VDD_GPU": {"avg": 31.54, "peak": 38.19, "min": 24.82}, "VIN": {"avg": 73.02, "peak": 113.67, "min": 55.81}, "VDD_CPU_SOC_MSS": {"avg": 15.16, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 31.54, "energy_joules_est": 48.71, "sample_count": 12, "duration_seconds": 1.544}, "timestamp": "2026-01-16T16:20:43.112104"}
{"image_index": 336, "image_name": "000000036660.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000036660.jpg", "image_width": 500, "image_height": 323, "image_resolution": "500x323", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2066.531, "latencies_ms": [2066.531], "images_per_second": 0.484, "prompt_tokens": 19, "response_tokens_est": 52, "n_tiles": 6, "output_text": "The image shows a pair of black flip-flops with white painted toenails. The wooden surface on which the flip-flops are placed has a warm, reddish-brown hue, and the lighting is soft and natural, casting gentle shadows.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25595.5, "ram_available_mb": 100176.7, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 25595.5, "ram_available_mb": 100176.7, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.72, "peak": 15.01, "min": 14.12}, "VDD_GPU": {"avg": 30.02, "peak": 38.6, "min": 23.64}, "VIN": {"avg": 70.68, "peak": 108.2, "min": 57.6}, "VDD_CPU_SOC_MSS": {"avg": 15.27, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 30.02, "energy_joules_est": 62.05, "sample_count": 15, "duration_seconds": 2.067}, "timestamp": "2026-01-16T16:20:45.188814"}
{"image_index": 337, "image_name": "000000036678.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000036678.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 976.958, "latencies_ms": [976.958], "images_per_second": 1.024, "prompt_tokens": 9, "response_tokens_est": 36, "n_tiles": 2, "output_text": "The image depicts the iconic Big Ben and the Houses of Parliament, illuminated against a cloudy sky, with a river in the foreground and a boat in the distance.", "error": null, "sys_before": {"cpu_percent": 27.0, "ram_used_mb": 25595.5, "ram_available_mb": 100176.7, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25595.5, "ram_available_mb": 100176.7, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5055.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.61, "peak": 15.01, "min": 14.1}, "VDD_GPU": {"avg": 26.62, "peak": 32.7, "min": 22.86}, "VIN": {"avg": 67.61, "peak": 75.97, "min": 63.59}, "VDD_CPU_SOC_MSS": {"avg": 15.58, "peak": 16.14, "min": 14.96}}, "power_watts_avg": 26.62, "energy_joules_est": 26.02, "sample_count": 7, "duration_seconds": 0.977}, "timestamp": "2026-01-16T16:20:46.196930"}
{"image_index": 337, "image_name": "000000036678.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000036678.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1079.991, "latencies_ms": [1079.991], "images_per_second": 0.926, "prompt_tokens": 23, "response_tokens_est": 40, "n_tiles": 2, "output_text": "object: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.5, "ram_available_mb": 100176.7, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 25595.5, "ram_available_mb": 100176.7, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5064.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.11, "peak": 15.52, "min": 14.61}, "VDD_GPU": {"avg": 25.91, "peak": 32.31, "min": 22.06}, "VIN": {"avg": 66.88, "peak": 93.72, "min": 61.66}, "VDD_CPU_SOC_MSS": {"avg": 15.6, "peak": 15.74, "min": 15.35}}, "power_watts_avg": 25.91, "energy_joules_est": 27.99, "sample_count": 8, "duration_seconds": 1.08}, "timestamp": "2026-01-16T16:20:47.283891"}
{"image_index": 337, "image_name": "000000036678.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000036678.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1353.417, "latencies_ms": [1353.417], "images_per_second": 0.739, "prompt_tokens": 27, "response_tokens_est": 52, "n_tiles": 2, "output_text": "The main objects in the image are a large building with a clock tower and a river with several boats. The building is in the background, while the river and boats are in the foreground. The clock tower is prominent and stands out against the skyline.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.5, "ram_available_mb": 100176.7, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25595.7, "ram_available_mb": 100176.4, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5067.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.12, "peak": 15.32, "min": 14.81}, "VDD_GPU": {"avg": 25.02, "peak": 32.3, "min": 21.28}, "VIN": {"avg": 66.51, "peak": 97.85, "min": 61.59}, "VDD_CPU_SOC_MSS": {"avg": 15.86, "peak": 16.14, "min": 15.35}}, "power_watts_avg": 25.02, "energy_joules_est": 33.87, "sample_count": 10, "duration_seconds": 1.354}, "timestamp": "2026-01-16T16:20:48.642963"}
{"image_index": 337, "image_name": "000000036678.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000036678.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1825.286, "latencies_ms": [1825.286], "images_per_second": 0.548, "prompt_tokens": 21, "response_tokens_est": 73, "n_tiles": 2, "output_text": "The image depicts a serene riverside scene with a large, illuminated building, likely the Houses of Parliament, situated on the right side. The sky is overcast, and the water reflects the soft glow of the building's lights. In the foreground, there are several boats, including a small ferry and a larger vessel, moving along the river.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.7, "ram_available_mb": 100176.4, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25595.5, "ram_available_mb": 100176.7, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5063.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.36, "peak": 15.62, "min": 14.81}, "VDD_GPU": {"avg": 23.86, "peak": 32.31, "min": 20.88}, "VIN": {"avg": 64.28, "peak": 76.06, "min": 57.51}, "VDD_CPU_SOC_MSS": {"avg": 16.03, "peak": 16.54, "min": 15.36}}, "power_watts_avg": 23.86, "energy_joules_est": 43.56, "sample_count": 14, "duration_seconds": 1.826}, "timestamp": "2026-01-16T16:20:50.474032"}
{"image_index": 337, "image_name": "000000036678.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000036678.jpg", "image_width": 640, "image_height": 361, "image_resolution": "640x361", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1164.832, "latencies_ms": [1164.832], "images_per_second": 0.858, "prompt_tokens": 19, "response_tokens_est": 44, "n_tiles": 2, "output_text": "The image showcases a grand, illuminated building with a clock tower, likely the Houses of Parliament, situated on a river. The sky is overcast, casting a soft, diffused light over the scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.5, "ram_available_mb": 100176.7, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 25595.5, "ram_available_mb": 100176.7, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5062.6, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.19, "peak": 15.52, "min": 14.81}, "VDD_GPU": {"avg": 25.56, "peak": 31.51, "min": 22.06}, "VIN": {"avg": 63.95, "peak": 75.84, "min": 55.73}, "VDD_CPU_SOC_MSS": {"avg": 15.95, "peak": 16.54, "min": 15.74}}, "power_watts_avg": 25.56, "energy_joules_est": 29.78, "sample_count": 8, "duration_seconds": 1.165}, "timestamp": "2026-01-16T16:20:51.644656"}
{"image_index": 338, "image_name": "000000036844.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000036844.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 3152.903, "latencies_ms": [3152.903], "images_per_second": 0.317, "prompt_tokens": 9, "response_tokens_est": 52, "n_tiles": 12, "output_text": "The image depicts a cozy living room with a mix of modern and vintage furniture, including a green couch, a red armchair, a black leather sofa, a small white coffee table, a black television, a red rug, and a wooden floor.", "error": null, "sys_before": {"cpu_percent": 41.6, "ram_used_mb": 25595.5, "ram_available_mb": 100176.7, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 25595.5, "ram_available_mb": 100176.7, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.18, "peak": 16.15, "min": 14.1}, "VDD_GPU": {"avg": 32.66, "peak": 40.57, "min": 26.39}, "VIN": {"avg": 73.54, "peak": 111.19, "min": 58.4}, "VDD_CPU_SOC_MSS": {"avg": 15.89, "peak": 17.32, "min": 15.35}}, "power_watts_avg": 32.66, "energy_joules_est": 102.98, "sample_count": 24, "duration_seconds": 3.153}, "timestamp": "2026-01-16T16:20:54.877197"}
{"image_index": 338, "image_name": "000000036844.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000036844.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 3184.951, "latencies_ms": [3184.951], "images_per_second": 0.314, "prompt_tokens": 23, "response_tokens_est": 49, "n_tiles": 12, "output_text": "1. Living room\n2. Sofa\n3. Coffee table\n4. TV stand\n5. TV\n6. Floor lamp\n7. Potted plants\n8. Chairs\n9. Floor\n10. Ceiling fan", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.5, "ram_available_mb": 100176.7, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 25595.5, "ram_available_mb": 100176.7, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.08, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 33.98, "peak": 42.53, "min": 26.39}, "VIN": {"avg": 76.81, "peak": 117.78, "min": 58.8}, "VDD_CPU_SOC_MSS": {"avg": 15.63, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 33.98, "energy_joules_est": 108.24, "sample_count": 24, "duration_seconds": 3.185}, "timestamp": "2026-01-16T16:20:58.068407"}
{"image_index": 338, "image_name": "000000036844.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000036844.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 5674.002, "latencies_ms": [5674.002], "images_per_second": 0.176, "prompt_tokens": 27, "response_tokens_est": 120, "n_tiles": 12, "output_text": "The main objects in the image are arranged in a way that showcases a mix of furniture and decor. The left side of the image features a red leather chair, a small round table, and a bicycle leaning against the wall. In the background, there is a green couch, a television on a stand, and a small wooden table with a mirror. The right side of the image includes a black leather couch, a white coffee table, and a white refrigerator. The floor is wooden, and there is a red rug under the couch. The overall arrangement creates a cozy and lived-in atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.5, "ram_available_mb": 100176.7, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 25595.5, "ram_available_mb": 100176.7, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.85, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 30.14, "peak": 42.53, "min": 25.6}, "VIN": {"avg": 73.06, "peak": 145.48, "min": 56.88}, "VDD_CPU_SOC_MSS": {"avg": 15.28, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 30.14, "energy_joules_est": 171.02, "sample_count": 45, "duration_seconds": 5.674}, "timestamp": "2026-01-16T16:21:03.748680"}
{"image_index": 338, "image_name": "000000036844.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000036844.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 4124.613, "latencies_ms": [4124.613], "images_per_second": 0.242, "prompt_tokens": 21, "response_tokens_est": 76, "n_tiles": 12, "output_text": "The image depicts a cozy living room with a wooden floor and a red rug. The room is well-lit with natural light coming through large windows, and it contains various furniture pieces such as a green couch, a black leather chair, a small table, and a television. The space appears to be lived-in and well-organized, with personal items scattered around.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.5, "ram_available_mb": 100176.7, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 25595.5, "ram_available_mb": 100176.7, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.73, "peak": 15.34, "min": 14.12}, "VDD_GPU": {"avg": 31.98, "peak": 42.53, "min": 25.6}, "VIN": {"avg": 73.24, "peak": 120.75, "min": 58.11}, "VDD_CPU_SOC_MSS": {"avg": 15.22, "peak": 16.14, "min": 14.56}}, "power_watts_avg": 31.98, "energy_joules_est": 131.92, "sample_count": 32, "duration_seconds": 4.125}, "timestamp": "2026-01-16T16:21:07.880092"}
{"image_index": 338, "image_name": "000000036844.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000036844.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2887.327, "latencies_ms": [2887.327], "images_per_second": 0.346, "prompt_tokens": 19, "response_tokens_est": 40, "n_tiles": 12, "output_text": "The room is well-lit with natural light streaming in through multiple windows, creating a bright and airy atmosphere. The wooden floor reflects the sunlight, adding to the overall brightness of the space.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.5, "ram_available_mb": 100176.7, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25595.2, "ram_available_mb": 100177.0, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.73, "peak": 15.24, "min": 14.12}, "VDD_GPU": {"avg": 34.7, "peak": 42.13, "min": 26.39}, "VIN": {"avg": 73.92, "peak": 113.35, "min": 58.65}, "VDD_CPU_SOC_MSS": {"avg": 15.32, "peak": 16.14, "min": 14.56}}, "power_watts_avg": 34.7, "energy_joules_est": 100.21, "sample_count": 22, "duration_seconds": 2.888}, "timestamp": "2026-01-16T16:21:10.773885"}
{"image_index": 339, "image_name": "000000036861.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000036861.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2482.581, "latencies_ms": [2482.581], "images_per_second": 0.403, "prompt_tokens": 9, "response_tokens_est": 32, "n_tiles": 12, "output_text": "The image shows a parking meter on a sidewalk, with a large advertisement for \"YEAR OF SAVING\" displayed on a building in the background.", "error": null, "sys_before": {"cpu_percent": 43.9, "ram_used_mb": 25595.2, "ram_available_mb": 100177.0, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 25595.2, "ram_available_mb": 100177.0, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.09, "peak": 16.05, "min": 13.69}, "VDD_GPU": {"avg": 35.04, "peak": 41.36, "min": 27.57}, "VIN": {"avg": 74.4, "peak": 115.74, "min": 55.23}, "VDD_CPU_SOC_MSS": {"avg": 15.95, "peak": 16.92, "min": 14.96}}, "power_watts_avg": 35.04, "energy_joules_est": 87.0, "sample_count": 19, "duration_seconds": 2.483}, "timestamp": "2026-01-16T16:21:13.343281"}
{"image_index": 339, "image_name": "000000036861.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000036861.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 3416.803, "latencies_ms": [3416.803], "images_per_second": 0.293, "prompt_tokens": 23, "response_tokens_est": 56, "n_tiles": 12, "output_text": "object: 1 parking meter\nobject: 1 parking meter\nobject: 1 parking meter\nobject: 1 parking meter\nobject: 1 parking meter\nobject: 1 parking meter\nobject: 1 parking meter\nobject: 1 parking meter", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.2, "ram_available_mb": 100177.0, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 25595.2, "ram_available_mb": 100177.0, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.1, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 33.59, "peak": 42.54, "min": 26.39}, "VIN": {"avg": 73.03, "peak": 110.98, "min": 58.98}, "VDD_CPU_SOC_MSS": {"avg": 15.82, "peak": 16.53, "min": 15.35}}, "power_watts_avg": 33.59, "energy_joules_est": 114.78, "sample_count": 26, "duration_seconds": 3.417}, "timestamp": "2026-01-16T16:21:16.767188"}
{"image_index": 339, "image_name": "000000036861.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000036861.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3759.072, "latencies_ms": [3759.072], "images_per_second": 0.266, "prompt_tokens": 27, "response_tokens_est": 66, "n_tiles": 12, "output_text": "The parking meter is positioned in the foreground, slightly to the left, with a red post supporting it. The background features a building with large windows and a sign that reads \"YEAR OF SAVING.\" The pavement is visible in the foreground, and there are no other significant objects or people in the immediate vicinity.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.2, "ram_available_mb": 100177.0, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 25595.2, "ram_available_mb": 100177.0, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.08, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 32.77, "peak": 42.54, "min": 26.39}, "VIN": {"avg": 73.46, "peak": 110.54, "min": 58.74}, "VDD_CPU_SOC_MSS": {"avg": 15.79, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 32.77, "energy_joules_est": 123.2, "sample_count": 29, "duration_seconds": 3.759}, "timestamp": "2026-01-16T16:21:20.533187"}
{"image_index": 339, "image_name": "000000036861.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000036861.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 4057.156, "latencies_ms": [4057.156], "images_per_second": 0.246, "prompt_tokens": 21, "response_tokens_est": 75, "n_tiles": 12, "output_text": "The image depicts a parking meter situated on a sidewalk in front of a building. The scene is set in an urban environment, likely in a city or town, with a clear sky and a building with large windows in the background. The parking meter is red and black, and it is positioned on the sidewalk, indicating that it is likely used for parking metering.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.2, "ram_available_mb": 100177.0, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25595.2, "ram_available_mb": 100177.0, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.08, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 32.27, "peak": 42.54, "min": 26.39}, "VIN": {"avg": 71.55, "peak": 114.03, "min": 53.48}, "VDD_CPU_SOC_MSS": {"avg": 15.69, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 32.27, "energy_joules_est": 130.94, "sample_count": 32, "duration_seconds": 4.058}, "timestamp": "2026-01-16T16:21:24.596836"}
{"image_index": 339, "image_name": "000000036861.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000036861.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2879.026, "latencies_ms": [2879.026], "images_per_second": 0.347, "prompt_tokens": 19, "response_tokens_est": 40, "n_tiles": 12, "output_text": "The parking meter in the image is predominantly black with a digital display. It is mounted on a red post. The surrounding environment includes a clear blue sky, a brick building, and a sidewalk.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.2, "ram_available_mb": 100177.0, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 25595.4, "ram_available_mb": 100176.8, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.07, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 34.61, "peak": 42.53, "min": 26.79}, "VIN": {"avg": 76.93, "peak": 115.47, "min": 58.14}, "VDD_CPU_SOC_MSS": {"avg": 15.67, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 34.61, "energy_joules_est": 99.66, "sample_count": 22, "duration_seconds": 2.88}, "timestamp": "2026-01-16T16:21:27.482257"}
{"image_index": 340, "image_name": "000000036936.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000036936.jpg", "image_width": 640, "image_height": 404, "image_resolution": "640x404", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1338.908, "latencies_ms": [1338.908], "images_per_second": 0.747, "prompt_tokens": 9, "response_tokens_est": 28, "n_tiles": 6, "output_text": "A group of people is gathered around a table in a cozy living room, with a festive atmosphere, as they enjoy a holiday celebration.", "error": null, "sys_before": {"cpu_percent": 41.7, "ram_used_mb": 25595.2, "ram_available_mb": 100177.0, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25595.2, "ram_available_mb": 100177.0, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.44, "peak": 14.91, "min": 13.92}, "VDD_GPU": {"avg": 32.58, "peak": 38.21, "min": 26.39}, "VIN": {"avg": 70.92, "peak": 90.99, "min": 61.2}, "VDD_CPU_SOC_MSS": {"avg": 15.23, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 32.58, "energy_joules_est": 43.63, "sample_count": 10, "duration_seconds": 1.339}, "timestamp": "2026-01-16T16:21:28.873211"}
{"image_index": 340, "image_name": "000000036936.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000036936.jpg", "image_width": 640, "image_height": 404, "image_resolution": "640x404", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 4054.177, "latencies_ms": [4054.177], "images_per_second": 0.247, "prompt_tokens": 23, "response_tokens_est": 128, "n_tiles": 6, "output_text": "- TV: 1\n- TV stand: 1\n- Candles: 2\n- Candle holder: 1\n- Candle: 1\n- Candlelight: 1\n- Candlelight holder: 1\n- Candlelight: 1\n- Candlelight holder: 1\n- Candlelight: 1\n- Candlelight holder: 1\n- Candlelight: 1\n- Candlelight holder: 1\n- Candlelight: 1\n- Candlelight holder: 1\n- Candlelight: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.2, "ram_available_mb": 100177.0, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 25595.4, "ram_available_mb": 100176.8, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.09, "peak": 15.32, "min": 14.22}, "VDD_GPU": {"avg": 27.01, "peak": 38.6, "min": 23.64}, "VIN": {"avg": 69.6, "peak": 110.73, "min": 61.8}, "VDD_CPU_SOC_MSS": {"avg": 15.61, "peak": 16.14, "min": 14.96}}, "power_watts_avg": 27.01, "energy_joules_est": 109.52, "sample_count": 32, "duration_seconds": 4.055}, "timestamp": "2026-01-16T16:21:32.934753"}
{"image_index": 340, "image_name": "000000036936.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000036936.jpg", "image_width": 640, "image_height": 404, "image_resolution": "640x404", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2394.387, "latencies_ms": [2394.387], "images_per_second": 0.418, "prompt_tokens": 27, "response_tokens_est": 66, "n_tiles": 6, "output_text": "The main objects in the image are a television, a couch, and a table. The television is positioned on the left side of the image, while the couch is in the background. The table is in the foreground, with various items such as a red gift, a candle, a phone, and snacks placed on it.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25595.4, "ram_available_mb": 100176.8, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 25595.4, "ram_available_mb": 100176.8, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.94, "peak": 15.32, "min": 14.22}, "VDD_GPU": {"avg": 29.26, "peak": 38.21, "min": 24.03}, "VIN": {"avg": 73.97, "peak": 111.2, "min": 62.94}, "VDD_CPU_SOC_MSS": {"avg": 15.55, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 29.26, "energy_joules_est": 70.08, "sample_count": 18, "duration_seconds": 2.395}, "timestamp": "2026-01-16T16:21:35.335407"}
{"image_index": 340, "image_name": "000000036936.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000036936.jpg", "image_width": 640, "image_height": 404, "image_resolution": "640x404", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2463.446, "latencies_ms": [2463.446], "images_per_second": 0.406, "prompt_tokens": 21, "response_tokens_est": 69, "n_tiles": 6, "output_text": "The image depicts a cozy living room scene where a group of people is gathered around a table. The room is decorated with festive elements, including a Christmas tree adorned with lights and a red and gold garland. The individuals are engaged in conversation, with one person holding a dog, and the atmosphere suggests a relaxed and festive gathering.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.4, "ram_available_mb": 100176.8, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 25595.4, "ram_available_mb": 100176.8, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.0, "peak": 15.32, "min": 14.32}, "VDD_GPU": {"avg": 28.9, "peak": 37.8, "min": 24.03}, "VIN": {"avg": 69.52, "peak": 106.72, "min": 55.48}, "VDD_CPU_SOC_MSS": {"avg": 15.52, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 28.9, "energy_joules_est": 71.21, "sample_count": 19, "duration_seconds": 2.464}, "timestamp": "2026-01-16T16:21:37.805068"}
{"image_index": 340, "image_name": "000000036936.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000036936.jpg", "image_width": 640, "image_height": 404, "image_resolution": "640x404", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1506.694, "latencies_ms": [1506.694], "images_per_second": 0.664, "prompt_tokens": 19, "response_tokens_est": 33, "n_tiles": 6, "output_text": "The room is warmly lit with a soft glow, creating a cozy atmosphere. The furniture and decorations are made of wood, adding to the homely feel.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.4, "ram_available_mb": 100176.8, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25595.4, "ram_available_mb": 100176.8, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.81, "peak": 15.11, "min": 14.32}, "VDD_GPU": {"avg": 32.44, "peak": 38.19, "min": 26.0}, "VIN": {"avg": 77.15, "peak": 123.96, "min": 64.4}, "VDD_CPU_SOC_MSS": {"avg": 15.53, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 32.44, "energy_joules_est": 48.89, "sample_count": 11, "duration_seconds": 1.507}, "timestamp": "2026-01-16T16:21:39.317598"}
{"image_index": 341, "image_name": "000000037670.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000037670.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2448.97, "latencies_ms": [2448.97], "images_per_second": 0.408, "prompt_tokens": 9, "response_tokens_est": 31, "n_tiles": 12, "output_text": "A person is holding a remote control in their hand, which is connected to a modern, sleek, white appliance with multiple buttons and a digital display.", "error": null, "sys_before": {"cpu_percent": 45.5, "ram_used_mb": 25595.4, "ram_available_mb": 100176.8, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 25595.4, "ram_available_mb": 100176.8, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.14, "peak": 16.15, "min": 13.9}, "VDD_GPU": {"avg": 34.66, "peak": 41.36, "min": 27.18}, "VIN": {"avg": 73.82, "peak": 98.34, "min": 58.24}, "VDD_CPU_SOC_MSS": {"avg": 16.04, "peak": 17.32, "min": 15.35}}, "power_watts_avg": 34.66, "energy_joules_est": 84.9, "sample_count": 19, "duration_seconds": 2.449}, "timestamp": "2026-01-16T16:21:41.855354"}
{"image_index": 341, "image_name": "000000037670.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000037670.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2882.22, "latencies_ms": [2882.22], "images_per_second": 0.347, "prompt_tokens": 23, "response_tokens_est": 40, "n_tiles": 12, "output_text": "1. Remote control\n2. Wall outlet\n3. Wall switch\n4. Wall switch\n5. Wall switch\n6. Wall switch\n7. Wall switch\n8. Wall switch", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.4, "ram_available_mb": 100176.8, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 25595.4, "ram_available_mb": 100176.8, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.08, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 35.07, "peak": 43.31, "min": 26.79}, "VIN": {"avg": 77.15, "peak": 126.69, "min": 58.57}, "VDD_CPU_SOC_MSS": {"avg": 15.71, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 35.07, "energy_joules_est": 101.1, "sample_count": 22, "duration_seconds": 2.883}, "timestamp": "2026-01-16T16:21:44.743944"}
{"image_index": 341, "image_name": "000000037670.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000037670.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 4296.531, "latencies_ms": [4296.531], "images_per_second": 0.233, "prompt_tokens": 27, "response_tokens_est": 82, "n_tiles": 12, "output_text": "The main object in the foreground is a white appliance, possibly a washing machine or dryer, with a control panel on its front. The control panel has several buttons and a display screen. The appliance is placed on a wooden floor, and there is a white door or cabinet to the left of it. The background is mostly white, with a plain wall and a small section of a door or cabinet visible.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.4, "ram_available_mb": 100176.8, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25595.6, "ram_available_mb": 100176.6, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.09, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 31.93, "peak": 42.53, "min": 26.0}, "VIN": {"avg": 72.24, "peak": 96.18, "min": 58.65}, "VDD_CPU_SOC_MSS": {"avg": 15.71, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 31.93, "energy_joules_est": 137.2, "sample_count": 34, "duration_seconds": 4.297}, "timestamp": "2026-01-16T16:21:49.047772"}
{"image_index": 341, "image_name": "000000037670.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000037670.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 4227.78, "latencies_ms": [4227.78], "images_per_second": 0.237, "prompt_tokens": 21, "response_tokens_est": 80, "n_tiles": 12, "output_text": "The image depicts a modern, minimalist bathroom setting with a focus on a sleek, white wall-mounted toilet. A person's hand is seen holding a small, rectangular device, possibly a phone or a remote control, which is being used to operate the toilet. The overall atmosphere is clean, uncluttered, and functional, with a focus on the technological aspect of the bathroom.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.6, "ram_available_mb": 100176.6, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 25595.6, "ram_available_mb": 100176.6, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.09, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 31.98, "peak": 42.53, "min": 26.0}, "VIN": {"avg": 73.86, "peak": 117.2, "min": 58.65}, "VDD_CPU_SOC_MSS": {"avg": 15.74, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 31.98, "energy_joules_est": 135.22, "sample_count": 33, "duration_seconds": 4.228}, "timestamp": "2026-01-16T16:21:53.282559"}
{"image_index": 341, "image_name": "000000037670.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000037670.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3382.761, "latencies_ms": [3382.761], "images_per_second": 0.296, "prompt_tokens": 19, "response_tokens_est": 55, "n_tiles": 12, "output_text": "The image features a modern, minimalist bathroom with a sleek, white toilet and a white wall. The lighting is soft and ambient, creating a clean and serene atmosphere. The materials used include stainless steel for the toilet and a polished, reflective surface for the wall.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.6, "ram_available_mb": 100176.6, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25595.6, "ram_available_mb": 100176.6, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.06, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 33.57, "peak": 42.54, "min": 26.4}, "VIN": {"avg": 73.97, "peak": 117.27, "min": 58.19}, "VDD_CPU_SOC_MSS": {"avg": 15.78, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 33.57, "energy_joules_est": 113.58, "sample_count": 26, "duration_seconds": 3.383}, "timestamp": "2026-01-16T16:21:56.672470"}
{"image_index": 342, "image_name": "000000037689.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000037689.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1282.443, "latencies_ms": [1282.443], "images_per_second": 0.78, "prompt_tokens": 9, "response_tokens_est": 26, "n_tiles": 6, "output_text": "A snowboarder is performing a trick in the air above a snowy slope, with spectators watching from the side.", "error": null, "sys_before": {"cpu_percent": 41.4, "ram_used_mb": 25595.9, "ram_available_mb": 100176.3, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25596.1, "ram_available_mb": 100176.1, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.34, "peak": 14.81, "min": 13.82}, "VDD_GPU": {"avg": 33.0, "peak": 37.82, "min": 27.18}, "VIN": {"avg": 69.28, "peak": 95.85, "min": 59.46}, "VDD_CPU_SOC_MSS": {"avg": 15.13, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 33.0, "energy_joules_est": 42.34, "sample_count": 9, "duration_seconds": 1.283}, "timestamp": "2026-01-16T16:21:58.025121"}
{"image_index": 342, "image_name": "000000037689.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000037689.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1740.85, "latencies_ms": [1740.85], "images_per_second": 0.574, "prompt_tokens": 23, "response_tokens_est": 40, "n_tiles": 6, "output_text": "object: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25596.1, "ram_available_mb": 100176.1, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25595.9, "ram_available_mb": 100176.3, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.59, "peak": 14.91, "min": 14.02}, "VDD_GPU": {"avg": 31.45, "peak": 38.59, "min": 24.42}, "VIN": {"avg": 70.3, "peak": 91.11, "min": 63.04}, "VDD_CPU_SOC_MSS": {"avg": 15.17, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 31.45, "energy_joules_est": 54.76, "sample_count": 13, "duration_seconds": 1.741}, "timestamp": "2026-01-16T16:21:59.772069"}
{"image_index": 342, "image_name": "000000037689.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000037689.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2891.929, "latencies_ms": [2891.929], "images_per_second": 0.346, "prompt_tokens": 27, "response_tokens_est": 81, "n_tiles": 6, "output_text": "The main objects in the image are a snowboarder performing a jump and a snowy mountain slope. The snowboarder is in the foreground, near the bottom of the image, while the snowy mountain slope is in the background, extending towards the top of the image. The snowboarder is near the snowy slope, and the snowy mountain slope is further away from the viewer.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25595.9, "ram_available_mb": 100176.3, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 25595.8, "ram_available_mb": 100176.3, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.81, "peak": 15.11, "min": 14.02}, "VDD_GPU": {"avg": 27.92, "peak": 38.6, "min": 23.24}, "VIN": {"avg": 70.45, "peak": 113.65, "min": 59.24}, "VDD_CPU_SOC_MSS": {"avg": 15.41, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 27.92, "energy_joules_est": 80.75, "sample_count": 22, "duration_seconds": 2.892}, "timestamp": "2026-01-16T16:22:02.670195"}
{"image_index": 342, "image_name": "000000037689.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000037689.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2270.273, "latencies_ms": [2270.273], "images_per_second": 0.44, "prompt_tokens": 21, "response_tokens_est": 59, "n_tiles": 6, "output_text": "The image depicts a snowboarding competition taking place on a snow-covered slope. Spectators are gathered on the side, watching the snowboarders perform tricks. The snowboarders are airborne, executing jumps and tricks, showcasing their skills in a snowy mountainous environment.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.8, "ram_available_mb": 100176.3, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 25595.8, "ram_available_mb": 100176.3, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.74, "peak": 15.01, "min": 14.02}, "VDD_GPU": {"avg": 28.93, "peak": 38.6, "min": 23.25}, "VIN": {"avg": 68.1, "peak": 102.97, "min": 59.91}, "VDD_CPU_SOC_MSS": {"avg": 15.27, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 28.93, "energy_joules_est": 65.7, "sample_count": 18, "duration_seconds": 2.271}, "timestamp": "2026-01-16T16:22:04.946853"}
{"image_index": 342, "image_name": "000000037689.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000037689.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2407.625, "latencies_ms": [2407.625], "images_per_second": 0.415, "prompt_tokens": 19, "response_tokens_est": 64, "n_tiles": 6, "output_text": "The image depicts a snowy landscape with a clear blue sky, indicating a sunny day. The snow appears to be freshly fallen, with a smooth, powdery texture. The snowboarder is captured mid-air, performing a trick, and the snowboarder is wearing a red jacket and black pants.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25595.8, "ram_available_mb": 100176.3, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 25595.8, "ram_available_mb": 100176.3, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.75, "peak": 15.01, "min": 14.02}, "VDD_GPU": {"avg": 28.73, "peak": 37.82, "min": 23.24}, "VIN": {"avg": 70.31, "peak": 109.53, "min": 57.76}, "VDD_CPU_SOC_MSS": {"avg": 15.4, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 28.73, "energy_joules_est": 69.19, "sample_count": 18, "duration_seconds": 2.408}, "timestamp": "2026-01-16T16:22:07.360589"}
{"image_index": 343, "image_name": "000000037740.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000037740.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2586.057, "latencies_ms": [2586.057], "images_per_second": 0.387, "prompt_tokens": 9, "response_tokens_est": 35, "n_tiles": 12, "output_text": "The image shows a well-organized home office with a desk, computer, and a chair, along with a potted plant and a white couch in the background.", "error": null, "sys_before": {"cpu_percent": 47.7, "ram_used_mb": 25595.8, "ram_available_mb": 100176.3, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 25595.8, "ram_available_mb": 100176.3, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.12, "peak": 16.15, "min": 13.79}, "VDD_GPU": {"avg": 34.15, "peak": 40.95, "min": 27.18}, "VIN": {"avg": 74.82, "peak": 109.92, "min": 58.86}, "VDD_CPU_SOC_MSS": {"avg": 15.98, "peak": 17.32, "min": 14.96}}, "power_watts_avg": 34.15, "energy_joules_est": 88.33, "sample_count": 20, "duration_seconds": 2.586}, "timestamp": "2026-01-16T16:22:10.034600"}
{"image_index": 343, "image_name": "000000037740.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000037740.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2685.737, "latencies_ms": [2685.737], "images_per_second": 0.372, "prompt_tokens": 23, "response_tokens_est": 34, "n_tiles": 12, "output_text": "1. Desk\n2. Chair\n3. Computer monitor\n4. Laptop\n5. Keyboard\n6. Mouse\n7. Wall outlet\n8. Floor", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.8, "ram_available_mb": 100176.3, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 25595.8, "ram_available_mb": 100176.3, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.05, "peak": 15.65, "min": 14.43}, "VDD_GPU": {"avg": 35.43, "peak": 42.94, "min": 27.18}, "VIN": {"avg": 74.8, "peak": 113.9, "min": 58.64}, "VDD_CPU_SOC_MSS": {"avg": 15.71, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 35.43, "energy_joules_est": 95.17, "sample_count": 21, "duration_seconds": 2.686}, "timestamp": "2026-01-16T16:22:12.726648"}
{"image_index": 343, "image_name": "000000037740.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000037740.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 4223.471, "latencies_ms": [4223.471], "images_per_second": 0.237, "prompt_tokens": 27, "response_tokens_est": 80, "n_tiles": 12, "output_text": "The main objects in the image are a desk, a chair, and a plant. The desk is positioned in the foreground, with a computer monitor, keyboard, mouse, and laptop on it. The chair is positioned to the left of the desk, and the plant is placed to the right of the desk. The plant is placed on a small white tray, which is placed on the floor.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25595.8, "ram_available_mb": 100176.3, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25595.3, "ram_available_mb": 100176.9, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.1, "peak": 15.75, "min": 14.53}, "VDD_GPU": {"avg": 32.21, "peak": 42.92, "min": 26.39}, "VIN": {"avg": 73.8, "peak": 124.04, "min": 51.64}, "VDD_CPU_SOC_MSS": {"avg": 15.7, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 32.21, "energy_joules_est": 136.06, "sample_count": 32, "duration_seconds": 4.224}, "timestamp": "2026-01-16T16:22:16.958237"}
{"image_index": 343, "image_name": "000000037740.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000037740.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3886.117, "latencies_ms": [3886.117], "images_per_second": 0.257, "prompt_tokens": 21, "response_tokens_est": 70, "n_tiles": 12, "output_text": "The image depicts a cozy, well-lit room with a modern, minimalist aesthetic. The room features a small desk with a computer setup, a comfortable white chair, and a potted plant adding a touch of greenery. The overall atmosphere is serene and inviting, with soft lighting and a clean, uncluttered space.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.3, "ram_available_mb": 100176.9, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25595.3, "ram_available_mb": 100176.9, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.09, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 32.56, "peak": 42.94, "min": 26.0}, "VIN": {"avg": 74.74, "peak": 117.24, "min": 54.53}, "VDD_CPU_SOC_MSS": {"avg": 15.71, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 32.56, "energy_joules_est": 126.56, "sample_count": 30, "duration_seconds": 3.887}, "timestamp": "2026-01-16T16:22:20.851204"}
{"image_index": 343, "image_name": "000000037740.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000037740.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2608.621, "latencies_ms": [2608.621], "images_per_second": 0.383, "prompt_tokens": 19, "response_tokens_est": 32, "n_tiles": 12, "output_text": "The room is well-lit with warm yellow lighting, creating a cozy atmosphere. The carpet is light beige, complementing the overall neutral color scheme.", "error": null, "sys_before": {"cpu_percent": 50.0, "ram_used_mb": 25595.3, "ram_available_mb": 100176.9, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 25595.6, "ram_available_mb": 100176.6, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.09, "peak": 15.65, "min": 14.63}, "VDD_GPU": {"avg": 35.66, "peak": 42.94, "min": 27.18}, "VIN": {"avg": 80.57, "peak": 130.45, "min": 55.79}, "VDD_CPU_SOC_MSS": {"avg": 15.75, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 35.66, "energy_joules_est": 93.05, "sample_count": 20, "duration_seconds": 2.609}, "timestamp": "2026-01-16T16:22:23.466709"}
{"image_index": 344, "image_name": "000000037751.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000037751.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2394.111, "latencies_ms": [2394.111], "images_per_second": 0.418, "prompt_tokens": 9, "response_tokens_est": 29, "n_tiles": 12, "output_text": "A person is riding a motorcycle on a rocky, mountainous road, with a scenic view of green hills and blue sky in the background.", "error": null, "sys_before": {"cpu_percent": 46.8, "ram_used_mb": 25595.6, "ram_available_mb": 100176.6, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 25595.3, "ram_available_mb": 100176.9, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.09, "peak": 16.15, "min": 13.59}, "VDD_GPU": {"avg": 35.39, "peak": 41.36, "min": 27.97}, "VIN": {"avg": 76.33, "peak": 119.13, "min": 58.1}, "VDD_CPU_SOC_MSS": {"avg": 15.96, "peak": 17.32, "min": 14.57}}, "power_watts_avg": 35.39, "energy_joules_est": 84.75, "sample_count": 18, "duration_seconds": 2.395}, "timestamp": "2026-01-16T16:22:25.962818"}
{"image_index": 344, "image_name": "000000037751.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000037751.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2745.838, "latencies_ms": [2745.838], "images_per_second": 0.364, "prompt_tokens": 23, "response_tokens_est": 36, "n_tiles": 12, "output_text": "1. Motorcycle\n2. Rider\n3. Helmet\n4. Gloves\n5. Backpack\n6. Vehicle\n7. Road\n8. Hills", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.3, "ram_available_mb": 100176.9, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 25595.3, "ram_available_mb": 100176.9, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.07, "peak": 15.65, "min": 14.43}, "VDD_GPU": {"avg": 35.52, "peak": 42.92, "min": 27.18}, "VIN": {"avg": 77.17, "peak": 119.5, "min": 58.79}, "VDD_CPU_SOC_MSS": {"avg": 15.78, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 35.52, "energy_joules_est": 97.55, "sample_count": 21, "duration_seconds": 2.746}, "timestamp": "2026-01-16T16:22:28.716357"}
{"image_index": 344, "image_name": "000000037751.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000037751.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3390.828, "latencies_ms": [3390.828], "images_per_second": 0.295, "prompt_tokens": 27, "response_tokens_est": 55, "n_tiles": 12, "output_text": "The main objects in the image are a person on a motorcycle and a mountainous landscape. The person is positioned in the foreground, riding the motorcycle on a rocky, uneven terrain. The mountainous landscape is in the background, stretching out with rolling hills and dense forests.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.3, "ram_available_mb": 100176.9, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25595.6, "ram_available_mb": 100176.6, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.08, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 33.77, "peak": 42.94, "min": 26.39}, "VIN": {"avg": 73.11, "peak": 116.39, "min": 58.62}, "VDD_CPU_SOC_MSS": {"avg": 15.77, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 33.77, "energy_joules_est": 114.52, "sample_count": 26, "duration_seconds": 3.391}, "timestamp": "2026-01-16T16:22:32.113940"}
{"image_index": 344, "image_name": "000000037751.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000037751.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3885.466, "latencies_ms": [3885.466], "images_per_second": 0.257, "prompt_tokens": 21, "response_tokens_est": 70, "n_tiles": 12, "output_text": "The image depicts a rugged, mountainous landscape with a dirt road winding through the terrain. A person is riding a motorcycle, wearing a helmet and a jacket, and appears to be enjoying the scenic ride. The setting is characterized by rocky paths, greenery, and a partly cloudy sky, creating a picturesque and adventurous atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.6, "ram_available_mb": 100176.6, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25595.3, "ram_available_mb": 100176.9, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.1, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 32.59, "peak": 42.94, "min": 26.39}, "VIN": {"avg": 76.21, "peak": 119.54, "min": 58.9}, "VDD_CPU_SOC_MSS": {"avg": 15.75, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 32.59, "energy_joules_est": 126.64, "sample_count": 30, "duration_seconds": 3.886}, "timestamp": "2026-01-16T16:22:36.005889"}
{"image_index": 344, "image_name": "000000037751.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000037751.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2656.231, "latencies_ms": [2656.231], "images_per_second": 0.376, "prompt_tokens": 19, "response_tokens_est": 34, "n_tiles": 12, "output_text": "The image features a rugged, rocky terrain with a clear blue sky overhead. The lighting is bright and natural, casting shadows that highlight the uneven surface of the ground.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.3, "ram_available_mb": 100176.9, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 25595.5, "ram_available_mb": 100176.6, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.13, "peak": 15.65, "min": 14.63}, "VDD_GPU": {"avg": 35.79, "peak": 42.94, "min": 27.57}, "VIN": {"avg": 78.57, "peak": 123.04, "min": 58.67}, "VDD_CPU_SOC_MSS": {"avg": 15.8, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 35.79, "energy_joules_est": 95.08, "sample_count": 20, "duration_seconds": 2.657}, "timestamp": "2026-01-16T16:22:38.668646"}
{"image_index": 345, "image_name": "000000037777.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000037777.jpg", "image_width": 352, "image_height": 230, "image_resolution": "352x230", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1424.83, "latencies_ms": [1424.83], "images_per_second": 0.702, "prompt_tokens": 9, "response_tokens_est": 32, "n_tiles": 6, "output_text": "The image shows a modern kitchen with wooden cabinets, a white refrigerator, a stove, and a countertop with a bowl of oranges on a table.", "error": null, "sys_before": {"cpu_percent": 44.9, "ram_used_mb": 25595.3, "ram_available_mb": 100176.9, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 25595.3, "ram_available_mb": 100176.9, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.49, "peak": 15.11, "min": 13.92}, "VDD_GPU": {"avg": 32.81, "peak": 38.21, "min": 26.79}, "VIN": {"avg": 74.15, "peak": 114.12, "min": 53.14}, "VDD_CPU_SOC_MSS": {"avg": 15.2, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 32.81, "energy_joules_est": 46.76, "sample_count": 10, "duration_seconds": 1.425}, "timestamp": "2026-01-16T16:22:40.152100"}
{"image_index": 345, "image_name": "000000037777.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000037777.jpg", "image_width": 352, "image_height": 230, "image_resolution": "352x230", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1775.686, "latencies_ms": [1775.686], "images_per_second": 0.563, "prompt_tokens": 23, "response_tokens_est": 42, "n_tiles": 6, "output_text": "kitchen: 2\ncabinets: 2\ndishwasher: 1\noven: 1\nfridge: 1\ntable: 1\nfruit bowl: 1", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 25595.3, "ram_available_mb": 100176.9, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 25595.3, "ram_available_mb": 100176.9, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.74, "peak": 15.11, "min": 14.12}, "VDD_GPU": {"avg": 31.42, "peak": 38.98, "min": 24.42}, "VIN": {"avg": 74.98, "peak": 117.58, "min": 60.38}, "VDD_CPU_SOC_MSS": {"avg": 15.23, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 31.42, "energy_joules_est": 55.81, "sample_count": 13, "duration_seconds": 1.776}, "timestamp": "2026-01-16T16:22:41.934581"}
{"image_index": 345, "image_name": "000000037777.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000037777.jpg", "image_width": 352, "image_height": 230, "image_resolution": "352x230", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2136.701, "latencies_ms": [2136.701], "images_per_second": 0.468, "prompt_tokens": 27, "response_tokens_est": 55, "n_tiles": 6, "output_text": "The main objects in the image are a kitchen and a dining area. The kitchen is located in the background, while the dining area is in the foreground. The dining table is positioned near the kitchen, and there is a bowl of fruit on the table in the foreground.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.3, "ram_available_mb": 100176.9, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 25595.5, "ram_available_mb": 100176.6, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.79, "peak": 15.11, "min": 14.12}, "VDD_GPU": {"avg": 29.81, "peak": 38.59, "min": 23.64}, "VIN": {"avg": 72.83, "peak": 115.55, "min": 60.08}, "VDD_CPU_SOC_MSS": {"avg": 15.38, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 29.81, "energy_joules_est": 63.71, "sample_count": 16, "duration_seconds": 2.137}, "timestamp": "2026-01-16T16:22:44.077555"}
{"image_index": 345, "image_name": "000000037777.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000037777.jpg", "image_width": 352, "image_height": 230, "image_resolution": "352x230", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2242.646, "latencies_ms": [2242.646], "images_per_second": 0.446, "prompt_tokens": 21, "response_tokens_est": 59, "n_tiles": 6, "output_text": "The image depicts a modern kitchen with a clean and bright ambiance. The kitchen features wooden cabinets, a white refrigerator, and a countertop with various kitchen appliances and utensils. There is a dining table with a bowl of oranges on it, suggesting a casual dining area.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.5, "ram_available_mb": 100176.6, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 25595.5, "ram_available_mb": 100176.7, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.82, "peak": 15.11, "min": 14.12}, "VDD_GPU": {"avg": 29.41, "peak": 38.21, "min": 23.64}, "VIN": {"avg": 70.67, "peak": 112.19, "min": 60.07}, "VDD_CPU_SOC_MSS": {"avg": 15.38, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 29.41, "energy_joules_est": 65.96, "sample_count": 17, "duration_seconds": 2.243}, "timestamp": "2026-01-16T16:22:46.326568"}
{"image_index": 345, "image_name": "000000037777.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000037777.jpg", "image_width": 352, "image_height": 230, "image_resolution": "352x230", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1776.619, "latencies_ms": [1776.619], "images_per_second": 0.563, "prompt_tokens": 19, "response_tokens_est": 42, "n_tiles": 6, "output_text": "The kitchen features light-colored walls and wooden cabinets, with a white refrigerator and a white stove. The room is well-lit with natural light streaming in through a window, creating a bright and welcoming atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.5, "ram_available_mb": 100176.7, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 25595.5, "ram_available_mb": 100176.7, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.72, "peak": 15.01, "min": 14.12}, "VDD_GPU": {"avg": 30.82, "peak": 38.21, "min": 24.42}, "VIN": {"avg": 71.15, "peak": 116.1, "min": 55.96}, "VDD_CPU_SOC_MSS": {"avg": 15.26, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 30.82, "energy_joules_est": 54.76, "sample_count": 13, "duration_seconds": 1.777}, "timestamp": "2026-01-16T16:22:48.113424"}
{"image_index": 346, "image_name": "000000037988.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000037988.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2380.775, "latencies_ms": [2380.775], "images_per_second": 0.42, "prompt_tokens": 9, "response_tokens_est": 29, "n_tiles": 12, "output_text": "A female tennis player is preparing to hit a tennis ball on a court, with a blue sign reading \"POLO\" in the background.", "error": null, "sys_before": {"cpu_percent": 44.0, "ram_used_mb": 25595.5, "ram_available_mb": 100176.7, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 25595.8, "ram_available_mb": 100176.4, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.83, "peak": 15.75, "min": 13.69}, "VDD_GPU": {"avg": 35.04, "peak": 40.57, "min": 27.57}, "VIN": {"avg": 74.03, "peak": 108.17, "min": 58.32}, "VDD_CPU_SOC_MSS": {"avg": 15.81, "peak": 16.92, "min": 14.96}}, "power_watts_avg": 35.04, "energy_joules_est": 83.44, "sample_count": 18, "duration_seconds": 2.381}, "timestamp": "2026-01-16T16:22:50.594418"}
{"image_index": 346, "image_name": "000000037988.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000037988.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 3123.822, "latencies_ms": [3123.822], "images_per_second": 0.32, "prompt_tokens": 23, "response_tokens_est": 47, "n_tiles": 12, "output_text": "1. Tennis player\n2. Tennis racket\n3. Tennis ball\n4. Tennis court\n5. Blue sign with \"POLO\" text\n6. Blue wall\n7. Green floor\n8. White cap", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.8, "ram_available_mb": 100176.4, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 25595.8, "ram_available_mb": 100176.4, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.77, "peak": 15.34, "min": 14.12}, "VDD_GPU": {"avg": 34.3, "peak": 42.92, "min": 26.0}, "VIN": {"avg": 78.07, "peak": 122.12, "min": 58.65}, "VDD_CPU_SOC_MSS": {"avg": 15.3, "peak": 16.14, "min": 14.56}}, "power_watts_avg": 34.3, "energy_joules_est": 107.17, "sample_count": 24, "duration_seconds": 3.124}, "timestamp": "2026-01-16T16:22:53.730178"}
{"image_index": 346, "image_name": "000000037988.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000037988.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 4235.821, "latencies_ms": [4235.821], "images_per_second": 0.236, "prompt_tokens": 27, "response_tokens_est": 79, "n_tiles": 12, "output_text": "The main object in the foreground is a tennis player, standing on the court. The player is holding a tennis racket and appears to be preparing to hit a tennis ball. The background features a blue barrier with the word \"POLO\" on it, and another person is standing behind it. The tennis court is green, and there is a white line marking the boundary of the court.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.8, "ram_available_mb": 100176.4, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25595.8, "ram_available_mb": 100176.4, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.72, "peak": 15.34, "min": 14.12}, "VDD_GPU": {"avg": 31.86, "peak": 42.15, "min": 25.6}, "VIN": {"avg": 72.13, "peak": 106.84, "min": 58.96}, "VDD_CPU_SOC_MSS": {"avg": 15.23, "peak": 16.14, "min": 14.56}}, "power_watts_avg": 31.86, "energy_joules_est": 134.98, "sample_count": 33, "duration_seconds": 4.237}, "timestamp": "2026-01-16T16:22:57.973389"}
{"image_index": 346, "image_name": "000000037988.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000037988.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 4300.654, "latencies_ms": [4300.654], "images_per_second": 0.233, "prompt_tokens": 21, "response_tokens_est": 81, "n_tiles": 12, "output_text": "The image depicts a scene on a tennis court, where a female tennis player is in the midst of a serve. The court is marked with white lines, and the player is wearing a white cap, a white tank top, and black shorts. The player is holding a tennis racket and appears to be focused on the ball, which is in the air, indicating the moment of the serve.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.8, "ram_available_mb": 100176.4, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 25595.7, "ram_available_mb": 100176.4, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.74, "peak": 15.34, "min": 14.12}, "VDD_GPU": {"avg": 31.63, "peak": 41.76, "min": 25.6}, "VIN": {"avg": 71.65, "peak": 115.25, "min": 58.71}, "VDD_CPU_SOC_MSS": {"avg": 15.23, "peak": 16.14, "min": 14.56}}, "power_watts_avg": 31.63, "energy_joules_est": 136.04, "sample_count": 33, "duration_seconds": 4.301}, "timestamp": "2026-01-16T16:23:02.280811"}
{"image_index": 346, "image_name": "000000037988.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000037988.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3538.058, "latencies_ms": [3538.058], "images_per_second": 0.283, "prompt_tokens": 19, "response_tokens_est": 59, "n_tiles": 12, "output_text": "The image shows a tennis player on a green court, wearing a white shirt and black skirt, with white shoes. The court has a blue boundary line and a white line, and there is a microphone on the left side. The lighting is bright, and the weather appears to be clear.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.7, "ram_available_mb": 100176.4, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25595.7, "ram_available_mb": 100176.4, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.73, "peak": 15.24, "min": 14.22}, "VDD_GPU": {"avg": 33.03, "peak": 42.53, "min": 26.0}, "VIN": {"avg": 76.47, "peak": 137.77, "min": 57.27}, "VDD_CPU_SOC_MSS": {"avg": 15.25, "peak": 16.14, "min": 14.56}}, "power_watts_avg": 33.03, "energy_joules_est": 116.88, "sample_count": 28, "duration_seconds": 3.539}, "timestamp": "2026-01-16T16:23:05.825285"}
{"image_index": 347, "image_name": "000000038048.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038048.jpg", "image_width": 299, "image_height": 500, "image_resolution": "299x500", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1339.015, "latencies_ms": [1339.015], "images_per_second": 0.747, "prompt_tokens": 9, "response_tokens_est": 28, "n_tiles": 6, "output_text": "A red fire hydrant stands on a sidewalk, with a pedestrian crossing sign above it, and a person is visible in the background.", "error": null, "sys_before": {"cpu_percent": 37.5, "ram_used_mb": 25595.7, "ram_available_mb": 100176.4, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 25596.0, "ram_available_mb": 100176.2, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.33, "peak": 14.91, "min": 13.82}, "VDD_GPU": {"avg": 32.22, "peak": 37.82, "min": 26.0}, "VIN": {"avg": 78.8, "peak": 124.27, "min": 64.53}, "VDD_CPU_SOC_MSS": {"avg": 15.0, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 32.22, "energy_joules_est": 43.15, "sample_count": 10, "duration_seconds": 1.339}, "timestamp": "2026-01-16T16:23:07.226500"}
{"image_index": 347, "image_name": "000000038048.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038048.jpg", "image_width": 299, "image_height": 500, "image_resolution": "299x500", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1371.549, "latencies_ms": [1371.549], "images_per_second": 0.729, "prompt_tokens": 23, "response_tokens_est": 27, "n_tiles": 6, "output_text": "fire hydrant: 1\nsign: 1\ntree: 1\nhouse: 1\nman: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25596.0, "ram_available_mb": 100176.2, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 25596.0, "ram_available_mb": 100176.2, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.53, "peak": 14.81, "min": 14.02}, "VDD_GPU": {"avg": 33.32, "peak": 38.59, "min": 26.39}, "VIN": {"avg": 70.48, "peak": 101.08, "min": 63.23}, "VDD_CPU_SOC_MSS": {"avg": 15.19, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 33.32, "energy_joules_est": 45.73, "sample_count": 10, "duration_seconds": 1.372}, "timestamp": "2026-01-16T16:23:08.604380"}
{"image_index": 347, "image_name": "000000038048.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038048.jpg", "image_width": 299, "image_height": 500, "image_resolution": "299x500", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2018.548, "latencies_ms": [2018.548], "images_per_second": 0.495, "prompt_tokens": 27, "response_tokens_est": 50, "n_tiles": 6, "output_text": "The main object in the foreground is a red fire hydrant, which is positioned on the sidewalk. In the background, there is a person standing near the sidewalk. The fire hydrant is near the sidewalk, while the person is further away.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25596.0, "ram_available_mb": 100176.2, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 25596.0, "ram_available_mb": 100176.2, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.68, "peak": 15.01, "min": 14.02}, "VDD_GPU": {"avg": 30.41, "peak": 38.98, "min": 24.03}, "VIN": {"avg": 68.87, "peak": 96.46, "min": 50.59}, "VDD_CPU_SOC_MSS": {"avg": 15.22, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 30.41, "energy_joules_est": 61.4, "sample_count": 15, "duration_seconds": 2.019}, "timestamp": "2026-01-16T16:23:10.629113"}
{"image_index": 347, "image_name": "000000038048.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038048.jpg", "image_width": 299, "image_height": 500, "image_resolution": "299x500", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2045.599, "latencies_ms": [2045.599], "images_per_second": 0.489, "prompt_tokens": 21, "response_tokens_est": 51, "n_tiles": 6, "output_text": "The image depicts a scene on a sunny day with a red fire hydrant positioned on a sidewalk. In the background, there is a person standing near a building, and the setting appears to be a residential area with trees and a clear sky.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25596.0, "ram_available_mb": 100176.2, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 25596.2, "ram_available_mb": 100176.0, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.7, "peak": 15.01, "min": 14.12}, "VDD_GPU": {"avg": 29.69, "peak": 38.21, "min": 23.64}, "VIN": {"avg": 68.48, "peak": 116.99, "min": 55.32}, "VDD_CPU_SOC_MSS": {"avg": 15.35, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 29.69, "energy_joules_est": 60.74, "sample_count": 16, "duration_seconds": 2.046}, "timestamp": "2026-01-16T16:23:12.680582"}
{"image_index": 347, "image_name": "000000038048.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038048.jpg", "image_width": 299, "image_height": 500, "image_resolution": "299x500", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1707.771, "latencies_ms": [1707.771], "images_per_second": 0.586, "prompt_tokens": 19, "response_tokens_est": 39, "n_tiles": 6, "output_text": "The image features a bright red fire hydrant on a concrete sidewalk, with a clear blue sky overhead. The hydrant is illuminated by natural sunlight, casting a shadow on the ground.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25596.2, "ram_available_mb": 100176.0, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 25596.0, "ram_available_mb": 100176.2, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.65, "peak": 14.91, "min": 14.02}, "VDD_GPU": {"avg": 31.03, "peak": 38.19, "min": 24.42}, "VIN": {"avg": 69.13, "peak": 116.67, "min": 53.17}, "VDD_CPU_SOC_MSS": {"avg": 15.23, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 31.03, "energy_joules_est": 53.0, "sample_count": 13, "duration_seconds": 1.708}, "timestamp": "2026-01-16T16:23:14.394323"}
{"image_index": 348, "image_name": "000000038070.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038070.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2613.057, "latencies_ms": [2613.057], "images_per_second": 0.383, "prompt_tokens": 9, "response_tokens_est": 36, "n_tiles": 12, "output_text": "The image shows a toilet with a white seat and lid, situated on a carpeted floor, with a small blue seat and a white toilet paper holder attached to the wall.", "error": null, "sys_before": {"cpu_percent": 40.3, "ram_used_mb": 25596.0, "ram_available_mb": 100176.2, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 25596.0, "ram_available_mb": 100176.2, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.14, "peak": 16.15, "min": 13.9}, "VDD_GPU": {"avg": 34.37, "peak": 40.97, "min": 27.19}, "VIN": {"avg": 75.54, "peak": 122.46, "min": 50.81}, "VDD_CPU_SOC_MSS": {"avg": 16.02, "peak": 17.32, "min": 14.96}}, "power_watts_avg": 34.37, "energy_joules_est": 89.83, "sample_count": 20, "duration_seconds": 2.614}, "timestamp": "2026-01-16T16:23:17.098582"}
{"image_index": 348, "image_name": "000000038070.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038070.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 5851.47, "latencies_ms": [5851.47], "images_per_second": 0.171, "prompt_tokens": 23, "response_tokens_est": 129, "n_tiles": 12, "output_text": "toilet: 1\ntoilet paper: 1\ntoilet brush: 1\ntoilet paper roll: 1\ntoilet paper holder: 1\ntoilet paper: 1\ntoilet brush holder: 1\ntoilet brush: 1\ntoilet brush holder: 1\ntoilet paper holder: 1\ntoilet paper: 1\ntoilet brush holder: 1\ntoilet paper holder: 1\ntoilet paper: 1\ntoilet paper holder: 1\ntoilet paper: 1\ntoilet paper holder: 1\nto", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25596.0, "ram_available_mb": 100176.2, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 25595.7, "ram_available_mb": 100176.5, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.12, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 30.48, "peak": 42.94, "min": 26.0}, "VIN": {"avg": 73.96, "peak": 120.14, "min": 58.56}, "VDD_CPU_SOC_MSS": {"avg": 15.73, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 30.48, "energy_joules_est": 178.38, "sample_count": 46, "duration_seconds": 5.852}, "timestamp": "2026-01-16T16:23:22.956741"}
{"image_index": 348, "image_name": "000000038070.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038070.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3583.084, "latencies_ms": [3583.084], "images_per_second": 0.279, "prompt_tokens": 27, "response_tokens_est": 61, "n_tiles": 12, "output_text": "The main object in the foreground is a toilet with a lid on top. The toilet is positioned on a carpeted floor, and there is a small blue object near the toilet, possibly a seat cover or a container. The background features a wall, and the toilet is situated close to the wall.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.7, "ram_available_mb": 100176.5, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25595.7, "ram_available_mb": 100176.5, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.1, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 33.09, "peak": 42.54, "min": 26.39}, "VIN": {"avg": 72.52, "peak": 116.3, "min": 58.53}, "VDD_CPU_SOC_MSS": {"avg": 15.75, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 33.09, "energy_joules_est": 118.58, "sample_count": 28, "duration_seconds": 3.584}, "timestamp": "2026-01-16T16:23:26.546392"}
{"image_index": 348, "image_name": "000000038070.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038070.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3038.832, "latencies_ms": [3038.832], "images_per_second": 0.329, "prompt_tokens": 21, "response_tokens_est": 45, "n_tiles": 12, "output_text": "The image depicts a bathroom setting with a toilet and a toilet paper holder. The toilet is white and placed on a beige carpeted floor. There is a person standing next to the toilet, wearing black shoes.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.7, "ram_available_mb": 100176.5, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 25596.0, "ram_available_mb": 100176.2, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.11, "peak": 15.65, "min": 14.63}, "VDD_GPU": {"avg": 34.63, "peak": 42.94, "min": 26.79}, "VIN": {"avg": 75.62, "peak": 98.4, "min": 58.81}, "VDD_CPU_SOC_MSS": {"avg": 15.73, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 34.63, "energy_joules_est": 105.26, "sample_count": 23, "duration_seconds": 3.04}, "timestamp": "2026-01-16T16:23:29.591987"}
{"image_index": 348, "image_name": "000000038070.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038070.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2875.649, "latencies_ms": [2875.649], "images_per_second": 0.348, "prompt_tokens": 19, "response_tokens_est": 40, "n_tiles": 12, "output_text": "The toilet in the image is white, with a glossy finish, and is situated on a carpeted floor. The lighting is soft and diffused, casting a gentle glow on the scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25596.0, "ram_available_mb": 100176.2, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 25596.0, "ram_available_mb": 100176.2, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.08, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 35.06, "peak": 42.94, "min": 26.79}, "VIN": {"avg": 74.02, "peak": 107.48, "min": 58.62}, "VDD_CPU_SOC_MSS": {"avg": 15.75, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 35.06, "energy_joules_est": 100.84, "sample_count": 22, "duration_seconds": 2.876}, "timestamp": "2026-01-16T16:23:32.478289"}
{"image_index": 349, "image_name": "000000038118.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038118.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1581.185, "latencies_ms": [1581.185], "images_per_second": 0.632, "prompt_tokens": 9, "response_tokens_est": 37, "n_tiles": 6, "output_text": "A skier in a red jacket and black pants is skiing down a snowy slope, with ski poles in hand, and the snow is visibly disturbed by their movement.", "error": null, "sys_before": {"cpu_percent": 32.5, "ram_used_mb": 25595.7, "ram_available_mb": 100176.5, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25595.7, "ram_available_mb": 100176.5, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.46, "peak": 15.11, "min": 13.82}, "VDD_GPU": {"avg": 31.25, "peak": 37.82, "min": 24.82}, "VIN": {"avg": 79.73, "peak": 122.63, "min": 63.66}, "VDD_CPU_SOC_MSS": {"avg": 15.19, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 31.25, "energy_joules_est": 49.42, "sample_count": 12, "duration_seconds": 1.581}, "timestamp": "2026-01-16T16:23:34.125636"}
{"image_index": 349, "image_name": "000000038118.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038118.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1679.725, "latencies_ms": [1679.725], "images_per_second": 0.595, "prompt_tokens": 23, "response_tokens_est": 38, "n_tiles": 6, "output_text": "1. Skier\n2. Ski poles\n3. Ski\n4. Snow\n5. Snowboard\n6. Snowboarder\n7. Snow\n8. Snowboard", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.7, "ram_available_mb": 100176.5, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25595.7, "ram_available_mb": 100176.5, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.65, "peak": 14.91, "min": 14.12}, "VDD_GPU": {"avg": 31.54, "peak": 38.6, "min": 24.82}, "VIN": {"avg": 67.11, "peak": 79.57, "min": 60.54}, "VDD_CPU_SOC_MSS": {"avg": 15.19, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 31.54, "energy_joules_est": 52.99, "sample_count": 12, "duration_seconds": 1.68}, "timestamp": "2026-01-16T16:23:35.811505"}
{"image_index": 349, "image_name": "000000038118.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038118.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2327.431, "latencies_ms": [2327.431], "images_per_second": 0.43, "prompt_tokens": 27, "response_tokens_est": 61, "n_tiles": 6, "output_text": "The main objects in the image are a person skiing and a snow-covered mountain. The person is positioned in the foreground, closer to the viewer, while the mountain is in the background, further away. The ski tracks are visible on the snow, indicating the paths taken by the skier.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.7, "ram_available_mb": 100176.5, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 25596.2, "ram_available_mb": 100176.0, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.75, "peak": 15.01, "min": 14.02}, "VDD_GPU": {"avg": 28.99, "peak": 38.59, "min": 23.24}, "VIN": {"avg": 69.43, "peak": 100.58, "min": 56.9}, "VDD_CPU_SOC_MSS": {"avg": 15.27, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 28.99, "energy_joules_est": 67.48, "sample_count": 18, "duration_seconds": 2.328}, "timestamp": "2026-01-16T16:23:38.145137"}
{"image_index": 349, "image_name": "000000038118.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038118.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2159.629, "latencies_ms": [2159.629], "images_per_second": 0.463, "prompt_tokens": 21, "response_tokens_est": 55, "n_tiles": 6, "output_text": "The image depicts a snowy mountainous landscape under a clear blue sky. A skier dressed in red gear is navigating the slopes, using ski poles to maintain balance and speed. The snow-covered terrain is dotted with tracks, indicating previous skiers' paths.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25596.2, "ram_available_mb": 100176.0, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 25595.9, "ram_available_mb": 100176.2, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.72, "peak": 15.01, "min": 14.12}, "VDD_GPU": {"avg": 29.44, "peak": 38.19, "min": 23.64}, "VIN": {"avg": 71.57, "peak": 113.55, "min": 54.74}, "VDD_CPU_SOC_MSS": {"avg": 15.21, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 29.44, "energy_joules_est": 63.59, "sample_count": 16, "duration_seconds": 2.16}, "timestamp": "2026-01-16T16:23:40.314915"}
{"image_index": 349, "image_name": "000000038118.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038118.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2108.072, "latencies_ms": [2108.072], "images_per_second": 0.474, "prompt_tokens": 19, "response_tokens_est": 53, "n_tiles": 6, "output_text": "The image depicts a snowy mountain landscape under a clear blue sky. The snow-covered ground is dotted with tracks, indicating recent activity. The lighting is bright and natural, casting shadows that enhance the texture of the snow and the contours of the mountain.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.9, "ram_available_mb": 100176.2, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 25595.7, "ram_available_mb": 100176.5, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.67, "peak": 14.91, "min": 14.02}, "VDD_GPU": {"avg": 29.54, "peak": 38.21, "min": 23.64}, "VIN": {"avg": 70.73, "peak": 117.85, "min": 56.82}, "VDD_CPU_SOC_MSS": {"avg": 15.21, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 29.54, "energy_joules_est": 62.3, "sample_count": 16, "duration_seconds": 2.109}, "timestamp": "2026-01-16T16:23:42.429813"}
{"image_index": 350, "image_name": "000000038210.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038210.jpg", "image_width": 428, "image_height": 640, "image_resolution": "428x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1575.098, "latencies_ms": [1575.098], "images_per_second": 0.635, "prompt_tokens": 9, "response_tokens_est": 37, "n_tiles": 6, "output_text": "A man is skiing down a snowy trail, wearing a red and white jacket, black pants, and a beanie, with a bib number 30 on his back.", "error": null, "sys_before": {"cpu_percent": 38.0, "ram_used_mb": 25595.7, "ram_available_mb": 100176.5, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 25595.7, "ram_available_mb": 100176.5, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.51, "peak": 15.11, "min": 13.82}, "VDD_GPU": {"avg": 30.63, "peak": 37.03, "min": 24.82}, "VIN": {"avg": 75.72, "peak": 126.37, "min": 54.27}, "VDD_CPU_SOC_MSS": {"avg": 15.19, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 30.63, "energy_joules_est": 48.25, "sample_count": 12, "duration_seconds": 1.575}, "timestamp": "2026-01-16T16:23:44.066340"}
{"image_index": 350, "image_name": "000000038210.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038210.jpg", "image_width": 428, "image_height": 640, "image_resolution": "428x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1766.636, "latencies_ms": [1766.636], "images_per_second": 0.566, "prompt_tokens": 23, "response_tokens_est": 41, "n_tiles": 6, "output_text": "1. Ski poles\n2. Ski\n3. Ski skis\n4. Ski skier\n5. Ski jacket\n6. Ski pants\n7. Ski boots\n8. Ski hat", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25595.7, "ram_available_mb": 100176.5, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25595.9, "ram_available_mb": 100176.3, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.66, "peak": 14.91, "min": 14.12}, "VDD_GPU": {"avg": 31.3, "peak": 38.6, "min": 24.42}, "VIN": {"avg": 71.61, "peak": 96.89, "min": 58.92}, "VDD_CPU_SOC_MSS": {"avg": 15.2, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 31.3, "energy_joules_est": 55.32, "sample_count": 13, "duration_seconds": 1.767}, "timestamp": "2026-01-16T16:23:45.839485"}
{"image_index": 350, "image_name": "000000038210.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038210.jpg", "image_width": 428, "image_height": 640, "image_resolution": "428x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3005.458, "latencies_ms": [3005.458], "images_per_second": 0.333, "prompt_tokens": 27, "response_tokens_est": 85, "n_tiles": 6, "output_text": "The main object in the foreground is a skier wearing a red and white jacket, black pants, and a beanie. The skier is skiing on a snowy trail, with their skis parallel to the ground. In the background, there are other skiers and a snow-covered mountain. The skier in the background is further away, and the snow-covered mountain is in the far background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.9, "ram_available_mb": 100176.3, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 25595.7, "ram_available_mb": 100176.5, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.79, "peak": 15.11, "min": 14.02}, "VDD_GPU": {"avg": 27.69, "peak": 38.59, "min": 23.24}, "VIN": {"avg": 71.06, "peak": 119.02, "min": 56.52}, "VDD_CPU_SOC_MSS": {"avg": 15.44, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 27.69, "energy_joules_est": 83.23, "sample_count": 23, "duration_seconds": 3.006}, "timestamp": "2026-01-16T16:23:48.851049"}
{"image_index": 350, "image_name": "000000038210.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038210.jpg", "image_width": 428, "image_height": 640, "image_resolution": "428x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2411.66, "latencies_ms": [2411.66], "images_per_second": 0.415, "prompt_tokens": 21, "response_tokens_est": 64, "n_tiles": 6, "output_text": "The image depicts a snowy landscape with a person dressed in winter attire, including a beanie and ski gear, actively skiing down a snow-covered path. The scene is set in a forested area, with snow-covered trees and a clear sky, suggesting a cold, wintry environment.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.7, "ram_available_mb": 100176.5, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 25595.7, "ram_available_mb": 100176.5, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.75, "peak": 15.01, "min": 14.02}, "VDD_GPU": {"avg": 28.73, "peak": 38.19, "min": 23.25}, "VIN": {"avg": 68.56, "peak": 115.39, "min": 57.22}, "VDD_CPU_SOC_MSS": {"avg": 15.22, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 28.73, "energy_joules_est": 69.3, "sample_count": 18, "duration_seconds": 2.412}, "timestamp": "2026-01-16T16:23:51.270372"}
{"image_index": 350, "image_name": "000000038210.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038210.jpg", "image_width": 428, "image_height": 640, "image_resolution": "428x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2405.797, "latencies_ms": [2405.797], "images_per_second": 0.416, "prompt_tokens": 19, "response_tokens_est": 64, "n_tiles": 6, "output_text": "The image depicts a snowy landscape with a man dressed in a red and white jacket, black pants, and a gray beanie, holding ski poles. The scene is illuminated by soft, diffused light, likely from an overcast sky, and the snow-covered ground and trees are clearly visible.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.7, "ram_available_mb": 100176.5, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 25595.4, "ram_available_mb": 100176.8, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.76, "peak": 15.11, "min": 14.12}, "VDD_GPU": {"avg": 28.69, "peak": 37.8, "min": 23.24}, "VIN": {"avg": 69.4, "peak": 113.11, "min": 60.49}, "VDD_CPU_SOC_MSS": {"avg": 15.31, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 28.69, "energy_joules_est": 69.03, "sample_count": 18, "duration_seconds": 2.406}, "timestamp": "2026-01-16T16:23:53.682395"}
{"image_index": 351, "image_name": "000000038576.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038576.jpg", "image_width": 438, "image_height": 640, "image_resolution": "438x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1419.799, "latencies_ms": [1419.799], "images_per_second": 0.704, "prompt_tokens": 9, "response_tokens_est": 31, "n_tiles": 6, "output_text": "The image shows a black and white photo of a computer setup with a monitor displaying a desktop wallpaper, a keyboard, and a mouse on a desk.", "error": null, "sys_before": {"cpu_percent": 44.3, "ram_used_mb": 25595.4, "ram_available_mb": 100176.8, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 25595.7, "ram_available_mb": 100176.5, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.37, "peak": 14.91, "min": 13.82}, "VDD_GPU": {"avg": 31.51, "peak": 36.63, "min": 26.0}, "VIN": {"avg": 68.67, "peak": 115.2, "min": 50.06}, "VDD_CPU_SOC_MSS": {"avg": 15.12, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 31.51, "energy_joules_est": 44.75, "sample_count": 10, "duration_seconds": 1.42}, "timestamp": "2026-01-16T16:23:55.161768"}
{"image_index": 351, "image_name": "000000038576.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038576.jpg", "image_width": 438, "image_height": 640, "image_resolution": "438x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1735.722, "latencies_ms": [1735.722], "images_per_second": 0.576, "prompt_tokens": 23, "response_tokens_est": 40, "n_tiles": 6, "output_text": "object: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.7, "ram_available_mb": 100176.5, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25595.4, "ram_available_mb": 100176.8, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.63, "peak": 14.91, "min": 14.12}, "VDD_GPU": {"avg": 31.33, "peak": 38.59, "min": 24.42}, "VIN": {"avg": 64.97, "peak": 79.78, "min": 56.43}, "VDD_CPU_SOC_MSS": {"avg": 15.17, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 31.33, "energy_joules_est": 54.39, "sample_count": 13, "duration_seconds": 1.736}, "timestamp": "2026-01-16T16:23:56.903758"}
{"image_index": 351, "image_name": "000000038576.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038576.jpg", "image_width": 438, "image_height": 640, "image_resolution": "438x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2724.168, "latencies_ms": [2724.168], "images_per_second": 0.367, "prompt_tokens": 27, "response_tokens_est": 75, "n_tiles": 6, "output_text": "The main objects in the image are a computer monitor, a keyboard, and a mouse. The monitor is positioned on the left side of the image, with the keyboard and mouse placed in front of it. The keyboard is closer to the viewer, while the mouse is positioned slightly further back. The background is a plain wall, providing a neutral backdrop that highlights the objects.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25595.4, "ram_available_mb": 100176.8, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 25595.4, "ram_available_mb": 100176.8, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.77, "peak": 15.11, "min": 14.02}, "VDD_GPU": {"avg": 28.21, "peak": 38.6, "min": 23.24}, "VIN": {"avg": 67.61, "peak": 97.59, "min": 61.46}, "VDD_CPU_SOC_MSS": {"avg": 15.35, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 28.21, "energy_joules_est": 76.86, "sample_count": 21, "duration_seconds": 2.725}, "timestamp": "2026-01-16T16:23:59.633990"}
{"image_index": 351, "image_name": "000000038576.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038576.jpg", "image_width": 438, "image_height": 640, "image_resolution": "438x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2215.533, "latencies_ms": [2215.533], "images_per_second": 0.451, "prompt_tokens": 21, "response_tokens_est": 57, "n_tiles": 6, "output_text": "The image depicts a modern office workspace with a computer setup. The desk features a large monitor displaying a desktop wallpaper, a keyboard, and a mouse. The workspace is well-lit, and the overall setting suggests a professional environment, possibly a home office or a small office.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.4, "ram_available_mb": 100176.8, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 25595.6, "ram_available_mb": 100176.5, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.72, "peak": 15.01, "min": 14.02}, "VDD_GPU": {"avg": 29.2, "peak": 38.21, "min": 23.24}, "VIN": {"avg": 71.88, "peak": 112.94, "min": 57.23}, "VDD_CPU_SOC_MSS": {"avg": 15.24, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 29.2, "energy_joules_est": 64.71, "sample_count": 17, "duration_seconds": 2.216}, "timestamp": "2026-01-16T16:24:01.855644"}
{"image_index": 351, "image_name": "000000038576.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038576.jpg", "image_width": 438, "image_height": 640, "image_resolution": "438x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2275.347, "latencies_ms": [2275.347], "images_per_second": 0.439, "prompt_tokens": 19, "response_tokens_est": 59, "n_tiles": 6, "output_text": "The image is a black and white photograph featuring a modern computer setup. The computer monitor displays a desktop wallpaper with a geometric design, and the keyboard and mouse are black, contrasting with the light-colored desk surface. The lighting is soft and even, creating a clean and minimalist aesthetic.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.6, "ram_available_mb": 100176.5, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 25595.9, "ram_available_mb": 100176.3, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.71, "peak": 15.01, "min": 14.02}, "VDD_GPU": {"avg": 29.01, "peak": 38.21, "min": 23.24}, "VIN": {"avg": 67.21, "peak": 94.57, "min": 59.91}, "VDD_CPU_SOC_MSS": {"avg": 15.28, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 29.01, "energy_joules_est": 66.02, "sample_count": 17, "duration_seconds": 2.276}, "timestamp": "2026-01-16T16:24:04.136918"}
{"image_index": 352, "image_name": "000000038678.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038678.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2115.328, "latencies_ms": [2115.328], "images_per_second": 0.473, "prompt_tokens": 9, "response_tokens_est": 21, "n_tiles": 12, "output_text": "A woman is sitting in a train carriage, holding a doughnut with a smile on her face.", "error": null, "sys_before": {"cpu_percent": 46.0, "ram_used_mb": 25595.6, "ram_available_mb": 100176.5, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 2.4, "ram_used_mb": 25595.6, "ram_available_mb": 100176.5, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.12, "peak": 16.05, "min": 13.79}, "VDD_GPU": {"avg": 35.99, "peak": 41.36, "min": 28.76}, "VIN": {"avg": 81.64, "peak": 117.68, "min": 58.49}, "VDD_CPU_SOC_MSS": {"avg": 15.97, "peak": 17.32, "min": 14.96}}, "power_watts_avg": 35.99, "energy_joules_est": 76.15, "sample_count": 16, "duration_seconds": 2.116}, "timestamp": "2026-01-16T16:24:06.346952"}
{"image_index": 352, "image_name": "000000038678.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038678.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2610.405, "latencies_ms": [2610.405], "images_per_second": 0.383, "prompt_tokens": 23, "response_tokens_est": 32, "n_tiles": 12, "output_text": "1. Woman\n2. Woman\n3. Woman\n4. Woman\n5. Woman\n6. Woman\n7. Woman\n8. Woman", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.6, "ram_available_mb": 100176.5, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 25595.9, "ram_available_mb": 100176.3, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.07, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 36.13, "peak": 43.33, "min": 27.57}, "VIN": {"avg": 79.55, "peak": 120.16, "min": 59.11}, "VDD_CPU_SOC_MSS": {"avg": 15.66, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 36.13, "energy_joules_est": 94.33, "sample_count": 19, "duration_seconds": 2.611}, "timestamp": "2026-01-16T16:24:08.963806"}
{"image_index": 352, "image_name": "000000038678.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038678.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3216.879, "latencies_ms": [3216.879], "images_per_second": 0.311, "prompt_tokens": 27, "response_tokens_est": 50, "n_tiles": 12, "output_text": "The main objects in the image are a woman and a man. The woman is holding a doughnut in the foreground, while the man is partially visible in the background. The doughnut is near the woman, and the man is further away.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.9, "ram_available_mb": 100176.3, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25595.6, "ram_available_mb": 100176.5, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.09, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 33.94, "peak": 42.54, "min": 26.39}, "VIN": {"avg": 76.47, "peak": 122.34, "min": 58.54}, "VDD_CPU_SOC_MSS": {"avg": 15.62, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 33.94, "energy_joules_est": 109.19, "sample_count": 25, "duration_seconds": 3.217}, "timestamp": "2026-01-16T16:24:12.187994"}
{"image_index": 352, "image_name": "000000038678.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038678.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3416.099, "latencies_ms": [3416.099], "images_per_second": 0.293, "prompt_tokens": 21, "response_tokens_est": 56, "n_tiles": 12, "output_text": "The image depicts a scene inside a train carriage, where a woman is seated and holding a doughnut. The woman is wearing a striped shirt and has short hair. The train carriage is filled with passengers, and there is a window with blurred greenery outside.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.6, "ram_available_mb": 100176.5, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 25595.9, "ram_available_mb": 100176.3, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.09, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 33.32, "peak": 42.94, "min": 26.39}, "VIN": {"avg": 76.47, "peak": 132.34, "min": 45.14}, "VDD_CPU_SOC_MSS": {"avg": 15.64, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 33.32, "energy_joules_est": 113.83, "sample_count": 27, "duration_seconds": 3.416}, "timestamp": "2026-01-16T16:24:15.610534"}
{"image_index": 352, "image_name": "000000038678.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038678.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3824.286, "latencies_ms": [3824.286], "images_per_second": 0.261, "prompt_tokens": 19, "response_tokens_est": 68, "n_tiles": 12, "output_text": "The image features a woman with short, light-colored hair, wearing a maroon top, sitting in a train carriage. The interior is dimly lit, with a warm glow from overhead lights. The woman is holding a white plate with a golden-brown doughnut, and there is a clear plastic bag with a green label nearby.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.9, "ram_available_mb": 100176.3, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25595.9, "ram_available_mb": 100176.3, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.07, "peak": 15.65, "min": 14.43}, "VDD_GPU": {"avg": 32.63, "peak": 42.94, "min": 26.0}, "VIN": {"avg": 72.71, "peak": 113.32, "min": 56.11}, "VDD_CPU_SOC_MSS": {"avg": 15.69, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 32.63, "energy_joules_est": 124.81, "sample_count": 30, "duration_seconds": 3.825}, "timestamp": "2026-01-16T16:24:19.441882"}
{"image_index": 353, "image_name": "000000038825.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038825.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1283.399, "latencies_ms": [1283.399], "images_per_second": 0.779, "prompt_tokens": 9, "response_tokens_est": 26, "n_tiles": 6, "output_text": "The image depicts a zebra grazing on a lush green field, with its distinctive black and white stripes clearly visible.", "error": null, "sys_before": {"cpu_percent": 38.8, "ram_used_mb": 25595.9, "ram_available_mb": 100176.3, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 25596.1, "ram_available_mb": 100176.1, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.35, "peak": 14.81, "min": 13.92}, "VDD_GPU": {"avg": 32.91, "peak": 37.8, "min": 27.18}, "VIN": {"avg": 70.61, "peak": 93.1, "min": 49.6}, "VDD_CPU_SOC_MSS": {"avg": 15.05, "peak": 15.36, "min": 14.57}}, "power_watts_avg": 32.91, "energy_joules_est": 42.25, "sample_count": 9, "duration_seconds": 1.284}, "timestamp": "2026-01-16T16:24:20.794916"}
{"image_index": 353, "image_name": "000000038825.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038825.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1062.292, "latencies_ms": [1062.292], "images_per_second": 0.941, "prompt_tokens": 23, "response_tokens_est": 16, "n_tiles": 6, "output_text": "zebra: 2\ngrass: 1\nground: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25596.1, "ram_available_mb": 100176.1, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 25595.9, "ram_available_mb": 100176.3, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.56, "peak": 14.73, "min": 14.22}, "VDD_GPU": {"avg": 35.05, "peak": 38.59, "min": 29.15}, "VIN": {"avg": 67.02, "peak": 89.59, "min": 52.42}, "VDD_CPU_SOC_MSS": {"avg": 15.16, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 35.05, "energy_joules_est": 37.25, "sample_count": 8, "duration_seconds": 1.063}, "timestamp": "2026-01-16T16:24:21.863350"}
{"image_index": 353, "image_name": "000000038825.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038825.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2340.63, "latencies_ms": [2340.63], "images_per_second": 0.427, "prompt_tokens": 27, "response_tokens_est": 61, "n_tiles": 6, "output_text": "The main objects in the image are two zebras. The foreground features a zebra with a black and white striped pattern, while the background shows another zebra with similar stripes. The zebra in the foreground is closer to the camera, while the one in the background is further away.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.9, "ram_available_mb": 100176.3, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 25595.9, "ram_available_mb": 100176.3, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.69, "peak": 15.01, "min": 14.02}, "VDD_GPU": {"avg": 29.63, "peak": 39.77, "min": 23.25}, "VIN": {"avg": 69.13, "peak": 119.3, "min": 55.37}, "VDD_CPU_SOC_MSS": {"avg": 15.22, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 29.63, "energy_joules_est": 69.36, "sample_count": 18, "duration_seconds": 2.341}, "timestamp": "2026-01-16T16:24:24.211356"}
{"image_index": 353, "image_name": "000000038825.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038825.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1939.427, "latencies_ms": [1939.427], "images_per_second": 0.516, "prompt_tokens": 21, "response_tokens_est": 47, "n_tiles": 6, "output_text": "The image depicts a group of zebras grazing on a lush green field. The zebras are in a natural setting, likely a savanna or grassland, with their distinctive black and white stripes clearly visible.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.9, "ram_available_mb": 100176.3, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 25595.6, "ram_available_mb": 100176.6, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.66, "peak": 15.01, "min": 14.02}, "VDD_GPU": {"avg": 29.94, "peak": 38.21, "min": 23.64}, "VIN": {"avg": 70.25, "peak": 115.91, "min": 59.08}, "VDD_CPU_SOC_MSS": {"avg": 15.2, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 29.94, "energy_joules_est": 58.08, "sample_count": 15, "duration_seconds": 1.94}, "timestamp": "2026-01-16T16:24:26.157062"}
{"image_index": 353, "image_name": "000000038825.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038825.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1828.741, "latencies_ms": [1828.741], "images_per_second": 0.547, "prompt_tokens": 19, "response_tokens_est": 43, "n_tiles": 6, "output_text": "The image features a zebra with a striking black and white striped pattern, standing on a lush green grassy field. The lighting is natural, casting soft shadows on the ground, indicating a sunny day.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.6, "ram_available_mb": 100176.6, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 25595.9, "ram_available_mb": 100176.3, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.64, "peak": 14.91, "min": 14.02}, "VDD_GPU": {"avg": 30.28, "peak": 38.21, "min": 24.03}, "VIN": {"avg": 71.69, "peak": 116.8, "min": 54.31}, "VDD_CPU_SOC_MSS": {"avg": 15.18, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 30.28, "energy_joules_est": 55.39, "sample_count": 14, "duration_seconds": 1.829}, "timestamp": "2026-01-16T16:24:27.991954"}
{"image_index": 354, "image_name": "000000038829.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038829.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1314.384, "latencies_ms": [1314.384], "images_per_second": 0.761, "prompt_tokens": 9, "response_tokens_est": 27, "n_tiles": 6, "output_text": "A group of young students are riding bicycles in a city, with one of them wearing a white shirt and a white helmet.", "error": null, "sys_before": {"cpu_percent": 42.3, "ram_used_mb": 25595.6, "ram_available_mb": 100176.6, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 25595.8, "ram_available_mb": 100176.3, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.26, "peak": 14.71, "min": 13.82}, "VDD_GPU": {"avg": 32.57, "peak": 37.42, "min": 27.19}, "VIN": {"avg": 77.39, "peak": 110.87, "min": 50.73}, "VDD_CPU_SOC_MSS": {"avg": 15.05, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 32.57, "energy_joules_est": 42.83, "sample_count": 9, "duration_seconds": 1.315}, "timestamp": "2026-01-16T16:24:29.366774"}
{"image_index": 354, "image_name": "000000038829.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038829.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1678.206, "latencies_ms": [1678.206], "images_per_second": 0.596, "prompt_tokens": 23, "response_tokens_est": 39, "n_tiles": 6, "output_text": "1. Bicycle\n2. Motorcycle\n3. People\n4. Motorcycle\n5. Bicycle\n6. Motorcycle\n7. Bicycle\n8. Motorcycle", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25595.8, "ram_available_mb": 100176.3, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 25595.6, "ram_available_mb": 100176.6, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.75, "peak": 15.11, "min": 14.22}, "VDD_GPU": {"avg": 32.01, "peak": 39.0, "min": 25.21}, "VIN": {"avg": 74.9, "peak": 112.22, "min": 60.34}, "VDD_CPU_SOC_MSS": {"avg": 15.29, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 32.01, "energy_joules_est": 53.73, "sample_count": 12, "duration_seconds": 1.679}, "timestamp": "2026-01-16T16:24:31.051079"}
{"image_index": 354, "image_name": "000000038829.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038829.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3430.084, "latencies_ms": [3430.084], "images_per_second": 0.292, "prompt_tokens": 27, "response_tokens_est": 104, "n_tiles": 6, "output_text": "In the image, there is a green bicycle parked on the left side of the frame, with a person wearing a white shirt and dark pants seated on it. To the right of the bicycle, there is a black scooter with a person wearing a white helmet and a light-colored jacket seated on it. The background features a building with a sign that reads \"Coca-Cola,\" and there are other people walking around. The scooter and bicycle are positioned near the building, while the people are further away.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.6, "ram_available_mb": 100176.6, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 25595.6, "ram_available_mb": 100176.6, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.01, "peak": 15.32, "min": 14.22}, "VDD_GPU": {"avg": 27.4, "peak": 38.19, "min": 23.64}, "VIN": {"avg": 70.57, "peak": 128.31, "min": 56.44}, "VDD_CPU_SOC_MSS": {"avg": 15.57, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 27.4, "energy_joules_est": 94.0, "sample_count": 27, "duration_seconds": 3.431}, "timestamp": "2026-01-16T16:24:34.487603"}
{"image_index": 354, "image_name": "000000038829.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038829.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2747.242, "latencies_ms": [2747.242], "images_per_second": 0.364, "prompt_tokens": 21, "response_tokens_est": 79, "n_tiles": 6, "output_text": "The image depicts a bustling street scene in a city, likely in Southeast Asia, where a group of young individuals are riding bicycles. The setting is urban, with buildings and shops in the background, and the weather appears to be sunny. The individuals are dressed in casual attire, with some wearing white shirts and others in blue, suggesting they might be students or young professionals.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.6, "ram_available_mb": 100176.6, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 25595.8, "ram_available_mb": 100176.3, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.99, "peak": 15.22, "min": 14.32}, "VDD_GPU": {"avg": 28.32, "peak": 38.19, "min": 23.64}, "VIN": {"avg": 72.68, "peak": 108.18, "min": 62.45}, "VDD_CPU_SOC_MSS": {"avg": 15.54, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 28.32, "energy_joules_est": 77.81, "sample_count": 21, "duration_seconds": 2.748}, "timestamp": "2026-01-16T16:24:37.241082"}
{"image_index": 354, "image_name": "000000038829.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000038829.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2157.579, "latencies_ms": [2157.579], "images_per_second": 0.463, "prompt_tokens": 19, "response_tokens_est": 57, "n_tiles": 6, "output_text": "The image depicts a street scene with a mix of modern and traditional elements. Notable visual attributes include a green bicycle with a basket, a black scooter, and a white motorcycle. The lighting is natural, suggesting daytime, and the weather appears to be clear and sunny.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.8, "ram_available_mb": 100176.3, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 25595.6, "ram_available_mb": 100176.6, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.89, "peak": 15.22, "min": 14.32}, "VDD_GPU": {"avg": 29.81, "peak": 37.8, "min": 24.03}, "VIN": {"avg": 71.54, "peak": 120.55, "min": 61.86}, "VDD_CPU_SOC_MSS": {"avg": 15.43, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 29.81, "energy_joules_est": 64.33, "sample_count": 16, "duration_seconds": 2.158}, "timestamp": "2026-01-16T16:24:39.406238"}
{"image_index": 355, "image_name": "000000039405.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039405.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2455.15, "latencies_ms": [2455.15], "images_per_second": 0.407, "prompt_tokens": 9, "response_tokens_est": 31, "n_tiles": 12, "output_text": "A tennis player is preparing to hit a tennis ball on a well-maintained grass court, with spectators seated in the background watching the match.", "error": null, "sys_before": {"cpu_percent": 46.0, "ram_used_mb": 25595.6, "ram_available_mb": 100176.6, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 25595.8, "ram_available_mb": 100176.3, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.11, "peak": 16.15, "min": 13.79}, "VDD_GPU": {"avg": 34.95, "peak": 40.57, "min": 27.57}, "VIN": {"avg": 76.72, "peak": 116.68, "min": 56.96}, "VDD_CPU_SOC_MSS": {"avg": 15.94, "peak": 17.32, "min": 14.96}}, "power_watts_avg": 34.95, "energy_joules_est": 85.82, "sample_count": 18, "duration_seconds": 2.455}, "timestamp": "2026-01-16T16:24:41.954734"}
{"image_index": 355, "image_name": "000000039405.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039405.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2974.549, "latencies_ms": [2974.549], "images_per_second": 0.336, "prompt_tokens": 23, "response_tokens_est": 43, "n_tiles": 12, "output_text": "1. Tennis player\n2. Tennis racket\n3. Tennis ball\n4. Tennis court\n5. Stadium seating\n6. Spectators\n7. Green grass\n8. Person sitting on bench", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.8, "ram_available_mb": 100176.3, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 25595.8, "ram_available_mb": 100176.3, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.09, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 34.81, "peak": 42.54, "min": 26.79}, "VIN": {"avg": 75.03, "peak": 115.23, "min": 58.72}, "VDD_CPU_SOC_MSS": {"avg": 15.75, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 34.81, "energy_joules_est": 103.56, "sample_count": 22, "duration_seconds": 2.975}, "timestamp": "2026-01-16T16:24:44.935712"}
{"image_index": 355, "image_name": "000000039405.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039405.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 4437.751, "latencies_ms": [4437.751], "images_per_second": 0.225, "prompt_tokens": 27, "response_tokens_est": 86, "n_tiles": 12, "output_text": "The main objects in the image are a tennis court, a tennis player, and a crowd of spectators. The tennis player is positioned on the left side of the image, near the baseline, preparing to hit the tennis ball. The crowd of spectators is located in the background, seated on benches, watching the match. The tennis court is the central focus, with the player and the spectators forming the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.8, "ram_available_mb": 100176.3, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25595.6, "ram_available_mb": 100176.6, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.09, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 31.81, "peak": 42.53, "min": 26.0}, "VIN": {"avg": 72.05, "peak": 114.33, "min": 58.37}, "VDD_CPU_SOC_MSS": {"avg": 15.73, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 31.81, "energy_joules_est": 141.18, "sample_count": 34, "duration_seconds": 4.438}, "timestamp": "2026-01-16T16:24:49.381334"}
{"image_index": 355, "image_name": "000000039405.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039405.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3584.279, "latencies_ms": [3584.279], "images_per_second": 0.279, "prompt_tokens": 21, "response_tokens_est": 61, "n_tiles": 12, "output_text": "The image depicts a tennis match taking place on a well-maintained grass court, with spectators seated in the background. The players are actively engaged in the game, with one player in a white outfit preparing to hit the ball and another player in a white outfit ready to return the shot.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.6, "ram_available_mb": 100176.6, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25595.6, "ram_available_mb": 100176.6, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.1, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 33.04, "peak": 42.15, "min": 26.39}, "VIN": {"avg": 75.48, "peak": 114.01, "min": 59.35}, "VDD_CPU_SOC_MSS": {"avg": 15.63, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 33.04, "energy_joules_est": 118.43, "sample_count": 27, "duration_seconds": 3.585}, "timestamp": "2026-01-16T16:24:52.972262"}
{"image_index": 355, "image_name": "000000039405.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039405.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3144.788, "latencies_ms": [3144.788], "images_per_second": 0.318, "prompt_tokens": 19, "response_tokens_est": 48, "n_tiles": 12, "output_text": "The image depicts a tennis match taking place on a well-maintained grass court. The lighting is bright, likely from stadium lights, and the weather appears to be clear, with no visible signs of rain or adverse weather conditions.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 25595.6, "ram_available_mb": 100176.6, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 25595.8, "ram_available_mb": 100176.4, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.1, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 34.07, "peak": 42.92, "min": 26.39}, "VIN": {"avg": 74.67, "peak": 107.76, "min": 58.5}, "VDD_CPU_SOC_MSS": {"avg": 15.61, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 34.07, "energy_joules_est": 107.16, "sample_count": 24, "duration_seconds": 3.145}, "timestamp": "2026-01-16T16:24:56.124246"}
{"image_index": 356, "image_name": "000000039477.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039477.jpg", "image_width": 640, "image_height": 421, "image_resolution": "640x421", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1677.628, "latencies_ms": [1677.628], "images_per_second": 0.596, "prompt_tokens": 9, "response_tokens_est": 41, "n_tiles": 6, "output_text": "The image shows a cozy living room with a brown couch, a white cushion, a small plant on a table, a white sewing machine, a green plastic bag, and a window with white curtains.", "error": null, "sys_before": {"cpu_percent": 39.3, "ram_used_mb": 25595.8, "ram_available_mb": 100176.4, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25595.6, "ram_available_mb": 100176.6, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.54, "peak": 15.11, "min": 13.92}, "VDD_GPU": {"avg": 31.12, "peak": 37.82, "min": 24.82}, "VIN": {"avg": 74.39, "peak": 115.87, "min": 50.19}, "VDD_CPU_SOC_MSS": {"avg": 15.19, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 31.12, "energy_joules_est": 52.22, "sample_count": 12, "duration_seconds": 1.678}, "timestamp": "2026-01-16T16:24:57.867710"}
{"image_index": 356, "image_name": "000000039477.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039477.jpg", "image_width": 640, "image_height": 421, "image_resolution": "640x421", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2519.86, "latencies_ms": [2519.86], "images_per_second": 0.397, "prompt_tokens": 23, "response_tokens_est": 70, "n_tiles": 6, "output_text": "- TV stand: 1\n- TV: 1\n- Curtains: 1\n- Window: 1\n- Rug: 1\n- Pillow: 1\n- Cushion: 1\n- Potted plants: 2\n- Small plant: 1\n- Decorative item: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.6, "ram_available_mb": 100176.6, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 25595.6, "ram_available_mb": 100176.6, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.93, "peak": 15.22, "min": 14.22}, "VDD_GPU": {"avg": 28.9, "peak": 38.21, "min": 23.64}, "VIN": {"avg": 66.54, "peak": 102.59, "min": 58.2}, "VDD_CPU_SOC_MSS": {"avg": 15.48, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 28.9, "energy_joules_est": 72.84, "sample_count": 19, "duration_seconds": 2.52}, "timestamp": "2026-01-16T16:25:00.393788"}
{"image_index": 356, "image_name": "000000039477.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039477.jpg", "image_width": 640, "image_height": 421, "image_resolution": "640x421", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2918.422, "latencies_ms": [2918.422], "images_per_second": 0.343, "prompt_tokens": 27, "response_tokens_est": 82, "n_tiles": 6, "output_text": "The main objects in the image are a sewing machine, a small plant, a cushion, and a television stand. The sewing machine is positioned on the left side of the image, near the television stand. The small plant is placed on the right side of the sewing machine, near the cushion. The television stand is in the background, and the cushion is placed on the right side of the television stand.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.6, "ram_available_mb": 100176.6, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 25595.6, "ram_available_mb": 100176.6, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.83, "peak": 15.11, "min": 14.12}, "VDD_GPU": {"avg": 27.65, "peak": 37.82, "min": 23.24}, "VIN": {"avg": 64.38, "peak": 74.14, "min": 55.24}, "VDD_CPU_SOC_MSS": {"avg": 15.26, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 27.65, "energy_joules_est": 80.7, "sample_count": 22, "duration_seconds": 2.919}, "timestamp": "2026-01-16T16:25:03.318546"}
{"image_index": 356, "image_name": "000000039477.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039477.jpg", "image_width": 640, "image_height": 421, "image_resolution": "640x421", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2215.61, "latencies_ms": [2215.61], "images_per_second": 0.451, "prompt_tokens": 21, "response_tokens_est": 57, "n_tiles": 6, "output_text": "The image depicts a cozy living room with a small television on a wooden stand, a brown cushion on a sofa, a white cushion on a small table, and a plant on the floor. The room is well-lit with natural light coming through a window with white curtains.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.6, "ram_available_mb": 100176.6, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 25588.2, "ram_available_mb": 100184.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.74, "peak": 15.01, "min": 14.12}, "VDD_GPU": {"avg": 29.1, "peak": 38.21, "min": 23.24}, "VIN": {"avg": 71.98, "peak": 117.51, "min": 61.86}, "VDD_CPU_SOC_MSS": {"avg": 15.24, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 29.1, "energy_joules_est": 64.49, "sample_count": 17, "duration_seconds": 2.216}, "timestamp": "2026-01-16T16:25:05.540287"}
{"image_index": 356, "image_name": "000000039477.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039477.jpg", "image_width": 640, "image_height": 421, "image_resolution": "640x421", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1988.377, "latencies_ms": [1988.377], "images_per_second": 0.503, "prompt_tokens": 19, "response_tokens_est": 49, "n_tiles": 6, "output_text": "The room features a cozy and well-lit atmosphere with natural light streaming through the partially drawn white curtains. The walls are painted in a light color, and the furniture includes a brown sofa, a wooden coffee table, and a white cushion.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25588.2, "ram_available_mb": 100184.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 25588.2, "ram_available_mb": 100184.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.71, "peak": 15.01, "min": 14.12}, "VDD_GPU": {"avg": 29.73, "peak": 38.21, "min": 23.64}, "VIN": {"avg": 67.73, "peak": 109.81, "min": 60.59}, "VDD_CPU_SOC_MSS": {"avg": 15.19, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 29.73, "energy_joules_est": 59.13, "sample_count": 15, "duration_seconds": 1.989}, "timestamp": "2026-01-16T16:25:07.535154"}
{"image_index": 357, "image_name": "000000039480.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039480.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1518.728, "latencies_ms": [1518.728], "images_per_second": 0.658, "prompt_tokens": 9, "response_tokens_est": 35, "n_tiles": 6, "output_text": "A tennis player in a bright red outfit is holding a tennis racket and appears to be in a moment of celebration or reaction after a successful play on a clay court.", "error": null, "sys_before": {"cpu_percent": 40.0, "ram_used_mb": 25588.2, "ram_available_mb": 100184.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 25590.2, "ram_available_mb": 100182.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.52, "peak": 15.01, "min": 13.92}, "VDD_GPU": {"avg": 31.44, "peak": 37.42, "min": 25.6}, "VIN": {"avg": 72.36, "peak": 111.92, "min": 59.61}, "VDD_CPU_SOC_MSS": {"avg": 15.17, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 31.44, "energy_joules_est": 47.76, "sample_count": 11, "duration_seconds": 1.519}, "timestamp": "2026-01-16T16:25:09.101002"}
{"image_index": 357, "image_name": "000000039480.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039480.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1875.68, "latencies_ms": [1875.68], "images_per_second": 0.533, "prompt_tokens": 23, "response_tokens_est": 45, "n_tiles": 6, "output_text": "1. Tennis player\n2. Red clay court\n3. Tennis racket\n4. White visor\n5. Red dress\n6. White headband\n7. White tennis shoes\n8. Green padding", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25590.2, "ram_available_mb": 100182.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 25590.2, "ram_available_mb": 100182.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.7, "peak": 15.01, "min": 14.12}, "VDD_GPU": {"avg": 30.69, "peak": 38.59, "min": 24.03}, "VIN": {"avg": 73.29, "peak": 117.88, "min": 56.71}, "VDD_CPU_SOC_MSS": {"avg": 15.21, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 30.69, "energy_joules_est": 57.57, "sample_count": 14, "duration_seconds": 1.876}, "timestamp": "2026-01-16T16:25:10.982824"}
{"image_index": 357, "image_name": "000000039480.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039480.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2383.652, "latencies_ms": [2383.652], "images_per_second": 0.42, "prompt_tokens": 27, "response_tokens_est": 63, "n_tiles": 6, "output_text": "The main object in the foreground is a tennis player dressed in a bright orange dress, holding a tennis racket. The player is standing on a clay court, which is the main object in the background. The clay court is surrounded by a green fence and a green mat, which are also in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25590.2, "ram_available_mb": 100182.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 25590.4, "ram_available_mb": 100181.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.75, "peak": 15.01, "min": 14.02}, "VDD_GPU": {"avg": 28.93, "peak": 38.21, "min": 23.25}, "VIN": {"avg": 70.45, "peak": 113.34, "min": 56.75}, "VDD_CPU_SOC_MSS": {"avg": 15.24, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 28.93, "energy_joules_est": 68.97, "sample_count": 18, "duration_seconds": 2.384}, "timestamp": "2026-01-16T16:25:13.373339"}
{"image_index": 357, "image_name": "000000039480.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039480.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2778.535, "latencies_ms": [2778.535], "images_per_second": 0.36, "prompt_tokens": 21, "response_tokens_est": 77, "n_tiles": 6, "output_text": "The image captures a moment on a clay tennis court, where a female tennis player is in the midst of a match. She is dressed in a vibrant red outfit and is holding a tennis racket, poised to hit the ball. The court is marked with white lines, and the background features a green barrier and a crocodile logo, indicating the event's location and sponsors.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25590.4, "ram_available_mb": 100181.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 25590.2, "ram_available_mb": 100182.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.78, "peak": 15.01, "min": 14.02}, "VDD_GPU": {"avg": 27.95, "peak": 38.21, "min": 23.24}, "VIN": {"avg": 72.43, "peak": 116.75, "min": 61.0}, "VDD_CPU_SOC_MSS": {"avg": 15.3, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 27.95, "energy_joules_est": 77.67, "sample_count": 21, "duration_seconds": 2.779}, "timestamp": "2026-01-16T16:25:16.158074"}
{"image_index": 357, "image_name": "000000039480.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039480.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1827.421, "latencies_ms": [1827.421], "images_per_second": 0.547, "prompt_tokens": 19, "response_tokens_est": 43, "n_tiles": 6, "output_text": "The image features a red clay tennis court with a bright, natural light illuminating the scene. The surface of the court is smooth and appears to be well-maintained, with white boundary lines clearly visible.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25590.2, "ram_available_mb": 100182.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 25590.2, "ram_available_mb": 100182.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.66, "peak": 14.91, "min": 14.02}, "VDD_GPU": {"avg": 29.99, "peak": 37.8, "min": 23.64}, "VIN": {"avg": 72.23, "peak": 105.35, "min": 60.54}, "VDD_CPU_SOC_MSS": {"avg": 15.24, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 29.99, "energy_joules_est": 54.81, "sample_count": 14, "duration_seconds": 1.828}, "timestamp": "2026-01-16T16:25:17.991401"}
{"image_index": 358, "image_name": "000000039484.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039484.jpg", "image_width": 640, "image_height": 437, "image_resolution": "640x437", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1580.981, "latencies_ms": [1580.981], "images_per_second": 0.633, "prompt_tokens": 9, "response_tokens_est": 37, "n_tiles": 6, "output_text": "The image depicts a bustling urban street scene with a variety of parked cars, a busy intersection, and a mix of commercial establishments, including a restaurant and a theater.", "error": null, "sys_before": {"cpu_percent": 41.1, "ram_used_mb": 25590.2, "ram_available_mb": 100182.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 25590.1, "ram_available_mb": 100182.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.46, "peak": 15.11, "min": 13.82}, "VDD_GPU": {"avg": 30.66, "peak": 37.03, "min": 24.82}, "VIN": {"avg": 74.76, "peak": 123.5, "min": 54.37}, "VDD_CPU_SOC_MSS": {"avg": 15.16, "peak": 15.75, "min": 14.57}}, "power_watts_avg": 30.66, "energy_joules_est": 48.48, "sample_count": 12, "duration_seconds": 1.581}, "timestamp": "2026-01-16T16:25:19.633765"}
{"image_index": 358, "image_name": "000000039484.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039484.jpg", "image_width": 640, "image_height": 437, "image_resolution": "640x437", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 4071.565, "latencies_ms": [4071.565], "images_per_second": 0.246, "prompt_tokens": 23, "response_tokens_est": 128, "n_tiles": 6, "output_text": "object: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject:", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25590.1, "ram_available_mb": 100182.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25590.4, "ram_available_mb": 100181.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.06, "peak": 15.32, "min": 14.22}, "VDD_GPU": {"avg": 26.85, "peak": 38.6, "min": 23.64}, "VIN": {"avg": 66.33, "peak": 84.28, "min": 56.26}, "VDD_CPU_SOC_MSS": {"avg": 15.61, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 26.85, "energy_joules_est": 109.33, "sample_count": 31, "duration_seconds": 4.072}, "timestamp": "2026-01-16T16:25:23.712029"}
{"image_index": 358, "image_name": "000000039484.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039484.jpg", "image_width": 640, "image_height": 437, "image_resolution": "640x437", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2311.673, "latencies_ms": [2311.673], "images_per_second": 0.433, "prompt_tokens": 27, "response_tokens_est": 63, "n_tiles": 6, "output_text": "The main objects in the image are a street scene with various buildings, cars, and people. The foreground features a parked silver car, while the background includes a red building with a neon sign. The street is busy with cars and pedestrians, and there are street signs and a traffic light visible in the scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25590.4, "ram_available_mb": 100181.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 25590.1, "ram_available_mb": 100182.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.96, "peak": 15.32, "min": 14.22}, "VDD_GPU": {"avg": 29.31, "peak": 38.21, "min": 24.03}, "VIN": {"avg": 71.58, "peak": 102.51, "min": 61.12}, "VDD_CPU_SOC_MSS": {"avg": 15.54, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 29.31, "energy_joules_est": 67.77, "sample_count": 17, "duration_seconds": 2.312}, "timestamp": "2026-01-16T16:25:26.030030"}
{"image_index": 358, "image_name": "000000039484.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039484.jpg", "image_width": 640, "image_height": 437, "image_resolution": "640x437", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2927.297, "latencies_ms": [2927.297], "images_per_second": 0.342, "prompt_tokens": 21, "response_tokens_est": 86, "n_tiles": 6, "output_text": "The image depicts a bustling urban street scene with a variety of vehicles, including cars and a bus, lined up along the road. The street is flanked by buildings with different signs and storefronts, creating a lively and busy atmosphere. People can be seen sitting at outdoor tables, enjoying their time, while others are walking or standing near the buildings. The sky is overcast, suggesting a cloudy day.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25590.1, "ram_available_mb": 100182.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 25590.4, "ram_available_mb": 100181.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.05, "peak": 15.32, "min": 14.32}, "VDD_GPU": {"avg": 28.06, "peak": 37.82, "min": 23.64}, "VIN": {"avg": 69.87, "peak": 107.69, "min": 56.63}, "VDD_CPU_SOC_MSS": {"avg": 15.55, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 28.06, "energy_joules_est": 82.15, "sample_count": 22, "duration_seconds": 2.928}, "timestamp": "2026-01-16T16:25:28.963347"}
{"image_index": 358, "image_name": "000000039484.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039484.jpg", "image_width": 640, "image_height": 437, "image_resolution": "640x437", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2362.035, "latencies_ms": [2362.035], "images_per_second": 0.423, "prompt_tokens": 19, "response_tokens_est": 65, "n_tiles": 6, "output_text": "The image depicts a bustling urban street scene with a variety of notable visual attributes. The buildings are primarily brick and red, with some featuring green awnings. The street is lined with parked cars, and there are several streetlights and traffic signs visible. The sky is overcast, suggesting a cloudy day.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25590.4, "ram_available_mb": 100181.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 25590.3, "ram_available_mb": 100181.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.98, "peak": 15.32, "min": 14.32}, "VDD_GPU": {"avg": 29.15, "peak": 37.82, "min": 23.64}, "VIN": {"avg": 70.18, "peak": 111.37, "min": 52.3}, "VDD_CPU_SOC_MSS": {"avg": 15.48, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 29.15, "energy_joules_est": 68.87, "sample_count": 18, "duration_seconds": 2.363}, "timestamp": "2026-01-16T16:25:31.331793"}
{"image_index": 359, "image_name": "000000039551.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039551.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1520.71, "latencies_ms": [1520.71], "images_per_second": 0.658, "prompt_tokens": 9, "response_tokens_est": 35, "n_tiles": 6, "output_text": "A female tennis player is captured mid-action on a brightly lit tennis court, with her racket in hand and her body leaning forward in preparation to hit the ball.", "error": null, "sys_before": {"cpu_percent": 42.5, "ram_used_mb": 25590.3, "ram_available_mb": 100181.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25590.6, "ram_available_mb": 100181.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.48, "peak": 15.01, "min": 13.92}, "VDD_GPU": {"avg": 31.26, "peak": 36.63, "min": 25.6}, "VIN": {"avg": 73.59, "peak": 105.8, "min": 58.79}, "VDD_CPU_SOC_MSS": {"avg": 15.14, "peak": 15.75, "min": 14.57}}, "power_watts_avg": 31.26, "energy_joules_est": 47.55, "sample_count": 11, "duration_seconds": 1.521}, "timestamp": "2026-01-16T16:25:32.911435"}
{"image_index": 359, "image_name": "000000039551.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039551.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1754.93, "latencies_ms": [1754.93], "images_per_second": 0.57, "prompt_tokens": 23, "response_tokens_est": 42, "n_tiles": 6, "output_text": "1. Tennis player\n2. Tennis racket\n3. Tennis ball\n4. Tennis court\n5. Tennis net\n6. Tennis shoes\n7. Tennis outfit\n8. Tennis visor", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25590.6, "ram_available_mb": 100181.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25590.6, "ram_available_mb": 100181.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.81, "peak": 15.11, "min": 14.22}, "VDD_GPU": {"avg": 31.42, "peak": 38.21, "min": 24.82}, "VIN": {"avg": 75.79, "peak": 110.94, "min": 54.3}, "VDD_CPU_SOC_MSS": {"avg": 15.32, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 31.42, "energy_joules_est": 55.15, "sample_count": 13, "duration_seconds": 1.755}, "timestamp": "2026-01-16T16:25:34.672336"}
{"image_index": 359, "image_name": "000000039551.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039551.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2691.507, "latencies_ms": [2691.507], "images_per_second": 0.372, "prompt_tokens": 27, "response_tokens_est": 77, "n_tiles": 6, "output_text": "The main object in the foreground is a tennis player, who is positioned on the left side of the image. The tennis player is holding a tennis racket and appears to be preparing for a serve. The background consists of a green tennis court, with a yellow tennis ball visible near the player. The player's shadow is cast on the court, indicating the direction of the sunlight.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25590.6, "ram_available_mb": 100181.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 25590.6, "ram_available_mb": 100181.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.98, "peak": 15.32, "min": 14.22}, "VDD_GPU": {"avg": 28.54, "peak": 38.19, "min": 23.64}, "VIN": {"avg": 71.68, "peak": 111.83, "min": 58.11}, "VDD_CPU_SOC_MSS": {"avg": 15.47, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 28.54, "energy_joules_est": 76.83, "sample_count": 20, "duration_seconds": 2.692}, "timestamp": "2026-01-16T16:25:37.370353"}
{"image_index": 359, "image_name": "000000039551.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039551.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2764.289, "latencies_ms": [2764.289], "images_per_second": 0.362, "prompt_tokens": 21, "response_tokens_est": 80, "n_tiles": 6, "output_text": "The image captures a moment on a tennis court during a match. The player, dressed in a red outfit, is in the midst of a serve, with her racket in motion and her body leaning forward. The court is marked with white lines, and the surface appears to be a standard hard court. The lighting suggests it is daytime, and the player's shadow is visible on the court.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25590.6, "ram_available_mb": 100181.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 25590.6, "ram_available_mb": 100181.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.0, "peak": 15.32, "min": 14.32}, "VDD_GPU": {"avg": 28.42, "peak": 38.21, "min": 23.64}, "VIN": {"avg": 68.13, "peak": 112.88, "min": 55.15}, "VDD_CPU_SOC_MSS": {"avg": 15.5, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 28.42, "energy_joules_est": 78.58, "sample_count": 21, "duration_seconds": 2.765}, "timestamp": "2026-01-16T16:25:40.141225"}
{"image_index": 359, "image_name": "000000039551.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039551.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1658.736, "latencies_ms": [1658.736], "images_per_second": 0.603, "prompt_tokens": 19, "response_tokens_est": 39, "n_tiles": 6, "output_text": "The tennis player is wearing a red outfit, which stands out against the green and blue court. The lighting is bright, indicating it is a sunny day, casting clear shadows on the court.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25590.6, "ram_available_mb": 100181.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 25590.6, "ram_available_mb": 100181.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.88, "peak": 15.22, "min": 14.22}, "VDD_GPU": {"avg": 31.51, "peak": 37.82, "min": 25.21}, "VIN": {"avg": 72.8, "peak": 106.59, "min": 59.77}, "VDD_CPU_SOC_MSS": {"avg": 15.38, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 31.51, "energy_joules_est": 52.28, "sample_count": 12, "duration_seconds": 1.659}, "timestamp": "2026-01-16T16:25:41.806041"}
{"image_index": 360, "image_name": "000000039670.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039670.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 727.277, "latencies_ms": [727.277], "images_per_second": 1.375, "prompt_tokens": 9, "response_tokens_est": 14, "n_tiles": 4, "output_text": "A train is traveling on a track surrounded by lush greenery.", "error": null, "sys_before": {"cpu_percent": 40.8, "ram_used_mb": 25590.6, "ram_available_mb": 100181.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25590.6, "ram_available_mb": 100181.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4606.7, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5444.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.07, "peak": 14.4, "min": 13.72}, "VDD_GPU": {"avg": 31.91, "peak": 34.66, "min": 28.76}, "VIN": {"avg": 71.8, "peak": 88.9, "min": 57.67}, "VDD_CPU_SOC_MSS": {"avg": 14.72, "peak": 14.96, "min": 14.17}}, "power_watts_avg": 31.91, "energy_joules_est": 23.22, "sample_count": 5, "duration_seconds": 0.728}, "timestamp": "2026-01-16T16:25:42.584135"}
{"image_index": 360, "image_name": "000000039670.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039670.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 3544.171, "latencies_ms": [3544.171], "images_per_second": 0.282, "prompt_tokens": 23, "response_tokens_est": 128, "n_tiles": 4, "output_text": "object: train\nobject: train car\nobject: train tracks\nobject: greenery\nobject: bushes\nobject: trees\nobject: train engine\nobject: train car\nobject: train car doors\nobject: train car windows\nobject: train car doors\nobject: train car windows\nobject: train car doors\nobject: train car windows\nobject: train car doors\nobject: train car windows\nobject: train car doors\nobject: train car windows\nobject: train car doors\nobject: train car windows\nobject: train car doors\nobject: train car windows\nobject: train car doors", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25590.6, "ram_available_mb": 100181.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 25590.8, "ram_available_mb": 100181.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4606.7, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5455.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.03, "peak": 15.32, "min": 13.82}, "VDD_GPU": {"avg": 25.39, "peak": 37.03, "min": 22.45}, "VIN": {"avg": 64.2, "peak": 88.84, "min": 56.18}, "VDD_CPU_SOC_MSS": {"avg": 15.56, "peak": 16.14, "min": 14.56}}, "power_watts_avg": 25.39, "energy_joules_est": 90.0, "sample_count": 27, "duration_seconds": 3.545}, "timestamp": "2026-01-16T16:25:46.135207"}
{"image_index": 360, "image_name": "000000039670.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039670.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1291.311, "latencies_ms": [1291.311], "images_per_second": 0.774, "prompt_tokens": 27, "response_tokens_est": 37, "n_tiles": 4, "output_text": "The train is positioned on the left side of the image, with the tracks running parallel to it. The train is in the foreground, and the surrounding vegetation is in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25590.8, "ram_available_mb": 100181.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 25591.1, "ram_available_mb": 100181.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4606.7, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5458.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.66, "peak": 15.01, "min": 14.12}, "VDD_GPU": {"avg": 29.24, "peak": 35.06, "min": 24.03}, "VIN": {"avg": 68.8, "peak": 82.18, "min": 62.21}, "VDD_CPU_SOC_MSS": {"avg": 15.31, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 29.24, "energy_joules_est": 37.76, "sample_count": 9, "duration_seconds": 1.292}, "timestamp": "2026-01-16T16:25:47.436433"}
{"image_index": 360, "image_name": "000000039670.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039670.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1483.963, "latencies_ms": [1483.963], "images_per_second": 0.674, "prompt_tokens": 21, "response_tokens_est": 45, "n_tiles": 4, "output_text": "The image depicts a train traveling on a curving railway track surrounded by lush greenery. The train, with its blue and orange color scheme, is moving through a natural environment, possibly a rural or suburban area.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25591.1, "ram_available_mb": 100181.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 25590.8, "ram_available_mb": 100181.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4606.7, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5453.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.73, "peak": 15.22, "min": 14.02}, "VDD_GPU": {"avg": 28.4, "peak": 35.06, "min": 23.24}, "VIN": {"avg": 64.96, "peak": 72.65, "min": 57.85}, "VDD_CPU_SOC_MSS": {"avg": 15.35, "peak": 15.75, "min": 14.57}}, "power_watts_avg": 28.4, "energy_joules_est": 42.15, "sample_count": 11, "duration_seconds": 1.484}, "timestamp": "2026-01-16T16:25:48.926289"}
{"image_index": 360, "image_name": "000000039670.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039670.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1380.81, "latencies_ms": [1380.81], "images_per_second": 0.724, "prompt_tokens": 19, "response_tokens_est": 41, "n_tiles": 4, "output_text": "The train in the image is painted in a combination of blue and orange, with a white roof. The lighting is natural, suggesting it is daytime, and the weather appears to be clear and sunny.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25590.8, "ram_available_mb": 100181.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 25590.8, "ram_available_mb": 100181.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4606.7, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5451.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.78, "peak": 15.22, "min": 14.12}, "VDD_GPU": {"avg": 28.8, "peak": 35.06, "min": 23.64}, "VIN": {"avg": 66.07, "peak": 92.06, "min": 51.84}, "VDD_CPU_SOC_MSS": {"avg": 15.35, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 28.8, "energy_joules_est": 39.78, "sample_count": 10, "duration_seconds": 1.381}, "timestamp": "2026-01-16T16:25:50.313450"}
{"image_index": 361, "image_name": "000000039769.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039769.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2051.511, "latencies_ms": [2051.511], "images_per_second": 0.487, "prompt_tokens": 9, "response_tokens_est": 19, "n_tiles": 12, "output_text": "A tabby cat is lying on a pink blanket, appearing to be sleeping peacefully.", "error": null, "sys_before": {"cpu_percent": 49.6, "ram_used_mb": 25590.8, "ram_available_mb": 100181.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 2.3, "ram_used_mb": 25590.8, "ram_available_mb": 100181.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.16, "peak": 16.15, "min": 14.0}, "VDD_GPU": {"avg": 36.34, "peak": 40.97, "min": 29.54}, "VIN": {"avg": 74.97, "peak": 108.71, "min": 58.31}, "VDD_CPU_SOC_MSS": {"avg": 16.01, "peak": 17.32, "min": 14.96}}, "power_watts_avg": 36.34, "energy_joules_est": 74.57, "sample_count": 15, "duration_seconds": 2.052}, "timestamp": "2026-01-16T16:25:52.451136"}
{"image_index": 361, "image_name": "000000039769.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039769.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2106.232, "latencies_ms": [2106.232], "images_per_second": 0.475, "prompt_tokens": 23, "response_tokens_est": 17, "n_tiles": 12, "output_text": "cat: 1\nremote control: 2\nblanket: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25590.8, "ram_available_mb": 100181.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 2.3, "ram_used_mb": 25591.0, "ram_available_mb": 100181.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.08, "peak": 15.65, "min": 14.43}, "VDD_GPU": {"avg": 37.69, "peak": 42.94, "min": 29.94}, "VIN": {"avg": 77.03, "peak": 117.31, "min": 59.34}, "VDD_CPU_SOC_MSS": {"avg": 15.84, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 37.69, "energy_joules_est": 79.41, "sample_count": 16, "duration_seconds": 2.107}, "timestamp": "2026-01-16T16:25:54.564254"}
{"image_index": 361, "image_name": "000000039769.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039769.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3915.667, "latencies_ms": [3915.667], "images_per_second": 0.255, "prompt_tokens": 27, "response_tokens_est": 71, "n_tiles": 12, "output_text": "The main object in the foreground is a cat lying on a pink blanket. The cat is positioned near the center of the image, with its body facing the left side of the frame. In the background, there is a white remote control lying on the same pink blanket. The remote control is positioned to the left of the cat, slightly behind it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25591.0, "ram_available_mb": 100181.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25590.8, "ram_available_mb": 100181.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.09, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 32.86, "peak": 43.73, "min": 26.0}, "VIN": {"avg": 73.16, "peak": 103.74, "min": 58.39}, "VDD_CPU_SOC_MSS": {"avg": 15.64, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 32.86, "energy_joules_est": 128.68, "sample_count": 30, "duration_seconds": 3.916}, "timestamp": "2026-01-16T16:25:58.486873"}
{"image_index": 361, "image_name": "000000039769.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039769.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 4117.749, "latencies_ms": [4117.749], "images_per_second": 0.243, "prompt_tokens": 21, "response_tokens_est": 75, "n_tiles": 12, "output_text": "The image depicts a serene scene of a tabby cat lying on a pink blanket, seemingly asleep. The cat's body is stretched out, with its head resting on the blanket, while its paws are visible on the surface. The setting appears to be a cozy, indoor environment, possibly a living room or bedroom, with a soft, warm ambiance.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25590.8, "ram_available_mb": 100181.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 25590.8, "ram_available_mb": 100181.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.89, "peak": 15.65, "min": 14.43}, "VDD_GPU": {"avg": 31.83, "peak": 42.15, "min": 25.6}, "VIN": {"avg": 73.17, "peak": 112.43, "min": 58.1}, "VDD_CPU_SOC_MSS": {"avg": 15.33, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 31.83, "energy_joules_est": 131.08, "sample_count": 32, "duration_seconds": 4.118}, "timestamp": "2026-01-16T16:26:02.612208"}
{"image_index": 361, "image_name": "000000039769.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039769.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 4053.353, "latencies_ms": [4053.353], "images_per_second": 0.247, "prompt_tokens": 19, "response_tokens_est": 74, "n_tiles": 12, "output_text": "The image features a tabby cat with a striped pattern, predominantly in shades of brown, black, and white. The cat is resting on a pink blanket, which is soft and appears to be made of a plush fabric. The lighting in the image is bright, casting a warm glow on the cat and the blanket, creating a cozy and inviting atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25590.8, "ram_available_mb": 100181.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25590.8, "ram_available_mb": 100181.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.74, "peak": 15.34, "min": 14.12}, "VDD_GPU": {"avg": 32.03, "peak": 42.54, "min": 25.6}, "VIN": {"avg": 70.81, "peak": 122.09, "min": 58.11}, "VDD_CPU_SOC_MSS": {"avg": 15.23, "peak": 16.14, "min": 14.56}}, "power_watts_avg": 32.03, "energy_joules_est": 129.84, "sample_count": 31, "duration_seconds": 4.054}, "timestamp": "2026-01-16T16:26:06.672130"}
{"image_index": 362, "image_name": "000000039785.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039785.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2148.069, "latencies_ms": [2148.069], "images_per_second": 0.466, "prompt_tokens": 9, "response_tokens_est": 22, "n_tiles": 12, "output_text": "A man is surfing in a river, wearing a black wetsuit and holding a blue surfboard.", "error": null, "sys_before": {"cpu_percent": 43.9, "ram_used_mb": 25590.8, "ram_available_mb": 100181.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 2.5, "ram_used_mb": 25591.0, "ram_available_mb": 100181.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.1, "peak": 16.15, "min": 13.69}, "VDD_GPU": {"avg": 35.97, "peak": 41.36, "min": 28.76}, "VIN": {"avg": 81.08, "peak": 106.2, "min": 58.52}, "VDD_CPU_SOC_MSS": {"avg": 16.02, "peak": 17.32, "min": 14.96}}, "power_watts_avg": 35.97, "energy_joules_est": 77.28, "sample_count": 16, "duration_seconds": 2.149}, "timestamp": "2026-01-16T16:26:08.911105"}
{"image_index": 362, "image_name": "000000039785.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039785.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2473.417, "latencies_ms": [2473.417], "images_per_second": 0.404, "prompt_tokens": 23, "response_tokens_est": 28, "n_tiles": 12, "output_text": "surfer: 1\nwetsuit: 1\nboard: 1\nbench: 1\nriver: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25591.0, "ram_available_mb": 100181.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 2.5, "ram_used_mb": 25590.8, "ram_available_mb": 100181.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.08, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 36.45, "peak": 43.33, "min": 27.97}, "VIN": {"avg": 79.27, "peak": 131.47, "min": 56.1}, "VDD_CPU_SOC_MSS": {"avg": 15.7, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 36.45, "energy_joules_est": 90.18, "sample_count": 19, "duration_seconds": 2.474}, "timestamp": "2026-01-16T16:26:11.391974"}
{"image_index": 362, "image_name": "000000039785.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039785.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3415.264, "latencies_ms": [3415.264], "images_per_second": 0.293, "prompt_tokens": 27, "response_tokens_est": 56, "n_tiles": 12, "output_text": "The main objects in the image are a river, a man, and a bench. The river is in the foreground, with the man standing on a surfboard near the water's edge. The bench is located further back in the scene, providing a resting spot for people.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25590.8, "ram_available_mb": 100181.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 25591.0, "ram_available_mb": 100181.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.1, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 33.9, "peak": 43.31, "min": 26.39}, "VIN": {"avg": 79.06, "peak": 131.73, "min": 58.43}, "VDD_CPU_SOC_MSS": {"avg": 15.67, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 33.9, "energy_joules_est": 115.79, "sample_count": 26, "duration_seconds": 3.416}, "timestamp": "2026-01-16T16:26:14.813814"}
{"image_index": 362, "image_name": "000000039785.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039785.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3045.814, "latencies_ms": [3045.814], "images_per_second": 0.328, "prompt_tokens": 21, "response_tokens_est": 45, "n_tiles": 12, "output_text": "The image depicts a scene at a riverbank with a man in a black wetsuit standing on a surfboard, facing the river. The setting is outdoors, with trees and a bench visible in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25591.0, "ram_available_mb": 100181.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 25590.8, "ram_available_mb": 100181.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.1, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 34.42, "peak": 42.15, "min": 26.39}, "VIN": {"avg": 75.65, "peak": 144.6, "min": 54.97}, "VDD_CPU_SOC_MSS": {"avg": 15.73, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 34.42, "energy_joules_est": 104.85, "sample_count": 23, "duration_seconds": 3.046}, "timestamp": "2026-01-16T16:26:17.867518"}
{"image_index": 362, "image_name": "000000039785.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039785.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3749.765, "latencies_ms": [3749.765], "images_per_second": 0.267, "prompt_tokens": 19, "response_tokens_est": 66, "n_tiles": 12, "output_text": "The image depicts a scene with a man in a black wetsuit surfing on a turbulent river. The river is brownish, and the man is standing on a surfboard. The lighting is natural, suggesting it is daytime, and the weather appears to be windy, as indicated by the choppy water.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25590.8, "ram_available_mb": 100181.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25590.8, "ram_available_mb": 100181.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.1, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 32.89, "peak": 42.53, "min": 26.39}, "VIN": {"avg": 74.21, "peak": 113.82, "min": 58.78}, "VDD_CPU_SOC_MSS": {"avg": 15.7, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 32.89, "energy_joules_est": 123.34, "sample_count": 28, "duration_seconds": 3.75}, "timestamp": "2026-01-16T16:26:21.627707"}
{"image_index": 363, "image_name": "000000039914.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039914.jpg", "image_width": 487, "image_height": 640, "image_resolution": "487x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2387.421, "latencies_ms": [2387.421], "images_per_second": 0.419, "prompt_tokens": 9, "response_tokens_est": 29, "n_tiles": 12, "output_text": "A woman and a child are standing on a grassy field, with the woman holding a colorful kite that is flying in the air.", "error": null, "sys_before": {"cpu_percent": 40.0, "ram_used_mb": 25590.8, "ram_available_mb": 100181.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 2.6, "ram_used_mb": 25590.8, "ram_available_mb": 100181.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.11, "peak": 16.05, "min": 13.79}, "VDD_GPU": {"avg": 35.4, "peak": 41.34, "min": 27.97}, "VIN": {"avg": 76.27, "peak": 114.2, "min": 58.52}, "VDD_CPU_SOC_MSS": {"avg": 15.81, "peak": 16.92, "min": 14.96}}, "power_watts_avg": 35.4, "energy_joules_est": 84.53, "sample_count": 18, "duration_seconds": 2.388}, "timestamp": "2026-01-16T16:26:24.107077"}
{"image_index": 363, "image_name": "000000039914.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039914.jpg", "image_width": 487, "image_height": 640, "image_resolution": "487x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2609.246, "latencies_ms": [2609.246], "images_per_second": 0.383, "prompt_tokens": 23, "response_tokens_est": 32, "n_tiles": 12, "output_text": "1. Person\n2. Person\n3. Person\n4. Person\n5. Person\n6. Person\n7. Person\n8. Person", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25590.8, "ram_available_mb": 100181.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 25590.8, "ram_available_mb": 100181.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.07, "peak": 15.65, "min": 14.43}, "VDD_GPU": {"avg": 35.76, "peak": 42.94, "min": 27.57}, "VIN": {"avg": 75.78, "peak": 116.42, "min": 46.85}, "VDD_CPU_SOC_MSS": {"avg": 15.69, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 35.76, "energy_joules_est": 93.33, "sample_count": 20, "duration_seconds": 2.61}, "timestamp": "2026-01-16T16:26:26.722874"}
{"image_index": 363, "image_name": "000000039914.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039914.jpg", "image_width": 487, "image_height": 640, "image_resolution": "487x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3922.9, "latencies_ms": [3922.9], "images_per_second": 0.255, "prompt_tokens": 27, "response_tokens_est": 71, "n_tiles": 12, "output_text": "The main objects in the image are a woman and a child, both standing on the grass. The woman is wearing a black jacket and blue jeans, while the child is wearing a pink top and pink pants. The child is holding a colorful kite, which is near the woman. The kite is in the background, slightly out of focus.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25590.8, "ram_available_mb": 100181.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25590.7, "ram_available_mb": 100181.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.1, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 32.67, "peak": 43.33, "min": 26.39}, "VIN": {"avg": 77.53, "peak": 148.16, "min": 51.03}, "VDD_CPU_SOC_MSS": {"avg": 15.67, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 32.67, "energy_joules_est": 128.18, "sample_count": 31, "duration_seconds": 3.923}, "timestamp": "2026-01-16T16:26:30.652303"}
{"image_index": 363, "image_name": "000000039914.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039914.jpg", "image_width": 487, "image_height": 640, "image_resolution": "487x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3450.323, "latencies_ms": [3450.323], "images_per_second": 0.29, "prompt_tokens": 21, "response_tokens_est": 57, "n_tiles": 12, "output_text": "The image depicts a serene park scene with a woman and a child standing on a grassy field. The woman is holding a colorful kite, while the child is watching it fly. The setting is a peaceful park with trees and a clear blue sky in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25590.7, "ram_available_mb": 100181.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25590.7, "ram_available_mb": 100181.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.1, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 33.31, "peak": 42.54, "min": 26.39}, "VIN": {"avg": 74.83, "peak": 115.14, "min": 58.09}, "VDD_CPU_SOC_MSS": {"avg": 15.75, "peak": 16.53, "min": 15.35}}, "power_watts_avg": 33.31, "energy_joules_est": 114.95, "sample_count": 27, "duration_seconds": 3.451}, "timestamp": "2026-01-16T16:26:34.108805"}
{"image_index": 363, "image_name": "000000039914.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039914.jpg", "image_width": 487, "image_height": 640, "image_resolution": "487x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2910.81, "latencies_ms": [2910.81], "images_per_second": 0.344, "prompt_tokens": 19, "response_tokens_est": 41, "n_tiles": 12, "output_text": "The image depicts a vibrant outdoor scene with a clear blue sky, lush green grass, and a few bare trees in the background. The sunlight casts soft shadows, creating a warm and inviting atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25590.7, "ram_available_mb": 100181.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 25590.7, "ram_available_mb": 100181.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.08, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 34.95, "peak": 42.92, "min": 26.79}, "VIN": {"avg": 72.09, "peak": 116.31, "min": 53.45}, "VDD_CPU_SOC_MSS": {"avg": 15.69, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 34.95, "energy_joules_est": 101.74, "sample_count": 22, "duration_seconds": 2.911}, "timestamp": "2026-01-16T16:26:37.029983"}
{"image_index": 364, "image_name": "000000039951.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039951.jpg", "image_width": 640, "image_height": 445, "image_resolution": "640x445", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1284.437, "latencies_ms": [1284.437], "images_per_second": 0.779, "prompt_tokens": 9, "response_tokens_est": 26, "n_tiles": 6, "output_text": "A young tennis player is captured mid-action on a court, with a tennis ball in the air, ready to be hit.", "error": null, "sys_before": {"cpu_percent": 43.2, "ram_used_mb": 25590.7, "ram_available_mb": 100181.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 25590.7, "ram_available_mb": 100181.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.31, "peak": 14.81, "min": 13.92}, "VDD_GPU": {"avg": 33.04, "peak": 38.21, "min": 27.18}, "VIN": {"avg": 75.96, "peak": 124.96, "min": 64.24}, "VDD_CPU_SOC_MSS": {"avg": 15.0, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 33.04, "energy_joules_est": 42.45, "sample_count": 9, "duration_seconds": 1.285}, "timestamp": "2026-01-16T16:26:38.378860"}
{"image_index": 364, "image_name": "000000039951.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039951.jpg", "image_width": 640, "image_height": 445, "image_resolution": "640x445", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1858.277, "latencies_ms": [1858.277], "images_per_second": 0.538, "prompt_tokens": 23, "response_tokens_est": 46, "n_tiles": 6, "output_text": "1. Tennis player\n2. Tennis racket\n3. Tennis ball\n4. Green net\n5. Green tarp\n6. Blue tennis court\n7. White net\n8. Green tarpaulin", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25590.7, "ram_available_mb": 100181.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 25590.7, "ram_available_mb": 100181.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.82, "peak": 15.22, "min": 14.22}, "VDD_GPU": {"avg": 31.23, "peak": 38.98, "min": 24.42}, "VIN": {"avg": 71.87, "peak": 113.15, "min": 59.95}, "VDD_CPU_SOC_MSS": {"avg": 15.35, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 31.23, "energy_joules_est": 58.05, "sample_count": 14, "duration_seconds": 1.859}, "timestamp": "2026-01-16T16:26:40.243677"}
{"image_index": 364, "image_name": "000000039951.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039951.jpg", "image_width": 640, "image_height": 445, "image_resolution": "640x445", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2444.035, "latencies_ms": [2444.035], "images_per_second": 0.409, "prompt_tokens": 27, "response_tokens_est": 65, "n_tiles": 6, "output_text": "The main object in the foreground is a young tennis player, who is in the process of hitting a tennis ball. The player is positioned near the net, which is white and blue. The background features a green tarpaulin, likely covering the court. The net is also visible, running horizontally across the image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25590.7, "ram_available_mb": 100181.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 25591.0, "ram_available_mb": 100181.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.79, "peak": 15.01, "min": 14.12}, "VDD_GPU": {"avg": 28.8, "peak": 38.21, "min": 23.24}, "VIN": {"avg": 70.21, "peak": 104.83, "min": 59.46}, "VDD_CPU_SOC_MSS": {"avg": 15.39, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 28.8, "energy_joules_est": 70.4, "sample_count": 19, "duration_seconds": 2.444}, "timestamp": "2026-01-16T16:26:42.694032"}
{"image_index": 364, "image_name": "000000039951.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039951.jpg", "image_width": 640, "image_height": 445, "image_resolution": "640x445", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3139.111, "latencies_ms": [3139.111], "images_per_second": 0.319, "prompt_tokens": 21, "response_tokens_est": 90, "n_tiles": 6, "output_text": "The image captures a young tennis player in mid-action on a tennis court, with a net dividing the court. The player is wearing a white shirt, black shorts, and a red cap, and is focused on hitting a yellow tennis ball. The court is marked with white lines, and there is a green tarp covering part of the court. The setting appears to be a tennis tournament or practice session, with banners and signs in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25591.0, "ram_available_mb": 100181.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 25590.7, "ram_available_mb": 100181.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.84, "peak": 15.11, "min": 14.02}, "VDD_GPU": {"avg": 27.31, "peak": 38.21, "min": 23.24}, "VIN": {"avg": 66.52, "peak": 84.21, "min": 59.14}, "VDD_CPU_SOC_MSS": {"avg": 15.31, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 27.31, "energy_joules_est": 85.74, "sample_count": 25, "duration_seconds": 3.14}, "timestamp": "2026-01-16T16:26:45.839228"}
{"image_index": 364, "image_name": "000000039951.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039951.jpg", "image_width": 640, "image_height": 445, "image_resolution": "640x445", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2014.817, "latencies_ms": [2014.817], "images_per_second": 0.496, "prompt_tokens": 19, "response_tokens_est": 50, "n_tiles": 6, "output_text": "The image depicts a young tennis player in action on a court, wearing a white shirt, black shorts, and a red cap. The court surface is blue, and the net is white. The lighting is bright, indicating a sunny day.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25590.7, "ram_available_mb": 100181.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 25590.7, "ram_available_mb": 100181.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.7, "peak": 15.01, "min": 14.02}, "VDD_GPU": {"avg": 29.96, "peak": 38.21, "min": 23.64}, "VIN": {"avg": 72.46, "peak": 115.2, "min": 55.54}, "VDD_CPU_SOC_MSS": {"avg": 15.2, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 29.96, "energy_joules_est": 60.38, "sample_count": 15, "duration_seconds": 2.015}, "timestamp": "2026-01-16T16:26:47.864625"}
{"image_index": 365, "image_name": "000000039956.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039956.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2216.89, "latencies_ms": [2216.89], "images_per_second": 0.451, "prompt_tokens": 9, "response_tokens_est": 61, "n_tiles": 6, "output_text": "The image depicts a cluttered room with various items scattered around, including a bed with a brown blanket, a wooden nightstand, a wooden chair, and a pile of boxes and bags, suggesting that the room is in the process of being unpacked or is in a state of disarray.", "error": null, "sys_before": {"cpu_percent": 40.3, "ram_used_mb": 25590.7, "ram_available_mb": 100181.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 25590.7, "ram_available_mb": 100181.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.66, "peak": 15.22, "min": 13.82}, "VDD_GPU": {"avg": 28.87, "peak": 37.03, "min": 24.03}, "VIN": {"avg": 72.54, "peak": 119.1, "min": 53.18}, "VDD_CPU_SOC_MSS": {"avg": 15.26, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 28.87, "energy_joules_est": 64.01, "sample_count": 17, "duration_seconds": 2.217}, "timestamp": "2026-01-16T16:26:50.141685"}
{"image_index": 365, "image_name": "000000039956.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039956.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1855.648, "latencies_ms": [1855.648], "images_per_second": 0.539, "prompt_tokens": 23, "response_tokens_est": 46, "n_tiles": 6, "output_text": "bed: 1\nclothes: 1\nbox: 1\ntrash can: 1\nchair: 1\ndrawer: 1\ncabinet: 1\ntable: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25590.7, "ram_available_mb": 100181.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25590.7, "ram_available_mb": 100181.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.86, "peak": 15.22, "min": 14.22}, "VDD_GPU": {"avg": 30.67, "peak": 38.21, "min": 24.42}, "VIN": {"avg": 70.61, "peak": 98.36, "min": 58.73}, "VDD_CPU_SOC_MSS": {"avg": 15.41, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 30.67, "energy_joules_est": 56.93, "sample_count": 14, "duration_seconds": 1.856}, "timestamp": "2026-01-16T16:26:52.003275"}
{"image_index": 365, "image_name": "000000039956.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039956.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2183.343, "latencies_ms": [2183.343], "images_per_second": 0.458, "prompt_tokens": 27, "response_tokens_est": 58, "n_tiles": 6, "output_text": "The main objects in the image are scattered throughout the room, with the bed in the foreground and various items in the background. The bed is positioned near the left side of the image, while the items in the background are located further back, creating a sense of depth in the room.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25590.7, "ram_available_mb": 100181.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 25591.0, "ram_available_mb": 100181.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.91, "peak": 15.22, "min": 14.32}, "VDD_GPU": {"avg": 30.01, "peak": 38.21, "min": 24.03}, "VIN": {"avg": 70.42, "peak": 118.02, "min": 57.31}, "VDD_CPU_SOC_MSS": {"avg": 15.45, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 30.01, "energy_joules_est": 65.53, "sample_count": 16, "duration_seconds": 2.184}, "timestamp": "2026-01-16T16:26:54.193358"}
{"image_index": 365, "image_name": "000000039956.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039956.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2285.649, "latencies_ms": [2285.649], "images_per_second": 0.438, "prompt_tokens": 21, "response_tokens_est": 62, "n_tiles": 6, "output_text": "The image depicts a cluttered room with various items scattered around, including a bed with a brown blanket, a wooden nightstand, and a chair. The room appears to be in the process of being unpacked or tidied up, with boxes and items piled up on the floor and shelves.", "error": null, "sys_before": {"cpu_percent": 16.7, "ram_used_mb": 25591.0, "ram_available_mb": 100181.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 25590.7, "ram_available_mb": 100181.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.93, "peak": 15.22, "min": 14.22}, "VDD_GPU": {"avg": 29.59, "peak": 38.19, "min": 24.03}, "VIN": {"avg": 71.38, "peak": 107.33, "min": 58.88}, "VDD_CPU_SOC_MSS": {"avg": 15.49, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 29.59, "energy_joules_est": 67.64, "sample_count": 17, "duration_seconds": 2.286}, "timestamp": "2026-01-16T16:26:56.484786"}
{"image_index": 365, "image_name": "000000039956.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000039956.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1615.946, "latencies_ms": [1615.946], "images_per_second": 0.619, "prompt_tokens": 19, "response_tokens_est": 37, "n_tiles": 6, "output_text": "The room is dimly lit with warm, soft lighting, creating a cozy and intimate atmosphere. The walls are adorned with exposed brick, adding a rustic charm to the space.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25590.7, "ram_available_mb": 100181.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 25590.7, "ram_available_mb": 100181.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.82, "peak": 15.11, "min": 14.32}, "VDD_GPU": {"avg": 31.8, "peak": 37.8, "min": 25.21}, "VIN": {"avg": 71.28, "peak": 113.04, "min": 56.12}, "VDD_CPU_SOC_MSS": {"avg": 15.32, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 31.8, "energy_joules_est": 51.4, "sample_count": 12, "duration_seconds": 1.616}, "timestamp": "2026-01-16T16:26:58.106654"}
{"image_index": 366, "image_name": "000000040036.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000040036.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1506.736, "latencies_ms": [1506.736], "images_per_second": 0.664, "prompt_tokens": 9, "response_tokens_est": 33, "n_tiles": 6, "output_text": "A jockey is riding a horse in a race, with the horse's hooves off the ground, showcasing a dynamic and exciting moment in the competition.", "error": null, "sys_before": {"cpu_percent": 43.2, "ram_used_mb": 25590.7, "ram_available_mb": 100181.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25590.7, "ram_available_mb": 100181.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.33, "peak": 14.81, "min": 13.82}, "VDD_GPU": {"avg": 31.23, "peak": 37.42, "min": 25.21}, "VIN": {"avg": 64.09, "peak": 89.07, "min": 50.18}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 31.23, "energy_joules_est": 47.07, "sample_count": 11, "duration_seconds": 1.507}, "timestamp": "2026-01-16T16:26:59.681699"}
{"image_index": 366, "image_name": "000000040036.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000040036.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1908.016, "latencies_ms": [1908.016], "images_per_second": 0.524, "prompt_tokens": 23, "response_tokens_est": 46, "n_tiles": 6, "output_text": "1. Horse\n2. Rider\n3. Jumping fence\n4. Jumping bar\n5. Jumping pad\n6. Jumping rope\n7. Jumping rope holder\n8. Jumping pad holder", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25590.7, "ram_available_mb": 100181.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 25590.7, "ram_available_mb": 100181.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.61, "peak": 14.91, "min": 14.02}, "VDD_GPU": {"avg": 30.61, "peak": 38.6, "min": 24.03}, "VIN": {"avg": 67.41, "peak": 116.53, "min": 55.92}, "VDD_CPU_SOC_MSS": {"avg": 15.18, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 30.61, "energy_joules_est": 58.41, "sample_count": 14, "duration_seconds": 1.908}, "timestamp": "2026-01-16T16:27:01.595601"}
{"image_index": 366, "image_name": "000000040036.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000040036.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2333.417, "latencies_ms": [2333.417], "images_per_second": 0.429, "prompt_tokens": 27, "response_tokens_est": 61, "n_tiles": 6, "output_text": "The main object in the foreground is a brown horse with a rider on its back. The rider is wearing a red and green jacket and a helmet. The horse is equipped with a saddle and is in mid-jump over a wooden fence. The background features lush green trees, indicating an outdoor setting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25590.7, "ram_available_mb": 100181.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 25590.7, "ram_available_mb": 100181.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.73, "peak": 15.01, "min": 14.02}, "VDD_GPU": {"avg": 28.82, "peak": 38.21, "min": 23.24}, "VIN": {"avg": 68.36, "peak": 91.7, "min": 62.51}, "VDD_CPU_SOC_MSS": {"avg": 15.27, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 28.82, "energy_joules_est": 67.26, "sample_count": 18, "duration_seconds": 2.334}, "timestamp": "2026-01-16T16:27:03.936168"}
{"image_index": 366, "image_name": "000000040036.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000040036.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2631.952, "latencies_ms": [2631.952], "images_per_second": 0.38, "prompt_tokens": 21, "response_tokens_est": 72, "n_tiles": 6, "output_text": "The image captures a dynamic moment during a horse jumping event, where a jockey is riding a brown horse. The horse is mid-jump over a wooden fence, with the jockey wearing a red and green racing outfit and a helmet. The setting is outdoors, likely in a stable or a designated jumping area, with lush green trees in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25590.7, "ram_available_mb": 100181.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 25590.7, "ram_available_mb": 100181.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.76, "peak": 15.11, "min": 14.02}, "VDD_GPU": {"avg": 28.28, "peak": 38.21, "min": 23.24}, "VIN": {"avg": 69.36, "peak": 114.42, "min": 55.83}, "VDD_CPU_SOC_MSS": {"avg": 15.25, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 28.28, "energy_joules_est": 74.44, "sample_count": 20, "duration_seconds": 2.632}, "timestamp": "2026-01-16T16:27:06.574055"}
{"image_index": 366, "image_name": "000000040036.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000040036.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2293.206, "latencies_ms": [2293.206], "images_per_second": 0.436, "prompt_tokens": 19, "response_tokens_est": 60, "n_tiles": 6, "output_text": "The horse in the image is a rich brown color with a sleek, glossy coat. The rider is wearing a red and green racing outfit, complete with a helmet and protective gear. The lighting is bright and natural, suggesting a sunny day, and the weather appears to be clear and pleasant.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25590.7, "ram_available_mb": 100181.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 25590.7, "ram_available_mb": 100181.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.77, "peak": 15.11, "min": 14.02}, "VDD_GPU": {"avg": 29.2, "peak": 38.21, "min": 23.25}, "VIN": {"avg": 70.56, "peak": 117.69, "min": 54.55}, "VDD_CPU_SOC_MSS": {"avg": 15.24, "peak": 15.74, "min": 14.56}}, "power_watts_avg": 29.2, "energy_joules_est": 66.97, "sample_count": 17, "duration_seconds": 2.293}, "timestamp": "2026-01-16T16:27:08.873092"}
{"image_index": 367, "image_name": "000000040083.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000040083.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1474.5, "latencies_ms": [1474.5], "images_per_second": 0.678, "prompt_tokens": 9, "response_tokens_est": 33, "n_tiles": 6, "output_text": "A man is sitting on a folding chair, leaning against a box filled with various items, while another man stands nearby, both seemingly engaged in their respective activities.", "error": null, "sys_before": {"cpu_percent": 46.2, "ram_used_mb": 25590.7, "ram_available_mb": 100181.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 25590.7, "ram_available_mb": 100181.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.42, "peak": 15.01, "min": 13.82}, "VDD_GPU": {"avg": 31.23, "peak": 37.03, "min": 25.6}, "VIN": {"avg": 70.28, "peak": 107.01, "min": 54.13}, "VDD_CPU_SOC_MSS": {"avg": 15.1, "peak": 15.75, "min": 14.57}}, "power_watts_avg": 31.23, "energy_joules_est": 46.06, "sample_count": 11, "duration_seconds": 1.475}, "timestamp": "2026-01-16T16:27:10.404991"}
{"image_index": 367, "image_name": "000000040083.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000040083.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 4057.292, "latencies_ms": [4057.292], "images_per_second": 0.246, "prompt_tokens": 23, "response_tokens_est": 128, "n_tiles": 6, "output_text": "object: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject:", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25590.7, "ram_available_mb": 100181.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 25590.7, "ram_available_mb": 100181.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.12, "peak": 15.32, "min": 14.32}, "VDD_GPU": {"avg": 26.87, "peak": 38.21, "min": 23.64}, "VIN": {"avg": 65.61, "peak": 72.67, "min": 55.62}, "VDD_CPU_SOC_MSS": {"avg": 15.6, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 26.87, "energy_joules_est": 109.03, "sample_count": 32, "duration_seconds": 4.058}, "timestamp": "2026-01-16T16:27:14.468219"}
{"image_index": 367, "image_name": "000000040083.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000040083.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2310.766, "latencies_ms": [2310.766], "images_per_second": 0.433, "prompt_tokens": 27, "response_tokens_est": 63, "n_tiles": 6, "output_text": "The main objects in the image are a man sitting on a folding chair, a bicycle leaning against a pole, and a box on the ground. The man is positioned in the foreground, while the bicycle and box are in the background. The man is near the bicycle, and the box is near the ground.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25590.7, "ram_available_mb": 100181.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 25591.2, "ram_available_mb": 100181.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.95, "peak": 15.22, "min": 14.32}, "VDD_GPU": {"avg": 29.35, "peak": 38.21, "min": 24.03}, "VIN": {"avg": 72.9, "peak": 115.39, "min": 64.74}, "VDD_CPU_SOC_MSS": {"avg": 15.51, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 29.35, "energy_joules_est": 67.83, "sample_count": 18, "duration_seconds": 2.311}, "timestamp": "2026-01-16T16:27:16.785661"}
{"image_index": 367, "image_name": "000000040083.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000040083.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2768.292, "latencies_ms": [2768.292], "images_per_second": 0.361, "prompt_tokens": 21, "response_tokens_est": 80, "n_tiles": 6, "output_text": "The image depicts a street scene in an urban area, likely in a city with East Asian characters on signs. A man is sitting on a folding chair, surrounded by various items on a table, including a bicycle. Another man is standing nearby, seemingly engaged in conversation or looking at something on the table. The setting appears to be a busy street with parked cars and buildings in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25591.2, "ram_available_mb": 100181.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 25591.4, "ram_available_mb": 100180.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.01, "peak": 15.32, "min": 14.32}, "VDD_GPU": {"avg": 28.36, "peak": 37.82, "min": 23.64}, "VIN": {"avg": 70.14, "peak": 111.85, "min": 60.94}, "VDD_CPU_SOC_MSS": {"avg": 15.54, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 28.36, "energy_joules_est": 78.52, "sample_count": 21, "duration_seconds": 2.769}, "timestamp": "2026-01-16T16:27:19.559929"}
{"image_index": 367, "image_name": "000000040083.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000040083.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2471.126, "latencies_ms": [2471.126], "images_per_second": 0.405, "prompt_tokens": 19, "response_tokens_est": 69, "n_tiles": 6, "output_text": "The image is a black and white photograph depicting a street scene with notable visual attributes. The scene is characterized by a mix of materials, including a wooden bench, a metal folding chair, and a wooden crate filled with various items. The lighting is natural, likely from sunlight, casting shadows and highlighting the textures of the objects in the scene.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25591.4, "ram_available_mb": 100180.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 25591.6, "ram_available_mb": 100180.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.99, "peak": 15.32, "min": 14.32}, "VDD_GPU": {"avg": 28.88, "peak": 37.8, "min": 23.64}, "VIN": {"avg": 72.58, "peak": 119.46, "min": 61.12}, "VDD_CPU_SOC_MSS": {"avg": 15.54, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 28.88, "energy_joules_est": 71.38, "sample_count": 19, "duration_seconds": 2.472}, "timestamp": "2026-01-16T16:27:22.037086"}
{"image_index": 368, "image_name": "000000040471.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000040471.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1174.145, "latencies_ms": [1174.145], "images_per_second": 0.852, "prompt_tokens": 9, "response_tokens_est": 22, "n_tiles": 6, "output_text": "The image shows a kitchen with white cabinets, a black ceiling fan, and a window with blinds.", "error": null, "sys_before": {"cpu_percent": 40.8, "ram_used_mb": 25591.4, "ram_available_mb": 100180.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25591.6, "ram_available_mb": 100180.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.35, "peak": 14.71, "min": 13.92}, "VDD_GPU": {"avg": 32.99, "peak": 37.03, "min": 27.97}, "VIN": {"avg": 70.39, "peak": 102.2, "min": 50.92}, "VDD_CPU_SOC_MSS": {"avg": 15.16, "peak": 15.36, "min": 14.57}}, "power_watts_avg": 32.99, "energy_joules_est": 38.75, "sample_count": 8, "duration_seconds": 1.175}, "timestamp": "2026-01-16T16:27:23.272302"}
{"image_index": 368, "image_name": "000000040471.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000040471.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 4075.939, "latencies_ms": [4075.939], "images_per_second": 0.245, "prompt_tokens": 23, "response_tokens_est": 129, "n_tiles": 6, "output_text": "- Kitchen\n- Count: 1\n- Cabinets\n- Count: 3\n- Count: 2\n- Count: 1\n- Count: 1\n- Count: 1\n- Count: 1\n- Count: 1\n- Count: 1\n- Count: 1\n- Count: 1\n- Count: 1\n- Count: 1\n- Count: 1\n- Count: 1\n- Count: 1\n- Count: 1\n- Count: 1\n- Count: 1\n- Count: 1\n-", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25591.6, "ram_available_mb": 100180.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 25591.4, "ram_available_mb": 100180.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.02, "peak": 15.32, "min": 14.22}, "VDD_GPU": {"avg": 27.1, "peak": 38.59, "min": 23.64}, "VIN": {"avg": 67.58, "peak": 118.92, "min": 49.11}, "VDD_CPU_SOC_MSS": {"avg": 15.57, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 27.1, "energy_joules_est": 110.47, "sample_count": 31, "duration_seconds": 4.076}, "timestamp": "2026-01-16T16:27:27.358462"}
{"image_index": 368, "image_name": "000000040471.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000040471.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3155.586, "latencies_ms": [3155.586], "images_per_second": 0.317, "prompt_tokens": 27, "response_tokens_est": 94, "n_tiles": 6, "output_text": "The main objects in the image are the kitchen cabinets and appliances. The cabinets are located on the left side of the image, while the refrigerator and microwave are on the right side. The countertop is situated between the cabinets and appliances, with various kitchen items placed on it. The floor is tiled, and there is a window with blinds above the countertop. The ceiling fan is hanging from the ceiling, and there is a light fixture above the window.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25591.4, "ram_available_mb": 100180.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 25591.6, "ram_available_mb": 100180.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.03, "peak": 15.32, "min": 14.22}, "VDD_GPU": {"avg": 27.76, "peak": 38.21, "min": 23.64}, "VIN": {"avg": 71.38, "peak": 115.84, "min": 61.99}, "VDD_CPU_SOC_MSS": {"avg": 15.54, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 27.76, "energy_joules_est": 87.61, "sample_count": 23, "duration_seconds": 3.156}, "timestamp": "2026-01-16T16:27:30.520243"}
{"image_index": 368, "image_name": "000000040471.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000040471.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2236.661, "latencies_ms": [2236.661], "images_per_second": 0.447, "prompt_tokens": 21, "response_tokens_est": 60, "n_tiles": 6, "output_text": "The image depicts a dimly lit kitchen with a white ceiling and white cabinets. The kitchen is equipped with a white refrigerator, a stove, and a window with blinds. The scene appears to be in a residential setting, possibly during the evening or night, as the lighting is low.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25591.6, "ram_available_mb": 100180.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 25591.1, "ram_available_mb": 100181.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.91, "peak": 15.22, "min": 14.22}, "VDD_GPU": {"avg": 29.54, "peak": 37.82, "min": 24.03}, "VIN": {"avg": 68.92, "peak": 104.17, "min": 54.88}, "VDD_CPU_SOC_MSS": {"avg": 15.48, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 29.54, "energy_joules_est": 66.08, "sample_count": 16, "duration_seconds": 2.237}, "timestamp": "2026-01-16T16:27:32.762758"}
{"image_index": 368, "image_name": "000000040471.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000040471.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1534.432, "latencies_ms": [1534.432], "images_per_second": 0.652, "prompt_tokens": 19, "response_tokens_est": 34, "n_tiles": 6, "output_text": "The kitchen features a white color scheme with a light-colored floor and walls. The lighting is dim, with a ceiling fan and a single light fixture providing illumination.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25591.1, "ram_available_mb": 100181.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 25591.3, "ram_available_mb": 100180.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.78, "peak": 15.11, "min": 14.22}, "VDD_GPU": {"avg": 31.87, "peak": 38.6, "min": 25.21}, "VIN": {"avg": 73.04, "peak": 105.97, "min": 62.0}, "VDD_CPU_SOC_MSS": {"avg": 15.29, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 31.87, "energy_joules_est": 48.92, "sample_count": 12, "duration_seconds": 1.535}, "timestamp": "2026-01-16T16:27:34.303387"}
{"image_index": 369, "image_name": "000000040757.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000040757.jpg", "image_width": 425, "image_height": 640, "image_resolution": "425x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1175.932, "latencies_ms": [1175.932], "images_per_second": 0.85, "prompt_tokens": 9, "response_tokens_est": 22, "n_tiles": 6, "output_text": "A young child is lying on a bed, smiling and laughing, with a pacifier in their mouth.", "error": null, "sys_before": {"cpu_percent": 44.2, "ram_used_mb": 25591.3, "ram_available_mb": 100180.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25591.3, "ram_available_mb": 100180.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.32, "peak": 14.71, "min": 13.92}, "VDD_GPU": {"avg": 33.48, "peak": 37.42, "min": 28.37}, "VIN": {"avg": 78.86, "peak": 106.58, "min": 61.79}, "VDD_CPU_SOC_MSS": {"avg": 15.06, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 33.48, "energy_joules_est": 39.38, "sample_count": 8, "duration_seconds": 1.176}, "timestamp": "2026-01-16T16:27:35.541877"}
{"image_index": 369, "image_name": "000000040757.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000040757.jpg", "image_width": 425, "image_height": 640, "image_resolution": "425x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1592.529, "latencies_ms": [1592.529], "images_per_second": 0.628, "prompt_tokens": 23, "response_tokens_est": 36, "n_tiles": 6, "output_text": "1. Baby\n2. Crib\n3. Pillow\n4. Bed\n5. Child\n6. Baby\n7. Crib\n8. Pillow", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25591.3, "ram_available_mb": 100180.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 25591.1, "ram_available_mb": 100181.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.73, "peak": 15.11, "min": 14.22}, "VDD_GPU": {"avg": 32.23, "peak": 38.98, "min": 25.21}, "VIN": {"avg": 72.75, "peak": 116.98, "min": 58.82}, "VDD_CPU_SOC_MSS": {"avg": 15.25, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 32.23, "energy_joules_est": 51.34, "sample_count": 12, "duration_seconds": 1.593}, "timestamp": "2026-01-16T16:27:37.140585"}
{"image_index": 369, "image_name": "000000040757.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000040757.jpg", "image_width": 425, "image_height": 640, "image_resolution": "425x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2745.625, "latencies_ms": [2745.625], "images_per_second": 0.364, "prompt_tokens": 27, "response_tokens_est": 76, "n_tiles": 6, "output_text": "The main object in the image is a young child lying on a bed. The child is positioned in the foreground, with a focus on their face and upper body. The bed has a patterned cover with various cartoon-like vehicles and objects, and the child is lying on a pillow with a similar pattern. The background is mostly dark, emphasizing the child and the bed.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25591.1, "ram_available_mb": 100181.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 25591.1, "ram_available_mb": 100181.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.8, "peak": 15.01, "min": 14.12}, "VDD_GPU": {"avg": 28.32, "peak": 38.59, "min": 23.24}, "VIN": {"avg": 69.78, "peak": 117.65, "min": 55.75}, "VDD_CPU_SOC_MSS": {"avg": 15.28, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 28.32, "energy_joules_est": 77.77, "sample_count": 21, "duration_seconds": 2.746}, "timestamp": "2026-01-16T16:27:39.892143"}
{"image_index": 369, "image_name": "000000040757.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000040757.jpg", "image_width": 425, "image_height": 640, "image_resolution": "425x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2044.062, "latencies_ms": [2044.062], "images_per_second": 0.489, "prompt_tokens": 21, "response_tokens_est": 51, "n_tiles": 6, "output_text": "The image depicts a young child lying on a bed, seemingly asleep. The bed is covered with a patterned blanket featuring various cartoon-like vehicles and characters. The child is wearing a light-colored tank top and appears to be enjoying a peaceful moment.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25591.1, "ram_available_mb": 100181.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 25591.1, "ram_available_mb": 100181.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.69, "peak": 15.01, "min": 14.02}, "VDD_GPU": {"avg": 29.78, "peak": 38.21, "min": 23.64}, "VIN": {"avg": 68.72, "peak": 103.79, "min": 55.23}, "VDD_CPU_SOC_MSS": {"avg": 15.22, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 29.78, "energy_joules_est": 60.88, "sample_count": 15, "duration_seconds": 2.044}, "timestamp": "2026-01-16T16:27:41.942133"}
{"image_index": 369, "image_name": "000000040757.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000040757.jpg", "image_width": 425, "image_height": 640, "image_resolution": "425x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1874.378, "latencies_ms": [1874.378], "images_per_second": 0.534, "prompt_tokens": 19, "response_tokens_est": 45, "n_tiles": 6, "output_text": "The image depicts a young child lying on a bed with a light-colored, patterned blanket. The child is wearing a white tank top and is surrounded by soft, ambient lighting, creating a cozy and warm atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25591.1, "ram_available_mb": 100181.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 25591.1, "ram_available_mb": 100181.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.67, "peak": 15.01, "min": 14.02}, "VDD_GPU": {"avg": 30.42, "peak": 38.21, "min": 24.03}, "VIN": {"avg": 70.68, "peak": 112.78, "min": 55.45}, "VDD_CPU_SOC_MSS": {"avg": 15.21, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 30.42, "energy_joules_est": 57.03, "sample_count": 14, "duration_seconds": 1.875}, "timestamp": "2026-01-16T16:27:43.822593"}
{"image_index": 370, "image_name": "000000041488.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000041488.jpg", "image_width": 640, "image_height": 369, "image_resolution": "640x369", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1598.313, "latencies_ms": [1598.313], "images_per_second": 0.626, "prompt_tokens": 9, "response_tokens_est": 38, "n_tiles": 6, "output_text": "The image shows a green highway sign with white lettering that reads \"Queens Bronx\" and features the Interstate 278 highway logo, indicating the location of the roadway.", "error": null, "sys_before": {"cpu_percent": 45.9, "ram_used_mb": 25591.1, "ram_available_mb": 100181.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 25590.2, "ram_available_mb": 100182.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.51, "peak": 15.11, "min": 13.82}, "VDD_GPU": {"avg": 30.62, "peak": 37.03, "min": 24.82}, "VIN": {"avg": 71.6, "peak": 121.72, "min": 51.62}, "VDD_CPU_SOC_MSS": {"avg": 15.22, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 30.62, "energy_joules_est": 48.95, "sample_count": 12, "duration_seconds": 1.599}, "timestamp": "2026-01-16T16:27:45.477855"}
{"image_index": 370, "image_name": "000000041488.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000041488.jpg", "image_width": 640, "image_height": 369, "image_resolution": "640x369", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1698.237, "latencies_ms": [1698.237], "images_per_second": 0.589, "prompt_tokens": 23, "response_tokens_est": 40, "n_tiles": 6, "output_text": "object: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25590.2, "ram_available_mb": 100182.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 25590.4, "ram_available_mb": 100181.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.81, "peak": 15.11, "min": 14.22}, "VDD_GPU": {"avg": 31.21, "peak": 38.19, "min": 24.83}, "VIN": {"avg": 70.06, "peak": 115.37, "min": 55.54}, "VDD_CPU_SOC_MSS": {"avg": 15.41, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 31.21, "energy_joules_est": 53.01, "sample_count": 13, "duration_seconds": 1.699}, "timestamp": "2026-01-16T16:27:47.182032"}
{"image_index": 370, "image_name": "000000041488.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000041488.jpg", "image_width": 640, "image_height": 369, "image_resolution": "640x369", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2924.603, "latencies_ms": [2924.603], "images_per_second": 0.342, "prompt_tokens": 27, "response_tokens_est": 83, "n_tiles": 6, "output_text": "The main object in the foreground is a green highway sign with white text. The sign is mounted on a metal structure, likely part of a highway or bridge. In the background, there is a faint outline of a metal structure, possibly part of the same bridge or another nearby structure. The sign is positioned near the center of the image, with the metal structure to the right and the sky to the left.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25590.4, "ram_available_mb": 100181.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 25590.6, "ram_available_mb": 100181.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.86, "peak": 15.11, "min": 14.12}, "VDD_GPU": {"avg": 28.04, "peak": 38.6, "min": 23.24}, "VIN": {"avg": 73.99, "peak": 125.02, "min": 59.54}, "VDD_CPU_SOC_MSS": {"avg": 15.43, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 28.04, "energy_joules_est": 82.02, "sample_count": 22, "duration_seconds": 2.925}, "timestamp": "2026-01-16T16:27:50.112699"}
{"image_index": 370, "image_name": "000000041488.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000041488.jpg", "image_width": 640, "image_height": 369, "image_resolution": "640x369", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2790.993, "latencies_ms": [2790.993], "images_per_second": 0.358, "prompt_tokens": 21, "response_tokens_est": 78, "n_tiles": 6, "output_text": "The image depicts a green highway sign with white lettering, indicating \"Queens Bronx\" and the Interstate 278 highway number. The sign is mounted on a metal structure, possibly part of a bridge or overpass, with a blurred background featuring industrial elements. The scene suggests an urban setting, likely in Queens, New York, where the highway sign is located.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25590.6, "ram_available_mb": 100181.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 25590.3, "ram_available_mb": 100181.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.82, "peak": 15.11, "min": 14.12}, "VDD_GPU": {"avg": 27.93, "peak": 38.19, "min": 23.24}, "VIN": {"avg": 69.23, "peak": 108.18, "min": 55.9}, "VDD_CPU_SOC_MSS": {"avg": 15.43, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 27.93, "energy_joules_est": 77.96, "sample_count": 21, "duration_seconds": 2.791}, "timestamp": "2026-01-16T16:27:52.909433"}
{"image_index": 370, "image_name": "000000041488.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000041488.jpg", "image_width": 640, "image_height": 369, "image_resolution": "640x369", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2490.452, "latencies_ms": [2490.452], "images_per_second": 0.402, "prompt_tokens": 19, "response_tokens_est": 67, "n_tiles": 6, "output_text": "The image features a green highway sign with white lettering, mounted on a metal structure. The sign is illuminated by artificial lighting, casting a bright glow on the sign and creating a stark contrast with the surrounding environment. The weather appears to be overcast, with a diffused light that softens the edges of the image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25590.3, "ram_available_mb": 100181.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 25590.6, "ram_available_mb": 100181.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.77, "peak": 15.01, "min": 14.12}, "VDD_GPU": {"avg": 28.49, "peak": 38.21, "min": 23.24}, "VIN": {"avg": 68.73, "peak": 115.92, "min": 56.84}, "VDD_CPU_SOC_MSS": {"avg": 15.44, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 28.49, "energy_joules_est": 70.97, "sample_count": 19, "duration_seconds": 2.491}, "timestamp": "2026-01-16T16:27:55.406504"}
{"image_index": 371, "image_name": "000000041633.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000041633.jpg", "image_width": 640, "image_height": 429, "image_resolution": "640x429", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1251.71, "latencies_ms": [1251.71], "images_per_second": 0.799, "prompt_tokens": 9, "response_tokens_est": 25, "n_tiles": 6, "output_text": "The image features a vintage red Chevrolet car parked outdoors, with a clear sky and a few other cars in the background.", "error": null, "sys_before": {"cpu_percent": 37.3, "ram_used_mb": 25590.6, "ram_available_mb": 100181.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25590.3, "ram_available_mb": 100181.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.33, "peak": 14.81, "min": 13.82}, "VDD_GPU": {"avg": 32.48, "peak": 37.42, "min": 27.18}, "VIN": {"avg": 74.15, "peak": 123.64, "min": 49.99}, "VDD_CPU_SOC_MSS": {"avg": 15.05, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 32.48, "energy_joules_est": 40.67, "sample_count": 9, "duration_seconds": 1.252}, "timestamp": "2026-01-16T16:27:56.719141"}
{"image_index": 371, "image_name": "000000041633.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000041633.jpg", "image_width": 640, "image_height": 429, "image_resolution": "640x429", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1721.261, "latencies_ms": [1721.261], "images_per_second": 0.581, "prompt_tokens": 23, "response_tokens_est": 41, "n_tiles": 6, "output_text": "1. Red car\n2. Red truck\n3. Blue car\n4. Green tent\n5. White fence\n6. White lamp post\n7. White flag\n8. White sky", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25590.3, "ram_available_mb": 100181.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 25590.3, "ram_available_mb": 100181.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.81, "peak": 15.22, "min": 14.22}, "VDD_GPU": {"avg": 31.67, "peak": 38.6, "min": 24.83}, "VIN": {"avg": 78.52, "peak": 118.59, "min": 62.45}, "VDD_CPU_SOC_MSS": {"avg": 15.44, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 31.67, "energy_joules_est": 54.53, "sample_count": 13, "duration_seconds": 1.722}, "timestamp": "2026-01-16T16:27:58.446823"}
{"image_index": 371, "image_name": "000000041633.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000041633.jpg", "image_width": 640, "image_height": 429, "image_resolution": "640x429", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2849.318, "latencies_ms": [2849.318], "images_per_second": 0.351, "prompt_tokens": 27, "response_tokens_est": 81, "n_tiles": 6, "output_text": "The main object in the foreground is a vintage red Chevrolet truck with a shiny, reflective surface. The truck is parked on a concrete surface, and its reflection is visible on the ground. In the background, there are other cars parked, including a blue car and a green tent. The sky is partly cloudy, and the overall setting appears to be a car show or a gathering of classic vehicles.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25590.3, "ram_available_mb": 100181.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 25590.3, "ram_available_mb": 100181.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.95, "peak": 15.11, "min": 14.22}, "VDD_GPU": {"avg": 28.11, "peak": 38.21, "min": 23.25}, "VIN": {"avg": 66.55, "peak": 117.6, "min": 57.19}, "VDD_CPU_SOC_MSS": {"avg": 15.6, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 28.11, "energy_joules_est": 80.1, "sample_count": 22, "duration_seconds": 2.85}, "timestamp": "2026-01-16T16:28:01.304424"}
{"image_index": 371, "image_name": "000000041633.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000041633.jpg", "image_width": 640, "image_height": 429, "image_resolution": "640x429", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2519.406, "latencies_ms": [2519.406], "images_per_second": 0.397, "prompt_tokens": 21, "response_tokens_est": 68, "n_tiles": 6, "output_text": "The image depicts a classic red Chevrolet truck parked outdoors, likely at a car show or gathering. The setting is a spacious area with other vintage cars visible in the background, under a clear sky. The truck's shiny red paint and classic design suggest it is a well-preserved vehicle, possibly from the mid-20th century.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25590.3, "ram_available_mb": 100181.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 25590.6, "ram_available_mb": 100181.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.77, "peak": 15.01, "min": 14.02}, "VDD_GPU": {"avg": 28.3, "peak": 38.21, "min": 23.24}, "VIN": {"avg": 66.22, "peak": 122.65, "min": 56.54}, "VDD_CPU_SOC_MSS": {"avg": 15.4, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 28.3, "energy_joules_est": 71.31, "sample_count": 19, "duration_seconds": 2.52}, "timestamp": "2026-01-16T16:28:03.830001"}
{"image_index": 371, "image_name": "000000041633.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000041633.jpg", "image_width": 640, "image_height": 429, "image_resolution": "640x429", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2294.014, "latencies_ms": [2294.014], "images_per_second": 0.436, "prompt_tokens": 19, "response_tokens_est": 60, "n_tiles": 6, "output_text": "The notable visual attributes of the red car in the image include its glossy finish, which reflects the surrounding environment, and its classic design reminiscent of mid-20th century automobiles. The lighting is bright and natural, suggesting daytime, and the weather appears to be clear with a blue sky.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25590.6, "ram_available_mb": 100181.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 25590.6, "ram_available_mb": 100181.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.73, "peak": 15.01, "min": 14.02}, "VDD_GPU": {"avg": 28.91, "peak": 38.19, "min": 23.25}, "VIN": {"avg": 69.41, "peak": 117.0, "min": 55.08}, "VDD_CPU_SOC_MSS": {"avg": 15.33, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 28.91, "energy_joules_est": 66.33, "sample_count": 18, "duration_seconds": 2.294}, "timestamp": "2026-01-16T16:28:06.130420"}
{"image_index": 372, "image_name": "000000041635.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000041635.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2383.375, "latencies_ms": [2383.375], "images_per_second": 0.42, "prompt_tokens": 9, "response_tokens_est": 29, "n_tiles": 12, "output_text": "The image depicts a black and white photograph of a group of cows standing behind a wire fence, with a rural landscape in the background.", "error": null, "sys_before": {"cpu_percent": 42.3, "ram_used_mb": 25590.6, "ram_available_mb": 100181.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 25590.6, "ram_available_mb": 100181.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.11, "peak": 16.15, "min": 13.79}, "VDD_GPU": {"avg": 34.84, "peak": 40.57, "min": 27.57}, "VIN": {"avg": 73.66, "peak": 104.97, "min": 57.65}, "VDD_CPU_SOC_MSS": {"avg": 15.85, "peak": 17.32, "min": 14.96}}, "power_watts_avg": 34.84, "energy_joules_est": 83.05, "sample_count": 18, "duration_seconds": 2.384}, "timestamp": "2026-01-16T16:28:08.605439"}
{"image_index": 372, "image_name": "000000041635.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000041635.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2778.139, "latencies_ms": [2778.139], "images_per_second": 0.36, "prompt_tokens": 23, "response_tokens_est": 37, "n_tiles": 12, "output_text": "1. Cows\n2. Barbed wire\n3. Grass\n4. Trees\n5. Sky\n6. Farmland\n7. House\n8. Animals", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25590.6, "ram_available_mb": 100181.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 25590.3, "ram_available_mb": 100181.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.06, "peak": 15.65, "min": 14.43}, "VDD_GPU": {"avg": 35.28, "peak": 42.94, "min": 27.18}, "VIN": {"avg": 77.32, "peak": 113.23, "min": 55.15}, "VDD_CPU_SOC_MSS": {"avg": 15.63, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 35.28, "energy_joules_est": 98.03, "sample_count": 21, "duration_seconds": 2.779}, "timestamp": "2026-01-16T16:28:11.390230"}
{"image_index": 372, "image_name": "000000041635.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000041635.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3522.258, "latencies_ms": [3522.258], "images_per_second": 0.284, "prompt_tokens": 27, "response_tokens_est": 59, "n_tiles": 12, "output_text": "The main objects in the image are a group of cows, with the foreground and background cows being the most prominent. The cows are positioned in the foreground, with the foreground cow being the closest to the viewer. The background cows are further away, creating a sense of depth in the image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25590.3, "ram_available_mb": 100181.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25590.3, "ram_available_mb": 100181.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.08, "peak": 15.65, "min": 14.43}, "VDD_GPU": {"avg": 33.26, "peak": 42.92, "min": 26.39}, "VIN": {"avg": 75.1, "peak": 114.2, "min": 56.66}, "VDD_CPU_SOC_MSS": {"avg": 15.66, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 33.26, "energy_joules_est": 117.18, "sample_count": 27, "duration_seconds": 3.523}, "timestamp": "2026-01-16T16:28:14.919118"}
{"image_index": 372, "image_name": "000000041635.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000041635.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3413.993, "latencies_ms": [3413.993], "images_per_second": 0.293, "prompt_tokens": 21, "response_tokens_est": 56, "n_tiles": 12, "output_text": "The image depicts a rural scene with a barbed wire fence stretching across the frame. In the foreground, there are several cows, some of which are standing and others are lying down. The cows appear to be grazing on the grassy area enclosed by the fence.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25590.3, "ram_available_mb": 100181.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 25590.3, "ram_available_mb": 100181.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.08, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 33.42, "peak": 42.54, "min": 26.39}, "VIN": {"avg": 72.65, "peak": 113.38, "min": 58.43}, "VDD_CPU_SOC_MSS": {"avg": 15.69, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 33.42, "energy_joules_est": 114.11, "sample_count": 26, "duration_seconds": 3.414}, "timestamp": "2026-01-16T16:28:18.340391"}
{"image_index": 372, "image_name": "000000041635.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000041635.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3686.864, "latencies_ms": [3686.864], "images_per_second": 0.271, "prompt_tokens": 19, "response_tokens_est": 64, "n_tiles": 12, "output_text": "The image is a black and white photograph featuring a rustic, rural scene. The notable visual attributes include the barbed wire fence stretching across the image, the lush green grass, and the overcast sky. The lighting is soft and diffused, casting gentle shadows and highlighting the textures of the grass and the fence.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25590.3, "ram_available_mb": 100181.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25590.3, "ram_available_mb": 100181.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.07, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 32.84, "peak": 42.92, "min": 26.39}, "VIN": {"avg": 75.12, "peak": 116.74, "min": 58.45}, "VDD_CPU_SOC_MSS": {"avg": 15.75, "peak": 16.53, "min": 15.35}}, "power_watts_avg": 32.84, "energy_joules_est": 121.09, "sample_count": 29, "duration_seconds": 3.687}, "timestamp": "2026-01-16T16:28:22.033765"}
{"image_index": 373, "image_name": "000000041872.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000041872.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2106.554, "latencies_ms": [2106.554], "images_per_second": 0.475, "prompt_tokens": 9, "response_tokens_est": 57, "n_tiles": 6, "output_text": "The image depicts a cozy, warmly lit bedroom with a wooden bed frame, a beige bedspread, and a wooden chair with a cushion. The room features a fireplace with a mantel, a television mounted on the wall, and a window with closed blinds.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25590.3, "ram_available_mb": 100181.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 25589.6, "ram_available_mb": 100182.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.71, "peak": 15.22, "min": 13.92}, "VDD_GPU": {"avg": 29.54, "peak": 37.82, "min": 24.03}, "VIN": {"avg": 73.23, "peak": 115.19, "min": 52.89}, "VDD_CPU_SOC_MSS": {"avg": 15.38, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 29.54, "energy_joules_est": 62.25, "sample_count": 16, "duration_seconds": 2.107}, "timestamp": "2026-01-16T16:28:24.190756"}
{"image_index": 373, "image_name": "000000041872.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000041872.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1988.614, "latencies_ms": [1988.614], "images_per_second": 0.503, "prompt_tokens": 23, "response_tokens_est": 51, "n_tiles": 6, "output_text": "- bed: 1\n- nightstand: 1\n- lamp: 1\n- chair: 1\n- fireplace: 1\n- TV: 1\n- blinds: 1\n- wall clock: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25589.6, "ram_available_mb": 100182.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 25589.3, "ram_available_mb": 100182.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.88, "peak": 15.22, "min": 14.32}, "VDD_GPU": {"avg": 30.36, "peak": 38.21, "min": 24.42}, "VIN": {"avg": 68.65, "peak": 97.64, "min": 56.79}, "VDD_CPU_SOC_MSS": {"avg": 15.43, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 30.36, "energy_joules_est": 60.39, "sample_count": 15, "duration_seconds": 1.989}, "timestamp": "2026-01-16T16:28:26.185612"}
{"image_index": 373, "image_name": "000000041872.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000041872.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3099.044, "latencies_ms": [3099.044], "images_per_second": 0.323, "prompt_tokens": 27, "response_tokens_est": 92, "n_tiles": 6, "output_text": "The main objects in the image are a bed, a chair, and a fireplace. The bed is positioned in the foreground, with a cushion placed on the bed's headboard. The chair is positioned to the right of the bed, with a cushion on the seat. The fireplace is situated to the right of the bed, with a mantel above it. The fireplace is the most prominent object in the image, and it is positioned in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25589.3, "ram_available_mb": 100182.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 25589.3, "ram_available_mb": 100182.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.03, "peak": 15.32, "min": 14.32}, "VDD_GPU": {"avg": 27.9, "peak": 38.21, "min": 23.64}, "VIN": {"avg": 69.37, "peak": 103.65, "min": 56.48}, "VDD_CPU_SOC_MSS": {"avg": 15.6, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 27.9, "energy_joules_est": 86.47, "sample_count": 24, "duration_seconds": 3.099}, "timestamp": "2026-01-16T16:28:29.290405"}
{"image_index": 373, "image_name": "000000041872.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000041872.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2732.445, "latencies_ms": [2732.445], "images_per_second": 0.366, "prompt_tokens": 21, "response_tokens_est": 79, "n_tiles": 6, "output_text": "The image depicts a cozy, warmly lit bedroom with a wooden ceiling and walls. The room features a large bed with a brown bedspread, a wooden nightstand with a lamp, and a beige armchair. There is a fireplace with a stone surround and a small television mounted on the wall above it. The room is well-lit, creating a comfortable and inviting atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25589.3, "ram_available_mb": 100182.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 25589.3, "ram_available_mb": 100182.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.0, "peak": 15.32, "min": 14.22}, "VDD_GPU": {"avg": 28.34, "peak": 37.8, "min": 23.64}, "VIN": {"avg": 69.94, "peak": 108.09, "min": 55.87}, "VDD_CPU_SOC_MSS": {"avg": 15.64, "peak": 16.14, "min": 14.96}}, "power_watts_avg": 28.34, "energy_joules_est": 77.45, "sample_count": 21, "duration_seconds": 2.733}, "timestamp": "2026-01-16T16:28:32.028813"}
{"image_index": 373, "image_name": "000000041872.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000041872.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1838.974, "latencies_ms": [1838.974], "images_per_second": 0.544, "prompt_tokens": 19, "response_tokens_est": 46, "n_tiles": 6, "output_text": "The room features warm, earthy tones with wooden elements, creating a cozy and inviting atmosphere. The lighting is soft and ambient, with a combination of natural light from the window and warm, recessed lighting in the ceiling.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25589.3, "ram_available_mb": 100182.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 25589.6, "ram_available_mb": 100182.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.96, "peak": 15.32, "min": 14.32}, "VDD_GPU": {"avg": 30.67, "peak": 38.21, "min": 24.42}, "VIN": {"avg": 67.46, "peak": 99.74, "min": 56.99}, "VDD_CPU_SOC_MSS": {"avg": 15.52, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 30.67, "energy_joules_est": 56.41, "sample_count": 14, "duration_seconds": 1.839}, "timestamp": "2026-01-16T16:28:33.874037"}
{"image_index": 374, "image_name": "000000041888.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000041888.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2465.634, "latencies_ms": [2465.634], "images_per_second": 0.406, "prompt_tokens": 9, "response_tokens_est": 32, "n_tiles": 12, "output_text": "The image depicts a group of turkeys standing on dry, brown grass in a natural, open field, with one turkey slightly bending down to eat.", "error": null, "sys_before": {"cpu_percent": 38.7, "ram_used_mb": 25589.3, "ram_available_mb": 100182.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 25589.3, "ram_available_mb": 100182.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.21, "peak": 16.15, "min": 14.2}, "VDD_GPU": {"avg": 34.89, "peak": 41.36, "min": 27.57}, "VIN": {"avg": 77.68, "peak": 114.38, "min": 58.46}, "VDD_CPU_SOC_MSS": {"avg": 16.08, "peak": 17.32, "min": 15.36}}, "power_watts_avg": 34.89, "energy_joules_est": 86.04, "sample_count": 19, "duration_seconds": 2.466}, "timestamp": "2026-01-16T16:28:36.414955"}
{"image_index": 374, "image_name": "000000041888.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000041888.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2610.601, "latencies_ms": [2610.601], "images_per_second": 0.383, "prompt_tokens": 23, "response_tokens_est": 32, "n_tiles": 12, "output_text": "1. Turkey\n2. Turkey\n3. Turkey\n4. Turkey\n5. Turkey\n6. Turkey\n7. Turkey\n8. Turkey", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25589.3, "ram_available_mb": 100182.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 25589.8, "ram_available_mb": 100182.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.09, "peak": 15.65, "min": 14.43}, "VDD_GPU": {"avg": 35.71, "peak": 42.94, "min": 27.18}, "VIN": {"avg": 78.76, "peak": 125.79, "min": 58.27}, "VDD_CPU_SOC_MSS": {"avg": 15.71, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 35.71, "energy_joules_est": 93.24, "sample_count": 20, "duration_seconds": 2.611}, "timestamp": "2026-01-16T16:28:39.032097"}
{"image_index": 374, "image_name": "000000041888.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000041888.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3016.822, "latencies_ms": [3016.822], "images_per_second": 0.331, "prompt_tokens": 27, "response_tokens_est": 44, "n_tiles": 12, "output_text": "The main objects in the image are a group of turkeys. The turkeys are positioned in the foreground, with one standing and another lying down. The background features a dry, grassy field with sparse vegetation.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25589.8, "ram_available_mb": 100182.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 25589.6, "ram_available_mb": 100182.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.08, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 34.66, "peak": 42.92, "min": 26.79}, "VIN": {"avg": 79.38, "peak": 126.29, "min": 58.54}, "VDD_CPU_SOC_MSS": {"avg": 15.64, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 34.66, "energy_joules_est": 104.57, "sample_count": 23, "duration_seconds": 3.017}, "timestamp": "2026-01-16T16:28:42.055137"}
{"image_index": 374, "image_name": "000000041888.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000041888.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3243.744, "latencies_ms": [3243.744], "images_per_second": 0.308, "prompt_tokens": 21, "response_tokens_est": 51, "n_tiles": 12, "output_text": "The image depicts a serene natural setting with dry grass and sparse vegetation. In the foreground, there are two birds, likely peacocks, standing on the ground. The background features a blurred landscape with trees and a cloudy sky.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25589.6, "ram_available_mb": 100182.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 25589.8, "ram_available_mb": 100182.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.1, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 33.78, "peak": 42.54, "min": 26.39}, "VIN": {"avg": 75.72, "peak": 123.57, "min": 58.72}, "VDD_CPU_SOC_MSS": {"avg": 15.78, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 33.78, "energy_joules_est": 109.59, "sample_count": 25, "duration_seconds": 3.244}, "timestamp": "2026-01-16T16:28:45.306273"}
{"image_index": 374, "image_name": "000000041888.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000041888.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3217.655, "latencies_ms": [3217.655], "images_per_second": 0.311, "prompt_tokens": 19, "response_tokens_est": 50, "n_tiles": 12, "output_text": "The image depicts a scene with a dry, grassy landscape under a cloudy sky. The grass is brown and dry, indicating a lack of recent rainfall. The lighting is soft and diffused, casting a gentle glow over the scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25589.8, "ram_available_mb": 100182.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 25589.5, "ram_available_mb": 100182.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.07, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 34.05, "peak": 42.94, "min": 26.39}, "VIN": {"avg": 73.26, "peak": 113.91, "min": 58.37}, "VDD_CPU_SOC_MSS": {"avg": 15.68, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 34.05, "energy_joules_est": 109.58, "sample_count": 25, "duration_seconds": 3.218}, "timestamp": "2026-01-16T16:28:48.530262"}
{"image_index": 375, "image_name": "000000041990.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000041990.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2863.505, "latencies_ms": [2863.505], "images_per_second": 0.349, "prompt_tokens": 9, "response_tokens_est": 43, "n_tiles": 12, "output_text": "Two skiers are standing on a snowy slope, each holding a ski pole and wearing appropriate winter gear, including helmets and goggles, as they enjoy their time skiing in the snowy environment.", "error": null, "sys_before": {"cpu_percent": 22.2, "ram_used_mb": 25589.5, "ram_available_mb": 100182.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 25589.5, "ram_available_mb": 100182.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.17, "peak": 16.15, "min": 14.02}, "VDD_GPU": {"avg": 34.02, "peak": 42.15, "min": 26.79}, "VIN": {"avg": 75.48, "peak": 106.67, "min": 58.55}, "VDD_CPU_SOC_MSS": {"avg": 15.73, "peak": 17.32, "min": 14.57}}, "power_watts_avg": 34.02, "energy_joules_est": 97.43, "sample_count": 22, "duration_seconds": 2.864}, "timestamp": "2026-01-16T16:28:51.450206"}
{"image_index": 375, "image_name": "000000041990.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000041990.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2916.053, "latencies_ms": [2916.053], "images_per_second": 0.343, "prompt_tokens": 23, "response_tokens_est": 41, "n_tiles": 12, "output_text": "1. Ski poles\n2. Ski boots\n3. Ski skis\n4. Ski poles\n5. Ski poles\n6. Ski poles\n7. Ski poles\n8. Ski poles", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25589.5, "ram_available_mb": 100182.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 25589.8, "ram_available_mb": 100182.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.08, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 34.86, "peak": 42.54, "min": 26.79}, "VIN": {"avg": 75.57, "peak": 108.81, "min": 58.75}, "VDD_CPU_SOC_MSS": {"avg": 15.69, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 34.86, "energy_joules_est": 101.67, "sample_count": 22, "duration_seconds": 2.916}, "timestamp": "2026-01-16T16:28:54.372510"}
{"image_index": 375, "image_name": "000000041990.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000041990.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 5770.334, "latencies_ms": [5770.334], "images_per_second": 0.173, "prompt_tokens": 27, "response_tokens_est": 125, "n_tiles": 12, "output_text": "The main objects in the image are two skiers, one on the left and one on the right. The skier on the left is closer to the foreground, while the skier on the right is further back. The skier on the left is standing on a snow-covered slope, holding ski poles, and wearing a red jacket. The skier on the right is also on a snow-covered slope, but is closer to the background, wearing a black jacket and holding ski poles. Both skiers are in the snowy environment, and the snow-covered slope is the central focus of the image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25589.8, "ram_available_mb": 100182.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 25589.5, "ram_available_mb": 100182.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.1, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 30.47, "peak": 42.54, "min": 26.0}, "VIN": {"avg": 71.76, "peak": 115.92, "min": 58.67}, "VDD_CPU_SOC_MSS": {"avg": 15.71, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 30.47, "energy_joules_est": 175.84, "sample_count": 45, "duration_seconds": 5.771}, "timestamp": "2026-01-16T16:29:00.149433"}
{"image_index": 375, "image_name": "000000041990.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000041990.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 4018.595, "latencies_ms": [4018.595], "images_per_second": 0.249, "prompt_tokens": 21, "response_tokens_est": 73, "n_tiles": 12, "output_text": "The scene depicts a snowy landscape with two skiers, one in a red jacket and the other in a black jacket, standing on a snow-covered slope. They are both holding ski poles and appear to be enjoying their time skiing. The snowy environment and their attire suggest that they are in a mountainous region, possibly during winter.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25589.5, "ram_available_mb": 100182.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25589.5, "ram_available_mb": 100182.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.76, "peak": 15.34, "min": 14.12}, "VDD_GPU": {"avg": 32.16, "peak": 42.94, "min": 25.6}, "VIN": {"avg": 72.22, "peak": 120.58, "min": 58.48}, "VDD_CPU_SOC_MSS": {"avg": 15.22, "peak": 16.14, "min": 14.56}}, "power_watts_avg": 32.16, "energy_joules_est": 129.25, "sample_count": 32, "duration_seconds": 4.019}, "timestamp": "2026-01-16T16:29:04.174141"}
{"image_index": 375, "image_name": "000000041990.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000041990.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 4439.994, "latencies_ms": [4439.994], "images_per_second": 0.225, "prompt_tokens": 19, "response_tokens_est": 85, "n_tiles": 12, "output_text": "The image depicts a snowy landscape with two skiers, one in a white jacket and the other in a dark jacket, both wearing ski goggles and holding ski poles. The snow is white and appears to be freshly fallen, with trees heavily laden with snow in the background. The lighting is soft and diffused, likely due to the overcast sky, and the overall atmosphere is serene and peaceful.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25589.5, "ram_available_mb": 100182.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 25589.5, "ram_available_mb": 100182.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.73, "peak": 15.24, "min": 14.12}, "VDD_GPU": {"avg": 31.41, "peak": 42.53, "min": 25.6}, "VIN": {"avg": 70.51, "peak": 113.11, "min": 51.95}, "VDD_CPU_SOC_MSS": {"avg": 15.31, "peak": 16.14, "min": 14.56}}, "power_watts_avg": 31.41, "energy_joules_est": 139.48, "sample_count": 35, "duration_seconds": 4.44}, "timestamp": "2026-01-16T16:29:08.620485"}
{"image_index": 376, "image_name": "000000042070.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042070.jpg", "image_width": 640, "image_height": 512, "image_resolution": "640x512", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2851.38, "latencies_ms": [2851.38], "images_per_second": 0.351, "prompt_tokens": 9, "response_tokens_est": 43, "n_tiles": 12, "output_text": "The image shows a white bus with a digital display showing the number 51 and the word \"CROSSTOWN\" on its front, parked on a street with a partly cloudy sky in the background.", "error": null, "sys_before": {"cpu_percent": 44.4, "ram_used_mb": 25589.5, "ram_available_mb": 100182.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 25589.5, "ram_available_mb": 100182.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.07, "peak": 16.15, "min": 13.59}, "VDD_GPU": {"avg": 33.79, "peak": 41.76, "min": 26.39}, "VIN": {"avg": 74.38, "peak": 115.98, "min": 58.79}, "VDD_CPU_SOC_MSS": {"avg": 15.94, "peak": 17.32, "min": 14.57}}, "power_watts_avg": 33.79, "energy_joules_est": 96.36, "sample_count": 22, "duration_seconds": 2.852}, "timestamp": "2026-01-16T16:29:11.563878"}
{"image_index": 376, "image_name": "000000042070.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042070.jpg", "image_width": 640, "image_height": 512, "image_resolution": "640x512", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 3115.805, "latencies_ms": [3115.805], "images_per_second": 0.321, "prompt_tokens": 23, "response_tokens_est": 47, "n_tiles": 12, "output_text": "- Bus: 1\n- Bus number display: 1\n- Building: 1\n- Street light: 1\n- Street sign: 1\n- Street pole: 1\n- Car: 1", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25589.5, "ram_available_mb": 100182.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 25589.3, "ram_available_mb": 100182.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.07, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 34.3, "peak": 42.94, "min": 26.79}, "VIN": {"avg": 70.47, "peak": 110.86, "min": 46.26}, "VDD_CPU_SOC_MSS": {"avg": 15.75, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 34.3, "energy_joules_est": 106.89, "sample_count": 24, "duration_seconds": 3.116}, "timestamp": "2026-01-16T16:29:14.687201"}
{"image_index": 376, "image_name": "000000042070.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042070.jpg", "image_width": 640, "image_height": 512, "image_resolution": "640x512", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3693.998, "latencies_ms": [3693.998], "images_per_second": 0.271, "prompt_tokens": 27, "response_tokens_est": 64, "n_tiles": 12, "output_text": "The bus is positioned in the foreground, with its front facing the camera. The digital display on the bus's front shows the route number \"51\" and the destination \"CROSSTOWN.\" The bus is parked on the side of the road, with a building and a partly cloudy sky in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25589.3, "ram_available_mb": 100182.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25589.3, "ram_available_mb": 100182.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.07, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 33.09, "peak": 42.54, "min": 26.39}, "VIN": {"avg": 73.04, "peak": 120.77, "min": 54.78}, "VDD_CPU_SOC_MSS": {"avg": 15.77, "peak": 16.53, "min": 15.35}}, "power_watts_avg": 33.09, "energy_joules_est": 122.25, "sample_count": 28, "duration_seconds": 3.694}, "timestamp": "2026-01-16T16:29:18.387545"}
{"image_index": 376, "image_name": "000000042070.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042070.jpg", "image_width": 640, "image_height": 512, "image_resolution": "640x512", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3150.226, "latencies_ms": [3150.226], "images_per_second": 0.317, "prompt_tokens": 21, "response_tokens_est": 48, "n_tiles": 12, "output_text": "The image depicts a white bus with a digital display showing the number \"51\" and \"CROSSTOWN\" in red letters. The bus is parked on a street with a clear sky above and a building in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25589.3, "ram_available_mb": 100182.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 25589.5, "ram_available_mb": 100182.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.09, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 33.89, "peak": 42.13, "min": 26.39}, "VIN": {"avg": 75.07, "peak": 115.41, "min": 57.54}, "VDD_CPU_SOC_MSS": {"avg": 15.7, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 33.89, "energy_joules_est": 106.77, "sample_count": 24, "duration_seconds": 3.151}, "timestamp": "2026-01-16T16:29:21.543980"}
{"image_index": 376, "image_name": "000000042070.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042070.jpg", "image_width": 640, "image_height": 512, "image_resolution": "640x512", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3480.49, "latencies_ms": [3480.49], "images_per_second": 0.287, "prompt_tokens": 19, "response_tokens_est": 58, "n_tiles": 12, "output_text": "The bus in the image is predominantly white with blue and green accents. The digital display shows the number \"51\" and \"CROSSTOWN\" in red. The lighting is bright and natural, indicating daytime. The weather appears to be clear with a blue sky and some clouds.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25589.5, "ram_available_mb": 100182.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 25589.5, "ram_available_mb": 100182.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.07, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 33.48, "peak": 42.94, "min": 26.39}, "VIN": {"avg": 74.38, "peak": 125.78, "min": 58.72}, "VDD_CPU_SOC_MSS": {"avg": 15.79, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 33.48, "energy_joules_est": 116.54, "sample_count": 27, "duration_seconds": 3.481}, "timestamp": "2026-01-16T16:29:25.030917"}
{"image_index": 377, "image_name": "000000042102.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042102.jpg", "image_width": 246, "image_height": 640, "image_resolution": "246x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1957.045, "latencies_ms": [1957.045], "images_per_second": 0.511, "prompt_tokens": 9, "response_tokens_est": 28, "n_tiles": 10, "output_text": "The image shows a man dressed in a formal blue suit, white shirt, and striped tie, standing against a plain white wall.", "error": null, "sys_before": {"cpu_percent": 42.0, "ram_used_mb": 25589.5, "ram_available_mb": 100182.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25589.5, "ram_available_mb": 100182.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4613.5, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 6643.4, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.84, "peak": 15.44, "min": 14.22}, "VDD_GPU": {"avg": 35.03, "peak": 40.16, "min": 27.57}, "VIN": {"avg": 82.76, "peak": 128.68, "min": 66.4}, "VDD_CPU_SOC_MSS": {"avg": 15.64, "peak": 16.53, "min": 15.35}}, "power_watts_avg": 35.03, "energy_joules_est": 68.57, "sample_count": 15, "duration_seconds": 1.958}, "timestamp": "2026-01-16T16:29:27.055228"}
{"image_index": 377, "image_name": "000000042102.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042102.jpg", "image_width": 246, "image_height": 640, "image_resolution": "246x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2304.213, "latencies_ms": [2304.213], "images_per_second": 0.434, "prompt_tokens": 23, "response_tokens_est": 37, "n_tiles": 10, "output_text": "1. man\n2. suit\n3. tie\n4. skirt\n5. black shoes\n6. black tights\n7. black bag\n8. red carpet", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25589.5, "ram_available_mb": 100182.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25589.7, "ram_available_mb": 100182.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4613.5, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 6654.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.8, "peak": 15.24, "min": 14.22}, "VDD_GPU": {"avg": 35.33, "peak": 41.34, "min": 26.79}, "VIN": {"avg": 82.54, "peak": 137.26, "min": 58.89}, "VDD_CPU_SOC_MSS": {"avg": 15.49, "peak": 16.14, "min": 14.96}}, "power_watts_avg": 35.33, "energy_joules_est": 81.42, "sample_count": 17, "duration_seconds": 2.305}, "timestamp": "2026-01-16T16:29:29.370955"}
{"image_index": 377, "image_name": "000000042102.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042102.jpg", "image_width": 246, "image_height": 640, "image_resolution": "246x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3433.046, "latencies_ms": [3433.046], "images_per_second": 0.291, "prompt_tokens": 27, "response_tokens_est": 73, "n_tiles": 10, "output_text": "The main object in the image is a young man standing in front of a plain white wall. He is dressed in a dark blue suit, white shirt, and striped tie. His right hand is slightly raised, and he is holding a black bag in his left hand. The background is a plain white wall, and the man is positioned in the foreground.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25589.7, "ram_available_mb": 100182.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 25589.7, "ram_available_mb": 100182.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4613.5, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 6657.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.84, "peak": 15.14, "min": 14.22}, "VDD_GPU": {"avg": 31.77, "peak": 40.57, "min": 25.6}, "VIN": {"avg": 72.52, "peak": 127.98, "min": 59.23}, "VDD_CPU_SOC_MSS": {"avg": 15.43, "peak": 16.14, "min": 14.96}}, "power_watts_avg": 31.77, "energy_joules_est": 109.09, "sample_count": 27, "duration_seconds": 3.434}, "timestamp": "2026-01-16T16:29:32.811019"}
{"image_index": 377, "image_name": "000000042102.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042102.jpg", "image_width": 246, "image_height": 640, "image_resolution": "246x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2891.238, "latencies_ms": [2891.238], "images_per_second": 0.346, "prompt_tokens": 21, "response_tokens_est": 56, "n_tiles": 10, "output_text": "The image depicts a man dressed in a formal blue suit and tie standing against a plain white wall. He is holding a black handbag in his left hand and has his right hand slightly extended. The setting appears to be indoors, possibly in a professional or formal environment.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25589.7, "ram_available_mb": 100182.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 25589.7, "ram_available_mb": 100182.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4613.5, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 6652.6, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.84, "peak": 15.14, "min": 14.22}, "VDD_GPU": {"avg": 33.01, "peak": 41.34, "min": 25.6}, "VIN": {"avg": 73.04, "peak": 118.59, "min": 61.4}, "VDD_CPU_SOC_MSS": {"avg": 15.5, "peak": 16.14, "min": 14.96}}, "power_watts_avg": 33.01, "energy_joules_est": 95.45, "sample_count": 22, "duration_seconds": 2.892}, "timestamp": "2026-01-16T16:29:35.708513"}
{"image_index": 377, "image_name": "000000042102.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042102.jpg", "image_width": 246, "image_height": 640, "image_resolution": "246x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2860.312, "latencies_ms": [2860.312], "images_per_second": 0.35, "prompt_tokens": 19, "response_tokens_est": 55, "n_tiles": 10, "output_text": "The man is dressed in a dark blue suit with a white shirt and a striped tie featuring red, blue, and white colors. The lighting is bright, casting a clear shadow of the man on the wall. The man is wearing black tights and black shoes.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25589.7, "ram_available_mb": 100182.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25589.5, "ram_available_mb": 100182.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4613.5, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 6651.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.83, "peak": 15.14, "min": 14.22}, "VDD_GPU": {"avg": 32.87, "peak": 40.56, "min": 25.6}, "VIN": {"avg": 75.89, "peak": 115.68, "min": 65.28}, "VDD_CPU_SOC_MSS": {"avg": 15.48, "peak": 16.14, "min": 14.96}}, "power_watts_avg": 32.87, "energy_joules_est": 94.03, "sample_count": 22, "duration_seconds": 2.861}, "timestamp": "2026-01-16T16:29:38.575042"}
{"image_index": 378, "image_name": "000000042178.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042178.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2782.329, "latencies_ms": [2782.329], "images_per_second": 0.359, "prompt_tokens": 9, "response_tokens_est": 41, "n_tiles": 12, "output_text": "The image depicts a series of metallic, cylindrical structures arranged in a row, likely part of a large industrial or storage facility, with a hazy, overcast sky in the background.", "error": null, "sys_before": {"cpu_percent": 47.7, "ram_used_mb": 25589.5, "ram_available_mb": 100182.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 25589.5, "ram_available_mb": 100182.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.14, "peak": 16.15, "min": 13.9}, "VDD_GPU": {"avg": 34.21, "peak": 41.36, "min": 26.79}, "VIN": {"avg": 77.35, "peak": 102.02, "min": 62.86}, "VDD_CPU_SOC_MSS": {"avg": 15.88, "peak": 17.32, "min": 14.96}}, "power_watts_avg": 34.21, "energy_joules_est": 95.19, "sample_count": 21, "duration_seconds": 2.783}, "timestamp": "2026-01-16T16:29:41.441723"}
{"image_index": 378, "image_name": "000000042178.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042178.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2883.371, "latencies_ms": [2883.371], "images_per_second": 0.347, "prompt_tokens": 23, "response_tokens_est": 40, "n_tiles": 12, "output_text": "1. Power lines\n2. Power lines\n3. Power lines\n4. Power lines\n5. Power lines\n6. Power lines\n7. Power lines\n8. Power lines", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25589.5, "ram_available_mb": 100182.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 25590.5, "ram_available_mb": 100181.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.06, "peak": 15.65, "min": 14.43}, "VDD_GPU": {"avg": 34.84, "peak": 42.54, "min": 26.79}, "VIN": {"avg": 74.93, "peak": 110.82, "min": 58.78}, "VDD_CPU_SOC_MSS": {"avg": 15.67, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 34.84, "energy_joules_est": 100.47, "sample_count": 22, "duration_seconds": 2.884}, "timestamp": "2026-01-16T16:29:44.331499"}
{"image_index": 378, "image_name": "000000042178.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042178.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3223.023, "latencies_ms": [3223.023], "images_per_second": 0.31, "prompt_tokens": 27, "response_tokens_est": 50, "n_tiles": 12, "output_text": "The main objects in the image are a series of train tracks and overhead electric lines. The tracks are located in the foreground, while the electric lines stretch across the background. The tracks are near the foreground, and the electric lines are further back.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25590.5, "ram_available_mb": 100181.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 25590.7, "ram_available_mb": 100181.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.09, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 33.97, "peak": 42.53, "min": 26.39}, "VIN": {"avg": 72.91, "peak": 120.06, "min": 56.58}, "VDD_CPU_SOC_MSS": {"avg": 15.65, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 33.97, "energy_joules_est": 109.5, "sample_count": 25, "duration_seconds": 3.223}, "timestamp": "2026-01-16T16:29:47.560962"}
{"image_index": 378, "image_name": "000000042178.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042178.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 4192.745, "latencies_ms": [4192.745], "images_per_second": 0.239, "prompt_tokens": 21, "response_tokens_est": 79, "n_tiles": 12, "output_text": "The image depicts a large, industrial setting with numerous overhead electrical wires and poles. The scene is bathed in a hazy, yellowish light, suggesting either early morning or late afternoon. The setting appears to be a large, enclosed area, possibly a warehouse or a factory, with numerous large, cylindrical metallic structures, possibly storage tanks or containers, lined up in rows.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25590.7, "ram_available_mb": 100181.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25590.4, "ram_available_mb": 100181.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.1, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 32.08, "peak": 42.94, "min": 26.0}, "VIN": {"avg": 76.02, "peak": 131.03, "min": 58.34}, "VDD_CPU_SOC_MSS": {"avg": 15.73, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 32.08, "energy_joules_est": 134.52, "sample_count": 33, "duration_seconds": 4.193}, "timestamp": "2026-01-16T16:29:51.760281"}
{"image_index": 378, "image_name": "000000042178.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042178.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 4733.935, "latencies_ms": [4733.935], "images_per_second": 0.211, "prompt_tokens": 19, "response_tokens_est": 95, "n_tiles": 12, "output_text": "The image depicts a scene with a hazy, overcast sky, casting a soft, diffused light over the entire scene. The lighting is muted, with a warm, yellowish tint, creating a serene and somewhat melancholic atmosphere. The materials in the scene include a series of metallic structures, possibly part of a train or industrial setup, and the overall color palette is dominated by shades of gray and brown, with occasional hints of metallic sheen.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25590.4, "ram_available_mb": 100181.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 25590.4, "ram_available_mb": 100181.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.09, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 31.44, "peak": 42.53, "min": 26.0}, "VIN": {"avg": 75.5, "peak": 119.29, "min": 58.99}, "VDD_CPU_SOC_MSS": {"avg": 15.7, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 31.44, "energy_joules_est": 148.85, "sample_count": 37, "duration_seconds": 4.734}, "timestamp": "2026-01-16T16:29:56.500700"}
{"image_index": 379, "image_name": "000000042276.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042276.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2851.248, "latencies_ms": [2851.248], "images_per_second": 0.351, "prompt_tokens": 9, "response_tokens_est": 43, "n_tiles": 12, "output_text": "The image shows a cluttered bathroom floor with various items scattered around, including a white toilet, a pair of black shoes, a black helmet, a blue and white bottle, and a pair of orange gloves.", "error": null, "sys_before": {"cpu_percent": 43.0, "ram_used_mb": 25590.4, "ram_available_mb": 100181.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 25590.4, "ram_available_mb": 100181.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.13, "peak": 16.15, "min": 13.79}, "VDD_GPU": {"avg": 33.82, "peak": 41.74, "min": 26.79}, "VIN": {"avg": 74.44, "peak": 119.8, "min": 65.17}, "VDD_CPU_SOC_MSS": {"avg": 15.98, "peak": 17.32, "min": 14.96}}, "power_watts_avg": 33.82, "energy_joules_est": 96.45, "sample_count": 22, "duration_seconds": 2.852}, "timestamp": "2026-01-16T16:29:59.439480"}
{"image_index": 379, "image_name": "000000042276.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042276.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2923.863, "latencies_ms": [2923.863], "images_per_second": 0.342, "prompt_tokens": 23, "response_tokens_est": 41, "n_tiles": 12, "output_text": "1. Toilet\n2. Shoebox\n3. Gloves\n4. Gloves\n5. Gloves\n6. Gloves\n7. Gloves\n8. Gloves", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25590.4, "ram_available_mb": 100181.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 25590.2, "ram_available_mb": 100182.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.81, "peak": 15.34, "min": 14.22}, "VDD_GPU": {"avg": 34.59, "peak": 42.94, "min": 26.39}, "VIN": {"avg": 73.95, "peak": 118.14, "min": 58.72}, "VDD_CPU_SOC_MSS": {"avg": 15.42, "peak": 16.14, "min": 14.96}}, "power_watts_avg": 34.59, "energy_joules_est": 101.15, "sample_count": 23, "duration_seconds": 2.924}, "timestamp": "2026-01-16T16:30:02.370475"}
{"image_index": 379, "image_name": "000000042276.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042276.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3505.832, "latencies_ms": [3505.832], "images_per_second": 0.285, "prompt_tokens": 27, "response_tokens_est": 58, "n_tiles": 12, "output_text": "The main objects in the image are a toilet, a pair of shoes, and a helmet. The toilet is located in the background, while the shoes and helmet are in the foreground. The shoes are near the toilet, and the helmet is on the floor in front of the toilet.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25590.2, "ram_available_mb": 100182.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25590.2, "ram_available_mb": 100182.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.75, "peak": 15.34, "min": 14.12}, "VDD_GPU": {"avg": 33.22, "peak": 42.53, "min": 26.0}, "VIN": {"avg": 73.65, "peak": 122.88, "min": 58.76}, "VDD_CPU_SOC_MSS": {"avg": 15.24, "peak": 16.14, "min": 14.56}}, "power_watts_avg": 33.22, "energy_joules_est": 116.47, "sample_count": 27, "duration_seconds": 3.506}, "timestamp": "2026-01-16T16:30:05.886308"}
{"image_index": 379, "image_name": "000000042276.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042276.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 4128.794, "latencies_ms": [4128.794], "images_per_second": 0.242, "prompt_tokens": 21, "response_tokens_est": 76, "n_tiles": 12, "output_text": "The image depicts a cluttered bathroom floor with various items scattered around. The bathroom has a white toilet, a green stool with a plant on it, a pair of shoes, a black and white helmet, and a white towel hanging on the wall. The scene appears to be a mess, with items strewn about, suggesting recent activity or a lack of organization.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25590.2, "ram_available_mb": 100182.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25590.2, "ram_available_mb": 100182.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.73, "peak": 15.34, "min": 14.12}, "VDD_GPU": {"avg": 31.9, "peak": 42.53, "min": 25.6}, "VIN": {"avg": 71.61, "peak": 119.59, "min": 51.32}, "VDD_CPU_SOC_MSS": {"avg": 15.19, "peak": 16.14, "min": 14.56}}, "power_watts_avg": 31.9, "energy_joules_est": 131.72, "sample_count": 32, "duration_seconds": 4.129}, "timestamp": "2026-01-16T16:30:10.021783"}
{"image_index": 379, "image_name": "000000042276.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042276.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2884.363, "latencies_ms": [2884.363], "images_per_second": 0.347, "prompt_tokens": 19, "response_tokens_est": 40, "n_tiles": 12, "output_text": "The bathroom is well-lit with natural light, creating a bright and airy atmosphere. The walls are painted in a light color, complementing the overall clean and tidy look of the space.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25590.2, "ram_available_mb": 100182.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 25590.2, "ram_available_mb": 100182.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.77, "peak": 15.34, "min": 14.12}, "VDD_GPU": {"avg": 34.75, "peak": 42.54, "min": 26.39}, "VIN": {"avg": 75.51, "peak": 117.29, "min": 58.59}, "VDD_CPU_SOC_MSS": {"avg": 15.33, "peak": 16.14, "min": 14.56}}, "power_watts_avg": 34.75, "energy_joules_est": 100.25, "sample_count": 22, "duration_seconds": 2.885}, "timestamp": "2026-01-16T16:30:12.912434"}
{"image_index": 380, "image_name": "000000042296.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042296.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1336.677, "latencies_ms": [1336.677], "images_per_second": 0.748, "prompt_tokens": 9, "response_tokens_est": 28, "n_tiles": 6, "output_text": "A polar bear is seen playfully splashing in a pool of water, with one of its paws gripping a green ball.", "error": null, "sys_before": {"cpu_percent": 42.0, "ram_used_mb": 25590.2, "ram_available_mb": 100182.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25590.2, "ram_available_mb": 100182.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.31, "peak": 14.91, "min": 13.82}, "VDD_GPU": {"avg": 32.34, "peak": 37.8, "min": 26.4}, "VIN": {"avg": 69.99, "peak": 109.18, "min": 50.16}, "VDD_CPU_SOC_MSS": {"avg": 14.96, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 32.34, "energy_joules_est": 43.24, "sample_count": 10, "duration_seconds": 1.337}, "timestamp": "2026-01-16T16:30:14.316459"}
{"image_index": 380, "image_name": "000000042296.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042296.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 952.503, "latencies_ms": [952.503], "images_per_second": 1.05, "prompt_tokens": 23, "response_tokens_est": 12, "n_tiles": 6, "output_text": "ball: 3\npolar bear: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25590.2, "ram_available_mb": 100182.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 25590.7, "ram_available_mb": 100181.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.63, "peak": 14.83, "min": 14.22}, "VDD_GPU": {"avg": 35.85, "peak": 38.6, "min": 31.12}, "VIN": {"avg": 74.54, "peak": 110.27, "min": 57.23}, "VDD_CPU_SOC_MSS": {"avg": 15.24, "peak": 15.35, "min": 14.96}}, "power_watts_avg": 35.85, "energy_joules_est": 34.16, "sample_count": 7, "duration_seconds": 0.953}, "timestamp": "2026-01-16T16:30:15.275197"}
{"image_index": 380, "image_name": "000000042296.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042296.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2259.543, "latencies_ms": [2259.543], "images_per_second": 0.443, "prompt_tokens": 27, "response_tokens_est": 61, "n_tiles": 6, "output_text": "The main objects in the image are a polar bear and a ball. The polar bear is in the foreground, swimming near the green and yellow balls. The green ball is near the polar bear, while the yellow ball is further away. The background features a sandy area with rocks and some vegetation.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25590.7, "ram_available_mb": 100181.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 25590.4, "ram_available_mb": 100181.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.85, "peak": 15.32, "min": 14.12}, "VDD_GPU": {"avg": 30.82, "peak": 40.18, "min": 24.03}, "VIN": {"avg": 72.1, "peak": 114.51, "min": 53.19}, "VDD_CPU_SOC_MSS": {"avg": 15.47, "peak": 16.14, "min": 14.96}}, "power_watts_avg": 30.82, "energy_joules_est": 69.66, "sample_count": 17, "duration_seconds": 2.26}, "timestamp": "2026-01-16T16:30:17.545647"}
{"image_index": 380, "image_name": "000000042296.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042296.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2819.875, "latencies_ms": [2819.875], "images_per_second": 0.355, "prompt_tokens": 21, "response_tokens_est": 82, "n_tiles": 6, "output_text": "The image depicts a polar bear in a shallow body of water, likely a pool or a shallow river, engaging in playful behavior. The bear is seen holding a ball in its mouth, with its paws submerged in the water, creating a dynamic and lively scene. The setting appears to be an outdoor environment, possibly a zoo or a wildlife sanctuary, given the presence of rocks and natural surroundings.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25590.4, "ram_available_mb": 100181.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 25590.7, "ram_available_mb": 100181.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.02, "peak": 15.32, "min": 14.22}, "VDD_GPU": {"avg": 28.27, "peak": 38.21, "min": 23.64}, "VIN": {"avg": 74.32, "peak": 131.31, "min": 62.83}, "VDD_CPU_SOC_MSS": {"avg": 15.71, "peak": 16.14, "min": 14.96}}, "power_watts_avg": 28.27, "energy_joules_est": 79.73, "sample_count": 22, "duration_seconds": 2.82}, "timestamp": "2026-01-16T16:30:20.371729"}
{"image_index": 380, "image_name": "000000042296.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042296.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2023.882, "latencies_ms": [2023.882], "images_per_second": 0.494, "prompt_tokens": 19, "response_tokens_est": 52, "n_tiles": 6, "output_text": "The image depicts a polar bear swimming in a body of water, surrounded by green and yellow inflatable balls. The lighting is bright, indicating a sunny day, and the water is clear, allowing visibility of the bear's fur and the surrounding environment.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25590.7, "ram_available_mb": 100181.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 25590.7, "ram_available_mb": 100181.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.89, "peak": 15.22, "min": 14.32}, "VDD_GPU": {"avg": 30.17, "peak": 38.21, "min": 24.03}, "VIN": {"avg": 75.91, "peak": 116.91, "min": 52.62}, "VDD_CPU_SOC_MSS": {"avg": 15.59, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 30.17, "energy_joules_est": 61.07, "sample_count": 15, "duration_seconds": 2.024}, "timestamp": "2026-01-16T16:30:22.403354"}
{"image_index": 381, "image_name": "000000042528.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042528.jpg", "image_width": 640, "image_height": 495, "image_resolution": "640x495", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2214.389, "latencies_ms": [2214.389], "images_per_second": 0.452, "prompt_tokens": 9, "response_tokens_est": 24, "n_tiles": 12, "output_text": "A person is holding a Samsung flip phone in their hand, with a pair of blue jeans visible in the foreground.", "error": null, "sys_before": {"cpu_percent": 44.4, "ram_used_mb": 25590.7, "ram_available_mb": 100181.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 25590.4, "ram_available_mb": 100181.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.15, "peak": 16.15, "min": 13.9}, "VDD_GPU": {"avg": 35.66, "peak": 40.97, "min": 28.77}, "VIN": {"avg": 77.73, "peak": 115.69, "min": 58.85}, "VDD_CPU_SOC_MSS": {"avg": 16.16, "peak": 17.32, "min": 15.36}}, "power_watts_avg": 35.66, "energy_joules_est": 78.98, "sample_count": 17, "duration_seconds": 2.215}, "timestamp": "2026-01-16T16:30:24.707290"}
{"image_index": 381, "image_name": "000000042528.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042528.jpg", "image_width": 640, "image_height": 495, "image_resolution": "640x495", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2278.194, "latencies_ms": [2278.194], "images_per_second": 0.439, "prompt_tokens": 23, "response_tokens_est": 22, "n_tiles": 12, "output_text": "cell phone: 1\nclothes: 1\nfloor: 1\ntable: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25590.4, "ram_available_mb": 100181.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 2.4, "ram_used_mb": 25590.4, "ram_available_mb": 100181.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.06, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 37.33, "peak": 43.33, "min": 29.56}, "VIN": {"avg": 76.49, "peak": 108.54, "min": 55.49}, "VDD_CPU_SOC_MSS": {"avg": 15.77, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 37.33, "energy_joules_est": 85.06, "sample_count": 17, "duration_seconds": 2.279}, "timestamp": "2026-01-16T16:30:26.996009"}
{"image_index": 381, "image_name": "000000042528.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042528.jpg", "image_width": 640, "image_height": 495, "image_resolution": "640x495", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3253.083, "latencies_ms": [3253.083], "images_per_second": 0.307, "prompt_tokens": 27, "response_tokens_est": 51, "n_tiles": 12, "output_text": "The main object in the foreground is a person's hand holding a mobile phone. The phone is positioned near the person's hand, with the screen facing towards the camera. The background features a wooden floor and a window with a view of the sky.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25590.4, "ram_available_mb": 100181.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 25590.6, "ram_available_mb": 100181.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.07, "peak": 15.65, "min": 14.32}, "VDD_GPU": {"avg": 34.16, "peak": 43.33, "min": 26.4}, "VIN": {"avg": 75.04, "peak": 114.53, "min": 58.44}, "VDD_CPU_SOC_MSS": {"avg": 15.73, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 34.16, "energy_joules_est": 111.14, "sample_count": 25, "duration_seconds": 3.254}, "timestamp": "2026-01-16T16:30:30.255696"}
{"image_index": 381, "image_name": "000000042528.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042528.jpg", "image_width": 640, "image_height": 495, "image_resolution": "640x495", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3147.2, "latencies_ms": [3147.2], "images_per_second": 0.318, "prompt_tokens": 21, "response_tokens_est": 48, "n_tiles": 12, "output_text": "The image depicts a person holding a Samsung flip phone, which is placed on a wooden surface. The background shows a wooden floor and a window with a view of the sky. The person's hand is visible, holding the phone.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25590.6, "ram_available_mb": 100181.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 25590.4, "ram_available_mb": 100181.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.08, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 34.19, "peak": 42.94, "min": 26.39}, "VIN": {"avg": 72.39, "peak": 121.44, "min": 57.26}, "VDD_CPU_SOC_MSS": {"avg": 15.79, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 34.19, "energy_joules_est": 107.62, "sample_count": 24, "duration_seconds": 3.148}, "timestamp": "2026-01-16T16:30:33.409718"}
{"image_index": 381, "image_name": "000000042528.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042528.jpg", "image_width": 640, "image_height": 495, "image_resolution": "640x495", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2944.084, "latencies_ms": [2944.084], "images_per_second": 0.34, "prompt_tokens": 19, "response_tokens_est": 42, "n_tiles": 12, "output_text": "The image features a wooden floor with a light-colored wooden ceiling, creating a warm and cozy atmosphere. The lighting is natural, with sunlight streaming in through a window, casting a soft glow on the scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25590.4, "ram_available_mb": 100181.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 25590.4, "ram_available_mb": 100181.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.1, "peak": 15.75, "min": 14.53}, "VDD_GPU": {"avg": 34.51, "peak": 42.94, "min": 26.79}, "VIN": {"avg": 76.02, "peak": 112.96, "min": 59.25}, "VDD_CPU_SOC_MSS": {"avg": 15.8, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 34.51, "energy_joules_est": 101.61, "sample_count": 23, "duration_seconds": 2.944}, "timestamp": "2026-01-16T16:30:36.361209"}
{"image_index": 382, "image_name": "000000042563.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042563.jpg", "image_width": 624, "image_height": 640, "image_resolution": "624x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 583.485, "latencies_ms": [583.485], "images_per_second": 1.714, "prompt_tokens": 9, "response_tokens_est": 23, "n_tiles": 1, "output_text": "A yellow train is traveling on a snowy track surrounded by snow-covered trees and a cloudy sky.", "error": null, "sys_before": {"cpu_percent": 15.6, "ram_used_mb": 25590.4, "ram_available_mb": 100181.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 6.0, "ram_used_mb": 25590.4, "ram_available_mb": 100181.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 4845.7, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.94, "peak": 15.32, "min": 14.51}, "VDD_GPU": {"avg": 25.12, "peak": 27.57, "min": 23.25}, "VIN": {"avg": 61.42, "peak": 63.99, "min": 56.85}, "VDD_CPU_SOC_MSS": {"avg": 15.55, "peak": 16.14, "min": 14.96}}, "power_watts_avg": 25.12, "energy_joules_est": 14.67, "sample_count": 4, "duration_seconds": 0.584}, "timestamp": "2026-01-16T16:30:36.971220"}
{"image_index": 382, "image_name": "000000042563.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042563.jpg", "image_width": 624, "image_height": 640, "image_resolution": "624x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2813.804, "latencies_ms": [2813.804], "images_per_second": 0.355, "prompt_tokens": 23, "response_tokens_est": 129, "n_tiles": 1, "output_text": "train: 1\ntrees: 3\ntrees: 3\ntrees: 3\ntrees: 3\ntrees: 3\ntrees: 3\ntrees: 3\ntrees: 3\ntrees: 3\ntrees: 3\ntrees: 3\ntrees: 3\ntrees: 3\ntrees: 3\ntrees: 3\ntrees: 3\ntrees: 3\ntrees: 3\ntrees: 3\ntrees: 3\ntrees:", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25590.4, "ram_available_mb": 100181.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25590.6, "ram_available_mb": 100181.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 4856.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.65, "peak": 15.82, "min": 15.01}, "VDD_GPU": {"avg": 21.12, "peak": 26.39, "min": 20.09}, "VIN": {"avg": 64.21, "peak": 71.73, "min": 59.62}, "VDD_CPU_SOC_MSS": {"avg": 16.45, "peak": 16.54, "min": 15.75}}, "power_watts_avg": 21.12, "energy_joules_est": 59.44, "sample_count": 22, "duration_seconds": 2.814}, "timestamp": "2026-01-16T16:30:39.791178"}
{"image_index": 382, "image_name": "000000042563.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042563.jpg", "image_width": 624, "image_height": 640, "image_resolution": "624x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1565.855, "latencies_ms": [1565.855], "images_per_second": 0.639, "prompt_tokens": 27, "response_tokens_est": 70, "n_tiles": 1, "output_text": "The main object in the foreground is a yellow train traveling on the tracks. The train is positioned near the center of the image, slightly to the left. The background features a snowy landscape with trees and a snow-covered ground. The train is in the middle of the tracks, which stretch into the distance, creating a sense of depth.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25590.6, "ram_available_mb": 100181.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25590.6, "ram_available_mb": 100181.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 4859.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.64, "peak": 15.82, "min": 15.32}, "VDD_GPU": {"avg": 21.31, "peak": 24.42, "min": 20.09}, "VIN": {"avg": 63.45, "peak": 70.49, "min": 56.13}, "VDD_CPU_SOC_MSS": {"avg": 16.47, "peak": 16.54, "min": 16.14}}, "power_watts_avg": 21.31, "energy_joules_est": 33.38, "sample_count": 12, "duration_seconds": 1.566}, "timestamp": "2026-01-16T16:30:41.362797"}
{"image_index": 382, "image_name": "000000042563.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042563.jpg", "image_width": 624, "image_height": 640, "image_resolution": "624x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 881.442, "latencies_ms": [881.442], "images_per_second": 1.135, "prompt_tokens": 21, "response_tokens_est": 38, "n_tiles": 1, "output_text": "The image depicts a snowy scene with a train traveling on a railway track. The train is moving through a snowy landscape with trees and a cloudy sky in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25590.6, "ram_available_mb": 100181.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25590.9, "ram_available_mb": 100181.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 4854.7, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.55, "peak": 15.72, "min": 15.32}, "VDD_GPU": {"avg": 22.26, "peak": 24.03, "min": 20.88}, "VIN": {"avg": 65.06, "peak": 68.65, "min": 61.46}, "VDD_CPU_SOC_MSS": {"avg": 16.4, "peak": 16.54, "min": 16.14}}, "power_watts_avg": 22.26, "energy_joules_est": 19.63, "sample_count": 6, "duration_seconds": 0.882}, "timestamp": "2026-01-16T16:30:42.254101"}
{"image_index": 382, "image_name": "000000042563.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042563.jpg", "image_width": 624, "image_height": 640, "image_resolution": "624x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1376.076, "latencies_ms": [1376.076], "images_per_second": 0.727, "prompt_tokens": 19, "response_tokens_est": 61, "n_tiles": 1, "output_text": "The image depicts a snowy scene with a train traveling on a snow-covered railway track. The train is painted in a bright yellow color, contrasting sharply against the white snow. The sky is overcast, with a uniform gray hue, indicating a cold and possibly snowy weather condition.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25590.9, "ram_available_mb": 100181.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25590.9, "ram_available_mb": 100181.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 4853.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.57, "peak": 15.82, "min": 15.11}, "VDD_GPU": {"avg": 21.59, "peak": 24.42, "min": 20.09}, "VIN": {"avg": 62.22, "peak": 67.66, "min": 58.27}, "VDD_CPU_SOC_MSS": {"avg": 16.42, "peak": 16.54, "min": 16.14}}, "power_watts_avg": 21.59, "energy_joules_est": 29.72, "sample_count": 10, "duration_seconds": 1.377}, "timestamp": "2026-01-16T16:30:43.637638"}
{"image_index": 383, "image_name": "000000042628.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042628.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1198.824, "latencies_ms": [1198.824], "images_per_second": 0.834, "prompt_tokens": 9, "response_tokens_est": 23, "n_tiles": 6, "output_text": "A group of people are walking through a snowy area, with a large pile of snow blocking the path.", "error": null, "sys_before": {"cpu_percent": 38.3, "ram_used_mb": 25590.6, "ram_available_mb": 100181.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 25590.6, "ram_available_mb": 100181.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.65, "peak": 14.94, "min": 14.12}, "VDD_GPU": {"avg": 31.86, "peak": 36.24, "min": 27.18}, "VIN": {"avg": 74.14, "peak": 123.63, "min": 57.11}, "VDD_CPU_SOC_MSS": {"avg": 15.48, "peak": 16.14, "min": 14.96}}, "power_watts_avg": 31.86, "energy_joules_est": 38.21, "sample_count": 9, "duration_seconds": 1.199}, "timestamp": "2026-01-16T16:30:44.875044"}
{"image_index": 383, "image_name": "000000042628.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042628.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1695.658, "latencies_ms": [1695.658], "images_per_second": 0.59, "prompt_tokens": 23, "response_tokens_est": 40, "n_tiles": 6, "output_text": "object: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25590.6, "ram_available_mb": 100181.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25590.6, "ram_available_mb": 100181.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.78, "peak": 15.11, "min": 14.22}, "VDD_GPU": {"avg": 31.78, "peak": 38.98, "min": 24.82}, "VIN": {"avg": 74.08, "peak": 116.0, "min": 58.13}, "VDD_CPU_SOC_MSS": {"avg": 15.38, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 31.78, "energy_joules_est": 53.91, "sample_count": 13, "duration_seconds": 1.696}, "timestamp": "2026-01-16T16:30:46.576975"}
{"image_index": 383, "image_name": "000000042628.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042628.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2528.374, "latencies_ms": [2528.374], "images_per_second": 0.396, "prompt_tokens": 27, "response_tokens_est": 71, "n_tiles": 6, "output_text": "The main objects in the image are a snow-covered area with a fire hydrant in the foreground and a group of people in the background. The fire hydrant is located near the center of the image, while the people are further away, indicating a spatial relationship where the fire hydrant is closer to the viewer and the people are farther away.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25590.6, "ram_available_mb": 100181.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 25590.6, "ram_available_mb": 100181.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.98, "peak": 15.32, "min": 14.22}, "VDD_GPU": {"avg": 29.07, "peak": 38.21, "min": 23.64}, "VIN": {"avg": 69.77, "peak": 116.22, "min": 57.18}, "VDD_CPU_SOC_MSS": {"avg": 15.58, "peak": 16.14, "min": 14.96}}, "power_watts_avg": 29.07, "energy_joules_est": 73.51, "sample_count": 19, "duration_seconds": 2.529}, "timestamp": "2026-01-16T16:30:49.111288"}
{"image_index": 383, "image_name": "000000042628.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042628.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2284.192, "latencies_ms": [2284.192], "images_per_second": 0.438, "prompt_tokens": 21, "response_tokens_est": 62, "n_tiles": 6, "output_text": "The image depicts a snowy urban scene with a large pile of snow blocking a street. Several people are walking through the snow, some carrying bags and others with snowboards. The setting appears to be a city street during winter, with vehicles parked on the side and a brick wall in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25590.6, "ram_available_mb": 100181.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 25590.8, "ram_available_mb": 100181.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.94, "peak": 15.22, "min": 14.32}, "VDD_GPU": {"avg": 29.47, "peak": 37.82, "min": 24.03}, "VIN": {"avg": 69.18, "peak": 97.69, "min": 56.54}, "VDD_CPU_SOC_MSS": {"avg": 15.56, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 29.47, "energy_joules_est": 67.33, "sample_count": 17, "duration_seconds": 2.285}, "timestamp": "2026-01-16T16:30:51.401947"}
{"image_index": 383, "image_name": "000000042628.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042628.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2333.364, "latencies_ms": [2333.364], "images_per_second": 0.429, "prompt_tokens": 19, "response_tokens_est": 64, "n_tiles": 6, "output_text": "The image depicts a snow-covered scene with a large pile of snow in the foreground, a few people walking through the snow, and a fire hydrant partially buried in the snow. The lighting is dim, suggesting it is either early morning or late afternoon, and the overall atmosphere is cold and wintry.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25590.8, "ram_available_mb": 100181.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 25590.8, "ram_available_mb": 100181.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.94, "peak": 15.22, "min": 14.32}, "VDD_GPU": {"avg": 29.28, "peak": 38.59, "min": 24.03}, "VIN": {"avg": 67.02, "peak": 107.86, "min": 55.3}, "VDD_CPU_SOC_MSS": {"avg": 15.51, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 29.28, "energy_joules_est": 68.34, "sample_count": 18, "duration_seconds": 2.334}, "timestamp": "2026-01-16T16:30:53.742616"}
{"image_index": 384, "image_name": "000000042888.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042888.jpg", "image_width": 500, "image_height": 374, "image_resolution": "500x374", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2680.932, "latencies_ms": [2680.932], "images_per_second": 0.373, "prompt_tokens": 9, "response_tokens_est": 38, "n_tiles": 12, "output_text": "The image shows a street sign indicating a \"TOW ZONE\" with a red arrow pointing to the right, and a yellow sign with a cartoonish face attached to a pole.", "error": null, "sys_before": {"cpu_percent": 42.3, "ram_used_mb": 25590.8, "ram_available_mb": 100181.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 25590.8, "ram_available_mb": 100181.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.16, "peak": 16.05, "min": 14.1}, "VDD_GPU": {"avg": 34.43, "peak": 40.97, "min": 27.18}, "VIN": {"avg": 73.56, "peak": 114.77, "min": 58.27}, "VDD_CPU_SOC_MSS": {"avg": 15.94, "peak": 17.32, "min": 15.35}}, "power_watts_avg": 34.43, "energy_joules_est": 92.32, "sample_count": 20, "duration_seconds": 2.681}, "timestamp": "2026-01-16T16:30:56.500181"}
{"image_index": 384, "image_name": "000000042888.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042888.jpg", "image_width": 500, "image_height": 374, "image_resolution": "500x374", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2879.24, "latencies_ms": [2879.24], "images_per_second": 0.347, "prompt_tokens": 23, "response_tokens_est": 40, "n_tiles": 12, "output_text": "object: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25590.8, "ram_available_mb": 100181.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 25590.8, "ram_available_mb": 100181.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.07, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 35.11, "peak": 42.94, "min": 26.79}, "VIN": {"avg": 75.02, "peak": 131.89, "min": 58.71}, "VDD_CPU_SOC_MSS": {"avg": 15.73, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 35.11, "energy_joules_est": 101.11, "sample_count": 22, "duration_seconds": 2.88}, "timestamp": "2026-01-16T16:30:59.390105"}
{"image_index": 384, "image_name": "000000042888.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042888.jpg", "image_width": 500, "image_height": 374, "image_resolution": "500x374", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3269.246, "latencies_ms": [3269.246], "images_per_second": 0.306, "prompt_tokens": 27, "response_tokens_est": 51, "n_tiles": 12, "output_text": "The main objects in the image are a street sign and a tree. The street sign is positioned in the foreground, while the tree is situated in the background. The tree is located near the street sign, providing a natural backdrop to the urban setting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25590.8, "ram_available_mb": 100181.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 25590.6, "ram_available_mb": 100181.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.8, "peak": 15.34, "min": 14.22}, "VDD_GPU": {"avg": 33.94, "peak": 42.92, "min": 26.0}, "VIN": {"avg": 76.53, "peak": 118.54, "min": 54.92}, "VDD_CPU_SOC_MSS": {"avg": 15.35, "peak": 16.14, "min": 14.56}}, "power_watts_avg": 33.94, "energy_joules_est": 110.97, "sample_count": 25, "duration_seconds": 3.27}, "timestamp": "2026-01-16T16:31:02.669269"}
{"image_index": 384, "image_name": "000000042888.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042888.jpg", "image_width": 500, "image_height": 374, "image_resolution": "500x374", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3439.574, "latencies_ms": [3439.574], "images_per_second": 0.291, "prompt_tokens": 21, "response_tokens_est": 56, "n_tiles": 12, "output_text": "The image depicts a street scene with a yellow traffic sign attached to a metal pole. The sign reads \"TOW ZONE\" and features a red prohibition symbol. The surrounding area is lush with green foliage, indicating a park or a tree-lined street.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25590.6, "ram_available_mb": 100181.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25590.3, "ram_available_mb": 100181.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.72, "peak": 15.34, "min": 14.12}, "VDD_GPU": {"avg": 33.24, "peak": 42.54, "min": 26.0}, "VIN": {"avg": 71.85, "peak": 112.9, "min": 58.59}, "VDD_CPU_SOC_MSS": {"avg": 15.32, "peak": 16.14, "min": 14.56}}, "power_watts_avg": 33.24, "energy_joules_est": 114.35, "sample_count": 27, "duration_seconds": 3.44}, "timestamp": "2026-01-16T16:31:06.116383"}
{"image_index": 384, "image_name": "000000042888.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042888.jpg", "image_width": 500, "image_height": 374, "image_resolution": "500x374", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3430.461, "latencies_ms": [3430.461], "images_per_second": 0.292, "prompt_tokens": 19, "response_tokens_est": 56, "n_tiles": 12, "output_text": "The image features a street sign with a white background and a red border, featuring a red arrow indicating a no-parking zone. The sign is mounted on a metal pole, and the surrounding area is lush with green foliage, suggesting a sunny day with ample sunlight.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25590.3, "ram_available_mb": 100181.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 25590.3, "ram_available_mb": 100181.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.75, "peak": 15.34, "min": 14.12}, "VDD_GPU": {"avg": 33.24, "peak": 42.53, "min": 26.0}, "VIN": {"avg": 73.26, "peak": 120.78, "min": 58.62}, "VDD_CPU_SOC_MSS": {"avg": 15.27, "peak": 16.14, "min": 14.56}}, "power_watts_avg": 33.24, "energy_joules_est": 114.04, "sample_count": 27, "duration_seconds": 3.431}, "timestamp": "2026-01-16T16:31:09.553474"}
{"image_index": 385, "image_name": "000000042889.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042889.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2450.056, "latencies_ms": [2450.056], "images_per_second": 0.408, "prompt_tokens": 9, "response_tokens_est": 31, "n_tiles": 12, "output_text": "The image shows a collection of vintage electronic devices, including a keyboard, a mouse, and a small white speaker, all resting on a red surface.", "error": null, "sys_before": {"cpu_percent": 41.9, "ram_used_mb": 25590.3, "ram_available_mb": 100181.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 25590.6, "ram_available_mb": 100181.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.12, "peak": 16.15, "min": 13.82}, "VDD_GPU": {"avg": 35.3, "peak": 41.36, "min": 27.57}, "VIN": {"avg": 76.93, "peak": 120.65, "min": 58.61}, "VDD_CPU_SOC_MSS": {"avg": 15.81, "peak": 17.32, "min": 14.56}}, "power_watts_avg": 35.3, "energy_joules_est": 86.5, "sample_count": 18, "duration_seconds": 2.45}, "timestamp": "2026-01-16T16:31:12.090199"}
{"image_index": 385, "image_name": "000000042889.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042889.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2770.335, "latencies_ms": [2770.335], "images_per_second": 0.361, "prompt_tokens": 23, "response_tokens_est": 37, "n_tiles": 12, "output_text": "1. teddy bear\n2. glasses\n3. earphones\n4. cell phone\n5. keyboard\n6. mouse\n7. cord\n8. computer mouse", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25590.6, "ram_available_mb": 100181.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 25590.6, "ram_available_mb": 100181.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.08, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 35.51, "peak": 42.54, "min": 27.18}, "VIN": {"avg": 75.19, "peak": 115.08, "min": 58.81}, "VDD_CPU_SOC_MSS": {"avg": 15.67, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 35.51, "energy_joules_est": 98.39, "sample_count": 21, "duration_seconds": 2.771}, "timestamp": "2026-01-16T16:31:14.866816"}
{"image_index": 385, "image_name": "000000042889.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042889.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3555.981, "latencies_ms": [3555.981], "images_per_second": 0.281, "prompt_tokens": 27, "response_tokens_est": 60, "n_tiles": 12, "output_text": "The main objects in the image are a teddy bear, a keyboard, and a mouse. The teddy bear is positioned in the foreground, while the keyboard and mouse are in the background. The keyboard is on the left side of the image, and the mouse is on the right side.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25590.6, "ram_available_mb": 100181.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25590.3, "ram_available_mb": 100181.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.1, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 33.29, "peak": 42.53, "min": 26.39}, "VIN": {"avg": 72.59, "peak": 111.43, "min": 58.77}, "VDD_CPU_SOC_MSS": {"avg": 15.7, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 33.29, "energy_joules_est": 118.39, "sample_count": 27, "duration_seconds": 3.556}, "timestamp": "2026-01-16T16:31:18.429140"}
{"image_index": 385, "image_name": "000000042889.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042889.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 4058.79, "latencies_ms": [4058.79], "images_per_second": 0.246, "prompt_tokens": 21, "response_tokens_est": 75, "n_tiles": 12, "output_text": "The image depicts a cozy, dimly lit room with a red surface, possibly a table or desk, where a collection of vintage electronic devices and a plush teddy bear are placed. The teddy bear, with its soft fur and round glasses, appears to be a comforting presence amidst the retro gadgets, suggesting a nostalgic or sentimental atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25590.3, "ram_available_mb": 100181.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 25590.6, "ram_available_mb": 100181.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.09, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 32.26, "peak": 42.92, "min": 26.01}, "VIN": {"avg": 72.68, "peak": 105.89, "min": 57.88}, "VDD_CPU_SOC_MSS": {"avg": 15.67, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 32.26, "energy_joules_est": 130.95, "sample_count": 32, "duration_seconds": 4.059}, "timestamp": "2026-01-16T16:31:22.494115"}
{"image_index": 385, "image_name": "000000042889.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000042889.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2740.362, "latencies_ms": [2740.362], "images_per_second": 0.365, "prompt_tokens": 19, "response_tokens_est": 36, "n_tiles": 12, "output_text": "The image features a teddy bear with a furry texture, wearing glasses and sitting on a red surface. The lighting is dim, casting shadows and creating a cozy atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25590.6, "ram_available_mb": 100181.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 25590.8, "ram_available_mb": 100181.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.09, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 35.26, "peak": 42.94, "min": 27.18}, "VIN": {"avg": 81.09, "peak": 124.47, "min": 58.41}, "VDD_CPU_SOC_MSS": {"avg": 15.76, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 35.26, "energy_joules_est": 96.63, "sample_count": 21, "duration_seconds": 2.741}, "timestamp": "2026-01-16T16:31:25.240584"}
{"image_index": 386, "image_name": "000000043314.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000043314.jpg", "image_width": 500, "image_height": 376, "image_resolution": "500x376", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2547.0, "latencies_ms": [2547.0], "images_per_second": 0.393, "prompt_tokens": 9, "response_tokens_est": 34, "n_tiles": 12, "output_text": "The image captures a dynamic moment of a skier in mid-air, performing a jump over a snowy mountain slope, with the snow and trees in the background.", "error": null, "sys_before": {"cpu_percent": 43.1, "ram_used_mb": 25590.6, "ram_available_mb": 100181.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 25590.8, "ram_available_mb": 100181.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.15, "peak": 16.15, "min": 13.92}, "VDD_GPU": {"avg": 34.74, "peak": 41.76, "min": 27.18}, "VIN": {"avg": 73.47, "peak": 106.3, "min": 56.34}, "VDD_CPU_SOC_MSS": {"avg": 15.86, "peak": 17.32, "min": 14.96}}, "power_watts_avg": 34.74, "energy_joules_est": 88.49, "sample_count": 20, "duration_seconds": 2.547}, "timestamp": "2026-01-16T16:31:27.875053"}
{"image_index": 386, "image_name": "000000043314.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000043314.jpg", "image_width": 500, "image_height": 376, "image_resolution": "500x376", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2877.672, "latencies_ms": [2877.672], "images_per_second": 0.348, "prompt_tokens": 23, "response_tokens_est": 40, "n_tiles": 12, "output_text": "object: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25590.8, "ram_available_mb": 100181.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 25590.5, "ram_available_mb": 100181.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.1, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 35.15, "peak": 43.33, "min": 26.79}, "VIN": {"avg": 74.03, "peak": 106.58, "min": 58.65}, "VDD_CPU_SOC_MSS": {"avg": 15.66, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 35.15, "energy_joules_est": 101.17, "sample_count": 22, "duration_seconds": 2.878}, "timestamp": "2026-01-16T16:31:30.763416"}
{"image_index": 386, "image_name": "000000043314.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000043314.jpg", "image_width": 500, "image_height": 376, "image_resolution": "500x376", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 4095.181, "latencies_ms": [4095.181], "images_per_second": 0.244, "prompt_tokens": 27, "response_tokens_est": 76, "n_tiles": 12, "output_text": "The main objects in the image are a snowboarder and a snowy mountain landscape. The snowboarder is positioned in the foreground, slightly off-center to the left, while the snowy mountain landscape is in the background, stretching across the entire image. The snowboarder is near the snow-covered mountain, with the snowy terrain and trees in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25590.5, "ram_available_mb": 100181.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25590.5, "ram_available_mb": 100181.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.1, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 32.41, "peak": 42.94, "min": 26.39}, "VIN": {"avg": 73.14, "peak": 115.53, "min": 58.53}, "VDD_CPU_SOC_MSS": {"avg": 15.66, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 32.41, "energy_joules_est": 132.74, "sample_count": 31, "duration_seconds": 4.096}, "timestamp": "2026-01-16T16:31:34.870128"}
{"image_index": 386, "image_name": "000000043314.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000043314.jpg", "image_width": 500, "image_height": 376, "image_resolution": "500x376", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3351.153, "latencies_ms": [3351.153], "images_per_second": 0.298, "prompt_tokens": 21, "response_tokens_est": 54, "n_tiles": 12, "output_text": "The image captures a dynamic winter scene featuring a skier in mid-air, executing a jump over a snowy mountain. The skier is dressed in a white and orange outfit, and the snowy landscape is adorned with snow-covered trees and rocky terrain.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25590.5, "ram_available_mb": 100181.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 25590.5, "ram_available_mb": 100181.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.09, "peak": 15.65, "min": 14.43}, "VDD_GPU": {"avg": 33.39, "peak": 42.54, "min": 26.39}, "VIN": {"avg": 74.22, "peak": 111.65, "min": 58.53}, "VDD_CPU_SOC_MSS": {"avg": 15.65, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 33.39, "energy_joules_est": 111.91, "sample_count": 26, "duration_seconds": 3.351}, "timestamp": "2026-01-16T16:31:38.227765"}
{"image_index": 386, "image_name": "000000043314.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000043314.jpg", "image_width": 500, "image_height": 376, "image_resolution": "500x376", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2781.797, "latencies_ms": [2781.797], "images_per_second": 0.359, "prompt_tokens": 19, "response_tokens_est": 37, "n_tiles": 12, "output_text": "The image depicts a snowy mountain landscape with a clear blue sky. The snow-covered ground and trees are illuminated by the sunlight, creating a bright and vibrant scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25590.5, "ram_available_mb": 100181.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 25590.5, "ram_available_mb": 100181.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.08, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 35.28, "peak": 42.53, "min": 27.18}, "VIN": {"avg": 78.52, "peak": 117.94, "min": 58.91}, "VDD_CPU_SOC_MSS": {"avg": 15.76, "peak": 16.53, "min": 15.35}}, "power_watts_avg": 35.28, "energy_joules_est": 98.16, "sample_count": 21, "duration_seconds": 2.782}, "timestamp": "2026-01-16T16:31:41.019955"}
{"image_index": 387, "image_name": "000000043435.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000043435.jpg", "image_width": 638, "image_height": 640, "image_resolution": "638x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1185.84, "latencies_ms": [1185.84], "images_per_second": 0.843, "prompt_tokens": 9, "response_tokens_est": 33, "n_tiles": 4, "output_text": "A surfer is skillfully riding a wave on a surfboard, with the water splashing around them and the sun casting a warm glow on the scene.", "error": null, "sys_before": {"cpu_percent": 29.7, "ram_used_mb": 25590.5, "ram_available_mb": 100181.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 25591.0, "ram_available_mb": 100181.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4606.7, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5444.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.35, "peak": 15.01, "min": 13.61}, "VDD_GPU": {"avg": 30.13, "peak": 35.06, "min": 24.82}, "VIN": {"avg": 73.89, "peak": 108.29, "min": 62.81}, "VDD_CPU_SOC_MSS": {"avg": 15.01, "peak": 15.75, "min": 14.18}}, "power_watts_avg": 30.13, "energy_joules_est": 35.74, "sample_count": 8, "duration_seconds": 1.186}, "timestamp": "2026-01-16T16:31:42.263668"}
{"image_index": 387, "image_name": "000000043435.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000043435.jpg", "image_width": 638, "image_height": 640, "image_resolution": "638x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1368.962, "latencies_ms": [1368.962], "images_per_second": 0.73, "prompt_tokens": 23, "response_tokens_est": 40, "n_tiles": 4, "output_text": "object: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25591.0, "ram_available_mb": 100181.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 25590.8, "ram_available_mb": 100181.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4606.7, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5455.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.7, "peak": 15.11, "min": 14.02}, "VDD_GPU": {"avg": 29.19, "peak": 35.45, "min": 24.03}, "VIN": {"avg": 68.95, "peak": 98.1, "min": 54.32}, "VDD_CPU_SOC_MSS": {"avg": 15.27, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 29.19, "energy_joules_est": 39.97, "sample_count": 10, "duration_seconds": 1.369}, "timestamp": "2026-01-16T16:31:43.638939"}
{"image_index": 387, "image_name": "000000043435.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000043435.jpg", "image_width": 638, "image_height": 640, "image_resolution": "638x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1991.651, "latencies_ms": [1991.651], "images_per_second": 0.502, "prompt_tokens": 27, "response_tokens_est": 65, "n_tiles": 4, "output_text": "The main object in the image is a surfer riding a wave. The surfer is positioned in the foreground, slightly to the right, and is riding the wave that is in the background. The wave is large and occupies the majority of the frame, with the surfer positioned on the wave's crest.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25590.8, "ram_available_mb": 100181.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 25590.8, "ram_available_mb": 100181.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4606.7, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5458.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.87, "peak": 15.22, "min": 14.12}, "VDD_GPU": {"avg": 26.97, "peak": 35.06, "min": 22.45}, "VIN": {"avg": 69.18, "peak": 108.76, "min": 58.62}, "VDD_CPU_SOC_MSS": {"avg": 15.49, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 26.97, "energy_joules_est": 53.73, "sample_count": 15, "duration_seconds": 1.992}, "timestamp": "2026-01-16T16:31:45.637963"}
{"image_index": 387, "image_name": "000000043435.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000043435.jpg", "image_width": 638, "image_height": 640, "image_resolution": "638x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2339.721, "latencies_ms": [2339.721], "images_per_second": 0.427, "prompt_tokens": 21, "response_tokens_est": 79, "n_tiles": 4, "output_text": "The image captures a dynamic scene of a surfer riding a wave in the ocean. The surfer is skillfully maneuvering on a surfboard, with the wave curling around them, creating a frothy white crest. The setting is a vast ocean with a clear sky, and the surfer is the central focus of the image, showcasing the thrill and beauty of surfing.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25590.8, "ram_available_mb": 100181.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 25590.8, "ram_available_mb": 100181.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4606.7, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5453.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.94, "peak": 15.32, "min": 14.12}, "VDD_GPU": {"avg": 25.91, "peak": 34.66, "min": 22.45}, "VIN": {"avg": 67.89, "peak": 100.23, "min": 56.81}, "VDD_CPU_SOC_MSS": {"avg": 15.66, "peak": 16.14, "min": 14.96}}, "power_watts_avg": 25.91, "energy_joules_est": 60.63, "sample_count": 18, "duration_seconds": 2.34}, "timestamp": "2026-01-16T16:31:47.983902"}
{"image_index": 387, "image_name": "000000043435.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000043435.jpg", "image_width": 638, "image_height": 640, "image_resolution": "638x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1766.582, "latencies_ms": [1766.582], "images_per_second": 0.566, "prompt_tokens": 19, "response_tokens_est": 56, "n_tiles": 4, "output_text": "The image captures a dynamic scene of a surfer riding a wave, with the surfer and the wave in sharp focus against a backdrop of a calm ocean. The lighting is natural, with sunlight reflecting off the water and the surfer, creating a vibrant and energetic atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25590.8, "ram_available_mb": 100181.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 25591.0, "ram_available_mb": 100181.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4606.7, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5451.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.84, "peak": 15.22, "min": 14.12}, "VDD_GPU": {"avg": 27.3, "peak": 34.66, "min": 22.86}, "VIN": {"avg": 66.3, "peak": 90.55, "min": 58.16}, "VDD_CPU_SOC_MSS": {"avg": 15.48, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 27.3, "energy_joules_est": 48.24, "sample_count": 13, "duration_seconds": 1.767}, "timestamp": "2026-01-16T16:31:49.760635"}
{"image_index": 388, "image_name": "000000043581.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000043581.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1710.092, "latencies_ms": [1710.092], "images_per_second": 0.585, "prompt_tokens": 9, "response_tokens_est": 42, "n_tiles": 6, "output_text": "The image shows a pizza with various toppings, including melted cheese, pieces of meat, and green vegetables, all placed on a metal pizza tray, which is placed on a table with a blue cloth.", "error": null, "sys_before": {"cpu_percent": 42.4, "ram_used_mb": 25591.0, "ram_available_mb": 100181.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 25591.0, "ram_available_mb": 100181.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.59, "peak": 15.11, "min": 13.92}, "VDD_GPU": {"avg": 30.09, "peak": 37.42, "min": 24.42}, "VIN": {"avg": 70.11, "peak": 99.47, "min": 49.89}, "VDD_CPU_SOC_MSS": {"avg": 15.38, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 30.09, "energy_joules_est": 51.47, "sample_count": 13, "duration_seconds": 1.71}, "timestamp": "2026-01-16T16:31:51.523634"}
{"image_index": 388, "image_name": "000000043581.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000043581.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1544.355, "latencies_ms": [1544.355], "images_per_second": 0.648, "prompt_tokens": 23, "response_tokens_est": 34, "n_tiles": 6, "output_text": "1. Pizza\n2. Plate\n3. Pizza\n4. Sauce\n5. Cheese\n6. Vegetables\n7. Meat\n8. Garlic", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25591.0, "ram_available_mb": 100181.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 25591.2, "ram_available_mb": 100180.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.73, "peak": 15.01, "min": 14.22}, "VDD_GPU": {"avg": 32.05, "peak": 37.8, "min": 25.6}, "VIN": {"avg": 74.0, "peak": 109.83, "min": 60.09}, "VDD_CPU_SOC_MSS": {"avg": 15.39, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 32.05, "energy_joules_est": 49.51, "sample_count": 11, "duration_seconds": 1.545}, "timestamp": "2026-01-16T16:31:53.073987"}
{"image_index": 388, "image_name": "000000043581.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000043581.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2468.809, "latencies_ms": [2468.809], "images_per_second": 0.405, "prompt_tokens": 27, "response_tokens_est": 66, "n_tiles": 6, "output_text": "The main object in the foreground is a pizza with a variety of toppings, including cheese, vegetables, and possibly meat. The pizza is placed on a metal tray. In the background, there is a jar of sauce and a napkin or paper towel. The pizza is positioned on a table with a blue cloth underneath.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25591.2, "ram_available_mb": 100180.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 25591.2, "ram_available_mb": 100180.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.76, "peak": 15.01, "min": 14.12}, "VDD_GPU": {"avg": 28.84, "peak": 38.59, "min": 23.25}, "VIN": {"avg": 66.38, "peak": 99.41, "min": 55.91}, "VDD_CPU_SOC_MSS": {"avg": 15.37, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 28.84, "energy_joules_est": 71.21, "sample_count": 19, "duration_seconds": 2.469}, "timestamp": "2026-01-16T16:31:55.548761"}
{"image_index": 388, "image_name": "000000043581.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000043581.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2220.46, "latencies_ms": [2220.46], "images_per_second": 0.45, "prompt_tokens": 21, "response_tokens_est": 57, "n_tiles": 6, "output_text": "The image depicts a pizza on a blue plate, placed on a table with a red surface. The pizza appears to be freshly baked, with toppings of cheese, vegetables, and possibly meat. The setting suggests a casual dining environment, possibly a restaurant or a home kitchen.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25591.2, "ram_available_mb": 100180.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 25591.2, "ram_available_mb": 100180.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.73, "peak": 15.01, "min": 14.12}, "VDD_GPU": {"avg": 28.99, "peak": 38.21, "min": 23.24}, "VIN": {"avg": 72.66, "peak": 117.29, "min": 57.95}, "VDD_CPU_SOC_MSS": {"avg": 15.26, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 28.99, "energy_joules_est": 64.38, "sample_count": 17, "duration_seconds": 2.221}, "timestamp": "2026-01-16T16:31:57.775532"}
{"image_index": 388, "image_name": "000000043581.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000043581.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2247.54, "latencies_ms": [2247.54], "images_per_second": 0.445, "prompt_tokens": 19, "response_tokens_est": 58, "n_tiles": 6, "output_text": "The image features a pizza with a golden-brown crust, topped with melted cheese and various toppings, including pieces of meat and vegetables. The pizza is placed on a blue plate, and the background is dimly lit with a bluish hue, creating a cozy and inviting atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25591.2, "ram_available_mb": 100180.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 25591.2, "ram_available_mb": 100180.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.73, "peak": 15.01, "min": 14.12}, "VDD_GPU": {"avg": 29.12, "peak": 38.19, "min": 23.24}, "VIN": {"avg": 72.55, "peak": 117.2, "min": 56.3}, "VDD_CPU_SOC_MSS": {"avg": 15.24, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 29.12, "energy_joules_est": 65.46, "sample_count": 17, "duration_seconds": 2.248}, "timestamp": "2026-01-16T16:32:00.028905"}
{"image_index": 389, "image_name": "000000043737.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000043737.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2445.879, "latencies_ms": [2445.879], "images_per_second": 0.409, "prompt_tokens": 9, "response_tokens_est": 31, "n_tiles": 12, "output_text": "The image depicts a street scene during winter, with a prominent black clock mounted on a pole, surrounded by buildings and a snow-covered sidewalk.", "error": null, "sys_before": {"cpu_percent": 40.6, "ram_used_mb": 25591.2, "ram_available_mb": 100180.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 25591.5, "ram_available_mb": 100180.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.14, "peak": 16.15, "min": 14.0}, "VDD_GPU": {"avg": 34.72, "peak": 40.97, "min": 27.57}, "VIN": {"avg": 76.0, "peak": 113.4, "min": 58.94}, "VDD_CPU_SOC_MSS": {"avg": 15.99, "peak": 17.32, "min": 14.96}}, "power_watts_avg": 34.72, "energy_joules_est": 84.94, "sample_count": 19, "duration_seconds": 2.446}, "timestamp": "2026-01-16T16:32:02.550901"}
{"image_index": 389, "image_name": "000000043737.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000043737.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 3150.06, "latencies_ms": [3150.06], "images_per_second": 0.317, "prompt_tokens": 23, "response_tokens_est": 48, "n_tiles": 12, "output_text": "- clock: 1\n- building: 1\n- street: 1\n- car: 1\n- sign: 1\n- tree: 1\n- sidewalk: 1\n- building: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25591.5, "ram_available_mb": 100180.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 25591.5, "ram_available_mb": 100180.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.07, "peak": 15.65, "min": 14.43}, "VDD_GPU": {"avg": 34.3, "peak": 42.94, "min": 26.39}, "VIN": {"avg": 75.95, "peak": 117.36, "min": 58.74}, "VDD_CPU_SOC_MSS": {"avg": 15.78, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 34.3, "energy_joules_est": 108.07, "sample_count": 24, "duration_seconds": 3.151}, "timestamp": "2026-01-16T16:32:05.708910"}
{"image_index": 389, "image_name": "000000043737.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000043737.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 4265.75, "latencies_ms": [4265.75], "images_per_second": 0.234, "prompt_tokens": 27, "response_tokens_est": 81, "n_tiles": 12, "output_text": "The main objects in the image are a street clock and a row of buildings. The street clock is positioned on the left side of the image, near the foreground, and is mounted on a tall pole. The buildings are located in the background, on the right side of the image, and are partially visible. The street clock is near the sidewalk, while the buildings are further back on the street.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25591.5, "ram_available_mb": 100180.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 25591.7, "ram_available_mb": 100180.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.09, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 32.13, "peak": 42.15, "min": 26.39}, "VIN": {"avg": 72.38, "peak": 122.85, "min": 58.32}, "VDD_CPU_SOC_MSS": {"avg": 15.77, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 32.13, "energy_joules_est": 137.07, "sample_count": 32, "duration_seconds": 4.266}, "timestamp": "2026-01-16T16:32:09.981217"}
{"image_index": 389, "image_name": "000000043737.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000043737.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3954.844, "latencies_ms": [3954.844], "images_per_second": 0.253, "prompt_tokens": 21, "response_tokens_est": 72, "n_tiles": 12, "output_text": "The image depicts a snowy urban street scene during twilight. The street is lined with buildings, and a large, ornate clock is prominently displayed on a pole. The clock face shows the time as approximately 10:10. The scene is quiet, with no people visible, and the buildings have festive decorations, suggesting a holiday season.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25591.7, "ram_available_mb": 100180.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25591.0, "ram_available_mb": 100181.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.1, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 32.48, "peak": 42.54, "min": 26.39}, "VIN": {"avg": 71.23, "peak": 112.29, "min": 58.48}, "VDD_CPU_SOC_MSS": {"avg": 15.8, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 32.48, "energy_joules_est": 128.46, "sample_count": 30, "duration_seconds": 3.955}, "timestamp": "2026-01-16T16:32:13.946685"}
{"image_index": 389, "image_name": "000000043737.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000043737.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3714.942, "latencies_ms": [3714.942], "images_per_second": 0.269, "prompt_tokens": 19, "response_tokens_est": 65, "n_tiles": 12, "output_text": "The image depicts a street scene during winter, with a prominent black clock mounted on a pole. The clock face is white with black Roman numerals, and the hands indicate it is around 10:10. The surrounding area is covered in snow, and the sky is clear, suggesting a cold but sunny day.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25591.0, "ram_available_mb": 100181.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25590.6, "ram_available_mb": 100181.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.09, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 32.83, "peak": 42.54, "min": 26.39}, "VIN": {"avg": 73.65, "peak": 112.57, "min": 56.35}, "VDD_CPU_SOC_MSS": {"avg": 15.6, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 32.83, "energy_joules_est": 121.98, "sample_count": 29, "duration_seconds": 3.716}, "timestamp": "2026-01-16T16:32:17.668193"}
{"image_index": 390, "image_name": "000000043816.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000043816.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1626.593, "latencies_ms": [1626.593], "images_per_second": 0.615, "prompt_tokens": 9, "response_tokens_est": 39, "n_tiles": 6, "output_text": "A baseball player in a white and blue uniform is swinging a bat at a baseball, while a catcher in red gear is crouched behind home plate, ready to catch the ball.", "error": null, "sys_before": {"cpu_percent": 34.8, "ram_used_mb": 25590.6, "ram_available_mb": 100181.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 25590.6, "ram_available_mb": 100181.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.58, "peak": 15.11, "min": 13.92}, "VDD_GPU": {"avg": 31.22, "peak": 38.21, "min": 25.21}, "VIN": {"avg": 69.75, "peak": 97.93, "min": 50.21}, "VDD_CPU_SOC_MSS": {"avg": 15.19, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 31.22, "energy_joules_est": 50.79, "sample_count": 12, "duration_seconds": 1.627}, "timestamp": "2026-01-16T16:32:19.347105"}
{"image_index": 390, "image_name": "000000043816.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000043816.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1967.545, "latencies_ms": [1967.545], "images_per_second": 0.508, "prompt_tokens": 23, "response_tokens_est": 50, "n_tiles": 6, "output_text": "baseball player: 1\ncatcher: 1\numpire: 1\nhome plate: 1\nglove: 1\nbat: 1\npitcher: 1\ncatcher's mask: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25590.6, "ram_available_mb": 100181.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 25590.6, "ram_available_mb": 100181.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.88, "peak": 15.22, "min": 14.22}, "VDD_GPU": {"avg": 30.36, "peak": 38.19, "min": 24.42}, "VIN": {"avg": 71.37, "peak": 101.56, "min": 52.14}, "VDD_CPU_SOC_MSS": {"avg": 15.51, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 30.36, "energy_joules_est": 59.75, "sample_count": 15, "duration_seconds": 1.968}, "timestamp": "2026-01-16T16:32:21.320982"}
{"image_index": 390, "image_name": "000000043816.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000043816.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2881.899, "latencies_ms": [2881.899], "images_per_second": 0.347, "prompt_tokens": 27, "response_tokens_est": 84, "n_tiles": 6, "output_text": "The baseball player is positioned in the foreground, wearing a white and blue uniform, holding a bat, and preparing to swing at the incoming ball. The catcher is in the background, crouched behind home plate, wearing a red uniform and a catcher's mask, ready to catch the ball. The baseball is in the air, slightly to the left of the player, indicating the action of the swing.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25590.6, "ram_available_mb": 100181.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 25590.8, "ram_available_mb": 100181.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.01, "peak": 15.32, "min": 14.32}, "VDD_GPU": {"avg": 28.29, "peak": 38.19, "min": 23.64}, "VIN": {"avg": 71.0, "peak": 112.68, "min": 55.36}, "VDD_CPU_SOC_MSS": {"avg": 15.6, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 28.29, "energy_joules_est": 81.54, "sample_count": 22, "duration_seconds": 2.882}, "timestamp": "2026-01-16T16:32:24.208626"}
{"image_index": 390, "image_name": "000000043816.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000043816.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2232.864, "latencies_ms": [2232.864], "images_per_second": 0.448, "prompt_tokens": 21, "response_tokens_est": 60, "n_tiles": 6, "output_text": "The image captures a dynamic moment during a baseball game, featuring a batter in mid-swing, a catcher in position, and a umpire ready to make a call. The scene takes place on a well-maintained baseball field with a dirt infield and green grass surrounding it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25590.8, "ram_available_mb": 100181.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 25590.8, "ram_available_mb": 100181.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.95, "peak": 15.32, "min": 14.32}, "VDD_GPU": {"avg": 29.4, "peak": 37.8, "min": 24.03}, "VIN": {"avg": 68.23, "peak": 114.16, "min": 48.94}, "VDD_CPU_SOC_MSS": {"avg": 15.45, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 29.4, "energy_joules_est": 65.66, "sample_count": 17, "duration_seconds": 2.233}, "timestamp": "2026-01-16T16:32:26.447689"}
{"image_index": 390, "image_name": "000000043816.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000043816.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1809.917, "latencies_ms": [1809.917], "images_per_second": 0.553, "prompt_tokens": 19, "response_tokens_est": 44, "n_tiles": 6, "output_text": "The baseball player is wearing a blue helmet and white pinstripe uniform, which stands out against the green grass and brown dirt of the field. The lighting is bright and sunny, casting sharp shadows on the ground.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25590.8, "ram_available_mb": 100181.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 25590.8, "ram_available_mb": 100181.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.86, "peak": 15.22, "min": 14.32}, "VDD_GPU": {"avg": 31.09, "peak": 38.19, "min": 24.82}, "VIN": {"avg": 79.24, "peak": 118.32, "min": 61.48}, "VDD_CPU_SOC_MSS": {"avg": 15.47, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 31.09, "energy_joules_est": 56.28, "sample_count": 13, "duration_seconds": 1.81}, "timestamp": "2026-01-16T16:32:28.263552"}
{"image_index": 391, "image_name": "000000044068.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044068.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2277.428, "latencies_ms": [2277.428], "images_per_second": 0.439, "prompt_tokens": 9, "response_tokens_est": 26, "n_tiles": 12, "output_text": "A plush teddy bear with a red and white checkered bow around its neck is resting on a red leather chair.", "error": null, "sys_before": {"cpu_percent": 45.3, "ram_used_mb": 25590.6, "ram_available_mb": 100181.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 2.6, "ram_used_mb": 25591.3, "ram_available_mb": 100180.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.24, "peak": 16.15, "min": 14.2}, "VDD_GPU": {"avg": 35.61, "peak": 40.97, "min": 28.37}, "VIN": {"avg": 75.66, "peak": 112.17, "min": 61.44}, "VDD_CPU_SOC_MSS": {"avg": 16.12, "peak": 17.32, "min": 15.35}}, "power_watts_avg": 35.61, "energy_joules_est": 81.11, "sample_count": 17, "duration_seconds": 2.278}, "timestamp": "2026-01-16T16:32:30.611258"}
{"image_index": 391, "image_name": "000000044068.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044068.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 3084.023, "latencies_ms": [3084.023], "images_per_second": 0.324, "prompt_tokens": 23, "response_tokens_est": 46, "n_tiles": 12, "output_text": "1. teddy bear\n2. chair\n3. cushion\n4. wicker basket\n5. red bow tie\n6. red pillow\n7. blue and white striped fabric\n8. wicker chair", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25591.3, "ram_available_mb": 100180.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 25591.3, "ram_available_mb": 100180.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.07, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 34.49, "peak": 42.94, "min": 26.8}, "VIN": {"avg": 74.76, "peak": 109.08, "min": 58.87}, "VDD_CPU_SOC_MSS": {"avg": 15.73, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 34.49, "energy_joules_est": 106.38, "sample_count": 23, "duration_seconds": 3.084}, "timestamp": "2026-01-16T16:32:33.701779"}
{"image_index": 391, "image_name": "000000044068.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044068.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3829.638, "latencies_ms": [3829.638], "images_per_second": 0.261, "prompt_tokens": 27, "response_tokens_est": 68, "n_tiles": 12, "output_text": "The main object in the foreground is a brown teddy bear with a red and white plaid bow around its neck. The bear is resting on a red leather cushion. In the background, there is a wicker chair with a brown cushion. The wicker chair is partially visible, with the cushion resting on the chair's seat.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25591.3, "ram_available_mb": 100180.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25591.3, "ram_available_mb": 100180.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.09, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 32.67, "peak": 42.92, "min": 26.39}, "VIN": {"avg": 74.57, "peak": 113.74, "min": 58.47}, "VDD_CPU_SOC_MSS": {"avg": 15.75, "peak": 16.53, "min": 15.35}}, "power_watts_avg": 32.67, "energy_joules_est": 125.12, "sample_count": 30, "duration_seconds": 3.83}, "timestamp": "2026-01-16T16:32:37.537821"}
{"image_index": 391, "image_name": "000000044068.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044068.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 4060.203, "latencies_ms": [4060.203], "images_per_second": 0.246, "prompt_tokens": 21, "response_tokens_est": 75, "n_tiles": 12, "output_text": "The image depicts a cozy indoor scene featuring a plush teddy bear with a red and white checkered bow around its neck. The bear is resting on a cushioned surface, possibly a chair, with a woven basket underneath. The setting appears to be a warm and inviting space, possibly a living room or bedroom, with soft lighting and a homely atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25591.3, "ram_available_mb": 100180.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 25591.3, "ram_available_mb": 100180.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.09, "peak": 15.75, "min": 14.53}, "VDD_GPU": {"avg": 32.31, "peak": 42.53, "min": 26.0}, "VIN": {"avg": 73.79, "peak": 114.42, "min": 58.52}, "VDD_CPU_SOC_MSS": {"avg": 15.62, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 32.31, "energy_joules_est": 131.2, "sample_count": 31, "duration_seconds": 4.061}, "timestamp": "2026-01-16T16:32:41.604420"}
{"image_index": 391, "image_name": "000000044068.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044068.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3147.605, "latencies_ms": [3147.605], "images_per_second": 0.318, "prompt_tokens": 19, "response_tokens_est": 48, "n_tiles": 12, "output_text": "The image features a brown teddy bear with a red and white checkered bow around its neck. The bear is resting on a red leather chair with a woven seat cushion. The lighting is soft and warm, creating a cozy atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25591.3, "ram_available_mb": 100180.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 25591.5, "ram_available_mb": 100180.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.08, "peak": 15.65, "min": 14.51}, "VDD_GPU": {"avg": 34.15, "peak": 42.53, "min": 26.39}, "VIN": {"avg": 76.99, "peak": 121.59, "min": 46.28}, "VDD_CPU_SOC_MSS": {"avg": 15.75, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 34.15, "energy_joules_est": 107.51, "sample_count": 24, "duration_seconds": 3.148}, "timestamp": "2026-01-16T16:32:44.758246"}
{"image_index": 392, "image_name": "000000044195.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044195.jpg", "image_width": 361, "image_height": 500, "image_resolution": "361x500", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1979.72, "latencies_ms": [1979.72], "images_per_second": 0.505, "prompt_tokens": 9, "response_tokens_est": 17, "n_tiles": 12, "output_text": "Two people are standing on a snowy mountain slope, holding a snowboard.", "error": null, "sys_before": {"cpu_percent": 43.0, "ram_used_mb": 25591.5, "ram_available_mb": 100180.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 2.3, "ram_used_mb": 25591.5, "ram_available_mb": 100180.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.14, "peak": 16.05, "min": 13.9}, "VDD_GPU": {"avg": 36.9, "peak": 41.36, "min": 30.33}, "VIN": {"avg": 76.7, "peak": 119.04, "min": 58.77}, "VDD_CPU_SOC_MSS": {"avg": 15.98, "peak": 17.32, "min": 14.96}}, "power_watts_avg": 36.9, "energy_joules_est": 73.06, "sample_count": 15, "duration_seconds": 1.98}, "timestamp": "2026-01-16T16:32:46.829050"}
{"image_index": 392, "image_name": "000000044195.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044195.jpg", "image_width": 361, "image_height": 500, "image_resolution": "361x500", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2776.545, "latencies_ms": [2776.545], "images_per_second": 0.36, "prompt_tokens": 23, "response_tokens_est": 37, "n_tiles": 12, "output_text": "1. Snowboard\n2. Person\n3. Snow\n4. Ski\n5. Ski poles\n6. Ski boots\n7. Ski jacket\n8. Ski pants", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25591.5, "ram_available_mb": 100180.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 25591.5, "ram_available_mb": 100180.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.09, "peak": 15.75, "min": 14.53}, "VDD_GPU": {"avg": 35.67, "peak": 43.33, "min": 27.18}, "VIN": {"avg": 80.23, "peak": 121.89, "min": 58.81}, "VDD_CPU_SOC_MSS": {"avg": 15.71, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 35.67, "energy_joules_est": 99.06, "sample_count": 21, "duration_seconds": 2.777}, "timestamp": "2026-01-16T16:32:49.612109"}
{"image_index": 392, "image_name": "000000044195.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044195.jpg", "image_width": 361, "image_height": 500, "image_resolution": "361x500", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 4124.408, "latencies_ms": [4124.408], "images_per_second": 0.242, "prompt_tokens": 27, "response_tokens_est": 77, "n_tiles": 12, "output_text": "The main objects in the image are two individuals standing on a snowy slope. The person on the left is wearing a red jacket and black pants, while the person on the right is wearing a red jacket and black pants. The snowy slope is in the foreground, with the individuals standing near the center of the image. The background features a clear blue sky and distant mountains.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25591.5, "ram_available_mb": 100180.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 25590.5, "ram_available_mb": 100181.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.12, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 32.24, "peak": 42.92, "min": 26.39}, "VIN": {"avg": 74.28, "peak": 111.33, "min": 58.85}, "VDD_CPU_SOC_MSS": {"avg": 15.7, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 32.24, "energy_joules_est": 132.99, "sample_count": 33, "duration_seconds": 4.125}, "timestamp": "2026-01-16T16:32:53.743291"}
{"image_index": 392, "image_name": "000000044195.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044195.jpg", "image_width": 361, "image_height": 500, "image_resolution": "361x500", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3417.569, "latencies_ms": [3417.569], "images_per_second": 0.293, "prompt_tokens": 21, "response_tokens_est": 56, "n_tiles": 12, "output_text": "The image depicts a snowy mountainous landscape under a clear blue sky. Two individuals are standing on the snow, one holding a snowboard. The snowboarder is wearing a red jacket and black pants, while the other is in a red jacket and black pants.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25590.5, "ram_available_mb": 100181.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 25590.4, "ram_available_mb": 100181.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.1, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 33.36, "peak": 42.54, "min": 26.39}, "VIN": {"avg": 71.68, "peak": 109.87, "min": 55.83}, "VDD_CPU_SOC_MSS": {"avg": 15.66, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 33.36, "energy_joules_est": 114.03, "sample_count": 27, "duration_seconds": 3.418}, "timestamp": "2026-01-16T16:32:57.167343"}
{"image_index": 392, "image_name": "000000044195.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044195.jpg", "image_width": 361, "image_height": 500, "image_resolution": "361x500", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3113.247, "latencies_ms": [3113.247], "images_per_second": 0.321, "prompt_tokens": 19, "response_tokens_est": 47, "n_tiles": 12, "output_text": "The image depicts a bright, sunny day with clear blue skies. The snowy landscape is illuminated by the sun, casting a warm glow on the snow and creating a vivid contrast between the bright sunlight and the darker shadows.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25590.4, "ram_available_mb": 100181.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 25590.3, "ram_available_mb": 100181.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.09, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 34.35, "peak": 42.94, "min": 26.39}, "VIN": {"avg": 80.34, "peak": 121.94, "min": 58.63}, "VDD_CPU_SOC_MSS": {"avg": 15.68, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 34.35, "energy_joules_est": 106.97, "sample_count": 24, "duration_seconds": 3.114}, "timestamp": "2026-01-16T16:33:00.287540"}
{"image_index": 393, "image_name": "000000044260.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044260.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1602.389, "latencies_ms": [1602.389], "images_per_second": 0.624, "prompt_tokens": 9, "response_tokens_est": 38, "n_tiles": 6, "output_text": "The image depicts a close-up view of a tree branch adorned with several red apples, some of which are hanging from the branch, while the rest are resting on the ground.", "error": null, "sys_before": {"cpu_percent": 34.8, "ram_used_mb": 25590.3, "ram_available_mb": 100181.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 25590.3, "ram_available_mb": 100181.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.53, "peak": 15.11, "min": 13.92}, "VDD_GPU": {"avg": 31.18, "peak": 38.19, "min": 24.82}, "VIN": {"avg": 73.95, "peak": 107.98, "min": 53.82}, "VDD_CPU_SOC_MSS": {"avg": 15.19, "peak": 15.75, "min": 14.57}}, "power_watts_avg": 31.18, "energy_joules_est": 49.97, "sample_count": 12, "duration_seconds": 1.603}, "timestamp": "2026-01-16T16:33:01.944413"}
{"image_index": 393, "image_name": "000000044260.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044260.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 900.251, "latencies_ms": [900.251], "images_per_second": 1.111, "prompt_tokens": 23, "response_tokens_est": 10, "n_tiles": 6, "output_text": "apple: 4\ntree: 1", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25590.3, "ram_available_mb": 100181.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 25590.3, "ram_available_mb": 100181.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.68, "peak": 14.94, "min": 14.22}, "VDD_GPU": {"avg": 36.17, "peak": 38.59, "min": 32.7}, "VIN": {"avg": 73.53, "peak": 98.33, "min": 51.94}, "VDD_CPU_SOC_MSS": {"avg": 15.28, "peak": 15.74, "min": 14.96}}, "power_watts_avg": 36.17, "energy_joules_est": 32.58, "sample_count": 6, "duration_seconds": 0.901}, "timestamp": "2026-01-16T16:33:02.850780"}
{"image_index": 393, "image_name": "000000044260.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044260.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2440.89, "latencies_ms": [2440.89], "images_per_second": 0.41, "prompt_tokens": 27, "response_tokens_est": 65, "n_tiles": 6, "output_text": "The main objects in the image are a tree branch with red apples hanging from it. The apples are located near the center of the image, with the branch extending towards the right side. The background features a blurred natural setting with more trees and foliage, creating a sense of depth and context for the apples.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25590.3, "ram_available_mb": 100181.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 25590.3, "ram_available_mb": 100181.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.72, "peak": 15.01, "min": 14.02}, "VDD_GPU": {"avg": 29.73, "peak": 40.56, "min": 23.25}, "VIN": {"avg": 69.99, "peak": 115.91, "min": 54.97}, "VDD_CPU_SOC_MSS": {"avg": 15.31, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 29.73, "energy_joules_est": 72.58, "sample_count": 19, "duration_seconds": 2.441}, "timestamp": "2026-01-16T16:33:05.298217"}
{"image_index": 393, "image_name": "000000044260.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044260.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2020.333, "latencies_ms": [2020.333], "images_per_second": 0.495, "prompt_tokens": 21, "response_tokens_est": 50, "n_tiles": 6, "output_text": "The image depicts a rustic scene of an old, gnarled tree with a few red apples hanging from its branches. The background is blurred, but hints of a forested area can be seen, suggesting a natural, outdoor setting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25590.3, "ram_available_mb": 100181.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 25590.3, "ram_available_mb": 100181.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.7, "peak": 15.01, "min": 14.12}, "VDD_GPU": {"avg": 29.86, "peak": 38.21, "min": 23.64}, "VIN": {"avg": 71.04, "peak": 102.75, "min": 55.26}, "VDD_CPU_SOC_MSS": {"avg": 15.22, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 29.86, "energy_joules_est": 60.34, "sample_count": 15, "duration_seconds": 2.021}, "timestamp": "2026-01-16T16:33:07.324555"}
{"image_index": 393, "image_name": "000000044260.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044260.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2548.719, "latencies_ms": [2548.719], "images_per_second": 0.392, "prompt_tokens": 19, "response_tokens_est": 69, "n_tiles": 6, "output_text": "The image features a tree with a brown trunk and branches, adorned with red apples. The apples are bright red, contrasting against the darker tones of the tree bark. The lighting is soft and warm, suggesting a late afternoon or early evening setting, with a bokeh effect in the background, indicating a shallow depth of field.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25590.3, "ram_available_mb": 100181.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 25590.3, "ram_available_mb": 100181.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.79, "peak": 15.11, "min": 14.02}, "VDD_GPU": {"avg": 28.38, "peak": 38.19, "min": 23.24}, "VIN": {"avg": 71.21, "peak": 109.2, "min": 62.56}, "VDD_CPU_SOC_MSS": {"avg": 15.34, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 28.38, "energy_joules_est": 72.35, "sample_count": 20, "duration_seconds": 2.549}, "timestamp": "2026-01-16T16:33:09.879262"}
{"image_index": 394, "image_name": "000000044279.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044279.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1176.458, "latencies_ms": [1176.458], "images_per_second": 0.85, "prompt_tokens": 9, "response_tokens_est": 22, "n_tiles": 6, "output_text": "The image depicts a busy kitchen scene with two men wearing white uniforms, engaged in food preparation activities.", "error": null, "sys_before": {"cpu_percent": 37.9, "ram_used_mb": 25590.3, "ram_available_mb": 100181.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 25590.6, "ram_available_mb": 100181.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.37, "peak": 14.81, "min": 13.92}, "VDD_GPU": {"avg": 32.56, "peak": 37.42, "min": 27.18}, "VIN": {"avg": 74.25, "peak": 109.35, "min": 59.23}, "VDD_CPU_SOC_MSS": {"avg": 15.09, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 32.56, "energy_joules_est": 38.32, "sample_count": 9, "duration_seconds": 1.177}, "timestamp": "2026-01-16T16:33:11.105989"}
{"image_index": 394, "image_name": "000000044279.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044279.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1515.8, "latencies_ms": [1515.8], "images_per_second": 0.66, "prompt_tokens": 23, "response_tokens_est": 32, "n_tiles": 6, "output_text": "1. Chef\n2. Chef\n3. Chef\n4. Chef\n5. Chef\n6. Chef\n7. Chef\n8. Chef", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25590.6, "ram_available_mb": 100181.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 25590.3, "ram_available_mb": 100181.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.55, "peak": 14.81, "min": 14.02}, "VDD_GPU": {"avg": 32.94, "peak": 39.38, "min": 25.6}, "VIN": {"avg": 75.22, "peak": 127.43, "min": 60.17}, "VDD_CPU_SOC_MSS": {"avg": 15.14, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 32.94, "energy_joules_est": 49.95, "sample_count": 11, "duration_seconds": 1.516}, "timestamp": "2026-01-16T16:33:12.628223"}
{"image_index": 394, "image_name": "000000044279.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044279.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3421.962, "latencies_ms": [3421.962], "images_per_second": 0.292, "prompt_tokens": 27, "response_tokens_est": 100, "n_tiles": 6, "output_text": "In the image, the main objects are the two men working in the kitchen. The man in the foreground is focused on pouring a substance from a container into a bowl, while the man in the background is handling a tray of food. The kitchen is well-equipped with various utensils and appliances, and the countertops are cluttered with various items. The man in the foreground is positioned near the center of the image, while the man in the background is slightly to the left.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25590.3, "ram_available_mb": 100181.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 25590.3, "ram_available_mb": 100181.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.81, "peak": 15.11, "min": 14.02}, "VDD_GPU": {"avg": 27.2, "peak": 38.6, "min": 23.24}, "VIN": {"avg": 68.04, "peak": 108.61, "min": 55.84}, "VDD_CPU_SOC_MSS": {"avg": 15.46, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 27.2, "energy_joules_est": 93.08, "sample_count": 27, "duration_seconds": 3.422}, "timestamp": "2026-01-16T16:33:16.056145"}
{"image_index": 394, "image_name": "000000044279.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044279.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2523.381, "latencies_ms": [2523.381], "images_per_second": 0.396, "prompt_tokens": 21, "response_tokens_est": 68, "n_tiles": 6, "output_text": "The image depicts a bustling kitchen scene with two men wearing white uniforms, engaged in food preparation. They are surrounded by various kitchen tools and equipment, including a large stainless steel pot, a cutting board, and a stainless steel sink. The setting appears to be a professional kitchen, likely in a restaurant or a food service establishment.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25590.3, "ram_available_mb": 100181.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 25590.3, "ram_available_mb": 100181.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.76, "peak": 15.01, "min": 14.02}, "VDD_GPU": {"avg": 28.57, "peak": 37.82, "min": 23.24}, "VIN": {"avg": 70.62, "peak": 114.39, "min": 56.45}, "VDD_CPU_SOC_MSS": {"avg": 15.23, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 28.57, "energy_joules_est": 72.11, "sample_count": 19, "duration_seconds": 2.524}, "timestamp": "2026-01-16T16:33:18.586069"}
{"image_index": 394, "image_name": "000000044279.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044279.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1959.877, "latencies_ms": [1959.877], "images_per_second": 0.51, "prompt_tokens": 19, "response_tokens_est": 48, "n_tiles": 6, "output_text": "The image depicts a bustling kitchen scene with a metallic and industrial aesthetic. The lighting is bright, casting a warm glow over the workspace. The materials used include stainless steel surfaces, metal utensils, and various kitchen tools.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25590.3, "ram_available_mb": 100181.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 25590.3, "ram_available_mb": 100181.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.68, "peak": 15.01, "min": 14.02}, "VDD_GPU": {"avg": 30.1, "peak": 38.6, "min": 23.64}, "VIN": {"avg": 71.8, "peak": 113.47, "min": 59.79}, "VDD_CPU_SOC_MSS": {"avg": 15.2, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 30.1, "energy_joules_est": 59.01, "sample_count": 15, "duration_seconds": 1.961}, "timestamp": "2026-01-16T16:33:20.552293"}
{"image_index": 395, "image_name": "000000044590.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044590.jpg", "image_width": 640, "image_height": 246, "image_resolution": "640x246", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2045.657, "latencies_ms": [2045.657], "images_per_second": 0.489, "prompt_tokens": 9, "response_tokens_est": 31, "n_tiles": 10, "output_text": "A group of people is gathered around a row of motorcycles parked on the side of a road, with some standing and others sitting on the bikes.", "error": null, "sys_before": {"cpu_percent": 50.6, "ram_used_mb": 25590.3, "ram_available_mb": 100181.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 25590.6, "ram_available_mb": 100181.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4613.5, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 6643.4, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.8, "peak": 15.34, "min": 14.0}, "VDD_GPU": {"avg": 34.77, "peak": 39.39, "min": 27.57}, "VIN": {"avg": 71.9, "peak": 108.37, "min": 56.02}, "VDD_CPU_SOC_MSS": {"avg": 15.53, "peak": 16.14, "min": 14.96}}, "power_watts_avg": 34.77, "energy_joules_est": 71.14, "sample_count": 15, "duration_seconds": 2.046}, "timestamp": "2026-01-16T16:33:22.664844"}
{"image_index": 395, "image_name": "000000044590.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044590.jpg", "image_width": 640, "image_height": 246, "image_resolution": "640x246", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1496.943, "latencies_ms": [1496.943], "images_per_second": 0.668, "prompt_tokens": 23, "response_tokens_est": 11, "n_tiles": 10, "output_text": "motorcycle: 5\npeople: 3", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25590.6, "ram_available_mb": 100181.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 2.2, "ram_used_mb": 25590.3, "ram_available_mb": 100181.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4613.5, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 6654.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.81, "peak": 15.14, "min": 14.22}, "VDD_GPU": {"avg": 38.66, "peak": 41.34, "min": 31.91}, "VIN": {"avg": 80.95, "peak": 123.07, "min": 58.26}, "VDD_CPU_SOC_MSS": {"avg": 15.53, "peak": 16.14, "min": 14.96}}, "power_watts_avg": 38.66, "energy_joules_est": 57.89, "sample_count": 11, "duration_seconds": 1.497}, "timestamp": "2026-01-16T16:33:24.168167"}
{"image_index": 395, "image_name": "000000044590.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044590.jpg", "image_width": 640, "image_height": 246, "image_resolution": "640x246", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3173.899, "latencies_ms": [3173.899], "images_per_second": 0.315, "prompt_tokens": 27, "response_tokens_est": 63, "n_tiles": 10, "output_text": "The main objects in the image are motorcycles and people. The motorcycles are parked on the side of the road, with the closest one to the foreground and the others further back. The people are standing near the motorcycles, with some of them standing close to the motorcycles and others standing further away.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25590.3, "ram_available_mb": 100181.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25590.6, "ram_available_mb": 100181.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4613.5, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 6657.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.64, "peak": 14.94, "min": 14.02}, "VDD_GPU": {"avg": 33.05, "peak": 42.92, "min": 25.21}, "VIN": {"avg": 74.66, "peak": 112.92, "min": 56.06}, "VDD_CPU_SOC_MSS": {"avg": 15.22, "peak": 15.74, "min": 14.56}}, "power_watts_avg": 33.05, "energy_joules_est": 104.91, "sample_count": 24, "duration_seconds": 3.174}, "timestamp": "2026-01-16T16:33:27.348355"}
{"image_index": 395, "image_name": "000000044590.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044590.jpg", "image_width": 640, "image_height": 246, "image_resolution": "640x246", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3265.032, "latencies_ms": [3265.032], "images_per_second": 0.306, "prompt_tokens": 21, "response_tokens_est": 66, "n_tiles": 10, "output_text": "The image depicts a group of people gathered around a row of motorcycles parked on the side of a road. The setting appears to be outdoors during the daytime, with a cloudy sky overhead. The group seems to be engaged in a conversation or discussion, possibly about the motorcycles or the activities they are participating in.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25590.5, "ram_available_mb": 100181.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 25590.3, "ram_available_mb": 100181.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4613.5, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 6652.6, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.64, "peak": 14.94, "min": 13.92}, "VDD_GPU": {"avg": 31.86, "peak": 41.34, "min": 25.21}, "VIN": {"avg": 72.21, "peak": 128.2, "min": 58.38}, "VDD_CPU_SOC_MSS": {"avg": 15.26, "peak": 15.74, "min": 14.56}}, "power_watts_avg": 31.86, "energy_joules_est": 104.04, "sample_count": 25, "duration_seconds": 3.265}, "timestamp": "2026-01-16T16:33:30.621068"}
{"image_index": 395, "image_name": "000000044590.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044590.jpg", "image_width": 640, "image_height": 246, "image_resolution": "640x246", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2940.825, "latencies_ms": [2940.825], "images_per_second": 0.34, "prompt_tokens": 19, "response_tokens_est": 56, "n_tiles": 10, "output_text": "The image depicts a group of people standing next to a row of motorcycles on a roadside. The sky is overcast, casting a soft, diffused light over the scene. The motorcycles are predominantly black and blue, with some featuring red and white accents.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25590.3, "ram_available_mb": 100181.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 25590.3, "ram_available_mb": 100181.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4613.5, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 6651.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.62, "peak": 14.94, "min": 13.92}, "VDD_GPU": {"avg": 32.43, "peak": 41.34, "min": 25.21}, "VIN": {"avg": 71.87, "peak": 105.43, "min": 58.08}, "VDD_CPU_SOC_MSS": {"avg": 15.3, "peak": 15.74, "min": 14.56}}, "power_watts_avg": 32.43, "energy_joules_est": 95.38, "sample_count": 23, "duration_seconds": 2.941}, "timestamp": "2026-01-16T16:33:33.568246"}
{"image_index": 396, "image_name": "000000044652.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044652.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1631.337, "latencies_ms": [1631.337], "images_per_second": 0.613, "prompt_tokens": 9, "response_tokens_est": 39, "n_tiles": 6, "output_text": "A black and white photograph captures a single-engine airplane in mid-flight, with its propellers spinning and the aircraft's tail and wings clearly visible against a backdrop of a cloudy sky.", "error": null, "sys_before": {"cpu_percent": 46.4, "ram_used_mb": 25590.3, "ram_available_mb": 100181.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 25590.3, "ram_available_mb": 100181.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.48, "peak": 15.11, "min": 13.82}, "VDD_GPU": {"avg": 30.86, "peak": 37.8, "min": 24.82}, "VIN": {"avg": 69.93, "peak": 106.83, "min": 60.7}, "VDD_CPU_SOC_MSS": {"avg": 15.16, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 30.86, "energy_joules_est": 50.35, "sample_count": 12, "duration_seconds": 1.632}, "timestamp": "2026-01-16T16:33:35.261019"}
{"image_index": 396, "image_name": "000000044652.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044652.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1693.409, "latencies_ms": [1693.409], "images_per_second": 0.591, "prompt_tokens": 23, "response_tokens_est": 40, "n_tiles": 6, "output_text": "object: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25590.3, "ram_available_mb": 100181.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25590.5, "ram_available_mb": 100181.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.84, "peak": 15.22, "min": 14.22}, "VDD_GPU": {"avg": 31.27, "peak": 38.21, "min": 24.82}, "VIN": {"avg": 74.73, "peak": 116.12, "min": 53.31}, "VDD_CPU_SOC_MSS": {"avg": 15.47, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 31.27, "energy_joules_est": 52.96, "sample_count": 13, "duration_seconds": 1.694}, "timestamp": "2026-01-16T16:33:36.960616"}
{"image_index": 396, "image_name": "000000044652.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044652.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2306.008, "latencies_ms": [2306.008], "images_per_second": 0.434, "prompt_tokens": 27, "response_tokens_est": 63, "n_tiles": 6, "output_text": "The main objects in the image are a small airplane and a cloud. The airplane is positioned in the foreground, slightly to the left, with its nose pointing towards the right side of the image. The cloud is located in the background, towards the top right corner, and is relatively large compared to the airplane.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25590.5, "ram_available_mb": 100181.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 25590.3, "ram_available_mb": 100181.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.0, "peak": 15.32, "min": 14.32}, "VDD_GPU": {"avg": 29.59, "peak": 38.19, "min": 24.03}, "VIN": {"avg": 69.82, "peak": 117.84, "min": 57.12}, "VDD_CPU_SOC_MSS": {"avg": 15.63, "peak": 16.14, "min": 14.96}}, "power_watts_avg": 29.59, "energy_joules_est": 68.25, "sample_count": 17, "duration_seconds": 2.306}, "timestamp": "2026-01-16T16:33:39.276940"}
{"image_index": 396, "image_name": "000000044652.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044652.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1826.994, "latencies_ms": [1826.994], "images_per_second": 0.547, "prompt_tokens": 21, "response_tokens_est": 45, "n_tiles": 6, "output_text": "The image depicts a black and white photograph of a small airplane in flight, with its propellers spinning. The airplane is captured against a backdrop of a cloudy sky, creating a dramatic and somewhat moody atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25590.3, "ram_available_mb": 100181.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 25590.5, "ram_available_mb": 100181.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.86, "peak": 15.22, "min": 14.22}, "VDD_GPU": {"avg": 30.64, "peak": 37.8, "min": 24.42}, "VIN": {"avg": 67.86, "peak": 99.02, "min": 58.3}, "VDD_CPU_SOC_MSS": {"avg": 15.47, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 30.64, "energy_joules_est": 55.99, "sample_count": 14, "duration_seconds": 1.827}, "timestamp": "2026-01-16T16:33:41.111283"}
{"image_index": 396, "image_name": "000000044652.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044652.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2208.06, "latencies_ms": [2208.06], "images_per_second": 0.453, "prompt_tokens": 19, "response_tokens_est": 57, "n_tiles": 6, "output_text": "The image features a black and white photograph of a small airplane in flight against a cloudy sky. The airplane has a sleek design with a white body and black accents, and the lighting is dramatic, highlighting the contours of the aircraft and the contrast between the dark and light elements.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25590.5, "ram_available_mb": 100181.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 25590.3, "ram_available_mb": 100181.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.79, "peak": 15.11, "min": 14.12}, "VDD_GPU": {"avg": 29.47, "peak": 38.59, "min": 23.24}, "VIN": {"avg": 73.81, "peak": 113.01, "min": 62.85}, "VDD_CPU_SOC_MSS": {"avg": 15.35, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 29.47, "energy_joules_est": 65.08, "sample_count": 17, "duration_seconds": 2.208}, "timestamp": "2026-01-16T16:33:43.325617"}
{"image_index": 397, "image_name": "000000044699.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044699.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1395.33, "latencies_ms": [1395.33], "images_per_second": 0.717, "prompt_tokens": 9, "response_tokens_est": 30, "n_tiles": 6, "output_text": "A group of sheep stands on a rocky hillside with a serene lake in the background, surrounded by green hills and a clear blue sky.", "error": null, "sys_before": {"cpu_percent": 36.8, "ram_used_mb": 25590.3, "ram_available_mb": 100181.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 25590.3, "ram_available_mb": 100181.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.38, "peak": 15.01, "min": 13.82}, "VDD_GPU": {"avg": 31.51, "peak": 37.03, "min": 26.0}, "VIN": {"avg": 76.58, "peak": 113.95, "min": 51.24}, "VDD_CPU_SOC_MSS": {"avg": 15.04, "peak": 15.74, "min": 14.56}}, "power_watts_avg": 31.51, "energy_joules_est": 43.98, "sample_count": 10, "duration_seconds": 1.396}, "timestamp": "2026-01-16T16:33:44.783441"}
{"image_index": 397, "image_name": "000000044699.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044699.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1696.318, "latencies_ms": [1696.318], "images_per_second": 0.59, "prompt_tokens": 23, "response_tokens_est": 40, "n_tiles": 6, "output_text": "object: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25590.3, "ram_available_mb": 100181.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 25590.5, "ram_available_mb": 100181.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.82, "peak": 15.22, "min": 14.22}, "VDD_GPU": {"avg": 31.42, "peak": 38.21, "min": 24.82}, "VIN": {"avg": 75.79, "peak": 117.65, "min": 55.96}, "VDD_CPU_SOC_MSS": {"avg": 15.38, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 31.42, "energy_joules_est": 53.31, "sample_count": 13, "duration_seconds": 1.697}, "timestamp": "2026-01-16T16:33:46.485983"}
{"image_index": 397, "image_name": "000000044699.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044699.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2422.402, "latencies_ms": [2422.402], "images_per_second": 0.413, "prompt_tokens": 27, "response_tokens_est": 67, "n_tiles": 6, "output_text": "In the image, the main objects are a group of sheep located in the foreground and a body of water in the background. The sheep are positioned on a rocky terrain, with the largest sheep standing in the center and the others scattered around. The body of water is situated to the right, providing a natural boundary to the scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25590.5, "ram_available_mb": 100181.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 25590.5, "ram_available_mb": 100181.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.98, "peak": 15.32, "min": 14.32}, "VDD_GPU": {"avg": 29.37, "peak": 38.19, "min": 24.03}, "VIN": {"avg": 70.82, "peak": 106.71, "min": 55.92}, "VDD_CPU_SOC_MSS": {"avg": 15.57, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 29.37, "energy_joules_est": 71.15, "sample_count": 18, "duration_seconds": 2.423}, "timestamp": "2026-01-16T16:33:48.914401"}
{"image_index": 397, "image_name": "000000044699.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044699.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1890.655, "latencies_ms": [1890.655], "images_per_second": 0.529, "prompt_tokens": 21, "response_tokens_est": 47, "n_tiles": 6, "output_text": "The image depicts a serene landscape featuring a group of sheep grazing on a grassy hillside. The sheep are surrounded by a picturesque view of a calm, turquoise lake nestled between rolling hills and mountains.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25590.5, "ram_available_mb": 100181.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 25590.3, "ram_available_mb": 100181.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.88, "peak": 15.22, "min": 14.32}, "VDD_GPU": {"avg": 30.58, "peak": 37.82, "min": 24.43}, "VIN": {"avg": 72.84, "peak": 105.79, "min": 62.9}, "VDD_CPU_SOC_MSS": {"avg": 15.52, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 30.58, "energy_joules_est": 57.84, "sample_count": 14, "duration_seconds": 1.891}, "timestamp": "2026-01-16T16:33:50.811607"}
{"image_index": 397, "image_name": "000000044699.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044699.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1962.449, "latencies_ms": [1962.449], "images_per_second": 0.51, "prompt_tokens": 19, "response_tokens_est": 50, "n_tiles": 6, "output_text": "The image depicts a serene landscape with a group of sheep standing on a rocky hillside. The sheep are predominantly white, with some having darker patches. The scene is bathed in natural sunlight, casting soft shadows on the rocky terrain.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25590.3, "ram_available_mb": 100181.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 25590.8, "ram_available_mb": 100181.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.88, "peak": 15.22, "min": 14.22}, "VDD_GPU": {"avg": 30.28, "peak": 38.21, "min": 24.04}, "VIN": {"avg": 72.22, "peak": 114.92, "min": 61.1}, "VDD_CPU_SOC_MSS": {"avg": 15.56, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 30.28, "energy_joules_est": 59.44, "sample_count": 15, "duration_seconds": 1.963}, "timestamp": "2026-01-16T16:33:52.780287"}
{"image_index": 398, "image_name": "000000044877.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044877.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1175.175, "latencies_ms": [1175.175], "images_per_second": 0.851, "prompt_tokens": 9, "response_tokens_est": 22, "n_tiles": 6, "output_text": "The image shows a woman in a wheelchair holding a tennis racket, with another person in the background.", "error": null, "sys_before": {"cpu_percent": 45.9, "ram_used_mb": 25590.5, "ram_available_mb": 100181.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 25590.5, "ram_available_mb": 100181.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.39, "peak": 14.81, "min": 13.92}, "VDD_GPU": {"avg": 32.65, "peak": 37.03, "min": 27.18}, "VIN": {"avg": 70.33, "peak": 120.23, "min": 49.41}, "VDD_CPU_SOC_MSS": {"avg": 15.18, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 32.65, "energy_joules_est": 38.38, "sample_count": 9, "duration_seconds": 1.176}, "timestamp": "2026-01-16T16:33:54.008329"}
{"image_index": 398, "image_name": "000000044877.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044877.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1904.932, "latencies_ms": [1904.932], "images_per_second": 0.525, "prompt_tokens": 23, "response_tokens_est": 46, "n_tiles": 6, "output_text": "1. Woman in wheelchair\n2. Tennis racket\n3. Wheelchair\n4. Tennis ball\n5. Tennis court\n6. Tennis player\n7. Tennis player's shirt\n8. Tennis player's shorts", "error": null, "sys_before": {"cpu_percent": 50.0, "ram_used_mb": 25590.5, "ram_available_mb": 100181.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 25590.5, "ram_available_mb": 100181.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.65, "peak": 15.01, "min": 14.02}, "VDD_GPU": {"avg": 31.06, "peak": 39.39, "min": 24.03}, "VIN": {"avg": 71.57, "peak": 107.62, "min": 57.38}, "VDD_CPU_SOC_MSS": {"avg": 15.21, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 31.06, "energy_joules_est": 59.18, "sample_count": 14, "duration_seconds": 1.905}, "timestamp": "2026-01-16T16:33:55.919213"}
{"image_index": 398, "image_name": "000000044877.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044877.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2802.102, "latencies_ms": [2802.102], "images_per_second": 0.357, "prompt_tokens": 27, "response_tokens_est": 78, "n_tiles": 6, "output_text": "The main objects in the image are a woman in a wheelchair and a tennis racket. The woman is positioned in the foreground, with the racket held in her right hand. The background features another person, slightly out of focus, who is also holding a tennis racket. The wheelchair and racket are positioned near the woman, indicating that she is actively engaged in playing tennis.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25590.5, "ram_available_mb": 100181.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 25590.7, "ram_available_mb": 100181.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.8, "peak": 15.11, "min": 14.12}, "VDD_GPU": {"avg": 27.92, "peak": 38.21, "min": 23.24}, "VIN": {"avg": 68.87, "peak": 107.68, "min": 56.98}, "VDD_CPU_SOC_MSS": {"avg": 15.3, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 27.92, "energy_joules_est": 78.26, "sample_count": 22, "duration_seconds": 2.803}, "timestamp": "2026-01-16T16:33:58.728168"}
{"image_index": 398, "image_name": "000000044877.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044877.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2413.065, "latencies_ms": [2413.065], "images_per_second": 0.414, "prompt_tokens": 21, "response_tokens_est": 64, "n_tiles": 6, "output_text": "The scene is set in an indoor sports facility, likely a tennis court, where a woman in a wheelchair is actively playing tennis. She is holding a tennis racket and appears to be focused on the game. The setting is simple and functional, with a plain background that does not distract from the main subject.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25590.7, "ram_available_mb": 100181.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 25591.2, "ram_available_mb": 100181.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.74, "peak": 15.01, "min": 14.02}, "VDD_GPU": {"avg": 28.78, "peak": 37.82, "min": 23.24}, "VIN": {"avg": 70.18, "peak": 116.98, "min": 57.84}, "VDD_CPU_SOC_MSS": {"avg": 15.25, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 28.78, "energy_joules_est": 69.46, "sample_count": 18, "duration_seconds": 2.414}, "timestamp": "2026-01-16T16:34:01.148510"}
{"image_index": 398, "image_name": "000000044877.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000044877.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1573.918, "latencies_ms": [1573.918], "images_per_second": 0.635, "prompt_tokens": 19, "response_tokens_est": 34, "n_tiles": 6, "output_text": "The woman in the wheelchair is wearing a gray t-shirt with a white logo and a purple design. The lighting in the image is bright, suggesting it is daytime.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25591.2, "ram_available_mb": 100181.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 25591.2, "ram_available_mb": 100181.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.62, "peak": 14.91, "min": 14.02}, "VDD_GPU": {"avg": 31.28, "peak": 38.19, "min": 24.82}, "VIN": {"avg": 68.12, "peak": 113.06, "min": 55.14}, "VDD_CPU_SOC_MSS": {"avg": 15.19, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 31.28, "energy_joules_est": 49.25, "sample_count": 12, "duration_seconds": 1.574}, "timestamp": "2026-01-16T16:34:02.728484"}
{"image_index": 399, "image_name": "000000045070.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000045070.jpg", "image_width": 640, "image_height": 596, "image_resolution": "640x596", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 925.765, "latencies_ms": [925.765], "images_per_second": 1.08, "prompt_tokens": 9, "response_tokens_est": 39, "n_tiles": 1, "output_text": "A young girl is seated on a purple horse, wearing a black helmet, a pink and white plaid shirt, blue jeans, and brown boots, with a brown saddle on her back.", "error": null, "sys_before": {"cpu_percent": 24.1, "ram_used_mb": 25591.2, "ram_available_mb": 100181.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25591.2, "ram_available_mb": 100181.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 4845.7, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.14, "peak": 15.62, "min": 14.4}, "VDD_GPU": {"avg": 23.24, "peak": 26.79, "min": 20.89}, "VIN": {"avg": 64.31, "peak": 71.71, "min": 60.28}, "VDD_CPU_SOC_MSS": {"avg": 15.92, "peak": 16.54, "min": 14.96}}, "power_watts_avg": 23.24, "energy_joules_est": 21.53, "sample_count": 7, "duration_seconds": 0.926}, "timestamp": "2026-01-16T16:34:03.679486"}
{"image_index": 399, "image_name": "000000045070.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000045070.jpg", "image_width": 640, "image_height": 596, "image_resolution": "640x596", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1720.492, "latencies_ms": [1720.492], "images_per_second": 0.581, "prompt_tokens": 23, "response_tokens_est": 76, "n_tiles": 1, "output_text": "- Child: 1\n- Helmet: 1\n- Shirt: 1\n- Jeans: 1\n- Saddle: 1\n- Stirrup: 1\n- Equestrian footwear: 1\n- Saddle horn: 1\n- Saddle pad: 1\n- Saddle horn strap: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25591.2, "ram_available_mb": 100181.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25591.2, "ram_available_mb": 100181.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 4856.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.52, "peak": 15.82, "min": 15.01}, "VDD_GPU": {"avg": 21.4, "peak": 25.22, "min": 20.1}, "VIN": {"avg": 64.14, "peak": 68.24, "min": 62.28}, "VDD_CPU_SOC_MSS": {"avg": 16.48, "peak": 16.54, "min": 16.15}}, "power_watts_avg": 21.4, "energy_joules_est": 36.82, "sample_count": 13, "duration_seconds": 1.721}, "timestamp": "2026-01-16T16:34:05.405624"}
{"image_index": 399, "image_name": "000000045070.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000045070.jpg", "image_width": 640, "image_height": 596, "image_resolution": "640x596", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1067.683, "latencies_ms": [1067.683], "images_per_second": 0.937, "prompt_tokens": 27, "response_tokens_est": 46, "n_tiles": 1, "output_text": "The main object in the foreground is a young child sitting on a purple horse saddle. The child is wearing a black helmet and a pink and white plaid shirt. The background features a blurred natural setting with green trees.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25591.2, "ram_available_mb": 100181.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25591.7, "ram_available_mb": 100180.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 4859.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.58, "peak": 15.82, "min": 15.32}, "VDD_GPU": {"avg": 21.71, "peak": 24.03, "min": 20.48}, "VIN": {"avg": 64.24, "peak": 65.59, "min": 61.97}, "VDD_CPU_SOC_MSS": {"avg": 16.44, "peak": 16.54, "min": 16.14}}, "power_watts_avg": 21.71, "energy_joules_est": 23.19, "sample_count": 8, "duration_seconds": 1.068}, "timestamp": "2026-01-16T16:34:06.479712"}
{"image_index": 399, "image_name": "000000045070.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000045070.jpg", "image_width": 640, "image_height": 596, "image_resolution": "640x596", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1352.191, "latencies_ms": [1352.191], "images_per_second": 0.74, "prompt_tokens": 21, "response_tokens_est": 59, "n_tiles": 1, "output_text": "In the image, a young child is seated on a purple horse, wearing a black helmet and a pink and white plaid shirt. The child is looking down, possibly at something on the ground. The background features lush green trees, indicating an outdoor setting, likely a farm or ranch.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25591.7, "ram_available_mb": 100180.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25591.7, "ram_available_mb": 100180.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 4854.7, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.63, "peak": 15.72, "min": 15.32}, "VDD_GPU": {"avg": 21.48, "peak": 24.42, "min": 20.1}, "VIN": {"avg": 63.3, "peak": 67.35, "min": 60.95}, "VDD_CPU_SOC_MSS": {"avg": 16.42, "peak": 16.54, "min": 16.14}}, "power_watts_avg": 21.48, "energy_joules_est": 29.05, "sample_count": 10, "duration_seconds": 1.353}, "timestamp": "2026-01-16T16:34:07.837800"}
{"image_index": 399, "image_name": "000000045070.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000045070.jpg", "image_width": 640, "image_height": 596, "image_resolution": "640x596", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1196.673, "latencies_ms": [1196.673], "images_per_second": 0.836, "prompt_tokens": 19, "response_tokens_est": 52, "n_tiles": 1, "output_text": "The notable visual attributes of the image include a young child wearing a black helmet, a pink and white plaid shirt, blue jeans, and brown boots. The child is seated on a purple horse saddle, with a green tree background and overcast lighting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25591.7, "ram_available_mb": 100180.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25591.9, "ram_available_mb": 100180.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 4853.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.63, "peak": 15.82, "min": 15.22}, "VDD_GPU": {"avg": 21.54, "peak": 24.04, "min": 20.1}, "VIN": {"avg": 65.34, "peak": 73.06, "min": 60.17}, "VDD_CPU_SOC_MSS": {"avg": 16.45, "peak": 16.54, "min": 16.14}}, "power_watts_avg": 21.54, "energy_joules_est": 25.78, "sample_count": 9, "duration_seconds": 1.197}, "timestamp": "2026-01-16T16:34:09.040223"}
{"image_index": 400, "image_name": "000000045090.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000045090.jpg", "image_width": 500, "image_height": 259, "image_resolution": "500x259", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1057.276, "latencies_ms": [1057.276], "images_per_second": 0.946, "prompt_tokens": 9, "response_tokens_est": 40, "n_tiles": 2, "output_text": "The image captures a dynamic scene of surfers riding waves in the ocean, with one surfer in the foreground skillfully maneuvering a surfboard amidst the frothy, blue-green waters.", "error": null, "sys_before": {"cpu_percent": 31.6, "ram_used_mb": 25591.9, "ram_available_mb": 100180.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25592.4, "ram_available_mb": 100179.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5055.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.15, "peak": 15.52, "min": 14.61}, "VDD_GPU": {"avg": 25.21, "peak": 31.12, "min": 22.06}, "VIN": {"avg": 65.9, "peak": 75.53, "min": 63.56}, "VDD_CPU_SOC_MSS": {"avg": 15.95, "peak": 16.14, "min": 15.74}}, "power_watts_avg": 25.21, "energy_joules_est": 26.66, "sample_count": 8, "duration_seconds": 1.058}, "timestamp": "2026-01-16T16:34:10.117604"}
{"image_index": 400, "image_name": "000000045090.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000045090.jpg", "image_width": 500, "image_height": 259, "image_resolution": "500x259", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1079.329, "latencies_ms": [1079.329], "images_per_second": 0.927, "prompt_tokens": 23, "response_tokens_est": 40, "n_tiles": 2, "output_text": "object: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25592.4, "ram_available_mb": 100179.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25592.2, "ram_available_mb": 100180.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5064.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.99, "peak": 15.22, "min": 14.71}, "VDD_GPU": {"avg": 25.66, "peak": 31.51, "min": 22.07}, "VIN": {"avg": 65.76, "peak": 89.63, "min": 58.72}, "VDD_CPU_SOC_MSS": {"avg": 15.85, "peak": 16.15, "min": 15.36}}, "power_watts_avg": 25.66, "energy_joules_est": 27.7, "sample_count": 8, "duration_seconds": 1.08}, "timestamp": "2026-01-16T16:34:11.202490"}
{"image_index": 400, "image_name": "000000045090.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000045090.jpg", "image_width": 500, "image_height": 259, "image_resolution": "500x259", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1260.825, "latencies_ms": [1260.825], "images_per_second": 0.793, "prompt_tokens": 27, "response_tokens_est": 48, "n_tiles": 2, "output_text": "The main objects in the image are the ocean waves and the surfers. The waves are in the background, while the surfers are in the foreground. The surfers are closer to the viewer, and the waves are further away.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25592.2, "ram_available_mb": 100180.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25592.2, "ram_available_mb": 100180.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5067.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.0, "peak": 15.42, "min": 14.51}, "VDD_GPU": {"avg": 25.57, "peak": 31.51, "min": 22.07}, "VIN": {"avg": 67.29, "peak": 98.7, "min": 61.79}, "VDD_CPU_SOC_MSS": {"avg": 15.84, "peak": 16.14, "min": 15.36}}, "power_watts_avg": 25.57, "energy_joules_est": 32.26, "sample_count": 9, "duration_seconds": 1.262}, "timestamp": "2026-01-16T16:34:12.470450"}
{"image_index": 400, "image_name": "000000045090.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000045090.jpg", "image_width": 500, "image_height": 259, "image_resolution": "500x259", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1615.335, "latencies_ms": [1615.335], "images_per_second": 0.619, "prompt_tokens": 21, "response_tokens_est": 64, "n_tiles": 2, "output_text": "The image captures a dynamic scene of surfers riding waves in a blue ocean. The surfers are seen in action, with one in the foreground riding a wave and another in the background also riding a wave. The setting is a coastal area, likely in New Zealand, as indicated by the text in the image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25592.2, "ram_available_mb": 100180.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25592.2, "ram_available_mb": 100180.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5063.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.25, "peak": 15.62, "min": 14.61}, "VDD_GPU": {"avg": 24.19, "peak": 31.12, "min": 21.27}, "VIN": {"avg": 68.06, "peak": 103.08, "min": 60.46}, "VDD_CPU_SOC_MSS": {"avg": 15.88, "peak": 16.14, "min": 15.36}}, "power_watts_avg": 24.19, "energy_joules_est": 39.09, "sample_count": 12, "duration_seconds": 1.616}, "timestamp": "2026-01-16T16:34:14.091808"}
{"image_index": 400, "image_name": "000000045090.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000045090.jpg", "image_width": 500, "image_height": 259, "image_resolution": "500x259", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1191.685, "latencies_ms": [1191.685], "images_per_second": 0.839, "prompt_tokens": 19, "response_tokens_est": 45, "n_tiles": 2, "output_text": "The image captures a dynamic scene of surfers riding waves in a blue ocean. The surfers are dressed in black wetsuits, and the waves are breaking with white foam, indicating a sunny day with clear skies.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25592.2, "ram_available_mb": 100180.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25591.9, "ram_available_mb": 100180.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5062.6, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.16, "peak": 15.52, "min": 14.71}, "VDD_GPU": {"avg": 24.91, "peak": 30.33, "min": 21.66}, "VIN": {"avg": 66.86, "peak": 103.24, "min": 60.86}, "VDD_CPU_SOC_MSS": {"avg": 15.75, "peak": 16.14, "min": 15.36}}, "power_watts_avg": 24.91, "energy_joules_est": 29.69, "sample_count": 9, "duration_seconds": 1.192}, "timestamp": "2026-01-16T16:34:15.289266"}
{"image_index": 401, "image_name": "000000045229.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000045229.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2251.408, "latencies_ms": [2251.408], "images_per_second": 0.444, "prompt_tokens": 9, "response_tokens_est": 25, "n_tiles": 12, "output_text": "The image shows a cozy kitchen with a window, a refrigerator, and various kitchen items on the countertops and shelves.", "error": null, "sys_before": {"cpu_percent": 45.3, "ram_used_mb": 25591.9, "ram_available_mb": 100180.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 2.6, "ram_used_mb": 25591.9, "ram_available_mb": 100180.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.2, "peak": 16.15, "min": 14.2}, "VDD_GPU": {"avg": 35.26, "peak": 40.57, "min": 28.37}, "VIN": {"avg": 75.57, "peak": 108.3, "min": 58.53}, "VDD_CPU_SOC_MSS": {"avg": 16.0, "peak": 17.32, "min": 15.35}}, "power_watts_avg": 35.26, "energy_joules_est": 79.39, "sample_count": 17, "duration_seconds": 2.252}, "timestamp": "2026-01-16T16:34:17.607909"}
{"image_index": 401, "image_name": "000000045229.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000045229.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 5848.191, "latencies_ms": [5848.191], "images_per_second": 0.171, "prompt_tokens": 23, "response_tokens_est": 129, "n_tiles": 12, "output_text": "- window: 2\n- cabinet: 1\n- plant: 1\n- shelf: 1\n- drawer: 1\n- stove: 1\n- cup: 1\n- drawer: 1\n- drawer: 1\n- drawer: 1\n- drawer: 1\n- drawer: 1\n- drawer: 1\n- drawer: 1\n- drawer: 1\n- drawer: 1\n- drawer: 1\n- drawer: 1\n- drawer: 1\n- drawer: 1\n- drawer: 1\n- drawer", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25591.9, "ram_available_mb": 100180.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25591.9, "ram_available_mb": 100180.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.12, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 30.37, "peak": 42.94, "min": 26.0}, "VIN": {"avg": 71.01, "peak": 111.09, "min": 57.99}, "VDD_CPU_SOC_MSS": {"avg": 15.7, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 30.37, "energy_joules_est": 177.62, "sample_count": 46, "duration_seconds": 5.848}, "timestamp": "2026-01-16T16:34:23.462557"}
{"image_index": 401, "image_name": "000000045229.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000045229.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 4297.128, "latencies_ms": [4297.128], "images_per_second": 0.233, "prompt_tokens": 27, "response_tokens_est": 82, "n_tiles": 12, "output_text": "The main objects in the image are located in the kitchen, with the refrigerator and the window being the most prominent features. The refrigerator is situated on the left side of the image, while the window is on the right side. The kitchen counter is in the foreground, with various kitchen items and utensils placed on it. The shelves and cabinets are in the background, providing storage space for kitchen essentials.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25591.9, "ram_available_mb": 100180.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 25591.4, "ram_available_mb": 100180.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.09, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 32.03, "peak": 42.53, "min": 26.0}, "VIN": {"avg": 70.37, "peak": 95.93, "min": 58.37}, "VDD_CPU_SOC_MSS": {"avg": 15.71, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 32.03, "energy_joules_est": 137.65, "sample_count": 33, "duration_seconds": 4.297}, "timestamp": "2026-01-16T16:34:27.766120"}
{"image_index": 401, "image_name": "000000045229.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000045229.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3408.378, "latencies_ms": [3408.378], "images_per_second": 0.293, "prompt_tokens": 21, "response_tokens_est": 56, "n_tiles": 12, "output_text": "The image depicts a cozy kitchen scene with warm lighting. The kitchen features a wooden cabinet with a window, a refrigerator, and various kitchen items on the countertops. The overall atmosphere is homely and inviting, with a focus on the natural light streaming through the window.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25591.4, "ram_available_mb": 100180.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 25591.4, "ram_available_mb": 100180.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.1, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 33.56, "peak": 42.15, "min": 26.4}, "VIN": {"avg": 74.17, "peak": 115.8, "min": 58.31}, "VDD_CPU_SOC_MSS": {"avg": 15.7, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 33.56, "energy_joules_est": 114.4, "sample_count": 26, "duration_seconds": 3.409}, "timestamp": "2026-01-16T16:34:31.181138"}
{"image_index": 401, "image_name": "000000045229.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000045229.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3208.718, "latencies_ms": [3208.718], "images_per_second": 0.312, "prompt_tokens": 19, "response_tokens_est": 50, "n_tiles": 12, "output_text": "The kitchen is warmly lit with a soft, yellowish glow, creating a cozy atmosphere. The wooden cabinets and countertops are painted in a warm, reddish-brown hue, complementing the natural light streaming in through the window.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25591.4, "ram_available_mb": 100180.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 25591.4, "ram_available_mb": 100180.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.1, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 33.94, "peak": 42.15, "min": 26.39}, "VIN": {"avg": 78.17, "peak": 118.16, "min": 62.0}, "VDD_CPU_SOC_MSS": {"avg": 15.68, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 33.94, "energy_joules_est": 108.91, "sample_count": 25, "duration_seconds": 3.209}, "timestamp": "2026-01-16T16:34:34.396142"}
{"image_index": 402, "image_name": "000000045472.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000045472.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1470.941, "latencies_ms": [1470.941], "images_per_second": 0.68, "prompt_tokens": 9, "response_tokens_est": 33, "n_tiles": 6, "output_text": "The image shows a rustic setting with a red ceramic pot containing numerous pink wooden sticks, a pile of fresh oranges, and a red bowl on the table.", "error": null, "sys_before": {"cpu_percent": 43.8, "ram_used_mb": 25591.4, "ram_available_mb": 100180.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 25591.4, "ram_available_mb": 100180.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.43, "peak": 15.01, "min": 13.82}, "VDD_GPU": {"avg": 31.69, "peak": 37.82, "min": 25.6}, "VIN": {"avg": 67.87, "peak": 89.72, "min": 61.05}, "VDD_CPU_SOC_MSS": {"avg": 15.1, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 31.69, "energy_joules_est": 46.63, "sample_count": 11, "duration_seconds": 1.472}, "timestamp": "2026-01-16T16:34:35.935514"}
{"image_index": 402, "image_name": "000000045472.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000045472.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1676.538, "latencies_ms": [1676.538], "images_per_second": 0.596, "prompt_tokens": 23, "response_tokens_est": 39, "n_tiles": 6, "output_text": "1. Pineapple\n2. Orange\n3. Red cup\n4. Red cup\n5. Red cup\n6. Red cup\n7. Red cup\n8. Red cup", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25591.4, "ram_available_mb": 100180.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25591.6, "ram_available_mb": 100180.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.78, "peak": 15.11, "min": 14.22}, "VDD_GPU": {"avg": 31.94, "peak": 38.6, "min": 25.21}, "VIN": {"avg": 71.58, "peak": 111.73, "min": 52.82}, "VDD_CPU_SOC_MSS": {"avg": 15.39, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 31.94, "energy_joules_est": 53.56, "sample_count": 12, "duration_seconds": 1.677}, "timestamp": "2026-01-16T16:34:37.617982"}
{"image_index": 402, "image_name": "000000045472.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000045472.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 4055.452, "latencies_ms": [4055.452], "images_per_second": 0.247, "prompt_tokens": 27, "response_tokens_est": 129, "n_tiles": 6, "output_text": "The main objects in the image are a pineapple, a red bowl, a red cup, and a plate of oranges. The pineapple is positioned to the left, with its stem extending towards the top left corner of the image. The red bowl is placed in the center foreground, with its spiky top facing the viewer. The red cup is positioned to the left of the bowl, and the plate of oranges is placed to the right of the bowl. The oranges are stacked on top of the plate, with the largest one in the center and the others surrounding it. The background is dark, providing a contrast that", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25591.6, "ram_available_mb": 100180.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 25592.1, "ram_available_mb": 100180.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.11, "peak": 15.32, "min": 14.22}, "VDD_GPU": {"avg": 26.9, "peak": 38.21, "min": 23.64}, "VIN": {"avg": 66.29, "peak": 84.82, "min": 55.62}, "VDD_CPU_SOC_MSS": {"avg": 15.66, "peak": 16.14, "min": 14.96}}, "power_watts_avg": 26.9, "energy_joules_est": 109.1, "sample_count": 32, "duration_seconds": 4.056}, "timestamp": "2026-01-16T16:34:41.679430"}
{"image_index": 402, "image_name": "000000045472.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000045472.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2518.952, "latencies_ms": [2518.952], "images_per_second": 0.397, "prompt_tokens": 21, "response_tokens_est": 71, "n_tiles": 6, "output_text": "The image depicts a rustic and somewhat cluttered setting with a red table or countertop. On the table, there are several red cups, a pile of oranges, a red pot with pink sticks, and a large pineapple. The overall scene suggests a casual, possibly homey environment with a focus on the oranges and the pot.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25592.1, "ram_available_mb": 100180.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 25592.1, "ram_available_mb": 100180.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.99, "peak": 15.32, "min": 14.32}, "VDD_GPU": {"avg": 28.96, "peak": 37.82, "min": 23.64}, "VIN": {"avg": 66.65, "peak": 98.7, "min": 54.93}, "VDD_CPU_SOC_MSS": {"avg": 15.52, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 28.96, "energy_joules_est": 72.96, "sample_count": 19, "duration_seconds": 2.519}, "timestamp": "2026-01-16T16:34:44.208439"}
{"image_index": 402, "image_name": "000000045472.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000045472.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2842.988, "latencies_ms": [2842.988], "images_per_second": 0.352, "prompt_tokens": 19, "response_tokens_est": 83, "n_tiles": 6, "output_text": "The image features a vibrant red background with a large, dried pineapple placed on the left side. The pineapple has green and brown hues, and its stem is visible. On the right side, there is a pile of fresh, ripe oranges with a glossy surface, and a red ceramic bowl containing a red liquid. The lighting is dim, casting shadows and highlighting the textures of the objects.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25592.1, "ram_available_mb": 100180.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 25592.1, "ram_available_mb": 100180.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.01, "peak": 15.32, "min": 14.22}, "VDD_GPU": {"avg": 28.26, "peak": 38.21, "min": 23.64}, "VIN": {"avg": 70.58, "peak": 108.75, "min": 58.47}, "VDD_CPU_SOC_MSS": {"avg": 15.57, "peak": 16.14, "min": 14.96}}, "power_watts_avg": 28.26, "energy_joules_est": 80.35, "sample_count": 22, "duration_seconds": 2.843}, "timestamp": "2026-01-16T16:34:47.057246"}
{"image_index": 403, "image_name": "000000045550.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000045550.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2348.173, "latencies_ms": [2348.173], "images_per_second": 0.426, "prompt_tokens": 9, "response_tokens_est": 28, "n_tiles": 12, "output_text": "A man is holding a plate of fried chicken and a side of fries, with a small container of ketchup on the side.", "error": null, "sys_before": {"cpu_percent": 40.7, "ram_used_mb": 25592.1, "ram_available_mb": 100180.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 25592.1, "ram_available_mb": 100180.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.14, "peak": 16.05, "min": 14.0}, "VDD_GPU": {"avg": 35.1, "peak": 40.95, "min": 27.97}, "VIN": {"avg": 77.47, "peak": 118.52, "min": 58.94}, "VDD_CPU_SOC_MSS": {"avg": 16.1, "peak": 17.32, "min": 15.35}}, "power_watts_avg": 35.1, "energy_joules_est": 82.43, "sample_count": 18, "duration_seconds": 2.348}, "timestamp": "2026-01-16T16:34:49.495335"}
{"image_index": 403, "image_name": "000000045550.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000045550.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 5852.12, "latencies_ms": [5852.12], "images_per_second": 0.171, "prompt_tokens": 23, "response_tokens_est": 129, "n_tiles": 12, "output_text": "- plate: 1\n- sandwich: 1\n- fork: 0\n- knife: 0\n- napkin: 0\n- cup: 1\n- spoon: 0\n- plate: 1\n- fork: 0\n- knife: 0\n- napkin: 0\n- cup: 1\n- spoon: 0\n- plate: 1\n- fork: 0\n- knife: 0\n- napkin: 0\n- cup: 1\n- spoon: 0\n- plate: 1\n- fork: 0", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25592.1, "ram_available_mb": 100180.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 25592.1, "ram_available_mb": 100180.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.1, "peak": 15.54, "min": 14.43}, "VDD_GPU": {"avg": 30.46, "peak": 42.94, "min": 26.0}, "VIN": {"avg": 71.3, "peak": 121.7, "min": 57.5}, "VDD_CPU_SOC_MSS": {"avg": 15.66, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 30.46, "energy_joules_est": 178.28, "sample_count": 47, "duration_seconds": 5.853}, "timestamp": "2026-01-16T16:34:55.354376"}
{"image_index": 403, "image_name": "000000045550.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000045550.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3421.503, "latencies_ms": [3421.503], "images_per_second": 0.292, "prompt_tokens": 27, "response_tokens_est": 56, "n_tiles": 12, "output_text": "The main object in the foreground is a plate of fried chicken, which is being held by a person. The plate is placed on a table. In the background, there are other people and a clock on the wall. The person holding the plate is standing near the table.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25592.1, "ram_available_mb": 100180.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25592.1, "ram_available_mb": 100180.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.08, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 33.45, "peak": 42.15, "min": 26.39}, "VIN": {"avg": 75.35, "peak": 109.68, "min": 58.36}, "VDD_CPU_SOC_MSS": {"avg": 15.69, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 33.45, "energy_joules_est": 114.47, "sample_count": 26, "duration_seconds": 3.422}, "timestamp": "2026-01-16T16:34:58.782449"}
{"image_index": 403, "image_name": "000000045550.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000045550.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3314.538, "latencies_ms": [3314.538], "images_per_second": 0.302, "prompt_tokens": 21, "response_tokens_est": 52, "n_tiles": 12, "output_text": "The image depicts a man holding a plate of food, which includes a piece of fried chicken and a piece of breaded chicken. The setting appears to be an indoor dining area, possibly a restaurant, with other patrons and tables visible in the background.", "error": null, "sys_before": {"cpu_percent": 40.0, "ram_used_mb": 25592.1, "ram_available_mb": 100180.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 25592.1, "ram_available_mb": 100180.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.87, "peak": 15.65, "min": 14.32}, "VDD_GPU": {"avg": 33.42, "peak": 42.94, "min": 26.0}, "VIN": {"avg": 69.98, "peak": 99.03, "min": 56.9}, "VDD_CPU_SOC_MSS": {"avg": 15.41, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 33.42, "energy_joules_est": 110.79, "sample_count": 26, "duration_seconds": 3.315}, "timestamp": "2026-01-16T16:35:02.103823"}
{"image_index": 403, "image_name": "000000045550.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000045550.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3091.589, "latencies_ms": [3091.589], "images_per_second": 0.323, "prompt_tokens": 19, "response_tokens_est": 46, "n_tiles": 12, "output_text": "The image features a man with a beard and glasses, holding a plate of fried chicken and a side of fries. The lighting is warm, and the background is slightly blurred, with a clock and a menu board visible.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25592.1, "ram_available_mb": 100180.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 25592.1, "ram_available_mb": 100180.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.73, "peak": 15.24, "min": 14.22}, "VDD_GPU": {"avg": 34.14, "peak": 42.54, "min": 26.0}, "VIN": {"avg": 75.42, "peak": 121.94, "min": 58.87}, "VDD_CPU_SOC_MSS": {"avg": 15.27, "peak": 16.14, "min": 14.56}}, "power_watts_avg": 34.14, "energy_joules_est": 105.56, "sample_count": 24, "duration_seconds": 3.092}, "timestamp": "2026-01-16T16:35:05.201857"}
{"image_index": 404, "image_name": "000000045596.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000045596.jpg", "image_width": 408, "image_height": 640, "image_resolution": "408x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1470.484, "latencies_ms": [1470.484], "images_per_second": 0.68, "prompt_tokens": 9, "response_tokens_est": 33, "n_tiles": 6, "output_text": "The image shows a rainy day with a person holding an umbrella, a bicycle parked on the sidewalk, and a building with scaffolding visible in the background.", "error": null, "sys_before": {"cpu_percent": 34.9, "ram_used_mb": 25592.1, "ram_available_mb": 100180.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25592.1, "ram_available_mb": 100180.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.33, "peak": 15.01, "min": 13.72}, "VDD_GPU": {"avg": 31.87, "peak": 37.82, "min": 25.6}, "VIN": {"avg": 70.64, "peak": 111.37, "min": 50.15}, "VDD_CPU_SOC_MSS": {"avg": 15.0, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 31.87, "energy_joules_est": 46.88, "sample_count": 11, "duration_seconds": 1.471}, "timestamp": "2026-01-16T16:35:06.739135"}
{"image_index": 404, "image_name": "000000045596.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000045596.jpg", "image_width": 408, "image_height": 640, "image_resolution": "408x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1671.271, "latencies_ms": [1671.271], "images_per_second": 0.598, "prompt_tokens": 23, "response_tokens_est": 39, "n_tiles": 6, "output_text": "1. Building\n2. Window\n3. Pedestrian\n4. Bicycle\n5. Bicycle rack\n6. Greenery\n7. Street\n8. Street light", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25592.1, "ram_available_mb": 100180.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25592.3, "ram_available_mb": 100179.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.8, "peak": 15.11, "min": 14.22}, "VDD_GPU": {"avg": 31.51, "peak": 38.19, "min": 24.82}, "VIN": {"avg": 76.68, "peak": 114.13, "min": 61.21}, "VDD_CPU_SOC_MSS": {"avg": 15.35, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 31.51, "energy_joules_est": 52.68, "sample_count": 13, "duration_seconds": 1.672}, "timestamp": "2026-01-16T16:35:08.416410"}
{"image_index": 404, "image_name": "000000045596.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000045596.jpg", "image_width": 408, "image_height": 640, "image_resolution": "408x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2077.11, "latencies_ms": [2077.11], "images_per_second": 0.481, "prompt_tokens": 27, "response_tokens_est": 54, "n_tiles": 6, "output_text": "The main objects in the image are a building, a bicycle rack, and a person holding an umbrella. The bicycle rack is located near the building, while the person is near the bicycle rack. The person is holding an umbrella, which is positioned near the building.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25592.3, "ram_available_mb": 100179.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 25592.3, "ram_available_mb": 100179.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.89, "peak": 15.22, "min": 14.22}, "VDD_GPU": {"avg": 29.84, "peak": 38.21, "min": 24.03}, "VIN": {"avg": 65.79, "peak": 79.21, "min": 55.46}, "VDD_CPU_SOC_MSS": {"avg": 15.5, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 29.84, "energy_joules_est": 61.99, "sample_count": 16, "duration_seconds": 2.077}, "timestamp": "2026-01-16T16:35:10.499679"}
{"image_index": 404, "image_name": "000000045596.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000045596.jpg", "image_width": 408, "image_height": 640, "image_resolution": "408x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2981.684, "latencies_ms": [2981.684], "images_per_second": 0.335, "prompt_tokens": 21, "response_tokens_est": 88, "n_tiles": 6, "output_text": "The image depicts an urban scene viewed through a large window. The setting appears to be a modern building with a glass facade, and the weather is rainy, as evidenced by the wet ground and the presence of raindrops. There are several bicycles parked along the sidewalk, and a person is seen walking in the rain. The overall atmosphere is quiet and somewhat gloomy due to the overcast sky and the rain.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25592.3, "ram_available_mb": 100179.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 25592.1, "ram_available_mb": 100180.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.04, "peak": 15.32, "min": 14.32}, "VDD_GPU": {"avg": 28.1, "peak": 38.19, "min": 23.64}, "VIN": {"avg": 71.52, "peak": 115.55, "min": 58.86}, "VDD_CPU_SOC_MSS": {"avg": 15.56, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 28.1, "energy_joules_est": 83.8, "sample_count": 23, "duration_seconds": 2.982}, "timestamp": "2026-01-16T16:35:13.487590"}
{"image_index": 404, "image_name": "000000045596.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000045596.jpg", "image_width": 408, "image_height": 640, "image_resolution": "408x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1646.639, "latencies_ms": [1646.639], "images_per_second": 0.607, "prompt_tokens": 19, "response_tokens_est": 38, "n_tiles": 6, "output_text": "The image shows a rainy day with a grey sky, creating a somber atmosphere. The rain is visible on the ground, and the building's glass windows reflect the grey sky.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25592.1, "ram_available_mb": 100180.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 25592.1, "ram_available_mb": 100180.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.81, "peak": 15.11, "min": 14.32}, "VDD_GPU": {"avg": 31.58, "peak": 37.82, "min": 25.21}, "VIN": {"avg": 69.71, "peak": 127.85, "min": 54.2}, "VDD_CPU_SOC_MSS": {"avg": 15.45, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 31.58, "energy_joules_est": 52.02, "sample_count": 12, "duration_seconds": 1.647}, "timestamp": "2026-01-16T16:35:15.140446"}
{"image_index": 405, "image_name": "000000045728.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000045728.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2311.331, "latencies_ms": [2311.331], "images_per_second": 0.433, "prompt_tokens": 9, "response_tokens_est": 27, "n_tiles": 12, "output_text": "The image shows a close-up of a bowl of macaroni and cheese, which is a creamy, cheesy pasta dish.", "error": null, "sys_before": {"cpu_percent": 41.9, "ram_used_mb": 25592.1, "ram_available_mb": 100180.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 25592.1, "ram_available_mb": 100180.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.9, "peak": 15.85, "min": 13.9}, "VDD_GPU": {"avg": 35.47, "peak": 40.97, "min": 27.57}, "VIN": {"avg": 77.61, "peak": 118.39, "min": 59.36}, "VDD_CPU_SOC_MSS": {"avg": 15.68, "peak": 16.92, "min": 14.96}}, "power_watts_avg": 35.47, "energy_joules_est": 81.99, "sample_count": 17, "duration_seconds": 2.312}, "timestamp": "2026-01-16T16:35:17.547017"}
{"image_index": 405, "image_name": "000000045728.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000045728.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 3151.722, "latencies_ms": [3151.722], "images_per_second": 0.317, "prompt_tokens": 23, "response_tokens_est": 48, "n_tiles": 12, "output_text": "1. Macaroni\n2. Macaroni\n3. Macaroni\n4. Macaroni\n5. Macaroni\n6. Macaroni\n7. Macaroni\n8. Macaroni", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25592.1, "ram_available_mb": 100180.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 25592.3, "ram_available_mb": 100179.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.77, "peak": 15.34, "min": 14.12}, "VDD_GPU": {"avg": 34.28, "peak": 42.94, "min": 26.0}, "VIN": {"avg": 77.23, "peak": 130.18, "min": 58.49}, "VDD_CPU_SOC_MSS": {"avg": 15.32, "peak": 16.14, "min": 14.56}}, "power_watts_avg": 34.28, "energy_joules_est": 108.06, "sample_count": 24, "duration_seconds": 3.152}, "timestamp": "2026-01-16T16:35:20.709479"}
{"image_index": 405, "image_name": "000000045728.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000045728.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3337.389, "latencies_ms": [3337.389], "images_per_second": 0.3, "prompt_tokens": 27, "response_tokens_est": 53, "n_tiles": 12, "output_text": "The main objects in the image are a bowl of macaroni and cheese. The macaroni is in the foreground, while the cheese sauce is in the background. The macaroni is near the cheese sauce, creating a visual connection between the two.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25592.3, "ram_available_mb": 100179.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 25592.3, "ram_available_mb": 100179.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.74, "peak": 15.34, "min": 14.12}, "VDD_GPU": {"avg": 33.59, "peak": 42.15, "min": 26.0}, "VIN": {"avg": 71.74, "peak": 119.0, "min": 57.47}, "VDD_CPU_SOC_MSS": {"avg": 15.24, "peak": 16.14, "min": 14.56}}, "power_watts_avg": 33.59, "energy_joules_est": 112.12, "sample_count": 25, "duration_seconds": 3.338}, "timestamp": "2026-01-16T16:35:24.053257"}
{"image_index": 405, "image_name": "000000045728.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000045728.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3058.326, "latencies_ms": [3058.326], "images_per_second": 0.327, "prompt_tokens": 21, "response_tokens_est": 45, "n_tiles": 12, "output_text": "The image shows a close-up view of a bowl filled with a creamy, white substance, likely a type of pasta. The background is blurred, but it appears to be a kitchen setting with a dark countertop.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25592.3, "ram_available_mb": 100179.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 25592.3, "ram_available_mb": 100179.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.75, "peak": 15.24, "min": 14.12}, "VDD_GPU": {"avg": 34.22, "peak": 42.54, "min": 26.0}, "VIN": {"avg": 76.19, "peak": 111.48, "min": 58.95}, "VDD_CPU_SOC_MSS": {"avg": 15.32, "peak": 16.14, "min": 14.56}}, "power_watts_avg": 34.22, "energy_joules_est": 104.67, "sample_count": 23, "duration_seconds": 3.059}, "timestamp": "2026-01-16T16:35:27.119585"}
{"image_index": 405, "image_name": "000000045728.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000045728.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3327.809, "latencies_ms": [3327.809], "images_per_second": 0.3, "prompt_tokens": 19, "response_tokens_est": 53, "n_tiles": 12, "output_text": "The image features a close-up view of a creamy, white pasta dish with a glossy sheen, indicating it might be coated with a sauce or cream. The lighting is soft and diffused, casting gentle shadows and highlighting the smooth texture of the pasta.", "error": null, "sys_before": {"cpu_percent": 14.3, "ram_used_mb": 25592.3, "ram_available_mb": 100179.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 25592.3, "ram_available_mb": 100179.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.75, "peak": 15.34, "min": 14.12}, "VDD_GPU": {"avg": 33.56, "peak": 42.54, "min": 26.0}, "VIN": {"avg": 71.42, "peak": 108.35, "min": 56.64}, "VDD_CPU_SOC_MSS": {"avg": 15.31, "peak": 16.14, "min": 14.56}}, "power_watts_avg": 33.56, "energy_joules_est": 111.7, "sample_count": 26, "duration_seconds": 3.328}, "timestamp": "2026-01-16T16:35:30.453919"}
{"image_index": 406, "image_name": "000000046031.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046031.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2615.217, "latencies_ms": [2615.217], "images_per_second": 0.382, "prompt_tokens": 9, "response_tokens_est": 36, "n_tiles": 12, "output_text": "The image shows a cluttered desk with a laptop, a desktop computer monitor, a smartphone, a keyboard, and a mouse, all displaying green leaf patterns on their screens.", "error": null, "sys_before": {"cpu_percent": 40.8, "ram_used_mb": 25592.3, "ram_available_mb": 100179.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 25592.3, "ram_available_mb": 100179.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.07, "peak": 16.05, "min": 13.59}, "VDD_GPU": {"avg": 34.58, "peak": 41.34, "min": 27.18}, "VIN": {"avg": 75.46, "peak": 108.55, "min": 59.05}, "VDD_CPU_SOC_MSS": {"avg": 15.86, "peak": 16.92, "min": 14.57}}, "power_watts_avg": 34.58, "energy_joules_est": 90.45, "sample_count": 20, "duration_seconds": 2.616}, "timestamp": "2026-01-16T16:35:33.167653"}
{"image_index": 406, "image_name": "000000046031.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046031.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2744.395, "latencies_ms": [2744.395], "images_per_second": 0.364, "prompt_tokens": 23, "response_tokens_est": 36, "n_tiles": 12, "output_text": "1. Laptop\n2. Computer mouse\n3. Keyboard\n4. Monitor\n5. Laptop mouse\n6. Computer mouse\n7. Monitor\n8. Laptop mouse", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25592.3, "ram_available_mb": 100179.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 25592.3, "ram_available_mb": 100179.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.06, "peak": 15.65, "min": 14.43}, "VDD_GPU": {"avg": 35.34, "peak": 42.94, "min": 27.19}, "VIN": {"avg": 77.13, "peak": 141.88, "min": 58.53}, "VDD_CPU_SOC_MSS": {"avg": 15.78, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 35.34, "energy_joules_est": 97.0, "sample_count": 21, "duration_seconds": 2.745}, "timestamp": "2026-01-16T16:35:35.918552"}
{"image_index": 406, "image_name": "000000046031.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046031.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 4337.249, "latencies_ms": [4337.249], "images_per_second": 0.231, "prompt_tokens": 27, "response_tokens_est": 83, "n_tiles": 12, "output_text": "The main objects in the image are a laptop, a mouse, and a computer monitor. The laptop is positioned in the foreground, slightly to the right, with the monitor displaying a green leaf pattern. The mouse is placed near the laptop, closer to the foreground. The background is mostly obscured by the monitor and the laptop, but there appears to be a white curtain or curtain in the upper left corner.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25592.3, "ram_available_mb": 100179.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 25592.3, "ram_available_mb": 100179.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.1, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 31.95, "peak": 42.94, "min": 26.0}, "VIN": {"avg": 71.21, "peak": 119.21, "min": 54.28}, "VDD_CPU_SOC_MSS": {"avg": 15.68, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 31.95, "energy_joules_est": 138.59, "sample_count": 34, "duration_seconds": 4.338}, "timestamp": "2026-01-16T16:35:40.262585"}
{"image_index": 406, "image_name": "000000046031.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046031.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3721.782, "latencies_ms": [3721.782], "images_per_second": 0.269, "prompt_tokens": 21, "response_tokens_est": 65, "n_tiles": 12, "output_text": "The image depicts a cluttered workspace with a laptop, a desktop computer, a smartphone, and a mouse on a white surface. The scene appears to be in a home or office setting, with a green wallpaper on the computer screens, and the overall atmosphere is one of a busy, tech-savvy environment.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25592.3, "ram_available_mb": 100179.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25592.0, "ram_available_mb": 100180.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.1, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 32.88, "peak": 42.54, "min": 26.39}, "VIN": {"avg": 75.4, "peak": 116.89, "min": 58.47}, "VDD_CPU_SOC_MSS": {"avg": 15.72, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 32.88, "energy_joules_est": 122.39, "sample_count": 29, "duration_seconds": 3.722}, "timestamp": "2026-01-16T16:35:43.991256"}
{"image_index": 406, "image_name": "000000046031.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046031.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3668.382, "latencies_ms": [3668.382], "images_per_second": 0.273, "prompt_tokens": 19, "response_tokens_est": 64, "n_tiles": 12, "output_text": "The image shows a workspace with a laptop, a desktop computer monitor, and a desktop computer monitor. The workspace is well-lit with natural light coming through a window, creating a bright and airy atmosphere. The colors are predominantly neutral, with the white surfaces and the green and black screens providing a balanced contrast.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25592.0, "ram_available_mb": 100180.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25592.0, "ram_available_mb": 100180.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.12, "peak": 15.65, "min": 14.43}, "VDD_GPU": {"avg": 32.9, "peak": 42.54, "min": 26.39}, "VIN": {"avg": 73.39, "peak": 110.84, "min": 57.75}, "VDD_CPU_SOC_MSS": {"avg": 15.76, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 32.9, "energy_joules_est": 120.71, "sample_count": 29, "duration_seconds": 3.669}, "timestamp": "2026-01-16T16:35:47.668500"}
{"image_index": 407, "image_name": "000000046048.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046048.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2503.937, "latencies_ms": [2503.937], "images_per_second": 0.399, "prompt_tokens": 9, "response_tokens_est": 33, "n_tiles": 12, "output_text": "A young girl is sitting on a bed in a cozy bedroom, surrounded by toys and books, while a lamp and a small table with a lamp are nearby.", "error": null, "sys_before": {"cpu_percent": 42.0, "ram_used_mb": 25592.0, "ram_available_mb": 100180.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 25591.8, "ram_available_mb": 100180.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.17, "peak": 16.15, "min": 13.9}, "VDD_GPU": {"avg": 35.01, "peak": 41.34, "min": 27.57}, "VIN": {"avg": 76.48, "peak": 113.3, "min": 58.52}, "VDD_CPU_SOC_MSS": {"avg": 16.0, "peak": 17.32, "min": 14.96}}, "power_watts_avg": 35.01, "energy_joules_est": 87.68, "sample_count": 19, "duration_seconds": 2.504}, "timestamp": "2026-01-16T16:35:50.259196"}
{"image_index": 407, "image_name": "000000046048.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046048.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2841.018, "latencies_ms": [2841.018], "images_per_second": 0.352, "prompt_tokens": 23, "response_tokens_est": 39, "n_tiles": 12, "output_text": "bed: 1\npillow: 2\nnightstand: 1\nlamp: 1\nbook: 1\ntoy: 1\nbox: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25591.8, "ram_available_mb": 100180.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 25591.8, "ram_available_mb": 100180.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.11, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 35.02, "peak": 42.94, "min": 26.79}, "VIN": {"avg": 75.61, "peak": 113.72, "min": 58.73}, "VDD_CPU_SOC_MSS": {"avg": 15.8, "peak": 16.53, "min": 15.35}}, "power_watts_avg": 35.02, "energy_joules_est": 99.51, "sample_count": 22, "duration_seconds": 2.841}, "timestamp": "2026-01-16T16:35:53.107407"}
{"image_index": 407, "image_name": "000000046048.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046048.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3978.266, "latencies_ms": [3978.266], "images_per_second": 0.251, "prompt_tokens": 27, "response_tokens_est": 73, "n_tiles": 12, "output_text": "The main objects in the image are a bed, a chair, and a table. The bed is positioned in the foreground, with a child sitting on it. The chair is located to the left of the bed, and the table is situated to the right of the bed. The table is near the bed, and the chair is further away from the bed.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25591.8, "ram_available_mb": 100180.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25591.8, "ram_available_mb": 100180.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.11, "peak": 15.65, "min": 14.63}, "VDD_GPU": {"avg": 32.64, "peak": 42.94, "min": 26.39}, "VIN": {"avg": 71.26, "peak": 113.85, "min": 58.5}, "VDD_CPU_SOC_MSS": {"avg": 15.76, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 32.64, "energy_joules_est": 129.87, "sample_count": 31, "duration_seconds": 3.979}, "timestamp": "2026-01-16T16:35:57.095417"}
{"image_index": 407, "image_name": "000000046048.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046048.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3703.353, "latencies_ms": [3703.353], "images_per_second": 0.27, "prompt_tokens": 21, "response_tokens_est": 64, "n_tiles": 12, "output_text": "The image depicts a cozy bedroom setting with a bed, a nightstand, and a chair. A young girl is sitting on the bed, seemingly engaged in an activity, possibly reading or playing with toys. The room has warm, earthy tones, and the bed is adorned with a patterned blanket.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25591.8, "ram_available_mb": 100180.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25591.8, "ram_available_mb": 100180.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.05, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 33.03, "peak": 42.94, "min": 26.0}, "VIN": {"avg": 72.47, "peak": 113.25, "min": 55.19}, "VDD_CPU_SOC_MSS": {"avg": 15.62, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 33.03, "energy_joules_est": 122.34, "sample_count": 28, "duration_seconds": 3.704}, "timestamp": "2026-01-16T16:36:00.805793"}
{"image_index": 407, "image_name": "000000046048.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046048.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3088.608, "latencies_ms": [3088.608], "images_per_second": 0.324, "prompt_tokens": 19, "response_tokens_est": 46, "n_tiles": 12, "output_text": "The room is warmly lit with a soft orange hue, creating a cozy atmosphere. The bedspread is richly patterned with red and gold hues, complementing the wooden headboard and the woven bedside table.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25591.8, "ram_available_mb": 100180.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 25591.8, "ram_available_mb": 100180.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.78, "peak": 15.34, "min": 14.12}, "VDD_GPU": {"avg": 34.25, "peak": 42.15, "min": 26.39}, "VIN": {"avg": 74.47, "peak": 118.27, "min": 57.99}, "VDD_CPU_SOC_MSS": {"avg": 15.3, "peak": 16.14, "min": 14.56}}, "power_watts_avg": 34.25, "energy_joules_est": 105.8, "sample_count": 23, "duration_seconds": 3.089}, "timestamp": "2026-01-16T16:36:03.900918"}
{"image_index": 408, "image_name": "000000046252.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046252.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2682.571, "latencies_ms": [2682.571], "images_per_second": 0.373, "prompt_tokens": 9, "response_tokens_est": 38, "n_tiles": 12, "output_text": "A baseball player in a red uniform is preparing to swing at a pitch, while a catcher and an umpire are crouched behind him, ready to catch the ball.", "error": null, "sys_before": {"cpu_percent": 40.2, "ram_used_mb": 25591.8, "ram_available_mb": 100180.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 25591.7, "ram_available_mb": 100180.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.1, "peak": 16.15, "min": 13.59}, "VDD_GPU": {"avg": 34.31, "peak": 42.15, "min": 26.79}, "VIN": {"avg": 77.71, "peak": 119.69, "min": 58.35}, "VDD_CPU_SOC_MSS": {"avg": 15.82, "peak": 17.32, "min": 14.57}}, "power_watts_avg": 34.31, "energy_joules_est": 92.06, "sample_count": 21, "duration_seconds": 2.683}, "timestamp": "2026-01-16T16:36:06.676026"}
{"image_index": 408, "image_name": "000000046252.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046252.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2809.581, "latencies_ms": [2809.581], "images_per_second": 0.356, "prompt_tokens": 23, "response_tokens_est": 38, "n_tiles": 12, "output_text": "baseball bat: 1\ncatcher's mitt: 1\numpire: 1\nplayer: 1\npitcher: 1\nhome plate: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25591.7, "ram_available_mb": 100180.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 25592.0, "ram_available_mb": 100180.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.1, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 35.38, "peak": 42.54, "min": 27.18}, "VIN": {"avg": 76.28, "peak": 118.04, "min": 58.8}, "VDD_CPU_SOC_MSS": {"avg": 15.73, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 35.38, "energy_joules_est": 99.41, "sample_count": 21, "duration_seconds": 2.81}, "timestamp": "2026-01-16T16:36:09.492442"}
{"image_index": 408, "image_name": "000000046252.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046252.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3858.693, "latencies_ms": [3858.693], "images_per_second": 0.259, "prompt_tokens": 27, "response_tokens_est": 69, "n_tiles": 12, "output_text": "The baseball player is positioned in the foreground, crouched near the home plate, ready to swing at the incoming pitch. The catcher is in the background, crouched behind the home plate, ready to catch the ball. The umpire is further back, standing near the umpire's box, observing the play.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 25592.0, "ram_available_mb": 100180.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25591.7, "ram_available_mb": 100180.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.11, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 32.73, "peak": 42.54, "min": 26.38}, "VIN": {"avg": 73.19, "peak": 120.49, "min": 56.55}, "VDD_CPU_SOC_MSS": {"avg": 15.76, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 32.73, "energy_joules_est": 126.3, "sample_count": 29, "duration_seconds": 3.859}, "timestamp": "2026-01-16T16:36:13.359045"}
{"image_index": 408, "image_name": "000000046252.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046252.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3719.705, "latencies_ms": [3719.705], "images_per_second": 0.269, "prompt_tokens": 21, "response_tokens_est": 65, "n_tiles": 12, "output_text": "The image captures a baseball game in progress on a sunny day. The batter is in the midst of swinging at the pitch, while the catcher is crouched behind home plate, ready to catch the ball. The scene is set on a dirt field with a well-maintained grassy area in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25591.7, "ram_available_mb": 100180.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25592.0, "ram_available_mb": 100180.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.1, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 32.84, "peak": 42.54, "min": 26.39}, "VIN": {"avg": 73.25, "peak": 114.28, "min": 58.84}, "VDD_CPU_SOC_MSS": {"avg": 15.75, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 32.84, "energy_joules_est": 122.17, "sample_count": 29, "duration_seconds": 3.72}, "timestamp": "2026-01-16T16:36:17.085726"}
{"image_index": 408, "image_name": "000000046252.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046252.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2640.158, "latencies_ms": [2640.158], "images_per_second": 0.379, "prompt_tokens": 19, "response_tokens_est": 33, "n_tiles": 12, "output_text": "The baseball field is well-maintained with a vibrant green grass and a dirt infield. The sunlight casts sharp shadows, indicating a bright, sunny day.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25592.0, "ram_available_mb": 100180.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 25591.7, "ram_available_mb": 100180.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.1, "peak": 15.65, "min": 14.63}, "VDD_GPU": {"avg": 35.67, "peak": 42.54, "min": 27.57}, "VIN": {"avg": 75.58, "peak": 113.45, "min": 58.56}, "VDD_CPU_SOC_MSS": {"avg": 15.73, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 35.67, "energy_joules_est": 94.19, "sample_count": 20, "duration_seconds": 2.64}, "timestamp": "2026-01-16T16:36:19.732711"}
{"image_index": 409, "image_name": "000000046378.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046378.jpg", "image_width": 640, "image_height": 360, "image_resolution": "640x360", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 970.4, "latencies_ms": [970.4], "images_per_second": 1.031, "prompt_tokens": 9, "response_tokens_est": 36, "n_tiles": 2, "output_text": "A cat is seen in the image, its mouth open and its teeth visible as it appears to be in the process of eating a fish, which is lying on the ground.", "error": null, "sys_before": {"cpu_percent": 34.2, "ram_used_mb": 25591.7, "ram_available_mb": 100180.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 25591.7, "ram_available_mb": 100180.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5055.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.67, "peak": 15.22, "min": 14.1}, "VDD_GPU": {"avg": 27.75, "peak": 34.66, "min": 23.25}, "VIN": {"avg": 64.83, "peak": 76.0, "min": 58.86}, "VDD_CPU_SOC_MSS": {"avg": 15.41, "peak": 16.14, "min": 14.96}}, "power_watts_avg": 27.75, "energy_joules_est": 26.94, "sample_count": 7, "duration_seconds": 0.971}, "timestamp": "2026-01-16T16:36:20.733324"}
{"image_index": 409, "image_name": "000000046378.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046378.jpg", "image_width": 640, "image_height": 360, "image_resolution": "640x360", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 522.388, "latencies_ms": [522.388], "images_per_second": 1.914, "prompt_tokens": 23, "response_tokens_est": 15, "n_tiles": 2, "output_text": "cat: 1\nfish: 1\nground: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25591.7, "ram_available_mb": 100180.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.0, "ram_used_mb": 25591.7, "ram_available_mb": 100180.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5064.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.71, "peak": 14.81, "min": 14.51}, "VDD_GPU": {"avg": 29.81, "peak": 31.91, "min": 27.57}, "VIN": {"avg": 73.6, "peak": 98.14, "min": 60.48}, "VDD_CPU_SOC_MSS": {"avg": 15.22, "peak": 15.35, "min": 14.96}}, "power_watts_avg": 29.81, "energy_joules_est": 15.59, "sample_count": 3, "duration_seconds": 0.523}, "timestamp": "2026-01-16T16:36:21.262950"}
{"image_index": 409, "image_name": "000000046378.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046378.jpg", "image_width": 640, "image_height": 360, "image_resolution": "640x360", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1491.521, "latencies_ms": [1491.521], "images_per_second": 0.67, "prompt_tokens": 27, "response_tokens_est": 56, "n_tiles": 2, "output_text": "The main object in the foreground is a cat, which is positioned near the center of the image. The cat is interacting with a dead bird, which is located in the background. The cat's body is slightly angled towards the bird, indicating a close proximity and interaction.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25591.7, "ram_available_mb": 100180.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 25591.7, "ram_available_mb": 100180.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5067.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.05, "peak": 15.52, "min": 14.4}, "VDD_GPU": {"avg": 25.39, "peak": 34.27, "min": 21.27}, "VIN": {"avg": 66.99, "peak": 100.54, "min": 62.39}, "VDD_CPU_SOC_MSS": {"avg": 15.78, "peak": 16.14, "min": 14.96}}, "power_watts_avg": 25.39, "energy_joules_est": 37.87, "sample_count": 11, "duration_seconds": 1.492}, "timestamp": "2026-01-16T16:36:22.761105"}
{"image_index": 409, "image_name": "000000046378.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046378.jpg", "image_width": 640, "image_height": 360, "image_resolution": "640x360", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1747.93, "latencies_ms": [1747.93], "images_per_second": 0.572, "prompt_tokens": 21, "response_tokens_est": 67, "n_tiles": 2, "output_text": "The image depicts a cat in a natural setting, possibly a park or garden, where it is seen eating a fish. The cat is focused on its meal, with its mouth open and its whiskers clearly visible. The fish is lying on the ground, and the cat's fur is predominantly white with grey and black stripes.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25591.7, "ram_available_mb": 100180.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25591.7, "ram_available_mb": 100180.5, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5063.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.18, "peak": 15.52, "min": 14.61}, "VDD_GPU": {"avg": 23.76, "peak": 31.12, "min": 20.89}, "VIN": {"avg": 66.56, "peak": 92.59, "min": 60.36}, "VDD_CPU_SOC_MSS": {"avg": 15.93, "peak": 16.14, "min": 15.36}}, "power_watts_avg": 23.76, "energy_joules_est": 41.54, "sample_count": 13, "duration_seconds": 1.748}, "timestamp": "2026-01-16T16:36:24.515060"}
{"image_index": 409, "image_name": "000000046378.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046378.jpg", "image_width": 640, "image_height": 360, "image_resolution": "640x360", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1726.794, "latencies_ms": [1726.794], "images_per_second": 0.579, "prompt_tokens": 19, "response_tokens_est": 66, "n_tiles": 2, "output_text": "The image features a cat with a predominantly gray and white fur coat, which is highlighted by the natural light casting soft shadows on the ground. The cat is seen in a close-up, with its head resting on a piece of wood, surrounded by a mix of dried leaves and twigs, indicating a natural, outdoor setting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25591.7, "ram_available_mb": 100180.5, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 25592.0, "ram_available_mb": 100180.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5062.6, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.25, "peak": 15.52, "min": 14.61}, "VDD_GPU": {"avg": 23.52, "peak": 30.73, "min": 20.89}, "VIN": {"avg": 66.68, "peak": 103.5, "min": 61.0}, "VDD_CPU_SOC_MSS": {"avg": 15.87, "peak": 16.14, "min": 15.36}}, "power_watts_avg": 23.52, "energy_joules_est": 40.62, "sample_count": 13, "duration_seconds": 1.727}, "timestamp": "2026-01-16T16:36:26.247862"}
{"image_index": 410, "image_name": "000000046463.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046463.jpg", "image_width": 500, "image_height": 400, "image_resolution": "500x400", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2447.422, "latencies_ms": [2447.422], "images_per_second": 0.409, "prompt_tokens": 9, "response_tokens_est": 31, "n_tiles": 12, "output_text": "The image shows a close-up of a person's hand holding a piece of food, which appears to be a slice of pizza with a green topping.", "error": null, "sys_before": {"cpu_percent": 45.3, "ram_used_mb": 25591.9, "ram_available_mb": 100180.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 25591.9, "ram_available_mb": 100180.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.23, "peak": 16.15, "min": 14.2}, "VDD_GPU": {"avg": 34.46, "peak": 40.97, "min": 27.19}, "VIN": {"avg": 75.66, "peak": 114.36, "min": 58.9}, "VDD_CPU_SOC_MSS": {"avg": 16.08, "peak": 17.32, "min": 15.35}}, "power_watts_avg": 34.46, "energy_joules_est": 84.37, "sample_count": 19, "duration_seconds": 2.448}, "timestamp": "2026-01-16T16:36:28.762935"}
{"image_index": 410, "image_name": "000000046463.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046463.jpg", "image_width": 500, "image_height": 400, "image_resolution": "500x400", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2674.137, "latencies_ms": [2674.137], "images_per_second": 0.374, "prompt_tokens": 23, "response_tokens_est": 34, "n_tiles": 12, "output_text": "1. Bread\n2. Knife\n3. Cutting board\n4. Knife\n5. Knife\n6. Knife\n7. Knife\n8. Knife", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25591.9, "ram_available_mb": 100180.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 25591.9, "ram_available_mb": 100180.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.1, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 35.62, "peak": 43.31, "min": 27.18}, "VIN": {"avg": 75.15, "peak": 113.28, "min": 56.35}, "VDD_CPU_SOC_MSS": {"avg": 15.67, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 35.62, "energy_joules_est": 95.26, "sample_count": 21, "duration_seconds": 2.674}, "timestamp": "2026-01-16T16:36:31.443716"}
{"image_index": 410, "image_name": "000000046463.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046463.jpg", "image_width": 500, "image_height": 400, "image_resolution": "500x400", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3824.627, "latencies_ms": [3824.627], "images_per_second": 0.261, "prompt_tokens": 27, "response_tokens_est": 68, "n_tiles": 12, "output_text": "The main object in the foreground is a piece of food, which appears to be a slice of bread with a green leafy topping. The background is slightly blurred, but it seems to be a kitchen counter or table. The green leafy topping is located on the right side of the image, near the edge of the counter.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25591.9, "ram_available_mb": 100180.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25592.2, "ram_available_mb": 100180.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.1, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 32.94, "peak": 42.54, "min": 26.39}, "VIN": {"avg": 74.68, "peak": 116.73, "min": 58.99}, "VDD_CPU_SOC_MSS": {"avg": 15.65, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 32.94, "energy_joules_est": 125.99, "sample_count": 29, "duration_seconds": 3.825}, "timestamp": "2026-01-16T16:36:35.274560"}
{"image_index": 410, "image_name": "000000046463.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046463.jpg", "image_width": 500, "image_height": 400, "image_resolution": "500x400", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3413.751, "latencies_ms": [3413.751], "images_per_second": 0.293, "prompt_tokens": 21, "response_tokens_est": 56, "n_tiles": 12, "output_text": "The image shows a close-up of a person's hand holding a piece of food, which appears to be a slice of pizza. The hand is positioned on a white surface, possibly a cutting board, and the focus is on the food, making the background slightly blurred.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25592.2, "ram_available_mb": 100180.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 25591.9, "ram_available_mb": 100180.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.1, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 33.54, "peak": 42.94, "min": 26.39}, "VIN": {"avg": 75.27, "peak": 109.49, "min": 57.54}, "VDD_CPU_SOC_MSS": {"avg": 15.64, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 33.54, "energy_joules_est": 114.52, "sample_count": 26, "duration_seconds": 3.415}, "timestamp": "2026-01-16T16:36:38.695139"}
{"image_index": 410, "image_name": "000000046463.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046463.jpg", "image_width": 500, "image_height": 400, "image_resolution": "500x400", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3778.399, "latencies_ms": [3778.399], "images_per_second": 0.265, "prompt_tokens": 19, "response_tokens_est": 67, "n_tiles": 12, "output_text": "The image shows a close-up of a person's hand holding a piece of food with a green leafy vegetable on top. The food appears to be a sandwich or a wrap, and the background is blurred, with a white surface and some indistinct objects. The lighting is soft and natural, suggesting an indoor setting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25591.9, "ram_available_mb": 100180.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25591.9, "ram_available_mb": 100180.2, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.11, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 32.8, "peak": 42.54, "min": 26.39}, "VIN": {"avg": 73.59, "peak": 126.52, "min": 55.75}, "VDD_CPU_SOC_MSS": {"avg": 15.69, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 32.8, "energy_joules_est": 123.96, "sample_count": 29, "duration_seconds": 3.779}, "timestamp": "2026-01-16T16:36:42.480700"}
{"image_index": 411, "image_name": "000000046497.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046497.jpg", "image_width": 500, "image_height": 332, "image_resolution": "500x332", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1763.019, "latencies_ms": [1763.019], "images_per_second": 0.567, "prompt_tokens": 9, "response_tokens_est": 44, "n_tiles": 6, "output_text": "Two young girls are sitting on a gray inflatable ring on the deck of a sailboat, with one of them wearing a pink shirt and the other a white shirt, both looking out over the calm blue water.", "error": null, "sys_before": {"cpu_percent": 43.7, "ram_used_mb": 25591.9, "ram_available_mb": 100180.2, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 25592.4, "ram_available_mb": 100179.7, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.59, "peak": 15.11, "min": 13.92}, "VDD_GPU": {"avg": 30.85, "peak": 37.82, "min": 24.82}, "VIN": {"avg": 70.31, "peak": 115.01, "min": 53.81}, "VDD_CPU_SOC_MSS": {"avg": 15.23, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 30.85, "energy_joules_est": 54.4, "sample_count": 13, "duration_seconds": 1.764}, "timestamp": "2026-01-16T16:36:44.300593"}
{"image_index": 411, "image_name": "000000046497.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046497.jpg", "image_width": 500, "image_height": 332, "image_resolution": "500x332", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1512.351, "latencies_ms": [1512.351], "images_per_second": 0.661, "prompt_tokens": 23, "response_tokens_est": 33, "n_tiles": 6, "output_text": "1. Sailboat\n2. Sail\n3. Woman\n4. Woman\n5. Woman\n6. Woman\n7. Woman\n8. Woman", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25592.4, "ram_available_mb": 100179.7, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25592.2, "ram_available_mb": 100180.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.75, "peak": 15.11, "min": 14.22}, "VDD_GPU": {"avg": 32.37, "peak": 38.21, "min": 25.6}, "VIN": {"avg": 74.13, "peak": 106.4, "min": 62.01}, "VDD_CPU_SOC_MSS": {"avg": 15.39, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 32.37, "energy_joules_est": 48.96, "sample_count": 11, "duration_seconds": 1.513}, "timestamp": "2026-01-16T16:36:45.819117"}
{"image_index": 411, "image_name": "000000046497.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046497.jpg", "image_width": 500, "image_height": 332, "image_resolution": "500x332", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2719.539, "latencies_ms": [2719.539], "images_per_second": 0.368, "prompt_tokens": 27, "response_tokens_est": 78, "n_tiles": 6, "output_text": "The main objects in the image are the two young girls and the boat. The girls are positioned in the foreground, with one sitting on a gray cushion and the other on a white cushion. The boat is in the background, with its sail and rigging visible. The girls are near the edge of the boat, with the cushion they are sitting on being the closest to the camera.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25592.2, "ram_available_mb": 100180.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 25592.4, "ram_available_mb": 100179.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.02, "peak": 15.32, "min": 14.22}, "VDD_GPU": {"avg": 28.59, "peak": 38.59, "min": 23.64}, "VIN": {"avg": 67.85, "peak": 103.97, "min": 53.32}, "VDD_CPU_SOC_MSS": {"avg": 15.54, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 28.59, "energy_joules_est": 77.76, "sample_count": 21, "duration_seconds": 2.72}, "timestamp": "2026-01-16T16:36:48.544776"}
{"image_index": 411, "image_name": "000000046497.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046497.jpg", "image_width": 500, "image_height": 332, "image_resolution": "500x332", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2415.352, "latencies_ms": [2415.352], "images_per_second": 0.414, "prompt_tokens": 21, "response_tokens_est": 67, "n_tiles": 6, "output_text": "The image depicts a serene scene on a boat, with two young girls sitting on a gray cushion. They are enjoying the view of the blue water, and one of them is wearing a straw hat. The setting is outdoors, likely on a sunny day, with the girls engaged in a leisurely activity on the water.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25592.4, "ram_available_mb": 100179.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 25592.4, "ram_available_mb": 100179.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.99, "peak": 15.32, "min": 14.32}, "VDD_GPU": {"avg": 29.03, "peak": 38.21, "min": 23.64}, "VIN": {"avg": 68.5, "peak": 112.66, "min": 53.74}, "VDD_CPU_SOC_MSS": {"avg": 15.5, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 29.03, "energy_joules_est": 70.13, "sample_count": 19, "duration_seconds": 2.416}, "timestamp": "2026-01-16T16:36:50.966319"}
{"image_index": 411, "image_name": "000000046497.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046497.jpg", "image_width": 500, "image_height": 332, "image_resolution": "500x332", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2067.615, "latencies_ms": [2067.615], "images_per_second": 0.484, "prompt_tokens": 19, "response_tokens_est": 54, "n_tiles": 6, "output_text": "The image depicts a sunny day on a boat with clear blue water. The boat's white hull and the bright sunlight create a vivid contrast. The girls are wearing light-colored clothing, and the bright orange lifebuoy adds a pop of color to the scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25592.4, "ram_available_mb": 100179.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 25592.4, "ram_available_mb": 100179.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.95, "peak": 15.32, "min": 14.32}, "VDD_GPU": {"avg": 29.86, "peak": 38.19, "min": 24.03}, "VIN": {"avg": 69.02, "peak": 100.12, "min": 56.85}, "VDD_CPU_SOC_MSS": {"avg": 15.5, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 29.86, "energy_joules_est": 61.76, "sample_count": 16, "duration_seconds": 2.068}, "timestamp": "2026-01-16T16:36:53.040588"}
{"image_index": 412, "image_name": "000000046804.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046804.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1884.136, "latencies_ms": [1884.136], "images_per_second": 0.531, "prompt_tokens": 9, "response_tokens_est": 14, "n_tiles": 12, "output_text": "A sheep stands in a grassy field with a rocky background.", "error": null, "sys_before": {"cpu_percent": 43.0, "ram_used_mb": 25592.4, "ram_available_mb": 100179.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 2.1, "ram_used_mb": 25592.4, "ram_available_mb": 100179.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.23, "peak": 16.15, "min": 14.1}, "VDD_GPU": {"avg": 36.97, "peak": 41.36, "min": 29.15}, "VIN": {"avg": 80.36, "peak": 121.03, "min": 58.11}, "VDD_CPU_SOC_MSS": {"avg": 16.17, "peak": 17.32, "min": 15.35}}, "power_watts_avg": 36.97, "energy_joules_est": 69.69, "sample_count": 14, "duration_seconds": 1.885}, "timestamp": "2026-01-16T16:36:54.999573"}
{"image_index": 412, "image_name": "000000046804.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046804.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2811.78, "latencies_ms": [2811.78], "images_per_second": 0.356, "prompt_tokens": 23, "response_tokens_est": 38, "n_tiles": 12, "output_text": "1. Sheep\n2. Grass\n3. Stone wall\n4. Lichen\n5. Bushes\n6. Rocks\n7. Tree\n8. Shadows", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25592.4, "ram_available_mb": 100179.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 25592.2, "ram_available_mb": 100180.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.08, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 35.7, "peak": 44.12, "min": 26.8}, "VIN": {"avg": 71.53, "peak": 106.17, "min": 50.5}, "VDD_CPU_SOC_MSS": {"avg": 15.72, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 35.7, "energy_joules_est": 100.39, "sample_count": 21, "duration_seconds": 2.812}, "timestamp": "2026-01-16T16:36:57.819512"}
{"image_index": 412, "image_name": "000000046804.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046804.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3662.609, "latencies_ms": [3662.609], "images_per_second": 0.273, "prompt_tokens": 27, "response_tokens_est": 62, "n_tiles": 12, "output_text": "The main object in the foreground is a sheep standing on a grassy field. The sheep is positioned near the center of the image, with its body facing the camera and its head turned slightly to the side. The background features a rocky wall with patches of yellow moss, adding depth to the scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25592.2, "ram_available_mb": 100180.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25592.4, "ram_available_mb": 100179.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.98, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 32.88, "peak": 42.15, "min": 25.6}, "VIN": {"avg": 73.34, "peak": 127.45, "min": 55.36}, "VDD_CPU_SOC_MSS": {"avg": 15.54, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 32.88, "energy_joules_est": 120.44, "sample_count": 28, "duration_seconds": 3.663}, "timestamp": "2026-01-16T16:37:01.488701"}
{"image_index": 412, "image_name": "000000046804.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046804.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2823.608, "latencies_ms": [2823.608], "images_per_second": 0.354, "prompt_tokens": 21, "response_tokens_est": 38, "n_tiles": 12, "output_text": "The image depicts a sheep standing in a grassy field with a rocky, yellowish wall in the background. The sheep appears to be calm and is looking directly at the camera.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25592.4, "ram_available_mb": 100179.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 25592.4, "ram_available_mb": 100179.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.75, "peak": 15.34, "min": 14.12}, "VDD_GPU": {"avg": 35.03, "peak": 42.53, "min": 26.39}, "VIN": {"avg": 72.38, "peak": 118.96, "min": 58.42}, "VDD_CPU_SOC_MSS": {"avg": 15.35, "peak": 16.14, "min": 14.56}}, "power_watts_avg": 35.03, "energy_joules_est": 98.92, "sample_count": 21, "duration_seconds": 2.824}, "timestamp": "2026-01-16T16:37:04.319330"}
{"image_index": 412, "image_name": "000000046804.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046804.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3575.08, "latencies_ms": [3575.08], "images_per_second": 0.28, "prompt_tokens": 19, "response_tokens_est": 60, "n_tiles": 12, "output_text": "The sheep in the image is predominantly white, with a fluffy, dense wool coat that appears soft and well-groomed. The lighting is bright and natural, suggesting a sunny day with clear skies. The grass is green and appears to be well-maintained, indicating a healthy environment.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25592.4, "ram_available_mb": 100179.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 25592.1, "ram_available_mb": 100180.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.76, "peak": 15.34, "min": 14.12}, "VDD_GPU": {"avg": 33.0, "peak": 42.15, "min": 26.0}, "VIN": {"avg": 71.28, "peak": 97.98, "min": 56.34}, "VDD_CPU_SOC_MSS": {"avg": 15.25, "peak": 16.14, "min": 14.56}}, "power_watts_avg": 33.0, "energy_joules_est": 117.99, "sample_count": 27, "duration_seconds": 3.576}, "timestamp": "2026-01-16T16:37:07.901064"}
{"image_index": 413, "image_name": "000000046872.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046872.jpg", "image_width": 640, "image_height": 391, "image_resolution": "640x391", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1738.483, "latencies_ms": [1738.483], "images_per_second": 0.575, "prompt_tokens": 9, "response_tokens_est": 43, "n_tiles": 6, "output_text": "A man in a white t-shirt and plaid shorts is standing next to a large, yellow and black trailer, holding onto a yellow hose, while another man in a blue shirt is pointing towards the trailer.", "error": null, "sys_before": {"cpu_percent": 41.2, "ram_used_mb": 25592.1, "ram_available_mb": 100180.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 25592.1, "ram_available_mb": 100180.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.48, "peak": 15.11, "min": 13.72}, "VDD_GPU": {"avg": 30.69, "peak": 37.42, "min": 24.82}, "VIN": {"avg": 68.73, "peak": 109.93, "min": 55.46}, "VDD_CPU_SOC_MSS": {"avg": 15.11, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 30.69, "energy_joules_est": 53.38, "sample_count": 13, "duration_seconds": 1.739}, "timestamp": "2026-01-16T16:37:09.702538"}
{"image_index": 413, "image_name": "000000046872.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046872.jpg", "image_width": 640, "image_height": 391, "image_resolution": "640x391", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1514.692, "latencies_ms": [1514.692], "images_per_second": 0.66, "prompt_tokens": 23, "response_tokens_est": 32, "n_tiles": 6, "output_text": "1. Truck\n2. Person\n3. Person\n4. Person\n5. Person\n6. Person\n7. Person\n8. Person", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25592.1, "ram_available_mb": 100180.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 25592.1, "ram_available_mb": 100180.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.79, "peak": 15.11, "min": 14.32}, "VDD_GPU": {"avg": 32.58, "peak": 38.21, "min": 26.0}, "VIN": {"avg": 71.41, "peak": 121.14, "min": 56.48}, "VDD_CPU_SOC_MSS": {"avg": 15.39, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 32.58, "energy_joules_est": 49.36, "sample_count": 11, "duration_seconds": 1.515}, "timestamp": "2026-01-16T16:37:11.224640"}
{"image_index": 413, "image_name": "000000046872.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046872.jpg", "image_width": 640, "image_height": 391, "image_resolution": "640x391", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2270.879, "latencies_ms": [2270.879], "images_per_second": 0.44, "prompt_tokens": 27, "response_tokens_est": 59, "n_tiles": 6, "output_text": "The main objects in the image are a large yellow and black trailer, a man standing on the trailer, and a parked truck in the background. The trailer is positioned in the foreground, with the man standing on it. The truck is parked in the background, further away from the trailer.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25592.1, "ram_available_mb": 100180.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 25592.1, "ram_available_mb": 100180.0, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.75, "peak": 15.01, "min": 14.12}, "VDD_GPU": {"avg": 29.5, "peak": 38.6, "min": 23.64}, "VIN": {"avg": 69.49, "peak": 97.53, "min": 62.26}, "VDD_CPU_SOC_MSS": {"avg": 15.31, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 29.5, "energy_joules_est": 67.0, "sample_count": 17, "duration_seconds": 2.271}, "timestamp": "2026-01-16T16:37:13.501791"}
{"image_index": 413, "image_name": "000000046872.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046872.jpg", "image_width": 640, "image_height": 391, "image_resolution": "640x391", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2436.019, "latencies_ms": [2436.019], "images_per_second": 0.411, "prompt_tokens": 21, "response_tokens_est": 65, "n_tiles": 6, "output_text": "The scene depicts a man standing next to a large, yellow and black trailer on a paved surface, possibly a parking lot. He is holding a hose, suggesting he is either preparing to fill the trailer or has just finished. The background features a variety of vehicles and trailers, indicating a storage or loading area.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25592.1, "ram_available_mb": 100180.0, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 25592.6, "ram_available_mb": 100179.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.78, "peak": 15.01, "min": 14.12}, "VDD_GPU": {"avg": 28.91, "peak": 38.21, "min": 23.24}, "VIN": {"avg": 69.26, "peak": 106.14, "min": 56.84}, "VDD_CPU_SOC_MSS": {"avg": 15.35, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 28.91, "energy_joules_est": 70.43, "sample_count": 18, "duration_seconds": 2.436}, "timestamp": "2026-01-16T16:37:15.944038"}
{"image_index": 413, "image_name": "000000046872.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000046872.jpg", "image_width": 640, "image_height": 391, "image_resolution": "640x391", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2360.153, "latencies_ms": [2360.153], "images_per_second": 0.424, "prompt_tokens": 19, "response_tokens_est": 62, "n_tiles": 6, "output_text": "The image features a large, yellow and black cylindrical object on a flatbed trailer, with a man standing beside it. The trailer is parked on a paved surface, and the background includes trees and a building. The lighting is natural, suggesting daytime, and the weather appears to be clear.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25592.6, "ram_available_mb": 100179.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 25592.6, "ram_available_mb": 100179.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.74, "peak": 15.01, "min": 14.02}, "VDD_GPU": {"avg": 28.69, "peak": 38.21, "min": 23.24}, "VIN": {"avg": 70.25, "peak": 115.27, "min": 55.45}, "VDD_CPU_SOC_MSS": {"avg": 15.31, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 28.69, "energy_joules_est": 67.72, "sample_count": 18, "duration_seconds": 2.36}, "timestamp": "2026-01-16T16:37:18.310377"}
{"image_index": 414, "image_name": "000000047010.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047010.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2365.752, "latencies_ms": [2365.752], "images_per_second": 0.423, "prompt_tokens": 9, "response_tokens_est": 29, "n_tiles": 12, "output_text": "The image depicts a giraffe walking through a dirt path surrounded by lush greenery, with a small goat lying on the ground nearby.", "error": null, "sys_before": {"cpu_percent": 43.2, "ram_used_mb": 25592.4, "ram_available_mb": 100179.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 2.6, "ram_used_mb": 25592.4, "ram_available_mb": 100179.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.17, "peak": 16.15, "min": 13.9}, "VDD_GPU": {"avg": 35.16, "peak": 40.97, "min": 27.97}, "VIN": {"avg": 74.63, "peak": 116.05, "min": 59.45}, "VDD_CPU_SOC_MSS": {"avg": 15.94, "peak": 17.32, "min": 14.96}}, "power_watts_avg": 35.16, "energy_joules_est": 83.2, "sample_count": 18, "duration_seconds": 2.366}, "timestamp": "2026-01-16T16:37:20.760404"}
{"image_index": 414, "image_name": "000000047010.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047010.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1958.99, "latencies_ms": [1958.99], "images_per_second": 0.51, "prompt_tokens": 23, "response_tokens_est": 13, "n_tiles": 12, "output_text": "giraffe: 2\ngoat: 1", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25592.4, "ram_available_mb": 100179.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 2.1, "ram_used_mb": 25592.4, "ram_available_mb": 100179.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.17, "peak": 15.65, "min": 14.63}, "VDD_GPU": {"avg": 38.36, "peak": 42.92, "min": 32.3}, "VIN": {"avg": 82.18, "peak": 122.38, "min": 58.3}, "VDD_CPU_SOC_MSS": {"avg": 15.8, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 38.36, "energy_joules_est": 75.16, "sample_count": 15, "duration_seconds": 1.959}, "timestamp": "2026-01-16T16:37:22.727104"}
{"image_index": 414, "image_name": "000000047010.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047010.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3638.158, "latencies_ms": [3638.158], "images_per_second": 0.275, "prompt_tokens": 27, "response_tokens_est": 63, "n_tiles": 12, "output_text": "In the image, the giraffes are positioned in the foreground, with the closest giraffe standing near the water's edge. The second giraffe is further back, walking on the dirt. The background features a dense forest with various trees and shrubs, creating a natural habitat for the giraffes.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25592.4, "ram_available_mb": 100179.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25592.4, "ram_available_mb": 100179.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.1, "peak": 15.54, "min": 14.43}, "VDD_GPU": {"avg": 33.69, "peak": 44.12, "min": 26.39}, "VIN": {"avg": 73.82, "peak": 121.0, "min": 58.48}, "VDD_CPU_SOC_MSS": {"avg": 15.75, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 33.69, "energy_joules_est": 122.58, "sample_count": 28, "duration_seconds": 3.638}, "timestamp": "2026-01-16T16:37:26.371941"}
{"image_index": 414, "image_name": "000000047010.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047010.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 4083.905, "latencies_ms": [4083.905], "images_per_second": 0.245, "prompt_tokens": 21, "response_tokens_est": 76, "n_tiles": 12, "output_text": "The image depicts a serene and natural setting, likely within a zoo or wildlife reserve, featuring a dirt path surrounded by lush greenery. The path is flanked by trees and bushes, with a small pond reflecting the surrounding foliage. On the path, there are giraffes and a goat, with the giraffes walking and the goat resting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25592.4, "ram_available_mb": 100179.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25592.4, "ram_available_mb": 100179.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.11, "peak": 15.65, "min": 14.63}, "VDD_GPU": {"avg": 32.31, "peak": 42.54, "min": 26.39}, "VIN": {"avg": 71.45, "peak": 105.86, "min": 58.47}, "VDD_CPU_SOC_MSS": {"avg": 15.7, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 32.31, "energy_joules_est": 131.96, "sample_count": 32, "duration_seconds": 4.084}, "timestamp": "2026-01-16T16:37:30.462630"}
{"image_index": 414, "image_name": "000000047010.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047010.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3550.812, "latencies_ms": [3550.812], "images_per_second": 0.282, "prompt_tokens": 19, "response_tokens_est": 60, "n_tiles": 12, "output_text": "The image depicts a giraffe standing in a natural, grassy environment with a dirt path. The giraffe has a brown and white coat, and its long neck and legs are prominently visible. The lighting is natural, suggesting it is daytime, and the weather appears to be clear.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 25592.4, "ram_available_mb": 100179.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25592.4, "ram_available_mb": 100179.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.1, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 33.19, "peak": 42.94, "min": 26.39}, "VIN": {"avg": 73.63, "peak": 118.37, "min": 51.3}, "VDD_CPU_SOC_MSS": {"avg": 15.75, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 33.19, "energy_joules_est": 117.88, "sample_count": 28, "duration_seconds": 3.552}, "timestamp": "2026-01-16T16:37:34.020297"}
{"image_index": 415, "image_name": "000000047112.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047112.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 3182.761, "latencies_ms": [3182.761], "images_per_second": 0.314, "prompt_tokens": 9, "response_tokens_est": 53, "n_tiles": 12, "output_text": "The image shows a freshly baked pizza with melted cheese, sliced mushrooms, and pieces of ham, all resting on a white plate, set on a table covered with a blue tablecloth, with a glass of beer and a glass of water in the background.", "error": null, "sys_before": {"cpu_percent": 43.4, "ram_used_mb": 25592.4, "ram_available_mb": 100179.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 25592.4, "ram_available_mb": 100179.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.14, "peak": 16.05, "min": 13.9}, "VDD_GPU": {"avg": 33.2, "peak": 41.36, "min": 26.39}, "VIN": {"avg": 76.45, "peak": 116.04, "min": 58.76}, "VDD_CPU_SOC_MSS": {"avg": 15.99, "peak": 17.32, "min": 14.96}}, "power_watts_avg": 33.2, "energy_joules_est": 105.68, "sample_count": 24, "duration_seconds": 3.183}, "timestamp": "2026-01-16T16:37:37.302105"}
{"image_index": 415, "image_name": "000000047112.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047112.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 5852.438, "latencies_ms": [5852.438], "images_per_second": 0.171, "prompt_tokens": 23, "response_tokens_est": 128, "n_tiles": 12, "output_text": "object: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject:", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25592.4, "ram_available_mb": 100179.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 25592.4, "ram_available_mb": 100179.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.12, "peak": 15.65, "min": 14.43}, "VDD_GPU": {"avg": 30.51, "peak": 42.94, "min": 26.0}, "VIN": {"avg": 71.56, "peak": 107.77, "min": 57.72}, "VDD_CPU_SOC_MSS": {"avg": 15.76, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 30.51, "energy_joules_est": 178.58, "sample_count": 46, "duration_seconds": 5.853}, "timestamp": "2026-01-16T16:37:43.161634"}
{"image_index": 415, "image_name": "000000047112.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047112.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 4568.995, "latencies_ms": [4568.995], "images_per_second": 0.219, "prompt_tokens": 27, "response_tokens_est": 90, "n_tiles": 12, "output_text": "The main object in the foreground is a large, freshly baked pizza with melted cheese and toppings. The pizza is placed on a white plate, which is placed on a table covered with a blue tablecloth. In the background, there are two wine glasses and a bottle of wine on a bar counter, indicating that the setting is a restaurant. The pizza is positioned near the center of the image, drawing attention to its size and appearance.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25592.4, "ram_available_mb": 100179.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 25592.4, "ram_available_mb": 100179.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.11, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 31.59, "peak": 42.15, "min": 26.0}, "VIN": {"avg": 72.74, "peak": 112.77, "min": 59.14}, "VDD_CPU_SOC_MSS": {"avg": 15.69, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 31.59, "energy_joules_est": 144.35, "sample_count": 36, "duration_seconds": 4.569}, "timestamp": "2026-01-16T16:37:47.737000"}
{"image_index": 415, "image_name": "000000047112.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047112.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 4803.26, "latencies_ms": [4803.26], "images_per_second": 0.208, "prompt_tokens": 21, "response_tokens_est": 97, "n_tiles": 12, "output_text": "The image depicts a cozy dining setting in a restaurant, with a table set for two. The table is covered with a white tablecloth, and there is a large, freshly baked pizza placed on a white plate. The pizza is topped with melted cheese, sliced mushrooms, and pieces of ham, and it appears to be freshly baked. In the background, there are other tables and chairs, and a bar with various bottles and glasses, suggesting a lively and inviting atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25592.4, "ram_available_mb": 100179.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 25592.6, "ram_available_mb": 100179.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.1, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 31.34, "peak": 42.54, "min": 26.0}, "VIN": {"avg": 72.03, "peak": 113.56, "min": 58.26}, "VDD_CPU_SOC_MSS": {"avg": 15.71, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 31.34, "energy_joules_est": 150.55, "sample_count": 37, "duration_seconds": 4.804}, "timestamp": "2026-01-16T16:37:52.547003"}
{"image_index": 415, "image_name": "000000047112.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047112.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3887.663, "latencies_ms": [3887.663], "images_per_second": 0.257, "prompt_tokens": 19, "response_tokens_est": 70, "n_tiles": 12, "output_text": "The image features a large, golden-brown pizza with melted cheese and various toppings, including mushrooms and possibly ham, resting on a white plate. The lighting is warm and inviting, casting a soft glow on the pizza and creating a cozy atmosphere. The background includes a dimly lit bar with bottles and glasses, adding to the ambiance.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25592.6, "ram_available_mb": 100179.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25592.6, "ram_available_mb": 100179.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.09, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 32.66, "peak": 42.54, "min": 26.39}, "VIN": {"avg": 75.52, "peak": 121.71, "min": 58.97}, "VDD_CPU_SOC_MSS": {"avg": 15.72, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 32.66, "energy_joules_est": 126.99, "sample_count": 30, "duration_seconds": 3.888}, "timestamp": "2026-01-16T16:37:56.445901"}
{"image_index": 416, "image_name": "000000047121.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047121.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2389.237, "latencies_ms": [2389.237], "images_per_second": 0.419, "prompt_tokens": 9, "response_tokens_est": 29, "n_tiles": 12, "output_text": "A black cat is resting on the edge of a white bathtub, with a bottle of shampoo and a soap dispenser nearby.", "error": null, "sys_before": {"cpu_percent": 45.1, "ram_used_mb": 25592.6, "ram_available_mb": 100179.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 25592.3, "ram_available_mb": 100179.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.09, "peak": 16.15, "min": 13.59}, "VDD_GPU": {"avg": 35.28, "peak": 41.36, "min": 27.97}, "VIN": {"avg": 77.68, "peak": 115.65, "min": 59.3}, "VDD_CPU_SOC_MSS": {"avg": 15.99, "peak": 17.32, "min": 14.57}}, "power_watts_avg": 35.28, "energy_joules_est": 84.3, "sample_count": 18, "duration_seconds": 2.39}, "timestamp": "2026-01-16T16:37:58.935648"}
{"image_index": 416, "image_name": "000000047121.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047121.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 3107.099, "latencies_ms": [3107.099], "images_per_second": 0.322, "prompt_tokens": 23, "response_tokens_est": 46, "n_tiles": 12, "output_text": "1. Cat\n2. Bathtub\n3. Showerhead\n4. Toilet paper\n5. Toilet brush\n6. Toilet paper roll\n7. Toilet paper\n8. Toilet brush", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25592.3, "ram_available_mb": 100179.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 25592.3, "ram_available_mb": 100179.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.87, "peak": 15.54, "min": 14.32}, "VDD_GPU": {"avg": 34.33, "peak": 43.33, "min": 26.0}, "VIN": {"avg": 73.84, "peak": 107.88, "min": 58.18}, "VDD_CPU_SOC_MSS": {"avg": 15.42, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 34.33, "energy_joules_est": 106.68, "sample_count": 24, "duration_seconds": 3.108}, "timestamp": "2026-01-16T16:38:02.049157"}
{"image_index": 416, "image_name": "000000047121.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047121.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 4854.252, "latencies_ms": [4854.252], "images_per_second": 0.206, "prompt_tokens": 27, "response_tokens_est": 97, "n_tiles": 12, "output_text": "The main object in the foreground is a black cat, which is perched on the edge of a white bathtub. The cat is positioned near the edge of the tub, with its head resting on the edge and its body leaning slightly towards the water. To the left of the cat, there is a white soap dispenser and a small white dish on the countertop. The background features a white wall and a white bathtub, creating a clean and simple setting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25592.3, "ram_available_mb": 100179.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 25592.3, "ram_available_mb": 100179.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.74, "peak": 15.34, "min": 14.12}, "VDD_GPU": {"avg": 31.03, "peak": 42.54, "min": 25.6}, "VIN": {"avg": 69.67, "peak": 114.23, "min": 56.81}, "VDD_CPU_SOC_MSS": {"avg": 15.17, "peak": 16.14, "min": 14.56}}, "power_watts_avg": 31.03, "energy_joules_est": 150.64, "sample_count": 38, "duration_seconds": 4.855}, "timestamp": "2026-01-16T16:38:06.911035"}
{"image_index": 416, "image_name": "000000047121.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047121.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3268.765, "latencies_ms": [3268.765], "images_per_second": 0.306, "prompt_tokens": 21, "response_tokens_est": 51, "n_tiles": 12, "output_text": "The image depicts a bathroom setting with a white sink and a black cat resting on the sink. The cat appears to be sniffing or exploring the sink, while a bottle of shampoo and a soap dispenser are placed on the counter nearby.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25592.3, "ram_available_mb": 100179.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 25592.3, "ram_available_mb": 100179.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.76, "peak": 15.34, "min": 14.12}, "VDD_GPU": {"avg": 33.65, "peak": 42.54, "min": 26.0}, "VIN": {"avg": 77.73, "peak": 128.67, "min": 56.91}, "VDD_CPU_SOC_MSS": {"avg": 15.29, "peak": 16.14, "min": 14.56}}, "power_watts_avg": 33.65, "energy_joules_est": 110.01, "sample_count": 25, "duration_seconds": 3.269}, "timestamp": "2026-01-16T16:38:10.186340"}
{"image_index": 416, "image_name": "000000047121.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047121.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3469.42, "latencies_ms": [3469.42], "images_per_second": 0.288, "prompt_tokens": 19, "response_tokens_est": 57, "n_tiles": 12, "output_text": "The image features a black cat resting on a white bathtub. The bathtub is situated against a light-colored wall, and the cat is positioned near a silver faucet and a clear glass. The lighting in the room is soft and warm, creating a cozy atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25592.3, "ram_available_mb": 100179.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25592.3, "ram_available_mb": 100179.8, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.74, "peak": 15.34, "min": 14.12}, "VDD_GPU": {"avg": 33.25, "peak": 42.53, "min": 26.0}, "VIN": {"avg": 76.53, "peak": 122.28, "min": 56.17}, "VDD_CPU_SOC_MSS": {"avg": 15.29, "peak": 16.14, "min": 14.57}}, "power_watts_avg": 33.25, "energy_joules_est": 115.39, "sample_count": 27, "duration_seconds": 3.47}, "timestamp": "2026-01-16T16:38:13.662585"}
{"image_index": 417, "image_name": "000000047571.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047571.jpg", "image_width": 639, "image_height": 640, "image_resolution": "639x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1340.423, "latencies_ms": [1340.423], "images_per_second": 0.746, "prompt_tokens": 9, "response_tokens_est": 39, "n_tiles": 4, "output_text": "A horse-drawn cart is seen in a muddy field, with a woman and a man seated inside, both wearing hats and boots, and the horse is equipped with a harness.", "error": null, "sys_before": {"cpu_percent": 34.7, "ram_used_mb": 25592.3, "ram_available_mb": 100179.8, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 25592.3, "ram_available_mb": 100179.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4606.7, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5444.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.37, "peak": 15.11, "min": 13.51}, "VDD_GPU": {"avg": 28.64, "peak": 34.66, "min": 23.64}, "VIN": {"avg": 62.24, "peak": 65.43, "min": 47.91}, "VDD_CPU_SOC_MSS": {"avg": 15.0, "peak": 15.75, "min": 14.17}}, "power_watts_avg": 28.64, "energy_joules_est": 38.4, "sample_count": 10, "duration_seconds": 1.341}, "timestamp": "2026-01-16T16:38:15.060755"}
{"image_index": 417, "image_name": "000000047571.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047571.jpg", "image_width": 639, "image_height": 640, "image_resolution": "639x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1194.383, "latencies_ms": [1194.383], "images_per_second": 0.837, "prompt_tokens": 23, "response_tokens_est": 33, "n_tiles": 4, "output_text": "1. Carriage\n2. Horse\n3. Person\n4. Person\n5. Person\n6. Person\n7. Person\n8. Person", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25592.3, "ram_available_mb": 100179.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 25592.3, "ram_available_mb": 100179.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4606.7, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5455.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.65, "peak": 15.11, "min": 14.02}, "VDD_GPU": {"avg": 29.67, "peak": 35.45, "min": 24.42}, "VIN": {"avg": 67.85, "peak": 98.42, "min": 59.43}, "VDD_CPU_SOC_MSS": {"avg": 15.31, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 29.67, "energy_joules_est": 35.45, "sample_count": 9, "duration_seconds": 1.195}, "timestamp": "2026-01-16T16:38:16.261240"}
{"image_index": 417, "image_name": "000000047571.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047571.jpg", "image_width": 639, "image_height": 640, "image_resolution": "639x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1909.314, "latencies_ms": [1909.314], "images_per_second": 0.524, "prompt_tokens": 27, "response_tokens_est": 59, "n_tiles": 4, "output_text": "The main objects in the image are a horse and a cart. The horse is in the foreground, moving towards the right side of the image. The cart is positioned near the horse, with its wheels and the horse's harness visible. The background features a building and a clear blue sky.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25592.3, "ram_available_mb": 100179.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 25592.3, "ram_available_mb": 100179.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4606.7, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5458.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.78, "peak": 15.22, "min": 14.02}, "VDD_GPU": {"avg": 26.92, "peak": 36.24, "min": 22.07}, "VIN": {"avg": 66.18, "peak": 89.57, "min": 61.46}, "VDD_CPU_SOC_MSS": {"avg": 15.46, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 26.92, "energy_joules_est": 51.41, "sample_count": 15, "duration_seconds": 1.91}, "timestamp": "2026-01-16T16:38:18.176507"}
{"image_index": 417, "image_name": "000000047571.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047571.jpg", "image_width": 639, "image_height": 640, "image_resolution": "639x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1700.137, "latencies_ms": [1700.137], "images_per_second": 0.588, "prompt_tokens": 21, "response_tokens_est": 51, "n_tiles": 4, "output_text": "The image depicts a scene in a rural setting with a horse-drawn cart and a person riding it. The cart is in a muddy area with water reflecting the surroundings, and the person is wearing a hat and a plaid shirt.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25592.3, "ram_available_mb": 100179.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 25592.6, "ram_available_mb": 100179.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4606.7, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5453.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.73, "peak": 15.11, "min": 14.0}, "VDD_GPU": {"avg": 27.06, "peak": 34.66, "min": 22.46}, "VIN": {"avg": 66.57, "peak": 107.65, "min": 53.69}, "VDD_CPU_SOC_MSS": {"avg": 15.38, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 27.06, "energy_joules_est": 46.02, "sample_count": 13, "duration_seconds": 1.701}, "timestamp": "2026-01-16T16:38:19.882749"}
{"image_index": 417, "image_name": "000000047571.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047571.jpg", "image_width": 639, "image_height": 640, "image_resolution": "639x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1643.567, "latencies_ms": [1643.567], "images_per_second": 0.608, "prompt_tokens": 19, "response_tokens_est": 49, "n_tiles": 4, "output_text": "The image features a horse-drawn cart with a dark brown horse and a woman in a plaid shirt and hat. The scene is set on a muddy, wet ground with a clear blue sky overhead, indicating a sunny day.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25592.6, "ram_available_mb": 100179.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 25592.6, "ram_available_mb": 100179.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4606.7, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5451.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.71, "peak": 15.11, "min": 14.02}, "VDD_GPU": {"avg": 27.55, "peak": 35.06, "min": 22.85}, "VIN": {"avg": 68.15, "peak": 101.05, "min": 62.41}, "VDD_CPU_SOC_MSS": {"avg": 15.42, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 27.55, "energy_joules_est": 45.3, "sample_count": 12, "duration_seconds": 1.644}, "timestamp": "2026-01-16T16:38:21.533439"}
{"image_index": 418, "image_name": "000000047585.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047585.jpg", "image_width": 424, "image_height": 640, "image_resolution": "424x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1417.905, "latencies_ms": [1417.905], "images_per_second": 0.705, "prompt_tokens": 9, "response_tokens_est": 31, "n_tiles": 6, "output_text": "A bride and groom are walking down the aisle in a wedding ceremony, with a man in a suit holding an umbrella to shield them from the rain.", "error": null, "sys_before": {"cpu_percent": 47.0, "ram_used_mb": 25592.6, "ram_available_mb": 100179.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25592.6, "ram_available_mb": 100179.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.43, "peak": 15.01, "min": 13.82}, "VDD_GPU": {"avg": 31.05, "peak": 36.63, "min": 25.6}, "VIN": {"avg": 75.5, "peak": 95.1, "min": 63.13}, "VDD_CPU_SOC_MSS": {"avg": 15.21, "peak": 15.75, "min": 14.57}}, "power_watts_avg": 31.05, "energy_joules_est": 44.04, "sample_count": 11, "duration_seconds": 1.418}, "timestamp": "2026-01-16T16:38:23.011014"}
{"image_index": 418, "image_name": "000000047585.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047585.jpg", "image_width": 424, "image_height": 640, "image_resolution": "424x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1735.033, "latencies_ms": [1735.033], "images_per_second": 0.576, "prompt_tokens": 23, "response_tokens_est": 40, "n_tiles": 6, "output_text": "object: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25592.6, "ram_available_mb": 100179.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 25591.9, "ram_available_mb": 100180.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.66, "peak": 14.91, "min": 14.12}, "VDD_GPU": {"avg": 31.33, "peak": 38.98, "min": 24.43}, "VIN": {"avg": 74.68, "peak": 105.46, "min": 62.42}, "VDD_CPU_SOC_MSS": {"avg": 15.23, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 31.33, "energy_joules_est": 54.38, "sample_count": 13, "duration_seconds": 1.736}, "timestamp": "2026-01-16T16:38:24.752872"}
{"image_index": 418, "image_name": "000000047585.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047585.jpg", "image_width": 424, "image_height": 640, "image_resolution": "424x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2272.215, "latencies_ms": [2272.215], "images_per_second": 0.44, "prompt_tokens": 27, "response_tokens_est": 59, "n_tiles": 6, "output_text": "The main objects in the image are a bride and groom, standing in the foreground. The groom is holding an umbrella, and the bride is holding a bouquet of flowers. The background features a stone building and other guests, with the bride and groom positioned slightly off-center to the right.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25591.9, "ram_available_mb": 100180.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 25591.9, "ram_available_mb": 100180.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.72, "peak": 15.11, "min": 14.02}, "VDD_GPU": {"avg": 29.24, "peak": 38.21, "min": 23.64}, "VIN": {"avg": 67.51, "peak": 82.02, "min": 62.73}, "VDD_CPU_SOC_MSS": {"avg": 15.31, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 29.24, "energy_joules_est": 66.45, "sample_count": 17, "duration_seconds": 2.273}, "timestamp": "2026-01-16T16:38:27.031588"}
{"image_index": 418, "image_name": "000000047585.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047585.jpg", "image_width": 424, "image_height": 640, "image_resolution": "424x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2574.838, "latencies_ms": [2574.838], "images_per_second": 0.388, "prompt_tokens": 21, "response_tokens_est": 70, "n_tiles": 6, "output_text": "The scene depicts a wedding ceremony taking place outdoors on a grassy lawn. The bride, dressed in a white gown, is holding a bouquet of flowers, while the groom stands beside her, dressed in a black suit. The setting includes a stone building in the background, and the atmosphere appears to be joyful and celebratory.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25591.9, "ram_available_mb": 100180.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 25592.1, "ram_available_mb": 100180.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.79, "peak": 15.11, "min": 14.02}, "VDD_GPU": {"avg": 28.34, "peak": 38.21, "min": 23.24}, "VIN": {"avg": 68.13, "peak": 103.91, "min": 60.88}, "VDD_CPU_SOC_MSS": {"avg": 15.29, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 28.34, "energy_joules_est": 72.98, "sample_count": 20, "duration_seconds": 2.575}, "timestamp": "2026-01-16T16:38:29.612734"}
{"image_index": 418, "image_name": "000000047585.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047585.jpg", "image_width": 424, "image_height": 640, "image_resolution": "424x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1823.835, "latencies_ms": [1823.835], "images_per_second": 0.548, "prompt_tokens": 19, "response_tokens_est": 43, "n_tiles": 6, "output_text": "The image features a bride and groom standing under a black and white striped umbrella, holding a bouquet of flowers. The setting appears to be outdoors during the daytime, with natural lighting illuminating the scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25592.1, "ram_available_mb": 100180.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 25591.9, "ram_available_mb": 100180.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.67, "peak": 15.01, "min": 14.02}, "VDD_GPU": {"avg": 30.47, "peak": 38.19, "min": 24.04}, "VIN": {"avg": 73.2, "peak": 118.93, "min": 61.0}, "VDD_CPU_SOC_MSS": {"avg": 15.19, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 30.47, "energy_joules_est": 55.58, "sample_count": 14, "duration_seconds": 1.824}, "timestamp": "2026-01-16T16:38:31.442287"}
{"image_index": 419, "image_name": "000000047740.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047740.jpg", "image_width": 640, "image_height": 359, "image_resolution": "640x359", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1136.819, "latencies_ms": [1136.819], "images_per_second": 0.88, "prompt_tokens": 9, "response_tokens_est": 43, "n_tiles": 2, "output_text": "Two individuals are relaxing on a sandy beach, with one lying on their back and the other sitting on the sand, both enjoying the tranquil atmosphere of the ocean and the colorful kite in the sky.", "error": null, "sys_before": {"cpu_percent": 34.3, "ram_used_mb": 25591.9, "ram_available_mb": 100180.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25591.9, "ram_available_mb": 100180.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5055.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.87, "peak": 15.62, "min": 14.1}, "VDD_GPU": {"avg": 25.95, "peak": 33.09, "min": 22.06}, "VIN": {"avg": 63.96, "peak": 75.7, "min": 61.11}, "VDD_CPU_SOC_MSS": {"avg": 15.25, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 25.95, "energy_joules_est": 29.51, "sample_count": 8, "duration_seconds": 1.137}, "timestamp": "2026-01-16T16:38:32.606159"}
{"image_index": 419, "image_name": "000000047740.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047740.jpg", "image_width": 640, "image_height": 359, "image_resolution": "640x359", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1186.225, "latencies_ms": [1186.225], "images_per_second": 0.843, "prompt_tokens": 23, "response_tokens_est": 45, "n_tiles": 2, "output_text": "beach: 1\nsand: 1\nperson: 2\nshorts: 1\nshirt: 1\nwatch: 1\nkite: 1\nballoon: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25591.9, "ram_available_mb": 100180.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25591.9, "ram_available_mb": 100180.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5064.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.11, "peak": 15.42, "min": 14.71}, "VDD_GPU": {"avg": 25.56, "peak": 31.5, "min": 21.67}, "VIN": {"avg": 70.14, "peak": 98.82, "min": 62.72}, "VDD_CPU_SOC_MSS": {"avg": 15.88, "peak": 16.54, "min": 15.35}}, "power_watts_avg": 25.56, "energy_joules_est": 30.34, "sample_count": 9, "duration_seconds": 1.187}, "timestamp": "2026-01-16T16:38:33.798908"}
{"image_index": 419, "image_name": "000000047740.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047740.jpg", "image_width": 640, "image_height": 359, "image_resolution": "640x359", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1755.498, "latencies_ms": [1755.498], "images_per_second": 0.57, "prompt_tokens": 27, "response_tokens_est": 70, "n_tiles": 2, "output_text": "The main objects in the image are a person lying on the sandy beach, a colorful kite in the foreground, and another person in the background. The person on the beach is positioned in the foreground, while the kite is in the foreground as well. The person in the background is further away, indicating a depth of field effect.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25591.9, "ram_available_mb": 100180.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25592.8, "ram_available_mb": 100179.3, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5067.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.32, "peak": 15.82, "min": 14.61}, "VDD_GPU": {"avg": 24.18, "peak": 31.91, "min": 21.27}, "VIN": {"avg": 66.52, "peak": 102.27, "min": 61.51}, "VDD_CPU_SOC_MSS": {"avg": 15.93, "peak": 16.14, "min": 15.35}}, "power_watts_avg": 24.18, "energy_joules_est": 42.45, "sample_count": 13, "duration_seconds": 1.756}, "timestamp": "2026-01-16T16:38:35.560289"}
{"image_index": 419, "image_name": "000000047740.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047740.jpg", "image_width": 640, "image_height": 359, "image_resolution": "640x359", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1733.274, "latencies_ms": [1733.274], "images_per_second": 0.577, "prompt_tokens": 21, "response_tokens_est": 69, "n_tiles": 2, "output_text": "The image depicts a sunny beach scene with two individuals lying on the sand, enjoying the beach. One person is lying on their back with their legs extended, while the other is lying on their stomach with their legs bent at the knees. A colorful kite is visible in the background, adding to the leisurely atmosphere of the beach.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25592.8, "ram_available_mb": 100179.3, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25593.1, "ram_available_mb": 100179.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5063.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.33, "peak": 15.72, "min": 14.71}, "VDD_GPU": {"avg": 23.91, "peak": 31.51, "min": 20.89}, "VIN": {"avg": 66.45, "peak": 100.42, "min": 59.85}, "VDD_CPU_SOC_MSS": {"avg": 15.75, "peak": 16.14, "min": 14.96}}, "power_watts_avg": 23.91, "energy_joules_est": 41.45, "sample_count": 13, "duration_seconds": 1.734}, "timestamp": "2026-01-16T16:38:37.299406"}
{"image_index": 419, "image_name": "000000047740.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047740.jpg", "image_width": 640, "image_height": 359, "image_resolution": "640x359", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1196.172, "latencies_ms": [1196.172], "images_per_second": 0.836, "prompt_tokens": 19, "response_tokens_est": 45, "n_tiles": 2, "output_text": "The image depicts a sunny beach scene with sandy terrain, clear blue skies, and gentle waves. The beach is adorned with colorful beach balls and a vibrant kite, creating a lively and cheerful atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25593.1, "ram_available_mb": 100179.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25593.1, "ram_available_mb": 100179.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5062.6, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.04, "peak": 15.32, "min": 14.61}, "VDD_GPU": {"avg": 25.07, "peak": 30.73, "min": 22.07}, "VIN": {"avg": 62.65, "peak": 64.48, "min": 58.37}, "VDD_CPU_SOC_MSS": {"avg": 15.7, "peak": 16.14, "min": 15.35}}, "power_watts_avg": 25.07, "energy_joules_est": 30.0, "sample_count": 8, "duration_seconds": 1.196}, "timestamp": "2026-01-16T16:38:38.501696"}
{"image_index": 420, "image_name": "000000047769.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047769.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2653.422, "latencies_ms": [2653.422], "images_per_second": 0.377, "prompt_tokens": 9, "response_tokens_est": 37, "n_tiles": 12, "output_text": "The image shows a cozy living room with a brown sofa, a red armchair, a black stool, a small table with a lamp, and a television on a stand.", "error": null, "sys_before": {"cpu_percent": 50.6, "ram_used_mb": 25593.1, "ram_available_mb": 100179.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 25593.1, "ram_available_mb": 100179.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.21, "peak": 16.15, "min": 14.3}, "VDD_GPU": {"avg": 34.19, "peak": 40.57, "min": 26.79}, "VIN": {"avg": 76.17, "peak": 113.82, "min": 60.18}, "VDD_CPU_SOC_MSS": {"avg": 15.94, "peak": 17.32, "min": 15.35}}, "power_watts_avg": 34.19, "energy_joules_est": 90.73, "sample_count": 20, "duration_seconds": 2.654}, "timestamp": "2026-01-16T16:38:41.219810"}
{"image_index": 420, "image_name": "000000047769.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047769.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2946.849, "latencies_ms": [2946.849], "images_per_second": 0.339, "prompt_tokens": 23, "response_tokens_est": 42, "n_tiles": 12, "output_text": "1. TV stand\n2. TV\n3. Coffee table\n4. Red sofa\n5. Black stools\n6. White lamp\n7. White lampshade\n8. White curtains", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25593.1, "ram_available_mb": 100179.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 25593.1, "ram_available_mb": 100179.1, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.1, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 34.73, "peak": 42.15, "min": 26.79}, "VIN": {"avg": 73.62, "peak": 116.39, "min": 57.9}, "VDD_CPU_SOC_MSS": {"avg": 15.67, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 34.73, "energy_joules_est": 102.36, "sample_count": 22, "duration_seconds": 2.947}, "timestamp": "2026-01-16T16:38:44.177330"}
{"image_index": 420, "image_name": "000000047769.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047769.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 5487.535, "latencies_ms": [5487.535], "images_per_second": 0.182, "prompt_tokens": 27, "response_tokens_est": 117, "n_tiles": 12, "output_text": "The main objects in the image include a brown sofa, a red chair, a black stool, a small table, a lamp, a television, and a window with blinds. The sofa is positioned in the foreground, while the red chair is slightly to the right. The black stool is placed in front of the red chair. The small table is situated between the sofa and the stool. The lamp is positioned on the table, and the television is placed on the right side of the image. The window with blinds is located behind the sofa and the television.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25593.1, "ram_available_mb": 100179.1, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 25592.8, "ram_available_mb": 100179.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.11, "peak": 15.65, "min": 14.43}, "VDD_GPU": {"avg": 30.69, "peak": 42.54, "min": 26.0}, "VIN": {"avg": 76.76, "peak": 118.9, "min": 60.02}, "VDD_CPU_SOC_MSS": {"avg": 15.65, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 30.69, "energy_joules_est": 168.42, "sample_count": 43, "duration_seconds": 5.488}, "timestamp": "2026-01-16T16:38:49.671560"}
{"image_index": 420, "image_name": "000000047769.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047769.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 4260.173, "latencies_ms": [4260.173], "images_per_second": 0.235, "prompt_tokens": 21, "response_tokens_est": 81, "n_tiles": 12, "output_text": "The image depicts a cozy living room with a warm, inviting ambiance. The room features a brown leather sofa, a black wooden stool, a red armchair, a small table with a lamp, and a television on a stand. The room is well-lit, with natural light coming through the window, and the overall setting suggests a comfortable and relaxed space for relaxation and socializing.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25592.8, "ram_available_mb": 100179.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25592.8, "ram_available_mb": 100179.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.1, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 31.94, "peak": 42.15, "min": 26.0}, "VIN": {"avg": 71.86, "peak": 115.98, "min": 58.34}, "VDD_CPU_SOC_MSS": {"avg": 15.63, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 31.94, "energy_joules_est": 136.08, "sample_count": 33, "duration_seconds": 4.261}, "timestamp": "2026-01-16T16:38:53.938279"}
{"image_index": 420, "image_name": "000000047769.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047769.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3010.813, "latencies_ms": [3010.813], "images_per_second": 0.332, "prompt_tokens": 19, "response_tokens_est": 44, "n_tiles": 12, "output_text": "The room is warmly lit with a soft yellow glow from the wall-mounted lamp, creating a cozy atmosphere. The wooden window shutters and the wooden floor add a natural, earthy feel to the space.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25592.8, "ram_available_mb": 100179.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 25592.6, "ram_available_mb": 100179.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.09, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 34.44, "peak": 42.54, "min": 26.79}, "VIN": {"avg": 76.57, "peak": 118.59, "min": 58.61}, "VDD_CPU_SOC_MSS": {"avg": 15.64, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 34.44, "energy_joules_est": 103.72, "sample_count": 23, "duration_seconds": 3.012}, "timestamp": "2026-01-16T16:38:56.956004"}
{"image_index": 421, "image_name": "000000047801.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047801.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 668.086, "latencies_ms": [668.086], "images_per_second": 1.497, "prompt_tokens": 9, "response_tokens_est": 27, "n_tiles": 1, "output_text": "A man is eating a slice of cake outdoors, holding a fork and a plate, with trees and greenery in the background.", "error": null, "sys_before": {"cpu_percent": 23.3, "ram_used_mb": 25592.6, "ram_available_mb": 100179.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25592.8, "ram_available_mb": 100179.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 4845.7, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.06, "peak": 15.32, "min": 14.71}, "VDD_GPU": {"avg": 24.52, "peak": 26.79, "min": 22.85}, "VIN": {"avg": 64.21, "peak": 66.83, "min": 60.68}, "VDD_CPU_SOC_MSS": {"avg": 15.55, "peak": 16.14, "min": 14.96}}, "power_watts_avg": 24.52, "energy_joules_est": 16.4, "sample_count": 4, "duration_seconds": 0.669}, "timestamp": "2026-01-16T16:38:57.650379"}
{"image_index": 421, "image_name": "000000047801.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047801.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1032.155, "latencies_ms": [1032.155], "images_per_second": 0.969, "prompt_tokens": 23, "response_tokens_est": 45, "n_tiles": 1, "output_text": "object: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25592.8, "ram_available_mb": 100179.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25592.8, "ram_available_mb": 100179.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 4856.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.46, "peak": 15.62, "min": 15.11}, "VDD_GPU": {"avg": 22.57, "peak": 25.22, "min": 20.88}, "VIN": {"avg": 63.68, "peak": 65.7, "min": 61.29}, "VDD_CPU_SOC_MSS": {"avg": 16.31, "peak": 16.54, "min": 15.74}}, "power_watts_avg": 22.57, "energy_joules_est": 23.31, "sample_count": 7, "duration_seconds": 1.033}, "timestamp": "2026-01-16T16:38:58.688622"}
{"image_index": 421, "image_name": "000000047801.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047801.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1009.491, "latencies_ms": [1009.491], "images_per_second": 0.991, "prompt_tokens": 27, "response_tokens_est": 44, "n_tiles": 1, "output_text": "The main object in the foreground is a man holding a plate with a slice of cake. He is wearing glasses and a blue t-shirt. The background features a lush green park with trees and a clear blue sky.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25592.8, "ram_available_mb": 100179.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25592.8, "ram_available_mb": 100179.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 4859.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.61, "peak": 15.72, "min": 15.32}, "VDD_GPU": {"avg": 22.23, "peak": 24.82, "min": 20.88}, "VIN": {"avg": 61.35, "peak": 65.25, "min": 54.98}, "VDD_CPU_SOC_MSS": {"avg": 16.42, "peak": 16.54, "min": 16.14}}, "power_watts_avg": 22.23, "energy_joules_est": 22.45, "sample_count": 7, "duration_seconds": 1.01}, "timestamp": "2026-01-16T16:38:59.703567"}
{"image_index": 421, "image_name": "000000047801.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047801.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1249.631, "latencies_ms": [1249.631], "images_per_second": 0.8, "prompt_tokens": 21, "response_tokens_est": 54, "n_tiles": 1, "output_text": "The image depicts a man sitting outdoors, enjoying a slice of cake. He is holding a white plate with a slice of cake and a spoon, and is wearing glasses. The setting appears to be a park or garden with greenery and trees in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25592.8, "ram_available_mb": 100179.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 6.1, "ram_used_mb": 25592.6, "ram_available_mb": 100179.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 4854.7, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.59, "peak": 15.72, "min": 15.32}, "VDD_GPU": {"avg": 21.67, "peak": 24.41, "min": 20.1}, "VIN": {"avg": 62.82, "peak": 69.62, "min": 59.0}, "VDD_CPU_SOC_MSS": {"avg": 16.44, "peak": 16.54, "min": 16.14}}, "power_watts_avg": 21.67, "energy_joules_est": 27.09, "sample_count": 9, "duration_seconds": 1.25}, "timestamp": "2026-01-16T16:39:00.958899"}
{"image_index": 421, "image_name": "000000047801.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047801.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1163.671, "latencies_ms": [1163.671], "images_per_second": 0.859, "prompt_tokens": 19, "response_tokens_est": 50, "n_tiles": 1, "output_text": "The notable visual attributes of the image include a person with a bald head wearing glasses, a blue t-shirt, and holding a plate with a slice of cake. The background features lush green trees under a clear blue sky, indicating a sunny day.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25592.6, "ram_available_mb": 100179.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25592.6, "ram_available_mb": 100179.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 4853.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.47, "peak": 15.62, "min": 15.11}, "VDD_GPU": {"avg": 21.67, "peak": 24.43, "min": 20.1}, "VIN": {"avg": 64.23, "peak": 71.93, "min": 61.28}, "VDD_CPU_SOC_MSS": {"avg": 16.54, "peak": 16.93, "min": 16.14}}, "power_watts_avg": 21.67, "energy_joules_est": 25.23, "sample_count": 9, "duration_seconds": 1.164}, "timestamp": "2026-01-16T16:39:02.128929"}
{"image_index": 422, "image_name": "000000047819.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047819.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2048.991, "latencies_ms": [2048.991], "images_per_second": 0.488, "prompt_tokens": 9, "response_tokens_est": 19, "n_tiles": 12, "output_text": "A man is standing next to a brown horse, holding a basket filled with various items.", "error": null, "sys_before": {"cpu_percent": 46.2, "ram_used_mb": 25592.6, "ram_available_mb": 100179.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 2.4, "ram_used_mb": 25592.8, "ram_available_mb": 100179.4, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.3, "peak": 16.15, "min": 14.51}, "VDD_GPU": {"avg": 35.7, "peak": 40.18, "min": 28.36}, "VIN": {"avg": 75.59, "peak": 114.86, "min": 58.58}, "VDD_CPU_SOC_MSS": {"avg": 16.31, "peak": 17.32, "min": 15.36}}, "power_watts_avg": 35.7, "energy_joules_est": 73.16, "sample_count": 16, "duration_seconds": 2.049}, "timestamp": "2026-01-16T16:39:04.240376"}
{"image_index": 422, "image_name": "000000047819.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047819.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2803.576, "latencies_ms": [2803.576], "images_per_second": 0.357, "prompt_tokens": 23, "response_tokens_est": 38, "n_tiles": 12, "output_text": "1. Man\n2. Shelter\n3. Tree\n4. Shelter\n5. Shelter\n6. Shelter\n7. Shelter\n8. Shelter", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25592.8, "ram_available_mb": 100179.4, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 25592.5, "ram_available_mb": 100179.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.1, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 35.47, "peak": 42.94, "min": 27.18}, "VIN": {"avg": 74.08, "peak": 110.01, "min": 58.88}, "VDD_CPU_SOC_MSS": {"avg": 15.65, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 35.47, "energy_joules_est": 99.45, "sample_count": 21, "duration_seconds": 2.804}, "timestamp": "2026-01-16T16:39:07.050239"}
{"image_index": 422, "image_name": "000000047819.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047819.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2882.586, "latencies_ms": [2882.586], "images_per_second": 0.347, "prompt_tokens": 27, "response_tokens_est": 40, "n_tiles": 12, "output_text": "The main objects in the image are a man and a brown horse. The man is positioned in the foreground, standing near the horse. The horse is in the background, slightly to the right.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25592.5, "ram_available_mb": 100179.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 25592.5, "ram_available_mb": 100179.6, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.07, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 35.02, "peak": 42.92, "min": 26.79}, "VIN": {"avg": 80.27, "peak": 124.81, "min": 58.49}, "VDD_CPU_SOC_MSS": {"avg": 15.66, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 35.02, "energy_joules_est": 100.97, "sample_count": 22, "duration_seconds": 2.883}, "timestamp": "2026-01-16T16:39:09.940304"}
{"image_index": 422, "image_name": "000000047819.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047819.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3718.752, "latencies_ms": [3718.752], "images_per_second": 0.269, "prompt_tokens": 21, "response_tokens_est": 65, "n_tiles": 12, "output_text": "The image depicts a man standing next to a brown horse in a natural, outdoor setting. The man is wearing a purple vest over a white shirt and gray pants, and he appears to be holding a basket filled with various items. The horse is adorned with a colorful blanket and is being led by the man.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25592.5, "ram_available_mb": 100179.6, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25592.3, "ram_available_mb": 100179.9, "ram_percent": 20.3}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.09, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 32.97, "peak": 43.33, "min": 26.39}, "VIN": {"avg": 72.38, "peak": 110.14, "min": 54.63}, "VDD_CPU_SOC_MSS": {"avg": 15.68, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 32.97, "energy_joules_est": 122.62, "sample_count": 29, "duration_seconds": 3.719}, "timestamp": "2026-01-16T16:39:13.665421"}
{"image_index": 422, "image_name": "000000047819.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047819.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3110.165, "latencies_ms": [3110.165], "images_per_second": 0.322, "prompt_tokens": 19, "response_tokens_est": 47, "n_tiles": 12, "output_text": "The man is wearing a purple vest over a white shirt, gray pants, and brown shoes. The sunlight casts a warm glow on the scene, highlighting the textures of the man's clothing and the earthy tones of the surroundings.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25592.3, "ram_available_mb": 100179.9, "ram_percent": 20.3}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 25596.0, "ram_available_mb": 100176.2, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.1, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 34.04, "peak": 42.15, "min": 26.79}, "VIN": {"avg": 76.34, "peak": 121.38, "min": 58.39}, "VDD_CPU_SOC_MSS": {"avg": 15.75, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 34.04, "energy_joules_est": 105.89, "sample_count": 24, "duration_seconds": 3.111}, "timestamp": "2026-01-16T16:39:16.781990"}
{"image_index": 423, "image_name": "000000047828.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047828.jpg", "image_width": 640, "image_height": 318, "image_resolution": "640x318", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1922.715, "latencies_ms": [1922.715], "images_per_second": 0.52, "prompt_tokens": 9, "response_tokens_est": 78, "n_tiles": 2, "output_text": "The image depicts a night scene featuring a beautifully illuminated bridge with a series of arches, reflecting the blue lights on the water below. The bridge is adorned with a series of lights that create a mesmerizing and serene atmosphere, while a group of people can be seen walking along the bridge's walkway, adding a sense of scale and life to the scene.", "error": null, "sys_before": {"cpu_percent": 34.3, "ram_used_mb": 25596.0, "ram_available_mb": 100176.2, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25596.7, "ram_available_mb": 100175.5, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5055.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.15, "peak": 15.62, "min": 14.1}, "VDD_GPU": {"avg": 24.27, "peak": 34.27, "min": 20.89}, "VIN": {"avg": 67.83, "peak": 108.19, "min": 62.11}, "VDD_CPU_SOC_MSS": {"avg": 15.77, "peak": 16.54, "min": 14.56}}, "power_watts_avg": 24.27, "energy_joules_est": 46.68, "sample_count": 15, "duration_seconds": 1.923}, "timestamp": "2026-01-16T16:39:18.733861"}
{"image_index": 423, "image_name": "000000047828.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047828.jpg", "image_width": 640, "image_height": 318, "image_resolution": "640x318", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1183.891, "latencies_ms": [1183.891], "images_per_second": 0.845, "prompt_tokens": 23, "response_tokens_est": 45, "n_tiles": 2, "output_text": "object: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25596.7, "ram_available_mb": 100175.5, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25596.7, "ram_available_mb": 100175.5, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5064.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.22, "peak": 15.52, "min": 14.71}, "VDD_GPU": {"avg": 25.21, "peak": 31.51, "min": 21.67}, "VIN": {"avg": 65.87, "peak": 77.25, "min": 62.54}, "VDD_CPU_SOC_MSS": {"avg": 15.79, "peak": 16.14, "min": 15.36}}, "power_watts_avg": 25.21, "energy_joules_est": 29.85, "sample_count": 9, "duration_seconds": 1.184}, "timestamp": "2026-01-16T16:39:19.923552"}
{"image_index": 423, "image_name": "000000047828.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047828.jpg", "image_width": 640, "image_height": 318, "image_resolution": "640x318", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2314.561, "latencies_ms": [2314.561], "images_per_second": 0.432, "prompt_tokens": 27, "response_tokens_est": 95, "n_tiles": 2, "output_text": "The main objects in the image are a group of people on the left side, a large bridge in the background, and a body of water on the right side. The bridge is the most prominent object, extending across the water and illuminated with blue lights. The people are situated on the left side of the bridge, near the water's edge. The water is the central feature, reflecting the bridge and the lights, and the people are near the water's edge.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25596.7, "ram_available_mb": 100175.5, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25596.7, "ram_available_mb": 100175.5, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5067.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.41, "peak": 15.72, "min": 14.71}, "VDD_GPU": {"avg": 23.24, "peak": 31.12, "min": 20.88}, "VIN": {"avg": 66.66, "peak": 105.07, "min": 61.53}, "VDD_CPU_SOC_MSS": {"avg": 15.92, "peak": 16.54, "min": 14.96}}, "power_watts_avg": 23.24, "energy_joules_est": 53.8, "sample_count": 18, "duration_seconds": 2.315}, "timestamp": "2026-01-16T16:39:22.243708"}
{"image_index": 423, "image_name": "000000047828.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047828.jpg", "image_width": 640, "image_height": 318, "image_resolution": "640x318", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1471.924, "latencies_ms": [1471.924], "images_per_second": 0.679, "prompt_tokens": 21, "response_tokens_est": 58, "n_tiles": 2, "output_text": "The image depicts a nighttime scene of a large, illuminated bridge spanning a body of water. The bridge is adorned with blue lights, creating a striking reflection on the water's surface. People can be seen walking along the bridge's walkway, enjoying the scenic view.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25596.7, "ram_available_mb": 100175.5, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25596.5, "ram_available_mb": 100175.7, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5063.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.22, "peak": 15.52, "min": 14.81}, "VDD_GPU": {"avg": 24.46, "peak": 30.71, "min": 21.27}, "VIN": {"avg": 69.35, "peak": 102.33, "min": 59.51}, "VDD_CPU_SOC_MSS": {"avg": 15.96, "peak": 16.14, "min": 15.35}}, "power_watts_avg": 24.46, "energy_joules_est": 36.01, "sample_count": 11, "duration_seconds": 1.472}, "timestamp": "2026-01-16T16:39:23.721547"}
{"image_index": 423, "image_name": "000000047828.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000047828.jpg", "image_width": 640, "image_height": 318, "image_resolution": "640x318", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1688.465, "latencies_ms": [1688.465], "images_per_second": 0.592, "prompt_tokens": 19, "response_tokens_est": 67, "n_tiles": 2, "output_text": "The image showcases a nighttime scene featuring a strikingly illuminated bridge with a series of arches. The bridge is adorned with blue lights, creating a mesmerizing reflection on the calm water below. The sky is dark, indicating it is nighttime, and the overall atmosphere is serene and captivating.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25596.5, "ram_available_mb": 100175.7, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25596.5, "ram_available_mb": 100175.7, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5062.6, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.24, "peak": 15.62, "min": 14.61}, "VDD_GPU": {"avg": 24.0, "peak": 31.91, "min": 20.89}, "VIN": {"avg": 65.3, "peak": 75.9, "min": 61.55}, "VDD_CPU_SOC_MSS": {"avg": 15.96, "peak": 16.14, "min": 15.36}}, "power_watts_avg": 24.0, "energy_joules_est": 40.53, "sample_count": 13, "duration_seconds": 1.689}, "timestamp": "2026-01-16T16:39:25.416064"}
{"image_index": 424, "image_name": "000000048153.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000048153.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 3183.712, "latencies_ms": [3183.712], "images_per_second": 0.314, "prompt_tokens": 9, "response_tokens_est": 53, "n_tiles": 12, "output_text": "The image shows a close-up of a person's lower torso and legs, wearing blue denim jeans, with a pink, shiny object on their back, possibly a bag or a piece of clothing, and a blue and green painted bench in the background.", "error": null, "sys_before": {"cpu_percent": 47.2, "ram_used_mb": 25596.5, "ram_available_mb": 100175.7, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 25596.5, "ram_available_mb": 100175.7, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.19, "peak": 16.15, "min": 14.3}, "VDD_GPU": {"avg": 32.57, "peak": 40.97, "min": 26.39}, "VIN": {"avg": 78.0, "peak": 115.77, "min": 59.0}, "VDD_CPU_SOC_MSS": {"avg": 15.83, "peak": 17.32, "min": 15.35}}, "power_watts_avg": 32.57, "energy_joules_est": 103.72, "sample_count": 25, "duration_seconds": 3.184}, "timestamp": "2026-01-16T16:39:28.666788"}
{"image_index": 424, "image_name": "000000048153.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000048153.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2879.857, "latencies_ms": [2879.857], "images_per_second": 0.347, "prompt_tokens": 23, "response_tokens_est": 40, "n_tiles": 12, "output_text": "1. Pink chair\n2. Blue chair\n3. Blue blanket\n4. Blue bench\n5. Blue floor\n6. Green floor\n7. Pink object\n8. Pink object", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25596.5, "ram_available_mb": 100175.7, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 25596.4, "ram_available_mb": 100175.7, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.09, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 34.73, "peak": 42.13, "min": 26.79}, "VIN": {"avg": 78.16, "peak": 134.79, "min": 54.78}, "VDD_CPU_SOC_MSS": {"avg": 15.69, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 34.73, "energy_joules_est": 100.05, "sample_count": 22, "duration_seconds": 2.881}, "timestamp": "2026-01-16T16:39:31.553794"}
{"image_index": 424, "image_name": "000000048153.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000048153.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3562.946, "latencies_ms": [3562.946], "images_per_second": 0.281, "prompt_tokens": 27, "response_tokens_est": 61, "n_tiles": 12, "output_text": "The main object in the foreground is a blue denim garment, which appears to be a pair of jeans. The background features a colorful, painted surface with a map of Europe and parts of Africa visible. The pink object, likely a chair or bench, is situated near the blue denim garment.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25596.4, "ram_available_mb": 100175.7, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 25596.4, "ram_available_mb": 100175.7, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.13, "peak": 15.65, "min": 14.43}, "VDD_GPU": {"avg": 33.32, "peak": 42.92, "min": 26.39}, "VIN": {"avg": 74.22, "peak": 116.48, "min": 58.53}, "VDD_CPU_SOC_MSS": {"avg": 15.75, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 33.32, "energy_joules_est": 118.73, "sample_count": 28, "duration_seconds": 3.563}, "timestamp": "2026-01-16T16:39:35.123368"}
{"image_index": 424, "image_name": "000000048153.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000048153.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3829.577, "latencies_ms": [3829.577], "images_per_second": 0.261, "prompt_tokens": 21, "response_tokens_est": 69, "n_tiles": 12, "output_text": "The image depicts a scene with a person sitting on a bench, which is painted in a colorful, abstract design. The person is wearing a blue denim garment, and the bench has a vibrant blue and green pattern. The setting appears to be outdoors, possibly in a park or public area, with the bench being a focal point.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25596.4, "ram_available_mb": 100175.7, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 25596.7, "ram_available_mb": 100175.5, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.15, "peak": 15.65, "min": 14.63}, "VDD_GPU": {"avg": 32.76, "peak": 42.94, "min": 26.39}, "VIN": {"avg": 74.74, "peak": 113.53, "min": 53.79}, "VDD_CPU_SOC_MSS": {"avg": 15.77, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 32.76, "energy_joules_est": 125.47, "sample_count": 30, "duration_seconds": 3.83}, "timestamp": "2026-01-16T16:39:38.959200"}
{"image_index": 424, "image_name": "000000048153.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000048153.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3340.918, "latencies_ms": [3340.918], "images_per_second": 0.299, "prompt_tokens": 19, "response_tokens_est": 54, "n_tiles": 12, "output_text": "The image features a vividly colored bench with a blue and green painted surface. The bench is adorned with a bright pink cushion, adding a pop of color to the scene. The lighting is bright, casting clear shadows and highlighting the textures of the materials used.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25596.7, "ram_available_mb": 100175.5, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 25596.4, "ram_available_mb": 100175.7, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.11, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 33.59, "peak": 42.54, "min": 26.39}, "VIN": {"avg": 74.44, "peak": 119.52, "min": 58.59}, "VDD_CPU_SOC_MSS": {"avg": 15.75, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 33.59, "energy_joules_est": 112.23, "sample_count": 26, "duration_seconds": 3.341}, "timestamp": "2026-01-16T16:39:42.306403"}
{"image_index": 425, "image_name": "000000048396.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000048396.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1847.279, "latencies_ms": [1847.279], "images_per_second": 0.541, "prompt_tokens": 9, "response_tokens_est": 47, "n_tiles": 6, "output_text": "A woman with a smile is holding a knife, standing in front of a red wall, while a young boy with a cheerful expression is sitting at a table, with a red cake and a white cup in front of him.", "error": null, "sys_before": {"cpu_percent": 41.8, "ram_used_mb": 25596.4, "ram_available_mb": 100175.7, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 25596.4, "ram_available_mb": 100175.8, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.57, "peak": 15.22, "min": 13.82}, "VDD_GPU": {"avg": 30.36, "peak": 37.41, "min": 24.42}, "VIN": {"avg": 65.8, "peak": 81.49, "min": 53.46}, "VDD_CPU_SOC_MSS": {"avg": 15.3, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 30.36, "energy_joules_est": 56.1, "sample_count": 14, "duration_seconds": 1.848}, "timestamp": "2026-01-16T16:39:44.217939"}
{"image_index": 425, "image_name": "000000048396.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000048396.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2179.919, "latencies_ms": [2179.919], "images_per_second": 0.459, "prompt_tokens": 23, "response_tokens_est": 56, "n_tiles": 6, "output_text": "1. Woman: 1\n2. Knife: 1\n3. Woman: 1\n4. Woman: 1\n5. Woman: 1\n6. Woman: 1\n7. Woman: 1\n8. Woman: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25596.4, "ram_available_mb": 100175.8, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 25596.4, "ram_available_mb": 100175.8, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.78, "peak": 15.11, "min": 14.12}, "VDD_GPU": {"avg": 29.31, "peak": 38.59, "min": 23.64}, "VIN": {"avg": 72.71, "peak": 118.79, "min": 62.36}, "VDD_CPU_SOC_MSS": {"avg": 15.35, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 29.31, "energy_joules_est": 63.91, "sample_count": 17, "duration_seconds": 2.18}, "timestamp": "2026-01-16T16:39:46.404716"}
{"image_index": 425, "image_name": "000000048396.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000048396.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1905.508, "latencies_ms": [1905.508], "images_per_second": 0.525, "prompt_tokens": 27, "response_tokens_est": 46, "n_tiles": 6, "output_text": "The main objects in the image are a woman and a young boy. The woman is in the foreground, standing next to a table with a red cake. The young boy is in the background, standing near a doorway.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25596.4, "ram_available_mb": 100175.8, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 25596.7, "ram_available_mb": 100175.5, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.67, "peak": 15.01, "min": 14.12}, "VDD_GPU": {"avg": 30.44, "peak": 38.19, "min": 24.03}, "VIN": {"avg": 68.07, "peak": 100.41, "min": 60.11}, "VDD_CPU_SOC_MSS": {"avg": 15.21, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 30.44, "energy_joules_est": 58.02, "sample_count": 14, "duration_seconds": 1.906}, "timestamp": "2026-01-16T16:39:48.316743"}
{"image_index": 425, "image_name": "000000048396.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000048396.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3646.154, "latencies_ms": [3646.154], "images_per_second": 0.274, "prompt_tokens": 21, "response_tokens_est": 108, "n_tiles": 6, "output_text": "The scene depicts a birthday celebration taking place in a cozy, warmly lit room. A woman, dressed in a light green cardigan over a white top, is holding a knife, possibly in preparation for cutting a cake. Beside her, a young boy in a blue and white plaid shirt is smiling and looking towards the camera, with a white cup on the table in front of him. The cake, adorned with a red spiderman design, is placed on a table, indicating the occasion is a birthday party.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25596.7, "ram_available_mb": 100175.5, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 25596.4, "ram_available_mb": 100175.8, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.85, "peak": 15.11, "min": 14.02}, "VDD_GPU": {"avg": 26.75, "peak": 38.19, "min": 23.24}, "VIN": {"avg": 67.0, "peak": 94.84, "min": 57.88}, "VDD_CPU_SOC_MSS": {"avg": 15.48, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 26.75, "energy_joules_est": 97.55, "sample_count": 28, "duration_seconds": 3.647}, "timestamp": "2026-01-16T16:39:51.969042"}
{"image_index": 425, "image_name": "000000048396.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000048396.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2743.414, "latencies_ms": [2743.414], "images_per_second": 0.365, "prompt_tokens": 19, "response_tokens_est": 76, "n_tiles": 6, "output_text": "The image features a woman with long brown hair, wearing a light green cardigan over a white top, standing in front of a red wall. She is smiling and holding a knife in her right hand. The lighting is warm and soft, creating a cozy atmosphere. The woman is wearing a floral skirt, and there is a red cake with a spider design on the table.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25596.4, "ram_available_mb": 100175.8, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 25596.7, "ram_available_mb": 100175.5, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.8, "peak": 15.11, "min": 14.12}, "VDD_GPU": {"avg": 27.97, "peak": 38.19, "min": 23.24}, "VIN": {"avg": 70.98, "peak": 114.21, "min": 59.84}, "VDD_CPU_SOC_MSS": {"avg": 15.37, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 27.97, "energy_joules_est": 76.74, "sample_count": 21, "duration_seconds": 2.744}, "timestamp": "2026-01-16T16:39:54.718435"}
{"image_index": 426, "image_name": "000000048504.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000048504.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1171.718, "latencies_ms": [1171.718], "images_per_second": 0.853, "prompt_tokens": 9, "response_tokens_est": 22, "n_tiles": 6, "output_text": "A woman is seen riding an elephant in a large arena, with the elephant being led by another person.", "error": null, "sys_before": {"cpu_percent": 43.8, "ram_used_mb": 25596.7, "ram_available_mb": 100175.5, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25596.4, "ram_available_mb": 100175.8, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.42, "peak": 14.81, "min": 13.92}, "VDD_GPU": {"avg": 32.61, "peak": 37.03, "min": 27.18}, "VIN": {"avg": 74.99, "peak": 121.61, "min": 54.86}, "VDD_CPU_SOC_MSS": {"avg": 15.09, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 32.61, "energy_joules_est": 38.23, "sample_count": 9, "duration_seconds": 1.172}, "timestamp": "2026-01-16T16:39:55.938486"}
{"image_index": 426, "image_name": "000000048504.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000048504.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2312.889, "latencies_ms": [2312.889], "images_per_second": 0.432, "prompt_tokens": 23, "response_tokens_est": 63, "n_tiles": 6, "output_text": "1. Elephant: 1\n2. Person: 1\n3. Rope: 1\n4. Stirrups: 1\n5. Ramp: 1\n6. Platform: 1\n7. Floor: 1\n8. Barrier: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25596.4, "ram_available_mb": 100175.8, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 25596.4, "ram_available_mb": 100175.8, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.88, "peak": 15.22, "min": 14.22}, "VDD_GPU": {"avg": 29.85, "peak": 39.39, "min": 24.03}, "VIN": {"avg": 65.73, "peak": 81.78, "min": 57.5}, "VDD_CPU_SOC_MSS": {"avg": 15.44, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 29.85, "energy_joules_est": 69.06, "sample_count": 18, "duration_seconds": 2.314}, "timestamp": "2026-01-16T16:39:58.258619"}
{"image_index": 426, "image_name": "000000048504.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000048504.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3211.251, "latencies_ms": [3211.251], "images_per_second": 0.311, "prompt_tokens": 27, "response_tokens_est": 94, "n_tiles": 6, "output_text": "In the image, the main focus is on the two elephants in the foreground, with one elephant being led by a person in a red vest. The person is guiding the elephant towards the right side of the frame. The background features a large arena with a concrete floor and metal railings, and there are other people and equipment scattered around. The elephants are positioned on a dirt-covered ground, and there is a red and yellow striped barrier in the foreground.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25596.4, "ram_available_mb": 100175.8, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 25596.4, "ram_available_mb": 100175.8, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.97, "peak": 15.22, "min": 14.32}, "VDD_GPU": {"avg": 27.57, "peak": 37.82, "min": 23.24}, "VIN": {"avg": 66.69, "peak": 74.95, "min": 55.71}, "VDD_CPU_SOC_MSS": {"avg": 15.5, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 27.57, "energy_joules_est": 88.55, "sample_count": 24, "duration_seconds": 3.212}, "timestamp": "2026-01-16T16:40:01.476103"}
{"image_index": 426, "image_name": "000000048504.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000048504.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2520.317, "latencies_ms": [2520.317], "images_per_second": 0.397, "prompt_tokens": 21, "response_tokens_est": 68, "n_tiles": 6, "output_text": "The image depicts an indoor arena with a dirt floor, where a man is seen standing near a large elephant, likely preparing it for a performance or event. The setting appears to be a rodeo or circus, with a person in a red vest and white gloves assisting the elephant, possibly preparing it for a ride or demonstration.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25596.4, "ram_available_mb": 100175.8, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 25596.6, "ram_available_mb": 100175.5, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.76, "peak": 15.01, "min": 14.12}, "VDD_GPU": {"avg": 28.53, "peak": 37.82, "min": 23.24}, "VIN": {"avg": 67.7, "peak": 97.07, "min": 55.18}, "VDD_CPU_SOC_MSS": {"avg": 15.23, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 28.53, "energy_joules_est": 71.92, "sample_count": 19, "duration_seconds": 2.521}, "timestamp": "2026-01-16T16:40:04.002519"}
{"image_index": 426, "image_name": "000000048504.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000048504.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2047.48, "latencies_ms": [2047.48], "images_per_second": 0.488, "prompt_tokens": 19, "response_tokens_est": 51, "n_tiles": 6, "output_text": "The image depicts an indoor arena with a dirt floor and metal railings. The lighting is artificial, with spotlights illuminating the scene. The arena is adorned with a colorful, geometric-patterned floor and a red and yellow barrier.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25596.6, "ram_available_mb": 100175.5, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 25596.4, "ram_available_mb": 100175.8, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.67, "peak": 15.01, "min": 14.02}, "VDD_GPU": {"avg": 29.75, "peak": 38.21, "min": 23.64}, "VIN": {"avg": 72.41, "peak": 115.41, "min": 59.85}, "VDD_CPU_SOC_MSS": {"avg": 15.2, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 29.75, "energy_joules_est": 60.92, "sample_count": 15, "duration_seconds": 2.048}, "timestamp": "2026-01-16T16:40:06.058803"}
{"image_index": 427, "image_name": "000000048555.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000048555.jpg", "image_width": 640, "image_height": 465, "image_resolution": "640x465", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2581.337, "latencies_ms": [2581.337], "images_per_second": 0.387, "prompt_tokens": 9, "response_tokens_est": 35, "n_tiles": 12, "output_text": "A group of jockeys on horseback are racing across a sandy beach, with the horses galloping vigorously and the riders leaning forward in concentration.", "error": null, "sys_before": {"cpu_percent": 44.8, "ram_used_mb": 25596.1, "ram_available_mb": 100176.0, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 25596.1, "ram_available_mb": 100176.0, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.16, "peak": 16.15, "min": 14.0}, "VDD_GPU": {"avg": 34.33, "peak": 40.57, "min": 27.18}, "VIN": {"avg": 73.05, "peak": 96.72, "min": 58.58}, "VDD_CPU_SOC_MSS": {"avg": 15.84, "peak": 17.32, "min": 14.96}}, "power_watts_avg": 34.33, "energy_joules_est": 88.63, "sample_count": 20, "duration_seconds": 2.582}, "timestamp": "2026-01-16T16:40:08.712636"}
{"image_index": 427, "image_name": "000000048555.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000048555.jpg", "image_width": 640, "image_height": 465, "image_resolution": "640x465", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2610.61, "latencies_ms": [2610.61], "images_per_second": 0.383, "prompt_tokens": 23, "response_tokens_est": 32, "n_tiles": 12, "output_text": "1. Horse\n2. Rider\n3. Horse\n4. Rider\n5. Horse\n6. Rider\n7. Horse\n8. Rider", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25596.1, "ram_available_mb": 100176.0, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 25596.4, "ram_available_mb": 100175.8, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.1, "peak": 15.65, "min": 14.43}, "VDD_GPU": {"avg": 35.55, "peak": 42.94, "min": 27.18}, "VIN": {"avg": 71.3, "peak": 102.04, "min": 51.78}, "VDD_CPU_SOC_MSS": {"avg": 15.69, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 35.55, "energy_joules_est": 92.82, "sample_count": 20, "duration_seconds": 2.611}, "timestamp": "2026-01-16T16:40:11.329698"}
{"image_index": 427, "image_name": "000000048555.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000048555.jpg", "image_width": 640, "image_height": 465, "image_resolution": "640x465", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3217.378, "latencies_ms": [3217.378], "images_per_second": 0.311, "prompt_tokens": 27, "response_tokens_est": 50, "n_tiles": 12, "output_text": "The main objects in the image are the horses and the riders. The horses are in the foreground, while the riders are positioned slightly behind them. The riders are near the center of the image, with the horses positioned to the left and right.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25596.4, "ram_available_mb": 100175.8, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 25596.4, "ram_available_mb": 100175.8, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.09, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 33.97, "peak": 42.54, "min": 26.39}, "VIN": {"avg": 79.26, "peak": 121.31, "min": 58.55}, "VDD_CPU_SOC_MSS": {"avg": 15.64, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 33.97, "energy_joules_est": 109.31, "sample_count": 25, "duration_seconds": 3.218}, "timestamp": "2026-01-16T16:40:14.553543"}
{"image_index": 427, "image_name": "000000048555.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000048555.jpg", "image_width": 640, "image_height": 465, "image_resolution": "640x465", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3343.804, "latencies_ms": [3343.804], "images_per_second": 0.299, "prompt_tokens": 21, "response_tokens_est": 54, "n_tiles": 12, "output_text": "The image depicts a horse race in progress, with a group of jockeys on horseback racing towards the finish line. The scene is set on a track, and the horses are moving at a high speed, creating a blur effect in the photograph.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25596.4, "ram_available_mb": 100175.8, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 25596.6, "ram_available_mb": 100175.5, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.1, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 33.78, "peak": 42.53, "min": 26.39}, "VIN": {"avg": 73.37, "peak": 116.88, "min": 57.0}, "VDD_CPU_SOC_MSS": {"avg": 15.68, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 33.78, "energy_joules_est": 112.98, "sample_count": 25, "duration_seconds": 3.344}, "timestamp": "2026-01-16T16:40:17.904248"}
{"image_index": 427, "image_name": "000000048555.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000048555.jpg", "image_width": 640, "image_height": 465, "image_resolution": "640x465", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3546.721, "latencies_ms": [3546.721], "images_per_second": 0.282, "prompt_tokens": 19, "response_tokens_est": 60, "n_tiles": 12, "output_text": "The image is a black and white photograph featuring a horse race. The horses are in motion, creating a blur effect in the background, and the horses and riders are in sharp focus. The lighting is natural, with a soft, diffused quality, and the overall atmosphere is dynamic and energetic.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25596.6, "ram_available_mb": 100175.5, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 25596.4, "ram_available_mb": 100175.8, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.1, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 33.25, "peak": 42.54, "min": 26.39}, "VIN": {"avg": 76.22, "peak": 117.18, "min": 46.08}, "VDD_CPU_SOC_MSS": {"avg": 15.66, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 33.25, "energy_joules_est": 117.94, "sample_count": 27, "duration_seconds": 3.547}, "timestamp": "2026-01-16T16:40:21.457848"}
{"image_index": 428, "image_name": "000000048564.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000048564.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1256.935, "latencies_ms": [1256.935], "images_per_second": 0.796, "prompt_tokens": 9, "response_tokens_est": 25, "n_tiles": 6, "output_text": "A man wearing a black beanie and a black jacket is holding a phone to his ear, seemingly in a conversation.", "error": null, "sys_before": {"cpu_percent": 45.2, "ram_used_mb": 25596.4, "ram_available_mb": 100175.8, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25596.4, "ram_available_mb": 100175.8, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.34, "peak": 14.81, "min": 13.82}, "VDD_GPU": {"avg": 33.09, "peak": 37.82, "min": 27.57}, "VIN": {"avg": 78.24, "peak": 98.92, "min": 64.13}, "VDD_CPU_SOC_MSS": {"avg": 15.0, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 33.09, "energy_joules_est": 41.6, "sample_count": 9, "duration_seconds": 1.257}, "timestamp": "2026-01-16T16:40:22.777531"}
{"image_index": 428, "image_name": "000000048564.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000048564.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2169.527, "latencies_ms": [2169.527], "images_per_second": 0.461, "prompt_tokens": 23, "response_tokens_est": 58, "n_tiles": 6, "output_text": "1. Snowboarder\n2. Snowboard\n3. Snow\n4. Snowboarder's helmet\n5. Snowboarder's goggles\n6. Snowboarder's jacket\n7. Snowboarder's beanie\n8. Snowboarder's scarf", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25596.4, "ram_available_mb": 100175.8, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 25596.4, "ram_available_mb": 100175.8, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.89, "peak": 15.32, "min": 14.22}, "VDD_GPU": {"avg": 30.3, "peak": 38.98, "min": 24.03}, "VIN": {"avg": 73.64, "peak": 116.32, "min": 53.96}, "VDD_CPU_SOC_MSS": {"avg": 15.48, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 30.3, "energy_joules_est": 65.75, "sample_count": 16, "duration_seconds": 2.17}, "timestamp": "2026-01-16T16:40:24.953324"}
{"image_index": 428, "image_name": "000000048564.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000048564.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1884.029, "latencies_ms": [1884.029], "images_per_second": 0.531, "prompt_tokens": 27, "response_tokens_est": 47, "n_tiles": 6, "output_text": "The main object in the foreground is a person wearing a black jacket and a blue scarf. The person is holding a phone to their ear. In the background, there is a snowy landscape with trees and a house visible.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25596.4, "ram_available_mb": 100175.8, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 25596.4, "ram_available_mb": 100175.8, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.89, "peak": 15.22, "min": 14.32}, "VDD_GPU": {"avg": 30.87, "peak": 38.21, "min": 24.43}, "VIN": {"avg": 71.06, "peak": 117.5, "min": 57.47}, "VDD_CPU_SOC_MSS": {"avg": 15.47, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 30.87, "energy_joules_est": 58.17, "sample_count": 14, "duration_seconds": 1.884}, "timestamp": "2026-01-16T16:40:26.843358"}
{"image_index": 428, "image_name": "000000048564.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000048564.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2390.905, "latencies_ms": [2390.905], "images_per_second": 0.418, "prompt_tokens": 21, "response_tokens_est": 66, "n_tiles": 6, "output_text": "The image depicts a man in a winter setting, standing outdoors with snow on the ground. He is wearing a black jacket and a beanie, and appears to be in the middle of a conversation on his phone. The background shows a snowy landscape with trees and a building, indicating a cold, wintry environment.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25596.4, "ram_available_mb": 100175.8, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 25596.4, "ram_available_mb": 100175.8, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.97, "peak": 15.22, "min": 14.32}, "VDD_GPU": {"avg": 29.24, "peak": 38.19, "min": 23.64}, "VIN": {"avg": 73.84, "peak": 114.32, "min": 60.31}, "VDD_CPU_SOC_MSS": {"avg": 15.59, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 29.24, "energy_joules_est": 69.92, "sample_count": 18, "duration_seconds": 2.391}, "timestamp": "2026-01-16T16:40:29.240403"}
{"image_index": 428, "image_name": "000000048564.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000048564.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1827.939, "latencies_ms": [1827.939], "images_per_second": 0.547, "prompt_tokens": 19, "response_tokens_est": 45, "n_tiles": 6, "output_text": "The notable visual attributes of the image include a person wearing a black beanie and a black jacket with a green button on the left side. The lighting is bright, and the person is outdoors in a snowy environment.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25596.4, "ram_available_mb": 100175.8, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 25596.3, "ram_available_mb": 100175.8, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.88, "peak": 15.22, "min": 14.32}, "VDD_GPU": {"avg": 30.67, "peak": 37.82, "min": 24.42}, "VIN": {"avg": 72.19, "peak": 112.2, "min": 57.53}, "VDD_CPU_SOC_MSS": {"avg": 15.49, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 30.67, "energy_joules_est": 56.08, "sample_count": 14, "duration_seconds": 1.828}, "timestamp": "2026-01-16T16:40:31.074534"}
{"image_index": 429, "image_name": "000000048924.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000048924.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1069.369, "latencies_ms": [1069.369], "images_per_second": 0.935, "prompt_tokens": 9, "response_tokens_est": 18, "n_tiles": 6, "output_text": "A motorcycle is parked next to a green tent in a forested area during sunset.", "error": null, "sys_before": {"cpu_percent": 47.6, "ram_used_mb": 25596.3, "ram_available_mb": 100175.8, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25596.3, "ram_available_mb": 100175.8, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.3, "peak": 14.61, "min": 13.92}, "VDD_GPU": {"avg": 33.28, "peak": 37.01, "min": 28.36}, "VIN": {"avg": 74.04, "peak": 106.95, "min": 63.22}, "VDD_CPU_SOC_MSS": {"avg": 15.11, "peak": 15.36, "min": 14.57}}, "power_watts_avg": 33.28, "energy_joules_est": 35.6, "sample_count": 8, "duration_seconds": 1.07}, "timestamp": "2026-01-16T16:40:32.205574"}
{"image_index": 429, "image_name": "000000048924.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000048924.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1595.444, "latencies_ms": [1595.444], "images_per_second": 0.627, "prompt_tokens": 23, "response_tokens_est": 35, "n_tiles": 6, "output_text": "1. Motorcycle\n2. Tent\n3. Bushes\n4. Trees\n5. Sun\n6. Sky\n7. Grass\n8. Water", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25596.1, "ram_available_mb": 100176.1, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 25596.3, "ram_available_mb": 100175.8, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.57, "peak": 14.91, "min": 14.02}, "VDD_GPU": {"avg": 32.36, "peak": 39.77, "min": 24.82}, "VIN": {"avg": 75.76, "peak": 113.67, "min": 58.33}, "VDD_CPU_SOC_MSS": {"avg": 15.22, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 32.36, "energy_joules_est": 51.64, "sample_count": 12, "duration_seconds": 1.596}, "timestamp": "2026-01-16T16:40:33.807110"}
{"image_index": 429, "image_name": "000000048924.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000048924.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2864.753, "latencies_ms": [2864.753], "images_per_second": 0.349, "prompt_tokens": 27, "response_tokens_est": 80, "n_tiles": 6, "output_text": "The main object in the foreground is a motorcycle, which is positioned near the center of the image. The motorcycle is facing towards the right side of the frame. In the background, there is a green tent, which is situated to the left of the motorcycle. The tent is partially obscured by the motorcycle. The setting appears to be a natural, outdoor environment with trees and a clear sky.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25596.3, "ram_available_mb": 100175.8, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 25596.1, "ram_available_mb": 100176.1, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.77, "peak": 15.01, "min": 14.02}, "VDD_GPU": {"avg": 28.04, "peak": 38.6, "min": 23.24}, "VIN": {"avg": 68.2, "peak": 115.31, "min": 60.6}, "VDD_CPU_SOC_MSS": {"avg": 15.3, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 28.04, "energy_joules_est": 80.34, "sample_count": 22, "duration_seconds": 2.865}, "timestamp": "2026-01-16T16:40:36.678443"}
{"image_index": 429, "image_name": "000000048924.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000048924.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1990.696, "latencies_ms": [1990.696], "images_per_second": 0.502, "prompt_tokens": 21, "response_tokens_est": 49, "n_tiles": 6, "output_text": "The image depicts a tranquil outdoor setting during sunset, with a green tent and a motorcycle parked on a grassy area. The scene is serene, with the soft glow of the setting sun casting a warm light over the landscape.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25596.1, "ram_available_mb": 100176.1, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 25596.1, "ram_available_mb": 100176.1, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.68, "peak": 15.01, "min": 14.12}, "VDD_GPU": {"avg": 29.78, "peak": 37.82, "min": 23.64}, "VIN": {"avg": 68.94, "peak": 106.31, "min": 56.42}, "VDD_CPU_SOC_MSS": {"avg": 15.25, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 29.78, "energy_joules_est": 59.3, "sample_count": 15, "duration_seconds": 1.991}, "timestamp": "2026-01-16T16:40:38.675257"}
{"image_index": 429, "image_name": "000000048924.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000048924.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2852.915, "latencies_ms": [2852.915], "images_per_second": 0.351, "prompt_tokens": 19, "response_tokens_est": 80, "n_tiles": 6, "output_text": "The image depicts a green tent set up in a natural, wooded area during sunset. The tent is illuminated by the warm, golden light of the setting sun, casting a soft glow on the surrounding foliage. The scene is characterized by the contrast between the vibrant green of the tent and the muted tones of the forest, as well as the gentle hues of the sky.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25596.1, "ram_available_mb": 100176.1, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 25596.1, "ram_available_mb": 100176.1, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.82, "peak": 15.11, "min": 14.12}, "VDD_GPU": {"avg": 27.86, "peak": 38.19, "min": 23.24}, "VIN": {"avg": 71.19, "peak": 115.74, "min": 57.48}, "VDD_CPU_SOC_MSS": {"avg": 15.46, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 27.86, "energy_joules_est": 79.49, "sample_count": 22, "duration_seconds": 2.853}, "timestamp": "2026-01-16T16:40:41.534399"}
{"image_index": 430, "image_name": "000000049060.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000049060.jpg", "image_width": 640, "image_height": 411, "image_resolution": "640x411", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1764.738, "latencies_ms": [1764.738], "images_per_second": 0.567, "prompt_tokens": 9, "response_tokens_est": 44, "n_tiles": 6, "output_text": "A vintage black and white photograph captures a scene at a train station where a train is arriving or departing, with passengers waiting on the platform, and a steam locomotive emitting smoke from its chimney.", "error": null, "sys_before": {"cpu_percent": 40.6, "ram_used_mb": 25596.1, "ram_available_mb": 100176.1, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 25596.1, "ram_available_mb": 100176.1, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.6, "peak": 15.11, "min": 13.92}, "VDD_GPU": {"avg": 30.21, "peak": 37.42, "min": 24.42}, "VIN": {"avg": 72.47, "peak": 102.97, "min": 55.68}, "VDD_CPU_SOC_MSS": {"avg": 15.32, "peak": 15.75, "min": 14.57}}, "power_watts_avg": 30.21, "energy_joules_est": 53.32, "sample_count": 13, "duration_seconds": 1.765}, "timestamp": "2026-01-16T16:40:43.346549"}
{"image_index": 430, "image_name": "000000049060.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000049060.jpg", "image_width": 640, "image_height": 411, "image_resolution": "640x411", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 4076.375, "latencies_ms": [4076.375], "images_per_second": 0.245, "prompt_tokens": 23, "response_tokens_est": 129, "n_tiles": 6, "output_text": "- Train: 1\n- Train Car: 1\n- Train Engine: 1\n- Train Carriage: 1\n- Train Car: 1\n- Train Car: 1\n- Train Car: 1\n- Train Car: 1\n- Train Car: 1\n- Train Car: 1\n- Train Car: 1\n- Train Car: 1\n- Train Car: 1\n- Train Car: 1\n- Train Car: 1\n- Train Car: 1\n- Train Car: 1\n- Train Car: 1\n- Train", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25596.1, "ram_available_mb": 100176.1, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.7, "ram_used_mb": 25596.3, "ram_available_mb": 100175.9, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.06, "peak": 15.22, "min": 14.32}, "VDD_GPU": {"avg": 26.77, "peak": 38.21, "min": 23.64}, "VIN": {"avg": 70.76, "peak": 110.99, "min": 56.32}, "VDD_CPU_SOC_MSS": {"avg": 15.6, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 26.77, "energy_joules_est": 109.14, "sample_count": 32, "duration_seconds": 4.077}, "timestamp": "2026-01-16T16:40:47.428788"}
{"image_index": 430, "image_name": "000000049060.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000049060.jpg", "image_width": 640, "image_height": 411, "image_resolution": "640x411", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2321.367, "latencies_ms": [2321.367], "images_per_second": 0.431, "prompt_tokens": 27, "response_tokens_est": 63, "n_tiles": 6, "output_text": "The main objects in the image are a vintage steam locomotive and a group of people standing near the train. The locomotive is positioned in the foreground, with the people standing in the background. The train is on the tracks, and the people are near the platform where the train is stationed.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25596.3, "ram_available_mb": 100175.9, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 25596.3, "ram_available_mb": 100175.9, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.92, "peak": 15.22, "min": 14.22}, "VDD_GPU": {"avg": 29.17, "peak": 37.82, "min": 23.64}, "VIN": {"avg": 74.69, "peak": 117.0, "min": 61.38}, "VDD_CPU_SOC_MSS": {"avg": 15.48, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 29.17, "energy_joules_est": 67.72, "sample_count": 18, "duration_seconds": 2.322}, "timestamp": "2026-01-16T16:40:49.756345"}
{"image_index": 430, "image_name": "000000049060.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000049060.jpg", "image_width": 640, "image_height": 411, "image_resolution": "640x411", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1861.068, "latencies_ms": [1861.068], "images_per_second": 0.537, "prompt_tokens": 21, "response_tokens_est": 46, "n_tiles": 6, "output_text": "The image depicts a vintage scene at a train station, featuring a steam locomotive with a train car visible in the background. A group of people is gathered on the platform, possibly waiting for the train to arrive.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25596.3, "ram_available_mb": 100175.9, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25596.3, "ram_available_mb": 100175.9, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.84, "peak": 15.11, "min": 14.32}, "VDD_GPU": {"avg": 30.72, "peak": 37.8, "min": 24.42}, "VIN": {"avg": 67.33, "peak": 121.14, "min": 54.56}, "VDD_CPU_SOC_MSS": {"avg": 15.38, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 30.72, "energy_joules_est": 57.18, "sample_count": 14, "duration_seconds": 1.861}, "timestamp": "2026-01-16T16:40:51.623205"}
{"image_index": 430, "image_name": "000000049060.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000049060.jpg", "image_width": 640, "image_height": 411, "image_resolution": "640x411", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2314.108, "latencies_ms": [2314.108], "images_per_second": 0.432, "prompt_tokens": 19, "response_tokens_est": 63, "n_tiles": 6, "output_text": "The image is a black and white photograph featuring a vintage steam locomotive with a prominent smokestack and a large, rounded front. The locomotive is stationed at a platform with a few people standing nearby, and the scene is set against a backdrop of residential buildings and trees under a clear sky.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25596.3, "ram_available_mb": 100175.9, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 25596.3, "ram_available_mb": 100175.9, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.95, "peak": 15.22, "min": 14.32}, "VDD_GPU": {"avg": 29.37, "peak": 38.21, "min": 23.64}, "VIN": {"avg": 66.92, "peak": 94.65, "min": 55.37}, "VDD_CPU_SOC_MSS": {"avg": 15.51, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 29.37, "energy_joules_est": 67.98, "sample_count": 18, "duration_seconds": 2.314}, "timestamp": "2026-01-16T16:40:53.943285"}
{"image_index": 431, "image_name": "000000049091.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000049091.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1602.436, "latencies_ms": [1602.436], "images_per_second": 0.624, "prompt_tokens": 9, "response_tokens_est": 38, "n_tiles": 6, "output_text": "The image depicts a densely packed urban street scene, filled with numerous signs in both English and Chinese characters, indicating a bustling commercial area with a variety of businesses and services.", "error": null, "sys_before": {"cpu_percent": 38.2, "ram_used_mb": 25596.3, "ram_available_mb": 100175.9, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 25596.6, "ram_available_mb": 100175.6, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.6, "peak": 15.11, "min": 14.02}, "VDD_GPU": {"avg": 30.63, "peak": 37.42, "min": 24.82}, "VIN": {"avg": 73.51, "peak": 104.74, "min": 62.69}, "VDD_CPU_SOC_MSS": {"avg": 15.35, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 30.63, "energy_joules_est": 49.09, "sample_count": 12, "duration_seconds": 1.603}, "timestamp": "2026-01-16T16:40:55.589379"}
{"image_index": 431, "image_name": "000000049091.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000049091.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2775.702, "latencies_ms": [2775.702], "images_per_second": 0.36, "prompt_tokens": 23, "response_tokens_est": 80, "n_tiles": 6, "output_text": "- 1. Signboard: 8\n- 2. Signboard: 8\n- 3. Signboard: 8\n- 4. Signboard: 8\n- 5. Signboard: 8\n- 6. Signboard: 8\n- 7. Signboard: 8\n- 8. Signboard: 8", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25596.6, "ram_available_mb": 100175.6, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 25596.2, "ram_available_mb": 100176.0, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.98, "peak": 15.22, "min": 14.32}, "VDD_GPU": {"avg": 28.36, "peak": 38.19, "min": 23.64}, "VIN": {"avg": 73.04, "peak": 107.69, "min": 60.25}, "VDD_CPU_SOC_MSS": {"avg": 15.56, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 28.36, "energy_joules_est": 78.73, "sample_count": 21, "duration_seconds": 2.776}, "timestamp": "2026-01-16T16:40:58.371099"}
{"image_index": 431, "image_name": "000000049091.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000049091.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2344.075, "latencies_ms": [2344.075], "images_per_second": 0.427, "prompt_tokens": 27, "response_tokens_est": 63, "n_tiles": 6, "output_text": "The main objects in the image are numerous signs with Japanese characters, which are densely packed and appear to be hanging from a metal framework. The signs are in the foreground, with the background consisting of a multi-story building. The signs are positioned close to each other, creating a crowded and cluttered appearance.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25596.2, "ram_available_mb": 100176.0, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 25596.8, "ram_available_mb": 100175.4, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.91, "peak": 15.22, "min": 14.22}, "VDD_GPU": {"avg": 29.17, "peak": 38.21, "min": 23.24}, "VIN": {"avg": 72.28, "peak": 120.0, "min": 62.18}, "VDD_CPU_SOC_MSS": {"avg": 15.57, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 29.17, "energy_joules_est": 68.39, "sample_count": 18, "duration_seconds": 2.345}, "timestamp": "2026-01-16T16:41:00.721206"}
{"image_index": 431, "image_name": "000000049091.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000049091.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3252.159, "latencies_ms": [3252.159], "images_per_second": 0.307, "prompt_tokens": 21, "response_tokens_est": 94, "n_tiles": 6, "output_text": "The image depicts a bustling urban street scene, likely in a Chinatown area, characterized by a dense array of signage in both English and Chinese characters. The signage is predominantly advertising various businesses, including clothing stores, restaurants, and possibly a bank or financial services. The scene is filled with a sense of activity and commerce, with numerous signs hanging from the buildings, creating a visually busy and vibrant atmosphere typical of a busy Chinatown district.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25596.8, "ram_available_mb": 100175.4, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 25597.0, "ram_available_mb": 100175.1, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.81, "peak": 15.11, "min": 14.02}, "VDD_GPU": {"avg": 27.37, "peak": 38.21, "min": 23.24}, "VIN": {"avg": 68.7, "peak": 110.73, "min": 52.99}, "VDD_CPU_SOC_MSS": {"avg": 15.42, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 27.37, "energy_joules_est": 89.02, "sample_count": 25, "duration_seconds": 3.253}, "timestamp": "2026-01-16T16:41:03.983414"}
{"image_index": 431, "image_name": "000000049091.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000049091.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2856.616, "latencies_ms": [2856.616], "images_per_second": 0.35, "prompt_tokens": 19, "response_tokens_est": 80, "n_tiles": 6, "output_text": "The image is a black and white photograph capturing a bustling urban street scene. The notable visual attributes include the dense array of signage in various languages, predominantly in black and white, with some signs featuring red and blue colors. The lighting is dim, likely due to the overcast sky, and the materials used for the signs appear to be metal or plastic, typical of urban street signs.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25597.0, "ram_available_mb": 100175.1, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 25597.0, "ram_available_mb": 100175.2, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.77, "peak": 15.01, "min": 14.02}, "VDD_GPU": {"avg": 27.84, "peak": 38.19, "min": 23.24}, "VIN": {"avg": 70.42, "peak": 111.45, "min": 61.1}, "VDD_CPU_SOC_MSS": {"avg": 15.43, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 27.84, "energy_joules_est": 79.54, "sample_count": 22, "duration_seconds": 2.857}, "timestamp": "2026-01-16T16:41:06.846305"}
{"image_index": 432, "image_name": "000000049259.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000049259.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1359.851, "latencies_ms": [1359.851], "images_per_second": 0.735, "prompt_tokens": 9, "response_tokens_est": 29, "n_tiles": 6, "output_text": "The image shows a sign that reads \"CLOSED\" on a grassy area, indicating that the area is not open for public use.", "error": null, "sys_before": {"cpu_percent": 44.0, "ram_used_mb": 25597.0, "ram_available_mb": 100175.2, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25597.0, "ram_available_mb": 100175.2, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.36, "peak": 14.91, "min": 13.82}, "VDD_GPU": {"avg": 31.75, "peak": 36.63, "min": 26.0}, "VIN": {"avg": 70.96, "peak": 117.44, "min": 53.61}, "VDD_CPU_SOC_MSS": {"avg": 15.12, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 31.75, "energy_joules_est": 43.19, "sample_count": 10, "duration_seconds": 1.36}, "timestamp": "2026-01-16T16:41:08.269452"}
{"image_index": 432, "image_name": "000000049259.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000049259.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1906.96, "latencies_ms": [1906.96], "images_per_second": 0.524, "prompt_tokens": 23, "response_tokens_est": 48, "n_tiles": 6, "output_text": "- sign: 2\n- grass: 1\n- bench: 1\n- water: 1\n- person: 1\n- sign: 1\n- tree: 1\n- bench: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25597.0, "ram_available_mb": 100175.2, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 25597.3, "ram_available_mb": 100174.9, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.81, "peak": 15.22, "min": 14.22}, "VDD_GPU": {"avg": 31.06, "peak": 38.59, "min": 24.42}, "VIN": {"avg": 69.05, "peak": 92.46, "min": 57.37}, "VDD_CPU_SOC_MSS": {"avg": 15.44, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 31.06, "energy_joules_est": 59.24, "sample_count": 14, "duration_seconds": 1.907}, "timestamp": "2026-01-16T16:41:10.182523"}
{"image_index": 432, "image_name": "000000049259.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000049259.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2072.8, "latencies_ms": [2072.8], "images_per_second": 0.482, "prompt_tokens": 27, "response_tokens_est": 52, "n_tiles": 6, "output_text": "The main objects in the image are a sign and a bench. The sign is placed on the ground in the foreground, while the bench is situated in the background. The sign is near the bench, indicating that it is part of the same outdoor setting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25597.3, "ram_available_mb": 100174.9, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 25597.0, "ram_available_mb": 100175.2, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.72, "peak": 15.01, "min": 14.12}, "VDD_GPU": {"avg": 29.91, "peak": 38.6, "min": 23.64}, "VIN": {"avg": 73.4, "peak": 112.7, "min": 60.9}, "VDD_CPU_SOC_MSS": {"avg": 15.26, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 29.91, "energy_joules_est": 62.01, "sample_count": 16, "duration_seconds": 2.073}, "timestamp": "2026-01-16T16:41:12.261492"}
{"image_index": 432, "image_name": "000000049259.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000049259.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2777.299, "latencies_ms": [2777.299], "images_per_second": 0.36, "prompt_tokens": 21, "response_tokens_est": 77, "n_tiles": 6, "output_text": "The image depicts a scene at a lakeside park where a man is lying on a concrete bench, seemingly relaxing. The ground is covered with grass and some fallen leaves, and there is a wooden sign with the words \"CLOSED\" and \"Laketon Trail Cross\" on it. The setting appears to be a peaceful, natural environment with a clear blue sky overhead.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25597.0, "ram_available_mb": 100175.2, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 25596.8, "ram_available_mb": 100175.4, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.79, "peak": 15.01, "min": 14.12}, "VDD_GPU": {"avg": 27.91, "peak": 38.19, "min": 23.24}, "VIN": {"avg": 64.59, "peak": 74.18, "min": 54.85}, "VDD_CPU_SOC_MSS": {"avg": 15.45, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 27.91, "energy_joules_est": 77.52, "sample_count": 21, "duration_seconds": 2.778}, "timestamp": "2026-01-16T16:41:15.045266"}
{"image_index": 432, "image_name": "000000049259.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000049259.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2040.506, "latencies_ms": [2040.506], "images_per_second": 0.49, "prompt_tokens": 19, "response_tokens_est": 51, "n_tiles": 6, "output_text": "The image features a blue wooden sign with white lettering that reads \"CLOSED\" and \"Laketon Trail Crossing.\" The sign is placed on a grassy area with a clear blue sky overhead, indicating a sunny day with good lighting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25596.8, "ram_available_mb": 100175.4, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 25596.8, "ram_available_mb": 100175.4, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.7, "peak": 15.01, "min": 14.02}, "VDD_GPU": {"avg": 29.64, "peak": 38.21, "min": 23.64}, "VIN": {"avg": 69.55, "peak": 121.93, "min": 52.23}, "VDD_CPU_SOC_MSS": {"avg": 15.28, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 29.64, "energy_joules_est": 60.49, "sample_count": 16, "duration_seconds": 2.041}, "timestamp": "2026-01-16T16:41:17.092046"}
{"image_index": 433, "image_name": "000000049269.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000049269.jpg", "image_width": 431, "image_height": 640, "image_resolution": "431x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1526.579, "latencies_ms": [1526.579], "images_per_second": 0.655, "prompt_tokens": 9, "response_tokens_est": 35, "n_tiles": 6, "output_text": "A golden retriever is lying on the grass, panting with its tongue out, while a brown horse stands behind it, both appearing to be enjoying the sunny day.", "error": null, "sys_before": {"cpu_percent": 44.2, "ram_used_mb": 25596.8, "ram_available_mb": 100175.4, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25596.8, "ram_available_mb": 100175.4, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.42, "peak": 15.01, "min": 13.82}, "VDD_GPU": {"avg": 31.26, "peak": 37.42, "min": 25.21}, "VIN": {"avg": 74.01, "peak": 126.74, "min": 58.62}, "VDD_CPU_SOC_MSS": {"avg": 15.17, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 31.26, "energy_joules_est": 47.73, "sample_count": 11, "duration_seconds": 1.527}, "timestamp": "2026-01-16T16:41:18.677963"}
{"image_index": 433, "image_name": "000000049269.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000049269.jpg", "image_width": 431, "image_height": 640, "image_resolution": "431x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1033.163, "latencies_ms": [1033.163], "images_per_second": 0.968, "prompt_tokens": 23, "response_tokens_est": 15, "n_tiles": 6, "output_text": "dog: 1\nhorse: 2\ngrass: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25596.8, "ram_available_mb": 100175.4, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 25597.0, "ram_available_mb": 100175.2, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.6, "peak": 14.83, "min": 14.22}, "VDD_GPU": {"avg": 35.4, "peak": 38.21, "min": 31.51}, "VIN": {"avg": 86.56, "peak": 111.0, "min": 63.37}, "VDD_CPU_SOC_MSS": {"avg": 15.18, "peak": 15.35, "min": 14.96}}, "power_watts_avg": 35.4, "energy_joules_est": 36.59, "sample_count": 7, "duration_seconds": 1.034}, "timestamp": "2026-01-16T16:41:19.717506"}
{"image_index": 433, "image_name": "000000049269.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000049269.jpg", "image_width": 431, "image_height": 640, "image_resolution": "431x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2374.113, "latencies_ms": [2374.113], "images_per_second": 0.421, "prompt_tokens": 27, "response_tokens_est": 65, "n_tiles": 6, "output_text": "The main objects in the image are a brown horse and a golden retriever. The brown horse is in the background, while the golden retriever is in the foreground. The horse is slightly out of focus, indicating it is farther away, while the dog is in sharp focus, suggesting it is closer to the camera.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25597.0, "ram_available_mb": 100175.2, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 25596.7, "ram_available_mb": 100175.4, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.87, "peak": 15.22, "min": 14.22}, "VDD_GPU": {"avg": 29.89, "peak": 39.38, "min": 23.64}, "VIN": {"avg": 69.52, "peak": 112.05, "min": 57.36}, "VDD_CPU_SOC_MSS": {"avg": 15.42, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 29.89, "energy_joules_est": 70.98, "sample_count": 18, "duration_seconds": 2.375}, "timestamp": "2026-01-16T16:41:22.099273"}
{"image_index": 433, "image_name": "000000049269.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000049269.jpg", "image_width": 431, "image_height": 640, "image_resolution": "431x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2076.367, "latencies_ms": [2076.367], "images_per_second": 0.482, "prompt_tokens": 21, "response_tokens_est": 54, "n_tiles": 6, "output_text": "The image captures a serene scene in a lush green field, where a golden retriever and a brown horse are the main subjects. The dog is lying down, seemingly relaxed, while the horse stands nearby, both appearing content and at ease in their natural environment.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 25596.7, "ram_available_mb": 100175.4, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 25596.7, "ram_available_mb": 100175.4, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.88, "peak": 15.22, "min": 14.22}, "VDD_GPU": {"avg": 30.07, "peak": 37.82, "min": 24.03}, "VIN": {"avg": 67.87, "peak": 99.38, "min": 53.58}, "VDD_CPU_SOC_MSS": {"avg": 15.43, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 30.07, "energy_joules_est": 62.45, "sample_count": 15, "duration_seconds": 2.077}, "timestamp": "2026-01-16T16:41:24.185810"}
{"image_index": 433, "image_name": "000000049269.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000049269.jpg", "image_width": 431, "image_height": 640, "image_resolution": "431x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1806.731, "latencies_ms": [1806.731], "images_per_second": 0.553, "prompt_tokens": 19, "response_tokens_est": 44, "n_tiles": 6, "output_text": "The image features a golden retriever dog with a fluffy coat, lying on a lush green field under a clear blue sky. The lighting is bright and natural, casting soft shadows on the dog and the grass.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25596.7, "ram_available_mb": 100175.4, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 25597.0, "ram_available_mb": 100175.2, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.8, "peak": 15.11, "min": 14.22}, "VDD_GPU": {"avg": 31.06, "peak": 37.82, "min": 24.82}, "VIN": {"avg": 69.54, "peak": 85.88, "min": 63.02}, "VDD_CPU_SOC_MSS": {"avg": 15.38, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 31.06, "energy_joules_est": 56.13, "sample_count": 13, "duration_seconds": 1.807}, "timestamp": "2026-01-16T16:41:25.998718"}
{"image_index": 434, "image_name": "000000049759.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000049759.jpg", "image_width": 640, "image_height": 457, "image_resolution": "640x457", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1948.166, "latencies_ms": [1948.166], "images_per_second": 0.513, "prompt_tokens": 9, "response_tokens_est": 16, "n_tiles": 12, "output_text": "A group of people are playing volleyball in an indoor gymnasium.", "error": null, "sys_before": {"cpu_percent": 46.1, "ram_used_mb": 25596.7, "ram_available_mb": 100175.4, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 2.2, "ram_used_mb": 25596.7, "ram_available_mb": 100175.4, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.16, "peak": 16.15, "min": 13.9}, "VDD_GPU": {"avg": 36.52, "peak": 40.97, "min": 29.94}, "VIN": {"avg": 80.43, "peak": 117.61, "min": 58.43}, "VDD_CPU_SOC_MSS": {"avg": 16.01, "peak": 17.32, "min": 15.35}}, "power_watts_avg": 36.52, "energy_joules_est": 71.16, "sample_count": 15, "duration_seconds": 1.948}, "timestamp": "2026-01-16T16:41:28.034441"}
{"image_index": 434, "image_name": "000000049759.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000049759.jpg", "image_width": 640, "image_height": 457, "image_resolution": "640x457", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2710.15, "latencies_ms": [2710.15], "images_per_second": 0.369, "prompt_tokens": 23, "response_tokens_est": 35, "n_tiles": 12, "output_text": "1. Volleyball\n2. Court\n3. Players\n4. Team\n5. Coach\n6. Spectators\n7. Ball\n8. Court", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25596.7, "ram_available_mb": 100175.4, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 25596.7, "ram_available_mb": 100175.4, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.06, "peak": 15.65, "min": 14.43}, "VDD_GPU": {"avg": 35.78, "peak": 43.71, "min": 27.18}, "VIN": {"avg": 82.62, "peak": 120.27, "min": 58.6}, "VDD_CPU_SOC_MSS": {"avg": 15.69, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 35.78, "energy_joules_est": 96.98, "sample_count": 21, "duration_seconds": 2.711}, "timestamp": "2026-01-16T16:41:30.750847"}
{"image_index": 434, "image_name": "000000049759.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000049759.jpg", "image_width": 640, "image_height": 457, "image_resolution": "640x457", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 4637.939, "latencies_ms": [4637.939], "images_per_second": 0.216, "prompt_tokens": 27, "response_tokens_est": 92, "n_tiles": 12, "output_text": "The main objects in the image are a group of people playing volleyball in an indoor gymnasium. The foreground features a man in a blue sleeveless top and shorts, who is standing near the net, preparing to serve the volleyball. The background shows other players and spectators, with some standing near the walls and others near the court. The net is prominently placed between the two groups of people, dividing the space into two halves.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25596.7, "ram_available_mb": 100175.4, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 25596.7, "ram_available_mb": 100175.4, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.09, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 31.52, "peak": 42.54, "min": 26.0}, "VIN": {"avg": 72.76, "peak": 107.72, "min": 57.46}, "VDD_CPU_SOC_MSS": {"avg": 15.7, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 31.52, "energy_joules_est": 146.21, "sample_count": 36, "duration_seconds": 4.639}, "timestamp": "2026-01-16T16:41:35.396800"}
{"image_index": 434, "image_name": "000000049759.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000049759.jpg", "image_width": 640, "image_height": 457, "image_resolution": "640x457", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3183.117, "latencies_ms": [3183.117], "images_per_second": 0.314, "prompt_tokens": 21, "response_tokens_est": 49, "n_tiles": 12, "output_text": "The image depicts an indoor sports facility with a blue floor marked with white lines, likely a basketball court. Several individuals, including both men and women, are present, some standing and others in motion, engaged in a volleyball game.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25596.7, "ram_available_mb": 100175.4, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25596.7, "ram_available_mb": 100175.5, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.08, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 33.99, "peak": 42.53, "min": 26.39}, "VIN": {"avg": 75.04, "peak": 134.2, "min": 58.49}, "VDD_CPU_SOC_MSS": {"avg": 15.71, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 33.99, "energy_joules_est": 108.21, "sample_count": 24, "duration_seconds": 3.184}, "timestamp": "2026-01-16T16:41:38.590540"}
{"image_index": 434, "image_name": "000000049759.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000049759.jpg", "image_width": 640, "image_height": 457, "image_resolution": "640x457", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3281.79, "latencies_ms": [3281.79], "images_per_second": 0.305, "prompt_tokens": 19, "response_tokens_est": 52, "n_tiles": 12, "output_text": "The image depicts a brightly lit indoor sports court with a blue floor and white boundary lines. The lighting is evenly distributed, creating a well-lit environment. The court is made of a hard, possibly rubberized material, suitable for sports activities.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25596.7, "ram_available_mb": 100175.5, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25597.0, "ram_available_mb": 100175.2, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.07, "peak": 15.65, "min": 14.43}, "VDD_GPU": {"avg": 33.94, "peak": 42.15, "min": 26.39}, "VIN": {"avg": 74.18, "peak": 119.47, "min": 58.57}, "VDD_CPU_SOC_MSS": {"avg": 15.7, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 33.94, "energy_joules_est": 111.4, "sample_count": 25, "duration_seconds": 3.282}, "timestamp": "2026-01-16T16:41:41.882894"}
{"image_index": 435, "image_name": "000000049761.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000049761.jpg", "image_width": 640, "image_height": 479, "image_resolution": "640x479", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 3090.301, "latencies_ms": [3090.301], "images_per_second": 0.324, "prompt_tokens": 9, "response_tokens_est": 50, "n_tiles": 12, "output_text": "The image depicts a serene savanna landscape with a herd of zebras grazing on the grassy plains, while a large flock of pink flamingos stands in the background, creating a striking contrast against the greenery.", "error": null, "sys_before": {"cpu_percent": 41.7, "ram_used_mb": 25596.7, "ram_available_mb": 100175.5, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25597.0, "ram_available_mb": 100175.2, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.13, "peak": 16.15, "min": 13.9}, "VDD_GPU": {"avg": 33.5, "peak": 41.74, "min": 26.39}, "VIN": {"avg": 74.58, "peak": 114.38, "min": 56.2}, "VDD_CPU_SOC_MSS": {"avg": 15.95, "peak": 17.32, "min": 14.96}}, "power_watts_avg": 33.5, "energy_joules_est": 103.54, "sample_count": 23, "duration_seconds": 3.091}, "timestamp": "2026-01-16T16:41:45.061118"}
{"image_index": 435, "image_name": "000000049761.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000049761.jpg", "image_width": 640, "image_height": 479, "image_resolution": "640x479", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2476.689, "latencies_ms": [2476.689], "images_per_second": 0.404, "prompt_tokens": 23, "response_tokens_est": 28, "n_tiles": 12, "output_text": "zebra: 4\nbuffalo: 2\nantelope: 1\npink flamingos: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25597.0, "ram_available_mb": 100175.2, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 25596.7, "ram_available_mb": 100175.5, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.06, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 36.2, "peak": 42.94, "min": 27.98}, "VIN": {"avg": 72.35, "peak": 95.98, "min": 54.29}, "VDD_CPU_SOC_MSS": {"avg": 15.72, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 36.2, "energy_joules_est": 89.67, "sample_count": 19, "duration_seconds": 2.477}, "timestamp": "2026-01-16T16:41:47.544017"}
{"image_index": 435, "image_name": "000000049761.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000049761.jpg", "image_width": 640, "image_height": 479, "image_resolution": "640x479", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 4437.877, "latencies_ms": [4437.877], "images_per_second": 0.225, "prompt_tokens": 27, "response_tokens_est": 86, "n_tiles": 12, "output_text": "In the image, the foreground features a group of zebras grazing on the grassy field. The background is dominated by a vast expanse of water, likely a lake or a river, with numerous pink flamingos standing in the water. The zebras are positioned near the water's edge, while the flamingos are further away, creating a clear spatial relationship between the foreground and background elements.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25596.7, "ram_available_mb": 100175.5, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 25596.7, "ram_available_mb": 100175.5, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.09, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 31.77, "peak": 42.15, "min": 26.0}, "VIN": {"avg": 74.16, "peak": 117.51, "min": 56.16}, "VDD_CPU_SOC_MSS": {"avg": 15.63, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 31.77, "energy_joules_est": 141.0, "sample_count": 35, "duration_seconds": 4.438}, "timestamp": "2026-01-16T16:41:51.988283"}
{"image_index": 435, "image_name": "000000049761.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000049761.jpg", "image_width": 640, "image_height": 479, "image_resolution": "640x479", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3318.881, "latencies_ms": [3318.881], "images_per_second": 0.301, "prompt_tokens": 21, "response_tokens_est": 53, "n_tiles": 12, "output_text": "The image depicts a serene savanna landscape with a herd of zebras grazing on the grassy plain. In the background, a large flock of pink flamingos is standing in the water, creating a striking contrast against the greenery.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25596.7, "ram_available_mb": 100175.5, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 25596.7, "ram_available_mb": 100175.5, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.08, "peak": 15.65, "min": 14.43}, "VDD_GPU": {"avg": 33.63, "peak": 42.94, "min": 26.39}, "VIN": {"avg": 72.56, "peak": 109.64, "min": 51.3}, "VDD_CPU_SOC_MSS": {"avg": 15.71, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 33.63, "energy_joules_est": 111.63, "sample_count": 26, "duration_seconds": 3.319}, "timestamp": "2026-01-16T16:41:55.313511"}
{"image_index": 435, "image_name": "000000049761.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000049761.jpg", "image_width": 640, "image_height": 479, "image_resolution": "640x479", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2879.41, "latencies_ms": [2879.41], "images_per_second": 0.347, "prompt_tokens": 19, "response_tokens_est": 40, "n_tiles": 12, "output_text": "The image depicts a serene savanna landscape with a clear sky, lush green grass, and a variety of wildlife. The lighting is bright and natural, casting soft shadows on the ground.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25596.7, "ram_available_mb": 100175.5, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 25596.7, "ram_available_mb": 100175.5, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.08, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 35.01, "peak": 42.94, "min": 26.8}, "VIN": {"avg": 82.63, "peak": 139.69, "min": 58.9}, "VDD_CPU_SOC_MSS": {"avg": 15.69, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 35.01, "energy_joules_est": 100.82, "sample_count": 22, "duration_seconds": 2.88}, "timestamp": "2026-01-16T16:41:58.203175"}
{"image_index": 436, "image_name": "000000049810.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000049810.jpg", "image_width": 640, "image_height": 424, "image_resolution": "640x424", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1153.262, "latencies_ms": [1153.262], "images_per_second": 0.867, "prompt_tokens": 9, "response_tokens_est": 21, "n_tiles": 6, "output_text": "A ginger and white cat is sitting on a wooden bench, looking at its reflection in a window.", "error": null, "sys_before": {"cpu_percent": 44.2, "ram_used_mb": 25596.7, "ram_available_mb": 100175.5, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 25597.0, "ram_available_mb": 100175.2, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.26, "peak": 14.71, "min": 13.82}, "VDD_GPU": {"avg": 33.83, "peak": 37.82, "min": 28.76}, "VIN": {"avg": 78.4, "peak": 106.91, "min": 61.36}, "VDD_CPU_SOC_MSS": {"avg": 14.91, "peak": 15.36, "min": 14.57}}, "power_watts_avg": 33.83, "energy_joules_est": 39.03, "sample_count": 8, "duration_seconds": 1.154}, "timestamp": "2026-01-16T16:41:59.419822"}
{"image_index": 436, "image_name": "000000049810.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000049810.jpg", "image_width": 640, "image_height": 424, "image_resolution": "640x424", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1525.486, "latencies_ms": [1525.486], "images_per_second": 0.656, "prompt_tokens": 23, "response_tokens_est": 32, "n_tiles": 6, "output_text": "1. Cat\n2. Cat\n3. Cat\n4. Cat\n5. Cat\n6. Cat\n7. Cat\n8. Cat", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25597.0, "ram_available_mb": 100175.2, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25597.0, "ram_available_mb": 100175.2, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.56, "peak": 14.91, "min": 14.12}, "VDD_GPU": {"avg": 32.83, "peak": 38.98, "min": 25.6}, "VIN": {"avg": 64.22, "peak": 73.72, "min": 55.51}, "VDD_CPU_SOC_MSS": {"avg": 15.21, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 32.83, "energy_joules_est": 50.1, "sample_count": 11, "duration_seconds": 1.526}, "timestamp": "2026-01-16T16:42:00.951126"}
{"image_index": 436, "image_name": "000000049810.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000049810.jpg", "image_width": 640, "image_height": 424, "image_resolution": "640x424", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2441.51, "latencies_ms": [2441.51], "images_per_second": 0.41, "prompt_tokens": 27, "response_tokens_est": 65, "n_tiles": 6, "output_text": "The main object in the foreground is a ginger and white cat, which is sitting on a wooden bench. The cat is positioned near the center of the image, with its head turned towards the camera. The background features a wooden bench with vertical slats, and the reflection of the cat is visible on the glass surface.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25597.0, "ram_available_mb": 100175.2, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 25596.9, "ram_available_mb": 100175.2, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.73, "peak": 15.01, "min": 14.02}, "VDD_GPU": {"avg": 29.06, "peak": 38.59, "min": 23.24}, "VIN": {"avg": 67.25, "peak": 95.05, "min": 53.76}, "VDD_CPU_SOC_MSS": {"avg": 15.31, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 29.06, "energy_joules_est": 70.97, "sample_count": 18, "duration_seconds": 2.442}, "timestamp": "2026-01-16T16:42:03.399188"}
{"image_index": 436, "image_name": "000000049810.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000049810.jpg", "image_width": 640, "image_height": 424, "image_resolution": "640x424", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2463.894, "latencies_ms": [2463.894], "images_per_second": 0.406, "prompt_tokens": 21, "response_tokens_est": 66, "n_tiles": 6, "output_text": "The image depicts a cozy scene with a ginger and white cat sitting on a wooden bench. The cat is looking at its reflection in a glass surface, creating a sense of curiosity and playfulness. The setting appears to be outdoors, possibly in a park or garden, with soft lighting and a natural, rustic ambiance.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25596.9, "ram_available_mb": 100175.2, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 25597.2, "ram_available_mb": 100175.0, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.78, "peak": 15.01, "min": 14.12}, "VDD_GPU": {"avg": 28.59, "peak": 38.21, "min": 23.24}, "VIN": {"avg": 67.29, "peak": 90.64, "min": 58.55}, "VDD_CPU_SOC_MSS": {"avg": 15.33, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 28.59, "energy_joules_est": 70.46, "sample_count": 19, "duration_seconds": 2.465}, "timestamp": "2026-01-16T16:42:05.869758"}
{"image_index": 436, "image_name": "000000049810.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000049810.jpg", "image_width": 640, "image_height": 424, "image_resolution": "640x424", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1705.178, "latencies_ms": [1705.178], "images_per_second": 0.586, "prompt_tokens": 19, "response_tokens_est": 39, "n_tiles": 6, "output_text": "The image features a ginger and white cat sitting on a wooden deck. The cat's fur appears soft and slightly shiny, and the lighting is natural, casting a warm glow on the scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25597.2, "ram_available_mb": 100175.0, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 25596.9, "ram_available_mb": 100175.2, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.65, "peak": 15.01, "min": 14.02}, "VDD_GPU": {"avg": 30.85, "peak": 38.21, "min": 24.43}, "VIN": {"avg": 67.22, "peak": 110.58, "min": 55.64}, "VDD_CPU_SOC_MSS": {"avg": 15.29, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 30.85, "energy_joules_est": 52.61, "sample_count": 13, "duration_seconds": 1.705}, "timestamp": "2026-01-16T16:42:07.580760"}
{"image_index": 437, "image_name": "000000050006.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050006.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2352.184, "latencies_ms": [2352.184], "images_per_second": 0.425, "prompt_tokens": 9, "response_tokens_est": 28, "n_tiles": 12, "output_text": "A row of boats are moored in a marina by a calm body of water, with a small town visible in the background.", "error": null, "sys_before": {"cpu_percent": 48.6, "ram_used_mb": 25596.9, "ram_available_mb": 100175.2, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 25596.2, "ram_available_mb": 100175.9, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.14, "peak": 16.15, "min": 13.9}, "VDD_GPU": {"avg": 35.26, "peak": 41.36, "min": 27.97}, "VIN": {"avg": 83.75, "peak": 120.37, "min": 58.53}, "VDD_CPU_SOC_MSS": {"avg": 15.85, "peak": 17.32, "min": 14.96}}, "power_watts_avg": 35.26, "energy_joules_est": 82.95, "sample_count": 18, "duration_seconds": 2.353}, "timestamp": "2026-01-16T16:42:10.012256"}
{"image_index": 437, "image_name": "000000050006.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050006.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2872.158, "latencies_ms": [2872.158], "images_per_second": 0.348, "prompt_tokens": 23, "response_tokens_est": 40, "n_tiles": 12, "output_text": "object: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25596.2, "ram_available_mb": 100175.9, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 25595.9, "ram_available_mb": 100176.3, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.08, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 35.25, "peak": 43.33, "min": 26.79}, "VIN": {"avg": 73.37, "peak": 109.39, "min": 56.72}, "VDD_CPU_SOC_MSS": {"avg": 15.69, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 35.25, "energy_joules_est": 101.26, "sample_count": 22, "duration_seconds": 2.873}, "timestamp": "2026-01-16T16:42:12.892500"}
{"image_index": 437, "image_name": "000000050006.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050006.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3624.494, "latencies_ms": [3624.494], "images_per_second": 0.276, "prompt_tokens": 27, "response_tokens_est": 62, "n_tiles": 12, "output_text": "The main objects in the image are a boat and a dock. The boat is located in the foreground, near the dock, and is docked. The dock is situated in the middle ground, extending into the water. The background features a hilly landscape with trees and buildings, creating a scenic view.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.9, "ram_available_mb": 100176.3, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25595.7, "ram_available_mb": 100176.5, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.08, "peak": 15.65, "min": 14.43}, "VDD_GPU": {"avg": 33.09, "peak": 42.54, "min": 26.39}, "VIN": {"avg": 74.16, "peak": 115.45, "min": 58.97}, "VDD_CPU_SOC_MSS": {"avg": 15.66, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 33.09, "energy_joules_est": 119.95, "sample_count": 28, "duration_seconds": 3.625}, "timestamp": "2026-01-16T16:42:16.523410"}
{"image_index": 437, "image_name": "000000050006.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050006.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 4225.574, "latencies_ms": [4225.574], "images_per_second": 0.237, "prompt_tokens": 21, "response_tokens_est": 80, "n_tiles": 12, "output_text": "The image depicts a serene lakeside scene with a calm body of water in the foreground, surrounded by lush greenery and a few buildings. Several boats are moored along the shore, and a lamppost stands prominently in the middle of the water. The overall atmosphere is peaceful and tranquil, with the boats and buildings adding a touch of human presence to the natural setting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.7, "ram_available_mb": 100176.5, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 25596.1, "ram_available_mb": 100176.0, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.09, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 32.1, "peak": 42.54, "min": 26.39}, "VIN": {"avg": 75.87, "peak": 126.22, "min": 58.4}, "VDD_CPU_SOC_MSS": {"avg": 15.73, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 32.1, "energy_joules_est": 135.66, "sample_count": 33, "duration_seconds": 4.226}, "timestamp": "2026-01-16T16:42:20.755239"}
{"image_index": 437, "image_name": "000000050006.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050006.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3479.185, "latencies_ms": [3479.185], "images_per_second": 0.287, "prompt_tokens": 19, "response_tokens_est": 58, "n_tiles": 12, "output_text": "The image depicts a serene lakeside scene with a calm, blue-green water body. The sky is overcast, casting a soft, diffused light over the entire scene. The boats are white, with some featuring red accents, and are moored along the shoreline.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25596.1, "ram_available_mb": 100176.0, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25595.9, "ram_available_mb": 100176.3, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.08, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 33.27, "peak": 42.54, "min": 26.39}, "VIN": {"avg": 70.86, "peak": 107.62, "min": 58.53}, "VDD_CPU_SOC_MSS": {"avg": 15.73, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 33.27, "energy_joules_est": 115.78, "sample_count": 27, "duration_seconds": 3.48}, "timestamp": "2026-01-16T16:42:24.241853"}
{"image_index": 438, "image_name": "000000050145.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050145.jpg", "image_width": 480, "image_height": 320, "image_resolution": "480x320", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1362.309, "latencies_ms": [1362.309], "images_per_second": 0.734, "prompt_tokens": 9, "response_tokens_est": 29, "n_tiles": 6, "output_text": "A man stands in the foreground, shirtless and wearing light-colored pants, holding a bicycle handlebar, with a bicycle parked beside him.", "error": null, "sys_before": {"cpu_percent": 40.9, "ram_used_mb": 25595.9, "ram_available_mb": 100176.3, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25595.9, "ram_available_mb": 100176.3, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.45, "peak": 14.91, "min": 13.92}, "VDD_GPU": {"avg": 32.58, "peak": 37.8, "min": 26.39}, "VIN": {"avg": 81.7, "peak": 124.43, "min": 53.68}, "VDD_CPU_SOC_MSS": {"avg": 15.07, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 32.58, "energy_joules_est": 44.4, "sample_count": 10, "duration_seconds": 1.363}, "timestamp": "2026-01-16T16:42:25.654243"}
{"image_index": 438, "image_name": "000000050145.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050145.jpg", "image_width": 480, "image_height": 320, "image_resolution": "480x320", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2637.864, "latencies_ms": [2637.864], "images_per_second": 0.379, "prompt_tokens": 23, "response_tokens_est": 72, "n_tiles": 6, "output_text": "1. Bicycle: 1\n2. Man: 1\n3. Street sign: 1\n4. Motorcycle: 1\n5. Motorcycle rider: 1\n6. Pedestrian: 1\n7. Pedestrian with umbrella: 1\n8. Pedestrian in a dress: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25595.9, "ram_available_mb": 100176.3, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 25596.1, "ram_available_mb": 100176.0, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.77, "peak": 15.01, "min": 14.02}, "VDD_GPU": {"avg": 28.66, "peak": 39.0, "min": 23.24}, "VIN": {"avg": 67.69, "peak": 116.7, "min": 56.21}, "VDD_CPU_SOC_MSS": {"avg": 15.39, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 28.66, "energy_joules_est": 75.62, "sample_count": 20, "duration_seconds": 2.639}, "timestamp": "2026-01-16T16:42:28.298396"}
{"image_index": 438, "image_name": "000000050145.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050145.jpg", "image_width": 480, "image_height": 320, "image_resolution": "480x320", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2243.033, "latencies_ms": [2243.033], "images_per_second": 0.446, "prompt_tokens": 27, "response_tokens_est": 58, "n_tiles": 6, "output_text": "The main object in the foreground is a man standing next to a bicycle. He is wearing a towel around his waist. In the background, there are various signs and a person riding a bicycle. The signs are located near the man, and the person riding the bicycle is further away.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25596.1, "ram_available_mb": 100176.0, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 25596.1, "ram_available_mb": 100176.1, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.76, "peak": 15.11, "min": 14.02}, "VDD_GPU": {"avg": 29.1, "peak": 38.19, "min": 23.25}, "VIN": {"avg": 71.54, "peak": 112.67, "min": 54.23}, "VDD_CPU_SOC_MSS": {"avg": 15.24, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 29.1, "energy_joules_est": 65.3, "sample_count": 17, "duration_seconds": 2.244}, "timestamp": "2026-01-16T16:42:30.548015"}
{"image_index": 438, "image_name": "000000050145.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050145.jpg", "image_width": 480, "image_height": 320, "image_resolution": "480x320", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2779.424, "latencies_ms": [2779.424], "images_per_second": 0.36, "prompt_tokens": 21, "response_tokens_est": 77, "n_tiles": 6, "output_text": "The image depicts a street scene in an urban area, likely in a developing country given the presence of traditional clothing and the language on the signs. A man stands in the foreground, shirtless and wearing light-colored pants, holding a bicycle. In the background, there are various signs in a non-English language, and a few people can be seen walking or standing around.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25596.1, "ram_available_mb": 100176.1, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 25596.4, "ram_available_mb": 100175.8, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.81, "peak": 15.01, "min": 14.12}, "VDD_GPU": {"avg": 27.93, "peak": 38.19, "min": 23.24}, "VIN": {"avg": 72.18, "peak": 115.77, "min": 58.12}, "VDD_CPU_SOC_MSS": {"avg": 15.41, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 27.93, "energy_joules_est": 77.64, "sample_count": 21, "duration_seconds": 2.78}, "timestamp": "2026-01-16T16:42:33.333822"}
{"image_index": 438, "image_name": "000000050145.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050145.jpg", "image_width": 480, "image_height": 320, "image_resolution": "480x320", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2410.924, "latencies_ms": [2410.924], "images_per_second": 0.415, "prompt_tokens": 19, "response_tokens_est": 64, "n_tiles": 6, "output_text": "The image is a black and white photograph featuring a man standing in front of a store with a sign in Chinese. The man is shirtless, wearing light-colored pants, and holding a bicycle. The store has a signboard with Chinese characters, and there are other indistinct figures and objects in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25596.4, "ram_available_mb": 100175.8, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 25596.6, "ram_available_mb": 100175.6, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.73, "peak": 15.01, "min": 14.02}, "VDD_GPU": {"avg": 28.78, "peak": 37.82, "min": 23.24}, "VIN": {"avg": 68.62, "peak": 109.84, "min": 55.72}, "VDD_CPU_SOC_MSS": {"avg": 15.33, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 28.78, "energy_joules_est": 69.39, "sample_count": 18, "duration_seconds": 2.411}, "timestamp": "2026-01-16T16:42:35.750558"}
{"image_index": 439, "image_name": "000000050149.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050149.jpg", "image_width": 500, "image_height": 376, "image_resolution": "500x376", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2278.576, "latencies_ms": [2278.576], "images_per_second": 0.439, "prompt_tokens": 9, "response_tokens_est": 26, "n_tiles": 12, "output_text": "The image shows a bunch of ripe bananas hanging from a wire, with a few bunches hanging from a wooden pole.", "error": null, "sys_before": {"cpu_percent": 46.2, "ram_used_mb": 25596.4, "ram_available_mb": 100175.8, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 25596.4, "ram_available_mb": 100175.8, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.14, "peak": 16.15, "min": 13.9}, "VDD_GPU": {"avg": 35.55, "peak": 40.97, "min": 28.36}, "VIN": {"avg": 76.43, "peak": 107.26, "min": 59.11}, "VDD_CPU_SOC_MSS": {"avg": 16.04, "peak": 17.32, "min": 15.35}}, "power_watts_avg": 35.55, "energy_joules_est": 81.01, "sample_count": 17, "duration_seconds": 2.279}, "timestamp": "2026-01-16T16:42:38.109300"}
{"image_index": 439, "image_name": "000000050149.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050149.jpg", "image_width": 500, "image_height": 376, "image_resolution": "500x376", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 5862.404, "latencies_ms": [5862.404], "images_per_second": 0.171, "prompt_tokens": 23, "response_tokens_est": 129, "n_tiles": 12, "output_text": "bananas: 20\nbottle-water: 1\nbucket: 1\ndoor: 1\ndoorframe: 1\ndoorframe: 1\ndoorframe: 1\ndoorframe: 1\ndoorframe: 1\ndoorframe: 1\ndoorframe: 1\ndoorframe: 1\ndoorframe: 1\ndoorframe: 1\ndoorframe: 1\ndoorframe: 1\ndoorframe: 1\ndoorframe: 1\ndoorframe: 1\ndoorframe: 1\ndoorframe: 1\ndoorframe", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25596.4, "ram_available_mb": 100175.8, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 25596.4, "ram_available_mb": 100175.8, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.09, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 30.52, "peak": 43.33, "min": 26.0}, "VIN": {"avg": 70.87, "peak": 113.16, "min": 51.35}, "VDD_CPU_SOC_MSS": {"avg": 15.58, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 30.52, "energy_joules_est": 178.94, "sample_count": 46, "duration_seconds": 5.863}, "timestamp": "2026-01-16T16:42:43.978124"}
{"image_index": 439, "image_name": "000000050149.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050149.jpg", "image_width": 500, "image_height": 376, "image_resolution": "500x376", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3289.247, "latencies_ms": [3289.247], "images_per_second": 0.304, "prompt_tokens": 27, "response_tokens_est": 52, "n_tiles": 12, "output_text": "The bananas are hanging from a wire or string, with some bunches closer to the foreground and others further back. The basket containing the bananas is placed near the left side of the image, while the kitchen sink is located near the right side.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25596.4, "ram_available_mb": 100175.8, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25596.6, "ram_available_mb": 100175.6, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.09, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 33.92, "peak": 42.94, "min": 26.39}, "VIN": {"avg": 72.18, "peak": 117.37, "min": 58.57}, "VDD_CPU_SOC_MSS": {"avg": 15.6, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 33.92, "energy_joules_est": 111.58, "sample_count": 25, "duration_seconds": 3.29}, "timestamp": "2026-01-16T16:42:47.274492"}
{"image_index": 439, "image_name": "000000050149.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050149.jpg", "image_width": 500, "image_height": 376, "image_resolution": "500x376", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3755.941, "latencies_ms": [3755.941], "images_per_second": 0.266, "prompt_tokens": 21, "response_tokens_est": 66, "n_tiles": 12, "output_text": "The image depicts a market stall with a large number of ripe, yellow bananas hanging from a wire. The bananas are displayed in a neat and organized manner, with some hanging from hooks and others hanging freely. The stall appears to be located in a small, rustic market setting, with a simple and functional design.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25596.6, "ram_available_mb": 100175.6, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25596.3, "ram_available_mb": 100175.8, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.08, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 32.8, "peak": 42.94, "min": 26.0}, "VIN": {"avg": 74.31, "peak": 113.15, "min": 58.64}, "VDD_CPU_SOC_MSS": {"avg": 15.62, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 32.8, "energy_joules_est": 123.21, "sample_count": 29, "duration_seconds": 3.756}, "timestamp": "2026-01-16T16:42:51.036631"}
{"image_index": 439, "image_name": "000000050149.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050149.jpg", "image_width": 500, "image_height": 376, "image_resolution": "500x376", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3052.784, "latencies_ms": [3052.784], "images_per_second": 0.328, "prompt_tokens": 19, "response_tokens_est": 45, "n_tiles": 12, "output_text": "The image showcases a bunch of ripe, yellow bananas hanging from a wire, with a rustic, weathered look. The lighting is natural, casting soft shadows and highlighting the vibrant yellow color of the bananas.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25596.3, "ram_available_mb": 100175.8, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 25596.6, "ram_available_mb": 100175.6, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.05, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 34.33, "peak": 42.94, "min": 26.39}, "VIN": {"avg": 74.72, "peak": 117.87, "min": 58.62}, "VDD_CPU_SOC_MSS": {"avg": 15.65, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 34.33, "energy_joules_est": 104.81, "sample_count": 24, "duration_seconds": 3.053}, "timestamp": "2026-01-16T16:42:54.095881"}
{"image_index": 440, "image_name": "000000050165.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050165.jpg", "image_width": 640, "image_height": 431, "image_resolution": "640x431", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1261.093, "latencies_ms": [1261.093], "images_per_second": 0.793, "prompt_tokens": 9, "response_tokens_est": 25, "n_tiles": 6, "output_text": "A vintage green and white train is traveling on a track through a rural landscape with green fields and hills in the background.", "error": null, "sys_before": {"cpu_percent": 39.5, "ram_used_mb": 25596.6, "ram_available_mb": 100175.6, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25596.6, "ram_available_mb": 100175.6, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.29, "peak": 14.81, "min": 13.82}, "VDD_GPU": {"avg": 33.0, "peak": 37.42, "min": 27.18}, "VIN": {"avg": 75.44, "peak": 109.59, "min": 49.87}, "VDD_CPU_SOC_MSS": {"avg": 14.91, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 33.0, "energy_joules_est": 41.64, "sample_count": 9, "duration_seconds": 1.262}, "timestamp": "2026-01-16T16:42:55.427433"}
{"image_index": 440, "image_name": "000000050165.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050165.jpg", "image_width": 640, "image_height": 431, "image_resolution": "640x431", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1703.463, "latencies_ms": [1703.463], "images_per_second": 0.587, "prompt_tokens": 23, "response_tokens_est": 40, "n_tiles": 6, "output_text": "object: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25596.6, "ram_available_mb": 100175.6, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 25596.3, "ram_available_mb": 100175.8, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.75, "peak": 15.11, "min": 14.22}, "VDD_GPU": {"avg": 31.63, "peak": 38.59, "min": 24.82}, "VIN": {"avg": 64.72, "peak": 72.85, "min": 56.22}, "VDD_CPU_SOC_MSS": {"avg": 15.38, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 31.63, "energy_joules_est": 53.89, "sample_count": 13, "duration_seconds": 1.704}, "timestamp": "2026-01-16T16:42:57.137038"}
{"image_index": 440, "image_name": "000000050165.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050165.jpg", "image_width": 640, "image_height": 431, "image_resolution": "640x431", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2664.108, "latencies_ms": [2664.108], "images_per_second": 0.375, "prompt_tokens": 27, "response_tokens_est": 73, "n_tiles": 6, "output_text": "The main object in the foreground is a green and white train car, which is positioned near the middle of the image. The train car is connected to a series of red freight cars that are positioned further back on the tracks. The background features a lush green field and a line of trees, while a distant mountain range is visible under a partly cloudy sky.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25596.3, "ram_available_mb": 100175.8, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 25596.6, "ram_available_mb": 100175.6, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.8, "peak": 15.01, "min": 14.12}, "VDD_GPU": {"avg": 28.36, "peak": 38.59, "min": 23.24}, "VIN": {"avg": 69.86, "peak": 107.4, "min": 62.23}, "VDD_CPU_SOC_MSS": {"avg": 15.37, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 28.36, "energy_joules_est": 75.56, "sample_count": 20, "duration_seconds": 2.664}, "timestamp": "2026-01-16T16:42:59.807326"}
{"image_index": 440, "image_name": "000000050165.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050165.jpg", "image_width": 640, "image_height": 431, "image_resolution": "640x431", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2524.521, "latencies_ms": [2524.521], "images_per_second": 0.396, "prompt_tokens": 21, "response_tokens_est": 68, "n_tiles": 6, "output_text": "The image depicts a serene countryside scene with a train traveling on a track through a lush green field. The train, featuring a green and white color scheme, is accompanied by a series of red and brown freight cars. The landscape is dotted with trees and a distant hill, and the sky is clear with a few scattered clouds.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25596.6, "ram_available_mb": 100175.6, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 25596.3, "ram_available_mb": 100175.8, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.76, "peak": 15.01, "min": 14.02}, "VDD_GPU": {"avg": 28.42, "peak": 37.82, "min": 23.24}, "VIN": {"avg": 69.03, "peak": 117.44, "min": 57.37}, "VDD_CPU_SOC_MSS": {"avg": 15.29, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 28.42, "energy_joules_est": 71.76, "sample_count": 19, "duration_seconds": 2.525}, "timestamp": "2026-01-16T16:43:02.338159"}
{"image_index": 440, "image_name": "000000050165.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050165.jpg", "image_width": 640, "image_height": 431, "image_resolution": "640x431", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1627.078, "latencies_ms": [1627.078], "images_per_second": 0.615, "prompt_tokens": 19, "response_tokens_est": 36, "n_tiles": 6, "output_text": "The image depicts a green and white train moving on a track through a rural landscape. The sky is clear with a few clouds, and the lighting suggests it is daytime.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25596.3, "ram_available_mb": 100175.8, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 25596.6, "ram_available_mb": 100175.6, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.6, "peak": 14.91, "min": 14.02}, "VDD_GPU": {"avg": 31.38, "peak": 37.8, "min": 24.82}, "VIN": {"avg": 71.96, "peak": 110.45, "min": 62.53}, "VDD_CPU_SOC_MSS": {"avg": 15.19, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 31.38, "energy_joules_est": 51.07, "sample_count": 12, "duration_seconds": 1.628}, "timestamp": "2026-01-16T16:43:03.971443"}
{"image_index": 441, "image_name": "000000050326.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050326.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2331.597, "latencies_ms": [2331.597], "images_per_second": 0.429, "prompt_tokens": 9, "response_tokens_est": 28, "n_tiles": 12, "output_text": "A man is standing on a sandy beach, holding a kite in his hands, with the ocean waves crashing in the background.", "error": null, "sys_before": {"cpu_percent": 45.9, "ram_used_mb": 25596.6, "ram_available_mb": 100175.6, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 25596.6, "ram_available_mb": 100175.6, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.88, "peak": 15.85, "min": 13.79}, "VDD_GPU": {"avg": 35.27, "peak": 40.97, "min": 27.57}, "VIN": {"avg": 76.79, "peak": 122.72, "min": 57.99}, "VDD_CPU_SOC_MSS": {"avg": 15.77, "peak": 16.92, "min": 14.96}}, "power_watts_avg": 35.27, "energy_joules_est": 82.25, "sample_count": 18, "duration_seconds": 2.332}, "timestamp": "2026-01-16T16:43:06.396444"}
{"image_index": 441, "image_name": "000000050326.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050326.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 3022.846, "latencies_ms": [3022.846], "images_per_second": 0.331, "prompt_tokens": 23, "response_tokens_est": 45, "n_tiles": 12, "output_text": "beach: 1\nman: 1\nhat: 1\nshorts: 1\nshirt: 1\nwristband: 1\ncamera: 1\nkite: 1", "error": null, "sys_before": {"cpu_percent": 16.7, "ram_used_mb": 25596.6, "ram_available_mb": 100175.6, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 25596.6, "ram_available_mb": 100175.6, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.82, "peak": 15.34, "min": 14.12}, "VDD_GPU": {"avg": 34.73, "peak": 42.94, "min": 26.39}, "VIN": {"avg": 74.3, "peak": 113.77, "min": 58.68}, "VDD_CPU_SOC_MSS": {"avg": 15.34, "peak": 16.14, "min": 14.56}}, "power_watts_avg": 34.73, "energy_joules_est": 105.0, "sample_count": 23, "duration_seconds": 3.023}, "timestamp": "2026-01-16T16:43:09.425823"}
{"image_index": 441, "image_name": "000000050326.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050326.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3606.274, "latencies_ms": [3606.274], "images_per_second": 0.277, "prompt_tokens": 27, "response_tokens_est": 62, "n_tiles": 12, "output_text": "The main object in the foreground is a person standing on the sandy beach. The person is wearing a black shirt and khaki shorts, and they are holding a white object in their right hand. The background features the ocean with waves crashing onto the shore, and a green surfboard is partially visible.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25596.6, "ram_available_mb": 100175.6, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25596.6, "ram_available_mb": 100175.6, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.81, "peak": 15.34, "min": 14.12}, "VDD_GPU": {"avg": 33.04, "peak": 42.53, "min": 26.0}, "VIN": {"avg": 74.81, "peak": 145.97, "min": 59.08}, "VDD_CPU_SOC_MSS": {"avg": 15.45, "peak": 16.14, "min": 14.96}}, "power_watts_avg": 33.04, "energy_joules_est": 119.16, "sample_count": 28, "duration_seconds": 3.607}, "timestamp": "2026-01-16T16:43:13.038587"}
{"image_index": 441, "image_name": "000000050326.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050326.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2700.409, "latencies_ms": [2700.409], "images_per_second": 0.37, "prompt_tokens": 21, "response_tokens_est": 35, "n_tiles": 12, "output_text": "The image depicts a person standing on a sandy beach with the ocean in the background. The person is holding a kite, which is flying in the air.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25596.6, "ram_available_mb": 100175.6, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 25596.6, "ram_available_mb": 100175.6, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.8, "peak": 15.34, "min": 14.12}, "VDD_GPU": {"avg": 35.37, "peak": 42.13, "min": 27.18}, "VIN": {"avg": 78.39, "peak": 116.81, "min": 58.88}, "VDD_CPU_SOC_MSS": {"avg": 15.39, "peak": 16.14, "min": 14.56}}, "power_watts_avg": 35.37, "energy_joules_est": 95.53, "sample_count": 20, "duration_seconds": 2.701}, "timestamp": "2026-01-16T16:43:15.745492"}
{"image_index": 441, "image_name": "000000050326.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050326.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3428.878, "latencies_ms": [3428.878], "images_per_second": 0.292, "prompt_tokens": 19, "response_tokens_est": 57, "n_tiles": 12, "output_text": "The image shows a person standing on a sandy beach with the ocean in the background. The person is wearing a black shirt, khaki shorts, and black sandals. The sky is partly cloudy, and the lighting is bright, indicating it is likely a sunny day.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25596.6, "ram_available_mb": 100175.6, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25596.6, "ram_available_mb": 100175.6, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.83, "peak": 15.34, "min": 14.22}, "VDD_GPU": {"avg": 33.48, "peak": 42.54, "min": 26.0}, "VIN": {"avg": 73.71, "peak": 120.36, "min": 58.64}, "VDD_CPU_SOC_MSS": {"avg": 15.41, "peak": 16.14, "min": 14.56}}, "power_watts_avg": 33.48, "energy_joules_est": 114.81, "sample_count": 26, "duration_seconds": 3.429}, "timestamp": "2026-01-16T16:43:19.181046"}
{"image_index": 442, "image_name": "000000050331.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050331.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2623.505, "latencies_ms": [2623.505], "images_per_second": 0.381, "prompt_tokens": 9, "response_tokens_est": 36, "n_tiles": 12, "output_text": "The image shows a close-up view of a garden bed containing a variety of leafy vegetables, including broccoli and kale, which are growing in terracotta pots.", "error": null, "sys_before": {"cpu_percent": 41.6, "ram_used_mb": 25596.6, "ram_available_mb": 100175.6, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 25596.5, "ram_available_mb": 100175.6, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.06, "peak": 16.05, "min": 13.49}, "VDD_GPU": {"avg": 34.47, "peak": 41.76, "min": 27.18}, "VIN": {"avg": 77.57, "peak": 108.24, "min": 58.44}, "VDD_CPU_SOC_MSS": {"avg": 15.84, "peak": 16.92, "min": 14.57}}, "power_watts_avg": 34.47, "energy_joules_est": 90.45, "sample_count": 20, "duration_seconds": 2.624}, "timestamp": "2026-01-16T16:43:21.898200"}
{"image_index": 442, "image_name": "000000050331.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050331.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2105.903, "latencies_ms": [2105.903], "images_per_second": 0.475, "prompt_tokens": 23, "response_tokens_est": 17, "n_tiles": 12, "output_text": "broccoli: 1\npot: 1\npot plant: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25596.5, "ram_available_mb": 100175.6, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 2.3, "ram_used_mb": 25596.5, "ram_available_mb": 100175.6, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.11, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 37.79, "peak": 42.94, "min": 30.33}, "VIN": {"avg": 83.05, "peak": 115.56, "min": 58.54}, "VDD_CPU_SOC_MSS": {"avg": 15.77, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 37.79, "energy_joules_est": 79.6, "sample_count": 16, "duration_seconds": 2.106}, "timestamp": "2026-01-16T16:43:24.010330"}
{"image_index": 442, "image_name": "000000050331.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050331.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3960.047, "latencies_ms": [3960.047], "images_per_second": 0.253, "prompt_tokens": 27, "response_tokens_est": 72, "n_tiles": 12, "output_text": "The main objects in the image are a variety of leafy greens, including broccoli and kale, which are growing in a garden bed. The broccoli is in the foreground, while the kale is in the background. The garden bed is located near the center of the image, with the soil and plants occupying the majority of the frame.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25596.5, "ram_available_mb": 100175.6, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25596.8, "ram_available_mb": 100175.4, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.07, "peak": 15.65, "min": 14.32}, "VDD_GPU": {"avg": 32.79, "peak": 43.71, "min": 26.39}, "VIN": {"avg": 71.99, "peak": 113.98, "min": 56.71}, "VDD_CPU_SOC_MSS": {"avg": 15.67, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 32.79, "energy_joules_est": 129.88, "sample_count": 31, "duration_seconds": 3.961}, "timestamp": "2026-01-16T16:43:27.977528"}
{"image_index": 442, "image_name": "000000050331.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050331.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3382.27, "latencies_ms": [3382.27], "images_per_second": 0.296, "prompt_tokens": 21, "response_tokens_est": 55, "n_tiles": 12, "output_text": "The image depicts a garden scene with a variety of leafy greens, including broccoli and kale, growing in terracotta pots. The setting appears to be outdoors, with the soil and plants visible, suggesting a well-maintained and cared-for garden.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25596.8, "ram_available_mb": 100175.4, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 25596.8, "ram_available_mb": 100175.4, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.09, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 33.74, "peak": 42.94, "min": 26.39}, "VIN": {"avg": 71.77, "peak": 101.14, "min": 57.94}, "VDD_CPU_SOC_MSS": {"avg": 15.7, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 33.74, "energy_joules_est": 114.13, "sample_count": 26, "duration_seconds": 3.383}, "timestamp": "2026-01-16T16:43:31.370585"}
{"image_index": 442, "image_name": "000000050331.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050331.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3985.562, "latencies_ms": [3985.562], "images_per_second": 0.251, "prompt_tokens": 19, "response_tokens_est": 73, "n_tiles": 12, "output_text": "The image shows a garden bed with a variety of leafy greens, including a large, green broccoli head and other leafy vegetables. The plants are growing in a terracotta pot, and the soil appears to be well-tilled and moist. The lighting is natural, suggesting it is daytime, and the weather seems to be clear and sunny.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25596.8, "ram_available_mb": 100175.4, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25600.8, "ram_available_mb": 100171.4, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.08, "peak": 15.65, "min": 14.43}, "VDD_GPU": {"avg": 32.48, "peak": 42.53, "min": 26.39}, "VIN": {"avg": 72.13, "peak": 109.05, "min": 57.95}, "VDD_CPU_SOC_MSS": {"avg": 15.66, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 32.48, "energy_joules_est": 129.48, "sample_count": 31, "duration_seconds": 3.986}, "timestamp": "2026-01-16T16:43:35.364256"}
{"image_index": 443, "image_name": "000000050380.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050380.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1364.782, "latencies_ms": [1364.782], "images_per_second": 0.733, "prompt_tokens": 9, "response_tokens_est": 29, "n_tiles": 6, "output_text": "An elderly man is holding a young boy's hand while walking beside a small, light-colored horse, which is wearing a red halter.", "error": null, "sys_before": {"cpu_percent": 41.3, "ram_used_mb": 25600.8, "ram_available_mb": 100171.4, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 25600.5, "ram_available_mb": 100171.6, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.41, "peak": 15.01, "min": 13.82}, "VDD_GPU": {"avg": 32.26, "peak": 37.8, "min": 26.39}, "VIN": {"avg": 74.25, "peak": 112.13, "min": 50.18}, "VDD_CPU_SOC_MSS": {"avg": 15.08, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 32.26, "energy_joules_est": 44.05, "sample_count": 10, "duration_seconds": 1.365}, "timestamp": "2026-01-16T16:43:36.789683"}
{"image_index": 443, "image_name": "000000050380.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050380.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1674.022, "latencies_ms": [1674.022], "images_per_second": 0.597, "prompt_tokens": 23, "response_tokens_est": 39, "n_tiles": 6, "output_text": "1. Man\n2. Boy\n3. Horse\n4. Pony\n5. Ponytail\n6. Shirt\n7. Pants\n8. Lifeguard", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25600.5, "ram_available_mb": 100171.6, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 25600.5, "ram_available_mb": 100171.6, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.78, "peak": 15.11, "min": 14.22}, "VDD_GPU": {"avg": 31.69, "peak": 38.59, "min": 24.82}, "VIN": {"avg": 76.6, "peak": 123.68, "min": 62.34}, "VDD_CPU_SOC_MSS": {"avg": 15.44, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 31.69, "energy_joules_est": 53.06, "sample_count": 13, "duration_seconds": 1.674}, "timestamp": "2026-01-16T16:43:38.469856"}
{"image_index": 443, "image_name": "000000050380.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050380.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2660.618, "latencies_ms": [2660.618], "images_per_second": 0.376, "prompt_tokens": 27, "response_tokens_est": 76, "n_tiles": 6, "output_text": "In the image, the elderly man is standing in the foreground, holding a blue leash attached to a small, light-colored horse. The horse is positioned near the man, with its head turned slightly towards the camera. The background features a red building with white trim, a window, and a door, as well as a small white lantern and a paved area.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25600.5, "ram_available_mb": 100171.6, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 25600.5, "ram_available_mb": 100171.7, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.0, "peak": 15.32, "min": 14.22}, "VDD_GPU": {"avg": 28.66, "peak": 38.59, "min": 23.64}, "VIN": {"avg": 69.66, "peak": 112.96, "min": 55.82}, "VDD_CPU_SOC_MSS": {"avg": 15.65, "peak": 16.14, "min": 14.96}}, "power_watts_avg": 28.66, "energy_joules_est": 76.27, "sample_count": 21, "duration_seconds": 2.661}, "timestamp": "2026-01-16T16:43:41.136766"}
{"image_index": 443, "image_name": "000000050380.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050380.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2556.97, "latencies_ms": [2556.97], "images_per_second": 0.391, "prompt_tokens": 21, "response_tokens_est": 72, "n_tiles": 6, "output_text": "The image depicts an outdoor scene with an elderly man and a young boy standing beside a small, light-colored horse. The man is holding the horse's reins, while the boy is looking down at the horse. The setting appears to be a quaint, possibly rural area, with a red building and a cobblestone path in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25600.5, "ram_available_mb": 100171.7, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 25600.8, "ram_available_mb": 100171.4, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.99, "peak": 15.32, "min": 14.32}, "VDD_GPU": {"avg": 28.74, "peak": 38.21, "min": 23.64}, "VIN": {"avg": 66.86, "peak": 117.61, "min": 52.47}, "VDD_CPU_SOC_MSS": {"avg": 15.65, "peak": 16.14, "min": 14.96}}, "power_watts_avg": 28.74, "energy_joules_est": 73.5, "sample_count": 20, "duration_seconds": 2.557}, "timestamp": "2026-01-16T16:43:43.699778"}
{"image_index": 443, "image_name": "000000050380.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050380.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1977.692, "latencies_ms": [1977.692], "images_per_second": 0.506, "prompt_tokens": 19, "response_tokens_est": 51, "n_tiles": 6, "output_text": "The image features a young boy with light-colored hair and a gray shirt, standing beside a light brown horse with a white blaze. The horse is wearing a red halter. The scene is set outdoors during the daytime under clear, bright lighting.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25600.8, "ram_available_mb": 100171.4, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 25600.5, "ram_available_mb": 100171.7, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.94, "peak": 15.32, "min": 14.32}, "VDD_GPU": {"avg": 30.23, "peak": 37.82, "min": 24.42}, "VIN": {"avg": 70.36, "peak": 112.31, "min": 57.63}, "VDD_CPU_SOC_MSS": {"avg": 15.56, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 30.23, "energy_joules_est": 59.79, "sample_count": 15, "duration_seconds": 1.978}, "timestamp": "2026-01-16T16:43:45.683454"}
{"image_index": 444, "image_name": "000000050638.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050638.jpg", "image_width": 640, "image_height": 393, "image_resolution": "640x393", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1270.716, "latencies_ms": [1270.716], "images_per_second": 0.787, "prompt_tokens": 9, "response_tokens_est": 26, "n_tiles": 6, "output_text": "A young child with light-colored hair is holding a brown stuffed animal while walking through a field of tall grass and blue flowers.", "error": null, "sys_before": {"cpu_percent": 43.9, "ram_used_mb": 25600.5, "ram_available_mb": 100171.7, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25600.5, "ram_available_mb": 100171.7, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.51, "peak": 15.01, "min": 13.92}, "VDD_GPU": {"avg": 32.74, "peak": 37.42, "min": 27.18}, "VIN": {"avg": 76.89, "peak": 106.26, "min": 58.01}, "VDD_CPU_SOC_MSS": {"avg": 15.27, "peak": 15.75, "min": 14.57}}, "power_watts_avg": 32.74, "energy_joules_est": 41.62, "sample_count": 9, "duration_seconds": 1.271}, "timestamp": "2026-01-16T16:43:47.003686"}
{"image_index": 444, "image_name": "000000050638.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050638.jpg", "image_width": 640, "image_height": 393, "image_resolution": "640x393", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2078.79, "latencies_ms": [2078.79], "images_per_second": 0.481, "prompt_tokens": 23, "response_tokens_est": 53, "n_tiles": 6, "output_text": "- child: 1\n- teddy bear: 1\n- blue flowers: 1\n- green grass: 1\n- dirt path: 1\n- tree: 1\n- bush: 1\n- sky: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25600.5, "ram_available_mb": 100171.7, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 25600.8, "ram_available_mb": 100171.4, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.77, "peak": 15.11, "min": 14.12}, "VDD_GPU": {"avg": 30.49, "peak": 39.0, "min": 24.03}, "VIN": {"avg": 70.22, "peak": 104.29, "min": 56.22}, "VDD_CPU_SOC_MSS": {"avg": 15.35, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 30.49, "energy_joules_est": 63.4, "sample_count": 15, "duration_seconds": 2.079}, "timestamp": "2026-01-16T16:43:49.088591"}
{"image_index": 444, "image_name": "000000050638.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050638.jpg", "image_width": 640, "image_height": 393, "image_resolution": "640x393", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2164.822, "latencies_ms": [2164.822], "images_per_second": 0.462, "prompt_tokens": 27, "response_tokens_est": 56, "n_tiles": 6, "output_text": "The main object in the foreground is a young child holding a brown teddy bear. The child is standing on a dirt path surrounded by lush green grass and tall purple flowers. The background features a blurred field of flowers and greenery, indicating a natural, outdoor setting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25600.8, "ram_available_mb": 100171.4, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 25600.5, "ram_available_mb": 100171.7, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.79, "peak": 15.11, "min": 14.12}, "VDD_GPU": {"avg": 29.79, "peak": 38.21, "min": 23.64}, "VIN": {"avg": 66.99, "peak": 88.56, "min": 56.3}, "VDD_CPU_SOC_MSS": {"avg": 15.38, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 29.79, "energy_joules_est": 64.5, "sample_count": 16, "duration_seconds": 2.165}, "timestamp": "2026-01-16T16:43:51.259786"}
{"image_index": 444, "image_name": "000000050638.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050638.jpg", "image_width": 640, "image_height": 393, "image_resolution": "640x393", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2936.969, "latencies_ms": [2936.969], "images_per_second": 0.34, "prompt_tokens": 21, "response_tokens_est": 84, "n_tiles": 6, "output_text": "The image depicts a young child, likely a boy, standing in a lush, green field filled with tall grass and purple flowers. The child is holding a brown teddy bear in his arms, and he is wearing a striped blue and white shirt and blue jeans. The setting appears to be a park or a garden, with sunlight filtering through the foliage, creating a serene and peaceful atmosphere.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25600.5, "ram_available_mb": 100171.7, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 25600.5, "ram_available_mb": 100171.7, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.9, "peak": 15.22, "min": 14.12}, "VDD_GPU": {"avg": 27.99, "peak": 38.21, "min": 23.24}, "VIN": {"avg": 68.85, "peak": 113.78, "min": 56.5}, "VDD_CPU_SOC_MSS": {"avg": 15.48, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 27.99, "energy_joules_est": 82.21, "sample_count": 22, "duration_seconds": 2.937}, "timestamp": "2026-01-16T16:43:54.202669"}
{"image_index": 444, "image_name": "000000050638.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050638.jpg", "image_width": 640, "image_height": 393, "image_resolution": "640x393", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2276.727, "latencies_ms": [2276.727], "images_per_second": 0.439, "prompt_tokens": 19, "response_tokens_est": 60, "n_tiles": 6, "output_text": "The image features a young child with light-colored hair, wearing a blue and white striped shirt and blue jeans. The child is holding a brown teddy bear. The scene is set in a lush, green environment with sunlight filtering through the foliage, creating a warm and inviting atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25600.5, "ram_available_mb": 100171.7, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 25600.5, "ram_available_mb": 100171.7, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.8, "peak": 15.11, "min": 14.12}, "VDD_GPU": {"avg": 29.29, "peak": 38.19, "min": 23.64}, "VIN": {"avg": 72.65, "peak": 116.36, "min": 57.61}, "VDD_CPU_SOC_MSS": {"avg": 15.4, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 29.29, "energy_joules_est": 66.7, "sample_count": 17, "duration_seconds": 2.277}, "timestamp": "2026-01-16T16:43:56.485964"}
{"image_index": 445, "image_name": "000000050679.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050679.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1246.737, "latencies_ms": [1246.737], "images_per_second": 0.802, "prompt_tokens": 9, "response_tokens_est": 25, "n_tiles": 6, "output_text": "A large, ripe orange sits on the asphalt of a parking lot, surrounded by various cars and trees in the background.", "error": null, "sys_before": {"cpu_percent": 43.8, "ram_used_mb": 25600.5, "ram_available_mb": 100171.7, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25600.7, "ram_available_mb": 100171.4, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.44, "peak": 14.91, "min": 13.92}, "VDD_GPU": {"avg": 32.48, "peak": 37.42, "min": 27.18}, "VIN": {"avg": 67.32, "peak": 74.02, "min": 62.58}, "VDD_CPU_SOC_MSS": {"avg": 15.18, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 32.48, "energy_joules_est": 40.5, "sample_count": 9, "duration_seconds": 1.247}, "timestamp": "2026-01-16T16:43:57.781334"}
{"image_index": 445, "image_name": "000000050679.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050679.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1472.969, "latencies_ms": [1472.969], "images_per_second": 0.679, "prompt_tokens": 23, "response_tokens_est": 32, "n_tiles": 6, "output_text": "1. Orange\n2. Car\n3. Car\n4. Car\n5. Car\n6. Car\n7. Car\n8. Car", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25600.7, "ram_available_mb": 100171.4, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 25600.5, "ram_available_mb": 100171.7, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.79, "peak": 15.22, "min": 14.22}, "VDD_GPU": {"avg": 32.91, "peak": 39.0, "min": 26.0}, "VIN": {"avg": 75.36, "peak": 116.54, "min": 60.65}, "VDD_CPU_SOC_MSS": {"avg": 15.42, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 32.91, "energy_joules_est": 48.49, "sample_count": 11, "duration_seconds": 1.473}, "timestamp": "2026-01-16T16:43:59.260491"}
{"image_index": 445, "image_name": "000000050679.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050679.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3074.859, "latencies_ms": [3074.859], "images_per_second": 0.325, "prompt_tokens": 27, "response_tokens_est": 89, "n_tiles": 6, "output_text": "The main object in the foreground is an orange lying on the road. The orange is positioned near the center of the image, slightly to the left. In the background, there are several cars parked along the side of the road. The cars are parked in a line, with the closest one to the orange and the farthest one to the right. The orange is the focal point of the image, with the cars serving as a backdrop.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25600.5, "ram_available_mb": 100171.7, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 25600.7, "ram_available_mb": 100171.4, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.92, "peak": 15.11, "min": 14.12}, "VDD_GPU": {"avg": 27.85, "peak": 38.98, "min": 23.24}, "VIN": {"avg": 73.14, "peak": 122.1, "min": 60.86}, "VDD_CPU_SOC_MSS": {"avg": 15.48, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 27.85, "energy_joules_est": 85.65, "sample_count": 24, "duration_seconds": 3.075}, "timestamp": "2026-01-16T16:44:02.341614"}
{"image_index": 445, "image_name": "000000050679.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050679.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2687.506, "latencies_ms": [2687.506], "images_per_second": 0.372, "prompt_tokens": 21, "response_tokens_est": 75, "n_tiles": 6, "output_text": "The image depicts a scene on a city street with a large, ripe orange lying on the asphalt. The orange is positioned in the foreground, while a line of parked cars can be seen in the background, suggesting a parking lot or a street with vehicles. The setting appears to be during the daytime, with overcast skies and a few trees visible in the distance.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25600.7, "ram_available_mb": 100171.4, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 25600.7, "ram_available_mb": 100171.4, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.87, "peak": 15.22, "min": 14.12}, "VDD_GPU": {"avg": 28.32, "peak": 38.19, "min": 23.24}, "VIN": {"avg": 69.35, "peak": 111.69, "min": 57.45}, "VDD_CPU_SOC_MSS": {"avg": 15.43, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 28.32, "energy_joules_est": 76.12, "sample_count": 20, "duration_seconds": 2.688}, "timestamp": "2026-01-16T16:44:05.040342"}
{"image_index": 445, "image_name": "000000050679.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050679.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2489.733, "latencies_ms": [2489.733], "images_per_second": 0.402, "prompt_tokens": 19, "response_tokens_est": 68, "n_tiles": 6, "output_text": "The image features a large, ripe orange lying on the asphalt road. The orange has a vibrant orange color, with a slightly textured surface that suggests it may be slightly weathered. The lighting in the image is natural, with soft shadows cast by the orange, indicating that the photo was taken during the day under overcast skies.", "error": null, "sys_before": {"cpu_percent": 16.7, "ram_used_mb": 25600.7, "ram_available_mb": 100171.4, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 25600.5, "ram_available_mb": 100171.7, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.83, "peak": 15.11, "min": 14.12}, "VDD_GPU": {"avg": 28.63, "peak": 37.8, "min": 23.25}, "VIN": {"avg": 68.11, "peak": 115.66, "min": 56.99}, "VDD_CPU_SOC_MSS": {"avg": 15.44, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 28.63, "energy_joules_est": 71.29, "sample_count": 19, "duration_seconds": 2.49}, "timestamp": "2026-01-16T16:44:07.536549"}
{"image_index": 446, "image_name": "000000050811.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050811.jpg", "image_width": 640, "image_height": 605, "image_resolution": "640x605", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1577.572, "latencies_ms": [1577.572], "images_per_second": 0.634, "prompt_tokens": 9, "response_tokens_est": 70, "n_tiles": 1, "output_text": "A man in a gray suit and white shirt sits at a wooden table, smiling at the camera, with a white bowl containing a white substance on the table in front of him. On the table, there are two bottles of beer, one with a brown label and the other with a clear label, and a pair of keys on the table.", "error": null, "sys_before": {"cpu_percent": 25.9, "ram_used_mb": 25600.5, "ram_available_mb": 100171.7, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25600.7, "ram_available_mb": 100171.5, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 4845.7, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.51, "peak": 15.82, "min": 14.71}, "VDD_GPU": {"avg": 21.67, "peak": 25.6, "min": 20.1}, "VIN": {"avg": 65.29, "peak": 73.68, "min": 61.74}, "VDD_CPU_SOC_MSS": {"avg": 16.31, "peak": 16.93, "min": 15.36}}, "power_watts_avg": 21.67, "energy_joules_est": 34.19, "sample_count": 12, "duration_seconds": 1.578}, "timestamp": "2026-01-16T16:44:09.136704"}
{"image_index": 446, "image_name": "000000050811.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050811.jpg", "image_width": 640, "image_height": 605, "image_resolution": "640x605", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1098.059, "latencies_ms": [1098.059], "images_per_second": 0.911, "prompt_tokens": 23, "response_tokens_est": 48, "n_tiles": 1, "output_text": "- glass: 2\n- bottle: 2\n- bowl: 1\n- table: 1\n- man: 1\n- suit: 1\n- tie: 1\n- shirt: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25600.7, "ram_available_mb": 100171.5, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25600.7, "ram_available_mb": 100171.5, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 4856.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.65, "peak": 15.82, "min": 15.32}, "VDD_GPU": {"avg": 21.87, "peak": 24.03, "min": 20.49}, "VIN": {"avg": 64.48, "peak": 65.93, "min": 61.42}, "VDD_CPU_SOC_MSS": {"avg": 16.59, "peak": 16.93, "min": 16.14}}, "power_watts_avg": 21.87, "energy_joules_est": 24.02, "sample_count": 8, "duration_seconds": 1.098}, "timestamp": "2026-01-16T16:44:10.240661"}
{"image_index": 446, "image_name": "000000050811.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050811.jpg", "image_width": 640, "image_height": 605, "image_resolution": "640x605", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1915.508, "latencies_ms": [1915.508], "images_per_second": 0.522, "prompt_tokens": 27, "response_tokens_est": 86, "n_tiles": 1, "output_text": "The main object in the foreground is a white bowl filled with white, round objects, possibly food or decorative items. To the right of the bowl, there are two bottles of beer, one of which is labeled \"Budweiser.\" The background features a man sitting at a wooden table, wearing a gray suit and a white shirt with a blue tie. The man is smiling and appears to be in a relaxed setting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25600.7, "ram_available_mb": 100171.5, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 25600.5, "ram_available_mb": 100171.7, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 4859.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.77, "peak": 16.03, "min": 15.32}, "VDD_GPU": {"avg": 21.16, "peak": 24.43, "min": 20.09}, "VIN": {"avg": 64.74, "peak": 72.52, "min": 60.84}, "VDD_CPU_SOC_MSS": {"avg": 16.73, "peak": 16.93, "min": 16.14}}, "power_watts_avg": 21.16, "energy_joules_est": 40.54, "sample_count": 14, "duration_seconds": 1.916}, "timestamp": "2026-01-16T16:44:12.165814"}
{"image_index": 446, "image_name": "000000050811.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050811.jpg", "image_width": 640, "image_height": 605, "image_resolution": "640x605", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1893.111, "latencies_ms": [1893.111], "images_per_second": 0.528, "prompt_tokens": 21, "response_tokens_est": 85, "n_tiles": 1, "output_text": "The image depicts a man sitting at a table in a room with a plain white wall and wooden paneling. He is dressed in a gray suit and tie, with a white shirt underneath. On the table, there are two bottles of beer, a white bowl with a spoon, and a few keys. The man is smiling and appears to be in a relaxed setting, possibly during a casual meeting or social event.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25600.5, "ram_available_mb": 100171.7, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25600.5, "ram_available_mb": 100171.7, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 4854.7, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.8, "peak": 16.03, "min": 15.32}, "VDD_GPU": {"avg": 21.05, "peak": 24.03, "min": 20.09}, "VIN": {"avg": 64.29, "peak": 69.12, "min": 61.17}, "VDD_CPU_SOC_MSS": {"avg": 16.79, "peak": 16.93, "min": 16.14}}, "power_watts_avg": 21.05, "energy_joules_est": 39.86, "sample_count": 14, "duration_seconds": 1.893}, "timestamp": "2026-01-16T16:44:14.064577"}
{"image_index": 446, "image_name": "000000050811.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050811.jpg", "image_width": 640, "image_height": 605, "image_resolution": "640x605", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1098.765, "latencies_ms": [1098.765], "images_per_second": 0.91, "prompt_tokens": 19, "response_tokens_est": 48, "n_tiles": 1, "output_text": "The man is wearing a gray suit with a white shirt and a light blue tie. The background is plain and white, with a wooden table and a white bowl on it. The lighting is soft and even, creating a calm atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25600.5, "ram_available_mb": 100171.7, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25600.5, "ram_available_mb": 100171.7, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 4853.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.75, "peak": 15.93, "min": 15.42}, "VDD_GPU": {"avg": 21.76, "peak": 24.03, "min": 20.48}, "VIN": {"avg": 65.46, "peak": 72.91, "min": 60.45}, "VDD_CPU_SOC_MSS": {"avg": 16.53, "peak": 16.93, "min": 16.14}}, "power_watts_avg": 21.76, "energy_joules_est": 23.92, "sample_count": 8, "duration_seconds": 1.099}, "timestamp": "2026-01-16T16:44:15.169168"}
{"image_index": 447, "image_name": "000000050828.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050828.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 3147.434, "latencies_ms": [3147.434], "images_per_second": 0.318, "prompt_tokens": 9, "response_tokens_est": 52, "n_tiles": 12, "output_text": "The image shows a neatly made bed with white linens and neatly stacked pillows, set against a light-colored wall with a small window above it, and a chair and a small table with a phone and some items on it in the background.", "error": null, "sys_before": {"cpu_percent": 47.6, "ram_used_mb": 25600.5, "ram_available_mb": 100171.7, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 25600.7, "ram_available_mb": 100171.5, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.24, "peak": 16.15, "min": 14.61}, "VDD_GPU": {"avg": 32.73, "peak": 40.18, "min": 26.39}, "VIN": {"avg": 71.6, "peak": 104.43, "min": 59.04}, "VDD_CPU_SOC_MSS": {"avg": 16.09, "peak": 17.32, "min": 15.35}}, "power_watts_avg": 32.73, "energy_joules_est": 103.02, "sample_count": 24, "duration_seconds": 3.148}, "timestamp": "2026-01-16T16:44:18.384705"}
{"image_index": 447, "image_name": "000000050828.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050828.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 3145.058, "latencies_ms": [3145.058], "images_per_second": 0.318, "prompt_tokens": 23, "response_tokens_est": 48, "n_tiles": 12, "output_text": "bed: 1\npillows: 4\ntowels: 2\ncouch: 1\ndrawer: 1\ndrawer: 1\ndrawer: 1\ndrawer: 1", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25600.7, "ram_available_mb": 100171.5, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 25600.4, "ram_available_mb": 100171.7, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.07, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 34.14, "peak": 42.54, "min": 26.39}, "VIN": {"avg": 75.22, "peak": 117.42, "min": 58.75}, "VDD_CPU_SOC_MSS": {"avg": 15.63, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 34.14, "energy_joules_est": 107.39, "sample_count": 24, "duration_seconds": 3.146}, "timestamp": "2026-01-16T16:44:21.536332"}
{"image_index": 447, "image_name": "000000050828.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050828.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 4430.426, "latencies_ms": [4430.426], "images_per_second": 0.226, "prompt_tokens": 27, "response_tokens_est": 86, "n_tiles": 12, "output_text": "The main objects in the image are a neatly made bed with white linens and pillows, positioned in the foreground. The bed is placed against a light-colored wall, and there is a small table with a chair to the left of the bed. The table holds various items, including a phone and a small black bag. The background features a window with a view of a building outside, adding depth to the scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25600.4, "ram_available_mb": 100171.7, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 25600.4, "ram_available_mb": 100171.7, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.1, "peak": 15.65, "min": 14.63}, "VDD_GPU": {"avg": 31.78, "peak": 42.94, "min": 26.0}, "VIN": {"avg": 71.79, "peak": 113.2, "min": 53.01}, "VDD_CPU_SOC_MSS": {"avg": 15.78, "peak": 16.53, "min": 15.35}}, "power_watts_avg": 31.78, "energy_joules_est": 140.81, "sample_count": 35, "duration_seconds": 4.431}, "timestamp": "2026-01-16T16:44:25.974041"}
{"image_index": 447, "image_name": "000000050828.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050828.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3784.109, "latencies_ms": [3784.109], "images_per_second": 0.264, "prompt_tokens": 21, "response_tokens_est": 67, "n_tiles": 12, "output_text": "The image depicts a neatly arranged hotel room with a white bed and pillows, a small bedside table with a phone and a few items, and a chair. The room is well-lit, with a window providing natural light. The setting appears to be a hotel room, and the room is tidy and clean.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25600.4, "ram_available_mb": 100171.7, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25600.4, "ram_available_mb": 100171.7, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.1, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 32.72, "peak": 42.94, "min": 26.39}, "VIN": {"avg": 77.86, "peak": 143.03, "min": 57.07}, "VDD_CPU_SOC_MSS": {"avg": 15.77, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 32.72, "energy_joules_est": 123.83, "sample_count": 30, "duration_seconds": 3.785}, "timestamp": "2026-01-16T16:44:29.764645"}
{"image_index": 447, "image_name": "000000050828.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050828.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3080.547, "latencies_ms": [3080.547], "images_per_second": 0.325, "prompt_tokens": 19, "response_tokens_est": 46, "n_tiles": 12, "output_text": "The image depicts a neatly made bed with white linens and pillows, set against a light-colored wall. The room is well-lit with natural light coming from a window, creating a bright and clean atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25600.4, "ram_available_mb": 100171.7, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 25600.7, "ram_available_mb": 100171.5, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.06, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 34.25, "peak": 42.54, "min": 26.39}, "VIN": {"avg": 76.02, "peak": 106.77, "min": 58.46}, "VDD_CPU_SOC_MSS": {"avg": 15.63, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 34.25, "energy_joules_est": 105.52, "sample_count": 24, "duration_seconds": 3.081}, "timestamp": "2026-01-16T16:44:32.851803"}
{"image_index": 448, "image_name": "000000050844.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050844.jpg", "image_width": 334, "image_height": 500, "image_resolution": "334x500", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2026.905, "latencies_ms": [2026.905], "images_per_second": 0.493, "prompt_tokens": 9, "response_tokens_est": 54, "n_tiles": 6, "output_text": "The image features a collection of plush toys, including a teddy bear wearing a green hat, a teddy bear wearing a red scarf, and a snowman wearing a green hat, all of which are adorned with the Coca-Cola logo.", "error": null, "sys_before": {"cpu_percent": 47.6, "ram_used_mb": 25600.7, "ram_available_mb": 100171.5, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 25600.9, "ram_available_mb": 100171.2, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.64, "peak": 15.22, "min": 13.92}, "VDD_GPU": {"avg": 29.88, "peak": 38.21, "min": 24.03}, "VIN": {"avg": 68.34, "peak": 89.59, "min": 59.66}, "VDD_CPU_SOC_MSS": {"avg": 15.33, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 29.88, "energy_joules_est": 60.58, "sample_count": 15, "duration_seconds": 2.027}, "timestamp": "2026-01-16T16:44:34.938643"}
{"image_index": 448, "image_name": "000000050844.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050844.jpg", "image_width": 334, "image_height": 500, "image_resolution": "334x500", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2550.115, "latencies_ms": [2550.115], "images_per_second": 0.392, "prompt_tokens": 23, "response_tokens_est": 72, "n_tiles": 6, "output_text": "1. teddy bear: 1\n2. teddy bear: 1\n3. teddy bear: 1\n4. teddy bear: 1\n5. teddy bear: 1\n6. teddy bear: 1\n7. teddy bear: 1\n8. teddy bear: 1", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25600.9, "ram_available_mb": 100171.2, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 25600.9, "ram_available_mb": 100171.3, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.98, "peak": 15.32, "min": 14.22}, "VDD_GPU": {"avg": 28.79, "peak": 38.21, "min": 23.64}, "VIN": {"avg": 66.65, "peak": 76.47, "min": 58.33}, "VDD_CPU_SOC_MSS": {"avg": 15.49, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 28.79, "energy_joules_est": 73.43, "sample_count": 20, "duration_seconds": 2.551}, "timestamp": "2026-01-16T16:44:37.494987"}
{"image_index": 448, "image_name": "000000050844.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050844.jpg", "image_width": 334, "image_height": 500, "image_resolution": "334x500", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2364.404, "latencies_ms": [2364.404], "images_per_second": 0.423, "prompt_tokens": 27, "response_tokens_est": 65, "n_tiles": 6, "output_text": "The main objects in the image are a teddy bear, a snowman, and a bottle. The teddy bear is positioned in the foreground, slightly to the left, while the snowman is in the background, slightly to the right. The bottle is located near the teddy bear, closer to the foreground.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25600.9, "ram_available_mb": 100171.3, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 25600.9, "ram_available_mb": 100171.3, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.99, "peak": 15.32, "min": 14.32}, "VDD_GPU": {"avg": 29.24, "peak": 38.19, "min": 23.64}, "VIN": {"avg": 72.08, "peak": 117.48, "min": 59.81}, "VDD_CPU_SOC_MSS": {"avg": 15.55, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 29.24, "energy_joules_est": 69.14, "sample_count": 18, "duration_seconds": 2.365}, "timestamp": "2026-01-16T16:44:39.865039"}
{"image_index": 448, "image_name": "000000050844.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050844.jpg", "image_width": 334, "image_height": 500, "image_resolution": "334x500", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2256.207, "latencies_ms": [2256.207], "images_per_second": 0.443, "prompt_tokens": 21, "response_tokens_est": 61, "n_tiles": 6, "output_text": "The image depicts a cozy indoor scene featuring a plush teddy bear dressed in a green hat and a red scarf, holding a bottle. The bear is sitting on a blue fabric surface, possibly a couch or a bed, with a snowman and a teddy bear in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25600.9, "ram_available_mb": 100171.3, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 25600.7, "ram_available_mb": 100171.5, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.95, "peak": 15.32, "min": 14.32}, "VDD_GPU": {"avg": 29.64, "peak": 38.21, "min": 24.03}, "VIN": {"avg": 73.91, "peak": 120.65, "min": 60.42}, "VDD_CPU_SOC_MSS": {"avg": 15.54, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 29.64, "energy_joules_est": 66.89, "sample_count": 17, "duration_seconds": 2.257}, "timestamp": "2026-01-16T16:44:42.127459"}
{"image_index": 448, "image_name": "000000050844.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050844.jpg", "image_width": 334, "image_height": 500, "image_resolution": "334x500", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1834.2, "latencies_ms": [1834.2], "images_per_second": 0.545, "prompt_tokens": 19, "response_tokens_est": 45, "n_tiles": 6, "output_text": "The image features a teddy bear dressed in a green hat and a red scarf, standing against a blue background. The lighting is soft and warm, casting gentle shadows that enhance the texture of the bear's fur.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25600.7, "ram_available_mb": 100171.5, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 25600.7, "ram_available_mb": 100171.5, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.88, "peak": 15.22, "min": 14.32}, "VDD_GPU": {"avg": 30.75, "peak": 38.19, "min": 24.42}, "VIN": {"avg": 74.13, "peak": 121.96, "min": 57.49}, "VDD_CPU_SOC_MSS": {"avg": 15.44, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 30.75, "energy_joules_est": 56.42, "sample_count": 14, "duration_seconds": 1.835}, "timestamp": "2026-01-16T16:44:43.967875"}
{"image_index": 449, "image_name": "000000050896.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050896.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1040.387, "latencies_ms": [1040.387], "images_per_second": 0.961, "prompt_tokens": 9, "response_tokens_est": 27, "n_tiles": 4, "output_text": "The image shows a clear glass bowl containing several ripe, orange fruits, likely oranges, placed on a textured silver surface.", "error": null, "sys_before": {"cpu_percent": 39.3, "ram_used_mb": 25600.7, "ram_available_mb": 100171.5, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 25600.9, "ram_available_mb": 100171.3, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4606.7, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5444.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.35, "peak": 14.81, "min": 13.82}, "VDD_GPU": {"avg": 30.16, "peak": 33.88, "min": 25.6}, "VIN": {"avg": 69.44, "peak": 84.22, "min": 58.38}, "VDD_CPU_SOC_MSS": {"avg": 15.01, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 30.16, "energy_joules_est": 31.39, "sample_count": 7, "duration_seconds": 1.041}, "timestamp": "2026-01-16T16:44:45.055116"}
{"image_index": 449, "image_name": "000000050896.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050896.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 533.169, "latencies_ms": [533.169], "images_per_second": 1.876, "prompt_tokens": 23, "response_tokens_est": 6, "n_tiles": 4, "output_text": "oranges: 8", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25600.9, "ram_available_mb": 100171.3, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 25600.9, "ram_available_mb": 100171.3, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4606.7, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5455.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.36, "peak": 14.73, "min": 13.92}, "VDD_GPU": {"avg": 34.4, "peak": 35.45, "min": 32.3}, "VIN": {"avg": 73.5, "peak": 84.31, "min": 55.75}, "VDD_CPU_SOC_MSS": {"avg": 15.09, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 34.4, "energy_joules_est": 18.36, "sample_count": 3, "duration_seconds": 0.534}, "timestamp": "2026-01-16T16:44:45.594402"}
{"image_index": 449, "image_name": "000000050896.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050896.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1757.406, "latencies_ms": [1757.406], "images_per_second": 0.569, "prompt_tokens": 27, "response_tokens_est": 53, "n_tiles": 4, "output_text": "The main objects in the image are a bowl of oranges. The bowl is placed on a textured surface, likely a tablecloth. The oranges are arranged in the bowl, with the closest ones to the viewer being the ones in the foreground.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25600.9, "ram_available_mb": 100171.3, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 25600.6, "ram_available_mb": 100171.5, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4606.7, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5458.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.49, "peak": 15.11, "min": 13.61}, "VDD_GPU": {"avg": 29.09, "peak": 38.6, "min": 22.85}, "VIN": {"avg": 68.57, "peak": 109.68, "min": 56.2}, "VDD_CPU_SOC_MSS": {"avg": 15.17, "peak": 15.75, "min": 14.17}}, "power_watts_avg": 29.09, "energy_joules_est": 51.13, "sample_count": 13, "duration_seconds": 1.758}, "timestamp": "2026-01-16T16:44:47.357688"}
{"image_index": 449, "image_name": "000000050896.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050896.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1725.041, "latencies_ms": [1725.041], "images_per_second": 0.58, "prompt_tokens": 21, "response_tokens_est": 52, "n_tiles": 4, "output_text": "The image shows a clear glass bowl filled with several ripe, orange fruits, likely oranges, placed on a textured silver surface. The bowl is placed on a table, and the overall scene suggests a domestic setting, possibly a kitchen or dining area.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25600.6, "ram_available_mb": 100171.5, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 25600.6, "ram_available_mb": 100171.5, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4606.7, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5453.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.71, "peak": 15.11, "min": 14.02}, "VDD_GPU": {"avg": 27.21, "peak": 35.06, "min": 22.45}, "VIN": {"avg": 69.87, "peak": 102.97, "min": 61.06}, "VDD_CPU_SOC_MSS": {"avg": 15.39, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 27.21, "energy_joules_est": 46.95, "sample_count": 13, "duration_seconds": 1.725}, "timestamp": "2026-01-16T16:44:49.088617"}
{"image_index": 449, "image_name": "000000050896.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050896.jpg", "image_width": 640, "image_height": 640, "image_resolution": "640x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2373.857, "latencies_ms": [2373.857], "images_per_second": 0.421, "prompt_tokens": 19, "response_tokens_est": 77, "n_tiles": 4, "output_text": "The image features a bowl of bright orange oranges, which are vividly colored and appear fresh with a glossy surface. The lighting is soft and natural, casting gentle shadows and highlighting the vibrant orange hue of the oranges. The bowl is made of clear glass, and the surface beneath it is a textured, metallic-looking material, possibly a tablecloth or fabric.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25600.6, "ram_available_mb": 100171.5, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.9, "ram_used_mb": 25600.6, "ram_available_mb": 100171.5, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4606.7, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5451.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.83, "peak": 15.22, "min": 14.02}, "VDD_GPU": {"avg": 25.74, "peak": 35.06, "min": 22.06}, "VIN": {"avg": 66.16, "peak": 108.26, "min": 57.59}, "VDD_CPU_SOC_MSS": {"avg": 15.46, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 25.74, "energy_joules_est": 61.11, "sample_count": 18, "duration_seconds": 2.374}, "timestamp": "2026-01-16T16:44:51.468426"}
{"image_index": 450, "image_name": "000000050943.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050943.jpg", "image_width": 640, "image_height": 319, "image_resolution": "640x319", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 614.339, "latencies_ms": [614.339], "images_per_second": 1.628, "prompt_tokens": 9, "response_tokens_est": 20, "n_tiles": 2, "output_text": "A surfer is riding a wave in the ocean, with the water splashing around him.", "error": null, "sys_before": {"cpu_percent": 26.1, "ram_used_mb": 25600.6, "ram_available_mb": 100171.5, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25600.6, "ram_available_mb": 100171.5, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5055.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.58, "peak": 15.01, "min": 14.3}, "VDD_GPU": {"avg": 27.96, "peak": 31.5, "min": 24.82}, "VIN": {"avg": 64.18, "peak": 75.69, "min": 54.6}, "VDD_CPU_SOC_MSS": {"avg": 15.16, "peak": 15.36, "min": 14.96}}, "power_watts_avg": 27.96, "energy_joules_est": 17.18, "sample_count": 4, "duration_seconds": 0.615}, "timestamp": "2026-01-16T16:44:52.106173"}
{"image_index": 450, "image_name": "000000050943.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050943.jpg", "image_width": 640, "image_height": 319, "image_resolution": "640x319", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 921.061, "latencies_ms": [921.061], "images_per_second": 1.086, "prompt_tokens": 23, "response_tokens_est": 33, "n_tiles": 2, "output_text": "1. Wave\n2. Surfer\n3. Ocean\n4. Wave\n5. Water\n6. Wave\n7. Ocean\n8. Wave", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25600.6, "ram_available_mb": 100171.5, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 25601.1, "ram_available_mb": 100171.1, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5064.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.88, "peak": 15.22, "min": 14.4}, "VDD_GPU": {"avg": 27.57, "peak": 33.48, "min": 23.64}, "VIN": {"avg": 67.59, "peak": 90.49, "min": 60.53}, "VDD_CPU_SOC_MSS": {"avg": 15.42, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 27.57, "energy_joules_est": 25.41, "sample_count": 6, "duration_seconds": 0.922}, "timestamp": "2026-01-16T16:44:53.034335"}
{"image_index": 450, "image_name": "000000050943.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050943.jpg", "image_width": 640, "image_height": 319, "image_resolution": "640x319", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1349.934, "latencies_ms": [1349.934], "images_per_second": 0.741, "prompt_tokens": 27, "response_tokens_est": 52, "n_tiles": 2, "output_text": "The main objects in the image are two surfers riding waves. The surfer in the foreground is closer to the viewer, while the surfer in the background is further away. The waves are in the foreground, and the ocean is in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25601.1, "ram_available_mb": 100171.1, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25600.9, "ram_available_mb": 100171.3, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5067.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.1, "peak": 15.52, "min": 14.51}, "VDD_GPU": {"avg": 25.01, "peak": 31.89, "min": 21.66}, "VIN": {"avg": 68.03, "peak": 105.3, "min": 59.24}, "VDD_CPU_SOC_MSS": {"avg": 15.71, "peak": 16.14, "min": 14.96}}, "power_watts_avg": 25.01, "energy_joules_est": 33.77, "sample_count": 10, "duration_seconds": 1.35}, "timestamp": "2026-01-16T16:44:54.390302"}
{"image_index": 450, "image_name": "000000050943.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050943.jpg", "image_width": 640, "image_height": 319, "image_resolution": "640x319", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1746.358, "latencies_ms": [1746.358], "images_per_second": 0.573, "prompt_tokens": 21, "response_tokens_est": 70, "n_tiles": 2, "output_text": "The image captures a dynamic scene of a surfer riding a large wave in the ocean. The surfer is skillfully maneuvering their surfboard, while the wave is breaking powerfully to the right, creating a frothy white crest. The overcast sky suggests a potentially challenging weather condition, adding to the intensity of the surfing activity.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25600.9, "ram_available_mb": 100171.3, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25600.9, "ram_available_mb": 100171.3, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5063.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.33, "peak": 15.62, "min": 14.71}, "VDD_GPU": {"avg": 24.06, "peak": 31.51, "min": 20.88}, "VIN": {"avg": 67.24, "peak": 98.34, "min": 59.9}, "VDD_CPU_SOC_MSS": {"avg": 15.9, "peak": 16.15, "min": 15.35}}, "power_watts_avg": 24.06, "energy_joules_est": 42.03, "sample_count": 13, "duration_seconds": 1.747}, "timestamp": "2026-01-16T16:44:56.144964"}
{"image_index": 450, "image_name": "000000050943.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000050943.jpg", "image_width": 640, "image_height": 319, "image_resolution": "640x319", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1665.238, "latencies_ms": [1665.238], "images_per_second": 0.601, "prompt_tokens": 19, "response_tokens_est": 66, "n_tiles": 2, "output_text": "The image captures a dynamic scene of a surfer riding a large wave in the ocean. The wave is a striking shade of blue, with white foam cresting at its peak. The surfer is dressed in black, and the ocean appears to be under overcast skies, casting a muted light over the entire scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25600.9, "ram_available_mb": 100171.3, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25601.1, "ram_available_mb": 100171.1, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5062.6, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.22, "peak": 15.62, "min": 14.51}, "VDD_GPU": {"avg": 24.19, "peak": 31.51, "min": 21.27}, "VIN": {"avg": 66.25, "peak": 102.14, "min": 56.7}, "VDD_CPU_SOC_MSS": {"avg": 15.91, "peak": 16.15, "min": 15.35}}, "power_watts_avg": 24.19, "energy_joules_est": 40.29, "sample_count": 12, "duration_seconds": 1.666}, "timestamp": "2026-01-16T16:44:57.816145"}
{"image_index": 451, "image_name": "000000051008.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051008.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2288.143, "latencies_ms": [2288.143], "images_per_second": 0.437, "prompt_tokens": 9, "response_tokens_est": 26, "n_tiles": 12, "output_text": "A tabby cat is resting on a laptop keyboard, seemingly engrossed in the screen, which displays a Twitter page.", "error": null, "sys_before": {"cpu_percent": 50.0, "ram_used_mb": 25601.1, "ram_available_mb": 100171.1, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 25600.9, "ram_available_mb": 100171.3, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.19, "peak": 16.15, "min": 14.1}, "VDD_GPU": {"avg": 35.06, "peak": 40.18, "min": 27.97}, "VIN": {"avg": 78.74, "peak": 120.92, "min": 58.63}, "VDD_CPU_SOC_MSS": {"avg": 16.14, "peak": 17.32, "min": 15.36}}, "power_watts_avg": 35.06, "energy_joules_est": 80.23, "sample_count": 17, "duration_seconds": 2.288}, "timestamp": "2026-01-16T16:45:00.188221"}
{"image_index": 451, "image_name": "000000051008.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051008.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 3129.597, "latencies_ms": [3129.597], "images_per_second": 0.32, "prompt_tokens": 23, "response_tokens_est": 47, "n_tiles": 12, "output_text": "- laptop: 1\n- computer mouse: 1\n- keyboard: 1\n- computer monitor: 1\n- cat: 1\n- computer mouse pad: 1\n- computer mouse: 1", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25600.9, "ram_available_mb": 100171.3, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 25600.9, "ram_available_mb": 100171.3, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.75, "peak": 15.34, "min": 14.12}, "VDD_GPU": {"avg": 34.33, "peak": 43.33, "min": 26.0}, "VIN": {"avg": 74.46, "peak": 114.94, "min": 55.43}, "VDD_CPU_SOC_MSS": {"avg": 15.29, "peak": 16.14, "min": 14.56}}, "power_watts_avg": 34.33, "energy_joules_est": 107.46, "sample_count": 24, "duration_seconds": 3.13}, "timestamp": "2026-01-16T16:45:03.325540"}
{"image_index": 451, "image_name": "000000051008.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051008.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3302.724, "latencies_ms": [3302.724], "images_per_second": 0.303, "prompt_tokens": 27, "response_tokens_est": 52, "n_tiles": 12, "output_text": "The main object in the foreground is a cat, which is positioned near the laptop keyboard. The laptop is situated in the background, with its screen displaying a Twitter page. The cat appears to be resting on the keyboard, suggesting a comfortable and familiar environment.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25600.9, "ram_available_mb": 100171.3, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 25600.9, "ram_available_mb": 100171.3, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.74, "peak": 15.34, "min": 14.12}, "VDD_GPU": {"avg": 33.56, "peak": 42.54, "min": 26.0}, "VIN": {"avg": 72.14, "peak": 94.89, "min": 58.2}, "VDD_CPU_SOC_MSS": {"avg": 15.29, "peak": 16.14, "min": 14.56}}, "power_watts_avg": 33.56, "energy_joules_est": 110.85, "sample_count": 26, "duration_seconds": 3.303}, "timestamp": "2026-01-16T16:45:06.636958"}
{"image_index": 451, "image_name": "000000051008.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051008.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3782.37, "latencies_ms": [3782.37], "images_per_second": 0.264, "prompt_tokens": 21, "response_tokens_est": 66, "n_tiles": 12, "output_text": "The image depicts a domestic scene where a tabby cat is resting on a laptop keyboard. The cat is positioned on the right side of the laptop, with its head resting on the keyboard. The laptop is open, displaying a Twitter page, and the cat appears to be curious or interested in the content on the screen.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25600.9, "ram_available_mb": 100171.3, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25600.8, "ram_available_mb": 100171.3, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.73, "peak": 15.34, "min": 14.12}, "VDD_GPU": {"avg": 32.67, "peak": 42.54, "min": 26.0}, "VIN": {"avg": 68.95, "peak": 111.52, "min": 56.86}, "VDD_CPU_SOC_MSS": {"avg": 15.23, "peak": 16.14, "min": 14.56}}, "power_watts_avg": 32.67, "energy_joules_est": 123.59, "sample_count": 29, "duration_seconds": 3.783}, "timestamp": "2026-01-16T16:45:10.429894"}
{"image_index": 451, "image_name": "000000051008.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051008.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3330.959, "latencies_ms": [3330.959], "images_per_second": 0.3, "prompt_tokens": 19, "response_tokens_est": 53, "n_tiles": 12, "output_text": "The image features a cat with a white and brown tabby coat, resting on a laptop keyboard. The laptop screen displays a Twitter page, indicating a casual and relaxed setting. The lighting is soft and ambient, with a warm tone that enhances the cozy atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25600.8, "ram_available_mb": 100171.3, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 25600.8, "ram_available_mb": 100171.3, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.71, "peak": 15.24, "min": 14.1}, "VDD_GPU": {"avg": 33.41, "peak": 42.54, "min": 26.0}, "VIN": {"avg": 71.85, "peak": 108.19, "min": 56.04}, "VDD_CPU_SOC_MSS": {"avg": 15.25, "peak": 16.14, "min": 14.56}}, "power_watts_avg": 33.41, "energy_joules_est": 111.3, "sample_count": 26, "duration_seconds": 3.331}, "timestamp": "2026-01-16T16:45:13.767193"}
{"image_index": 452, "image_name": "000000051309.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051309.jpg", "image_width": 640, "image_height": 360, "image_resolution": "640x360", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 754.898, "latencies_ms": [754.898], "images_per_second": 1.325, "prompt_tokens": 9, "response_tokens_est": 26, "n_tiles": 2, "output_text": "A group of horses is gathered in a field, with some feeding on hay bales while others graze in the background.", "error": null, "sys_before": {"cpu_percent": 39.6, "ram_used_mb": 25600.8, "ram_available_mb": 100171.3, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25600.8, "ram_available_mb": 100171.3, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5055.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.26, "peak": 14.81, "min": 13.79}, "VDD_GPU": {"avg": 28.83, "peak": 33.86, "min": 24.82}, "VIN": {"avg": 74.36, "peak": 116.63, "min": 62.13}, "VDD_CPU_SOC_MSS": {"avg": 14.8, "peak": 15.36, "min": 14.18}}, "power_watts_avg": 28.83, "energy_joules_est": 21.78, "sample_count": 5, "duration_seconds": 0.756}, "timestamp": "2026-01-16T16:45:14.560669"}
{"image_index": 452, "image_name": "000000051309.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051309.jpg", "image_width": 640, "image_height": 360, "image_resolution": "640x360", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 945.228, "latencies_ms": [945.228], "images_per_second": 1.058, "prompt_tokens": 23, "response_tokens_est": 34, "n_tiles": 2, "output_text": "1. Horses\n2. Hay\n3. Haystack\n4. Hay\n5. Hay\n6. Hay\n7. Hay\n8. Hay", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25600.8, "ram_available_mb": 100171.3, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25600.8, "ram_available_mb": 100171.4, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5064.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.81, "peak": 15.22, "min": 14.3}, "VDD_GPU": {"avg": 26.78, "peak": 32.7, "min": 22.85}, "VIN": {"avg": 68.43, "peak": 98.05, "min": 61.82}, "VDD_CPU_SOC_MSS": {"avg": 15.58, "peak": 16.14, "min": 14.96}}, "power_watts_avg": 26.78, "energy_joules_est": 25.33, "sample_count": 7, "duration_seconds": 0.946}, "timestamp": "2026-01-16T16:45:15.512652"}
{"image_index": 452, "image_name": "000000051309.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051309.jpg", "image_width": 640, "image_height": 360, "image_resolution": "640x360", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1381.158, "latencies_ms": [1381.158], "images_per_second": 0.724, "prompt_tokens": 27, "response_tokens_est": 51, "n_tiles": 2, "output_text": "In the image, there is a group of horses in a field. The horses are in the foreground, with the brown horse in the center being the most prominent. The brown horse is near a pile of hay, which is located in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25600.8, "ram_available_mb": 100171.4, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25600.8, "ram_available_mb": 100171.4, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5067.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.94, "peak": 15.32, "min": 14.4}, "VDD_GPU": {"avg": 25.06, "peak": 32.3, "min": 21.28}, "VIN": {"avg": 68.28, "peak": 93.66, "min": 61.66}, "VDD_CPU_SOC_MSS": {"avg": 15.79, "peak": 16.14, "min": 14.96}}, "power_watts_avg": 25.06, "energy_joules_est": 34.62, "sample_count": 10, "duration_seconds": 1.382}, "timestamp": "2026-01-16T16:45:16.899405"}
{"image_index": 452, "image_name": "000000051309.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051309.jpg", "image_width": 640, "image_height": 360, "image_resolution": "640x360", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1330.54, "latencies_ms": [1330.54], "images_per_second": 0.752, "prompt_tokens": 21, "response_tokens_est": 49, "n_tiles": 2, "output_text": "The image depicts a group of horses in a pasture, with one horse in the foreground eating hay. The horses are gathered together, and the setting appears to be a rural area with greenery and a clear sky in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25600.8, "ram_available_mb": 100171.4, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25600.8, "ram_available_mb": 100171.4, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5063.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.09, "peak": 15.52, "min": 14.4}, "VDD_GPU": {"avg": 24.7, "peak": 31.13, "min": 21.27}, "VIN": {"avg": 66.79, "peak": 99.6, "min": 59.98}, "VDD_CPU_SOC_MSS": {"avg": 15.83, "peak": 16.14, "min": 15.35}}, "power_watts_avg": 24.7, "energy_joules_est": 32.87, "sample_count": 10, "duration_seconds": 1.331}, "timestamp": "2026-01-16T16:45:18.235880"}
{"image_index": 452, "image_name": "000000051309.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051309.jpg", "image_width": 640, "image_height": 360, "image_resolution": "640x360", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1165.915, "latencies_ms": [1165.915], "images_per_second": 0.858, "prompt_tokens": 19, "response_tokens_est": 42, "n_tiles": 2, "output_text": "The image depicts a group of horses in a pasture, with the horses' coats appearing dark brown. The lighting is natural, suggesting it is daytime, and the horses are grazing on hay.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25600.8, "ram_available_mb": 100171.4, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25600.8, "ram_available_mb": 100171.4, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5062.6, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.05, "peak": 15.42, "min": 14.61}, "VDD_GPU": {"avg": 24.9, "peak": 31.51, "min": 21.27}, "VIN": {"avg": 64.66, "peak": 75.76, "min": 60.87}, "VDD_CPU_SOC_MSS": {"avg": 15.84, "peak": 16.15, "min": 15.36}}, "power_watts_avg": 24.9, "energy_joules_est": 29.04, "sample_count": 9, "duration_seconds": 1.166}, "timestamp": "2026-01-16T16:45:19.407828"}
{"image_index": 453, "image_name": "000000051314.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051314.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2219.081, "latencies_ms": [2219.081], "images_per_second": 0.451, "prompt_tokens": 9, "response_tokens_est": 24, "n_tiles": 12, "output_text": "A surfer is riding a wave in the ocean, wearing a wetsuit and demonstrating skillful maneuvering.", "error": null, "sys_before": {"cpu_percent": 52.2, "ram_used_mb": 25600.8, "ram_available_mb": 100171.4, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 25601.6, "ram_available_mb": 100170.6, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.17, "peak": 16.05, "min": 14.1}, "VDD_GPU": {"avg": 35.36, "peak": 40.57, "min": 28.36}, "VIN": {"avg": 80.18, "peak": 136.78, "min": 58.42}, "VDD_CPU_SOC_MSS": {"avg": 16.07, "peak": 17.32, "min": 15.35}}, "power_watts_avg": 35.36, "energy_joules_est": 78.48, "sample_count": 17, "duration_seconds": 2.219}, "timestamp": "2026-01-16T16:45:21.709623"}
{"image_index": 453, "image_name": "000000051314.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051314.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2544.333, "latencies_ms": [2544.333], "images_per_second": 0.393, "prompt_tokens": 23, "response_tokens_est": 30, "n_tiles": 12, "output_text": "surfboard: 1\nsurfer: 1\nwater: 1\nocean: 1\nbeach: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25601.6, "ram_available_mb": 100170.6, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 25602.0, "ram_available_mb": 100170.1, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.07, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 36.43, "peak": 43.33, "min": 27.97}, "VIN": {"avg": 81.46, "peak": 123.63, "min": 59.12}, "VDD_CPU_SOC_MSS": {"avg": 15.72, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 36.43, "energy_joules_est": 92.71, "sample_count": 19, "duration_seconds": 2.545}, "timestamp": "2026-01-16T16:45:24.261543"}
{"image_index": 453, "image_name": "000000051314.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051314.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3757.111, "latencies_ms": [3757.111], "images_per_second": 0.266, "prompt_tokens": 27, "response_tokens_est": 66, "n_tiles": 12, "output_text": "The main object in the foreground is a surfer riding a wave. The surfer is positioned near the center of the image, with the wave being the right side of the frame. The background features a rocky shoreline and a partly cloudy sky, which provides a natural backdrop to the action taking place in the foreground.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25602.0, "ram_available_mb": 100170.1, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25601.8, "ram_available_mb": 100170.4, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.09, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 32.95, "peak": 42.94, "min": 26.39}, "VIN": {"avg": 73.75, "peak": 110.83, "min": 58.92}, "VDD_CPU_SOC_MSS": {"avg": 15.66, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 32.95, "energy_joules_est": 123.81, "sample_count": 29, "duration_seconds": 3.758}, "timestamp": "2026-01-16T16:45:28.025538"}
{"image_index": 453, "image_name": "000000051314.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051314.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3918.038, "latencies_ms": [3918.038], "images_per_second": 0.255, "prompt_tokens": 21, "response_tokens_est": 71, "n_tiles": 12, "output_text": "The image captures a surfer in action on a choppy ocean wave, skillfully maneuvering a surfboard. The surfer, dressed in a black wetsuit, is leaning forward with his arms extended, showcasing his expertise in surfing. The scene takes place in a coastal environment, with the surfer navigating through the turbulent waters.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25601.8, "ram_available_mb": 100170.4, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25601.8, "ram_available_mb": 100170.4, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.09, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 32.42, "peak": 42.15, "min": 26.0}, "VIN": {"avg": 73.71, "peak": 118.46, "min": 58.77}, "VDD_CPU_SOC_MSS": {"avg": 15.68, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 32.42, "energy_joules_est": 127.04, "sample_count": 30, "duration_seconds": 3.919}, "timestamp": "2026-01-16T16:45:31.954262"}
{"image_index": 453, "image_name": "000000051314.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051314.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3848.964, "latencies_ms": [3848.964], "images_per_second": 0.26, "prompt_tokens": 19, "response_tokens_est": 69, "n_tiles": 12, "output_text": "The image captures a surfer in a black wetsuit riding a wave on a surfboard. The surfer is partially submerged in the water, with splashes and waves around him. The lighting is bright, indicating a sunny day, and the colors are vibrant, with the green of the ocean contrasting against the blue sky.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25601.8, "ram_available_mb": 100170.4, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25602.3, "ram_available_mb": 100169.9, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.1, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 32.64, "peak": 42.15, "min": 26.39}, "VIN": {"avg": 77.16, "peak": 137.77, "min": 59.02}, "VDD_CPU_SOC_MSS": {"avg": 15.76, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 32.64, "energy_joules_est": 125.64, "sample_count": 30, "duration_seconds": 3.849}, "timestamp": "2026-01-16T16:45:35.809891"}
{"image_index": 454, "image_name": "000000051326.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051326.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2093.76, "latencies_ms": [2093.76], "images_per_second": 0.478, "prompt_tokens": 9, "response_tokens_est": 56, "n_tiles": 6, "output_text": "The image features a collection of Halloween-themed pumpkins, including a carved pumpkin with a smiling face, a pumpkin with a jack-o'-lantern face, and a pumpkin with a creature's face, all arranged on a table with a pink flower arrangement in the background.", "error": null, "sys_before": {"cpu_percent": 39.1, "ram_used_mb": 25602.3, "ram_available_mb": 100169.9, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 25602.8, "ram_available_mb": 100169.4, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.65, "peak": 15.22, "min": 13.92}, "VDD_GPU": {"avg": 29.25, "peak": 37.42, "min": 24.03}, "VIN": {"avg": 69.78, "peak": 119.54, "min": 56.97}, "VDD_CPU_SOC_MSS": {"avg": 15.35, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 29.25, "energy_joules_est": 61.25, "sample_count": 16, "duration_seconds": 2.094}, "timestamp": "2026-01-16T16:45:37.969318"}
{"image_index": 454, "image_name": "000000051326.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051326.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2356.122, "latencies_ms": [2356.122], "images_per_second": 0.424, "prompt_tokens": 23, "response_tokens_est": 65, "n_tiles": 6, "output_text": "1. Pumpkin: 2\n2. Floral arrangement: 1\n3. Man figure: 1\n4. Man figure: 1\n5. Man figure: 1\n6. Man figure: 1\n7. Man figure: 1\n8. Man figure: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25602.8, "ram_available_mb": 100169.4, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 25602.8, "ram_available_mb": 100169.4, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.96, "peak": 15.32, "min": 14.22}, "VDD_GPU": {"avg": 29.26, "peak": 38.21, "min": 24.03}, "VIN": {"avg": 75.97, "peak": 117.05, "min": 63.35}, "VDD_CPU_SOC_MSS": {"avg": 15.55, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 29.26, "energy_joules_est": 68.95, "sample_count": 18, "duration_seconds": 2.357}, "timestamp": "2026-01-16T16:45:40.331711"}
{"image_index": 454, "image_name": "000000051326.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051326.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2111.417, "latencies_ms": [2111.417], "images_per_second": 0.474, "prompt_tokens": 27, "response_tokens_est": 56, "n_tiles": 6, "output_text": "The main objects in the image are a collection of pumpkins and a floral arrangement. The pumpkins are arranged in a row, with the one in the foreground being the most prominent. The floral arrangement is placed in the background, slightly to the right of the pumpkins.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25602.8, "ram_available_mb": 100169.4, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 25602.8, "ram_available_mb": 100169.4, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.98, "peak": 15.32, "min": 14.32}, "VDD_GPU": {"avg": 29.89, "peak": 38.19, "min": 24.03}, "VIN": {"avg": 74.14, "peak": 119.71, "min": 63.94}, "VDD_CPU_SOC_MSS": {"avg": 15.6, "peak": 16.14, "min": 14.96}}, "power_watts_avg": 29.89, "energy_joules_est": 63.12, "sample_count": 16, "duration_seconds": 2.112}, "timestamp": "2026-01-16T16:45:42.449234"}
{"image_index": 454, "image_name": "000000051326.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051326.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2830.678, "latencies_ms": [2830.678], "images_per_second": 0.353, "prompt_tokens": 21, "response_tokens_est": 83, "n_tiles": 6, "output_text": "The image depicts a festive scene centered around a collection of carved pumpkins, each featuring unique and whimsical designs. The pumpkins are arranged on a table, with one prominently displaying a smiling face and another carved with a creature's face. The setting appears to be indoors, possibly in a home or a themed event space, as suggested by the presence of books and decorative items in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25602.8, "ram_available_mb": 100169.4, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 25602.7, "ram_available_mb": 100169.4, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.08, "peak": 15.32, "min": 14.32}, "VDD_GPU": {"avg": 28.33, "peak": 38.21, "min": 23.64}, "VIN": {"avg": 71.91, "peak": 113.0, "min": 57.23}, "VDD_CPU_SOC_MSS": {"avg": 15.77, "peak": 16.14, "min": 14.96}}, "power_watts_avg": 28.33, "energy_joules_est": 80.2, "sample_count": 22, "duration_seconds": 2.831}, "timestamp": "2026-01-16T16:45:45.285880"}
{"image_index": 454, "image_name": "000000051326.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051326.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2554.621, "latencies_ms": [2554.621], "images_per_second": 0.391, "prompt_tokens": 19, "response_tokens_est": 72, "n_tiles": 6, "output_text": "The image showcases a collection of carved pumpkins, each with unique and creative designs. The pumpkins are predominantly orange, with some featuring intricate details and faces. The lighting is soft and natural, casting gentle shadows that enhance the textures and details of the pumpkins. The overall atmosphere is cozy and festive, evoking a sense of celebration and creativity.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25602.7, "ram_available_mb": 100169.4, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25603.2, "ram_available_mb": 100168.9, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.99, "peak": 15.32, "min": 14.32}, "VDD_GPU": {"avg": 28.76, "peak": 38.21, "min": 23.64}, "VIN": {"avg": 72.58, "peak": 113.17, "min": 61.02}, "VDD_CPU_SOC_MSS": {"avg": 15.6, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 28.76, "energy_joules_est": 73.48, "sample_count": 19, "duration_seconds": 2.555}, "timestamp": "2026-01-16T16:45:47.846661"}
{"image_index": 455, "image_name": "000000051598.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051598.jpg", "image_width": 360, "image_height": 640, "image_resolution": "360x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1003.298, "latencies_ms": [1003.298], "images_per_second": 0.997, "prompt_tokens": 9, "response_tokens_est": 37, "n_tiles": 2, "output_text": "The image depicts a small, dimly lit bathroom with a white sink, a black trash bag, and a door slightly ajar, revealing a glimpse of the room beyond.", "error": null, "sys_before": {"cpu_percent": 34.4, "ram_used_mb": 25603.2, "ram_available_mb": 100168.9, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 25603.0, "ram_available_mb": 100169.2, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5055.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.82, "peak": 15.42, "min": 14.2}, "VDD_GPU": {"avg": 26.22, "peak": 32.68, "min": 22.45}, "VIN": {"avg": 64.79, "peak": 74.66, "min": 62.81}, "VDD_CPU_SOC_MSS": {"avg": 15.47, "peak": 16.14, "min": 14.96}}, "power_watts_avg": 26.22, "energy_joules_est": 26.32, "sample_count": 7, "duration_seconds": 1.004}, "timestamp": "2026-01-16T16:45:48.878151"}
{"image_index": 455, "image_name": "000000051598.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051598.jpg", "image_width": 360, "image_height": 640, "image_resolution": "360x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1367.557, "latencies_ms": [1367.557], "images_per_second": 0.731, "prompt_tokens": 23, "response_tokens_est": 53, "n_tiles": 2, "output_text": "- sink: 1\n- mirror: 1\n- soap dispenser: 1\n- soap: 1\n- soap bottle: 1\n- soap box: 1\n- trash bag: 1\n- door: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25603.0, "ram_available_mb": 100169.2, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 25603.2, "ram_available_mb": 100168.9, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5064.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.23, "peak": 15.72, "min": 14.71}, "VDD_GPU": {"avg": 25.09, "peak": 32.3, "min": 21.27}, "VIN": {"avg": 67.18, "peak": 99.03, "min": 61.86}, "VDD_CPU_SOC_MSS": {"avg": 15.95, "peak": 16.14, "min": 15.36}}, "power_watts_avg": 25.09, "energy_joules_est": 34.32, "sample_count": 10, "duration_seconds": 1.368}, "timestamp": "2026-01-16T16:45:50.251959"}
{"image_index": 455, "image_name": "000000051598.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051598.jpg", "image_width": 360, "image_height": 640, "image_resolution": "360x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1616.528, "latencies_ms": [1616.528], "images_per_second": 0.619, "prompt_tokens": 27, "response_tokens_est": 64, "n_tiles": 2, "output_text": "The main objects in the image are a sink, a trash bag, and a door. The sink is located on the left side of the image, near the wall. The trash bag is in the foreground, close to the sink. The door is located on the right side of the image, near the wall.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25603.2, "ram_available_mb": 100168.9, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25603.0, "ram_available_mb": 100169.2, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5067.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.32, "peak": 15.62, "min": 14.91}, "VDD_GPU": {"avg": 24.23, "peak": 31.91, "min": 20.88}, "VIN": {"avg": 66.21, "peak": 100.88, "min": 59.78}, "VDD_CPU_SOC_MSS": {"avg": 15.85, "peak": 16.15, "min": 15.35}}, "power_watts_avg": 24.23, "energy_joules_est": 39.18, "sample_count": 12, "duration_seconds": 1.617}, "timestamp": "2026-01-16T16:45:51.874342"}
{"image_index": 455, "image_name": "000000051598.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051598.jpg", "image_width": 360, "image_height": 640, "image_resolution": "360x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1680.498, "latencies_ms": [1680.498], "images_per_second": 0.595, "prompt_tokens": 21, "response_tokens_est": 67, "n_tiles": 2, "output_text": "The image depicts a small, dimly lit bathroom with a white sink and a mirror above it. The walls are bare and show signs of wear, and there is a black trash bag on the floor. The door is slightly ajar, and the overall setting appears to be in a state of disrepair or renovation.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25603.0, "ram_available_mb": 100169.2, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 25603.0, "ram_available_mb": 100169.2, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5063.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.26, "peak": 15.42, "min": 14.81}, "VDD_GPU": {"avg": 24.06, "peak": 30.73, "min": 20.89}, "VIN": {"avg": 67.22, "peak": 102.7, "min": 61.98}, "VDD_CPU_SOC_MSS": {"avg": 15.87, "peak": 16.54, "min": 15.35}}, "power_watts_avg": 24.06, "energy_joules_est": 40.45, "sample_count": 13, "duration_seconds": 1.681}, "timestamp": "2026-01-16T16:45:53.564989"}
{"image_index": 455, "image_name": "000000051598.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051598.jpg", "image_width": 360, "image_height": 640, "image_resolution": "360x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1126.331, "latencies_ms": [1126.331], "images_per_second": 0.888, "prompt_tokens": 19, "response_tokens_est": 42, "n_tiles": 2, "output_text": "The image shows a small, dimly lit bathroom with a white sink and a black trash bag on the floor. The walls are white, and there is a mirror with a silver frame above the sink.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25603.0, "ram_available_mb": 100169.2, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25603.0, "ram_available_mb": 100169.2, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5062.6, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.1, "peak": 15.52, "min": 14.51}, "VDD_GPU": {"avg": 25.21, "peak": 31.12, "min": 21.67}, "VIN": {"avg": 64.87, "peak": 75.7, "min": 56.38}, "VDD_CPU_SOC_MSS": {"avg": 15.55, "peak": 15.76, "min": 14.96}}, "power_watts_avg": 25.21, "energy_joules_est": 28.4, "sample_count": 8, "duration_seconds": 1.127}, "timestamp": "2026-01-16T16:45:54.697953"}
{"image_index": 456, "image_name": "000000051610.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051610.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1146.866, "latencies_ms": [1146.866], "images_per_second": 0.872, "prompt_tokens": 9, "response_tokens_est": 21, "n_tiles": 6, "output_text": "A young child is sitting on a bed, holding a small laptop, and looking at the screen.", "error": null, "sys_before": {"cpu_percent": 45.3, "ram_used_mb": 25603.0, "ram_available_mb": 100169.2, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25603.0, "ram_available_mb": 100169.2, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.43, "peak": 14.71, "min": 14.02}, "VDD_GPU": {"avg": 32.6, "peak": 37.03, "min": 27.97}, "VIN": {"avg": 73.51, "peak": 112.28, "min": 50.11}, "VDD_CPU_SOC_MSS": {"avg": 15.25, "peak": 15.36, "min": 14.96}}, "power_watts_avg": 32.6, "energy_joules_est": 37.4, "sample_count": 8, "duration_seconds": 1.147}, "timestamp": "2026-01-16T16:45:55.894469"}
{"image_index": 456, "image_name": "000000051610.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051610.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1735.247, "latencies_ms": [1735.247], "images_per_second": 0.576, "prompt_tokens": 23, "response_tokens_est": 40, "n_tiles": 6, "output_text": "1. Baby\n2. Laptop\n3. Bed\n4. Child\n5. Child's feet\n6. Child's hands\n7. Child's legs\n8. Child's body", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25603.0, "ram_available_mb": 100169.2, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 25603.0, "ram_available_mb": 100169.2, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.6, "peak": 15.01, "min": 14.02}, "VDD_GPU": {"avg": 31.84, "peak": 39.38, "min": 24.42}, "VIN": {"avg": 68.4, "peak": 93.39, "min": 60.3}, "VDD_CPU_SOC_MSS": {"avg": 15.17, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 31.84, "energy_joules_est": 55.28, "sample_count": 13, "duration_seconds": 1.736}, "timestamp": "2026-01-16T16:45:57.637530"}
{"image_index": 456, "image_name": "000000051610.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051610.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2555.413, "latencies_ms": [2555.413], "images_per_second": 0.391, "prompt_tokens": 27, "response_tokens_est": 69, "n_tiles": 6, "output_text": "The main objects in the image are a laptop and a baby. The laptop is positioned on the left side of the image, with its screen facing the baby. The baby is sitting on the right side of the image, with their feet resting on the laptop. The background is plain and white, providing a neutral setting for the main subjects.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25603.0, "ram_available_mb": 100169.2, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 25603.0, "ram_available_mb": 100169.2, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.75, "peak": 15.01, "min": 14.02}, "VDD_GPU": {"avg": 28.46, "peak": 38.6, "min": 23.24}, "VIN": {"avg": 67.76, "peak": 115.62, "min": 57.65}, "VDD_CPU_SOC_MSS": {"avg": 15.22, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 28.46, "energy_joules_est": 72.73, "sample_count": 20, "duration_seconds": 2.556}, "timestamp": "2026-01-16T16:46:00.201013"}
{"image_index": 456, "image_name": "000000051610.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051610.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2578.889, "latencies_ms": [2578.889], "images_per_second": 0.388, "prompt_tokens": 21, "response_tokens_est": 70, "n_tiles": 6, "output_text": "The image depicts a black and white scene featuring a young child sitting on a bed, with a laptop open in front of them. The child appears to be focused on the screen, possibly engaged in an activity or learning something on the laptop. The setting is a simple, unadorned bedroom with a plain white wall in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25603.0, "ram_available_mb": 100169.2, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 25603.2, "ram_available_mb": 100169.0, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.76, "peak": 15.01, "min": 14.02}, "VDD_GPU": {"avg": 28.38, "peak": 37.82, "min": 23.24}, "VIN": {"avg": 69.51, "peak": 114.86, "min": 54.94}, "VDD_CPU_SOC_MSS": {"avg": 15.27, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 28.38, "energy_joules_est": 73.2, "sample_count": 19, "duration_seconds": 2.579}, "timestamp": "2026-01-16T16:46:02.785988"}
{"image_index": 456, "image_name": "000000051610.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051610.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1846.657, "latencies_ms": [1846.657], "images_per_second": 0.542, "prompt_tokens": 19, "response_tokens_est": 44, "n_tiles": 6, "output_text": "The image is a black and white photograph featuring a young child sitting on a bed. The child is wearing a white shirt and has dark hair. The lighting is soft and even, creating a calm and intimate atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25603.2, "ram_available_mb": 100169.0, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 25603.2, "ram_available_mb": 100169.0, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.69, "peak": 15.01, "min": 14.12}, "VDD_GPU": {"avg": 30.33, "peak": 38.19, "min": 24.03}, "VIN": {"avg": 71.37, "peak": 119.05, "min": 56.31}, "VDD_CPU_SOC_MSS": {"avg": 15.18, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 30.33, "energy_joules_est": 56.03, "sample_count": 14, "duration_seconds": 1.847}, "timestamp": "2026-01-16T16:46:04.639212"}
{"image_index": 457, "image_name": "000000051712.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051712.jpg", "image_width": 640, "image_height": 368, "image_resolution": "640x368", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1278.945, "latencies_ms": [1278.945], "images_per_second": 0.782, "prompt_tokens": 9, "response_tokens_est": 26, "n_tiles": 6, "output_text": "A person is skiing down a snowy slope, wearing a brown jacket and a helmet, with snow flying around them.", "error": null, "sys_before": {"cpu_percent": 41.4, "ram_used_mb": 25603.2, "ram_available_mb": 100169.0, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25603.2, "ram_available_mb": 100169.0, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.37, "peak": 14.81, "min": 13.92}, "VDD_GPU": {"avg": 32.47, "peak": 37.01, "min": 27.18}, "VIN": {"avg": 75.92, "peak": 103.65, "min": 59.39}, "VDD_CPU_SOC_MSS": {"avg": 15.09, "peak": 15.36, "min": 14.57}}, "power_watts_avg": 32.47, "energy_joules_est": 41.54, "sample_count": 9, "duration_seconds": 1.279}, "timestamp": "2026-01-16T16:46:05.972662"}
{"image_index": 457, "image_name": "000000051712.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051712.jpg", "image_width": 640, "image_height": 368, "image_resolution": "640x368", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1699.077, "latencies_ms": [1699.077], "images_per_second": 0.589, "prompt_tokens": 23, "response_tokens_est": 40, "n_tiles": 6, "output_text": "object: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 25603.2, "ram_available_mb": 100169.0, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 25603.2, "ram_available_mb": 100169.0, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.79, "peak": 15.22, "min": 14.22}, "VDD_GPU": {"avg": 31.66, "peak": 38.6, "min": 24.82}, "VIN": {"avg": 67.78, "peak": 79.74, "min": 55.43}, "VDD_CPU_SOC_MSS": {"avg": 15.32, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 31.66, "energy_joules_est": 53.81, "sample_count": 13, "duration_seconds": 1.7}, "timestamp": "2026-01-16T16:46:07.678215"}
{"image_index": 457, "image_name": "000000051712.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051712.jpg", "image_width": 640, "image_height": 368, "image_resolution": "640x368", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2333.22, "latencies_ms": [2333.22], "images_per_second": 0.429, "prompt_tokens": 27, "response_tokens_est": 64, "n_tiles": 6, "output_text": "The main object in the foreground is a person skiing down a snowy slope. The person is wearing a brown jacket and a blue and orange helmet. The background features a snowy landscape with trees and a snow-covered hill. The person is positioned near the center of the image, slightly to the left.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25603.2, "ram_available_mb": 100169.0, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 25603.2, "ram_available_mb": 100169.0, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.95, "peak": 15.32, "min": 14.22}, "VDD_GPU": {"avg": 29.39, "peak": 38.6, "min": 24.03}, "VIN": {"avg": 69.75, "peak": 97.56, "min": 61.71}, "VDD_CPU_SOC_MSS": {"avg": 15.51, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 29.39, "energy_joules_est": 68.59, "sample_count": 18, "duration_seconds": 2.334}, "timestamp": "2026-01-16T16:46:10.017901"}
{"image_index": 457, "image_name": "000000051712.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051712.jpg", "image_width": 640, "image_height": 368, "image_resolution": "640x368", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2654.945, "latencies_ms": [2654.945], "images_per_second": 0.377, "prompt_tokens": 21, "response_tokens_est": 76, "n_tiles": 6, "output_text": "The image captures a dynamic winter scene where a skier is in mid-action on a snowy slope. The skier, dressed in a brown jacket and black pants, is navigating through the snow with ski poles in hand, creating a sense of motion and excitement. The surrounding environment is filled with snow-covered trees and a clear blue sky, enhancing the wintry atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25603.2, "ram_available_mb": 100169.0, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 25603.4, "ram_available_mb": 100168.7, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.02, "peak": 15.32, "min": 14.32}, "VDD_GPU": {"avg": 28.68, "peak": 38.19, "min": 23.64}, "VIN": {"avg": 71.19, "peak": 121.85, "min": 57.44}, "VDD_CPU_SOC_MSS": {"avg": 15.55, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 28.68, "energy_joules_est": 76.15, "sample_count": 20, "duration_seconds": 2.655}, "timestamp": "2026-01-16T16:46:12.678890"}
{"image_index": 457, "image_name": "000000051712.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051712.jpg", "image_width": 640, "image_height": 368, "image_resolution": "640x368", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1854.373, "latencies_ms": [1854.373], "images_per_second": 0.539, "prompt_tokens": 19, "response_tokens_est": 46, "n_tiles": 6, "output_text": "The skier is dressed in a brown jacket and black pants, with a colorful beanie and goggles. The snowy landscape is illuminated by soft, diffused lighting, suggesting a calm and serene winter day.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25603.4, "ram_available_mb": 100168.7, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 25603.4, "ram_available_mb": 100168.8, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.9, "peak": 15.22, "min": 14.32}, "VDD_GPU": {"avg": 30.61, "peak": 38.19, "min": 24.42}, "VIN": {"avg": 68.06, "peak": 100.08, "min": 45.88}, "VDD_CPU_SOC_MSS": {"avg": 15.38, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 30.61, "energy_joules_est": 56.78, "sample_count": 14, "duration_seconds": 1.855}, "timestamp": "2026-01-16T16:46:14.539590"}
{"image_index": 458, "image_name": "000000051738.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051738.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2413.989, "latencies_ms": [2413.989], "images_per_second": 0.414, "prompt_tokens": 9, "response_tokens_est": 30, "n_tiles": 12, "output_text": "The image shows a neatly arranged hotel room with a bed, a lamp, and a small table, all set against a light-colored wall.", "error": null, "sys_before": {"cpu_percent": 41.6, "ram_used_mb": 25603.2, "ram_available_mb": 100169.0, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 25603.4, "ram_available_mb": 100168.8, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.18, "peak": 16.15, "min": 14.1}, "VDD_GPU": {"avg": 35.19, "peak": 41.36, "min": 27.97}, "VIN": {"avg": 78.03, "peak": 111.67, "min": 58.38}, "VDD_CPU_SOC_MSS": {"avg": 15.96, "peak": 17.32, "min": 15.35}}, "power_watts_avg": 35.19, "energy_joules_est": 84.98, "sample_count": 18, "duration_seconds": 2.415}, "timestamp": "2026-01-16T16:46:17.023875"}
{"image_index": 458, "image_name": "000000051738.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051738.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2478.252, "latencies_ms": [2478.252], "images_per_second": 0.404, "prompt_tokens": 23, "response_tokens_est": 28, "n_tiles": 12, "output_text": "bed: 1\npillow: 2\ncouch: 1\nlamp: 1\ntable: 1", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25603.4, "ram_available_mb": 100168.8, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 2.6, "ram_used_mb": 25603.7, "ram_available_mb": 100168.5, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.06, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 36.11, "peak": 42.94, "min": 27.57}, "VIN": {"avg": 75.82, "peak": 113.09, "min": 59.47}, "VDD_CPU_SOC_MSS": {"avg": 15.7, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 36.11, "energy_joules_est": 89.5, "sample_count": 19, "duration_seconds": 2.479}, "timestamp": "2026-01-16T16:46:19.512521"}
{"image_index": 458, "image_name": "000000051738.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051738.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 4838.578, "latencies_ms": [4838.578], "images_per_second": 0.207, "prompt_tokens": 27, "response_tokens_est": 98, "n_tiles": 12, "output_text": "The main objects in the image are a bed and a nightstand. The bed is positioned in the foreground, with a white bedspread and multiple pillows. The nightstand is located to the right of the bed, with a lamp and a small stack of books. The background features a window with curtains, a blue armchair, and a blue sofa. The overall spatial relationship is that the bed is the central focus, with the nightstand and other furniture pieces positioned around it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25603.7, "ram_available_mb": 100168.5, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 25603.7, "ram_available_mb": 100168.5, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.08, "peak": 15.65, "min": 14.43}, "VDD_GPU": {"avg": 31.42, "peak": 42.94, "min": 26.0}, "VIN": {"avg": 74.51, "peak": 117.45, "min": 58.77}, "VDD_CPU_SOC_MSS": {"avg": 15.62, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 31.42, "energy_joules_est": 152.04, "sample_count": 37, "duration_seconds": 4.839}, "timestamp": "2026-01-16T16:46:24.357464"}
{"image_index": 458, "image_name": "000000051738.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051738.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3578.462, "latencies_ms": [3578.462], "images_per_second": 0.279, "prompt_tokens": 21, "response_tokens_est": 61, "n_tiles": 12, "output_text": "The image depicts a neatly arranged hotel room with a large bed at the center. The room is well-lit, featuring a white bedspread, a light-colored wall, and a small table with a lamp on it. There are also two chairs and a small table visible in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25603.7, "ram_available_mb": 100168.5, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25603.9, "ram_available_mb": 100168.3, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.08, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 33.04, "peak": 42.53, "min": 26.39}, "VIN": {"avg": 73.35, "peak": 112.87, "min": 58.2}, "VDD_CPU_SOC_MSS": {"avg": 15.68, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 33.04, "energy_joules_est": 118.25, "sample_count": 28, "duration_seconds": 3.579}, "timestamp": "2026-01-16T16:46:27.942504"}
{"image_index": 458, "image_name": "000000051738.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051738.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3578.489, "latencies_ms": [3578.489], "images_per_second": 0.279, "prompt_tokens": 19, "response_tokens_est": 61, "n_tiles": 12, "output_text": "The room is well-lit with natural light streaming in through large windows adorned with light-colored curtains. The walls are painted in a light, neutral color, and the furniture is made of a combination of materials, including a light-colored sofa, a wooden bed frame, and a blue armchair.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25603.9, "ram_available_mb": 100168.3, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 25603.6, "ram_available_mb": 100168.5, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.09, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 33.23, "peak": 42.13, "min": 26.39}, "VIN": {"avg": 72.47, "peak": 120.6, "min": 58.39}, "VDD_CPU_SOC_MSS": {"avg": 15.64, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 33.23, "energy_joules_est": 118.92, "sample_count": 27, "duration_seconds": 3.579}, "timestamp": "2026-01-16T16:46:31.527488"}
{"image_index": 459, "image_name": "000000051938.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051938.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2387.488, "latencies_ms": [2387.488], "images_per_second": 0.419, "prompt_tokens": 9, "response_tokens_est": 29, "n_tiles": 12, "output_text": "A skier is performing a jump over a red railing on a snowy slope, surrounded by other skiers and snowboarders.", "error": null, "sys_before": {"cpu_percent": 44.0, "ram_used_mb": 25603.6, "ram_available_mb": 100168.5, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 2.6, "ram_used_mb": 25603.6, "ram_available_mb": 100168.5, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.11, "peak": 16.05, "min": 13.79}, "VDD_GPU": {"avg": 35.41, "peak": 41.36, "min": 27.97}, "VIN": {"avg": 74.76, "peak": 115.65, "min": 58.28}, "VDD_CPU_SOC_MSS": {"avg": 15.79, "peak": 16.92, "min": 14.96}}, "power_watts_avg": 35.41, "energy_joules_est": 84.55, "sample_count": 18, "duration_seconds": 2.388}, "timestamp": "2026-01-16T16:46:34.007065"}
{"image_index": 459, "image_name": "000000051938.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051938.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2872.733, "latencies_ms": [2872.733], "images_per_second": 0.348, "prompt_tokens": 23, "response_tokens_est": 40, "n_tiles": 12, "output_text": "object: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25603.6, "ram_available_mb": 100168.5, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 25603.6, "ram_available_mb": 100168.5, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.1, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 35.02, "peak": 42.94, "min": 26.79}, "VIN": {"avg": 78.03, "peak": 115.03, "min": 58.57}, "VDD_CPU_SOC_MSS": {"avg": 15.66, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 35.02, "energy_joules_est": 100.61, "sample_count": 22, "duration_seconds": 2.873}, "timestamp": "2026-01-16T16:46:36.886726"}
{"image_index": 459, "image_name": "000000051938.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051938.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 4153.959, "latencies_ms": [4153.959], "images_per_second": 0.241, "prompt_tokens": 27, "response_tokens_est": 78, "n_tiles": 12, "output_text": "The main object in the foreground is a person skiing down a snowy slope. The person is positioned near the center of the image, slightly to the right. In the background, there is a ski lift with several people on it, and a red fence is visible. The ski lift is located further back on the slope, and the fence is near the top of the slope.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25603.6, "ram_available_mb": 100168.5, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25603.6, "ram_available_mb": 100168.5, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.12, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 32.28, "peak": 42.94, "min": 26.0}, "VIN": {"avg": 76.82, "peak": 133.09, "min": 55.34}, "VDD_CPU_SOC_MSS": {"avg": 15.67, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 32.28, "energy_joules_est": 134.1, "sample_count": 32, "duration_seconds": 4.154}, "timestamp": "2026-01-16T16:46:41.047361"}
{"image_index": 459, "image_name": "000000051938.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051938.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3613.866, "latencies_ms": [3613.866], "images_per_second": 0.277, "prompt_tokens": 21, "response_tokens_est": 62, "n_tiles": 12, "output_text": "The image depicts a snowy mountain slope with a skier in mid-air, performing a jump. The skier is dressed in a white jacket and orange pants, and is surrounded by other skiers and snowboarders on the slope. The sky is clear and blue, indicating a sunny day.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25603.6, "ram_available_mb": 100168.5, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25603.6, "ram_available_mb": 100168.5, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.09, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 32.92, "peak": 42.54, "min": 26.39}, "VIN": {"avg": 72.24, "peak": 107.09, "min": 58.24}, "VDD_CPU_SOC_MSS": {"avg": 15.66, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 32.92, "energy_joules_est": 118.99, "sample_count": 28, "duration_seconds": 3.615}, "timestamp": "2026-01-16T16:46:44.667900"}
{"image_index": 459, "image_name": "000000051938.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051938.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3247.63, "latencies_ms": [3247.63], "images_per_second": 0.308, "prompt_tokens": 19, "response_tokens_est": 51, "n_tiles": 12, "output_text": "The image depicts a vibrant winter scene with a clear blue sky, bright sunlight, and a bright red ski lift. The snow-covered mountain slopes are illuminated by the sunlight, creating a striking contrast between the bright colors and the white snow.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25603.6, "ram_available_mb": 100168.5, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 25603.6, "ram_available_mb": 100168.6, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.08, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 33.86, "peak": 42.53, "min": 26.39}, "VIN": {"avg": 73.59, "peak": 107.75, "min": 58.57}, "VDD_CPU_SOC_MSS": {"avg": 15.67, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 33.86, "energy_joules_est": 109.98, "sample_count": 25, "duration_seconds": 3.248}, "timestamp": "2026-01-16T16:46:47.921981"}
{"image_index": 460, "image_name": "000000051961.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051961.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1498.497, "latencies_ms": [1498.497], "images_per_second": 0.667, "prompt_tokens": 9, "response_tokens_est": 34, "n_tiles": 6, "output_text": "The image shows a close-up view of a metal parking meter with graffiti on its side, set against a backdrop of a colorful and heavily adorned urban wall.", "error": null, "sys_before": {"cpu_percent": 39.0, "ram_used_mb": 25603.6, "ram_available_mb": 100168.6, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 25603.6, "ram_available_mb": 100168.6, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.43, "peak": 15.01, "min": 13.82}, "VDD_GPU": {"avg": 31.76, "peak": 37.82, "min": 25.6}, "VIN": {"avg": 65.22, "peak": 117.42, "min": 50.68}, "VDD_CPU_SOC_MSS": {"avg": 15.18, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 31.76, "energy_joules_est": 47.61, "sample_count": 11, "duration_seconds": 1.499}, "timestamp": "2026-01-16T16:46:49.485761"}
{"image_index": 460, "image_name": "000000051961.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051961.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1647.15, "latencies_ms": [1647.15], "images_per_second": 0.607, "prompt_tokens": 23, "response_tokens_est": 38, "n_tiles": 6, "output_text": "1. Graffiti\n2. Metal parking meter\n3. Wall\n4. Street\n5. Sidewalk\n6. Urban environment\n7. Art\n8. Street", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25603.6, "ram_available_mb": 100168.6, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 25603.6, "ram_available_mb": 100168.6, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.77, "peak": 15.11, "min": 14.22}, "VDD_GPU": {"avg": 31.87, "peak": 38.19, "min": 25.21}, "VIN": {"avg": 66.21, "peak": 83.28, "min": 53.27}, "VDD_CPU_SOC_MSS": {"avg": 15.42, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 31.87, "energy_joules_est": 52.51, "sample_count": 12, "duration_seconds": 1.648}, "timestamp": "2026-01-16T16:46:51.139192"}
{"image_index": 460, "image_name": "000000051961.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051961.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2639.631, "latencies_ms": [2639.631], "images_per_second": 0.379, "prompt_tokens": 27, "response_tokens_est": 72, "n_tiles": 6, "output_text": "The main objects in the image are a metal parking meter and a wall adorned with graffiti. The parking meter is positioned near the bottom left corner of the image, while the graffiti is prominently displayed on the wall, occupying the majority of the background. The graffiti is in the foreground, with the parking meter situated in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25603.6, "ram_available_mb": 100168.6, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 25603.6, "ram_available_mb": 100168.6, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.79, "peak": 15.01, "min": 14.12}, "VDD_GPU": {"avg": 28.46, "peak": 38.19, "min": 23.24}, "VIN": {"avg": 70.0, "peak": 111.29, "min": 61.46}, "VDD_CPU_SOC_MSS": {"avg": 15.37, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 28.46, "energy_joules_est": 75.14, "sample_count": 20, "duration_seconds": 2.64}, "timestamp": "2026-01-16T16:46:53.784968"}
{"image_index": 460, "image_name": "000000051961.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051961.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2524.122, "latencies_ms": [2524.122], "images_per_second": 0.396, "prompt_tokens": 21, "response_tokens_est": 68, "n_tiles": 6, "output_text": "The image depicts a vibrant urban street scene with a graffiti-covered wall as the backdrop. The wall is adorned with various colorful graffiti tags, including a prominent one featuring a star. The scene is set in an urban environment, likely a city street, with a metal pole and a sign with graffiti on it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25603.6, "ram_available_mb": 100168.6, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 25603.6, "ram_available_mb": 100168.6, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.76, "peak": 15.01, "min": 14.02}, "VDD_GPU": {"avg": 28.57, "peak": 38.21, "min": 23.24}, "VIN": {"avg": 68.34, "peak": 112.51, "min": 57.16}, "VDD_CPU_SOC_MSS": {"avg": 15.27, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 28.57, "energy_joules_est": 72.13, "sample_count": 19, "duration_seconds": 2.525}, "timestamp": "2026-01-16T16:46:56.315223"}
{"image_index": 460, "image_name": "000000051961.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051961.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2780.837, "latencies_ms": [2780.837], "images_per_second": 0.36, "prompt_tokens": 19, "response_tokens_est": 77, "n_tiles": 6, "output_text": "The image showcases a vibrant urban street scene with a graffiti-covered wall. The wall is adorned with various colors, including blue, yellow, and black, creating a dynamic and colorful backdrop. The lighting is bright, casting sharp shadows and highlighting the textures of the graffiti. The weather appears to be clear, with no visible signs of rain or adverse weather conditions.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25603.6, "ram_available_mb": 100168.6, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 25603.6, "ram_available_mb": 100168.6, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.77, "peak": 15.01, "min": 14.02}, "VDD_GPU": {"avg": 28.02, "peak": 38.21, "min": 23.24}, "VIN": {"avg": 70.51, "peak": 113.32, "min": 56.73}, "VDD_CPU_SOC_MSS": {"avg": 15.24, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 28.02, "energy_joules_est": 77.93, "sample_count": 21, "duration_seconds": 2.781}, "timestamp": "2026-01-16T16:46:59.102387"}
{"image_index": 461, "image_name": "000000051976.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051976.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1134.251, "latencies_ms": [1134.251], "images_per_second": 0.882, "prompt_tokens": 9, "response_tokens_est": 20, "n_tiles": 6, "output_text": "A surfer is riding a wave in the ocean, skillfully maneuvering their surfboard.", "error": null, "sys_before": {"cpu_percent": 44.7, "ram_used_mb": 25603.6, "ram_available_mb": 100168.6, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25604.3, "ram_available_mb": 100167.8, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.25, "peak": 14.61, "min": 13.82}, "VDD_GPU": {"avg": 32.94, "peak": 37.03, "min": 27.97}, "VIN": {"avg": 74.67, "peak": 110.74, "min": 55.12}, "VDD_CPU_SOC_MSS": {"avg": 14.91, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 32.94, "energy_joules_est": 37.37, "sample_count": 8, "duration_seconds": 1.135}, "timestamp": "2026-01-16T16:47:00.294902"}
{"image_index": 461, "image_name": "000000051976.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051976.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1740.871, "latencies_ms": [1740.871], "images_per_second": 0.574, "prompt_tokens": 23, "response_tokens_est": 40, "n_tiles": 6, "output_text": "object: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25604.3, "ram_available_mb": 100167.8, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25604.3, "ram_available_mb": 100167.8, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.59, "peak": 14.91, "min": 14.02}, "VDD_GPU": {"avg": 31.51, "peak": 38.98, "min": 24.42}, "VIN": {"avg": 66.93, "peak": 111.21, "min": 56.06}, "VDD_CPU_SOC_MSS": {"avg": 15.11, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 31.51, "energy_joules_est": 54.87, "sample_count": 13, "duration_seconds": 1.741}, "timestamp": "2026-01-16T16:47:02.042087"}
{"image_index": 461, "image_name": "000000051976.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051976.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2362.085, "latencies_ms": [2362.085], "images_per_second": 0.423, "prompt_tokens": 27, "response_tokens_est": 62, "n_tiles": 6, "output_text": "The main object in the foreground is a surfer riding a wave. The wave is in the background, creating a clear visual separation between the surfer and the water. The surfer is positioned near the center of the image, while the wave is to the right, creating a dynamic and engaging scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25604.3, "ram_available_mb": 100167.8, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 25604.3, "ram_available_mb": 100167.8, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.7, "peak": 15.01, "min": 14.02}, "VDD_GPU": {"avg": 28.89, "peak": 38.6, "min": 23.24}, "VIN": {"avg": 69.3, "peak": 117.19, "min": 56.78}, "VDD_CPU_SOC_MSS": {"avg": 15.24, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 28.89, "energy_joules_est": 68.25, "sample_count": 18, "duration_seconds": 2.362}, "timestamp": "2026-01-16T16:47:04.410057"}
{"image_index": 461, "image_name": "000000051976.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051976.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2628.958, "latencies_ms": [2628.958], "images_per_second": 0.38, "prompt_tokens": 21, "response_tokens_est": 72, "n_tiles": 6, "output_text": "The image captures a dynamic scene of a surfer riding a wave in the ocean. The surfer is positioned on a white surfboard, skillfully navigating the wave, which is breaking to the right side of the surfer. The ocean is a deep blue, and the water appears to be relatively calm, with small ripples around the surfer.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25604.3, "ram_available_mb": 100167.8, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 25604.3, "ram_available_mb": 100167.8, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.79, "peak": 15.11, "min": 14.02}, "VDD_GPU": {"avg": 28.22, "peak": 38.21, "min": 23.24}, "VIN": {"avg": 68.47, "peak": 122.33, "min": 55.47}, "VDD_CPU_SOC_MSS": {"avg": 15.43, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 28.22, "energy_joules_est": 74.21, "sample_count": 20, "duration_seconds": 2.63}, "timestamp": "2026-01-16T16:47:07.046524"}
{"image_index": 461, "image_name": "000000051976.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000051976.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2321.639, "latencies_ms": [2321.639], "images_per_second": 0.431, "prompt_tokens": 19, "response_tokens_est": 61, "n_tiles": 6, "output_text": "The image depicts a scene of a surfer riding a wave in the ocean. The water is a deep blue, and the wave is white, indicating a strong and powerful wave. The lighting is natural, with the sun shining from the left side, casting a blue hue over the entire scene.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 25604.3, "ram_available_mb": 100167.8, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 25604.3, "ram_available_mb": 100167.8, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.73, "peak": 15.01, "min": 14.02}, "VDD_GPU": {"avg": 28.89, "peak": 38.6, "min": 23.24}, "VIN": {"avg": 71.01, "peak": 113.85, "min": 56.02}, "VDD_CPU_SOC_MSS": {"avg": 15.22, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 28.89, "energy_joules_est": 67.09, "sample_count": 18, "duration_seconds": 2.322}, "timestamp": "2026-01-16T16:47:09.374983"}
{"image_index": 462, "image_name": "000000052007.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052007.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 3957.195, "latencies_ms": [3957.195], "images_per_second": 0.253, "prompt_tokens": 9, "response_tokens_est": 76, "n_tiles": 12, "output_text": "A yellow double-decker bus is parked on a street, with a sign displaying the destination \"Lytham\" and the number \"11.\" Two men are standing on the sidewalk, one of whom is looking at the bus while the other is engaged in a conversation. The street is lined with a variety of plants and flowers, and there is a red brick pavement.", "error": null, "sys_before": {"cpu_percent": 41.7, "ram_used_mb": 25604.3, "ram_available_mb": 100167.8, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 25604.6, "ram_available_mb": 100167.6, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.14, "peak": 16.15, "min": 14.0}, "VDD_GPU": {"avg": 31.58, "peak": 40.97, "min": 26.0}, "VIN": {"avg": 73.09, "peak": 114.67, "min": 58.54}, "VDD_CPU_SOC_MSS": {"avg": 15.89, "peak": 17.32, "min": 15.35}}, "power_watts_avg": 31.58, "energy_joules_est": 124.98, "sample_count": 30, "duration_seconds": 3.958}, "timestamp": "2026-01-16T16:47:13.415631"}
{"image_index": 462, "image_name": "000000052007.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052007.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2580.904, "latencies_ms": [2580.904], "images_per_second": 0.387, "prompt_tokens": 23, "response_tokens_est": 31, "n_tiles": 12, "output_text": "bus: 1\npeople: 2\nbuilding: 1\nstreet: 1\nsign: 1\ntrees: 0", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25604.6, "ram_available_mb": 100167.6, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 25604.6, "ram_available_mb": 100167.6, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.06, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 35.67, "peak": 42.94, "min": 27.57}, "VIN": {"avg": 74.55, "peak": 123.62, "min": 54.82}, "VDD_CPU_SOC_MSS": {"avg": 15.73, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 35.67, "energy_joules_est": 92.08, "sample_count": 20, "duration_seconds": 2.581}, "timestamp": "2026-01-16T16:47:16.003306"}
{"image_index": 462, "image_name": "000000052007.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052007.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 4427.324, "latencies_ms": [4427.324], "images_per_second": 0.226, "prompt_tokens": 27, "response_tokens_est": 86, "n_tiles": 12, "output_text": "The bus is parked on the side of the road, with the number 346 displayed on its front. The bus is positioned near the curb, with the number 11 on the digital display. In the background, there is a building with a red roof and a blue structure, possibly a part of the bus station or a nearby building. The sidewalk is visible in the foreground, with a few people standing near the bus.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25604.6, "ram_available_mb": 100167.6, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 25604.8, "ram_available_mb": 100167.4, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.11, "peak": 15.75, "min": 14.53}, "VDD_GPU": {"avg": 31.92, "peak": 42.94, "min": 26.0}, "VIN": {"avg": 72.13, "peak": 104.81, "min": 58.57}, "VDD_CPU_SOC_MSS": {"avg": 15.78, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 31.92, "energy_joules_est": 141.34, "sample_count": 34, "duration_seconds": 4.428}, "timestamp": "2026-01-16T16:47:20.437547"}
{"image_index": 462, "image_name": "000000052007.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052007.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 4599.904, "latencies_ms": [4599.904], "images_per_second": 0.217, "prompt_tokens": 21, "response_tokens_est": 91, "n_tiles": 12, "output_text": "The image depicts a scene on a city street with a yellow double-decker bus displaying the destination \"Lytham 11\" on its digital sign. The bus is parked on the side of the road, and two men are standing nearby, one of whom is looking at the bus while the other is engaged in a conversation. The street is lined with a mix of brick and paving stones, and there are some flowers in the foreground.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25604.8, "ram_available_mb": 100167.4, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 25604.6, "ram_available_mb": 100167.6, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.1, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 31.61, "peak": 42.94, "min": 26.0}, "VIN": {"avg": 73.66, "peak": 119.28, "min": 58.26}, "VDD_CPU_SOC_MSS": {"avg": 15.66, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 31.61, "energy_joules_est": 145.41, "sample_count": 36, "duration_seconds": 4.6}, "timestamp": "2026-01-16T16:47:25.044048"}
{"image_index": 462, "image_name": "000000052007.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052007.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3209.065, "latencies_ms": [3209.065], "images_per_second": 0.312, "prompt_tokens": 19, "response_tokens_est": 50, "n_tiles": 12, "output_text": "The bus in the image is yellow with black accents, featuring a digital display showing the route number \"11\" and destination \"Lytham.\" The scene is well-lit with natural daylight, and the weather appears to be clear and sunny.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25604.6, "ram_available_mb": 100167.6, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 25604.6, "ram_available_mb": 100167.6, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.1, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 33.81, "peak": 42.53, "min": 26.39}, "VIN": {"avg": 74.56, "peak": 120.01, "min": 58.32}, "VDD_CPU_SOC_MSS": {"avg": 15.76, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 33.81, "energy_joules_est": 108.51, "sample_count": 25, "duration_seconds": 3.209}, "timestamp": "2026-01-16T16:47:28.259472"}
{"image_index": 463, "image_name": "000000052017.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052017.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1630.93, "latencies_ms": [1630.93], "images_per_second": 0.613, "prompt_tokens": 9, "response_tokens_est": 39, "n_tiles": 6, "output_text": "The image depicts a small, yellow and red airplane with the registration number \"SP-AWF\" on its fuselage, captured in mid-flight against a cloudy sky.", "error": null, "sys_before": {"cpu_percent": 43.0, "ram_used_mb": 25604.6, "ram_available_mb": 100167.6, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 25605.0, "ram_available_mb": 100167.1, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.49, "peak": 15.01, "min": 13.82}, "VDD_GPU": {"avg": 31.25, "peak": 37.82, "min": 24.82}, "VIN": {"avg": 72.08, "peak": 99.51, "min": 54.44}, "VDD_CPU_SOC_MSS": {"avg": 15.25, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 31.25, "energy_joules_est": 50.98, "sample_count": 12, "duration_seconds": 1.631}, "timestamp": "2026-01-16T16:47:29.952433"}
{"image_index": 463, "image_name": "000000052017.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052017.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1706.293, "latencies_ms": [1706.293], "images_per_second": 0.586, "prompt_tokens": 23, "response_tokens_est": 40, "n_tiles": 6, "output_text": "object: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25605.0, "ram_available_mb": 100167.1, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 25605.0, "ram_available_mb": 100167.1, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.8, "peak": 15.11, "min": 14.22}, "VDD_GPU": {"avg": 31.02, "peak": 38.19, "min": 24.42}, "VIN": {"avg": 72.49, "peak": 109.12, "min": 61.88}, "VDD_CPU_SOC_MSS": {"avg": 15.35, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 31.02, "energy_joules_est": 52.94, "sample_count": 13, "duration_seconds": 1.707}, "timestamp": "2026-01-16T16:47:31.665669"}
{"image_index": 463, "image_name": "000000052017.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052017.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2350.64, "latencies_ms": [2350.64], "images_per_second": 0.425, "prompt_tokens": 27, "response_tokens_est": 64, "n_tiles": 6, "output_text": "The main object in the image is a small yellow airplane with a red tail and landing gear. The airplane is positioned in the foreground, slightly to the left, and is in the process of landing. The background features a cloudy sky, providing a neutral backdrop that contrasts with the bright colors of the airplane.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25605.0, "ram_available_mb": 100167.1, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 25605.0, "ram_available_mb": 100167.1, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.92, "peak": 15.22, "min": 14.22}, "VDD_GPU": {"avg": 29.28, "peak": 38.21, "min": 23.64}, "VIN": {"avg": 73.21, "peak": 115.21, "min": 52.54}, "VDD_CPU_SOC_MSS": {"avg": 15.55, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 29.28, "energy_joules_est": 68.84, "sample_count": 18, "duration_seconds": 2.351}, "timestamp": "2026-01-16T16:47:34.022442"}
{"image_index": 463, "image_name": "000000052017.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052017.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2016.715, "latencies_ms": [2016.715], "images_per_second": 0.496, "prompt_tokens": 21, "response_tokens_est": 52, "n_tiles": 6, "output_text": "The image depicts a small, yellow airplane with a red tail and landing gear, captured in mid-flight against a cloudy sky. The airplane is likely preparing for landing, as its landing gear is extended and the plane is angled slightly downward.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25605.0, "ram_available_mb": 100167.1, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 25605.0, "ram_available_mb": 100167.1, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.89, "peak": 15.22, "min": 14.32}, "VDD_GPU": {"avg": 30.2, "peak": 37.82, "min": 24.42}, "VIN": {"avg": 76.66, "peak": 115.66, "min": 62.96}, "VDD_CPU_SOC_MSS": {"avg": 15.49, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 30.2, "energy_joules_est": 60.92, "sample_count": 15, "duration_seconds": 2.017}, "timestamp": "2026-01-16T16:47:36.045994"}
{"image_index": 463, "image_name": "000000052017.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052017.jpg", "image_width": 640, "image_height": 425, "image_resolution": "640x425", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2366.558, "latencies_ms": [2366.558], "images_per_second": 0.423, "prompt_tokens": 19, "response_tokens_est": 65, "n_tiles": 6, "output_text": "The notable visual attributes of the airplane in the image include its bright yellow body with a red tail and wingtip, which stands out against the overcast sky. The lighting is soft and diffused, casting a gentle glow on the aircraft's surface. The weather appears to be clear, with no visible clouds or precipitation.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25605.0, "ram_available_mb": 100167.1, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 25605.0, "ram_available_mb": 100167.2, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.94, "peak": 15.22, "min": 14.32}, "VDD_GPU": {"avg": 29.21, "peak": 38.19, "min": 23.64}, "VIN": {"avg": 68.21, "peak": 104.27, "min": 57.96}, "VDD_CPU_SOC_MSS": {"avg": 15.53, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 29.21, "energy_joules_est": 69.14, "sample_count": 18, "duration_seconds": 2.367}, "timestamp": "2026-01-16T16:47:38.418573"}
{"image_index": 464, "image_name": "000000052412.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052412.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1362.554, "latencies_ms": [1362.554], "images_per_second": 0.734, "prompt_tokens": 9, "response_tokens_est": 29, "n_tiles": 6, "output_text": "The image shows a large parking lot filled with numerous cars, with a view of a cityscape and a clear blue sky in the background.", "error": null, "sys_before": {"cpu_percent": 38.2, "ram_used_mb": 25605.0, "ram_available_mb": 100167.2, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25605.0, "ram_available_mb": 100167.2, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.47, "peak": 14.91, "min": 13.92}, "VDD_GPU": {"avg": 32.06, "peak": 37.42, "min": 26.39}, "VIN": {"avg": 69.84, "peak": 123.02, "min": 50.72}, "VDD_CPU_SOC_MSS": {"avg": 15.23, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 32.06, "energy_joules_est": 43.7, "sample_count": 10, "duration_seconds": 1.363}, "timestamp": "2026-01-16T16:47:39.837458"}
{"image_index": 464, "image_name": "000000052412.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052412.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1702.58, "latencies_ms": [1702.58], "images_per_second": 0.587, "prompt_tokens": 23, "response_tokens_est": 40, "n_tiles": 6, "output_text": "object: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25605.0, "ram_available_mb": 100167.2, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 25605.5, "ram_available_mb": 100166.7, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.77, "peak": 15.11, "min": 14.22}, "VDD_GPU": {"avg": 31.9, "peak": 38.59, "min": 25.21}, "VIN": {"avg": 74.01, "peak": 114.11, "min": 55.87}, "VDD_CPU_SOC_MSS": {"avg": 15.42, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 31.9, "energy_joules_est": 54.33, "sample_count": 12, "duration_seconds": 1.703}, "timestamp": "2026-01-16T16:47:41.550510"}
{"image_index": 464, "image_name": "000000052412.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052412.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2611.603, "latencies_ms": [2611.603], "images_per_second": 0.383, "prompt_tokens": 27, "response_tokens_est": 74, "n_tiles": 6, "output_text": "The main objects in the image are a parking lot filled with numerous cars, a large building in the background, and a clear blue sky above. The parking lot is located in the foreground, with the cars occupying the majority of the space. The building is situated in the background, slightly to the right, and the clear blue sky is above the parking lot.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25605.5, "ram_available_mb": 100166.7, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 25605.3, "ram_available_mb": 100166.9, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.97, "peak": 15.32, "min": 14.22}, "VDD_GPU": {"avg": 28.94, "peak": 38.21, "min": 23.64}, "VIN": {"avg": 72.84, "peak": 119.58, "min": 57.7}, "VDD_CPU_SOC_MSS": {"avg": 15.52, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 28.94, "energy_joules_est": 75.59, "sample_count": 19, "duration_seconds": 2.612}, "timestamp": "2026-01-16T16:47:44.172974"}
{"image_index": 464, "image_name": "000000052412.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052412.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1964.374, "latencies_ms": [1964.374], "images_per_second": 0.509, "prompt_tokens": 21, "response_tokens_est": 50, "n_tiles": 6, "output_text": "The image depicts an aerial view of a large parking lot filled with numerous cars, situated in an urban area with a mix of residential and commercial buildings in the background. The sky is clear with a few scattered clouds, suggesting a pleasant day.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25605.3, "ram_available_mb": 100166.9, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 25605.3, "ram_available_mb": 100166.9, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.89, "peak": 15.22, "min": 14.32}, "VDD_GPU": {"avg": 30.23, "peak": 37.82, "min": 24.42}, "VIN": {"avg": 74.11, "peak": 118.11, "min": 61.54}, "VDD_CPU_SOC_MSS": {"avg": 15.46, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 30.23, "energy_joules_est": 59.4, "sample_count": 15, "duration_seconds": 1.965}, "timestamp": "2026-01-16T16:47:46.143574"}
{"image_index": 464, "image_name": "000000052412.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052412.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1616.28, "latencies_ms": [1616.28], "images_per_second": 0.619, "prompt_tokens": 19, "response_tokens_est": 37, "n_tiles": 6, "output_text": "The image shows a parking lot filled with numerous cars, with a clear blue sky overhead. The parking lot is well-lit by natural sunlight, creating a bright and vibrant atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25605.3, "ram_available_mb": 100166.9, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25605.3, "ram_available_mb": 100166.9, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.8, "peak": 15.11, "min": 14.22}, "VDD_GPU": {"avg": 31.91, "peak": 38.21, "min": 25.21}, "VIN": {"avg": 74.64, "peak": 124.31, "min": 63.15}, "VDD_CPU_SOC_MSS": {"avg": 15.48, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 31.91, "energy_joules_est": 51.58, "sample_count": 12, "duration_seconds": 1.617}, "timestamp": "2026-01-16T16:47:47.766342"}
{"image_index": 465, "image_name": "000000052413.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052413.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2549.435, "latencies_ms": [2549.435], "images_per_second": 0.392, "prompt_tokens": 9, "response_tokens_est": 34, "n_tiles": 12, "output_text": "A child is holding a pink toy phone with a cartoon character on the screen, while sitting on a couch with a magazine and a can of soda in the background.", "error": null, "sys_before": {"cpu_percent": 45.0, "ram_used_mb": 25605.3, "ram_available_mb": 100166.9, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 25606.2, "ram_available_mb": 100165.9, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.17, "peak": 16.15, "min": 14.1}, "VDD_GPU": {"avg": 34.75, "peak": 41.36, "min": 27.18}, "VIN": {"avg": 75.35, "peak": 115.86, "min": 58.66}, "VDD_CPU_SOC_MSS": {"avg": 15.97, "peak": 17.32, "min": 15.35}}, "power_watts_avg": 34.75, "energy_joules_est": 88.6, "sample_count": 19, "duration_seconds": 2.55}, "timestamp": "2026-01-16T16:47:50.394579"}
{"image_index": 465, "image_name": "000000052413.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052413.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2947.934, "latencies_ms": [2947.934], "images_per_second": 0.339, "prompt_tokens": 23, "response_tokens_est": 42, "n_tiles": 12, "output_text": "- hand: 1\n- phone: 1\n- book: 1\n- cup: 1\n- chair: 1\n- gift: 1\n- blanket: 1", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25606.2, "ram_available_mb": 100165.9, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 25606.2, "ram_available_mb": 100165.9, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.07, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 34.68, "peak": 43.33, "min": 26.79}, "VIN": {"avg": 73.01, "peak": 105.27, "min": 58.61}, "VDD_CPU_SOC_MSS": {"avg": 15.64, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 34.68, "energy_joules_est": 102.25, "sample_count": 23, "duration_seconds": 2.948}, "timestamp": "2026-01-16T16:47:53.349025"}
{"image_index": 465, "image_name": "000000052413.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052413.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3481.879, "latencies_ms": [3481.879], "images_per_second": 0.287, "prompt_tokens": 27, "response_tokens_est": 58, "n_tiles": 12, "output_text": "The main object in the foreground is a child's pink mobile phone with a cartoon character on the screen. The child is holding the phone with both hands. In the background, there is a person's arm resting on a chair, and a can of soda is visible on a table.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25606.2, "ram_available_mb": 100165.9, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 25606.2, "ram_available_mb": 100165.9, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.09, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 33.47, "peak": 42.94, "min": 26.39}, "VIN": {"avg": 72.41, "peak": 116.77, "min": 47.35}, "VDD_CPU_SOC_MSS": {"avg": 15.63, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 33.47, "energy_joules_est": 116.55, "sample_count": 27, "duration_seconds": 3.482}, "timestamp": "2026-01-16T16:47:56.838944"}
{"image_index": 465, "image_name": "000000052413.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052413.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3490.403, "latencies_ms": [3490.403], "images_per_second": 0.286, "prompt_tokens": 21, "response_tokens_est": 58, "n_tiles": 12, "output_text": "The image depicts a cozy indoor setting where a person is seated on a couch, holding a pink mobile phone with a cartoon character on the screen. The person is also holding a magazine or book, and there are various items on the couch, including a can and a wrapped gift.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25606.2, "ram_available_mb": 100165.9, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25606.2, "ram_available_mb": 100166.0, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.08, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 33.44, "peak": 42.94, "min": 26.0}, "VIN": {"avg": 73.34, "peak": 114.44, "min": 51.6}, "VDD_CPU_SOC_MSS": {"avg": 15.6, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 33.44, "energy_joules_est": 116.73, "sample_count": 27, "duration_seconds": 3.491}, "timestamp": "2026-01-16T16:48:00.339811"}
{"image_index": 465, "image_name": "000000052413.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052413.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3438.219, "latencies_ms": [3438.219], "images_per_second": 0.291, "prompt_tokens": 19, "response_tokens_est": 56, "n_tiles": 12, "output_text": "The image features a person holding a pink mobile phone with a cartoon character on the screen. The phone is encased in a pink case, and the person is wearing a white shirt. The lighting is dim, and the background is blurred, suggesting a cozy indoor setting.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25606.2, "ram_available_mb": 100166.0, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 25606.5, "ram_available_mb": 100165.7, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.75, "peak": 15.34, "min": 14.12}, "VDD_GPU": {"avg": 33.36, "peak": 42.53, "min": 26.0}, "VIN": {"avg": 76.58, "peak": 121.34, "min": 58.86}, "VDD_CPU_SOC_MSS": {"avg": 15.23, "peak": 16.14, "min": 14.56}}, "power_watts_avg": 33.36, "energy_joules_est": 114.71, "sample_count": 26, "duration_seconds": 3.438}, "timestamp": "2026-01-16T16:48:03.784715"}
{"image_index": 466, "image_name": "000000052462.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052462.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1037.118, "latencies_ms": [1037.118], "images_per_second": 0.964, "prompt_tokens": 9, "response_tokens_est": 17, "n_tiles": 6, "output_text": "A zebra is standing in a field with tall, dry grass around it.", "error": null, "sys_before": {"cpu_percent": 42.2, "ram_used_mb": 25606.5, "ram_available_mb": 100165.7, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 25606.2, "ram_available_mb": 100166.0, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.15, "peak": 14.51, "min": 13.92}, "VDD_GPU": {"avg": 34.38, "peak": 37.82, "min": 29.94}, "VIN": {"avg": 79.86, "peak": 98.48, "min": 66.47}, "VDD_CPU_SOC_MSS": {"avg": 14.9, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 34.38, "energy_joules_est": 35.67, "sample_count": 7, "duration_seconds": 1.038}, "timestamp": "2026-01-16T16:48:04.886148"}
{"image_index": 466, "image_name": "000000052462.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052462.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1054.01, "latencies_ms": [1054.01], "images_per_second": 0.949, "prompt_tokens": 23, "response_tokens_est": 16, "n_tiles": 6, "output_text": "zebra: 1\ntree: 1\ngrass: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25606.2, "ram_available_mb": 100166.0, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 25606.5, "ram_available_mb": 100165.7, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.55, "peak": 14.81, "min": 14.22}, "VDD_GPU": {"avg": 35.84, "peak": 39.38, "min": 29.94}, "VIN": {"avg": 68.76, "peak": 97.94, "min": 60.21}, "VDD_CPU_SOC_MSS": {"avg": 15.2, "peak": 15.35, "min": 14.96}}, "power_watts_avg": 35.84, "energy_joules_est": 37.79, "sample_count": 8, "duration_seconds": 1.054}, "timestamp": "2026-01-16T16:48:05.946415"}
{"image_index": 466, "image_name": "000000052462.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052462.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2526.642, "latencies_ms": [2526.642], "images_per_second": 0.396, "prompt_tokens": 27, "response_tokens_est": 68, "n_tiles": 6, "output_text": "The main objects in the image are two zebras standing in a grassy field. The foreground features a zebra with a black and white striped coat, while the background shows another zebra with a similar pattern. The zebra in the foreground is closer to the camera, while the one in the background is further away.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25606.5, "ram_available_mb": 100165.7, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 25606.2, "ram_available_mb": 100166.0, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.73, "peak": 15.01, "min": 14.02}, "VDD_GPU": {"avg": 29.42, "peak": 39.77, "min": 23.24}, "VIN": {"avg": 69.19, "peak": 114.32, "min": 55.74}, "VDD_CPU_SOC_MSS": {"avg": 15.31, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 29.42, "energy_joules_est": 74.35, "sample_count": 19, "duration_seconds": 2.527}, "timestamp": "2026-01-16T16:48:08.480390"}
{"image_index": 466, "image_name": "000000052462.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052462.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2029.823, "latencies_ms": [2029.823], "images_per_second": 0.493, "prompt_tokens": 21, "response_tokens_est": 51, "n_tiles": 6, "output_text": "The image depicts a serene African savanna landscape with a zebra standing in the foreground. The zebra is surrounded by tall, dry grass, and the scene is bathed in the warm, golden light of a clear, sunny day.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 25606.2, "ram_available_mb": 100166.0, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 25606.2, "ram_available_mb": 100166.0, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.78, "peak": 15.11, "min": 14.12}, "VDD_GPU": {"avg": 29.77, "peak": 38.21, "min": 23.64}, "VIN": {"avg": 72.1, "peak": 112.97, "min": 54.72}, "VDD_CPU_SOC_MSS": {"avg": 15.3, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 29.77, "energy_joules_est": 60.44, "sample_count": 16, "duration_seconds": 2.03}, "timestamp": "2026-01-16T16:48:10.516264"}
{"image_index": 466, "image_name": "000000052462.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052462.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2446.273, "latencies_ms": [2446.273], "images_per_second": 0.409, "prompt_tokens": 19, "response_tokens_est": 65, "n_tiles": 6, "output_text": "The image features a zebra standing in a dry, grassy savanna. The zebra's distinctive black and white stripes are clearly visible against the backdrop of the dry, golden-brown grass. The lighting is bright and natural, suggesting a sunny day, with the sun casting shadows on the zebra's body.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25606.2, "ram_available_mb": 100166.0, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 25606.4, "ram_available_mb": 100165.7, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.74, "peak": 15.01, "min": 14.02}, "VDD_GPU": {"avg": 28.57, "peak": 38.6, "min": 23.24}, "VIN": {"avg": 71.15, "peak": 112.9, "min": 59.0}, "VDD_CPU_SOC_MSS": {"avg": 15.31, "peak": 15.75, "min": 14.57}}, "power_watts_avg": 28.57, "energy_joules_est": 69.9, "sample_count": 19, "duration_seconds": 2.447}, "timestamp": "2026-01-16T16:48:12.968557"}
{"image_index": 467, "image_name": "000000052507.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052507.jpg", "image_width": 640, "image_height": 438, "image_resolution": "640x438", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1115.685, "latencies_ms": [1115.685], "images_per_second": 0.896, "prompt_tokens": 9, "response_tokens_est": 20, "n_tiles": 6, "output_text": "A man is standing in the water holding a yellow surfboard, ready to ride the waves.", "error": null, "sys_before": {"cpu_percent": 46.8, "ram_used_mb": 25606.4, "ram_available_mb": 100165.7, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 25606.2, "ram_available_mb": 100166.0, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.29, "peak": 14.71, "min": 13.92}, "VDD_GPU": {"avg": 32.84, "peak": 37.03, "min": 28.36}, "VIN": {"avg": 73.04, "peak": 123.72, "min": 51.58}, "VDD_CPU_SOC_MSS": {"avg": 15.06, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 32.84, "energy_joules_est": 36.65, "sample_count": 8, "duration_seconds": 1.116}, "timestamp": "2026-01-16T16:48:14.142245"}
{"image_index": 467, "image_name": "000000052507.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052507.jpg", "image_width": 640, "image_height": 438, "image_resolution": "640x438", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1719.901, "latencies_ms": [1719.901], "images_per_second": 0.581, "prompt_tokens": 23, "response_tokens_est": 40, "n_tiles": 6, "output_text": "object: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25606.2, "ram_available_mb": 100166.0, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 25606.9, "ram_available_mb": 100165.2, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.68, "peak": 15.11, "min": 14.02}, "VDD_GPU": {"avg": 31.66, "peak": 39.39, "min": 24.42}, "VIN": {"avg": 69.96, "peak": 106.19, "min": 59.02}, "VDD_CPU_SOC_MSS": {"avg": 15.2, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 31.66, "energy_joules_est": 54.48, "sample_count": 13, "duration_seconds": 1.721}, "timestamp": "2026-01-16T16:48:15.868841"}
{"image_index": 467, "image_name": "000000052507.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052507.jpg", "image_width": 640, "image_height": 438, "image_resolution": "640x438", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2635.46, "latencies_ms": [2635.46], "images_per_second": 0.379, "prompt_tokens": 27, "response_tokens_est": 72, "n_tiles": 6, "output_text": "The main object in the foreground is a person standing in shallow water, holding a bright yellow surfboard. The person is positioned near the center of the image, slightly to the left. The background features the vast expanse of the ocean with waves crashing towards the shore. The surfboard is positioned near the person, closer to the water's edge.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25606.9, "ram_available_mb": 100165.2, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 25606.9, "ram_available_mb": 100165.3, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.8, "peak": 15.01, "min": 14.12}, "VDD_GPU": {"avg": 28.32, "peak": 38.59, "min": 23.24}, "VIN": {"avg": 68.23, "peak": 108.18, "min": 54.08}, "VDD_CPU_SOC_MSS": {"avg": 15.45, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 28.32, "energy_joules_est": 74.65, "sample_count": 20, "duration_seconds": 2.636}, "timestamp": "2026-01-16T16:48:18.510936"}
{"image_index": 467, "image_name": "000000052507.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052507.jpg", "image_width": 640, "image_height": 438, "image_resolution": "640x438", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2268.321, "latencies_ms": [2268.321], "images_per_second": 0.441, "prompt_tokens": 21, "response_tokens_est": 59, "n_tiles": 6, "output_text": "The image depicts a scene at the beach with a person standing in shallow water, holding a bright yellow surfboard. The person appears to be preparing to surf, with waves crashing around them. The setting is a sunny day at the beach, with clear blue skies and calm ocean waters.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25606.9, "ram_available_mb": 100165.3, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 25606.7, "ram_available_mb": 100165.5, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.74, "peak": 15.01, "min": 14.02}, "VDD_GPU": {"avg": 29.13, "peak": 38.21, "min": 23.24}, "VIN": {"avg": 70.79, "peak": 115.83, "min": 48.95}, "VDD_CPU_SOC_MSS": {"avg": 15.28, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 29.13, "energy_joules_est": 66.08, "sample_count": 17, "duration_seconds": 2.269}, "timestamp": "2026-01-16T16:48:20.785662"}
{"image_index": 467, "image_name": "000000052507.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052507.jpg", "image_width": 640, "image_height": 438, "image_resolution": "640x438", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2016.581, "latencies_ms": [2016.581], "images_per_second": 0.496, "prompt_tokens": 19, "response_tokens_est": 50, "n_tiles": 6, "output_text": "The image depicts a bright yellow surfboard with a black grip, being held by a person in the water. The scene is set on a sunny day with clear blue skies, and the waves are small and white, indicating a calm sea.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25606.7, "ram_available_mb": 100165.5, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 25606.7, "ram_available_mb": 100165.5, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.71, "peak": 15.01, "min": 14.02}, "VDD_GPU": {"avg": 29.83, "peak": 38.19, "min": 23.64}, "VIN": {"avg": 68.9, "peak": 102.56, "min": 57.46}, "VDD_CPU_SOC_MSS": {"avg": 15.22, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 29.83, "energy_joules_est": 60.16, "sample_count": 15, "duration_seconds": 2.017}, "timestamp": "2026-01-16T16:48:22.808507"}
{"image_index": 468, "image_name": "000000052565.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052565.jpg", "image_width": 640, "image_height": 458, "image_resolution": "640x458", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2050.504, "latencies_ms": [2050.504], "images_per_second": 0.488, "prompt_tokens": 9, "response_tokens_est": 19, "n_tiles": 12, "output_text": "A black and white image of a cow standing on a sandy beach near the water.", "error": null, "sys_before": {"cpu_percent": 34.2, "ram_used_mb": 25606.7, "ram_available_mb": 100165.5, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 2.3, "ram_used_mb": 25606.7, "ram_available_mb": 100165.5, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.22, "peak": 16.15, "min": 14.2}, "VDD_GPU": {"avg": 36.45, "peak": 40.97, "min": 29.94}, "VIN": {"avg": 76.23, "peak": 116.39, "min": 58.99}, "VDD_CPU_SOC_MSS": {"avg": 15.95, "peak": 17.32, "min": 14.96}}, "power_watts_avg": 36.45, "energy_joules_est": 74.76, "sample_count": 15, "duration_seconds": 2.051}, "timestamp": "2026-01-16T16:48:24.912090"}
{"image_index": 468, "image_name": "000000052565.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052565.jpg", "image_width": 640, "image_height": 458, "image_resolution": "640x458", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2610.634, "latencies_ms": [2610.634], "images_per_second": 0.383, "prompt_tokens": 23, "response_tokens_est": 32, "n_tiles": 12, "output_text": "1. Cow\n2. Cow\n3. Cow\n4. Cow\n5. Cow\n6. Cow\n7. Cow\n8. Cow", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25606.7, "ram_available_mb": 100165.5, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 25606.7, "ram_available_mb": 100165.5, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.09, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 35.74, "peak": 42.92, "min": 27.18}, "VIN": {"avg": 77.33, "peak": 118.0, "min": 56.24}, "VDD_CPU_SOC_MSS": {"avg": 15.71, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 35.74, "energy_joules_est": 93.32, "sample_count": 20, "duration_seconds": 2.611}, "timestamp": "2026-01-16T16:48:27.529505"}
{"image_index": 468, "image_name": "000000052565.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052565.jpg", "image_width": 640, "image_height": 458, "image_resolution": "640x458", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 4294.271, "latencies_ms": [4294.271], "images_per_second": 0.233, "prompt_tokens": 27, "response_tokens_est": 82, "n_tiles": 12, "output_text": "The main object in the image is a cow standing on the sandy beach. The cow is positioned in the foreground, with its body facing the camera and its head turned slightly to the side. The background consists of the calm sea, which is slightly blurred, indicating a shallow depth of field. The cow is near the water's edge, with its legs and tail extending into the sandy beach.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25606.7, "ram_available_mb": 100165.5, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25606.4, "ram_available_mb": 100165.8, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.09, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 32.14, "peak": 42.94, "min": 26.39}, "VIN": {"avg": 74.1, "peak": 117.18, "min": 58.49}, "VDD_CPU_SOC_MSS": {"avg": 15.7, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 32.14, "energy_joules_est": 138.03, "sample_count": 33, "duration_seconds": 4.295}, "timestamp": "2026-01-16T16:48:31.830424"}
{"image_index": 468, "image_name": "000000052565.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052565.jpg", "image_width": 640, "image_height": 458, "image_resolution": "640x458", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3785.893, "latencies_ms": [3785.893], "images_per_second": 0.264, "prompt_tokens": 21, "response_tokens_est": 67, "n_tiles": 12, "output_text": "The image depicts a black and white scene of a cow standing on a sandy beach near a body of water. The cow appears calm and is facing the camera, with its body partially submerged in the water. The setting is serene, with the cow's presence adding a touch of rural tranquility to the scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25606.4, "ram_available_mb": 100165.8, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25606.7, "ram_available_mb": 100165.5, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.09, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 32.75, "peak": 42.54, "min": 26.39}, "VIN": {"avg": 72.25, "peak": 100.06, "min": 58.58}, "VDD_CPU_SOC_MSS": {"avg": 15.61, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 32.75, "energy_joules_est": 124.0, "sample_count": 29, "duration_seconds": 3.786}, "timestamp": "2026-01-16T16:48:35.627002"}
{"image_index": 468, "image_name": "000000052565.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052565.jpg", "image_width": 640, "image_height": 458, "image_resolution": "640x458", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3718.063, "latencies_ms": [3718.063], "images_per_second": 0.269, "prompt_tokens": 19, "response_tokens_est": 65, "n_tiles": 12, "output_text": "The image is a black and white photograph featuring a cow standing on a sandy beach near the water. The cow has a predominantly black and white coat with white patches on its face and legs. The lighting is soft and diffused, casting gentle shadows on the sand, and the overall atmosphere is calm and serene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25606.7, "ram_available_mb": 100165.5, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25606.7, "ram_available_mb": 100165.5, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.08, "peak": 15.65, "min": 14.43}, "VDD_GPU": {"avg": 32.86, "peak": 42.54, "min": 26.39}, "VIN": {"avg": 76.2, "peak": 136.51, "min": 58.43}, "VDD_CPU_SOC_MSS": {"avg": 15.69, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 32.86, "energy_joules_est": 122.19, "sample_count": 29, "duration_seconds": 3.719}, "timestamp": "2026-01-16T16:48:39.351973"}
{"image_index": 469, "image_name": "000000052591.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052591.jpg", "image_width": 359, "image_height": 640, "image_resolution": "359x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1249.605, "latencies_ms": [1249.605], "images_per_second": 0.8, "prompt_tokens": 9, "response_tokens_est": 48, "n_tiles": 2, "output_text": "A woman in a black and white photo is standing on a snowy slope, holding a pair of skis and wearing a white sweater with a fur collar, a black belt, and black pants, with a smile on her face.", "error": null, "sys_before": {"cpu_percent": 41.0, "ram_used_mb": 25606.7, "ram_available_mb": 100165.5, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25606.4, "ram_available_mb": 100165.8, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5055.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.88, "peak": 15.32, "min": 14.1}, "VDD_GPU": {"avg": 26.13, "peak": 34.27, "min": 22.07}, "VIN": {"avg": 65.89, "peak": 76.38, "min": 60.92}, "VDD_CPU_SOC_MSS": {"avg": 15.44, "peak": 16.14, "min": 14.56}}, "power_watts_avg": 26.13, "energy_joules_est": 32.66, "sample_count": 9, "duration_seconds": 1.25}, "timestamp": "2026-01-16T16:48:40.630657"}
{"image_index": 469, "image_name": "000000052591.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052591.jpg", "image_width": 359, "image_height": 640, "image_resolution": "359x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 3064.888, "latencies_ms": [3064.888], "images_per_second": 0.326, "prompt_tokens": 23, "response_tokens_est": 129, "n_tiles": 2, "output_text": "woman: 1\nsnowsuit: 1\nglove: 1\nskis: 1\nskis pole: 1\nskis strap: 1\nskis boot: 1\nskis boot lace: 1\nskis boot buckle: 1\nskis boot strap: 1\nskis boot buckle: 1\nskis boot strap: 1\nskis boot buckle: 1\nskis boot strap: 1\nskis boot buckle: 1\nskis boot strap: 1\nskis boot buckle:", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25606.4, "ram_available_mb": 100165.8, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25606.4, "ram_available_mb": 100165.8, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5064.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.24, "peak": 15.62, "min": 14.4}, "VDD_GPU": {"avg": 22.62, "peak": 31.51, "min": 20.89}, "VIN": {"avg": 64.16, "peak": 102.45, "min": 52.19}, "VDD_CPU_SOC_MSS": {"avg": 16.19, "peak": 16.55, "min": 15.36}}, "power_watts_avg": 22.62, "energy_joules_est": 69.34, "sample_count": 24, "duration_seconds": 3.065}, "timestamp": "2026-01-16T16:48:43.701508"}
{"image_index": 469, "image_name": "000000052591.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052591.jpg", "image_width": 359, "image_height": 640, "image_resolution": "359x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2356.827, "latencies_ms": [2356.827], "images_per_second": 0.424, "prompt_tokens": 27, "response_tokens_est": 97, "n_tiles": 2, "output_text": "The main object in the foreground is a woman standing on a snowy slope, holding a pair of skis. She is positioned near the center of the image, with her body slightly angled towards the camera. The background features a snowy landscape with a few sparse trees and a partly cloudy sky. The skis are positioned to the left of the woman, and there is a snow-covered ground and a small snowman near the bottom left corner of the image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25606.4, "ram_available_mb": 100165.8, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25606.4, "ram_available_mb": 100165.8, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5067.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.33, "peak": 15.72, "min": 14.71}, "VDD_GPU": {"avg": 23.2, "peak": 31.12, "min": 20.88}, "VIN": {"avg": 66.14, "peak": 102.34, "min": 59.32}, "VDD_CPU_SOC_MSS": {"avg": 16.01, "peak": 16.14, "min": 15.74}}, "power_watts_avg": 23.2, "energy_joules_est": 54.69, "sample_count": 18, "duration_seconds": 2.357}, "timestamp": "2026-01-16T16:48:46.064182"}
{"image_index": 469, "image_name": "000000052591.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052591.jpg", "image_width": 359, "image_height": 640, "image_resolution": "359x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1934.111, "latencies_ms": [1934.111], "images_per_second": 0.517, "prompt_tokens": 21, "response_tokens_est": 78, "n_tiles": 2, "output_text": "The image depicts a black-and-white scene set in a snowy landscape, likely a ski resort or a snowy mountain area. A woman is standing on a snowy slope, holding a ski pole and wearing a winter outfit with a fur-lined jacket, a scarf, and gloves. She appears to be enjoying the snowy environment, smiling and looking towards the camera.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25606.4, "ram_available_mb": 100165.8, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25606.6, "ram_available_mb": 100165.5, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5063.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.22, "peak": 15.42, "min": 14.61}, "VDD_GPU": {"avg": 23.56, "peak": 31.53, "min": 20.89}, "VIN": {"avg": 65.16, "peak": 81.58, "min": 59.8}, "VDD_CPU_SOC_MSS": {"avg": 15.88, "peak": 16.14, "min": 14.96}}, "power_watts_avg": 23.56, "energy_joules_est": 45.57, "sample_count": 15, "duration_seconds": 1.934}, "timestamp": "2026-01-16T16:48:48.004172"}
{"image_index": 469, "image_name": "000000052591.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052591.jpg", "image_width": 359, "image_height": 640, "image_resolution": "359x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1754.315, "latencies_ms": [1754.315], "images_per_second": 0.57, "prompt_tokens": 19, "response_tokens_est": 70, "n_tiles": 2, "output_text": "The image is a black and white photograph featuring a woman standing in a snowy landscape. She is dressed in a white sweater with a fur collar and a black skirt, accessorized with a necklace and gloves. The lighting is soft and diffused, casting gentle shadows on the snow, and the overall weather appears to be cold and snowy.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25606.6, "ram_available_mb": 100165.5, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25606.1, "ram_available_mb": 100166.0, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5062.6, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.18, "peak": 15.52, "min": 14.51}, "VDD_GPU": {"avg": 23.82, "peak": 30.73, "min": 20.88}, "VIN": {"avg": 66.96, "peak": 104.29, "min": 59.06}, "VDD_CPU_SOC_MSS": {"avg": 15.9, "peak": 16.54, "min": 14.96}}, "power_watts_avg": 23.82, "energy_joules_est": 41.8, "sample_count": 13, "duration_seconds": 1.755}, "timestamp": "2026-01-16T16:48:49.766559"}
{"image_index": 470, "image_name": "000000052891.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052891.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1303.493, "latencies_ms": [1303.493], "images_per_second": 0.767, "prompt_tokens": 9, "response_tokens_est": 27, "n_tiles": 6, "output_text": "A dog is seen on a sandy beach, holding a yellow frisbee, with the ocean and sky in the background.", "error": null, "sys_before": {"cpu_percent": 42.6, "ram_used_mb": 25605.9, "ram_available_mb": 100166.3, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25605.9, "ram_available_mb": 100166.3, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.61, "peak": 15.01, "min": 14.02}, "VDD_GPU": {"avg": 31.43, "peak": 36.62, "min": 26.0}, "VIN": {"avg": 73.65, "peak": 98.43, "min": 57.45}, "VDD_CPU_SOC_MSS": {"avg": 15.28, "peak": 15.75, "min": 14.57}}, "power_watts_avg": 31.43, "energy_joules_est": 40.98, "sample_count": 10, "duration_seconds": 1.304}, "timestamp": "2026-01-16T16:48:51.111421"}
{"image_index": 470, "image_name": "000000052891.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052891.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1735.228, "latencies_ms": [1735.228], "images_per_second": 0.576, "prompt_tokens": 23, "response_tokens_est": 40, "n_tiles": 6, "output_text": "object: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25605.9, "ram_available_mb": 100166.3, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 25605.6, "ram_available_mb": 100166.5, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.65, "peak": 15.01, "min": 14.02}, "VDD_GPU": {"avg": 31.42, "peak": 38.98, "min": 24.42}, "VIN": {"avg": 75.53, "peak": 111.04, "min": 60.1}, "VDD_CPU_SOC_MSS": {"avg": 15.26, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 31.42, "energy_joules_est": 54.54, "sample_count": 13, "duration_seconds": 1.736}, "timestamp": "2026-01-16T16:48:52.852939"}
{"image_index": 470, "image_name": "000000052891.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052891.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2490.533, "latencies_ms": [2490.533], "images_per_second": 0.402, "prompt_tokens": 27, "response_tokens_est": 67, "n_tiles": 6, "output_text": "The main object in the foreground is a yellow frisbee, which is held by a dog. The dog is positioned on the sandy beach, with its head close to the frisbee. In the background, there is a large body of water and a distant landmass, indicating the dog is near the shoreline.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25605.6, "ram_available_mb": 100166.5, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 25605.6, "ram_available_mb": 100166.5, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.75, "peak": 15.01, "min": 14.02}, "VDD_GPU": {"avg": 28.75, "peak": 38.21, "min": 23.24}, "VIN": {"avg": 66.23, "peak": 116.22, "min": 55.31}, "VDD_CPU_SOC_MSS": {"avg": 15.42, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 28.75, "energy_joules_est": 71.62, "sample_count": 19, "duration_seconds": 2.491}, "timestamp": "2026-01-16T16:48:55.349907"}
{"image_index": 470, "image_name": "000000052891.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052891.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2686.184, "latencies_ms": [2686.184], "images_per_second": 0.372, "prompt_tokens": 21, "response_tokens_est": 74, "n_tiles": 6, "output_text": "The image depicts a beach scene with a dog playing with a yellow frisbee. The dog, with its fur appearing wet and matted, is lying on the sandy beach, focusing intently on the frisbee. The beach is surrounded by calm, turquoise waters, and there are a few people visible in the background, enjoying the beach.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25605.6, "ram_available_mb": 100166.5, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 25605.6, "ram_available_mb": 100166.5, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.8, "peak": 15.01, "min": 14.02}, "VDD_GPU": {"avg": 28.12, "peak": 38.19, "min": 23.24}, "VIN": {"avg": 69.16, "peak": 117.51, "min": 55.75}, "VDD_CPU_SOC_MSS": {"avg": 15.41, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 28.12, "energy_joules_est": 75.54, "sample_count": 21, "duration_seconds": 2.686}, "timestamp": "2026-01-16T16:48:58.042327"}
{"image_index": 470, "image_name": "000000052891.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052891.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2493.414, "latencies_ms": [2493.414], "images_per_second": 0.401, "prompt_tokens": 19, "response_tokens_est": 67, "n_tiles": 6, "output_text": "The image features a dog with a coat of black, white, and gray fur, standing on a sandy beach. The dog is holding a bright yellow frisbee, which contrasts sharply with the sandy background. The lighting is natural, suggesting daytime, and the sky appears overcast, with no direct sunlight visible.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25605.6, "ram_available_mb": 100166.5, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 25605.4, "ram_available_mb": 100166.8, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.75, "peak": 15.01, "min": 14.02}, "VDD_GPU": {"avg": 28.53, "peak": 38.21, "min": 23.24}, "VIN": {"avg": 69.57, "peak": 114.03, "min": 58.67}, "VDD_CPU_SOC_MSS": {"avg": 15.27, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 28.53, "energy_joules_est": 71.15, "sample_count": 19, "duration_seconds": 2.494}, "timestamp": "2026-01-16T16:49:00.542054"}
{"image_index": 471, "image_name": "000000052996.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052996.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1658.205, "latencies_ms": [1658.205], "images_per_second": 0.603, "prompt_tokens": 9, "response_tokens_est": 40, "n_tiles": 6, "output_text": "The image depicts a group of individuals in a kitchen setting, with one person in military attire standing next to a stainless steel refrigerator, while others are engaged in conversation or observing the surroundings.", "error": null, "sys_before": {"cpu_percent": 37.9, "ram_used_mb": 25605.4, "ram_available_mb": 100166.8, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 25605.4, "ram_available_mb": 100166.8, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.55, "peak": 15.11, "min": 13.82}, "VDD_GPU": {"avg": 30.27, "peak": 37.03, "min": 24.42}, "VIN": {"avg": 72.25, "peak": 109.72, "min": 50.69}, "VDD_CPU_SOC_MSS": {"avg": 15.2, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 30.27, "energy_joules_est": 50.21, "sample_count": 13, "duration_seconds": 1.659}, "timestamp": "2026-01-16T16:49:02.249660"}
{"image_index": 471, "image_name": "000000052996.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052996.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 4204.979, "latencies_ms": [4204.979], "images_per_second": 0.238, "prompt_tokens": 23, "response_tokens_est": 129, "n_tiles": 6, "output_text": "- Food: 1\n- Bowl: 1\n- Cutting board: 1\n- Plate: 1\n- Bowl: 1\n- Cutting board: 1\n- Plate: 1\n- Bowl: 1\n- Cutting board: 1\n- Plate: 1\n- Bowl: 1\n- Cutting board: 1\n- Plate: 1\n- Bowl: 1\n- Cutting board: 1\n- Plate: 1\n- Bowl: 1\n- Cutting board: 1\n- Plate: 1\n- Bowl", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25605.4, "ram_available_mb": 100166.8, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.8, "ram_used_mb": 25605.4, "ram_available_mb": 100166.8, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.88, "peak": 15.11, "min": 14.02}, "VDD_GPU": {"avg": 26.43, "peak": 38.6, "min": 23.24}, "VIN": {"avg": 67.18, "peak": 109.65, "min": 58.78}, "VDD_CPU_SOC_MSS": {"avg": 15.37, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 26.43, "energy_joules_est": 111.15, "sample_count": 33, "duration_seconds": 4.205}, "timestamp": "2026-01-16T16:49:06.464971"}
{"image_index": 471, "image_name": "000000052996.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052996.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2694.514, "latencies_ms": [2694.514], "images_per_second": 0.371, "prompt_tokens": 27, "response_tokens_est": 74, "n_tiles": 6, "output_text": "In the image, the main objects are located in the foreground and background. The individuals in the foreground are engaged in a discussion, while the refrigerator and various kitchen items are situated in the background. The refrigerator is positioned to the right of the individuals, and the kitchen items are scattered around the counter, with a green bowl and a wooden box visible near the foreground.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25605.4, "ram_available_mb": 100166.8, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 25605.4, "ram_available_mb": 100166.8, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.75, "peak": 15.01, "min": 14.02}, "VDD_GPU": {"avg": 28.1, "peak": 38.19, "min": 23.24}, "VIN": {"avg": 68.79, "peak": 97.47, "min": 55.05}, "VDD_CPU_SOC_MSS": {"avg": 15.32, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 28.1, "energy_joules_est": 75.72, "sample_count": 21, "duration_seconds": 2.695}, "timestamp": "2026-01-16T16:49:09.165437"}
{"image_index": 471, "image_name": "000000052996.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052996.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2632.137, "latencies_ms": [2632.137], "images_per_second": 0.38, "prompt_tokens": 21, "response_tokens_est": 72, "n_tiles": 6, "output_text": "The image depicts a group of people in a kitchen setting, likely engaged in a discussion or activity related to cooking or food preparation. The individuals are dressed in casual attire, with some wearing military-style uniforms, suggesting a possible military or humanitarian mission. The kitchen is equipped with various cooking utensils and equipment, indicating a functional and practical environment.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25605.4, "ram_available_mb": 100166.8, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 25605.6, "ram_available_mb": 100166.6, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.77, "peak": 15.01, "min": 14.02}, "VDD_GPU": {"avg": 28.32, "peak": 37.8, "min": 23.24}, "VIN": {"avg": 71.94, "peak": 117.14, "min": 55.8}, "VDD_CPU_SOC_MSS": {"avg": 15.31, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 28.32, "energy_joules_est": 74.56, "sample_count": 20, "duration_seconds": 2.633}, "timestamp": "2026-01-16T16:49:11.803844"}
{"image_index": 471, "image_name": "000000052996.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000052996.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1987.187, "latencies_ms": [1987.187], "images_per_second": 0.503, "prompt_tokens": 19, "response_tokens_est": 49, "n_tiles": 6, "output_text": "The image depicts a group of people in a kitchen setting, with a focus on a stainless steel refrigerator and various cooking utensils and containers. The lighting is bright, and the materials include stainless steel, plastic, and wooden elements.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25605.6, "ram_available_mb": 100166.6, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 25605.4, "ram_available_mb": 100166.8, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.69, "peak": 15.01, "min": 14.02}, "VDD_GPU": {"avg": 29.96, "peak": 38.19, "min": 23.64}, "VIN": {"avg": 70.73, "peak": 107.44, "min": 62.65}, "VDD_CPU_SOC_MSS": {"avg": 15.25, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 29.96, "energy_joules_est": 59.54, "sample_count": 15, "duration_seconds": 1.987}, "timestamp": "2026-01-16T16:49:13.797052"}
{"image_index": 472, "image_name": "000000053505.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000053505.jpg", "image_width": 428, "image_height": 640, "image_resolution": "428x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1232.194, "latencies_ms": [1232.194], "images_per_second": 0.812, "prompt_tokens": 9, "response_tokens_est": 24, "n_tiles": 6, "output_text": "The image shows a bathroom with a toilet, a toilet paper holder, and a soap dispenser on the wall.", "error": null, "sys_before": {"cpu_percent": 19.5, "ram_used_mb": 25605.4, "ram_available_mb": 100166.8, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 25605.4, "ram_available_mb": 100166.8, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.39, "peak": 14.81, "min": 13.92}, "VDD_GPU": {"avg": 32.65, "peak": 37.03, "min": 27.18}, "VIN": {"avg": 70.08, "peak": 110.76, "min": 54.52}, "VDD_CPU_SOC_MSS": {"avg": 15.09, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 32.65, "energy_joules_est": 40.26, "sample_count": 9, "duration_seconds": 1.233}, "timestamp": "2026-01-16T16:49:15.064610"}
{"image_index": 472, "image_name": "000000053505.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000053505.jpg", "image_width": 428, "image_height": 640, "image_resolution": "428x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1733.784, "latencies_ms": [1733.784], "images_per_second": 0.577, "prompt_tokens": 23, "response_tokens_est": 40, "n_tiles": 6, "output_text": "object: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25605.4, "ram_available_mb": 100166.8, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 25605.4, "ram_available_mb": 100166.8, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.63, "peak": 14.91, "min": 14.02}, "VDD_GPU": {"avg": 31.45, "peak": 39.38, "min": 24.42}, "VIN": {"avg": 69.42, "peak": 112.03, "min": 55.53}, "VDD_CPU_SOC_MSS": {"avg": 15.29, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 31.45, "energy_joules_est": 54.54, "sample_count": 13, "duration_seconds": 1.734}, "timestamp": "2026-01-16T16:49:16.804400"}
{"image_index": 472, "image_name": "000000053505.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000053505.jpg", "image_width": 428, "image_height": 640, "image_resolution": "428x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3256.664, "latencies_ms": [3256.664], "images_per_second": 0.307, "prompt_tokens": 27, "response_tokens_est": 94, "n_tiles": 6, "output_text": "The main objects in the image are a toilet, a toilet paper holder, and a soap dispenser. The toilet is positioned in the foreground, slightly to the right, while the toilet paper holder is mounted on the wall to the left of the toilet. The soap dispenser is placed on the wall near the toilet paper holder. The background features a shower area with a glass door, and the overall setting is a bathroom with beige walls and tiled flooring.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25605.4, "ram_available_mb": 100166.8, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 25605.4, "ram_available_mb": 100166.8, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.79, "peak": 15.01, "min": 14.02}, "VDD_GPU": {"avg": 27.45, "peak": 38.19, "min": 23.24}, "VIN": {"avg": 68.31, "peak": 115.09, "min": 55.2}, "VDD_CPU_SOC_MSS": {"avg": 15.37, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 27.45, "energy_joules_est": 89.41, "sample_count": 25, "duration_seconds": 3.257}, "timestamp": "2026-01-16T16:49:20.071273"}
{"image_index": 472, "image_name": "000000053505.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000053505.jpg", "image_width": 428, "image_height": 640, "image_resolution": "428x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2694.554, "latencies_ms": [2694.554], "images_per_second": 0.371, "prompt_tokens": 21, "response_tokens_est": 74, "n_tiles": 6, "output_text": "The image depicts a well-organized bathroom with a clean and modern design. The setting is a small, well-lit bathroom with a white toilet, a toilet paper holder, and a soap dispenser. The walls are tiled in a light color, and there is a towel rack with neatly folded towels. The overall atmosphere is tidy and inviting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25605.4, "ram_available_mb": 100166.8, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 25605.4, "ram_available_mb": 100166.8, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.75, "peak": 15.01, "min": 14.02}, "VDD_GPU": {"avg": 28.02, "peak": 38.21, "min": 23.24}, "VIN": {"avg": 68.78, "peak": 98.69, "min": 61.29}, "VDD_CPU_SOC_MSS": {"avg": 15.24, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 28.02, "energy_joules_est": 75.52, "sample_count": 21, "duration_seconds": 2.695}, "timestamp": "2026-01-16T16:49:22.772688"}
{"image_index": 472, "image_name": "000000053505.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000053505.jpg", "image_width": 428, "image_height": 640, "image_resolution": "428x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1512.913, "latencies_ms": [1512.913], "images_per_second": 0.661, "prompt_tokens": 19, "response_tokens_est": 32, "n_tiles": 6, "output_text": "The bathroom features a warm, beige color scheme with light-colored walls and tiled flooring. The lighting is soft and ambient, creating a cozy atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25605.4, "ram_available_mb": 100166.8, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25605.3, "ram_available_mb": 100166.8, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.58, "peak": 14.91, "min": 14.02}, "VDD_GPU": {"avg": 32.16, "peak": 38.19, "min": 25.6}, "VIN": {"avg": 72.0, "peak": 116.3, "min": 54.55}, "VDD_CPU_SOC_MSS": {"avg": 15.14, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 32.16, "energy_joules_est": 48.67, "sample_count": 11, "duration_seconds": 1.513}, "timestamp": "2026-01-16T16:49:24.295762"}
{"image_index": 473, "image_name": "000000053529.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000053529.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1225.3, "latencies_ms": [1225.3], "images_per_second": 0.816, "prompt_tokens": 9, "response_tokens_est": 24, "n_tiles": 6, "output_text": "A woman wearing a green hat is sitting in the back of a vehicle, which has a shiny, reflective surface.", "error": null, "sys_before": {"cpu_percent": 40.8, "ram_used_mb": 25605.3, "ram_available_mb": 100166.8, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25605.3, "ram_available_mb": 100166.8, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.29, "peak": 14.81, "min": 13.82}, "VDD_GPU": {"avg": 32.83, "peak": 37.42, "min": 27.18}, "VIN": {"avg": 71.92, "peak": 103.01, "min": 50.79}, "VDD_CPU_SOC_MSS": {"avg": 15.0, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 32.83, "energy_joules_est": 40.25, "sample_count": 9, "duration_seconds": 1.226}, "timestamp": "2026-01-16T16:49:25.578082"}
{"image_index": 473, "image_name": "000000053529.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000053529.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2558.722, "latencies_ms": [2558.722], "images_per_second": 0.391, "prompt_tokens": 23, "response_tokens_est": 72, "n_tiles": 6, "output_text": "1. St. Patrick's Day hat\n2. St. Patrick's Day hat\n3. St. Patrick's Day hat\n4. St. Patrick's Day hat\n5. St. Patrick's Day hat\n6. St. Patrick's Day hat\n7. St. Patrick's Day hat\n8. St. Patrick's Day hat", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25605.3, "ram_available_mb": 100166.8, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 25605.3, "ram_available_mb": 100166.8, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.91, "peak": 15.32, "min": 14.22}, "VDD_GPU": {"avg": 29.11, "peak": 39.0, "min": 23.64}, "VIN": {"avg": 68.74, "peak": 108.24, "min": 59.84}, "VDD_CPU_SOC_MSS": {"avg": 15.5, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 29.11, "energy_joules_est": 74.5, "sample_count": 19, "duration_seconds": 2.559}, "timestamp": "2026-01-16T16:49:28.142874"}
{"image_index": 473, "image_name": "000000053529.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000053529.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1834.517, "latencies_ms": [1834.517], "images_per_second": 0.545, "prompt_tokens": 27, "response_tokens_est": 45, "n_tiles": 6, "output_text": "The main objects in the image are a car and a dog. The car is positioned in the foreground, while the dog is in the background. The dog is near the car, and the car is near the dog.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25605.3, "ram_available_mb": 100166.8, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 25605.6, "ram_available_mb": 100166.6, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.88, "peak": 15.22, "min": 14.32}, "VDD_GPU": {"avg": 30.58, "peak": 37.82, "min": 24.42}, "VIN": {"avg": 75.7, "peak": 118.05, "min": 60.6}, "VDD_CPU_SOC_MSS": {"avg": 15.41, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 30.58, "energy_joules_est": 56.12, "sample_count": 14, "duration_seconds": 1.835}, "timestamp": "2026-01-16T16:49:29.984092"}
{"image_index": 473, "image_name": "000000053529.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000053529.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1938.146, "latencies_ms": [1938.146], "images_per_second": 0.516, "prompt_tokens": 21, "response_tokens_est": 49, "n_tiles": 6, "output_text": "The image depicts a scene inside a vehicle, likely a car, where a person wearing a green hat is seated. The vehicle's rear window is visible, and a green St. Patrick's Day-themed decoration is attached to the window.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25605.6, "ram_available_mb": 100166.6, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 25605.3, "ram_available_mb": 100166.8, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.86, "peak": 15.22, "min": 14.22}, "VDD_GPU": {"avg": 30.64, "peak": 37.82, "min": 24.42}, "VIN": {"avg": 67.17, "peak": 79.49, "min": 56.64}, "VDD_CPU_SOC_MSS": {"avg": 15.41, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 30.64, "energy_joules_est": 59.4, "sample_count": 14, "duration_seconds": 1.939}, "timestamp": "2026-01-16T16:49:31.928416"}
{"image_index": 473, "image_name": "000000053529.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000053529.jpg", "image_width": 500, "image_height": 333, "image_resolution": "500x333", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2528.69, "latencies_ms": [2528.69], "images_per_second": 0.395, "prompt_tokens": 19, "response_tokens_est": 71, "n_tiles": 6, "output_text": "The image features a vehicle with a shiny, reflective surface, likely made of metal, and a green St. Patrick's Day-themed decoration on the front. The vehicle's interior is visible, with a person wearing a green hat and a dog looking out of the window. The lighting is bright, suggesting daytime, and the weather appears to be clear.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25605.3, "ram_available_mb": 100166.8, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 25605.3, "ram_available_mb": 100166.8, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.96, "peak": 15.22, "min": 14.32}, "VDD_GPU": {"avg": 28.86, "peak": 38.21, "min": 23.64}, "VIN": {"avg": 68.26, "peak": 105.16, "min": 58.41}, "VDD_CPU_SOC_MSS": {"avg": 15.52, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 28.86, "energy_joules_est": 72.99, "sample_count": 19, "duration_seconds": 2.529}, "timestamp": "2026-01-16T16:49:34.463549"}
{"image_index": 474, "image_name": "000000053624.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000053624.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1407.525, "latencies_ms": [1407.525], "images_per_second": 0.71, "prompt_tokens": 9, "response_tokens_est": 31, "n_tiles": 6, "output_text": "A group of elephants are standing in a shallow pool of water, with their trunks raised in the air, appearing to be enjoying the refreshing water.", "error": null, "sys_before": {"cpu_percent": 37.7, "ram_used_mb": 25605.1, "ram_available_mb": 100167.1, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 25605.3, "ram_available_mb": 100166.8, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.53, "peak": 15.01, "min": 13.92}, "VDD_GPU": {"avg": 31.62, "peak": 37.42, "min": 25.6}, "VIN": {"avg": 74.21, "peak": 115.52, "min": 57.46}, "VDD_CPU_SOC_MSS": {"avg": 15.24, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 31.62, "energy_joules_est": 44.53, "sample_count": 11, "duration_seconds": 1.408}, "timestamp": "2026-01-16T16:49:35.920366"}
{"image_index": 474, "image_name": "000000053624.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000053624.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1727.161, "latencies_ms": [1727.161], "images_per_second": 0.579, "prompt_tokens": 23, "response_tokens_est": 41, "n_tiles": 6, "output_text": "elephant: 1\nwater: 1\nrock: 2\nrock: 1\nrock: 1\nrock: 1\nrock: 1\nrock: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25605.3, "ram_available_mb": 100166.8, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 25605.3, "ram_available_mb": 100166.8, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.8, "peak": 15.11, "min": 14.22}, "VDD_GPU": {"avg": 31.3, "peak": 38.6, "min": 24.82}, "VIN": {"avg": 70.63, "peak": 103.82, "min": 55.95}, "VDD_CPU_SOC_MSS": {"avg": 15.32, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 31.3, "energy_joules_est": 54.07, "sample_count": 13, "duration_seconds": 1.727}, "timestamp": "2026-01-16T16:49:37.657758"}
{"image_index": 474, "image_name": "000000053624.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000053624.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2584.659, "latencies_ms": [2584.659], "images_per_second": 0.387, "prompt_tokens": 27, "response_tokens_est": 73, "n_tiles": 6, "output_text": "In the image, the main object is an elephant standing in a shallow water puddle. The elephant is positioned in the foreground, with its trunk and ears slightly raised. The background features a fence, some greenery, and a few people. The water puddle is located near the elephant, and there are rocks and a few other animals in the vicinity.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25605.3, "ram_available_mb": 100166.8, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 25605.1, "ram_available_mb": 100167.1, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.94, "peak": 15.22, "min": 14.22}, "VDD_GPU": {"avg": 28.88, "peak": 38.19, "min": 23.64}, "VIN": {"avg": 70.36, "peak": 114.84, "min": 55.98}, "VDD_CPU_SOC_MSS": {"avg": 15.52, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 28.88, "energy_joules_est": 74.66, "sample_count": 19, "duration_seconds": 2.585}, "timestamp": "2026-01-16T16:49:40.248583"}
{"image_index": 474, "image_name": "000000053624.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000053624.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2232.254, "latencies_ms": [2232.254], "images_per_second": 0.448, "prompt_tokens": 21, "response_tokens_est": 60, "n_tiles": 6, "output_text": "The image depicts an elephant standing in a shallow water-filled enclosure, likely at a zoo or wildlife sanctuary. The elephant is surrounded by rocks and vegetation, with a few other elephants visible in the background. The setting appears to be a naturalistic environment designed to mimic the elephant's natural habitat.", "error": null, "sys_before": {"cpu_percent": 14.3, "ram_used_mb": 25605.1, "ram_available_mb": 100167.1, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 25605.1, "ram_available_mb": 100167.1, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.91, "peak": 15.22, "min": 14.32}, "VDD_GPU": {"avg": 29.66, "peak": 37.82, "min": 24.03}, "VIN": {"avg": 71.8, "peak": 113.67, "min": 56.68}, "VDD_CPU_SOC_MSS": {"avg": 15.5, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 29.66, "energy_joules_est": 66.23, "sample_count": 16, "duration_seconds": 2.233}, "timestamp": "2026-01-16T16:49:42.493251"}
{"image_index": 474, "image_name": "000000053624.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000053624.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2123.236, "latencies_ms": [2123.236], "images_per_second": 0.471, "prompt_tokens": 19, "response_tokens_est": 56, "n_tiles": 6, "output_text": "The image depicts an elephant standing in a shallow water pool, with its trunk raised. The surrounding environment includes a sandy ground, sparse vegetation, and a fence in the background. The lighting is natural, suggesting daytime, and the weather appears to be clear and sunny.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25605.1, "ram_available_mb": 100167.1, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 25605.1, "ram_available_mb": 100167.1, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.89, "peak": 15.22, "min": 14.22}, "VDD_GPU": {"avg": 29.69, "peak": 37.8, "min": 24.03}, "VIN": {"avg": 67.72, "peak": 121.83, "min": 56.37}, "VDD_CPU_SOC_MSS": {"avg": 15.48, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 29.69, "energy_joules_est": 63.05, "sample_count": 16, "duration_seconds": 2.124}, "timestamp": "2026-01-16T16:49:44.622567"}
{"image_index": 475, "image_name": "000000053626.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000053626.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2888.044, "latencies_ms": [2888.044], "images_per_second": 0.346, "prompt_tokens": 9, "response_tokens_est": 44, "n_tiles": 12, "output_text": "The image depicts a group of people standing on a snowy slope, each holding ski poles and wearing appropriate winter attire, including jackets and gloves, suggesting they are engaged in skiing or snowboarding activities.", "error": null, "sys_before": {"cpu_percent": 49.2, "ram_used_mb": 25605.1, "ram_available_mb": 100167.1, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25605.5, "ram_available_mb": 100166.6, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.12, "peak": 16.15, "min": 13.9}, "VDD_GPU": {"avg": 33.61, "peak": 41.36, "min": 26.39}, "VIN": {"avg": 83.21, "peak": 126.12, "min": 60.15}, "VDD_CPU_SOC_MSS": {"avg": 16.0, "peak": 17.32, "min": 15.35}}, "power_watts_avg": 33.61, "energy_joules_est": 97.08, "sample_count": 22, "duration_seconds": 2.888}, "timestamp": "2026-01-16T16:49:47.599086"}
{"image_index": 475, "image_name": "000000053626.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000053626.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2879.951, "latencies_ms": [2879.951], "images_per_second": 0.347, "prompt_tokens": 23, "response_tokens_est": 40, "n_tiles": 12, "output_text": "1. Ski poles\n2. Ski poles\n3. Ski poles\n4. Ski poles\n5. Ski poles\n6. Ski poles\n7. Ski poles\n8. Ski poles", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25605.5, "ram_available_mb": 100166.6, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25605.5, "ram_available_mb": 100166.6, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.05, "peak": 15.54, "min": 14.53}, "VDD_GPU": {"avg": 34.96, "peak": 42.92, "min": 26.79}, "VIN": {"avg": 74.9, "peak": 117.61, "min": 58.39}, "VDD_CPU_SOC_MSS": {"avg": 15.69, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 34.96, "energy_joules_est": 100.69, "sample_count": 22, "duration_seconds": 2.88}, "timestamp": "2026-01-16T16:49:50.486132"}
{"image_index": 475, "image_name": "000000053626.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000053626.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 4093.873, "latencies_ms": [4093.873], "images_per_second": 0.244, "prompt_tokens": 27, "response_tokens_est": 76, "n_tiles": 12, "output_text": "The main objects in the image are a group of people skiing on a snowy slope. The foreground features the skiers, with their ski poles and skis in view. The background shows a snowy mountain landscape with trees and a clear blue sky. The skiers are positioned near the center of the image, with the mountains and trees stretching out towards the edges.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25605.5, "ram_available_mb": 100166.6, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 25605.5, "ram_available_mb": 100166.6, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.08, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 32.41, "peak": 42.94, "min": 26.39}, "VIN": {"avg": 72.15, "peak": 104.71, "min": 57.08}, "VDD_CPU_SOC_MSS": {"avg": 15.77, "peak": 16.53, "min": 15.35}}, "power_watts_avg": 32.41, "energy_joules_est": 132.7, "sample_count": 32, "duration_seconds": 4.094}, "timestamp": "2026-01-16T16:49:54.586422"}
{"image_index": 475, "image_name": "000000053626.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000053626.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3716.451, "latencies_ms": [3716.451], "images_per_second": 0.269, "prompt_tokens": 21, "response_tokens_est": 65, "n_tiles": 12, "output_text": "The image depicts a group of people skiing down a snowy mountain slope. They are dressed in winter gear, including jackets, pants, and ski boots, and are holding ski poles. The setting is a snowy mountain with clear blue skies, and the group appears to be enjoying a day of skiing.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25605.5, "ram_available_mb": 100166.6, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25605.5, "ram_available_mb": 100166.6, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.09, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 32.9, "peak": 42.54, "min": 26.39}, "VIN": {"avg": 74.2, "peak": 113.2, "min": 58.66}, "VDD_CPU_SOC_MSS": {"avg": 15.73, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 32.9, "energy_joules_est": 122.29, "sample_count": 29, "duration_seconds": 3.717}, "timestamp": "2026-01-16T16:49:58.309561"}
{"image_index": 475, "image_name": "000000053626.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000053626.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3215.83, "latencies_ms": [3215.83], "images_per_second": 0.311, "prompt_tokens": 19, "response_tokens_est": 49, "n_tiles": 12, "output_text": "The image depicts a group of people skiing down a snowy mountain slope. The sky is clear and blue, indicating a sunny day. The snow is pristine and untouched, with a few scattered trees dotting the landscape.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25605.5, "ram_available_mb": 100166.6, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 25605.5, "ram_available_mb": 100166.6, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.93, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 33.73, "peak": 42.53, "min": 26.0}, "VIN": {"avg": 78.75, "peak": 132.98, "min": 58.55}, "VDD_CPU_SOC_MSS": {"avg": 15.46, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 33.73, "energy_joules_est": 108.49, "sample_count": 25, "duration_seconds": 3.216}, "timestamp": "2026-01-16T16:50:01.531877"}
{"image_index": 476, "image_name": "000000053909.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000053909.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2253.837, "latencies_ms": [2253.837], "images_per_second": 0.444, "prompt_tokens": 9, "response_tokens_est": 25, "n_tiles": 12, "output_text": "The image shows a person holding a smartphone with a visible screen displaying a date and time, and a blurred background.", "error": null, "sys_before": {"cpu_percent": 40.3, "ram_used_mb": 25605.5, "ram_available_mb": 100166.6, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 25605.8, "ram_available_mb": 100166.4, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.06, "peak": 16.15, "min": 13.59}, "VDD_GPU": {"avg": 35.75, "peak": 41.36, "min": 28.36}, "VIN": {"avg": 75.71, "peak": 99.11, "min": 58.63}, "VDD_CPU_SOC_MSS": {"avg": 15.86, "peak": 17.32, "min": 14.57}}, "power_watts_avg": 35.75, "energy_joules_est": 80.59, "sample_count": 17, "duration_seconds": 2.254}, "timestamp": "2026-01-16T16:50:03.878507"}
{"image_index": 476, "image_name": "000000053909.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000053909.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2744.705, "latencies_ms": [2744.705], "images_per_second": 0.364, "prompt_tokens": 23, "response_tokens_est": 36, "n_tiles": 12, "output_text": "1. Phone\n2. Keyboard\n3. Screen\n4. Phone Case\n5. Phone Battery\n6. Phone Case\n7. Phone\n8. Phone Case", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25605.8, "ram_available_mb": 100166.4, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 25605.5, "ram_available_mb": 100166.6, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.07, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 35.49, "peak": 43.33, "min": 27.18}, "VIN": {"avg": 75.97, "peak": 116.69, "min": 58.55}, "VDD_CPU_SOC_MSS": {"avg": 15.73, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 35.49, "energy_joules_est": 97.43, "sample_count": 21, "duration_seconds": 2.745}, "timestamp": "2026-01-16T16:50:06.629652"}
{"image_index": 476, "image_name": "000000053909.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000053909.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3181.481, "latencies_ms": [3181.481], "images_per_second": 0.314, "prompt_tokens": 27, "response_tokens_est": 49, "n_tiles": 12, "output_text": "The main object in the image is a smartphone held in the foreground. The background is blurred, indicating that the focus is on the smartphone. The smartphone is positioned near the center of the image, with the person's hand holding it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25605.5, "ram_available_mb": 100166.6, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 25605.8, "ram_available_mb": 100166.4, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.09, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 34.19, "peak": 43.33, "min": 26.39}, "VIN": {"avg": 75.04, "peak": 122.44, "min": 58.78}, "VDD_CPU_SOC_MSS": {"avg": 15.64, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 34.19, "energy_joules_est": 108.78, "sample_count": 25, "duration_seconds": 3.182}, "timestamp": "2026-01-16T16:50:09.817377"}
{"image_index": 476, "image_name": "000000053909.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000053909.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2940.877, "latencies_ms": [2940.877], "images_per_second": 0.34, "prompt_tokens": 21, "response_tokens_est": 42, "n_tiles": 12, "output_text": "The image shows a person holding a smartphone with a screen displaying a calendar. The person's hand is visible, and the background is blurred, indicating that the focus is on the smartphone and the calendar.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25605.8, "ram_available_mb": 100166.4, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 25605.3, "ram_available_mb": 100166.9, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.1, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 34.68, "peak": 42.54, "min": 26.79}, "VIN": {"avg": 80.21, "peak": 126.58, "min": 58.11}, "VDD_CPU_SOC_MSS": {"avg": 15.69, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 34.68, "energy_joules_est": 102.0, "sample_count": 22, "duration_seconds": 2.941}, "timestamp": "2026-01-16T16:50:12.766371"}
{"image_index": 476, "image_name": "000000053909.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000053909.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3952.149, "latencies_ms": [3952.149], "images_per_second": 0.253, "prompt_tokens": 19, "response_tokens_est": 72, "n_tiles": 12, "output_text": "The image shows a person holding a smartphone with a black case. The screen displays a wallpaper of a leafy tree against a cloudy sky, indicating a cool or cloudy weather. The lighting is natural, likely from an indoor source, and the overall colors are muted with a focus on the black and white contrast of the phone's screen and case.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25605.3, "ram_available_mb": 100166.9, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25605.3, "ram_available_mb": 100166.9, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.08, "peak": 15.54, "min": 14.53}, "VDD_GPU": {"avg": 32.53, "peak": 42.94, "min": 26.0}, "VIN": {"avg": 71.2, "peak": 96.72, "min": 58.88}, "VDD_CPU_SOC_MSS": {"avg": 15.6, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 32.53, "energy_joules_est": 128.58, "sample_count": 31, "duration_seconds": 3.953}, "timestamp": "2026-01-16T16:50:16.725756"}
{"image_index": 477, "image_name": "000000053994.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000053994.jpg", "image_width": 481, "image_height": 640, "image_resolution": "481x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2421.266, "latencies_ms": [2421.266], "images_per_second": 0.413, "prompt_tokens": 9, "response_tokens_est": 30, "n_tiles": 12, "output_text": "The image shows a red parking meter with a sign that reads \"DENVER'S ROAD HOME\" and encourages donations to end homelessness.", "error": null, "sys_before": {"cpu_percent": 44.9, "ram_used_mb": 25605.3, "ram_available_mb": 100166.9, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 25605.3, "ram_available_mb": 100166.9, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.1, "peak": 16.05, "min": 13.69}, "VDD_GPU": {"avg": 35.43, "peak": 41.36, "min": 27.97}, "VIN": {"avg": 74.48, "peak": 117.2, "min": 58.69}, "VDD_CPU_SOC_MSS": {"avg": 15.96, "peak": 17.32, "min": 14.57}}, "power_watts_avg": 35.43, "energy_joules_est": 85.8, "sample_count": 18, "duration_seconds": 2.422}, "timestamp": "2026-01-16T16:50:19.247037"}
{"image_index": 477, "image_name": "000000053994.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000053994.jpg", "image_width": 481, "image_height": 640, "image_resolution": "481x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2881.602, "latencies_ms": [2881.602], "images_per_second": 0.347, "prompt_tokens": 23, "response_tokens_est": 40, "n_tiles": 12, "output_text": "object: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25605.3, "ram_available_mb": 100166.9, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 25605.3, "ram_available_mb": 100166.9, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.07, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 35.09, "peak": 43.33, "min": 26.79}, "VIN": {"avg": 76.13, "peak": 104.34, "min": 58.45}, "VDD_CPU_SOC_MSS": {"avg": 15.71, "peak": 16.53, "min": 15.35}}, "power_watts_avg": 35.09, "energy_joules_est": 101.13, "sample_count": 22, "duration_seconds": 2.882}, "timestamp": "2026-01-16T16:50:22.134932"}
{"image_index": 477, "image_name": "000000053994.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000053994.jpg", "image_width": 481, "image_height": 640, "image_resolution": "481x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3686.737, "latencies_ms": [3686.737], "images_per_second": 0.271, "prompt_tokens": 27, "response_tokens_est": 64, "n_tiles": 12, "output_text": "The main object in the foreground is a red parking meter. It is positioned near a sidewalk and is surrounded by greenery. In the background, there is a sign that reads \"End Homelessness\" and a concrete wall. The parking meter is placed on the sidewalk, and the sign is mounted on a pole.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25605.3, "ram_available_mb": 100166.9, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25605.3, "ram_available_mb": 100166.9, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.1, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 32.98, "peak": 42.54, "min": 26.39}, "VIN": {"avg": 76.73, "peak": 118.14, "min": 58.64}, "VDD_CPU_SOC_MSS": {"avg": 15.61, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 32.98, "energy_joules_est": 121.6, "sample_count": 28, "duration_seconds": 3.687}, "timestamp": "2026-01-16T16:50:25.827841"}
{"image_index": 477, "image_name": "000000053994.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000053994.jpg", "image_width": 481, "image_height": 640, "image_resolution": "481x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3184.031, "latencies_ms": [3184.031], "images_per_second": 0.314, "prompt_tokens": 21, "response_tokens_est": 49, "n_tiles": 12, "output_text": "The image depicts a red parking meter situated in a residential area, with a concrete sidewalk and greenery in the background. The meter is labeled \"Denver's Road Home,\" indicating it is part of a campaign to end homelessness.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25605.3, "ram_available_mb": 100166.9, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 25605.0, "ram_available_mb": 100167.2, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.07, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 34.17, "peak": 42.92, "min": 26.4}, "VIN": {"avg": 75.02, "peak": 118.57, "min": 55.63}, "VDD_CPU_SOC_MSS": {"avg": 15.61, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 34.17, "energy_joules_est": 108.81, "sample_count": 24, "duration_seconds": 3.185}, "timestamp": "2026-01-16T16:50:29.023817"}
{"image_index": 477, "image_name": "000000053994.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000053994.jpg", "image_width": 481, "image_height": 640, "image_resolution": "481x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3142.281, "latencies_ms": [3142.281], "images_per_second": 0.318, "prompt_tokens": 19, "response_tokens_est": 48, "n_tiles": 12, "output_text": "The image features a red parking meter with a white and black label. The meter is situated on a concrete sidewalk, surrounded by greenery and a tree. The lighting is natural, suggesting daytime, and the weather appears to be clear.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25605.0, "ram_available_mb": 100167.2, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 25605.0, "ram_available_mb": 100167.2, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.07, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 34.11, "peak": 42.94, "min": 26.4}, "VIN": {"avg": 71.51, "peak": 114.21, "min": 58.43}, "VDD_CPU_SOC_MSS": {"avg": 15.6, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 34.11, "energy_joules_est": 107.2, "sample_count": 24, "duration_seconds": 3.143}, "timestamp": "2026-01-16T16:50:32.172574"}
{"image_index": 478, "image_name": "000000054123.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054123.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1518.616, "latencies_ms": [1518.616], "images_per_second": 0.658, "prompt_tokens": 9, "response_tokens_est": 35, "n_tiles": 6, "output_text": "The image depicts a group of zebras grazing in a savanna-like environment, with their distinctive black and white striped patterns clearly visible on their bodies.", "error": null, "sys_before": {"cpu_percent": 38.4, "ram_used_mb": 25605.0, "ram_available_mb": 100167.2, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 25605.0, "ram_available_mb": 100167.2, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.46, "peak": 15.01, "min": 13.82}, "VDD_GPU": {"avg": 31.69, "peak": 37.41, "min": 25.6}, "VIN": {"avg": 75.86, "peak": 126.44, "min": 52.49}, "VDD_CPU_SOC_MSS": {"avg": 15.17, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 31.69, "energy_joules_est": 48.14, "sample_count": 11, "duration_seconds": 1.519}, "timestamp": "2026-01-16T16:50:33.755278"}
{"image_index": 478, "image_name": "000000054123.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054123.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 922.765, "latencies_ms": [922.765], "images_per_second": 1.084, "prompt_tokens": 23, "response_tokens_est": 11, "n_tiles": 6, "output_text": "zebra: 4\ngrass: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25605.0, "ram_available_mb": 100167.2, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 25604.8, "ram_available_mb": 100167.4, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.67, "peak": 14.94, "min": 14.22}, "VDD_GPU": {"avg": 35.61, "peak": 38.59, "min": 31.51}, "VIN": {"avg": 90.53, "peak": 113.18, "min": 64.92}, "VDD_CPU_SOC_MSS": {"avg": 15.24, "peak": 15.74, "min": 14.96}}, "power_watts_avg": 35.61, "energy_joules_est": 32.89, "sample_count": 7, "duration_seconds": 0.924}, "timestamp": "2026-01-16T16:50:34.684749"}
{"image_index": 478, "image_name": "000000054123.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054123.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2560.086, "latencies_ms": [2560.086], "images_per_second": 0.391, "prompt_tokens": 27, "response_tokens_est": 72, "n_tiles": 6, "output_text": "The main objects in the image are zebras, with the foreground and background being the most prominent. The foreground features a zebra with its head lowered, while the background shows other zebras grazing. The zebra in the foreground is closer to the camera, while the others are further away, creating a sense of depth in the image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25604.8, "ram_available_mb": 100167.4, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 25605.0, "ram_available_mb": 100167.2, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.9, "peak": 15.32, "min": 14.22}, "VDD_GPU": {"avg": 29.77, "peak": 40.16, "min": 23.64}, "VIN": {"avg": 64.91, "peak": 81.34, "min": 57.5}, "VDD_CPU_SOC_MSS": {"avg": 15.44, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 29.77, "energy_joules_est": 76.23, "sample_count": 19, "duration_seconds": 2.561}, "timestamp": "2026-01-16T16:50:37.250847"}
{"image_index": 478, "image_name": "000000054123.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054123.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2072.115, "latencies_ms": [2072.115], "images_per_second": 0.483, "prompt_tokens": 21, "response_tokens_est": 54, "n_tiles": 6, "output_text": "The image depicts a serene African savanna landscape with a group of zebras grazing on tall, dry grass. The zebras are in a natural setting, surrounded by sparse vegetation and a clear sky, creating a peaceful and tranquil atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25605.0, "ram_available_mb": 100167.2, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 25604.7, "ram_available_mb": 100167.4, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.9, "peak": 15.22, "min": 14.32}, "VDD_GPU": {"avg": 30.25, "peak": 38.21, "min": 24.03}, "VIN": {"avg": 65.09, "peak": 79.36, "min": 56.77}, "VDD_CPU_SOC_MSS": {"avg": 15.48, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 30.25, "energy_joules_est": 62.69, "sample_count": 15, "duration_seconds": 2.073}, "timestamp": "2026-01-16T16:50:39.329953"}
{"image_index": 478, "image_name": "000000054123.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054123.jpg", "image_width": 640, "image_height": 428, "image_resolution": "640x428", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2074.646, "latencies_ms": [2074.646], "images_per_second": 0.482, "prompt_tokens": 19, "response_tokens_est": 54, "n_tiles": 6, "output_text": "The image depicts a group of zebras grazing in a savanna-like environment. The zebras have distinctive black and white stripes, and the lighting is natural, suggesting it is daytime. The grass is tall and dry, indicating a dry season.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25604.7, "ram_available_mb": 100167.4, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 25604.5, "ram_available_mb": 100167.7, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.89, "peak": 15.22, "min": 14.32}, "VDD_GPU": {"avg": 30.17, "peak": 37.8, "min": 24.03}, "VIN": {"avg": 64.47, "peak": 72.8, "min": 55.78}, "VDD_CPU_SOC_MSS": {"avg": 15.48, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 30.17, "energy_joules_est": 62.61, "sample_count": 15, "duration_seconds": 2.075}, "timestamp": "2026-01-16T16:50:41.410489"}
{"image_index": 479, "image_name": "000000054164.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054164.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1287.338, "latencies_ms": [1287.338], "images_per_second": 0.777, "prompt_tokens": 9, "response_tokens_est": 26, "n_tiles": 6, "output_text": "A young man is surfing on a wave in the ocean, wearing a black wetsuit and standing on a surfboard.", "error": null, "sys_before": {"cpu_percent": 41.8, "ram_used_mb": 25604.5, "ram_available_mb": 100167.7, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 25604.2, "ram_available_mb": 100167.9, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.44, "peak": 15.01, "min": 13.92}, "VDD_GPU": {"avg": 31.87, "peak": 36.63, "min": 26.39}, "VIN": {"avg": 72.84, "peak": 120.04, "min": 61.4}, "VDD_CPU_SOC_MSS": {"avg": 15.08, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 31.87, "energy_joules_est": 41.04, "sample_count": 10, "duration_seconds": 1.288}, "timestamp": "2026-01-16T16:50:42.757287"}
{"image_index": 479, "image_name": "000000054164.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054164.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1644.398, "latencies_ms": [1644.398], "images_per_second": 0.608, "prompt_tokens": 23, "response_tokens_est": 38, "n_tiles": 6, "output_text": "1. Surfer\n2. Wetsuit\n3. Ocean\n4. Wave\n5. Water\n6. Surfboard\n7. Wave\n8. Sunlight", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25604.2, "ram_available_mb": 100167.9, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 25604.5, "ram_available_mb": 100167.7, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.76, "peak": 15.11, "min": 14.22}, "VDD_GPU": {"avg": 32.26, "peak": 38.98, "min": 25.21}, "VIN": {"avg": 68.98, "peak": 113.85, "min": 55.67}, "VDD_CPU_SOC_MSS": {"avg": 15.32, "peak": 15.74, "min": 14.96}}, "power_watts_avg": 32.26, "energy_joules_est": 53.06, "sample_count": 12, "duration_seconds": 1.645}, "timestamp": "2026-01-16T16:50:44.407935"}
{"image_index": 479, "image_name": "000000054164.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054164.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1961.832, "latencies_ms": [1961.832], "images_per_second": 0.51, "prompt_tokens": 27, "response_tokens_est": 50, "n_tiles": 6, "output_text": "The main object in the foreground is a surfer riding a wave. The surfer is positioned on a surfboard, with the wave in the background. The surfboard is near the surfer, and the wave is far in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25604.5, "ram_available_mb": 100167.7, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 25604.5, "ram_available_mb": 100167.7, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.89, "peak": 15.32, "min": 14.22}, "VDD_GPU": {"avg": 30.54, "peak": 38.19, "min": 24.42}, "VIN": {"avg": 64.06, "peak": 74.2, "min": 55.48}, "VDD_CPU_SOC_MSS": {"avg": 15.43, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 30.54, "energy_joules_est": 59.92, "sample_count": 15, "duration_seconds": 1.962}, "timestamp": "2026-01-16T16:50:46.375727"}
{"image_index": 479, "image_name": "000000054164.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054164.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2710.597, "latencies_ms": [2710.597], "images_per_second": 0.369, "prompt_tokens": 21, "response_tokens_est": 78, "n_tiles": 6, "output_text": "The image captures a dynamic scene of a surfer riding a wave in the ocean. The surfer, dressed in a black wetsuit, is skillfully maneuvering on a surfboard, with the wave's crest and spray creating a visually striking backdrop. The setting is a large, open ocean with clear blue water, indicating a sunny day with good weather conditions for surfing.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25604.5, "ram_available_mb": 100167.7, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 25604.5, "ram_available_mb": 100167.7, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.0, "peak": 15.32, "min": 14.32}, "VDD_GPU": {"avg": 28.53, "peak": 38.19, "min": 23.64}, "VIN": {"avg": 68.81, "peak": 100.0, "min": 55.6}, "VDD_CPU_SOC_MSS": {"avg": 15.52, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 28.53, "energy_joules_est": 77.35, "sample_count": 21, "duration_seconds": 2.711}, "timestamp": "2026-01-16T16:50:49.092712"}
{"image_index": 479, "image_name": "000000054164.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054164.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2178.492, "latencies_ms": [2178.492], "images_per_second": 0.459, "prompt_tokens": 19, "response_tokens_est": 58, "n_tiles": 6, "output_text": "The image captures a surfer in a black wetsuit riding a wave, with the ocean's blue-green color dominating the scene. The lighting is bright and natural, indicating a sunny day, and the waves are white and frothy, suggesting a strong and powerful ocean current.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25604.5, "ram_available_mb": 100167.7, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 25604.7, "ram_available_mb": 100167.4, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.94, "peak": 15.32, "min": 14.32}, "VDD_GPU": {"avg": 29.61, "peak": 38.21, "min": 24.03}, "VIN": {"avg": 71.29, "peak": 110.2, "min": 54.1}, "VDD_CPU_SOC_MSS": {"avg": 15.45, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 29.61, "energy_joules_est": 64.52, "sample_count": 17, "duration_seconds": 2.179}, "timestamp": "2026-01-16T16:50:51.277482"}
{"image_index": 480, "image_name": "000000054592.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054592.jpg", "image_width": 640, "image_height": 418, "image_resolution": "640x418", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1203.454, "latencies_ms": [1203.454], "images_per_second": 0.831, "prompt_tokens": 9, "response_tokens_est": 23, "n_tiles": 6, "output_text": "A skier is standing on a snowy slope, holding a pair of skis and preparing to ski.", "error": null, "sys_before": {"cpu_percent": 37.7, "ram_used_mb": 25604.5, "ram_available_mb": 100167.7, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25604.5, "ram_available_mb": 100167.7, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.38, "peak": 14.81, "min": 13.92}, "VDD_GPU": {"avg": 32.48, "peak": 37.03, "min": 27.18}, "VIN": {"avg": 73.33, "peak": 106.54, "min": 62.18}, "VDD_CPU_SOC_MSS": {"avg": 15.09, "peak": 15.35, "min": 14.56}}, "power_watts_avg": 32.48, "energy_joules_est": 39.1, "sample_count": 9, "duration_seconds": 1.204}, "timestamp": "2026-01-16T16:50:52.540425"}
{"image_index": 480, "image_name": "000000054592.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054592.jpg", "image_width": 640, "image_height": 418, "image_resolution": "640x418", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1699.472, "latencies_ms": [1699.472], "images_per_second": 0.588, "prompt_tokens": 23, "response_tokens_est": 40, "n_tiles": 6, "output_text": "1. Ski poles\n2. Ski\n3. Ski jacket\n4. Ski boots\n5. Ski gloves\n6. Ski pants\n7. Backpack\n8. Backpack strap", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25604.5, "ram_available_mb": 100167.7, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25604.5, "ram_available_mb": 100167.7, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.77, "peak": 15.11, "min": 14.22}, "VDD_GPU": {"avg": 31.94, "peak": 39.0, "min": 24.82}, "VIN": {"avg": 76.59, "peak": 122.44, "min": 56.27}, "VDD_CPU_SOC_MSS": {"avg": 15.32, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 31.94, "energy_joules_est": 54.3, "sample_count": 13, "duration_seconds": 1.7}, "timestamp": "2026-01-16T16:50:54.246001"}
{"image_index": 480, "image_name": "000000054592.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054592.jpg", "image_width": 640, "image_height": 418, "image_resolution": "640x418", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2944.716, "latencies_ms": [2944.716], "images_per_second": 0.34, "prompt_tokens": 27, "response_tokens_est": 83, "n_tiles": 6, "output_text": "The main object in the foreground is a person standing on a snowy slope, holding a pair of skis. The person is wearing a white jacket and pants, and is positioned near the center of the image. In the background, there is a mountain range with a clear sky above it. The person's backpack is visible to the right of the image, and a pole is also present in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25604.5, "ram_available_mb": 100167.7, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 25604.5, "ram_available_mb": 100167.7, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.82, "peak": 15.01, "min": 14.12}, "VDD_GPU": {"avg": 27.85, "peak": 38.19, "min": 23.24}, "VIN": {"avg": 67.57, "peak": 105.39, "min": 55.55}, "VDD_CPU_SOC_MSS": {"avg": 15.47, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 27.85, "energy_joules_est": 82.02, "sample_count": 23, "duration_seconds": 2.945}, "timestamp": "2026-01-16T16:50:57.196702"}
{"image_index": 480, "image_name": "000000054592.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054592.jpg", "image_width": 640, "image_height": 418, "image_resolution": "640x418", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2324.518, "latencies_ms": [2324.518], "images_per_second": 0.43, "prompt_tokens": 21, "response_tokens_est": 61, "n_tiles": 6, "output_text": "The image depicts a person dressed in winter gear, including a jacket and pants, standing on a snowy slope. The individual is holding a pair of skis and appears to be preparing for a skiing activity, with the sun setting in the background, casting a warm glow over the scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25604.5, "ram_available_mb": 100167.7, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 25604.5, "ram_available_mb": 100167.7, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.74, "peak": 15.01, "min": 14.02}, "VDD_GPU": {"avg": 28.84, "peak": 37.8, "min": 23.24}, "VIN": {"avg": 69.2, "peak": 114.16, "min": 55.67}, "VDD_CPU_SOC_MSS": {"avg": 15.38, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 28.84, "energy_joules_est": 67.05, "sample_count": 18, "duration_seconds": 2.325}, "timestamp": "2026-01-16T16:50:59.527297"}
{"image_index": 480, "image_name": "000000054592.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054592.jpg", "image_width": 640, "image_height": 418, "image_resolution": "640x418", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2492.795, "latencies_ms": [2492.795], "images_per_second": 0.401, "prompt_tokens": 19, "response_tokens_est": 67, "n_tiles": 6, "output_text": "The image depicts a person dressed in a white ski suit and a dark jacket, standing on a snowy slope during what appears to be either sunrise or sunset. The lighting is soft and warm, casting a golden hue over the scene, while the snow is undisturbed, indicating a peaceful and serene environment.", "error": null, "sys_before": {"cpu_percent": 20.0, "ram_used_mb": 25604.5, "ram_available_mb": 100167.7, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 25604.5, "ram_available_mb": 100167.7, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.75, "peak": 15.01, "min": 14.12}, "VDD_GPU": {"avg": 28.59, "peak": 37.82, "min": 23.24}, "VIN": {"avg": 69.28, "peak": 116.07, "min": 55.32}, "VDD_CPU_SOC_MSS": {"avg": 15.38, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 28.59, "energy_joules_est": 71.28, "sample_count": 19, "duration_seconds": 2.493}, "timestamp": "2026-01-16T16:51:02.027334"}
{"image_index": 481, "image_name": "000000054593.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054593.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1737.024, "latencies_ms": [1737.024], "images_per_second": 0.576, "prompt_tokens": 9, "response_tokens_est": 43, "n_tiles": 6, "output_text": "A young baseball player in a red jersey and white pants is in the middle of a swing, with a catcher and an umpire crouched behind him, all set up on a baseball field.", "error": null, "sys_before": {"cpu_percent": 39.8, "ram_used_mb": 25604.5, "ram_available_mb": 100167.7, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 25604.4, "ram_available_mb": 100167.7, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.53, "peak": 15.11, "min": 13.82}, "VDD_GPU": {"avg": 30.09, "peak": 36.62, "min": 24.42}, "VIN": {"avg": 68.51, "peak": 97.38, "min": 53.77}, "VDD_CPU_SOC_MSS": {"avg": 15.2, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 30.09, "energy_joules_est": 52.28, "sample_count": 13, "duration_seconds": 1.737}, "timestamp": "2026-01-16T16:51:03.830815"}
{"image_index": 481, "image_name": "000000054593.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054593.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1856.499, "latencies_ms": [1856.499], "images_per_second": 0.539, "prompt_tokens": 23, "response_tokens_est": 46, "n_tiles": 6, "output_text": "baseball bat: 1\ncatcher's mask: 1\ncatcher's gear: 1\numpire: 1\nplayer: 1\nbaseball: 1\npitcher: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25604.4, "ram_available_mb": 100167.7, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 25604.4, "ram_available_mb": 100167.7, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.82, "peak": 15.22, "min": 14.22}, "VDD_GPU": {"avg": 30.89, "peak": 38.59, "min": 24.42}, "VIN": {"avg": 68.97, "peak": 109.54, "min": 55.42}, "VDD_CPU_SOC_MSS": {"avg": 15.43, "peak": 15.74, "min": 14.96}}, "power_watts_avg": 30.89, "energy_joules_est": 57.36, "sample_count": 14, "duration_seconds": 1.857}, "timestamp": "2026-01-16T16:51:05.693502"}
{"image_index": 481, "image_name": "000000054593.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054593.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3176.23, "latencies_ms": [3176.23], "images_per_second": 0.315, "prompt_tokens": 27, "response_tokens_est": 95, "n_tiles": 6, "output_text": "The main object in the foreground is a baseball player wearing a red jersey and white pants, preparing to swing at the incoming ball. The baseball player is positioned near the home plate, with the catcher and umpire in the background. The catcher is crouched behind the home plate, and the umpire is standing near the fence. The background features a chain-link fence, a parked car, and a grassy area with a few spectators.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25604.4, "ram_available_mb": 100167.7, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 25604.4, "ram_available_mb": 100167.7, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.04, "peak": 15.32, "min": 14.22}, "VDD_GPU": {"avg": 27.84, "peak": 38.21, "min": 23.64}, "VIN": {"avg": 70.47, "peak": 118.63, "min": 58.12}, "VDD_CPU_SOC_MSS": {"avg": 15.65, "peak": 16.14, "min": 14.96}}, "power_watts_avg": 27.84, "energy_joules_est": 88.44, "sample_count": 24, "duration_seconds": 3.177}, "timestamp": "2026-01-16T16:51:08.875885"}
{"image_index": 481, "image_name": "000000054593.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054593.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2763.067, "latencies_ms": [2763.067], "images_per_second": 0.362, "prompt_tokens": 21, "response_tokens_est": 80, "n_tiles": 6, "output_text": "The image captures a scene from a baseball game taking place on a dirt field enclosed by a chain-link fence. A young baseball player in a red jersey and white pants is in the midst of swinging at a pitch, while a catcher in black gear is crouched behind him, ready to catch the ball. In the background, spectators are seated on folding chairs, watching the game.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25604.4, "ram_available_mb": 100167.7, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.5, "ram_used_mb": 25604.9, "ram_available_mb": 100167.2, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.01, "peak": 15.32, "min": 14.22}, "VDD_GPU": {"avg": 28.4, "peak": 37.8, "min": 23.64}, "VIN": {"avg": 69.53, "peak": 121.61, "min": 58.93}, "VDD_CPU_SOC_MSS": {"avg": 15.52, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 28.4, "energy_joules_est": 78.48, "sample_count": 21, "duration_seconds": 2.764}, "timestamp": "2026-01-16T16:51:11.645266"}
{"image_index": 481, "image_name": "000000054593.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054593.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1644.854, "latencies_ms": [1644.854], "images_per_second": 0.608, "prompt_tokens": 19, "response_tokens_est": 38, "n_tiles": 6, "output_text": "The baseball player is wearing a red jersey and white pants, with a black helmet and black cleats. The scene is brightly lit by natural sunlight, casting shadows on the ground.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25604.9, "ram_available_mb": 100167.2, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 25604.7, "ram_available_mb": 100167.5, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.82, "peak": 15.11, "min": 14.32}, "VDD_GPU": {"avg": 31.71, "peak": 38.21, "min": 25.21}, "VIN": {"avg": 72.45, "peak": 113.73, "min": 57.91}, "VDD_CPU_SOC_MSS": {"avg": 15.42, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 31.71, "energy_joules_est": 52.17, "sample_count": 12, "duration_seconds": 1.645}, "timestamp": "2026-01-16T16:51:13.296544"}
{"image_index": 482, "image_name": "000000054605.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054605.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1011.649, "latencies_ms": [1011.649], "images_per_second": 0.988, "prompt_tokens": 9, "response_tokens_est": 43, "n_tiles": 1, "output_text": "A woman is sitting at a table, enjoying a chocolate and vanilla ice cream cone with a dollop of whipped cream on top, while a slice of cake is placed on a plate in front of her.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25604.7, "ram_available_mb": 100167.5, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25604.7, "ram_available_mb": 100167.5, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 4845.7, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.26, "peak": 15.62, "min": 14.61}, "VDD_GPU": {"avg": 23.19, "peak": 26.79, "min": 20.88}, "VIN": {"avg": 63.43, "peak": 68.53, "min": 60.05}, "VDD_CPU_SOC_MSS": {"avg": 16.03, "peak": 16.54, "min": 15.36}}, "power_watts_avg": 23.19, "energy_joules_est": 23.47, "sample_count": 7, "duration_seconds": 1.012}, "timestamp": "2026-01-16T16:51:14.332771"}
{"image_index": 482, "image_name": "000000054605.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054605.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1321.991, "latencies_ms": [1321.991], "images_per_second": 0.756, "prompt_tokens": 23, "response_tokens_est": 58, "n_tiles": 1, "output_text": "- Chocolate cake: 1\n- Whipped cream: 1\n- Whisk: 1\n- Glass: 1\n- Plate: 1\n- Fork: 2\n- Knife: 1\n- Table: 1\n- Person: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25604.7, "ram_available_mb": 100167.5, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25604.7, "ram_available_mb": 100167.5, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 4856.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.51, "peak": 15.62, "min": 15.11}, "VDD_GPU": {"avg": 21.63, "peak": 24.83, "min": 20.1}, "VIN": {"avg": 63.48, "peak": 68.88, "min": 60.36}, "VDD_CPU_SOC_MSS": {"avg": 16.46, "peak": 16.54, "min": 16.14}}, "power_watts_avg": 21.63, "energy_joules_est": 28.6, "sample_count": 10, "duration_seconds": 1.322}, "timestamp": "2026-01-16T16:51:15.660426"}
{"image_index": 482, "image_name": "000000054605.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054605.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1869.222, "latencies_ms": [1869.222], "images_per_second": 0.535, "prompt_tokens": 27, "response_tokens_est": 83, "n_tiles": 1, "output_text": "The main objects in the image are a chocolate-flavored milkshake and a slice of cake. The milkshake is positioned in the foreground on a saucer, with a straw in it. The cake is placed on a white plate in the background, slightly to the right. The background also includes a table with other items, such as a glass and a fork, and a person sitting at a table.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25604.7, "ram_available_mb": 100167.5, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25604.9, "ram_available_mb": 100167.2, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 4859.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.6, "peak": 15.72, "min": 15.22}, "VDD_GPU": {"avg": 21.13, "peak": 24.42, "min": 20.09}, "VIN": {"avg": 62.97, "peak": 71.78, "min": 56.66}, "VDD_CPU_SOC_MSS": {"avg": 16.45, "peak": 16.54, "min": 16.14}}, "power_watts_avg": 21.13, "energy_joules_est": 39.51, "sample_count": 14, "duration_seconds": 1.87}, "timestamp": "2026-01-16T16:51:17.535732"}
{"image_index": 482, "image_name": "000000054605.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054605.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1695.375, "latencies_ms": [1695.375], "images_per_second": 0.59, "prompt_tokens": 21, "response_tokens_est": 75, "n_tiles": 1, "output_text": "The image depicts a cozy cafe setting with a person seated at a table, enjoying a chocolate and vanilla ice cream cone. In front of them is a plate of cake, which appears to be a layered vanilla cake with a white frosting. The scene is set in a warm, inviting atmosphere with a focus on the dessert and the person's enjoyment of it.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25604.9, "ram_available_mb": 100167.2, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25604.7, "ram_available_mb": 100167.5, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 4854.7, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.64, "peak": 15.72, "min": 15.22}, "VDD_GPU": {"avg": 21.03, "peak": 24.04, "min": 20.09}, "VIN": {"avg": 62.8, "peak": 65.35, "min": 59.78}, "VDD_CPU_SOC_MSS": {"avg": 16.44, "peak": 16.54, "min": 16.14}}, "power_watts_avg": 21.03, "energy_joules_est": 35.66, "sample_count": 13, "duration_seconds": 1.696}, "timestamp": "2026-01-16T16:51:19.237299"}
{"image_index": 482, "image_name": "000000054605.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054605.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1910.302, "latencies_ms": [1910.302], "images_per_second": 0.523, "prompt_tokens": 19, "response_tokens_est": 85, "n_tiles": 1, "output_text": "The image features a dessert on a white plate, which is placed on a dark table. The dessert is a layered cake with a creamy white frosting. The background includes a glass of a chocolate and vanilla-flavored milkshake, a person sitting at a table, and a table setting with chairs and a tablecloth. The lighting is bright, and the overall ambiance suggests a warm, inviting atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25604.7, "ram_available_mb": 100167.5, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25604.7, "ram_available_mb": 100167.5, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 4853.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.56, "peak": 15.72, "min": 15.22}, "VDD_GPU": {"avg": 20.99, "peak": 24.03, "min": 20.1}, "VIN": {"avg": 63.46, "peak": 70.01, "min": 59.67}, "VDD_CPU_SOC_MSS": {"avg": 16.48, "peak": 16.54, "min": 16.14}}, "power_watts_avg": 20.99, "energy_joules_est": 40.1, "sample_count": 14, "duration_seconds": 1.911}, "timestamp": "2026-01-16T16:51:21.153507"}
{"image_index": 483, "image_name": "000000054628.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054628.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2481.67, "latencies_ms": [2481.67], "images_per_second": 0.403, "prompt_tokens": 9, "response_tokens_est": 32, "n_tiles": 12, "output_text": "The image shows a beautifully decorated cake with intricate designs and a variety of flowers, placed on a table surrounded by other tables and chairs in a dining area.", "error": null, "sys_before": {"cpu_percent": 49.5, "ram_used_mb": 25604.7, "ram_available_mb": 100167.5, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 25604.7, "ram_available_mb": 100167.5, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.2, "peak": 16.15, "min": 14.3}, "VDD_GPU": {"avg": 34.25, "peak": 40.18, "min": 27.18}, "VIN": {"avg": 79.99, "peak": 119.93, "min": 58.57}, "VDD_CPU_SOC_MSS": {"avg": 16.01, "peak": 17.32, "min": 15.35}}, "power_watts_avg": 34.25, "energy_joules_est": 85.01, "sample_count": 19, "duration_seconds": 2.482}, "timestamp": "2026-01-16T16:51:23.715811"}
{"image_index": 483, "image_name": "000000054628.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054628.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 5849.26, "latencies_ms": [5849.26], "images_per_second": 0.171, "prompt_tokens": 23, "response_tokens_est": 128, "n_tiles": 12, "output_text": "object: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject:", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25604.7, "ram_available_mb": 100167.5, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 25604.7, "ram_available_mb": 100167.5, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.1, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 30.5, "peak": 42.94, "min": 26.0}, "VIN": {"avg": 71.15, "peak": 120.16, "min": 58.41}, "VDD_CPU_SOC_MSS": {"avg": 15.73, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 30.5, "energy_joules_est": 178.41, "sample_count": 46, "duration_seconds": 5.85}, "timestamp": "2026-01-16T16:51:29.571713"}
{"image_index": 483, "image_name": "000000054628.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054628.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 4327.344, "latencies_ms": [4327.344], "images_per_second": 0.231, "prompt_tokens": 27, "response_tokens_est": 83, "n_tiles": 12, "output_text": "The main object in the foreground is a large, intricately decorated cake placed on a blue tablecloth. The cake is positioned near the center of the image, drawing attention to its elaborate design. In the background, there are multiple tables covered with white tablecloths, each adorned with various decorations and items. The tables are arranged in a semi-circle, creating a sense of gathering and celebration.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25604.7, "ram_available_mb": 100167.5, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 25604.6, "ram_available_mb": 100167.5, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.08, "peak": 15.54, "min": 14.53}, "VDD_GPU": {"avg": 31.88, "peak": 42.94, "min": 26.0}, "VIN": {"avg": 73.92, "peak": 132.31, "min": 58.25}, "VDD_CPU_SOC_MSS": {"avg": 15.75, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 31.88, "energy_joules_est": 137.97, "sample_count": 34, "duration_seconds": 4.328}, "timestamp": "2026-01-16T16:51:33.905884"}
{"image_index": 483, "image_name": "000000054628.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054628.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3519.263, "latencies_ms": [3519.263], "images_per_second": 0.284, "prompt_tokens": 21, "response_tokens_est": 59, "n_tiles": 12, "output_text": "The image depicts a beautifully decorated wedding reception venue with a large, intricately designed cake placed on a table. The room is filled with guests seated at round tables covered with white tablecloths, and a grand chandelier hangs from the ceiling, adding to the elegant atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25604.6, "ram_available_mb": 100167.5, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25603.7, "ram_available_mb": 100168.5, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.07, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 33.34, "peak": 42.54, "min": 26.39}, "VIN": {"avg": 71.15, "peak": 96.41, "min": 53.07}, "VDD_CPU_SOC_MSS": {"avg": 15.7, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 33.34, "energy_joules_est": 117.35, "sample_count": 27, "duration_seconds": 3.52}, "timestamp": "2026-01-16T16:51:37.435791"}
{"image_index": 483, "image_name": "000000054628.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054628.jpg", "image_width": 375, "image_height": 500, "image_resolution": "375x500", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3051.417, "latencies_ms": [3051.417], "images_per_second": 0.328, "prompt_tokens": 19, "response_tokens_est": 45, "n_tiles": 12, "output_text": "The cake in the image is predominantly white with intricate golden patterns. The lighting is warm, creating a cozy atmosphere. The setting appears to be indoors, with a blue tablecloth and a blue curtain in the background.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25603.7, "ram_available_mb": 100168.5, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 25604.9, "ram_available_mb": 100167.3, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.06, "peak": 15.65, "min": 14.43}, "VDD_GPU": {"avg": 34.58, "peak": 42.94, "min": 26.79}, "VIN": {"avg": 83.42, "peak": 135.28, "min": 58.44}, "VDD_CPU_SOC_MSS": {"avg": 15.69, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 34.58, "energy_joules_est": 105.53, "sample_count": 23, "duration_seconds": 3.052}, "timestamp": "2026-01-16T16:51:40.493890"}
{"image_index": 484, "image_name": "000000054654.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054654.jpg", "image_width": 407, "image_height": 640, "image_resolution": "407x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1367.441, "latencies_ms": [1367.441], "images_per_second": 0.731, "prompt_tokens": 9, "response_tokens_est": 29, "n_tiles": 6, "output_text": "A woman is standing in a kitchen, smiling at the camera, holding a plate of food in one hand and a fork in the other.", "error": null, "sys_before": {"cpu_percent": 38.0, "ram_used_mb": 25604.9, "ram_available_mb": 100167.3, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 25605.1, "ram_available_mb": 100167.0, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.41, "peak": 14.91, "min": 13.92}, "VDD_GPU": {"avg": 32.5, "peak": 37.82, "min": 26.39}, "VIN": {"avg": 84.1, "peak": 123.3, "min": 64.36}, "VDD_CPU_SOC_MSS": {"avg": 15.12, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 32.5, "energy_joules_est": 44.47, "sample_count": 10, "duration_seconds": 1.368}, "timestamp": "2026-01-16T16:51:41.916634"}
{"image_index": 484, "image_name": "000000054654.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054654.jpg", "image_width": 407, "image_height": 640, "image_resolution": "407x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1673.576, "latencies_ms": [1673.576], "images_per_second": 0.598, "prompt_tokens": 23, "response_tokens_est": 39, "n_tiles": 6, "output_text": "1. Woman\n2. Plate\n3. Stove\n4. Pot\n5. Garbage can\n6. Pots\n7. Pans\n8. Stove top", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25605.1, "ram_available_mb": 100167.0, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 25605.1, "ram_available_mb": 100167.0, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.77, "peak": 15.22, "min": 14.22}, "VDD_GPU": {"avg": 31.57, "peak": 38.59, "min": 24.82}, "VIN": {"avg": 65.3, "peak": 77.34, "min": 51.66}, "VDD_CPU_SOC_MSS": {"avg": 15.41, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 31.57, "energy_joules_est": 52.85, "sample_count": 13, "duration_seconds": 1.674}, "timestamp": "2026-01-16T16:51:43.596251"}
{"image_index": 484, "image_name": "000000054654.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054654.jpg", "image_width": 407, "image_height": 640, "image_resolution": "407x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2050.116, "latencies_ms": [2050.116], "images_per_second": 0.488, "prompt_tokens": 27, "response_tokens_est": 53, "n_tiles": 6, "output_text": "The main object in the foreground is a woman wearing a blue sweater with red and white patterns. She is standing near a stove with a pot on it. In the background, there is a kitchen counter with various items, including a green poster with a message.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25605.1, "ram_available_mb": 100167.0, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 25605.1, "ram_available_mb": 100167.0, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.92, "peak": 15.32, "min": 14.32}, "VDD_GPU": {"avg": 30.13, "peak": 38.19, "min": 24.03}, "VIN": {"avg": 72.56, "peak": 109.71, "min": 63.52}, "VDD_CPU_SOC_MSS": {"avg": 15.53, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 30.13, "energy_joules_est": 61.78, "sample_count": 16, "duration_seconds": 2.05}, "timestamp": "2026-01-16T16:51:45.652364"}
{"image_index": 484, "image_name": "000000054654.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054654.jpg", "image_width": 407, "image_height": 640, "image_resolution": "407x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2151.432, "latencies_ms": [2151.432], "images_per_second": 0.465, "prompt_tokens": 21, "response_tokens_est": 57, "n_tiles": 6, "output_text": "The image depicts a cozy kitchen scene with a woman standing in front of a stove. She is wearing a blue sweater with red and white patterns and is holding a plate of food. The kitchen has a rustic feel, with wooden cabinets and a patterned rug on the floor.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25605.1, "ram_available_mb": 100167.0, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 25605.4, "ram_available_mb": 100166.8, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.92, "peak": 15.22, "min": 14.32}, "VDD_GPU": {"avg": 29.89, "peak": 38.21, "min": 24.03}, "VIN": {"avg": 72.18, "peak": 116.01, "min": 59.93}, "VDD_CPU_SOC_MSS": {"avg": 15.57, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 29.89, "energy_joules_est": 64.32, "sample_count": 16, "duration_seconds": 2.152}, "timestamp": "2026-01-16T16:51:47.814094"}
{"image_index": 484, "image_name": "000000054654.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054654.jpg", "image_width": 407, "image_height": 640, "image_resolution": "407x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1885.922, "latencies_ms": [1885.922], "images_per_second": 0.53, "prompt_tokens": 19, "response_tokens_est": 47, "n_tiles": 6, "output_text": "The image features a woman in a blue sweater with red and white patterns, standing in a kitchen. The kitchen has a rustic feel with wooden cabinets and a white stove. The lighting is warm, and the overall atmosphere is cozy.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25605.4, "ram_available_mb": 100166.8, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 25604.9, "ram_available_mb": 100167.3, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.86, "peak": 15.22, "min": 14.22}, "VDD_GPU": {"avg": 30.69, "peak": 38.19, "min": 24.42}, "VIN": {"avg": 71.94, "peak": 118.27, "min": 50.9}, "VDD_CPU_SOC_MSS": {"avg": 15.35, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 30.69, "energy_joules_est": 57.89, "sample_count": 14, "duration_seconds": 1.886}, "timestamp": "2026-01-16T16:51:49.705960"}
{"image_index": 485, "image_name": "000000054931.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054931.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1041.919, "latencies_ms": [1041.919], "images_per_second": 0.96, "prompt_tokens": 9, "response_tokens_est": 17, "n_tiles": 6, "output_text": "A woman is holding a rope and walking on a dirt path in a park.", "error": null, "sys_before": {"cpu_percent": 37.9, "ram_used_mb": 25604.9, "ram_available_mb": 100167.3, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 25605.1, "ram_available_mb": 100167.1, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.36, "peak": 14.53, "min": 13.92}, "VDD_GPU": {"avg": 34.1, "peak": 37.8, "min": 29.94}, "VIN": {"avg": 73.46, "peak": 117.85, "min": 58.73}, "VDD_CPU_SOC_MSS": {"avg": 15.13, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 34.1, "energy_joules_est": 35.56, "sample_count": 7, "duration_seconds": 1.043}, "timestamp": "2026-01-16T16:51:50.794780"}
{"image_index": 485, "image_name": "000000054931.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054931.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1724.265, "latencies_ms": [1724.265], "images_per_second": 0.58, "prompt_tokens": 23, "response_tokens_est": 41, "n_tiles": 6, "output_text": "woman: 1\nshirt: 1\njeans: 1\nboot: 1\nrope: 1\nwoman's hair: 1\nwoman's face: 1", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25605.1, "ram_available_mb": 100167.1, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 25605.1, "ram_available_mb": 100167.1, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.74, "peak": 15.11, "min": 14.22}, "VDD_GPU": {"avg": 32.27, "peak": 39.38, "min": 24.82}, "VIN": {"avg": 70.24, "peak": 117.99, "min": 58.7}, "VDD_CPU_SOC_MSS": {"avg": 15.35, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 32.27, "energy_joules_est": 55.66, "sample_count": 13, "duration_seconds": 1.725}, "timestamp": "2026-01-16T16:51:52.526141"}
{"image_index": 485, "image_name": "000000054931.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054931.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1888.39, "latencies_ms": [1888.39], "images_per_second": 0.53, "prompt_tokens": 27, "response_tokens_est": 47, "n_tiles": 6, "output_text": "The main object in the foreground is a woman holding a rope. She is standing on a dirt ground, with a wooden fence visible in the background. The fence is partially obscured by trees, indicating a rural or park setting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25605.1, "ram_available_mb": 100167.1, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 25605.1, "ram_available_mb": 100167.1, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.86, "peak": 15.22, "min": 14.22}, "VDD_GPU": {"avg": 30.89, "peak": 38.59, "min": 24.42}, "VIN": {"avg": 73.85, "peak": 114.66, "min": 62.95}, "VDD_CPU_SOC_MSS": {"avg": 15.47, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 30.89, "energy_joules_est": 58.34, "sample_count": 14, "duration_seconds": 1.889}, "timestamp": "2026-01-16T16:51:54.420906"}
{"image_index": 485, "image_name": "000000054931.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054931.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2415.104, "latencies_ms": [2415.104], "images_per_second": 0.414, "prompt_tokens": 21, "response_tokens_est": 67, "n_tiles": 6, "output_text": "The image depicts a woman standing in a natural outdoor setting, holding a pair of light-colored ropes. She is dressed in a pink shirt, black pants, and brown boots, and appears to be in a relaxed, casual pose. The background features a dirt path surrounded by greenery, suggesting a peaceful, rural environment.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25605.1, "ram_available_mb": 100167.1, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 25605.4, "ram_available_mb": 100166.8, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.97, "peak": 15.32, "min": 14.22}, "VDD_GPU": {"avg": 29.04, "peak": 38.21, "min": 24.03}, "VIN": {"avg": 70.37, "peak": 125.77, "min": 60.16}, "VDD_CPU_SOC_MSS": {"avg": 15.56, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 29.04, "energy_joules_est": 70.14, "sample_count": 19, "duration_seconds": 2.415}, "timestamp": "2026-01-16T16:51:56.841823"}
{"image_index": 485, "image_name": "000000054931.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054931.jpg", "image_width": 427, "image_height": 640, "image_resolution": "427x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1455.677, "latencies_ms": [1455.677], "images_per_second": 0.687, "prompt_tokens": 19, "response_tokens_est": 31, "n_tiles": 6, "output_text": "The woman is wearing a pink button-up shirt and black pants, paired with brown boots. The lighting is bright and natural, suggesting it is daytime.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25605.4, "ram_available_mb": 100166.8, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 25605.1, "ram_available_mb": 100167.1, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.78, "peak": 15.11, "min": 14.22}, "VDD_GPU": {"avg": 32.44, "peak": 38.19, "min": 26.0}, "VIN": {"avg": 70.48, "peak": 91.93, "min": 57.31}, "VDD_CPU_SOC_MSS": {"avg": 15.35, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 32.44, "energy_joules_est": 47.23, "sample_count": 11, "duration_seconds": 1.456}, "timestamp": "2026-01-16T16:51:58.303823"}
{"image_index": 486, "image_name": "000000054967.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054967.jpg", "image_width": 424, "image_height": 640, "image_resolution": "424x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2155.167, "latencies_ms": [2155.167], "images_per_second": 0.464, "prompt_tokens": 9, "response_tokens_est": 58, "n_tiles": 6, "output_text": "The image depicts a busy urban street scene with multiple vehicles, including a truck, cars, and a bus, all traveling in the same direction. The street is lined with tall buildings, and there are various traffic signs and signals visible, indicating a well-organized traffic system.", "error": null, "sys_before": {"cpu_percent": 43.5, "ram_used_mb": 25605.1, "ram_available_mb": 100167.1, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 25605.6, "ram_available_mb": 100166.6, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.73, "peak": 15.11, "min": 13.92}, "VDD_GPU": {"avg": 29.3, "peak": 37.8, "min": 23.64}, "VIN": {"avg": 65.34, "peak": 75.67, "min": 56.03}, "VDD_CPU_SOC_MSS": {"avg": 15.4, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 29.3, "energy_joules_est": 63.16, "sample_count": 16, "duration_seconds": 2.155}, "timestamp": "2026-01-16T16:52:00.507956"}
{"image_index": 486, "image_name": "000000054967.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054967.jpg", "image_width": 424, "image_height": 640, "image_resolution": "424x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1517.466, "latencies_ms": [1517.466], "images_per_second": 0.659, "prompt_tokens": 23, "response_tokens_est": 32, "n_tiles": 6, "output_text": "1. Car\n2. Car\n3. Car\n4. Car\n5. Car\n6. Car\n7. Car\n8. Car", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25605.6, "ram_available_mb": 100166.6, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25605.3, "ram_available_mb": 100166.8, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.59, "peak": 14.91, "min": 14.02}, "VDD_GPU": {"avg": 31.91, "peak": 38.21, "min": 25.22}, "VIN": {"avg": 73.37, "peak": 106.82, "min": 55.56}, "VDD_CPU_SOC_MSS": {"avg": 15.14, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 31.91, "energy_joules_est": 48.44, "sample_count": 11, "duration_seconds": 1.518}, "timestamp": "2026-01-16T16:52:02.031598"}
{"image_index": 486, "image_name": "000000054967.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054967.jpg", "image_width": 424, "image_height": 640, "image_resolution": "424x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2697.304, "latencies_ms": [2697.304], "images_per_second": 0.371, "prompt_tokens": 27, "response_tokens_est": 74, "n_tiles": 6, "output_text": "The main objects in the image are a city street scene with a mix of vehicles and pedestrians. The foreground features a sidewalk with a person walking, while the background shows a dense urban environment with tall buildings and a mix of vehicles, including a bus and a truck. The street is divided into lanes with clear markings, and there are traffic signs and signals present.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25605.3, "ram_available_mb": 100166.8, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.6, "ram_used_mb": 25605.3, "ram_available_mb": 100166.8, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.77, "peak": 15.01, "min": 14.02}, "VDD_GPU": {"avg": 28.32, "peak": 38.6, "min": 23.24}, "VIN": {"avg": 70.7, "peak": 118.38, "min": 57.43}, "VDD_CPU_SOC_MSS": {"avg": 15.29, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 28.32, "energy_joules_est": 76.39, "sample_count": 20, "duration_seconds": 2.698}, "timestamp": "2026-01-16T16:52:04.735085"}
{"image_index": 486, "image_name": "000000054967.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054967.jpg", "image_width": 424, "image_height": 640, "image_resolution": "424x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2748.966, "latencies_ms": [2748.966], "images_per_second": 0.364, "prompt_tokens": 21, "response_tokens_est": 76, "n_tiles": 6, "output_text": "The image depicts a bustling urban street scene during the daytime, likely in a city with tall buildings. The street is filled with cars, including a white SUV and a dark-colored car, all moving in the same direction. Traffic lights are visible, indicating a controlled flow of traffic. The atmosphere appears to be relatively quiet, with no pedestrians visible in the immediate vicinity.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25605.3, "ram_available_mb": 100166.8, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 25605.3, "ram_available_mb": 100166.8, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.78, "peak": 15.01, "min": 14.12}, "VDD_GPU": {"avg": 27.8, "peak": 38.21, "min": 23.24}, "VIN": {"avg": 70.82, "peak": 112.35, "min": 59.41}, "VDD_CPU_SOC_MSS": {"avg": 15.3, "peak": 15.75, "min": 14.57}}, "power_watts_avg": 27.8, "energy_joules_est": 76.43, "sample_count": 21, "duration_seconds": 2.749}, "timestamp": "2026-01-16T16:52:07.490091"}
{"image_index": 486, "image_name": "000000054967.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000054967.jpg", "image_width": 424, "image_height": 640, "image_resolution": "424x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2272.814, "latencies_ms": [2272.814], "images_per_second": 0.44, "prompt_tokens": 19, "response_tokens_est": 59, "n_tiles": 6, "output_text": "The image depicts a city street scene during the daytime, with a clear blue sky and a few scattered clouds. The street is lined with tall buildings, and the overall lighting is bright and natural. The weather appears to be clear and sunny, with no visible signs of rain or snow.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25605.3, "ram_available_mb": 100166.8, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 25605.3, "ram_available_mb": 100166.8, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.73, "peak": 15.01, "min": 14.02}, "VDD_GPU": {"avg": 28.97, "peak": 38.19, "min": 23.25}, "VIN": {"avg": 69.38, "peak": 110.56, "min": 58.2}, "VDD_CPU_SOC_MSS": {"avg": 15.26, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 28.97, "energy_joules_est": 65.85, "sample_count": 17, "duration_seconds": 2.273}, "timestamp": "2026-01-16T16:52:09.770829"}
{"image_index": 487, "image_name": "000000055002.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055002.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2413.476, "latencies_ms": [2413.476], "images_per_second": 0.414, "prompt_tokens": 9, "response_tokens_est": 30, "n_tiles": 12, "output_text": "The image shows a close-up view of a stainless steel toilet with a blue toilet brush placed on the side, situated in a tiled bathroom.", "error": null, "sys_before": {"cpu_percent": 45.0, "ram_used_mb": 25605.3, "ram_available_mb": 100166.8, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 25605.3, "ram_available_mb": 100166.9, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.15, "peak": 16.15, "min": 13.9}, "VDD_GPU": {"avg": 34.99, "peak": 40.97, "min": 27.57}, "VIN": {"avg": 79.86, "peak": 118.61, "min": 53.73}, "VDD_CPU_SOC_MSS": {"avg": 15.98, "peak": 17.32, "min": 14.96}}, "power_watts_avg": 34.99, "energy_joules_est": 84.46, "sample_count": 18, "duration_seconds": 2.414}, "timestamp": "2026-01-16T16:52:12.272204"}
{"image_index": 487, "image_name": "000000055002.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055002.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2737.72, "latencies_ms": [2737.72], "images_per_second": 0.365, "prompt_tokens": 23, "response_tokens_est": 36, "n_tiles": 12, "output_text": "toilet brush: 1\ntoilet: 1\ntoilet paper: 1\ntoilet paper roll: 1\ntoilet paper holder: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25605.3, "ram_available_mb": 100166.9, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 2.8, "ram_used_mb": 25606.3, "ram_available_mb": 100165.9, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.08, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 35.53, "peak": 43.33, "min": 27.18}, "VIN": {"avg": 73.91, "peak": 110.29, "min": 57.87}, "VDD_CPU_SOC_MSS": {"avg": 15.67, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 35.53, "energy_joules_est": 97.28, "sample_count": 21, "duration_seconds": 2.738}, "timestamp": "2026-01-16T16:52:15.016169"}
{"image_index": 487, "image_name": "000000055002.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055002.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3284.44, "latencies_ms": [3284.44], "images_per_second": 0.304, "prompt_tokens": 27, "response_tokens_est": 52, "n_tiles": 12, "output_text": "The main object in the foreground is a toilet with a blue seat and lid. The toilet is situated on a tiled floor, with a metal toilet paper holder attached to the wall. The background features a tiled wall and a metal toilet paper holder.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25606.3, "ram_available_mb": 100165.9, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 25606.3, "ram_available_mb": 100165.9, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.1, "peak": 15.65, "min": 14.43}, "VDD_GPU": {"avg": 33.91, "peak": 42.94, "min": 26.4}, "VIN": {"avg": 73.48, "peak": 113.25, "min": 58.51}, "VDD_CPU_SOC_MSS": {"avg": 15.62, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 33.91, "energy_joules_est": 111.39, "sample_count": 25, "duration_seconds": 3.285}, "timestamp": "2026-01-16T16:52:18.307058"}
{"image_index": 487, "image_name": "000000055002.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055002.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3646.735, "latencies_ms": [3646.735], "images_per_second": 0.274, "prompt_tokens": 21, "response_tokens_est": 63, "n_tiles": 12, "output_text": "The image depicts a bathroom setting with a toilet and a handrail. The toilet is situated on a tiled floor, and there is a handrail adjacent to it. The overall scene appears to be in a public restroom, possibly a public restroom or a restroom in a hotel.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25606.3, "ram_available_mb": 100165.9, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25606.5, "ram_available_mb": 100165.6, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.09, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 33.03, "peak": 42.54, "min": 26.39}, "VIN": {"avg": 77.16, "peak": 118.93, "min": 56.0}, "VDD_CPU_SOC_MSS": {"avg": 15.66, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 33.03, "energy_joules_est": 120.47, "sample_count": 28, "duration_seconds": 3.647}, "timestamp": "2026-01-16T16:52:21.961252"}
{"image_index": 487, "image_name": "000000055002.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055002.jpg", "image_width": 500, "image_height": 375, "image_resolution": "500x375", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2809.99, "latencies_ms": [2809.99], "images_per_second": 0.356, "prompt_tokens": 19, "response_tokens_est": 38, "n_tiles": 12, "output_text": "The image shows a metallic toilet with a blue brush holder attached to it. The floor is tiled with light-colored tiles, and the lighting is bright, illuminating the scene clearly.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25606.5, "ram_available_mb": 100165.6, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 2.9, "ram_used_mb": 25606.8, "ram_available_mb": 100165.4, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.09, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 35.24, "peak": 42.15, "min": 27.18}, "VIN": {"avg": 79.55, "peak": 115.41, "min": 58.59}, "VDD_CPU_SOC_MSS": {"avg": 15.71, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 35.24, "energy_joules_est": 99.04, "sample_count": 21, "duration_seconds": 2.81}, "timestamp": "2026-01-16T16:52:24.778145"}
{"image_index": 488, "image_name": "000000055022.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055022.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2121.595, "latencies_ms": [2121.595], "images_per_second": 0.471, "prompt_tokens": 9, "response_tokens_est": 21, "n_tiles": 12, "output_text": "A pink bicycle is parked in a room with other bicycles and a woman walking in the background.", "error": null, "sys_before": {"cpu_percent": 41.4, "ram_used_mb": 25606.5, "ram_available_mb": 100165.6, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 2.4, "ram_used_mb": 25606.8, "ram_available_mb": 100165.4, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.1, "peak": 16.15, "min": 13.69}, "VDD_GPU": {"avg": 36.06, "peak": 41.36, "min": 28.76}, "VIN": {"avg": 83.43, "peak": 121.63, "min": 56.04}, "VDD_CPU_SOC_MSS": {"avg": 15.94, "peak": 17.32, "min": 14.57}}, "power_watts_avg": 36.06, "energy_joules_est": 76.51, "sample_count": 16, "duration_seconds": 2.122}, "timestamp": "2026-01-16T16:52:27.004538"}
{"image_index": 488, "image_name": "000000055022.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055022.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 3119.297, "latencies_ms": [3119.297], "images_per_second": 0.321, "prompt_tokens": 23, "response_tokens_est": 47, "n_tiles": 12, "output_text": "1. Pink bicycle\n2. Bicycle rack\n3. Bicycle\n4. Bicycle wheel\n5. Bicycle seat\n6. Bicycle handlebars\n7. Bicycle saddle\n8. Bicycle chain", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25606.8, "ram_available_mb": 100165.4, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 25606.5, "ram_available_mb": 100165.6, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.06, "peak": 15.54, "min": 14.53}, "VDD_GPU": {"avg": 34.45, "peak": 43.71, "min": 26.39}, "VIN": {"avg": 73.6, "peak": 107.74, "min": 58.84}, "VDD_CPU_SOC_MSS": {"avg": 15.7, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 34.45, "energy_joules_est": 107.48, "sample_count": 24, "duration_seconds": 3.12}, "timestamp": "2026-01-16T16:52:30.130221"}
{"image_index": 488, "image_name": "000000055022.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055022.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 4769.607, "latencies_ms": [4769.607], "images_per_second": 0.21, "prompt_tokens": 27, "response_tokens_est": 96, "n_tiles": 12, "output_text": "The main objects in the image are a pink bicycle and a bicycle rack. The bicycle is positioned in the foreground, with its handlebars and seat clearly visible. The bicycle rack is located to the left of the bicycle, providing a stable support for the bike. In the background, there is a bicycle rack mounted on the wall, and a person is walking past it. The floor is wooden, and the overall setting appears to be a bike shop or a bike storage area.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25606.5, "ram_available_mb": 100165.6, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 25606.5, "ram_available_mb": 100165.7, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.1, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 31.44, "peak": 42.53, "min": 26.0}, "VIN": {"avg": 71.79, "peak": 116.1, "min": 58.97}, "VDD_CPU_SOC_MSS": {"avg": 15.73, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 31.44, "energy_joules_est": 149.97, "sample_count": 37, "duration_seconds": 4.77}, "timestamp": "2026-01-16T16:52:34.906308"}
{"image_index": 488, "image_name": "000000055022.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055022.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3849.264, "latencies_ms": [3849.264], "images_per_second": 0.26, "prompt_tokens": 21, "response_tokens_est": 69, "n_tiles": 12, "output_text": "The image depicts a well-lit indoor setting with a wooden floor and a white wall in the background. There are several bicycles parked in a row, with one prominently featuring a pink frame and a basket. A woman is seen walking past the bicycles, and there is a bicycle rack with a pink bicycle leaning against it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25606.5, "ram_available_mb": 100165.7, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25606.5, "ram_available_mb": 100165.7, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.1, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 32.75, "peak": 42.94, "min": 26.39}, "VIN": {"avg": 75.26, "peak": 122.14, "min": 58.97}, "VDD_CPU_SOC_MSS": {"avg": 15.69, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 32.75, "energy_joules_est": 126.07, "sample_count": 30, "duration_seconds": 3.85}, "timestamp": "2026-01-16T16:52:38.761650"}
{"image_index": 488, "image_name": "000000055022.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055022.jpg", "image_width": 480, "image_height": 640, "image_resolution": "480x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2873.299, "latencies_ms": [2873.299], "images_per_second": 0.348, "prompt_tokens": 19, "response_tokens_est": 40, "n_tiles": 12, "output_text": "The image features a pink bicycle with a basket, which is prominently displayed on a wooden floor. The lighting is bright, casting clear shadows, and the weather appears to be clear and sunny.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25606.5, "ram_available_mb": 100165.7, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 25606.8, "ram_available_mb": 100165.4, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.07, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 34.95, "peak": 42.15, "min": 26.79}, "VIN": {"avg": 74.69, "peak": 111.38, "min": 58.89}, "VDD_CPU_SOC_MSS": {"avg": 15.75, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 34.95, "energy_joules_est": 100.44, "sample_count": 22, "duration_seconds": 2.874}, "timestamp": "2026-01-16T16:52:41.641524"}
{"image_index": 489, "image_name": "000000055072.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055072.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1285.622, "latencies_ms": [1285.622], "images_per_second": 0.778, "prompt_tokens": 9, "response_tokens_est": 26, "n_tiles": 6, "output_text": "A giraffe stands in a savannah, its long neck stretching upwards, with a backdrop of sparse trees and dry grass.", "error": null, "sys_before": {"cpu_percent": 39.1, "ram_used_mb": 25606.8, "ram_available_mb": 100165.4, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.7, "ram_used_mb": 25607.0, "ram_available_mb": 100165.2, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.36, "peak": 14.91, "min": 13.82}, "VDD_GPU": {"avg": 32.34, "peak": 37.8, "min": 26.39}, "VIN": {"avg": 61.65, "peak": 73.99, "min": 51.82}, "VDD_CPU_SOC_MSS": {"avg": 15.08, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 32.34, "energy_joules_est": 41.59, "sample_count": 10, "duration_seconds": 1.286}, "timestamp": "2026-01-16T16:52:42.992409"}
{"image_index": 489, "image_name": "000000055072.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055072.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 979.828, "latencies_ms": [979.828], "images_per_second": 1.021, "prompt_tokens": 23, "response_tokens_est": 13, "n_tiles": 6, "output_text": "giraffe: 1\ntrees: 2", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25607.0, "ram_available_mb": 100165.2, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 25606.5, "ram_available_mb": 100165.7, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.58, "peak": 14.83, "min": 14.22}, "VDD_GPU": {"avg": 35.85, "peak": 38.6, "min": 31.51}, "VIN": {"avg": 75.77, "peak": 108.23, "min": 57.39}, "VDD_CPU_SOC_MSS": {"avg": 15.29, "peak": 15.74, "min": 14.96}}, "power_watts_avg": 35.85, "energy_joules_est": 35.14, "sample_count": 7, "duration_seconds": 0.98}, "timestamp": "2026-01-16T16:52:43.978070"}
{"image_index": 489, "image_name": "000000055072.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055072.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1741.272, "latencies_ms": [1741.272], "images_per_second": 0.574, "prompt_tokens": 27, "response_tokens_est": 40, "n_tiles": 6, "output_text": "The giraffe is positioned in the foreground, with its long neck and legs extending towards the background. The background features a mix of trees and shrubs, creating a natural and serene setting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25606.5, "ram_available_mb": 100165.7, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 25606.5, "ram_available_mb": 100165.7, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.59, "peak": 14.91, "min": 14.02}, "VDD_GPU": {"avg": 32.06, "peak": 40.18, "min": 24.42}, "VIN": {"avg": 70.14, "peak": 111.21, "min": 57.67}, "VDD_CPU_SOC_MSS": {"avg": 15.14, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 32.06, "energy_joules_est": 55.83, "sample_count": 13, "duration_seconds": 1.742}, "timestamp": "2026-01-16T16:52:45.725504"}
{"image_index": 489, "image_name": "000000055072.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055072.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2384.113, "latencies_ms": [2384.113], "images_per_second": 0.419, "prompt_tokens": 21, "response_tokens_est": 63, "n_tiles": 6, "output_text": "The image depicts a giraffe standing in a savannah-like environment, characterized by dry grass and scattered trees. The giraffe is facing to the right, with its long neck and distinctive spotted coat clearly visible. The setting suggests a natural habitat, likely in Africa, where giraffes are commonly found.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25606.5, "ram_available_mb": 100165.7, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 25606.7, "ram_available_mb": 100165.4, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.74, "peak": 15.01, "min": 14.02}, "VDD_GPU": {"avg": 29.08, "peak": 38.59, "min": 23.24}, "VIN": {"avg": 68.55, "peak": 115.97, "min": 50.65}, "VDD_CPU_SOC_MSS": {"avg": 15.27, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 29.08, "energy_joules_est": 69.34, "sample_count": 18, "duration_seconds": 2.384}, "timestamp": "2026-01-16T16:52:48.119400"}
{"image_index": 489, "image_name": "000000055072.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055072.jpg", "image_width": 640, "image_height": 427, "image_resolution": "640x427", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2380.836, "latencies_ms": [2380.836], "images_per_second": 0.42, "prompt_tokens": 19, "response_tokens_est": 63, "n_tiles": 6, "output_text": "The giraffe in the image has a distinctive pattern of brown spots on a lighter background, which is typical of its species. The lighting is natural, with a soft, diffused quality that suggests either early morning or late afternoon. The weather appears to be clear, with no visible signs of rain or storm.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25606.7, "ram_available_mb": 100165.4, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 25606.7, "ram_available_mb": 100165.4, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.73, "peak": 15.01, "min": 14.02}, "VDD_GPU": {"avg": 28.89, "peak": 37.82, "min": 23.25}, "VIN": {"avg": 67.97, "peak": 109.55, "min": 58.61}, "VDD_CPU_SOC_MSS": {"avg": 15.33, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 28.89, "energy_joules_est": 68.79, "sample_count": 18, "duration_seconds": 2.381}, "timestamp": "2026-01-16T16:52:50.510125"}
{"image_index": 490, "image_name": "000000055150.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055150.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1818.939, "latencies_ms": [1818.939], "images_per_second": 0.55, "prompt_tokens": 9, "response_tokens_est": 46, "n_tiles": 6, "output_text": "A young girl with red hair is sitting in a black suitcase, smiling as she looks at the camera, while a young boy sits in a blue suitcase beside her, both seemingly enjoying their time in the parking lot.", "error": null, "sys_before": {"cpu_percent": 43.4, "ram_used_mb": 25606.7, "ram_available_mb": 100165.4, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 25606.5, "ram_available_mb": 100165.7, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.57, "peak": 15.22, "min": 13.82}, "VDD_GPU": {"avg": 29.71, "peak": 37.03, "min": 24.42}, "VIN": {"avg": 71.31, "peak": 109.26, "min": 56.4}, "VDD_CPU_SOC_MSS": {"avg": 15.24, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 29.71, "energy_joules_est": 54.05, "sample_count": 14, "duration_seconds": 1.819}, "timestamp": "2026-01-16T16:52:52.390305"}
{"image_index": 490, "image_name": "000000055150.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055150.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1484.625, "latencies_ms": [1484.625], "images_per_second": 0.674, "prompt_tokens": 23, "response_tokens_est": 32, "n_tiles": 6, "output_text": "1. Car\n2. Car\n3. Car\n4. Car\n5. Car\n6. Car\n7. Car\n8. Car", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25606.5, "ram_available_mb": 100165.7, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 25606.5, "ram_available_mb": 100165.7, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.77, "peak": 15.01, "min": 14.32}, "VDD_GPU": {"avg": 32.55, "peak": 38.21, "min": 26.0}, "VIN": {"avg": 75.2, "peak": 102.31, "min": 62.53}, "VDD_CPU_SOC_MSS": {"avg": 15.32, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 32.55, "energy_joules_est": 48.34, "sample_count": 11, "duration_seconds": 1.485}, "timestamp": "2026-01-16T16:52:53.880826"}
{"image_index": 490, "image_name": "000000055150.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055150.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2714.688, "latencies_ms": [2714.688], "images_per_second": 0.368, "prompt_tokens": 27, "response_tokens_est": 78, "n_tiles": 6, "output_text": "The main objects in the image are a child in a car seat, a child in a car seat, and a car with a child in the car seat. The child in the car seat is positioned in the foreground, with the car and the child in the car seat being the main subjects. The car is parked in the background, and there are other cars parked in the distance.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25606.5, "ram_available_mb": 100165.7, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 25606.5, "ram_available_mb": 100165.7, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.97, "peak": 15.32, "min": 14.22}, "VDD_GPU": {"avg": 28.76, "peak": 38.6, "min": 23.64}, "VIN": {"avg": 68.76, "peak": 111.35, "min": 57.06}, "VDD_CPU_SOC_MSS": {"avg": 15.49, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 28.76, "energy_joules_est": 78.09, "sample_count": 21, "duration_seconds": 2.715}, "timestamp": "2026-01-16T16:52:56.601522"}
{"image_index": 490, "image_name": "000000055150.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055150.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2442.072, "latencies_ms": [2442.072], "images_per_second": 0.409, "prompt_tokens": 21, "response_tokens_est": 68, "n_tiles": 6, "output_text": "The image depicts a busy parking lot scene with several cars parked in a line. A young girl with red hair is sitting in a black car's rear passenger seat, holding a piece of paper. The setting appears to be a commercial area, possibly near a shopping center, with various cars and a Budget store visible in the background.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25606.5, "ram_available_mb": 100165.7, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 25606.7, "ram_available_mb": 100165.5, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.98, "peak": 15.32, "min": 14.32}, "VDD_GPU": {"avg": 29.28, "peak": 38.19, "min": 24.03}, "VIN": {"avg": 73.95, "peak": 110.77, "min": 58.18}, "VDD_CPU_SOC_MSS": {"avg": 15.51, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 29.28, "energy_joules_est": 71.51, "sample_count": 18, "duration_seconds": 2.442}, "timestamp": "2026-01-16T16:52:59.051086"}
{"image_index": 490, "image_name": "000000055150.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055150.jpg", "image_width": 640, "image_height": 426, "image_resolution": "640x426", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2458.165, "latencies_ms": [2458.165], "images_per_second": 0.407, "prompt_tokens": 19, "response_tokens_est": 66, "n_tiles": 6, "output_text": "The image depicts a scene with a wet parking lot, reflecting the recent rain. The lighting is soft and diffused, likely due to overcast weather. The colors are muted, with the predominant hues being shades of gray and blue, accentuated by the wet surfaces and the reflections of the parked cars.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25606.7, "ram_available_mb": 100165.5, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 25606.5, "ram_available_mb": 100165.7, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.84, "peak": 15.01, "min": 14.32}, "VDD_GPU": {"avg": 28.74, "peak": 38.21, "min": 23.24}, "VIN": {"avg": 66.65, "peak": 124.44, "min": 54.31}, "VDD_CPU_SOC_MSS": {"avg": 15.37, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 28.74, "energy_joules_est": 70.66, "sample_count": 19, "duration_seconds": 2.459}, "timestamp": "2026-01-16T16:53:01.515272"}
{"image_index": 491, "image_name": "000000055167.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055167.jpg", "image_width": 640, "image_height": 360, "image_resolution": "640x360", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 713.759, "latencies_ms": [713.759], "images_per_second": 1.401, "prompt_tokens": 9, "response_tokens_est": 24, "n_tiles": 2, "output_text": "A yellow and blue bus is driving down a street lined with houses and trees, with a few people walking nearby.", "error": null, "sys_before": {"cpu_percent": 35.9, "ram_used_mb": 25606.5, "ram_available_mb": 100165.7, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25606.5, "ram_available_mb": 100165.7, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5055.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.42, "peak": 14.91, "min": 14.0}, "VDD_GPU": {"avg": 27.42, "peak": 31.91, "min": 24.04}, "VIN": {"avg": 66.9, "peak": 74.96, "min": 63.02}, "VDD_CPU_SOC_MSS": {"avg": 15.12, "peak": 15.74, "min": 14.57}}, "power_watts_avg": 27.42, "energy_joules_est": 19.58, "sample_count": 5, "duration_seconds": 0.714}, "timestamp": "2026-01-16T16:53:02.263425"}
{"image_index": 491, "image_name": "000000055167.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055167.jpg", "image_width": 640, "image_height": 360, "image_resolution": "640x360", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1038.56, "latencies_ms": [1038.56], "images_per_second": 0.963, "prompt_tokens": 23, "response_tokens_est": 38, "n_tiles": 2, "output_text": "bus: 1\nvan: 1\nhouse: 2\ntree: 1\nstreet light: 1\nbench: 1\ngarbage can: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25606.4, "ram_available_mb": 100165.7, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 5.2, "ram_used_mb": 25606.7, "ram_available_mb": 100165.5, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5064.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.89, "peak": 15.32, "min": 14.3}, "VDD_GPU": {"avg": 26.3, "peak": 32.7, "min": 22.46}, "VIN": {"avg": 67.78, "peak": 104.28, "min": 60.61}, "VDD_CPU_SOC_MSS": {"avg": 15.5, "peak": 16.14, "min": 14.96}}, "power_watts_avg": 26.3, "energy_joules_est": 27.33, "sample_count": 8, "duration_seconds": 1.039}, "timestamp": "2026-01-16T16:53:03.308342"}
{"image_index": 491, "image_name": "000000055167.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055167.jpg", "image_width": 640, "image_height": 360, "image_resolution": "640x360", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1782.994, "latencies_ms": [1782.994], "images_per_second": 0.561, "prompt_tokens": 27, "response_tokens_est": 71, "n_tiles": 2, "output_text": "The main objects in the image are a yellow and blue bus, a white van, and a small white building. The bus is in the foreground, slightly to the right, while the van is in the background, closer to the left side of the image. The small white building is near the van, closer to the right side of the image.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25606.7, "ram_available_mb": 100165.5, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25606.7, "ram_available_mb": 100165.5, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5067.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.16, "peak": 15.52, "min": 14.61}, "VDD_GPU": {"avg": 24.03, "peak": 31.51, "min": 20.88}, "VIN": {"avg": 65.62, "peak": 98.3, "min": 58.17}, "VDD_CPU_SOC_MSS": {"avg": 15.93, "peak": 16.54, "min": 14.96}}, "power_watts_avg": 24.03, "energy_joules_est": 42.86, "sample_count": 13, "duration_seconds": 1.783}, "timestamp": "2026-01-16T16:53:05.101223"}
{"image_index": 491, "image_name": "000000055167.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055167.jpg", "image_width": 640, "image_height": 360, "image_resolution": "640x360", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1535.173, "latencies_ms": [1535.173], "images_per_second": 0.651, "prompt_tokens": 21, "response_tokens_est": 60, "n_tiles": 2, "output_text": "The image depicts a vibrant scene of a bus in motion on a road, surrounded by a quaint, residential area with houses and trees. The setting appears to be in a rural or suburban area, with a well-maintained green lawn and a variety of colorful flowers in the foreground.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25606.7, "ram_available_mb": 100165.5, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 5.1, "ram_used_mb": 25606.4, "ram_available_mb": 100165.7, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5063.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.07, "peak": 15.52, "min": 14.51}, "VDD_GPU": {"avg": 24.36, "peak": 31.51, "min": 21.28}, "VIN": {"avg": 65.16, "peak": 75.94, "min": 58.98}, "VDD_CPU_SOC_MSS": {"avg": 15.75, "peak": 16.14, "min": 14.96}}, "power_watts_avg": 24.36, "energy_joules_est": 37.4, "sample_count": 11, "duration_seconds": 1.535}, "timestamp": "2026-01-16T16:53:06.642863"}
{"image_index": 491, "image_name": "000000055167.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055167.jpg", "image_width": 640, "image_height": 360, "image_resolution": "640x360", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1460.703, "latencies_ms": [1460.703], "images_per_second": 0.685, "prompt_tokens": 19, "response_tokens_est": 57, "n_tiles": 2, "output_text": "The image features a vibrant yellow and blue bus with the word \"CIVILINK\" on its side, driving on a road lined with green grass and a backdrop of a hilly landscape. The sky is overcast, casting a soft, diffused light over the scene.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25606.4, "ram_available_mb": 100165.7, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 5.4, "ram_used_mb": 25606.4, "ram_available_mb": 100165.7, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4604.4, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5062.6, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.05, "peak": 15.42, "min": 14.51}, "VDD_GPU": {"avg": 24.46, "peak": 31.51, "min": 21.28}, "VIN": {"avg": 66.57, "peak": 87.38, "min": 61.81}, "VDD_CPU_SOC_MSS": {"avg": 15.85, "peak": 16.54, "min": 15.35}}, "power_watts_avg": 24.46, "energy_joules_est": 35.74, "sample_count": 11, "duration_seconds": 1.461}, "timestamp": "2026-01-16T16:53:08.109401"}
{"image_index": 492, "image_name": "000000055299.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055299.jpg", "image_width": 640, "image_height": 429, "image_resolution": "640x429", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1336.374, "latencies_ms": [1336.374], "images_per_second": 0.748, "prompt_tokens": 9, "response_tokens_est": 28, "n_tiles": 6, "output_text": "A solitary pelican perches on a rocky outcrop overlooking a tranquil beach and calm sea, with a cloudy sky overhead.", "error": null, "sys_before": {"cpu_percent": 42.0, "ram_used_mb": 25606.4, "ram_available_mb": 100165.7, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.6, "ram_used_mb": 25606.4, "ram_available_mb": 100165.7, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.49, "peak": 14.91, "min": 13.92}, "VDD_GPU": {"avg": 31.39, "peak": 36.63, "min": 26.0}, "VIN": {"avg": 74.25, "peak": 113.14, "min": 61.96}, "VDD_CPU_SOC_MSS": {"avg": 15.23, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 31.39, "energy_joules_est": 41.96, "sample_count": 10, "duration_seconds": 1.337}, "timestamp": "2026-01-16T16:53:09.500314"}
{"image_index": 492, "image_name": "000000055299.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055299.jpg", "image_width": 640, "image_height": 429, "image_resolution": "640x429", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1702.967, "latencies_ms": [1702.967], "images_per_second": 0.587, "prompt_tokens": 23, "response_tokens_est": 40, "n_tiles": 6, "output_text": "object: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1\nobject: 1", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25606.4, "ram_available_mb": 100165.7, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 25606.4, "ram_available_mb": 100165.7, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.78, "peak": 15.11, "min": 14.22}, "VDD_GPU": {"avg": 31.45, "peak": 38.59, "min": 24.82}, "VIN": {"avg": 69.53, "peak": 122.79, "min": 56.64}, "VDD_CPU_SOC_MSS": {"avg": 15.41, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 31.45, "energy_joules_est": 53.57, "sample_count": 13, "duration_seconds": 1.703}, "timestamp": "2026-01-16T16:53:11.209075"}
{"image_index": 492, "image_name": "000000055299.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055299.jpg", "image_width": 640, "image_height": 429, "image_resolution": "640x429", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2612.034, "latencies_ms": [2612.034], "images_per_second": 0.383, "prompt_tokens": 27, "response_tokens_est": 71, "n_tiles": 6, "output_text": "The main objects in the image are a rocky outcrop, a seagull, and the ocean. The seagull is positioned on the rocky outcrop, which is in the foreground. The ocean is in the background, stretching out to meet the horizon. The rocky outcrop is near the seagull, providing a perch for it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25606.4, "ram_available_mb": 100165.7, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.4, "ram_used_mb": 25606.7, "ram_available_mb": 100165.5, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.78, "peak": 15.01, "min": 14.12}, "VDD_GPU": {"avg": 28.26, "peak": 38.59, "min": 23.24}, "VIN": {"avg": 72.93, "peak": 120.44, "min": 59.58}, "VDD_CPU_SOC_MSS": {"avg": 15.35, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 28.26, "energy_joules_est": 73.82, "sample_count": 20, "duration_seconds": 2.612}, "timestamp": "2026-01-16T16:53:13.827432"}
{"image_index": 492, "image_name": "000000055299.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055299.jpg", "image_width": 640, "image_height": 429, "image_resolution": "640x429", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2133.279, "latencies_ms": [2133.279], "images_per_second": 0.469, "prompt_tokens": 21, "response_tokens_est": 54, "n_tiles": 6, "output_text": "The image depicts a serene beach scene with a rocky shoreline in the foreground. A solitary bird is perched on a rock, overlooking the calm sea and distant mountains. The sky is filled with clouds, suggesting a tranquil and possibly overcast day.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25606.7, "ram_available_mb": 100165.5, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 25606.4, "ram_available_mb": 100165.8, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.72, "peak": 15.01, "min": 14.12}, "VDD_GPU": {"avg": 29.37, "peak": 38.19, "min": 23.64}, "VIN": {"avg": 74.65, "peak": 106.86, "min": 61.62}, "VDD_CPU_SOC_MSS": {"avg": 15.26, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 29.37, "energy_joules_est": 62.67, "sample_count": 16, "duration_seconds": 2.134}, "timestamp": "2026-01-16T16:53:15.966628"}
{"image_index": 492, "image_name": "000000055299.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055299.jpg", "image_width": 640, "image_height": 429, "image_resolution": "640x429", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 1797.644, "latencies_ms": [1797.644], "images_per_second": 0.556, "prompt_tokens": 19, "response_tokens_est": 42, "n_tiles": 6, "output_text": "The image depicts a serene beach scene with a rocky foreground and a calm sea in the background. The sky is partly cloudy, with sunlight filtering through, creating a soft, diffused light.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25606.4, "ram_available_mb": 100165.8, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.1, "ram_used_mb": 25606.4, "ram_available_mb": 100165.8, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.64, "peak": 14.91, "min": 14.02}, "VDD_GPU": {"avg": 30.66, "peak": 37.8, "min": 24.03}, "VIN": {"avg": 73.2, "peak": 119.39, "min": 54.68}, "VDD_CPU_SOC_MSS": {"avg": 15.17, "peak": 15.36, "min": 14.56}}, "power_watts_avg": 30.66, "energy_joules_est": 55.13, "sample_count": 13, "duration_seconds": 1.798}, "timestamp": "2026-01-16T16:53:17.770164"}
{"image_index": 493, "image_name": "000000055528.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055528.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2347.364, "latencies_ms": [2347.364], "images_per_second": 0.426, "prompt_tokens": 9, "response_tokens_est": 28, "n_tiles": 12, "output_text": "A young man is sitting on a couch, holding a piece of paper in his hand, and wearing a watch on his left wrist.", "error": null, "sys_before": {"cpu_percent": 47.9, "ram_used_mb": 25606.4, "ram_available_mb": 100165.8, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 25606.7, "ram_available_mb": 100165.5, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.1, "peak": 16.05, "min": 13.79}, "VDD_GPU": {"avg": 35.17, "peak": 40.97, "min": 27.97}, "VIN": {"avg": 76.19, "peak": 108.17, "min": 58.35}, "VDD_CPU_SOC_MSS": {"avg": 15.81, "peak": 16.92, "min": 14.96}}, "power_watts_avg": 35.17, "energy_joules_est": 82.57, "sample_count": 18, "duration_seconds": 2.348}, "timestamp": "2026-01-16T16:53:20.207212"}
{"image_index": 493, "image_name": "000000055528.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055528.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 5848.599, "latencies_ms": [5848.599], "images_per_second": 0.171, "prompt_tokens": 23, "response_tokens_est": 129, "n_tiles": 12, "output_text": "- remote control: 1\n- remote control: 1\n- remote control: 1\n- remote control: 1\n- remote control: 1\n- remote control: 1\n- remote control: 1\n- remote control: 1\n- remote control: 1\n- remote control: 1\n- remote control: 1\n- remote control: 1\n- remote control: 1\n- remote control: 1\n- remote control: 1\n- remote control: 1\n- remote control: 1\n- remote control: 1\n- remote", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25606.7, "ram_available_mb": 100165.5, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 25606.4, "ram_available_mb": 100165.8, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.11, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 30.53, "peak": 43.33, "min": 25.99}, "VIN": {"avg": 71.06, "peak": 111.36, "min": 57.11}, "VDD_CPU_SOC_MSS": {"avg": 15.7, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 30.53, "energy_joules_est": 178.57, "sample_count": 46, "duration_seconds": 5.849}, "timestamp": "2026-01-16T16:53:26.062322"}
{"image_index": 493, "image_name": "000000055528.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055528.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2893.512, "latencies_ms": [2893.512], "images_per_second": 0.346, "prompt_tokens": 27, "response_tokens_est": 41, "n_tiles": 12, "output_text": "The main object in the foreground is a person sitting on a couch. The person is holding a piece of paper in their right hand. In the background, there is a remote control on the couch.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25606.4, "ram_available_mb": 100165.8, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 25606.4, "ram_available_mb": 100165.8, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.12, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 35.0, "peak": 42.94, "min": 27.18}, "VIN": {"avg": 75.28, "peak": 117.41, "min": 58.43}, "VDD_CPU_SOC_MSS": {"avg": 15.76, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 35.0, "energy_joules_est": 101.29, "sample_count": 22, "duration_seconds": 2.894}, "timestamp": "2026-01-16T16:53:28.961819"}
{"image_index": 493, "image_name": "000000055528.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055528.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3776.542, "latencies_ms": [3776.542], "images_per_second": 0.265, "prompt_tokens": 21, "response_tokens_est": 68, "n_tiles": 12, "output_text": "The image depicts a young man sitting on a couch, holding a piece of paper in his hand. He is wearing glasses and a plaid shirt, and there is a remote control and a piece of paper on the couch next to him. The setting appears to be indoors, possibly in a living room or a casual home environment.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25606.4, "ram_available_mb": 100165.8, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25606.4, "ram_available_mb": 100165.8, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.16, "peak": 15.65, "min": 14.63}, "VDD_GPU": {"avg": 33.16, "peak": 43.33, "min": 26.4}, "VIN": {"avg": 72.1, "peak": 113.27, "min": 58.61}, "VDD_CPU_SOC_MSS": {"avg": 15.8, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 33.16, "energy_joules_est": 125.24, "sample_count": 29, "duration_seconds": 3.777}, "timestamp": "2026-01-16T16:53:32.744865"}
{"image_index": 493, "image_name": "000000055528.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055528.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2561.025, "latencies_ms": [2561.025], "images_per_second": 0.39, "prompt_tokens": 19, "response_tokens_est": 31, "n_tiles": 12, "output_text": "The image features a person sitting on a couch with a gray and black checkered blanket. The lighting is soft and warm, creating a cozy atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25606.4, "ram_available_mb": 100165.8, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 25606.4, "ram_available_mb": 100165.8, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.11, "peak": 15.65, "min": 14.63}, "VDD_GPU": {"avg": 35.9, "peak": 42.94, "min": 27.57}, "VIN": {"avg": 74.03, "peak": 120.68, "min": 57.99}, "VDD_CPU_SOC_MSS": {"avg": 15.78, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 35.9, "energy_joules_est": 91.96, "sample_count": 20, "duration_seconds": 2.562}, "timestamp": "2026-01-16T16:53:35.312487"}
{"image_index": 494, "image_name": "000000055950.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055950.jpg", "image_width": 426, "image_height": 640, "image_resolution": "426x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1589.455, "latencies_ms": [1589.455], "images_per_second": 0.629, "prompt_tokens": 9, "response_tokens_est": 38, "n_tiles": 6, "output_text": "A tennis player is poised to hit a tennis ball on a court, with his racket ready and his body slightly bent forward, indicating he is in the midst of a powerful serve.", "error": null, "sys_before": {"cpu_percent": 26.4, "ram_used_mb": 25606.4, "ram_available_mb": 100165.8, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25606.4, "ram_available_mb": 100165.8, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5866.0, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.62, "peak": 15.22, "min": 13.92}, "VDD_GPU": {"avg": 31.77, "peak": 38.19, "min": 25.21}, "VIN": {"avg": 71.6, "peak": 107.96, "min": 50.83}, "VDD_CPU_SOC_MSS": {"avg": 15.25, "peak": 15.75, "min": 14.56}}, "power_watts_avg": 31.77, "energy_joules_est": 50.51, "sample_count": 12, "duration_seconds": 1.59}, "timestamp": "2026-01-16T16:53:36.942874"}
{"image_index": 494, "image_name": "000000055950.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055950.jpg", "image_width": 426, "image_height": 640, "image_resolution": "426x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1680.404, "latencies_ms": [1680.404], "images_per_second": 0.595, "prompt_tokens": 23, "response_tokens_est": 40, "n_tiles": 6, "output_text": "1. Tennis player\n2. Tennis racket\n3. Tennis ball\n4. Tennis court\n5. Net\n6. Tennis shoes\n7. Tennis shirt\n8. Tennis hat", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25606.4, "ram_available_mb": 100165.8, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 25606.4, "ram_available_mb": 100165.8, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5876.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.91, "peak": 15.32, "min": 14.32}, "VDD_GPU": {"avg": 31.51, "peak": 38.21, "min": 25.21}, "VIN": {"avg": 66.55, "peak": 88.88, "min": 55.79}, "VDD_CPU_SOC_MSS": {"avg": 15.44, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 31.51, "energy_joules_est": 52.96, "sample_count": 13, "duration_seconds": 1.681}, "timestamp": "2026-01-16T16:53:38.629150"}
{"image_index": 494, "image_name": "000000055950.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055950.jpg", "image_width": 426, "image_height": 640, "image_resolution": "426x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1839.858, "latencies_ms": [1839.858], "images_per_second": 0.544, "prompt_tokens": 27, "response_tokens_est": 46, "n_tiles": 6, "output_text": "The main object in the foreground is a tennis player holding a tennis racket. The player is positioned near the net, which is in the foreground. The background features a tennis court with a green surface and white boundary lines.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25606.4, "ram_available_mb": 100165.8, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.2, "ram_used_mb": 25606.4, "ram_available_mb": 100165.8, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5878.1, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.96, "peak": 15.32, "min": 14.32}, "VDD_GPU": {"avg": 31.03, "peak": 38.6, "min": 24.82}, "VIN": {"avg": 74.44, "peak": 112.47, "min": 55.72}, "VDD_CPU_SOC_MSS": {"avg": 15.58, "peak": 16.14, "min": 14.96}}, "power_watts_avg": 31.03, "energy_joules_est": 57.1, "sample_count": 14, "duration_seconds": 1.84}, "timestamp": "2026-01-16T16:53:40.476055"}
{"image_index": 494, "image_name": "000000055950.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055950.jpg", "image_width": 426, "image_height": 640, "image_resolution": "426x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 2391.469, "latencies_ms": [2391.469], "images_per_second": 0.418, "prompt_tokens": 21, "response_tokens_est": 67, "n_tiles": 6, "output_text": "The image captures a moment on an outdoor tennis court during daylight. A male tennis player, dressed in an orange shirt and black shorts, is in the process of hitting a tennis ball. The court is marked with white lines, and the player is positioned near the net, indicating he is in the middle of a serve or return.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25606.4, "ram_available_mb": 100165.8, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 25606.6, "ram_available_mb": 100165.5, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5873.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.04, "peak": 15.42, "min": 14.32}, "VDD_GPU": {"avg": 29.43, "peak": 38.19, "min": 24.03}, "VIN": {"avg": 69.46, "peak": 101.37, "min": 62.17}, "VDD_CPU_SOC_MSS": {"avg": 15.55, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 29.43, "energy_joules_est": 70.39, "sample_count": 18, "duration_seconds": 2.392}, "timestamp": "2026-01-16T16:53:42.873397"}
{"image_index": 494, "image_name": "000000055950.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000055950.jpg", "image_width": 426, "image_height": 640, "image_resolution": "426x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2049.707, "latencies_ms": [2049.707], "images_per_second": 0.488, "prompt_tokens": 19, "response_tokens_est": 54, "n_tiles": 6, "output_text": "The image shows a tennis player in an orange shirt and black shorts, holding a tennis racket, standing on a green tennis court. The lighting is bright, indicating it is daytime, and the court surface appears to be well-maintained with white boundary lines.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25606.6, "ram_available_mb": 100165.5, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 25606.6, "ram_available_mb": 100165.5, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4608.9, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 5872.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.97, "peak": 15.32, "min": 14.32}, "VDD_GPU": {"avg": 30.41, "peak": 38.19, "min": 24.42}, "VIN": {"avg": 71.54, "peak": 114.54, "min": 57.29}, "VDD_CPU_SOC_MSS": {"avg": 15.54, "peak": 15.75, "min": 14.96}}, "power_watts_avg": 30.41, "energy_joules_est": 62.34, "sample_count": 15, "duration_seconds": 2.05}, "timestamp": "2026-01-16T16:53:44.933012"}
{"image_index": 495, "image_name": "000000056127.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000056127.jpg", "image_width": 580, "image_height": 640, "image_resolution": "580x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 1482.034, "latencies_ms": [1482.034], "images_per_second": 0.675, "prompt_tokens": 9, "response_tokens_est": 66, "n_tiles": 1, "output_text": "The image depicts a well-organized and functional kitchen area with a yellowish hue, featuring a combination of wooden cabinets, a black mat, a refrigerator, a stove, a sink, and a counter with a sink and faucet, all set against a backdrop of wooden beams and a ceiling with fluorescent lighting.", "error": null, "sys_before": {"cpu_percent": 8.3, "ram_used_mb": 25606.6, "ram_available_mb": 100165.5, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25606.6, "ram_available_mb": 100165.6, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 4845.7, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.51, "peak": 15.82, "min": 14.71}, "VDD_GPU": {"avg": 22.21, "peak": 26.4, "min": 20.48}, "VIN": {"avg": 64.11, "peak": 65.96, "min": 61.22}, "VDD_CPU_SOC_MSS": {"avg": 16.28, "peak": 16.93, "min": 15.35}}, "power_watts_avg": 22.21, "energy_joules_est": 32.93, "sample_count": 11, "duration_seconds": 1.482}, "timestamp": "2026-01-16T16:53:46.437045"}
{"image_index": 495, "image_name": "000000056127.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000056127.jpg", "image_width": 580, "image_height": 640, "image_resolution": "580x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2805.664, "latencies_ms": [2805.664], "images_per_second": 0.356, "prompt_tokens": 23, "response_tokens_est": 128, "n_tiles": 1, "output_text": "- table: 1\n- chair: 1\n- stove: 1\n- sink: 1\n- refrigerator: 1\n- cabinet: 1\n- shelf: 1\n- drawer: 1\n- faucet: 1\n- tablecloth: 1\n- rug: 1\n- light fixture: 1\n- door: 1\n- window: 1\n- vent: 1\n- ceiling: 1\n- pipe: 1\n- cabinet door: 1\n- drawer front: 1\n- cabinet door handle: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25606.6, "ram_available_mb": 100165.6, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25606.4, "ram_available_mb": 100165.8, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 4856.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.76, "peak": 15.93, "min": 15.42}, "VDD_GPU": {"avg": 20.86, "peak": 24.42, "min": 20.09}, "VIN": {"avg": 63.62, "peak": 72.54, "min": 57.27}, "VDD_CPU_SOC_MSS": {"avg": 16.81, "peak": 16.93, "min": 16.14}}, "power_watts_avg": 20.86, "energy_joules_est": 58.53, "sample_count": 22, "duration_seconds": 2.806}, "timestamp": "2026-01-16T16:53:49.248171"}
{"image_index": 495, "image_name": "000000056127.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000056127.jpg", "image_width": 580, "image_height": 640, "image_resolution": "580x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 2813.802, "latencies_ms": [2813.802], "images_per_second": 0.355, "prompt_tokens": 27, "response_tokens_est": 127, "n_tiles": 1, "output_text": "The main objects in the image are located in the foreground, with the kitchen area being the most prominent. The kitchen features a wooden countertop, a sink, a stove, and various cabinets. The stove is positioned to the left of the sink, and the countertop extends towards the right. The sink is located near the stove, and the cabinets are placed on either side of the countertop. The background includes a wooden door and a wooden chair, while the far right side of the image shows a wooden desk with a chair. The ceiling is painted in a light yellow color, and there are various pipes and fixtures visible.", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25606.4, "ram_available_mb": 100165.8, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25606.4, "ram_available_mb": 100165.8, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 4859.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.63, "peak": 15.72, "min": 15.22}, "VDD_GPU": {"avg": 20.74, "peak": 24.42, "min": 20.09}, "VIN": {"avg": 63.88, "peak": 69.49, "min": 57.93}, "VDD_CPU_SOC_MSS": {"avg": 16.54, "peak": 16.54, "min": 16.53}}, "power_watts_avg": 20.74, "energy_joules_est": 58.36, "sample_count": 22, "duration_seconds": 2.814}, "timestamp": "2026-01-16T16:53:52.067654"}
{"image_index": 495, "image_name": "000000056127.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000056127.jpg", "image_width": 580, "image_height": 640, "image_resolution": "580x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 1940.509, "latencies_ms": [1940.509], "images_per_second": 0.515, "prompt_tokens": 21, "response_tokens_est": 87, "n_tiles": 1, "output_text": "The image depicts the interior of a small, well-organized kitchen aboard a ship. The kitchen features wooden cabinets with glass doors, a black countertop, a sink with a faucet, a stove with a pot on it, and a refrigerator. The walls are painted in a light yellow color, and there are various kitchen utensils and appliances neatly arranged. The ceiling has wooden beams and fluorescent lights.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25606.4, "ram_available_mb": 100165.8, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 5.8, "ram_used_mb": 25606.4, "ram_available_mb": 100165.8, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 4854.7, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.63, "peak": 15.82, "min": 15.22}, "VDD_GPU": {"avg": 21.04, "peak": 24.43, "min": 20.09}, "VIN": {"avg": 63.33, "peak": 69.42, "min": 60.64}, "VDD_CPU_SOC_MSS": {"avg": 16.48, "peak": 16.93, "min": 16.14}}, "power_watts_avg": 21.04, "energy_joules_est": 40.84, "sample_count": 15, "duration_seconds": 1.941}, "timestamp": "2026-01-16T16:53:54.013969"}
{"image_index": 495, "image_name": "000000056127.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000056127.jpg", "image_width": 580, "image_height": 640, "image_resolution": "580x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 927.259, "latencies_ms": [927.259], "images_per_second": 1.078, "prompt_tokens": 19, "response_tokens_est": 40, "n_tiles": 1, "output_text": "The image depicts a well-lit, yellowish kitchen with wooden cabinets and a wooden countertop. The lighting is primarily from fluorescent lights, and the overall atmosphere is warm and inviting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25606.4, "ram_available_mb": 100165.8, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 5.6, "ram_used_mb": 25606.4, "ram_available_mb": 100165.8, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 4853.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.5, "peak": 15.72, "min": 15.22}, "VDD_GPU": {"avg": 22.13, "peak": 24.04, "min": 20.88}, "VIN": {"avg": 63.66, "peak": 65.15, "min": 62.69}, "VDD_CPU_SOC_MSS": {"avg": 16.34, "peak": 16.54, "min": 16.14}}, "power_watts_avg": 22.13, "energy_joules_est": 20.53, "sample_count": 6, "duration_seconds": 0.927}, "timestamp": "2026-01-16T16:53:54.946900"}
{"image_index": 496, "image_name": "000000056288.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000056288.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2678.618, "latencies_ms": [2678.618], "images_per_second": 0.373, "prompt_tokens": 9, "response_tokens_est": 38, "n_tiles": 12, "output_text": "The image shows a sandwich with various ingredients, including lettuce, tomato, cucumber, and ham, all placed on a white paper plate, which is placed on a wooden table.", "error": null, "sys_before": {"cpu_percent": 50.6, "ram_used_mb": 25606.4, "ram_available_mb": 100165.8, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.8, "ram_used_mb": 25606.6, "ram_available_mb": 100165.6, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.29, "peak": 16.15, "min": 14.81}, "VDD_GPU": {"avg": 34.03, "peak": 40.57, "min": 26.79}, "VIN": {"avg": 74.28, "peak": 117.67, "min": 58.18}, "VDD_CPU_SOC_MSS": {"avg": 16.12, "peak": 17.32, "min": 15.35}}, "power_watts_avg": 34.03, "energy_joules_est": 91.16, "sample_count": 20, "duration_seconds": 2.679}, "timestamp": "2026-01-16T16:53:57.689947"}
{"image_index": 496, "image_name": "000000056288.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000056288.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2481.216, "latencies_ms": [2481.216], "images_per_second": 0.403, "prompt_tokens": 23, "response_tokens_est": 28, "n_tiles": 12, "output_text": "- sandwich\n- lettuce\n- tomato\n- cucumber\n- carrot\n- pickles\n- onion\n- ham", "error": null, "sys_before": {"cpu_percent": 9.1, "ram_used_mb": 25606.6, "ram_available_mb": 100165.6, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25605.6, "ram_available_mb": 100166.6, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.04, "peak": 15.54, "min": 14.43}, "VDD_GPU": {"avg": 36.18, "peak": 42.94, "min": 27.97}, "VIN": {"avg": 74.24, "peak": 113.02, "min": 58.65}, "VDD_CPU_SOC_MSS": {"avg": 15.72, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 36.18, "energy_joules_est": 89.79, "sample_count": 19, "duration_seconds": 2.482}, "timestamp": "2026-01-16T16:54:00.183989"}
{"image_index": 496, "image_name": "000000056288.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000056288.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3677.899, "latencies_ms": [3677.899], "images_per_second": 0.272, "prompt_tokens": 27, "response_tokens_est": 63, "n_tiles": 12, "output_text": "The sandwich is placed in the foreground, with lettuce, tomato, and cucumber slices on a white paper plate. The sandwich is positioned near the center of the image, with a laptop in the background. The laptop is slightly out of focus, indicating that the sandwich is the main subject of the image.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25605.6, "ram_available_mb": 100166.6, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.0, "ram_used_mb": 25605.6, "ram_available_mb": 100166.6, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.75, "peak": 15.34, "min": 14.12}, "VDD_GPU": {"avg": 32.98, "peak": 42.54, "min": 26.0}, "VIN": {"avg": 75.97, "peak": 124.45, "min": 54.62}, "VDD_CPU_SOC_MSS": {"avg": 15.31, "peak": 16.14, "min": 14.56}}, "power_watts_avg": 32.98, "energy_joules_est": 121.31, "sample_count": 29, "duration_seconds": 3.678}, "timestamp": "2026-01-16T16:54:03.868591"}
{"image_index": 496, "image_name": "000000056288.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000056288.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 4257.755, "latencies_ms": [4257.755], "images_per_second": 0.235, "prompt_tokens": 21, "response_tokens_est": 80, "n_tiles": 12, "output_text": "The image depicts a sandwich placed on a white paper plate, which is situated on a wooden table. The sandwich is filled with various ingredients, including lettuce, tomato, cucumber, and ham, and is accompanied by a partially eaten piece of bread. In the background, there is a computer monitor displaying some text, suggesting that the setting might be a home office or a study area.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25605.6, "ram_available_mb": 100166.6, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 4.3, "ram_used_mb": 25605.6, "ram_available_mb": 100166.6, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.75, "peak": 15.34, "min": 14.12}, "VDD_GPU": {"avg": 31.75, "peak": 42.53, "min": 25.6}, "VIN": {"avg": 71.88, "peak": 118.73, "min": 58.19}, "VDD_CPU_SOC_MSS": {"avg": 15.27, "peak": 16.14, "min": 14.56}}, "power_watts_avg": 31.75, "energy_joules_est": 135.19, "sample_count": 33, "duration_seconds": 4.258}, "timestamp": "2026-01-16T16:54:08.132865"}
{"image_index": 496, "image_name": "000000056288.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000056288.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3397.767, "latencies_ms": [3397.767], "images_per_second": 0.294, "prompt_tokens": 19, "response_tokens_est": 55, "n_tiles": 12, "output_text": "The image shows a sandwich with a variety of ingredients, including lettuce, tomato, cucumber, and ham, all neatly arranged on a white paper plate. The lighting is bright, and the background includes a computer monitor and a television, suggesting an indoor setting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25605.3, "ram_available_mb": 100166.8, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.4, "ram_used_mb": 25605.3, "ram_available_mb": 100166.8, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 14.73, "peak": 15.34, "min": 14.12}, "VDD_GPU": {"avg": 33.31, "peak": 42.54, "min": 26.0}, "VIN": {"avg": 72.2, "peak": 110.75, "min": 58.8}, "VDD_CPU_SOC_MSS": {"avg": 15.24, "peak": 16.14, "min": 14.56}}, "power_watts_avg": 33.31, "energy_joules_est": 113.19, "sample_count": 27, "duration_seconds": 3.398}, "timestamp": "2026-01-16T16:54:11.537023"}
{"image_index": 497, "image_name": "000000056344.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000056344.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2675.766, "latencies_ms": [2675.766], "images_per_second": 0.374, "prompt_tokens": 9, "response_tokens_est": 38, "n_tiles": 12, "output_text": "The image shows a cluttered desk with various electronic devices, including a computer monitor, a keyboard, a mouse, a smartphone, and a tablet, all connected to a power source.", "error": null, "sys_before": {"cpu_percent": 44.2, "ram_used_mb": 25605.3, "ram_available_mb": 100166.8, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.0, "ram_used_mb": 25605.8, "ram_available_mb": 100166.3, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.11, "peak": 16.05, "min": 13.69}, "VDD_GPU": {"avg": 34.33, "peak": 41.36, "min": 27.18}, "VIN": {"avg": 75.82, "peak": 114.64, "min": 57.64}, "VDD_CPU_SOC_MSS": {"avg": 15.8, "peak": 16.92, "min": 14.57}}, "power_watts_avg": 34.33, "energy_joules_est": 91.88, "sample_count": 20, "duration_seconds": 2.676}, "timestamp": "2026-01-16T16:54:14.296329"}
{"image_index": 497, "image_name": "000000056344.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000056344.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 2570.396, "latencies_ms": [2570.396], "images_per_second": 0.389, "prompt_tokens": 23, "response_tokens_est": 31, "n_tiles": 12, "output_text": "- computer monitor: 2\n- keyboard: 1\n- mouse: 1\n- smartphone: 2\n- tablet: 1", "error": null, "sys_before": {"cpu_percent": 33.3, "ram_used_mb": 25605.8, "ram_available_mb": 100166.3, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 2.7, "ram_used_mb": 25605.8, "ram_available_mb": 100166.3, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.09, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 35.78, "peak": 42.94, "min": 27.57}, "VIN": {"avg": 77.87, "peak": 116.55, "min": 58.51}, "VDD_CPU_SOC_MSS": {"avg": 15.73, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 35.78, "energy_joules_est": 91.99, "sample_count": 20, "duration_seconds": 2.571}, "timestamp": "2026-01-16T16:54:16.874704"}
{"image_index": 497, "image_name": "000000056344.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000056344.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 5520.443, "latencies_ms": [5520.443], "images_per_second": 0.181, "prompt_tokens": 27, "response_tokens_est": 119, "n_tiles": 12, "output_text": "The main objects in the image are arranged in a somewhat scattered manner on a desk. The leftmost object is a large monitor, which is positioned in the foreground. The monitor is connected to a power source via a cable. In the background, there are two more monitors, one of which is turned off. The rightmost object is a tablet, which is placed on the desk, slightly to the right of the monitor. The tablet is also connected to a power source via a cable. The desk itself is the central object in the image, with the monitors and tablet placed on it.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25605.8, "ram_available_mb": 100166.3, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.9, "ram_used_mb": 25605.8, "ram_available_mb": 100166.4, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.15, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 30.81, "peak": 42.94, "min": 26.39}, "VIN": {"avg": 74.46, "peak": 145.15, "min": 57.86}, "VDD_CPU_SOC_MSS": {"avg": 15.73, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 30.81, "energy_joules_est": 170.1, "sample_count": 44, "duration_seconds": 5.521}, "timestamp": "2026-01-16T16:54:22.402467"}
{"image_index": 497, "image_name": "000000056344.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000056344.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3776.364, "latencies_ms": [3776.364], "images_per_second": 0.265, "prompt_tokens": 21, "response_tokens_est": 67, "n_tiles": 12, "output_text": "The image depicts a cluttered desk with various electronic devices and gadgets. The desk is cluttered with a white keyboard, a white mouse, a smartphone, an iPod, a tablet, and a laptop. The setting appears to be indoors, possibly in a home or office environment, and the focus is on the technological devices.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25605.8, "ram_available_mb": 100166.4, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25605.8, "ram_available_mb": 100166.4, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.11, "peak": 15.65, "min": 14.63}, "VDD_GPU": {"avg": 32.68, "peak": 42.94, "min": 26.39}, "VIN": {"avg": 72.65, "peak": 115.26, "min": 58.47}, "VDD_CPU_SOC_MSS": {"avg": 15.72, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 32.68, "energy_joules_est": 123.43, "sample_count": 30, "duration_seconds": 3.777}, "timestamp": "2026-01-16T16:54:26.184912"}
{"image_index": 497, "image_name": "000000056344.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000056344.jpg", "image_width": 640, "image_height": 480, "image_resolution": "640x480", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 3275.575, "latencies_ms": [3275.575], "images_per_second": 0.305, "prompt_tokens": 19, "response_tokens_est": 52, "n_tiles": 12, "output_text": "The image shows a cluttered desk with various electronic devices, including a computer monitor, a tablet, a smartphone, a keyboard, and a mouse. The desk is illuminated by a dim, warm light, creating a cozy and somewhat cluttered atmosphere.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25605.8, "ram_available_mb": 100166.4, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 25605.8, "ram_available_mb": 100166.4, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.09, "peak": 15.65, "min": 14.53}, "VDD_GPU": {"avg": 33.74, "peak": 42.94, "min": 26.39}, "VIN": {"avg": 76.7, "peak": 127.46, "min": 58.5}, "VDD_CPU_SOC_MSS": {"avg": 15.75, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 33.74, "energy_joules_est": 110.53, "sample_count": 26, "duration_seconds": 3.276}, "timestamp": "2026-01-16T16:54:29.467089"}
{"image_index": 498, "image_name": "000000056350.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000056350.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 709.106, "latencies_ms": [709.106], "images_per_second": 1.41, "prompt_tokens": 9, "response_tokens_est": 29, "n_tiles": 1, "output_text": "The image depicts a bathroom with a large, colorful mural of people in various poses and expressions, creating a lively and dynamic scene.", "error": null, "sys_before": {"cpu_percent": 21.4, "ram_used_mb": 25605.8, "ram_available_mb": 100166.4, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 5.9, "ram_used_mb": 25605.8, "ram_available_mb": 100166.4, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 4845.7, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.07, "peak": 15.42, "min": 14.61}, "VDD_GPU": {"avg": 24.34, "peak": 27.57, "min": 22.06}, "VIN": {"avg": 62.36, "peak": 64.21, "min": 56.44}, "VDD_CPU_SOC_MSS": {"avg": 15.67, "peak": 16.14, "min": 14.96}}, "power_watts_avg": 24.34, "energy_joules_est": 17.27, "sample_count": 5, "duration_seconds": 0.709}, "timestamp": "2026-01-16T16:54:30.201658"}
{"image_index": 498, "image_name": "000000056350.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000056350.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1116.258, "latencies_ms": [1116.258], "images_per_second": 0.896, "prompt_tokens": 23, "response_tokens_est": 49, "n_tiles": 1, "output_text": "1. Toilet paper roll\n2. Toilet paper\n3. Toilet paper\n4. Toilet paper\n5. Toilet paper\n6. Toilet paper\n7. Toilet paper\n8. Toilet paper", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25605.8, "ram_available_mb": 100166.4, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25605.8, "ram_available_mb": 100166.4, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 4856.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.49, "peak": 15.62, "min": 15.22}, "VDD_GPU": {"avg": 22.36, "peak": 25.6, "min": 20.49}, "VIN": {"avg": 63.62, "peak": 71.01, "min": 57.54}, "VDD_CPU_SOC_MSS": {"avg": 16.39, "peak": 16.54, "min": 16.14}}, "power_watts_avg": 22.36, "energy_joules_est": 24.97, "sample_count": 8, "duration_seconds": 1.117}, "timestamp": "2026-01-16T16:54:31.324256"}
{"image_index": 498, "image_name": "000000056350.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000056350.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 1675.517, "latencies_ms": [1675.517], "images_per_second": 0.597, "prompt_tokens": 27, "response_tokens_est": 75, "n_tiles": 1, "output_text": "The main objects in the image are a toilet and a tiled wall. The toilet is located in the background, while the tiled wall is in the foreground. The toilet is positioned near the top of the image, and the tiled wall is below it, creating a spatial relationship where the toilet is closer to the viewer and the tiled wall is further away.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25605.8, "ram_available_mb": 100166.4, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 5.5, "ram_used_mb": 25606.1, "ram_available_mb": 100166.1, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 4859.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.6, "peak": 15.72, "min": 15.22}, "VDD_GPU": {"avg": 21.34, "peak": 24.83, "min": 20.09}, "VIN": {"avg": 63.54, "peak": 72.57, "min": 56.38}, "VDD_CPU_SOC_MSS": {"avg": 16.51, "peak": 16.93, "min": 16.14}}, "power_watts_avg": 21.34, "energy_joules_est": 35.76, "sample_count": 13, "duration_seconds": 1.676}, "timestamp": "2026-01-16T16:54:33.005430"}
{"image_index": 498, "image_name": "000000056350.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000056350.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 987.678, "latencies_ms": [987.678], "images_per_second": 1.012, "prompt_tokens": 21, "response_tokens_est": 43, "n_tiles": 1, "output_text": "The image depicts a bathroom with a toilet and a tiled floor. A group of people is lying on the floor, seemingly in a playful or humorous manner, with their arms and legs spread out.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25606.1, "ram_available_mb": 100166.1, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 5.7, "ram_used_mb": 25605.8, "ram_available_mb": 100166.4, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 4854.7, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.56, "peak": 15.72, "min": 15.32}, "VDD_GPU": {"avg": 21.89, "peak": 24.03, "min": 20.49}, "VIN": {"avg": 61.9, "peak": 65.71, "min": 53.24}, "VDD_CPU_SOC_MSS": {"avg": 16.42, "peak": 16.54, "min": 16.14}}, "power_watts_avg": 21.89, "energy_joules_est": 21.63, "sample_count": 7, "duration_seconds": 0.988}, "timestamp": "2026-01-16T16:54:33.998641"}
{"image_index": 498, "image_name": "000000056350.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000056350.jpg", "image_width": 612, "image_height": 612, "image_resolution": "612x612", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 862.056, "latencies_ms": [862.056], "images_per_second": 1.16, "prompt_tokens": 19, "response_tokens_est": 37, "n_tiles": 1, "output_text": "The image features a bathroom with white tiled walls and floor, illuminated by bright lighting. The tiles are glossy, reflecting the light and creating a clean, modern aesthetic.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25605.8, "ram_available_mb": 100166.4, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 5.3, "ram_used_mb": 25605.8, "ram_available_mb": 100166.4, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4603.2, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 4853.2, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.55, "peak": 15.72, "min": 15.32}, "VDD_GPU": {"avg": 22.32, "peak": 24.43, "min": 20.88}, "VIN": {"avg": 61.69, "peak": 65.35, "min": 55.33}, "VDD_CPU_SOC_MSS": {"avg": 16.41, "peak": 16.54, "min": 16.14}}, "power_watts_avg": 22.32, "energy_joules_est": 19.25, "sample_count": 6, "duration_seconds": 0.862}, "timestamp": "2026-01-16T16:54:34.866745"}
{"image_index": 499, "image_name": "000000056545.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000056545.jpg", "image_width": 505, "image_height": 640, "image_resolution": "505x640", "task": "caption_brief", "task_purpose": "low-latency captioning", "task_prompt": "Write one detailed sentence describing the image.", "latency_ms": 2815.012, "latencies_ms": [2815.012], "images_per_second": 0.355, "prompt_tokens": 9, "response_tokens_est": 42, "n_tiles": 12, "output_text": "The image features a bird perched on a branch, with its feathers appearing to be a mix of grey and brown tones, and the background is a blurred green, suggesting a natural, outdoor setting.", "error": null, "sys_before": {"cpu_percent": 44.7, "ram_used_mb": 25605.8, "ram_available_mb": 100166.4, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.2, "ram_used_mb": 25606.1, "ram_available_mb": 100166.1, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7042.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.23, "peak": 16.15, "min": 14.4}, "VDD_GPU": {"avg": 33.32, "peak": 40.18, "min": 26.79}, "VIN": {"avg": 77.02, "peak": 124.14, "min": 58.54}, "VDD_CPU_SOC_MSS": {"avg": 16.07, "peak": 17.32, "min": 15.35}}, "power_watts_avg": 33.32, "energy_joules_est": 93.81, "sample_count": 22, "duration_seconds": 2.815}, "timestamp": "2026-01-16T16:54:37.757034"}
{"image_index": 499, "image_name": "000000056545.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000056545.jpg", "image_width": 505, "image_height": 640, "image_resolution": "505x640", "task": "objects_and_counts", "task_purpose": "object recognition (approximate)", "task_prompt": "List up to 8 main objects with approximate counts. Use format 'object: count' on separate lines.", "latency_ms": 1911.201, "latencies_ms": [1911.201], "images_per_second": 0.523, "prompt_tokens": 23, "response_tokens_est": 11, "n_tiles": 12, "output_text": "bird: 1\ntree branch: 1", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25606.1, "ram_available_mb": 100166.1, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 2.0, "ram_used_mb": 25606.3, "ram_available_mb": 100165.9, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7054.5, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.09, "peak": 15.54, "min": 14.53}, "VDD_GPU": {"avg": 38.49, "peak": 42.94, "min": 31.12}, "VIN": {"avg": 76.12, "peak": 116.02, "min": 51.94}, "VDD_CPU_SOC_MSS": {"avg": 15.86, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 38.49, "energy_joules_est": 73.58, "sample_count": 14, "duration_seconds": 1.912}, "timestamp": "2026-01-16T16:54:39.675549"}
{"image_index": 499, "image_name": "000000056545.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000056545.jpg", "image_width": 505, "image_height": 640, "image_resolution": "505x640", "task": "spatial_relationships", "task_purpose": "spatial grounding", "task_prompt": "Write 2-3 sentences describing spatial relationships between the main objects (left/right, foreground/background, near/far).", "latency_ms": 3352.62, "latencies_ms": [3352.62], "images_per_second": 0.298, "prompt_tokens": 27, "response_tokens_est": 54, "n_tiles": 12, "output_text": "The main object in the foreground is a bird perched on a tree branch. The bird is positioned near the center of the image, with its body facing slightly to the left. The background is a blurred green, indicating a dense forest or foliage.", "error": null, "sys_before": {"cpu_percent": 16.7, "ram_used_mb": 25606.3, "ram_available_mb": 100165.9, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.3, "ram_used_mb": 25606.0, "ram_available_mb": 100166.1, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7056.9, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.05, "peak": 15.54, "min": 14.32}, "VDD_GPU": {"avg": 34.41, "peak": 44.12, "min": 26.39}, "VIN": {"avg": 75.87, "peak": 121.03, "min": 58.36}, "VDD_CPU_SOC_MSS": {"avg": 15.59, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 34.41, "energy_joules_est": 115.37, "sample_count": 26, "duration_seconds": 3.353}, "timestamp": "2026-01-16T16:54:43.034556"}
{"image_index": 499, "image_name": "000000056545.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000056545.jpg", "image_width": 505, "image_height": 640, "image_resolution": "505x640", "task": "scene_context", "task_purpose": "scene understanding", "task_prompt": "Write exactly 2 sentences describing the overall scene and setting (where it is and what is happening).", "latency_ms": 3517.91, "latencies_ms": [3517.91], "images_per_second": 0.284, "prompt_tokens": 21, "response_tokens_est": 59, "n_tiles": 12, "output_text": "The image captures a bird perched on a tree branch, with a blurred background of green foliage. The bird appears to be a small, grey bird with a dark beak, and its feathers are slightly ruffled, suggesting it might be in a natural, outdoor setting.", "error": null, "sys_before": {"cpu_percent": 0.0, "ram_used_mb": 25606.0, "ram_available_mb": 100166.1, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.5, "ram_used_mb": 25606.0, "ram_available_mb": 100166.1, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7052.3, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.08, "peak": 15.65, "min": 14.43}, "VDD_GPU": {"avg": 33.18, "peak": 42.54, "min": 26.39}, "VIN": {"avg": 75.11, "peak": 119.19, "min": 58.95}, "VDD_CPU_SOC_MSS": {"avg": 15.72, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 33.18, "energy_joules_est": 116.75, "sample_count": 27, "duration_seconds": 3.519}, "timestamp": "2026-01-16T16:54:46.559275"}
{"image_index": 499, "image_name": "000000056545.jpg", "image_path": "/home/thorwaggle1/Desktop/SageEdge/Benchmarking/data/coco_val2017/000000056545.jpg", "image_width": 505, "image_height": 640, "image_resolution": "505x640", "task": "attributes", "task_purpose": "fine-grained perception", "task_prompt": "Write exactly 2 sentences describing notable visual attributes (colors, lighting, materials, weather).", "latency_ms": 2980.421, "latencies_ms": [2980.421], "images_per_second": 0.336, "prompt_tokens": 19, "response_tokens_est": 43, "n_tiles": 12, "output_text": "The bird in the image has a grayish-brown plumage with a slightly darker back and lighter underparts. The lighting is soft and diffused, creating a gentle bokeh effect in the background.", "error": null, "sys_before": {"cpu_percent": 25.0, "ram_used_mb": 25606.0, "ram_available_mb": 100166.1, "ram_percent": 20.4}, "sys_after": {"cpu_percent": 3.1, "ram_used_mb": 25606.0, "ram_available_mb": 100166.1, "ram_percent": 20.4}, "cuda_stats": {"cuda": true, "gpu_name": "NVIDIA Thor", "gpu_mem_alloc_mb": 4616.1, "gpu_mem_reserved_mb": 11608.0, "gpu_max_mem_alloc_mb": 7050.8, "gpu_max_mem_reserved_mb": 11608.0}, "power_stats": {"method": "tegrastats", "power_rails": {"VIN_SYS_5V0": {"avg": 15.07, "peak": 15.65, "min": 14.43}, "VDD_GPU": {"avg": 34.63, "peak": 42.92, "min": 26.79}, "VIN": {"avg": 77.16, "peak": 119.51, "min": 58.4}, "VDD_CPU_SOC_MSS": {"avg": 15.66, "peak": 16.53, "min": 14.96}}, "power_watts_avg": 34.63, "energy_joules_est": 103.24, "sample_count": 23, "duration_seconds": 2.981}, "timestamp": "2026-01-16T16:54:49.546794"}
